<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Otter&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#22522;&#20110;OpenFlamingo&#35757;&#32451;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25351;&#20196;&#36319;&#38543;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.03726</link><description>&lt;p&gt;
Otter: &#19968;&#31181;&#22810;&#27169;&#24577;&#27169;&#22411;&#21450;&#20854;&#19978;&#19979;&#25991;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Otter: A Multi-Modal Model with In-Context Instruction Tuning. (arXiv:2305.03726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03726
&lt;/p&gt;
&lt;p&gt;
Otter&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#22522;&#20110;OpenFlamingo&#35757;&#32451;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25351;&#20196;&#36319;&#38543;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;&#20110;&#39044;&#35757;&#32451;&#20102;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#32780;&#23637;&#31034;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20197;&#38646;/&#23569;&#25968;&#25454;&#23398;&#20064;&#30340;&#26174;&#33879;&#26222;&#36866;&#33021;&#21147;&#65292;&#20363;&#22914;GPT-3&#65292;&#23427;&#25512;&#20986;&#20102;InstrctGPT&#21644;ChatGPT&#65292;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#30495;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#25351;&#20196;&#35843;&#25972;&#24341;&#20837;&#21040;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#24819;&#27861;&#65292;&#21463;&#21040;Flamingo&#27169;&#22411;&#19978;&#28216;&#20132;&#26367;&#26684;&#24335;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#37319;&#29992;&#31867;&#20284;&#30340;&#26041;&#27861;&#26500;&#24314;&#20102;&#25105;&#20204;&#30340;MultI-Modal In-Context Instruction Tuning (MIMIC-IT)&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Otter&#65292;&#19968;&#31181;&#22522;&#20110;OpenFlamingo&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;(DeepMind&#30340;Flamingo&#30340;&#24320;&#28304;&#29256;&#26412;)&#65292;&#23427;&#22312;MIMIC-IT&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#38024;&#23545;&#30740;&#31350;&#20154;&#21592;&#20248;&#21270;&#20102;OpenFlamingo&#30340;&#23454;&#29616;&#65292;&#23558;&#25152;&#38656;&#30340;&#35757;&#32451;&#36164;&#28304;&#20174;1&#20010;A100 GPU&#38477;&#33267;4&#20010;RTX-3090 GPU&#65292;&#20174;&#32780;&#20351;&#30740;&#31350;&#26356;&#20855;&#27665;&#20027;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. We adopt a similar approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo's implementation for researchers, democratizing the required training resources from 1$\times$ A100 GPU to 4$\times$ RTX-3090 GPUs, and integrate both Op
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#23621;&#23478;&#20581;&#24247;&#35786;&#26029;&#35774;&#22791;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#23454;&#26102;&#35780;&#20272;&#38256;&#20992;&#32454;&#32990;&#30149;&#24739;&#32773;&#30340;&#36139;&#34880;&#31243;&#24230;&#65292;&#24182;&#25552;&#20379;&#23454;&#26102;&#20449;&#24687;&#65292;&#20197;&#20415;&#20943;&#23569;&#34880;&#31649;&#38459;&#22622;&#24615;&#21361;&#26426;&#21457;&#20316;&#30340;&#39057;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03715</link><description>&lt;p&gt;
&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20110;&#23621;&#23478;&#20581;&#24247;&#35786;&#26029;&#35774;&#22791;&#20013;&#65306;&#20197;&#38256;&#20992;&#32454;&#32990;&#36139;&#34880;&#31649;&#29702;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management. (arXiv:2305.03715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#23621;&#23478;&#20581;&#24247;&#35786;&#26029;&#35774;&#22791;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#23454;&#26102;&#35780;&#20272;&#38256;&#20992;&#32454;&#32990;&#30149;&#24739;&#32773;&#30340;&#36139;&#34880;&#31243;&#24230;&#65292;&#24182;&#25552;&#20379;&#23454;&#26102;&#20449;&#24687;&#65292;&#20197;&#20415;&#20943;&#23569;&#34880;&#31649;&#38459;&#22622;&#24615;&#21361;&#26426;&#21457;&#20316;&#30340;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#23621;&#23478;&#35774;&#22791;&#30340;&#28508;&#21147;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#19982;&#20854;&#20182;&#19987;&#38376;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65292;&#23454;&#26102;&#35780;&#20272;&#38256;&#20992;&#32454;&#32990;&#30149;&#24739;&#32773;&#30340;&#36139;&#34880;&#31243;&#24230;&#12290;&#35813;&#35774;&#22791;&#20381;&#36182;&#27979;&#37327;&#34880;&#31649;&#29983;&#25104;&#29289;&#27700;&#24179;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#26469;&#35780;&#20272;&#36139;&#34880;&#31243;&#24230;&#65292;&#20026;&#24739;&#32773;&#21644;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#23454;&#26102;&#20449;&#24687;&#65292;&#20197;&#20943;&#23569;&#34880;&#31649;&#38459;&#22622;&#24615;&#21361;&#26426;&#21457;&#20316;&#30340;&#39057;&#29575;&#65292;&#22240;&#20026;&#26089;&#26399;&#21457;&#29616;&#36139;&#34880;&#31243;&#24230;&#65292;&#20801;&#35768;&#21450;&#26102;&#24178;&#39044;&#65292;&#21487;&#33021;&#38477;&#20302;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#21457;&#29983;&#29575;&#12290;&#24320;&#21457;&#36825;&#31181;&#35774;&#22791;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#21019;&#24314;&#19968;&#20010;&#21487;&#38752;&#30340;&#38750;&#20405;&#20837;&#24615;&#24037;&#20855;&#26469;&#35780;&#20272;&#34880;&#31649;&#29983;&#25104;&#29289;&#27700;&#24179;&#65292;&#24314;&#31435;&#19968;&#20010;&#29983;&#29289;&#29289;&#29702;&#27169;&#22411;&#65292;&#20197;&#21450;&#32771;&#34385;LLM&#22914;&#20309;&#20195;&#34920;&#19968;&#20010;&#26080;&#27861;&#34892;&#21160;&#30340;&#24739;&#32773;&#19982;&#24613;&#25937;&#20154;&#21592;&#36827;&#34892;&#36890;&#20449;&#31561;&#23454;&#38469;&#32771;&#34385;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#31995;&#32479;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the potential of an ambulatory device that incorporates Large Language Models (LLMs) in cadence with other specialized ML models to assess anemia severity in sickle cell patients in real time. The device would rely on sensor data that measures angiogenic material levels to assess anemia severity, providing real-time information to patients and clinicians to reduce the frequency of vaso-occlusive crises because of the early detection of anemia severity, allowing for timely interventions and potentially reducing the likelihood of serious complications. The main challenges in developing such a device are the creation of a reliable non-invasive tool for angiogenic level assessment, a biophysics model and the practical consideration of an LLM communicating with emergency personnel on behalf of an incapacitated patient. A possible system is proposed, and the limitations of this approach are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Vera&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#29616;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.03695</link><description>&lt;p&gt;
Vera&#65306;&#19968;&#20010;&#29992;&#20110;&#36890;&#29992;&#24120;&#35782;&#35821;&#21477;&#21487;&#20449;&#24230;&#35780;&#20272;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements. (arXiv:2305.03695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Vera&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#29616;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#20170;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#20986;&#29616;&#33618;&#35884;&#21644;&#24847;&#22806;&#30340;&#24120;&#35782;&#22833;&#36133;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#39038;&#24615;&#39564;&#35777;&#26041;&#27861;&#65292;&#21453;&#24605;LM&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;Vera&#65292;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#23427;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;19&#20010;QA&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#21019;&#24314;&#30340;&#32422;700&#19975;&#26465;&#24120;&#35782;&#35821;&#21477;&#20197;&#21450;&#19977;&#20010;&#35757;&#32451;&#30446;&#26631;&#30340;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;Vera&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#21508;&#31181;&#24120;&#35782;&#39046;&#22495;&#20013;&#30340;&#27491;&#30830;&#21644;&#38169;&#35823;&#35821;&#21477;&#12290;&#24403;&#24212;&#29992;&#20110;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#21487;&#37325;&#29992;&#20110;&#24120;&#35782;&#39564;&#35777;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#23427;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;Vera&#22312;&#36807;&#28388;LM&#29983;&#25104;&#30340;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#22686;&#24378;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the much discussed capabilities of today's language models, they are still prone to silly and unexpected commonsense failures. We consider a retrospective verification approach that reflects on the correctness of LM outputs, and introduce Vera, a general-purpose model that estimates the plausibility of declarative statements based on commonsense knowledge. Trained on ~7M commonsense statements created from 19 QA datasets and two large-scale knowledge bases, and with a combination of three training objectives, Vera is a versatile model that effectively separates correct from incorrect statements across diverse commonsense domains. When applied to solving commonsense problems in the verification format, Vera substantially outperforms existing models that can be repurposed for commonsense verification, and it further exhibits generalization capabilities to unseen tasks and provides well-calibrated outputs. We find that Vera excels at filtering LM-generated commonsense knowledge an
&lt;/p&gt;</description></item><item><title>DAMO-NLP&#22242;&#38431;&#30340;U-RaNER&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#21152;&#20837;&#24102;&#26377;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;Wikidata&#30693;&#35782;&#24211;&#24182;&#37319;&#29992;infusion&#26041;&#27861;&#26469;&#22686;&#24378;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#20854;&#20182;&#31995;&#32479;&#23384;&#22312;&#30340;&#30693;&#35782;&#19981;&#36275;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#26377;&#38480;&#21644;&#21333;&#19968;&#26816;&#32034;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03688</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;2&#20013;&#30340;DAMO-NLP: &#19968;&#31181;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#32479;&#19968;&#26816;&#32034;&#22686;&#24378;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition. (arXiv:2305.03688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03688
&lt;/p&gt;
&lt;p&gt;
DAMO-NLP&#22242;&#38431;&#30340;U-RaNER&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#21152;&#20837;&#24102;&#26377;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;Wikidata&#30693;&#35782;&#24211;&#24182;&#37319;&#29992;infusion&#26041;&#27861;&#26469;&#22686;&#24378;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#20854;&#20182;&#31995;&#32479;&#23384;&#22312;&#30340;&#30693;&#35782;&#19981;&#36275;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#26377;&#38480;&#21644;&#21333;&#19968;&#26816;&#32034;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MultiCoNER 2&#20849;&#20139;&#20219;&#21153;&#26088;&#22312;&#35299;&#20915;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#32454;&#31890;&#24230;&#21644;&#22024;&#26434;&#24773;&#20917;&#65292;&#24182;&#32487;&#25215;&#20102;MultiCoNER 1&#20219;&#21153;&#30340;&#35821;&#20041;&#27495;&#20041;&#21644;&#20302;&#19978;&#19979;&#25991;&#29615;&#22659;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;MultiCoNER 1&#20013;&#30340;&#21069;&#20960;&#20010;&#39030;&#23574;&#31995;&#32479;&#35201;&#20040;&#32435;&#20837;&#30693;&#35782;&#24211;&#25110;&#19987;&#26377;&#21517;&#35789;&#34920;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#30693;&#35782;&#19981;&#36275;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#26377;&#38480;&#20197;&#21450;&#21333;&#19968;&#26816;&#32034;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;DAMO-NLP&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#32479;&#19968;&#26816;&#32034;&#22686;&#24378;&#31995;&#32479;&#65288;U-RaNER&#65289;&#12290;&#25105;&#20204;&#23545;&#19978;&#36848;&#20960;&#20010;&#39030;&#23574;&#31995;&#32479;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#24615;&#33021;&#29942;&#39048;&#22312;&#20110;&#30693;&#35782;&#19981;&#36275;&#65292;&#32780;&#19988;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#20351;&#24471;&#26816;&#32034;&#30693;&#35782;&#23545;&#27169;&#22411;&#19981;&#21487;&#35265;&#12290;&#20026;&#20102;&#22686;&#24378;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;Wikidata&#30693;&#35782;&#24211;&#65292;&#24182;&#37319;&#29992;infusion&#26041;&#27861;&#26469;&#25299;&#23485;&#19978;&#19979;&#25991;&#24341;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The MultiCoNER \RNum{2} shared task aims to tackle multilingual named entity recognition (NER) in fine-grained and noisy scenarios, and it inherits the semantic ambiguity and low-context setting of the MultiCoNER \RNum{1} task. To cope with these problems, the previous top systems in the MultiCoNER \RNum{1} either incorporate the knowledge bases or gazetteers. However, they still suffer from insufficient knowledge, limited context length, single retrieval strategy. In this paper, our team \textbf{DAMO-NLP} proposes a unified retrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We perform error analysis on the previous top systems and reveal that their performance bottleneck lies in insufficient knowledge. Also, we discover that the limited context length causes the retrieval knowledge to be invisible to the model. To enhance the retrieval context, we incorporate the entity-centric Wikidata knowledge base, while utilizing the infusion approach to broaden the contextua
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; WikiWeb2M &#30340; 2M &#20010;&#22810;&#27169;&#24577;&#32593;&#39029;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#35813;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;&#19977;&#20010;&#29983;&#25104;&#20219;&#21153;&#24182;&#39564;&#35777;&#20102;&#25104;&#21151;&#24615;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Prefix Global &#30340;&#26032;&#39062;&#27880;&#24847;&#26426;&#21046;&#65292;&#20063;&#23545;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#20026;&#22810;&#27169;&#24577;&#32593;&#39029;&#29702;&#35299;&#20219;&#21153;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#21644;&#23454;&#39564;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2305.03668</link><description>&lt;p&gt;
&#19968;&#20010;&#22810;&#23618;&#22810;&#27169;&#24577;&#32593;&#39029;&#29983;&#25104;&#20219;&#21153;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding. (arXiv:2305.03668v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03668
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; WikiWeb2M &#30340; 2M &#20010;&#22810;&#27169;&#24577;&#32593;&#39029;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#35813;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;&#19977;&#20010;&#29983;&#25104;&#20219;&#21153;&#24182;&#39564;&#35777;&#20102;&#25104;&#21151;&#24615;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Prefix Global &#30340;&#26032;&#39062;&#27880;&#24847;&#26426;&#21046;&#65292;&#20063;&#23545;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#20026;&#22810;&#27169;&#24577;&#32593;&#39029;&#29702;&#35299;&#20219;&#21153;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#21644;&#23454;&#39564;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#39029;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#35270;&#35273;-&#35821;&#35328;&#21644;&#32431;&#35821;&#35328;&#20219;&#21153;&#36164;&#28304;&#65292;&#20294;&#21482;&#26377;&#22270;&#20687;-&#26631;&#39064;&#23545;&#12289;&#38271;&#25991;&#26412;&#25991;&#31456;&#25110;&#21407;&#22987;HTML&#31561;&#37096;&#20998;&#32452;&#25104;&#30340;&#32593;&#39029;&#24471;&#20197;&#20445;&#23384;&#65292;&#27809;&#26377;&#19968;&#20010;&#21253;&#21547;&#20840;&#37096;&#20449;&#24687;&#30340;&#32593;&#39029;&#12290;&#22240;&#27492;&#65292;&#32593;&#39029;&#20219;&#21153;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#32467;&#26500;&#21450;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#21033;&#29992;&#26041;&#38754;&#19968;&#30452;&#21463;&#21040;&#20851;&#27880;&#30340;&#36739;&#23569;&#12290;&#20026;&#20102;&#30740;&#31350;&#22810;&#27169;&#24577;&#32593;&#39029;&#29702;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; Wikipedia Webpage &#22871;&#20214; (WikiWeb2M) &#65292;&#21253;&#21547; 2M &#20010;&#39029;&#38754;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#29983;&#25104;&#20219;&#21153;&#19978;&#39564;&#35777;&#20854;&#23454;&#29992;&#24615;: &#39029;&#38754;&#25551;&#36848;&#29983;&#25104;&#12289;&#31456;&#33410;&#25688;&#35201;&#21644;&#29615;&#22659;&#22270;&#20687;&#23383;&#24149;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#27880;&#24847;&#26426;&#21046; Prefix Global&#65292;&#23427;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#20869;&#23481;&#20316;&#20026;&#20840;&#23616;&#26631;&#35760;&#65292;&#20197;&#20415;&#20110;&#20851;&#27880;&#32593;&#39029;&#30340;&#20854;&#20313;&#37096;&#20998;&#20197;&#33719;&#21462;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#20351;&#29992;&#39029;&#38754;&#32467;&#26500;&#26469;&#20998;&#31163;&#36825;&#20123;&#26631;&#35760;&#65292;&#23427;&#27604;&#20840;&#27880;&#24847;&#21147;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;WikiWeb2M &#30340;&#26032;&#27880;&#37322;&#25913;&#36827;&#20102;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23545;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Webpages have been a rich, scalable resource for vision-language and language only tasks. Yet only pieces of webpages are kept: image-caption pairs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured image-text data left underused. To study multimodal webpage understanding, we introduce the Wikipedia Webpage suite (WikiWeb2M) of 2M pages. We verify its utility on three generative tasks: page description generation, section summarization, and contextual image captioning. We design a novel attention mechanism Prefix Global, which selects the most relevant image and text content as global tokens to attend to the rest of the webpage for context. By using page structure to separate such tokens, it performs better than full attention with lower computational complexity. Experiments show that the new annotations from WikiWeb2M improve task performance compared to data from prior work. We also include ablations on se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20837;&#38498;&#35760;&#24405;&#36827;&#34892;&#24739;&#32773;&#24182;&#21457;&#30151;&#39118;&#38505;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Longformer&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#24739;&#32773;&#30340;&#39118;&#38505;&#35780;&#20998;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#27431;&#27954;&#21307;&#38498;&#30340;&#24739;&#32773;&#25968;&#25454;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#21644;&#20854;&#20182;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.03661</link><description>&lt;p&gt;
&#20174;&#20837;&#38498;&#35760;&#24405;&#39044;&#27979;COVID-19&#21644;&#32954;&#28814;&#24182;&#21457;&#30151;
&lt;/p&gt;
&lt;p&gt;
Predicting COVID-19 and pneumonia complications from admission texts. (arXiv:2305.03661v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20837;&#38498;&#35760;&#24405;&#36827;&#34892;&#24739;&#32773;&#24182;&#21457;&#30151;&#39118;&#38505;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Longformer&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#24739;&#32773;&#30340;&#39118;&#38505;&#35780;&#20998;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#27431;&#27954;&#21307;&#38498;&#30340;&#24739;&#32773;&#25968;&#25454;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#21644;&#20854;&#20182;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20837;&#38498;&#25253;&#21578;&#30340;&#39118;&#38505;&#35780;&#20272;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22240;&#32954;&#28814;&#25110;COVID-19&#32780;&#20303;&#38498;&#30340;&#24739;&#32773;&#30340;&#24182;&#21457;&#30151;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;Longformer&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#20837;&#38498;&#35760;&#24405;&#21644;&#20854;&#20182;&#21487;&#29992;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#35745;&#31639;&#20986;&#24739;&#32773;&#30340;&#39118;&#38505;&#35780;&#20998;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#27431;&#27954;&#21307;&#38498;&#30340;&#24739;&#32773;&#25968;&#25454;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;Transformer&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#26426;&#26500;&#21644;&#35786;&#26029;&#20013;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#30340;&#20854;&#20182;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a novel approach to risk assessment for patients hospitalized with pneumonia or COVID-19 based on their admission reports. We applied a Longformer neural network to admission reports and other textual data available shortly after admission to compute risk scores for the patients. We used patient data of multiple European hospitals to demonstrate that our approach outperforms the Transformer baselines. Our experiments show that the proposed model generalises across institutions and diagnoses. Also, our method has several other advantages described in the paper.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#23884;&#20837;&#26469;&#26816;&#32034;&#30456;&#24212;&#30340;&#20505;&#36873;&#25918;&#23556;&#23398;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#39046;&#22495;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#25253;&#21578;&#65292;&#21487;&#25233;&#21046;&#34394;&#26500;&#30340;&#29983;&#25104;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#20020;&#24202;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.03660</link><description>&lt;p&gt;
&#21033;&#29992;OpenAI GPT&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models. (arXiv:2305.03660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#23884;&#20837;&#26469;&#26816;&#32034;&#30456;&#24212;&#30340;&#20505;&#36873;&#25918;&#23556;&#23398;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#39046;&#22495;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#25253;&#21578;&#65292;&#21487;&#25233;&#21046;&#34394;&#26500;&#30340;&#29983;&#25104;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#20020;&#24202;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Retrieval Augmented Generation (RAG) &#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#23884;&#20837;&#26469;&#26816;&#32034;&#30456;&#24212;&#30340;&#20505;&#36873;&#25918;&#23556;&#23398;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#20687;OpenAI text-davinci-003&#12289;gpt-3.5-turbo&#21644;gpt-4&#36825;&#26679;&#30340;&#36890;&#29992;&#39046;&#22495;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#25253;&#21578;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25233;&#21046;&#34394;&#26500;&#30340;&#29983;&#25104;&#24182;&#25552;&#20379;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#20197;&#25105;&#20204;&#25152;&#38656;&#30340;&#26684;&#24335;&#29983;&#25104;&#25253;&#21578;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20020;&#24202;&#25351;&#26631;&#65292;BERTScore&#20026;0.2865&#65288;&#916;+25.88%&#65289;&#65292;Semb Score&#20026;0.4026&#65288;&#916;+6.31%&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20020;&#24202;&#35774;&#32622;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#22686;&#24378;&#33258;&#21160;&#29983;&#25104;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#36807;&#31243;&#65292;&#21516;&#26102;&#20855;&#22791;&#36866;&#21512;&#35813;&#35774;&#32622;&#30340;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Retrieval Augmented Generation (RAG) as an approach for automated radiology report writing that leverages multimodally aligned embeddings from a contrastively pretrained vision language model for retrieval of relevant candidate radiology text for an input radiology image and a general domain generative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for report generation using the relevant radiology text retrieved. This approach keeps hallucinated generations under check and provides capabilities to generate report content in the format we desire leveraging the instruction following capabilities of these generative models. Our approach achieves better clinical metrics with a BERTScore of 0.2865 ({\Delta}+ 25.88%) and Semb score of 0.4026 ({\Delta}+ 6.31%). Our approach can be broadly relevant for different clinical settings as it allows to augment the automated radiology report generation process with content relevant for that setting while also having the abilit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30333;&#30418;&#22810;&#30446;&#26631;&#23545;&#35805;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;DGSlow&#12290;&#36890;&#36807;&#24179;&#34913;&#29983;&#25104;&#20934;&#30830;&#24615;&#21644;&#38271;&#24230;&#20004;&#20010;&#30446;&#26631;&#65292;DGSlow&#21033;&#29992;&#29983;&#25104;&#26356;&#38271;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03655</link><description>&lt;p&gt;
&#30333;&#30418;&#22810;&#30446;&#26631;&#23545;&#35805;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
White-Box Multi-Objective Adversarial Attack on Dialogue Generation. (arXiv:2305.03655v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30333;&#30418;&#22810;&#30446;&#26631;&#23545;&#35805;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;DGSlow&#12290;&#36890;&#36807;&#24179;&#34913;&#29983;&#25104;&#20934;&#30830;&#24615;&#21644;&#38271;&#24230;&#20004;&#20010;&#30446;&#26631;&#65292;DGSlow&#21033;&#29992;&#29983;&#25104;&#26356;&#38271;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#22312;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#29983;&#25104;&#31995;&#32479;&#20013;&#24456;&#21463;&#27426;&#36814;&#12290; &#28982;&#32780;&#65292;&#36825;&#31181;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#36825;&#22312;&#20256;&#32479;&#20219;&#21153;&#65288;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#30740;&#31350;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#23545;&#23427;&#20204;&#22312;DG&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#22909;&#22855;&#24515;&#12290; &#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#25915;&#20987;DG&#27169;&#22411;&#30340;&#26102;&#20505;&#65292;&#23545;&#24403;&#21069;&#21477;&#23376;&#30340;&#25200;&#21160;&#20960;&#20046;&#19981;&#20250;&#38477;&#20302;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#65292;&#22240;&#20026;&#26410;&#25913;&#21464;&#30340;&#32842;&#22825;&#35760;&#24405;&#20063;&#20250;&#34987;&#32771;&#34385;&#36827;&#34892;&#20915;&#31574;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#23545;&#25239;&#26679;&#26412;&#26469;&#36843;&#20351;&#29983;&#25104;&#26356;&#38271;&#30340;&#36755;&#20986;&#65292;&#26377;&#21033;&#20110;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615; - &#29983;&#25104;&#30340;&#21709;&#24212;&#36890;&#24120;&#26159;&#19981;&#30456;&#20851;&#12289;&#20887;&#38271;&#21644;&#37325;&#22797;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGSlow&#30340;&#30333;&#30418;&#22810;&#30446;&#26631;&#25915;&#20987;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DGSlow&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#22120;&#24179;&#34913;&#20004;&#20010;&#30446;&#26631; - &#29983;&#25104;&#20934;&#30830;&#24230;&#21644;&#38271;&#24230;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#26041;&#27861;&#23454;&#26045;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformers are popular in state-of-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making. Instead of merely pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that crafting adversarial samples to force longer generation outputs benefits attack effectiveness -- the generated responses are typically irrelevant, lengthy, and repetitive. To this end, we propose a white-box multi-objective attack method called DGSlow. Specifically, DGSlow balances two objectives -- generation accuracy and length, via a gradient-based multi-objective optimizer and applies an adapti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#35843;&#25972;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#20849;&#21516;&#25552;&#21462;RCT&#25253;&#21578;&#20013;&#30340;&#24178;&#39044;&#12289;&#32467;&#26524;&#21644;&#21457;&#29616;&#20449;&#24687;&#65292;&#23454;&#29616;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.03642</link><description>&lt;p&gt;
LLM&#27169;&#22411;&#20849;&#21516;&#25552;&#21462;RCT&#25253;&#21578;&#20013;&#24178;&#39044;&#12289;&#32467;&#26524;&#21644;&#21457;&#29616;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs. (arXiv:2305.03642v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#35843;&#25972;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#20849;&#21516;&#25552;&#21462;RCT&#25253;&#21578;&#20013;&#30340;&#24178;&#39044;&#12289;&#32467;&#26524;&#21644;&#21457;&#29616;&#20449;&#24687;&#65292;&#23454;&#29616;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCT&#65289;&#30340;&#32467;&#26524;&#30830;&#23450;&#24178;&#39044;&#25514;&#26045;&#30340;&#30456;&#23545;&#26377;&#25928;&#24615;&#65292;&#36827;&#32780;&#25104;&#20026;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#30103;&#20445;&#20581;&#30340;&#20851;&#38190;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;RCT&#32467;&#26524;&#20197;&#65288;&#36890;&#24120;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#65289;&#33258;&#28982;&#35821;&#35328;&#25991;&#31456;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#25551;&#36848;&#35797;&#39564;&#30340;&#35774;&#35745;&#12289;&#25191;&#34892;&#21644;&#32467;&#26524;&#65307;&#20020;&#24202;&#21307;&#29983;&#24517;&#39035;&#20174;&#36825;&#20123;&#25991;&#31456;&#20013;&#25163;&#21160;&#25552;&#21462;&#26377;&#20851;&#25152;&#20851;&#27880;&#30340;&#24178;&#39044;&#25514;&#26045;&#21644;&#32467;&#26524;&#30340;&#21457;&#29616;&#12290;&#36825;&#31181;&#32321;&#29712;&#30340;&#25163;&#21160;&#36807;&#31243;&#20419;&#20351;&#20154;&#20204;&#21033;&#29992;&#65288;&#21322;&#65289;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#20174;&#35797;&#39564;&#25253;&#21578;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#35777;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#22522;&#20110;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#20020;&#24202;&#25688;&#35201;&#20013;&#20849;&#21516;&#25552;&#21462;&#24178;&#39044;&#25514;&#26045;&#12289;&#32467;&#26524;&#21644;&#27604;&#36739;&#22240;&#32032;&#65288;ICO&#20803;&#32032;&#65289;&#65292;&#24182;&#25512;&#26029;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#20154;&#24037;&#65288;&#19987;&#23478;&#65289;&#21644;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;&#23558;&#35777;&#25454;&#25552;&#21462;&#26694;&#26550;&#20316;&#20026;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#20026;&#27492;&#30446;&#30340;&#24494;&#35843;LLMs&#21487;&#20197;&#23454;&#29616;&#30456;&#24403;&#22823;&#30340;&#65288;&#32422;20&#20010;&#28857;&#65289;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Results from Randomized Controlled Trials (RCTs) establish the comparative effectiveness of interventions, and are in turn critical inputs for evidence-based care. However, results from RCTs are presented in (often unstructured) natural language articles describing the design, execution, and outcomes of trials; clinicians must manually extract findings pertaining to interventions and outcomes of interest from such articles. This onerous manual process has motivated work on (semi-)automating extraction of structured evidence from trial reports. In this work we propose and evaluate a text-to-text model built on instruction-tuned Large Language Models (LLMs) to jointly extract Interventions, Outcomes, and Comparators (ICO elements) from clinical abstracts, and infer the associated results reported. Manual (expert) and automated evaluations indicate that framing evidence extraction as a conditional generation task and fine-tuning LLMs for this purpose realizes considerable ($\sim$20 point 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#36890;&#36807;&#25968;&#25454;&#25972;&#21512;&#26469;&#25552;&#39640;&#22270;&#20687;&#23383;&#24149;&#36136;&#37327;&#30340;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#36164;&#28304;&#35757;&#32451;&#20102;&#27604;&#22522;&#20934;&#27169;&#22411;&#26356;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.03610</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Curation for Image Captioning with Text-to-Image Generative Models. (arXiv:2305.03610v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#36890;&#36807;&#25968;&#25454;&#25972;&#21512;&#26469;&#25552;&#39640;&#22270;&#20687;&#23383;&#24149;&#36136;&#37327;&#30340;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#36164;&#28304;&#35757;&#32451;&#20102;&#27604;&#22522;&#20934;&#27169;&#22411;&#26356;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#20687;&#23383;&#24149;&#25216;&#26415;&#30340;&#21457;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#26085;&#30410;&#20381;&#36182;&#20110;&#35745;&#31639;&#36164;&#28304;&#21644;&#36234;&#26469;&#36234;&#22823;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#25454;&#25972;&#21512;&#30340;&#20004;&#31181;&#26041;&#27861;&#25506;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#25552;&#39640;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#36136;&#37327;&#26469;&#25913;&#21892;&#24615;&#33021;&#65306;&#19968;&#31181;&#26041;&#27861;&#20551;&#23450;&#30001;&#20110;&#22270;&#20687;&#21644;&#23383;&#24149;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#26576;&#20123;&#31034;&#20363;&#24212;&#35813;&#36991;&#20813;&#20351;&#29992;&#65292;&#21478;&#19968;&#31181;&#26041;&#27861;&#21017;&#20551;&#23450;&#19981;&#21305;&#37197;&#21487;&#20197;&#36890;&#36807;&#26367;&#25442;&#22270;&#20687;&#26469;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in image captioning are mainly driven by large-scale vision-language pretraining, relying heavily on computational resources and increasingly large multimodal datasets. Instead of scaling up pretraining data, we ask whether it is possible to improve performance by improving the quality of the samples in existing datasets. We pursue this question through two approaches to data curation: one that assumes that some examples should be avoided due to mismatches between the image and caption, and one that assumes that the mismatch can be addressed by replacing the image, for which we use the state-of-the-art Stable Diffusion model. These approaches are evaluated using the BLIP model on MS COCO and Flickr30K in both finetuning and few-shot learning settings. Our simple yet effective approaches consistently outperform baselines, indicating that better image captioning models can be trained by curating existing resources. Finally, we conduct a human study to understand the error
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35821;&#20041;&#24863;&#30693;&#30340;&#20840;&#23616;&#36866;&#24212;&#24490;&#29615;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#23548;&#35821;&#35328;&#27169;&#22359;&#21644;&#22806;&#35266;&#35821;&#20041;&#35270;&#35273;&#27169;&#22359;&#65292;&#20197;&#21450;&#20840;&#23616;&#33258;&#36866;&#24212;&#32858;&#21512;&#27169;&#22359;&#65292;&#25552;&#39640;&#20102;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#35821;&#20041;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.03602</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#37325;&#35821;&#20041;&#24863;&#30693;&#30340;&#20840;&#23616;&#36866;&#24212;&#24490;&#29615;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
A Dual Semantic-Aware Recurrent Global-Adaptive Network For Vision-and-Language Navigation. (arXiv:2305.03602v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35821;&#20041;&#24863;&#30693;&#30340;&#20840;&#23616;&#36866;&#24212;&#24490;&#29615;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#23548;&#35821;&#35328;&#27169;&#22359;&#21644;&#22806;&#35266;&#35821;&#20041;&#35270;&#35273;&#27169;&#22359;&#65292;&#20197;&#21450;&#20840;&#23616;&#33258;&#36866;&#24212;&#32858;&#21512;&#27169;&#22359;&#65292;&#25552;&#39640;&#20102;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#35821;&#20041;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#26159;&#19968;&#39033;&#29616;&#23454;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20195;&#29702;&#20351;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#32447;&#32034;&#23450;&#20301;&#30446;&#26631;&#21306;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35821;&#20041;&#24863;&#30693;&#30340;&#20840;&#23616;&#36866;&#24212;&#24490;&#29615;&#32593;&#32476; (DSRG) &#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;DSRG &#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#23548;&#35821;&#35328;&#27169;&#22359; (IGL) &#21644;&#19968;&#20010;&#22806;&#35266;&#35821;&#20041;&#35270;&#35273;&#27169;&#22359; (ASV) &#20998;&#21035;&#25552;&#39640;&#35270;&#35273;&#21644;&#35821;&#35328;&#35821;&#20041;&#23398;&#20064;&#12290;&#23545;&#20110;&#20869;&#23384;&#26426;&#21046;&#65292;&#36824;&#24341;&#20837;&#20102;&#20840;&#23616;&#33258;&#36866;&#24212;&#32858;&#21512;&#27169;&#22359; (GAA)&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-and-Language Navigation (VLN) is a realistic but challenging task that requires an agent to locate the target region using verbal and visual cues. While significant advancements have been achieved recently, there are still two broad limitations: (1) The explicit information mining for significant guiding semantics concealed in both vision and language is still under-explored; (2) The previously structured map method provides the average historical appearance of visited nodes, while it ignores distinctive contributions of various images and potent information retention in the reasoning process. This work proposes a dual semantic-aware recurrent global-adaptive network (DSRG) to address the above problems. First, DSRG proposes an instruction-guidance linguistic module (IGL) and an appearance-semantics visual module (ASV) for boosting vision and language semantic learning respectively. For the memory mechanism, a global adaptive aggregation module (GAA) is devised for explicit pano
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.03598</link><description>&lt;p&gt;
NLI4CT&#65306;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35299;&#37322;&#21644;&#26816;&#32034;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;&#21307;&#23398;&#35777;&#25454;&#65311;&#22810;&#24180;&#26469;&#65292;&#31215;&#32047;&#19979;&#26469;&#30340;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21253;&#21547;&#20102;&#21457;&#23637;&#20010;&#24615;&#21270;&#21307;&#23398;&#25152;&#24517;&#38656;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25214;&#21040;&#26368;&#20339;&#30340;&#23454;&#39564;&#27835;&#30103;&#35777;&#25454;&#65292;&#25163;&#21160;&#26816;&#26597;&#36229;&#36807;400,000&#20010;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#26159;&#23454;&#38469;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20801;&#35768;&#21487;&#25193;&#23637;&#35745;&#31639;&#25991;&#26412;&#34164;&#21547;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLI&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#20043;&#21069;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#26080;&#27861;&#25429;&#25417;CTR&#25512;&#29702;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#65292;&#20197;&#25512;&#36827;&#20851;&#20110;CTR&#25512;&#29702;&#30340;NLI&#30740;&#31350;&#12290;&#35813;&#36164;&#28304;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;NLI4CT&#65292;&#19968;&#20010;&#22522;&#20110;CTR&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#26469;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;&#65292;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03584</link><description>&lt;p&gt;
&#29616;&#22312;&#23427;&#21548;&#36215;&#26469;&#20687;&#20320;&#20102;&#65306;&#22312;&#35774;&#22791;&#19978;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;
&lt;/p&gt;
&lt;p&gt;
Now It Sounds Like You: Learning Personalized Vocabulary On Device. (arXiv:2305.03584v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03584
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#26469;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;&#65292;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#36827;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#20391;&#37325;&#20110;&#24212;&#29992;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#35774;&#22791;&#31471;&#35821;&#35328;&#24314;&#27169;&#12290;&#30001;&#20110;&#20869;&#23384;&#21644;&#24310;&#36831;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#25903;&#25345;&#23376;&#21333;&#35789;&#26631;&#35760;&#25110;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#20915;&#23450;&#37096;&#32626;&#23553;&#38381;&#35789;&#27719;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23553;&#38381;&#35789;&#27719;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#29305;&#23450;&#29992;&#25143;&#30340;&#29983;&#35789;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#65292;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#23545;&#20869;&#23384;&#21644;&#24310;&#36831;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#65292;&#26377;&#25928;&#22320;&#20174;&#20013;&#22830;&#27169;&#22411;&#20256;&#36755;&#30693;&#35782;&#65292;&#24182;&#20026;&#20010;&#24615;&#21270;&#35789;&#27719;&#23398;&#20064;&#21333;&#35789;&#23884;&#20837;&#12290;&#22312;&#19968;&#32452;&#24120;&#35265;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#29983;&#35789;&#25193;&#23637;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Federated Learning (FL) has shown significant advancements in its ability to perform various natural language processing (NLP) tasks. This work focuses on applying personalized FL for on-device language modeling. Due to limitations of memory and latency, these models cannot support the complexity of sub-word tokenization or beam search decoding, resulting in the decision to deploy a closed-vocabulary language model. However, closed-vocabulary models are unable to handle out-of-vocabulary (OOV) words belonging to specific users. To address this issue, We propose a novel technique called "OOV expansion" that improves OOV coverage and increases model accuracy while minimizing the impact on memory and latency. This method introduces a personalized "OOV adapter" that effectively transfers knowledge from a central model and learns word embedding for personalized vocabulary. OOV expansion significantly outperforms standard FL personalization methods on a set of common FL benc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#22312;&#35821;&#22659;&#23398;&#20064;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#22312;&#35821;&#22659;&#23398;&#20064;&#24212;&#35813;&#26159;&#20445;&#25345;&#32467;&#26524;&#19982;&#20854;&#35821;&#22659;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20855;&#26377;&#38271;&#26399;&#36830;&#36143;&#24615;&#30340;&#26679;&#20363;&#29992;&#20110;&#32763;&#35793;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03573</link><description>&lt;p&gt;
&#22312;&#35821;&#22659;&#23398;&#20064;&#20013;&#20445;&#25345;&#36830;&#36143;&#24615;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21363;&#24109;&#26426;&#22120;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models. (arXiv:2305.03573v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#22312;&#35821;&#22659;&#23398;&#20064;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#22312;&#35821;&#22659;&#23398;&#20064;&#24212;&#35813;&#26159;&#20445;&#25345;&#32467;&#26524;&#19982;&#20854;&#35821;&#22659;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20855;&#26377;&#38271;&#26399;&#36830;&#36143;&#24615;&#30340;&#26679;&#20363;&#29992;&#20110;&#32763;&#35793;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#25226;&#22312;&#35821;&#22659;&#23398;&#20064;&#29616;&#35937;&#30475;&#20570;&#26159;&#8220;&#20174;&#20363;&#23376;&#20013;&#23398;&#20064;&#8221;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#23558;&#22312;&#35821;&#22659;&#23398;&#20064;&#30475;&#20316;&#26159;&#29983;&#25104;&#32467;&#26524;&#20219;&#21153;&#65292;&#35201;&#27714;&#32467;&#26524;&#19982;&#20854;&#35821;&#22659;&#20855;&#26377;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;4&#20010;&#39046;&#22495;&#30340;&#38543;&#26426;&#26679;&#20363;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#26174;&#31034;&#39046;&#22495;&#20869;&#26679;&#20363;&#26102;&#65292;&#32763;&#35793;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#31227;&#21160;&#31383;&#21475;&#30340;&#39046;&#22495;&#20869;&#35774;&#32622;&#20013;&#30340;&#36830;&#36143;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#25991;&#29486;&#20013;&#20808;&#21069;&#30830;&#23450;&#30340;&#20854;&#20182;&#22240;&#32032;&#22914;&#38271;&#24230;&#12289;&#34920;&#38754;&#30456;&#20284;&#24615;&#21644;&#21477;&#23376;&#23884;&#20837;&#30456;&#20284;&#24615;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;3&#20010;&#27169;&#22411;&#65288;GPTNeo2.7B&#12289;Bloom3B&#12289;XGLM2.9B&#65289;&#21644;3&#20010;&#32763;&#35793;&#26041;&#21521;&#65288;en&#8594;{pt&#65292;de&#65292;fr}&#65289;&#20013;&#65292;&#26679;&#20363;&#19982;&#27979;&#35797;&#21477;&#23376;&#30340;&#38271;&#26399;&#36830;&#36143;&#24615;&#26159;&#27969;&#21521;&#32763;&#35793;&#24615;&#33021;&#30340;&#33391;&#22909;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The phenomena of in-context learning has typically been thought of as "learning from examples". In this work which focuses on Machine Translation, we present a perspective of in-context learning as the desired generation task maintaining coherency with its context, i.e., the prompt examples. We first investigate randomly sampled prompts across 4 domains, and find that translation performance improves when shown in-domain prompts. Next, we investigate coherency for the in-domain setting, which uses prompt examples from a moving window. We study this with respect to other factors that have previously been identified in the literature such as length, surface similarity and sentence embedding similarity. Our results across 3 models (GPTNeo2.7B, Bloom3B, XGLM2.9B), and three translation directions (\texttt{en}$\rightarrow$\{\texttt{pt, de, fr}\}) suggest that the long-term coherency of the prompts and the test sentence is a good indicator of downstream translation performance. In doing so, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23450;&#21046;&#21270;&#30340;&#38463;&#25289;&#20271;&#35821;&#38271;&#25991;&#26723;&#20998;&#31867;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#24182;&#19982;Longformer&#21644;RoBERT&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;Longformer&#21644;RoBERT&#12290;</title><link>http://arxiv.org/abs/2305.03519</link><description>&lt;p&gt;
&#21033;&#29992;BERT&#35821;&#35328;&#27169;&#22411;&#23545;&#38463;&#25289;&#20271;&#35821;&#38271;&#25991;&#26723;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Leveraging BERT Language Model for Arabic Long Document Classification. (arXiv:2305.03519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23450;&#21046;&#21270;&#30340;&#38463;&#25289;&#20271;&#35821;&#38271;&#25991;&#26723;&#20998;&#31867;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#24182;&#19982;Longformer&#21644;RoBERT&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;Longformer&#21644;RoBERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#20840;&#29699;&#38463;&#25289;&#20271;&#35821;&#20351;&#29992;&#32773;&#30340;&#25968;&#37327;&#20197;&#21450;&#22312;&#26576;&#20123;&#39046;&#22495;&#65288;&#22914;&#27861;&#24459;&#12289;&#21307;&#23398;&#29978;&#33267;&#26032;&#38395;&#65289;&#20013;&#32593;&#19978;&#30340;&#20869;&#23481;&#25968;&#37327;&#26174;&#33879;&#22686;&#38271;&#65292;&#38271;&#25991;&#26723;&#20250;&#34987;&#23450;&#26399;&#20135;&#29983;&#12290;&#20351;&#29992;&#20256;&#32479;&#23398;&#20064;&#27169;&#22411;&#23545;&#36825;&#20123;&#25991;&#26723;&#36827;&#34892;&#20998;&#31867;&#36890;&#24120;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#25991;&#26723;&#30340;&#38271;&#24230;&#22686;&#21152;&#20250;&#20351;&#35745;&#31639;&#35201;&#27714;&#25345;&#32493;&#19978;&#21319;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#19987;&#38376;&#20026;&#38271;&#25991;&#26412;&#25991;&#26723;&#23450;&#21046;&#36825;&#20123;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27169;&#22411;&#26469;&#20998;&#31867;&#38463;&#25289;&#20271;&#35821;&#30340;&#38271;&#25991;&#26723;&#12290;&#25105;&#20204;&#36824;&#24494;&#35843;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#21363;Longformer&#21644;RoBERT&#65292;&#23436;&#25104;&#20102;&#21516;&#26679;&#30340;&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#32467;&#26524;&#19982;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20004;&#20010;&#27169;&#22411;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#20013;&#37117;&#20248;&#20110;Longformer&#21644;RoBERT&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the number of Arabic speakers worldwide and the notably large amount of content in the web today in some fields such as law, medicine, or even news, documents of considerable length are produced regularly. Classifying those documents using traditional learning models is often impractical since extended length of the documents increases computational requirements to an unsustainable level. Thus, it is necessary to customize these models specifically for long textual documents. In this paper we propose two simple but effective models to classify long length Arabic documents. We also fine-tune two different models-namely, Longformer and RoBERT, for the same task and compare their results to our models. Both of our models outperform the Longformer and RoBERT in this task over two different datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#65288;BSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#30456;&#20284;&#20219;&#21153;&#30340;&#38598;&#21512;&#20013;&#35782;&#21035;&#23376;&#31354;&#38388;&#65292;&#20197;&#25913;&#21892;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#30340;&#36890;&#29992;&#24615;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#21644;LLMs&#19978;&#37117;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.03518</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Black-box Prompt Tuning with Subspace Learning. (arXiv:2305.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#65288;BSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#30456;&#20284;&#20219;&#21153;&#30340;&#38598;&#21512;&#20013;&#35782;&#21035;&#23376;&#31354;&#38388;&#65292;&#20197;&#25913;&#21892;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#30340;&#36890;&#29992;&#24615;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#21644;LLMs&#19978;&#37117;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#20351;&#29992;&#26080;&#23548;&#25968;&#20248;&#21270;&#31639;&#27861;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#25552;&#31034;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32593;&#32476;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;LLMs&#19978;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#36825;&#19982;&#19981;&#24688;&#24403;&#30340;&#23376;&#31354;&#38388;&#36873;&#25321;&#26377;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#65288;BSL&#65289;&#26469;&#25913;&#21892;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#30340;&#36890;&#29992;&#24615;&#12290;&#22522;&#20110;&#30456;&#20284;&#20219;&#21153;&#30340;&#38598;&#21512;&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#20197;&#30830;&#23450;&#23376;&#31354;&#38388;&#21487;&#20197;&#35782;&#21035;&#31867;&#20284;&#20219;&#21153;&#30340;&#26368;&#20248;&#25552;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#20445;&#35777;&#22312;&#30456;&#20284;&#20219;&#21153;&#19978;&#36827;&#34892;&#23376;&#31354;&#38388;&#20248;&#21270;&#25214;&#21040;&#19968;&#20010;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#25552;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;BSL&#26694;&#26550;&#26080;&#35770;&#22312;&#19979;&#28216;&#20219;&#21153;&#21644;LLMs&#19978;&#37117;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box prompt tuning uses derivative-free optimization algorithms to learn prompts in low-dimensional subspaces instead of back-propagating through the network of Large Language Models (LLMs). Recent studies have found that black-box prompt tuning lacks versatility across tasks and LLMs, which we believe is related to the inappropriate choice of subspaces. In this paper, we propose Black-box prompt tuning with Subspace Learning (BSL) to improve the versatility of black-box prompt tuning. Based on the assumption that nearly optimal prompts for similar tasks exist in a common subspace, we propose identifying such subspaces by meta-learning on a set of similar source tasks. Therefore, for a target task that shares similarities with source tasks, we guarantee that optimizing in the subspace can find a prompt that performs well on the target task. Experiments confirm that our BSL framework consistently achieves competitive performance regardless of downstream tasks and LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#34701;&#21512;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#24182;&#19988;&#36866;&#24212;&#20110;&#26032;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.03517</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#23454;&#29616;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#35270;&#35273;&#34701;&#21512;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Domain-Adaptive Visually-fused Event Detection from Text. (arXiv:2305.03517v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#34701;&#21512;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#24182;&#19988;&#36866;&#24212;&#20110;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#36741;&#21161;&#27169;&#24577;&#22914;&#22270;&#20687;&#25972;&#21512;&#21040;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#20013;&#24050;&#32463;&#24341;&#36215;&#20154;&#20204;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#24773;&#22659;&#30340;&#22797;&#26434;&#24615;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#30456;&#20851;&#30340;&#35270;&#35273;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20107;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36825;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#35768;&#22810;&#26631;&#35760;&#22909;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#26029;&#26102;&#26080;&#27861;&#33719;&#24471;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#38480;&#21046;&#20063;&#20250;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#20351;&#20854;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#38590;&#20197;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#35270;&#35273;&#34701;&#21512;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#24456;&#23569;&#30340;&#26631;&#35760;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#35270;&#35273;&#24819;&#35937;&#22120;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#20174;&#25991;&#26412;&#20013;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#23450;&#21046;&#21040;&#29305;&#23450;&#30340;&#39046;&#22495;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#21482;&#38656;&#23569;&#37327;&#30340;&#26631;&#35760;&#22270;&#20687;-&#25991;&#26412;&#23545;&#23601;&#21487;&#20197;&#36866;&#24212;&#20110;&#26032;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating auxiliary modalities such as images into event detection models has attracted increasing interest over the last few years. The complexity of natural language in describing situations has motivated researchers to leverage the related visual context to improve event detection performance. However, current approaches in this area suffer from data scarcity, where a large amount of labelled text-image pairs are required for model training. Furthermore, limited access to the visual context at inference time negatively impacts the performance of such models, which makes them practically ineffective in real-world scenarios. In this paper, we present a novel domain-adaptive visually-fused event detection approach that can be trained on a few labelled image-text paired data points. Specifically, we introduce a visual imaginator method that synthesises images from text in the absence of visual context. Moreover, the imaginator can be customised to a specific domain. In doing so, our
&lt;/p&gt;</description></item><item><title>ChatGraph&#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;</title><link>http://arxiv.org/abs/2305.03513</link><description>&lt;p&gt;
ChatGraph: &#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs. (arXiv:2305.03513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03513
&lt;/p&gt;
&lt;p&gt;
ChatGraph&#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#26368;&#36817;&#25512;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#38459;&#30861;&#20102;&#23427;&#30340;&#28508;&#22312;&#24212;&#29992;&#65306;&#65288;1&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#24494;&#35843;&#30340;&#19981;&#28789;&#27963;&#24615;&#21644;&#65288;2&#65289;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#30340;&#33021;&#21147;&#26469;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#65288;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#65292;&#21516;&#26102;&#25552;&#39640;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, as a recently launched large language model (LLM), has shown superior performance in various natural language processing (NLP) tasks. However, two major limitations hinder its potential applications: (1) the inflexibility of finetuning on downstream tasks and (2) the lack of interpretability in the decision-making process. To tackle these limitations, we propose a novel framework that leverages the power of ChatGPT for specific tasks, such as text classification, while improving its interpretability. The proposed framework conducts a knowledge graph extraction task to extract refined and structural knowledge from the raw data using ChatGPT. The rich knowledge is then converted into a graph, which is further used to train an interpretable linear classifier to make predictions. To evaluate the effectiveness of our proposed method, we conduct experiments on four datasets. The result shows that our method can significantly improve the performance compared to directly utilizing Cha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20351;&#29992;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#26816;&#32034;&#21644;&#21709;&#24212;&#29983;&#25104;&#21151;&#33021;&#65292;&#24182;&#22312;PhotoChat&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03512</link><description>&lt;p&gt;
&#26500;&#24314;&#22810;&#27169;&#24577;AI&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Building Multimodal AI Chatbots. (arXiv:2305.03512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20351;&#29992;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#26816;&#32034;&#21644;&#21709;&#24212;&#29983;&#25104;&#21151;&#33021;&#65292;&#24182;&#22312;PhotoChat&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;AI&#31995;&#32479;&#65292;&#19982;&#20154;&#31867;&#36827;&#34892;&#32842;&#22825;&#24182;&#20998;&#20139;&#30456;&#20851;&#29031;&#29255;&#12290;&#26089;&#26399;&#30340;&#24037;&#20316;&#20165;&#38480;&#20110;&#20851;&#20110;&#22270;&#20687;&#20013;&#29305;&#23450;&#23545;&#35937;&#25110;&#22330;&#26223;&#30340;&#23545;&#35805;&#65292;&#32780;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#23558;&#22270;&#20687;&#32435;&#20837;&#24320;&#25918;&#22495;&#23545;&#35805;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21709;&#24212;&#29983;&#25104;&#22120;&#26159;&#21333;&#27169;&#24577;&#30340;&#65292;&#20165;&#25509;&#21463;&#25991;&#26412;&#36755;&#20837;&#32780;&#19981;&#25509;&#21463;&#22270;&#20687;&#36755;&#20837;&#65292;&#22240;&#27492;&#23481;&#26131;&#29983;&#25104;&#19982;&#23545;&#35805;&#20013;&#20998;&#20139;&#30340;&#22270;&#20687;&#30456;&#30683;&#30462;&#30340;&#21709;&#24212;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#20351;&#29992;&#20004;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65306;&#19968;&#20010;&#29702;&#35299;&#25991;&#26412;&#30340;&#22270;&#20687;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#29702;&#35299;&#22270;&#20687;&#30340;&#21709;&#24212;&#29983;&#25104;&#22120;&#12290; &#22270;&#20687;&#26816;&#32034;&#22120;&#30001;ViT&#21644;BERT&#23454;&#29616;&#65292;&#26681;&#25454;&#23545;&#35805;&#21382;&#21490;&#35760;&#24405;&#21644;&#22270;&#20687;&#25968;&#25454;&#24211;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290; &#21709;&#24212;&#29983;&#25104;&#22120;&#30001;ViT&#21644;GPT-2/DialoGPT&#23454;&#29616;&#65292;&#29983;&#25104;&#26681;&#25454;&#23545;&#35805;&#21382;&#21490;&#21644;&#26368;&#36817;&#26816;&#32034;&#30340;&#22270;&#20687;&#21512;&#36866;&#30340;&#21709;&#24212;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#24102;&#26377;&#25104;&#23545;&#22270;&#20687;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;PhotoChat&#19978;&#36827;&#34892;&#20102;&#22521;&#35757;&#21644;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#20248;&#20110;&#20165;&#25991;&#26412;&#21644;&#21333;&#27169;&#24577;&#31995;&#32479;&#65292;&#36798;&#21040;&#20102;&#26356;&#39640;&#30340;&#22270;&#20687;&#22522;&#30784;&#20934;&#30830;&#24615;&#21644;&#26356;&#22909;&#30340;&#23545;&#35805;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to create a multimodal AI system that chats with humans and shares relevant photos. While earlier works were limited to dialogues about specific objects or scenes within images, recent works have incorporated images into open-domain dialogues. However, their response generators are unimodal, accepting text input but no image input, thus prone to generating responses contradictory to the images shared in the dialogue. Therefore, this work proposes a complete chatbot system using two multimodal deep learning models: an image retriever that understands texts and a response generator that understands images. The image retriever, implemented by ViT and BERT, selects the most relevant image given the dialogue history and a database of images. The response generator, implemented by ViT and GPT-2/DialoGPT, generates an appropriate response given the dialogue history and the most recently retrieved image. The two models are trained and evaluated on PhotoChat, an open-domain dialo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32763;&#35793;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#12289;&#36328;&#35821;&#35328;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26694;&#26550;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#22810;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.03510</link><description>&lt;p&gt;
&#22522;&#20110;&#32763;&#35793;&#23545;&#40784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment. (arXiv:2305.03510v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32763;&#35793;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#12289;&#36328;&#35821;&#35328;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26694;&#26550;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#22810;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#22312;&#36830;&#25509;&#22270;&#20687;&#21644;&#33521;&#35821;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#26368;&#36817;&#35797;&#22270;&#25193;&#23637;CLIP&#20197;&#25903;&#25345;&#20854;&#20182;&#35821;&#35328;&#65292;&#20294;&#30001;&#20110;&#36164;&#28304;&#19981;&#24179;&#34913;&#65292;&#35266;&#23519;&#21040;&#20102;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#26041;&#27861;&#20250;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;&#32763;&#35793;&#30340;&#23545;&#40784;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#35821;&#35328;&#24046;&#24322;&#65292;&#24182;&#25506;&#32034;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#22312;XTD&#21644;Multi30K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#38646;-shot&#12289;few-shot&#21644;&#20840;&#25968;&#25454;&#38598;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;11&#31181;&#35821;&#35328;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#20943;&#23569;&#20102;&#35821;&#35328;&#20043;&#38388;&#30340;&#22810;&#35821;&#35328;&#24046;&#24322;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models such as CLIP have witnessed remarkable success in connecting images and texts with a primary focus on English texts. Despite recent efforts to extend CLIP to support other languages, disparities in performance among different languages have been observed due to uneven resource availability. Additionally, current cross-lingual transfer methods of those pre-trained models would consume excessive resources for a large number of languages. Therefore, we propose a new parameter-efficient cross-lingual transfer learning framework that utilizes a translation-based alignment method to mitigate multilingual disparities and explores parameter-efficient fine-tuning methods for parameter-efficient cross-lingual transfer. Extensive experiments on XTD and Multi30K datasets, covering 11 languages under zero-shot, few-shot, and full-dataset learning scenarios, show that our framework significantly reduces the multilingual disparities among languages and improves 
&lt;/p&gt;</description></item><item><title>Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.03509</link><description>&lt;p&gt;
Diffusion Explainer&#65306;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#31283;&#23450;&#25193;&#25955;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03509
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#21019;&#36896;&#36924;&#30495;&#30340;&#22270;&#20687;&#32780;&#33719;&#24471;&#20102;&#20840;&#29699;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22797;&#26434;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#25805;&#20316;&#24448;&#24448;&#20351;&#24471;&#38750;&#19987;&#19994;&#20154;&#21592;&#38590;&#20197;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Diffusion Explainer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#12290;Diffusion Explainer&#32039;&#23494;&#22320;&#23558;&#31283;&#23450;&#25193;&#25955;&#30340;&#22797;&#26434;&#32452;&#20214;&#30340;&#35270;&#35273;&#27010;&#36848;&#19982;&#20854;&#28508;&#22312;&#25805;&#20316;&#30340;&#35814;&#32454;&#35828;&#26126;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#20351;&#29992;&#25143;&#21487;&#20197;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#12290;&#36890;&#36807;&#27604;&#36739;&#30001;&#20004;&#20010;&#30456;&#20851;&#25991;&#26412;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#34920;&#31034;&#30340;&#28436;&#21464;&#26469;&#25351;&#23548;&#31934;&#32454;&#26102;&#38388;&#27493;&#38271;&#65292;&#29992;&#25143;&#21487;&#20197;&#21457;&#29616;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;Diffusion Explainer&#22312;&#29992;&#25143;&#30340;Web&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#36816;&#34892;&#65292;&#26080;&#38656;&#23433;&#35013;&#25110;&#19987;&#38376;&#30340;&#30828;&#20214;&#65292;&#25193;&#22823;&#20102;&#20844;&#20247;&#23545;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#25945;&#32946;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI tec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#27861;&#24459;&#20889;&#20316;&#20013;&#24341;&#29992;&#20540;&#30340;&#37492;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21253;&#21547; 178M &#21477;&#23376;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#24182;&#27979;&#35797;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#19987;&#38376;&#38024;&#23545;&#35813;&#39046;&#22495;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.03508</link><description>&lt;p&gt;
CiteCaseLAW: &#29992;&#20110;&#27861;&#24459;&#36741;&#21161;&#20889;&#20316;&#30340;&#21028;&#20363;&#27861;&#24341;&#29992;&#20540;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CiteCaseLAW: Citation Worthiness Detection in Caselaw for Legal Assistive Writing. (arXiv:2305.03508v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#27861;&#24459;&#20889;&#20316;&#20013;&#24341;&#29992;&#20540;&#30340;&#37492;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21253;&#21547; 178M &#21477;&#23376;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#24182;&#27979;&#35797;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#19987;&#38376;&#38024;&#23545;&#35813;&#39046;&#22495;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#25991;&#20214;&#25776;&#20889;&#20013;&#65292;&#27491;&#30830;&#24341;&#29992;&#26696;&#20363;&#27861;&#21644;&#20854;&#20182;&#26469;&#28304;&#20197;&#35777;&#26126;&#22768;&#26126;&#21644;&#35770;&#28857;&#26159;&#20854;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#35201;&#32032;&#12290;&#29702;&#35299;&#27861;&#24459;&#39046;&#22495;&#24182;&#35782;&#21035;&#36866;&#24403;&#30340;&#24341;&#29992;&#19978;&#19979;&#25991;&#25110;&#20540;&#24471;&#24341;&#29992;&#30340;&#21477;&#23376;&#26159;&#38656;&#35201;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27861;&#24459;&#26415;&#35821;&#12289;&#35821;&#20041;&#21644;&#39640;&#24230;&#29305;&#24322;&#24615;&#30340;&#23384;&#22312;&#20351;&#24471;&#27861;&#24459;&#35821;&#35328;&#21464;&#24471;&#22797;&#26434;&#65292;&#20174;&#32780;&#20351;&#20219;&#20309;&#30456;&#20851;&#30340;&#27861;&#24459;&#20219;&#21153;&#38590;&#20197;&#33258;&#21160;&#21270;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#24341;&#29992;&#20540;&#37492;&#21035;&#30340;&#38382;&#39064;&#12290;&#23427;&#26088;&#22312;&#25104;&#20026;&#24403;&#20170;&#24341;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#21021;&#22987;&#27493;&#39588;&#65292;&#20197;&#20943;&#36731;&#25552;&#21462;&#36275;&#22815;&#30340;&#24341;&#25991;&#19978;&#19979;&#25991;&#30340;&#36127;&#25285;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174; Caselaw &#35775;&#38382;&#39033;&#30446; (CAP) &#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547; 178M &#21477;&#23376;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#20013;&#30340;&#24341;&#25991;&#20540;&#26816;&#27979;&#12290;&#22312;&#36825;&#20010;&#20840;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#19987;&#38376;&#38024;&#23545;&#35813;&#39046;&#22495;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#24448;&#24448;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In legal document writing, one of the key elements is properly citing the case laws and other sources to substantiate claims and arguments. Understanding the legal domain and identifying appropriate citation context or cite-worthy sentences are challenging tasks that demand expensive manual annotation. The presence of jargon, language semantics, and high domain specificity makes legal language complex, making any associated legal task hard for automation. The current work focuses on the problem of citation-worthiness identification. It is designed as the initial step in today's citation recommendation systems to lighten the burden of extracting an adequate set of citation contexts. To accomplish this, we introduce a labeled dataset of 178M sentences for citation-worthiness detection in the legal domain from the Caselaw Access Project (CAP). The performance of various deep learning models was examined on this novel dataset. The domain-specific pre-trained model tends to outperform other
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220; ReRead&#8221;&#30340;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#65292;&#20854;&#20197;&#20004;&#20010;&#38454;&#27573;&#65288;&#35777;&#25454;&#26816;&#32034;&#21644;&#22768;&#26126;&#39564;&#35777;&#65289;&#23454;&#29616;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#39564;&#35777;&#65292;&#24182;&#22312;FEVER&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03507</link><description>&lt;p&gt;
&#35835;&#20004;&#36941;&#65306;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#35777;&#25454;&#23454;&#29616;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence. (arXiv:2305.03507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220; ReRead&#8221;&#30340;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#65292;&#20854;&#20197;&#20004;&#20010;&#38454;&#27573;&#65288;&#35777;&#25454;&#26816;&#32034;&#21644;&#22768;&#26126;&#39564;&#35777;&#65289;&#23454;&#29616;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#39564;&#35777;&#65292;&#24182;&#22312;FEVER&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20107;&#23454;&#39564;&#35777;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#20174;&#21407;&#22987;&#25991;&#26723;&#20013;&#26816;&#32034;&#35777;&#25454;&#26469;&#39564;&#35777;&#22768;&#26126;&#30340;&#20107;&#23454;&#24615;&#12290; &#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#30340;&#36136;&#37327;&#22312;&#35813;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290; &#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#24212;&#35813;&#26159;&#21487;&#20449;&#30340;&#65288;&#21453;&#26144;&#20102;&#27169;&#22411;&#22312;&#22768;&#26126;&#39564;&#35777;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#65289;&#19988;&#21512;&#29702;&#30340;&#65288;&#23545;&#20154;&#31867;&#26377;&#35828;&#26381;&#21147;&#65289;&#65292;&#24182;&#33021;&#25552;&#39640;&#39564;&#35777;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290; &#23613;&#31649;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#22768;&#26126;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#35821;&#20041;&#25110;&#34920;&#38754;&#24418;&#24335;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#26816;&#32034;&#35777;&#25454;&#65292;&#20294;&#23427;&#20204;&#37117;&#20381;&#36182;&#20110;&#26576;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20250;&#38459;&#27490;&#23427;&#20204;&#28385;&#36275;&#25152;&#26377;&#19977;&#20010;&#35201;&#27714;&#12290; &#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220; ReRead&#8221;&#30340;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#65292;&#20197;&#26816;&#32034;&#35777;&#25454;&#24182;&#39564;&#35777;&#22768;&#26126;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20197;&#19979;&#20004;&#20010;&#38454;&#27573;&#65306;1&#65289;&#35777;&#25454;&#26816;&#32034;&#38454;&#27573;&#65292;&#35813;&#38454;&#27573;&#36890;&#36807;&#20351;&#29992;&#24544;&#23454;&#19988;&#21512;&#29702;&#30340;&#35777;&#25454;&#21462;&#22238;&#22120;&#26469;&#33719;&#21462;&#21487;&#35299;&#37322;&#30340;&#35777;&#25454;&#65307;2&#65289;&#22768;&#26126;&#39564;&#35777;&#38454;&#27573;&#65292;&#35813;&#38454;&#27573;&#37325;&#26032;&#23457;&#35270;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#20197;&#39564;&#35777;&#22768;&#26126;&#12290; &#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;FEVER&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world fact verification task aims to verify the factuality of a claim by retrieving evidence from the source document. The quality of the retrieved evidence plays an important role in claim verification. Ideally, the retrieved evidence should be faithful (reflecting the model's decision-making process in claim verification) and plausible (convincing to humans), and can improve the accuracy of verification task. Although existing approaches leverage the similarity measure of semantic or surface form between claims and documents to retrieve evidence, they all rely on certain heuristics that prevent them from satisfying all three requirements. In light of this, we propose a fact verification model named ReRead to retrieve evidence and verify claim that: (1) Train the evidence retriever to obtain interpretable evidence (i.e., faithfulness and plausibility criteria); (2) Train the claim verifier to revisit the evidence retrieved by the optimized evidence retriever to improve the accura
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#31995;&#25552;&#21462;&#30340;&#29702;&#25454;&#25277;&#21462;&#26694;&#26550;RE2&#65292;&#21033;&#29992;&#20004;&#20010;&#36830;&#32493;&#24615;&#21644;&#31232;&#30095;&#24615;&#22240;&#32032;&#20174;&#21477;&#23376;&#20013;&#33719;&#21462;&#30456;&#20851;&#32780;&#36830;&#36143;&#30340;&#29702;&#25454;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#20445;&#30041;&#30456;&#20851;&#20869;&#23481;&#24182;&#20174;&#21477;&#23376;&#20013;&#21435;&#25481;&#22122;&#38899;&#27573;&#33853;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#21462;&#20986;&#30340;&#29702;&#25454;&#23545;&#20110;&#25512;&#29702;&#21644;&#35299;&#37322;&#26159;&#26377;&#29992;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.03503</link><description>&lt;p&gt;
&#21512;&#29702;&#30475;&#24453;&#25152;&#30475;&#21040;&#30340;&#65306;&#20851;&#31995;&#25552;&#21462;&#30340;&#36830;&#32493;&#29702;&#25454;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Think Rationally about What You See: Continuous Rationale Extraction for Relation Extraction. (arXiv:2305.03503v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#31995;&#25552;&#21462;&#30340;&#29702;&#25454;&#25277;&#21462;&#26694;&#26550;RE2&#65292;&#21033;&#29992;&#20004;&#20010;&#36830;&#32493;&#24615;&#21644;&#31232;&#30095;&#24615;&#22240;&#32032;&#20174;&#21477;&#23376;&#20013;&#33719;&#21462;&#30456;&#20851;&#32780;&#36830;&#36143;&#30340;&#29702;&#25454;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#20445;&#30041;&#30456;&#20851;&#20869;&#23481;&#24182;&#20174;&#21477;&#23376;&#20013;&#21435;&#25481;&#22122;&#38899;&#27573;&#33853;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#21462;&#20986;&#30340;&#29702;&#25454;&#23545;&#20110;&#25512;&#29702;&#21644;&#35299;&#37322;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#26681;&#25454;&#20004;&#20010;&#23454;&#20307;&#30340;&#35821;&#22659;&#25552;&#21462;&#28508;&#22312;&#20851;&#31995;&#65292;&#22240;&#27492;&#65292;&#20174;&#21477;&#23376;&#20013;&#25512;&#23548;&#20986;&#21512;&#29702;&#30340;&#35821;&#22659;&#38750;&#24120;&#37325;&#35201;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#35201;&#20040;&#19987;&#27880;&#20110;&#22914;&#20309;&#21033;&#29992;&#23454;&#20307;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#23454;&#20307;&#31867;&#22411;&#65292;&#23454;&#20307;&#29992;&#35821;&#65289;&#26469;&#25512;&#26029;&#20851;&#31995;&#65292;&#20294;&#24573;&#30053;&#20102;&#20197;&#35821;&#22659;&#20026;&#37325;&#28857;&#30340;&#20869;&#23481;&#65292;&#35201;&#20040;&#20351;&#29992;&#21453;&#20107;&#23454;&#24605;&#32500;&#26469;&#28040;&#38500;&#27169;&#22411;&#23545;&#23454;&#20307;&#28508;&#22312;&#20851;&#31995;&#30340;&#20559;&#35265;&#65292;&#20294;&#20851;&#31995;&#25512;&#29702;&#36807;&#31243;&#20173;&#20250;&#21463;&#21040;&#26080;&#20851;&#20869;&#23481;&#30340;&#24178;&#25200;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#20445;&#30041;&#26377;&#20851;&#20869;&#23481;&#24182;&#20174;&#21477;&#23376;&#20013;&#21435;&#25481;&#22122;&#38899;&#27573;&#33853;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#20445;&#30041;&#30340;&#20869;&#23481;&#38656;&#35201;&#36275;&#22815;&#27969;&#30021;&#65292;&#20197;&#20445;&#25345;&#35821;&#20041;&#30340;&#36830;&#36143;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#25454;&#25277;&#21462;&#26694;&#26550;RE2&#65292;&#23427;&#21033;&#29992;&#20004;&#20010;&#36830;&#32493;&#24615;&#21644;&#31232;&#30095;&#24615;&#22240;&#32032;&#20174;&#21477;&#23376;&#20013;&#33719;&#21462;&#30456;&#20851;&#32780;&#36830;&#36143;&#30340;&#29702;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#40644;&#37329;&#29702;&#25454;&#26410;&#26631;&#35760;&#30340;&#38382;&#39064;&#65292;RE2&#24212;&#29992;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#29983;&#25104;&#20505;&#36873;&#29702;&#25454;&#65292;&#24182;&#36873;&#25321;&#26368;&#30456;&#20851;&#21644;&#36830;&#36143;&#30340;&#29702;&#25454;&#26469;&#25351;&#23548;RE&#27169;&#22411;&#12290;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#21462;&#20986;&#30340;&#29702;&#25454;&#23545;&#20110;&#25512;&#29702;&#21644;&#35299;&#37322;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) aims to extract potential relations according to the context of two entities, thus, deriving rational contexts from sentences plays an important role. Previous works either focus on how to leverage the entity information (e.g., entity types, entity verbalization) to inference relations, but ignore context-focused content, or use counterfactual thinking to remove the model's bias of potential relations in entities, but the relation reasoning process will still be hindered by irrelevant content. Therefore, how to preserve relevant content and remove noisy segments from sentences is a crucial task. In addition, retained content needs to be fluent enough to maintain semantic coherence and interpretability. In this work, we propose a novel rationale extraction framework named RE2, which leverages two continuity and sparsity factors to obtain relevant and coherent rationales from sentences. To solve the problem that the gold rationales are not labeled, RE2 applies an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;Wordle&#28216;&#25103;&#30340;&#35789;&#35821;&#38590;&#24230;&#39044;&#27979;&#26041;&#27861;&#21450;&#20854;&#24433;&#21709;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#22240;&#23376;&#20998;&#26512;&#19982;&#32447;&#24615;&#22238;&#24402;&#24314;&#31435;&#20102;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.03502</link><description>&lt;p&gt;
&#22522;&#20110;Wordle&#28216;&#25103;&#25506;&#31350;&#35789;&#35821;&#38590;&#24230;&#21450;&#20854;&#24433;&#21709;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Explore the difficulty of words and its influential attributes based on the Wordle game. (arXiv:2305.03502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;Wordle&#28216;&#25103;&#30340;&#35789;&#35821;&#38590;&#24230;&#39044;&#27979;&#26041;&#27861;&#21450;&#20854;&#24433;&#21709;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#22240;&#23376;&#20998;&#26512;&#19982;&#32447;&#24615;&#22238;&#24402;&#24314;&#31435;&#20102;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;Wordle&#28216;&#25103;&#20013;&#29468;&#27979;&#27425;&#25968;&#30340;&#20998;&#24067;&#21644;&#26399;&#26395;&#20316;&#20026;&#34913;&#37327;&#35789;&#35821;&#38590;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#25506;&#31350;&#20102;&#24433;&#21709;&#35789;&#35821;&#38590;&#24230;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#21033;&#29992;Monte Carlo&#27169;&#25311;&#29609;&#23478;&#30340;&#29468;&#27979;&#36807;&#31243;&#39044;&#27979;&#38590;&#24230;&#20998;&#24067;&#65292;&#21033;&#29992;Markov&#26469;&#29983;&#25104;&#35789;&#35821;&#30340;&#20851;&#32852;&#24615;&#65292;&#32553;&#23567;&#27599;&#20010;&#35789;&#35821;&#29468;&#27979;&#27425;&#25968;&#30340;&#23454;&#38469;&#20998;&#24067;&#19982;&#21407;&#22987;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#28982;&#21518;&#20351;&#29992;&#22871;&#32034;&#22238;&#24402;&#39044;&#27979;&#29468;&#27979;&#27425;&#25968;&#26399;&#26395;&#30340;&#20559;&#24046;&#65292;&#24182;&#21033;&#29992;&#20108;&#27425;&#35268;&#21010;&#33719;&#24471;&#21407;&#22987;&#20998;&#24067;&#30340;&#20462;&#27491;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20998;&#23618;&#32858;&#31867;&#26041;&#27861;&#23558;&#35789;&#35821;&#25353;&#29468;&#27979;&#27425;&#25968;&#26399;&#26395;&#20540;&#20998;&#20026;&#19981;&#21516;&#30340;&#38590;&#24230;&#32423;&#21035;&#65292;&#28982;&#21518;&#21033;&#29992;&#22240;&#23376;&#20998;&#26512;&#38477;&#20302;&#35789;&#27719;&#23646;&#24615;&#21464;&#37327;&#30340;&#25968;&#37327;&#65292;&#26174;&#33879;&#30340;&#22240;&#32032;&#21253;&#25324;&#37051;&#36817;&#35789;&#27719;&#30340;&#25968;&#37327;&#12289;&#23383;&#27597;&#30456;&#20284;&#24615;&#12289;&#23376;&#20018;&#30456;&#20284;&#24615;&#21644;&#35789;&#39057;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#24314;&#31435;&#20102;&#19978;&#36848;&#24433;&#21709;&#22240;&#32032;&#19982;&#35789;&#35821;&#38590;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We adopt the distribution and expectation of guessing times in game Wordle as metrics to predict the difficulty of words and explore their influence factors. In order to predictthe difficulty distribution, we use Monte Carlo to simulate the guessing process of players and then narrow the gap between raw and actual distribution of guessing times for each word with Markov which generates the associativity of words. Afterwards, we take advantage of lasso regression to predict the deviation of guessing times expectation and quadratic programming to obtain the correction of the original distribution.To predict the difficulty levels, we first use hierarchical clustering to classify the difficulty levels based on the expectation of guessing times. Afterwards we downscale the variables of lexical attributes based on factor analysis. Significant factors include the number of neighboring words, letter similarity, sub-string similarity, and word frequency. Finally, we build the relationship betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#23567;&#22411;&#30284;&#30151;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;BERT&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#31934;&#24230;&#20998;&#31867;&#30284;&#30151;&#26631;&#24535;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;94.45%&#30340;&#31934;&#24230;&#65292;&#27604;&#20197;&#24448;&#25991;&#29486;&#30340;&#30740;&#31350;&#32467;&#26524;&#39640;&#20986;&#33267;&#23569;8.04%&#12290;</title><link>http://arxiv.org/abs/2305.03501</link><description>&lt;p&gt;
&#21033;&#29992;Transformers&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#24449;&#30340;&#30284;&#30151;&#26631;&#24535;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cancer Hallmark Classification Using Bidirectional Encoder Representations From Transformers. (arXiv:2305.03501v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#23567;&#22411;&#30284;&#30151;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;BERT&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#31934;&#24230;&#20998;&#31867;&#30284;&#30151;&#26631;&#24535;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;94.45%&#30340;&#31934;&#24230;&#65292;&#27604;&#20197;&#24448;&#25991;&#29486;&#30340;&#30740;&#31350;&#32467;&#26524;&#39640;&#20986;&#33267;&#23569;8.04%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#31934;&#30830;&#22320;&#20998;&#31867;&#30284;&#30151;&#30340;&#26631;&#24535;&#65292;&#36825;&#26159;&#30284;&#30151;&#30740;&#31350;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#24449;&#30340;Transformers&#65288;BERT&#65289;&#26550;&#26500;&#65292;&#22312;&#23567;&#22411;&#30284;&#30151;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;94.45%&#30340;&#26174;&#33879;&#31934;&#24230;&#65292;&#36825;&#27604;&#25991;&#29486;&#20013;&#20960;&#20046;&#25152;&#26377;&#20043;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#37117;&#35201;&#39640;&#20986;&#33267;&#23569;8.04%&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#20998;&#31867;&#21644;&#29702;&#35299;&#30284;&#30151;&#30740;&#31350;&#25991;&#26412;&#25991;&#29486;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#22240;&#27492;&#23545;&#35813;&#39046;&#22495;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;&#30001;&#20110;&#30284;&#30151;&#20173;&#28982;&#26159;&#20840;&#29699;&#21313;&#22823;&#27515;&#22240;&#20043;&#19968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#36827;&#30284;&#30151;&#30740;&#31350;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to accurately classify the hallmarks of cancer, which is a crucial task in cancer research. Our proposed method utilizes the Bidirectional Encoder Representations from Transformers (BERT) architecture, which has shown exceptional performance in various downstream applications. By applying transfer learning, we fine-tuned the pre-trained BERT model on a small corpus of biomedical text documents related to cancer. The outcomes of our experimental investigations demonstrate that our approach attains a noteworthy accuracy of 94.45%, surpassing almost all prior findings with a substantial increase of at least 8.04% as reported in the literature. These findings highlight the effectiveness of our proposed model in accurately classifying and comprehending text documents for cancer research, thus contributing significantly to the field. As cancer remains one of the top ten leading causes of death globally, our approach holds great promise in advancing cancer
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21152;&#23494;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;NLP&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#38750;&#21152;&#23494;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03497</link><description>&lt;p&gt;
&#22312;&#21152;&#23494;&#25991;&#26412;&#19978;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20197;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Training Natural Language Processing Models on Encrypted Text for Enhanced Privacy. (arXiv:2305.03497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21152;&#23494;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;NLP&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#38750;&#21152;&#23494;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20113;&#26381;&#21153;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#25968;&#25454;&#38544;&#31169;&#24050;&#25104;&#20026;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#36825;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22788;&#29702;&#28041;&#21450;&#20010;&#20154;&#36890;&#20449;&#21644;&#26426;&#23494;&#25991;&#20214;&#31561;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21152;&#23494;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;NLP&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#38750;&#21152;&#23494;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#26550;&#26500;&#65292;&#21363;Doc2Vec + XGBoost&#21644;Doc2Vec + LSTM&#65292;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;20 Newsgroups&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21152;&#23494;&#21644;&#38750;&#21152;&#23494;&#27169;&#22411;&#22343;&#21487;&#23454;&#29616;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#35828;&#26126;&#25105;&#20204;&#30340;&#21152;&#23494;&#26041;&#27861;&#22312;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#32500;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;&#20026;&#20102;&#22797;&#21046;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#22312;&#20197;&#19979;&#22320;&#22336;&#25552;&#20379;&#20102;Colab&#31508;&#35760;&#26412;&#65306;https://t.ly/lR-TP
&lt;/p&gt;
&lt;p&gt;
With the increasing use of cloud-based services for training and deploying machine learning models, data privacy has become a major concern. This is particularly important for natural language processing (NLP) models, which often process sensitive information such as personal communications and confidential documents. In this study, we propose a method for training NLP models on encrypted text data to mitigate data privacy concerns while maintaining similar performance to models trained on non-encrypted data. We demonstrate our method using two different architectures, namely Doc2Vec+XGBoost and Doc2Vec+LSTM, and evaluate the models on the 20 Newsgroups dataset. Our results indicate that both encrypted and non-encrypted models achieve comparable performance, suggesting that our encryption method is effective in preserving data privacy without sacrificing model accuracy. In order to replicate our experiments, we have provided a Colab notebook at the following address: https://t.ly/lR-TP
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03495</link><description>&lt;p&gt;
&#22522;&#20110;&#8220;&#26799;&#24230;&#19979;&#38477;&#8221;&#19982; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automatic Prompt Optimization with "Gradient Descent" and Beam Search. (arXiv:2305.03495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03495
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36890;&#29992;&#26234;&#33021;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#24615;&#33021;&#65292;&#20294;&#20854;&#33021;&#21147;&#20173;&#39640;&#24230;&#20381;&#36182;&#20110;&#25163;&#20889;&#30340;&#25552;&#31034;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35797;&#38169;&#23581;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#38750;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#20351;&#29992;&#25968;&#20540;&#26799;&#24230;&#19979;&#38477;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language ``gradients'' that criticize the current prompt. The gradients are then ``propagated'' into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#31526;&#21495;&#25509;&#22320;&#20219;&#21153;&#65292;&#22312;&#25945;&#24072;&#30340;&#27867;&#25351;&#35821;&#21477;&#21644;&#19978;&#19979;&#25991;&#20013;&#30340;&#26263;&#31034;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#25968;&#25454;&#25509;&#22320;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03461</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#35821;&#22659;&#20013;&#30340;&#27867;&#25351;&#34920;&#36798;&#25506;&#31350;&#20132;&#20114;&#24335;&#33719;&#21462;&#31934;&#32454;&#35270;&#35273;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Interactive Acquisition of Fine-grained Visual Concepts by Exploiting Semantics of Generic Characterizations in Discourse. (arXiv:2305.03461v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#31526;&#21495;&#25509;&#22320;&#20219;&#21153;&#65292;&#22312;&#25945;&#24072;&#30340;&#27867;&#25351;&#35821;&#21477;&#21644;&#19978;&#19979;&#25991;&#20013;&#30340;&#26263;&#31034;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#25968;&#25454;&#25509;&#22320;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#28041;&#21450;&#36890;&#36807;&#19982;&#20154;&#31867;&#29992;&#25143;&#33258;&#28982;&#20132;&#20114;&#26469;&#23398;&#20064;&#26410;&#30693;&#39046;&#22495;&#27010;&#24565;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;ITL&#30340;&#38480;&#21046;&#19979;&#65292;&#25506;&#32034;&#20102;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31526;&#21495;&#25509;&#22320;&#20219;&#21153;&#8212;&#8212;&#21306;&#20998;&#22806;&#35266;&#38750;&#24120;&#30456;&#20284;&#30340;&#29289;&#20307;&#31867;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126;&#65292;&#21033;&#29992;&#25945;&#24072;&#30340;&#27867;&#25351;&#35821;&#21477;&#65288;&#20363;&#22914;&#65292;&#8220;X&#20855;&#26377;&#23646;&#24615;Z&#12290;&#8221;&#65289;&#20197;&#21450;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#26263;&#31034;&#65288;&#20363;&#22914;&#65292;&#8220;X&#21644;Y&#26377;&#20160;&#20040;&#19981;&#21516;&#65311;&#8221;&#30340;&#22238;&#31572;&#26159;&#65292;&#8220;&#25512;&#26029;Y&#19981;&#20855;&#22791;&#23646;&#24615;Z&#8221;&#65289;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#25928;&#30340;&#25968;&#25454;&#25509;&#22320;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive Task Learning (ITL) concerns learning about unforeseen domain concepts via natural interactions with human users. The learner faces a number of significant constraints: learning should be online, incremental and few-shot, as it is expected to perform tangible belief updates right after novel words denoting unforeseen concepts are introduced. In this work, we explore a challenging symbol grounding task--discriminating among object classes that look very similar--within the constraints imposed by ITL. We demonstrate empirically that more data-efficient grounding results from exploiting the truth-conditions of the teacher's generic statements (e.g., "Xs have attribute Z.") and their implicatures in context (e.g., as an answer to "How are Xs and Ys different?", one infers Y lacks attribute Z).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35270;&#35282;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#30340;&#28151;&#21512;&#22411;&#25968;&#20540;&#25512;&#29702;&#38382;&#31572;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#34920;&#26684;&#35270;&#35282;&#12289;&#20851;&#31995;&#35270;&#35282;&#21644;&#25968;&#20540;&#35270;&#35282;&#25429;&#25417;&#28151;&#21512;&#25968;&#25454;&#20043;&#38388;&#30340;&#31890;&#24230;&#20851;&#31995;&#21644;&#31354;&#38388;&#32467;&#26500;&#20449;&#24687;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03458</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#22411;&#25968;&#20540;&#25512;&#29702;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-View Graph Representation Learning for Answering Hybrid Numerical Reasoning Question. (arXiv:2305.03458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35270;&#35282;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#30340;&#28151;&#21512;&#22411;&#25968;&#20540;&#25512;&#29702;&#38382;&#31572;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#34920;&#26684;&#35270;&#35282;&#12289;&#20851;&#31995;&#35270;&#35282;&#21644;&#25968;&#20540;&#35270;&#35282;&#25429;&#25417;&#28151;&#21512;&#25968;&#25454;&#20043;&#38388;&#30340;&#31890;&#24230;&#20851;&#31995;&#21644;&#31354;&#38388;&#32467;&#26500;&#20449;&#24687;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#22411;&#38382;&#31572; (HybridQA) &#25968;&#25454;&#21547;&#26377;&#25991;&#26412;&#19982;&#34920;&#26684;&#25968;&#25454;&#65292;&#38656;&#35201;&#27169;&#22411;&#36873;&#25321;&#21512;&#36866;&#35777;&#25454;&#36827;&#34892;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#34920;&#36798;&#26641;&#30340;&#35299;&#30721;&#22120;&#26469;&#35299;&#20915;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32534;&#30721;&#22120;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;(MRC)&#26041;&#27861;,&#23427;&#20197;&#34920;&#26684;&#24207;&#21015;&#21644;&#25991;&#26412;&#25340;&#25509;&#20316;&#20026;&#36755;&#20837;&#65292;&#30772;&#22351;&#20102;&#34920;&#26684;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#31890;&#24230;&#20851;&#31995;&#20197;&#21450;&#34920;&#26684;&#26412;&#36523;&#30340;&#31354;&#38388;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#35270;&#35282;&#22270;&#24418; (MVG) &#32534;&#30721;&#22120;&#26469;&#32771;&#34385;&#31890;&#24230;&#20043;&#38388;&#20851;&#31995;&#24182;&#20174;&#22810;&#20010;&#35270;&#35282;&#25429;&#25417;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;MVGE&#20316;&#20026;&#27169;&#22359;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#38754;&#21521;&#20445;&#30041;&#28151;&#21512;&#25968;&#25454;&#21407;&#22987;&#29305;&#24449;&#30340;&#34920;&#26684;&#35270;&#22270;&#12289;&#20851;&#31995;&#35270;&#22270;&#21644;&#25968;&#20540;&#35270;&#22270;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#30340;&#34920;&#26684;&#25991;&#26412;&#28151;&#21512;QA&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;MVGE&#22312;&#25968;&#20540;&#25512;&#29702;&#23376;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#30456;&#23545;&#25552;&#39640;&#20102;27.0%&#12290;
&lt;/p&gt;
&lt;p&gt;
Hybrid question answering (HybridQA) over the financial report contains both textual and tabular data, and requires the model to select the appropriate evidence for the numerical reasoning task. Existing methods based on encoder-decoder framework employ a expression tree-based decoder to solve numerical reasoning problems. However, encoders rely more on Machine Reading Comprehension (MRC) methods, which take table serialization and text splicing as input, damaging the granularity relationship between table and text as well as the spatial structure information of table itself. In order to solve these problems, the paper proposes a Multi-View Graph (MVG) Encoder to take the relations among the granularity into account and capture the relations from multiple view. By utilizing MVGE as a module, we constuct Tabular View, Relation View and Numerical View which aim to retain the original characteristics of the hybrid data. We validate our model on the publicly available table-text hybrid QA 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#21495;&#25945;&#32946;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;COT&#21512;&#29702;&#21270;&#20449;&#21495;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.03453</link><description>&lt;p&gt;
T-SciQ: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#21495;&#25945;&#25480;&#22810;&#27169;&#24577;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#22312;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering. (arXiv:2305.03453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#21495;&#25945;&#32946;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;COT&#21512;&#29702;&#21270;&#20449;&#21495;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36817;&#26399;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#36824;&#23637;&#31034;&#20102;&#25191;&#34892;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#20197;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22797;&#26434;&#22810;&#27169;&#24577;&#22330;&#26223;&#19979;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#20363;&#22914;&#36890;&#36807;&#29992;&#39640;&#36136;&#37327;&#20154;&#24037;&#27880;&#37322;&#30340;&#38142;&#24335;&#24605;&#36335;&#26469;&#35843;&#25972;&#22810;&#27169;&#22411;&#27169;&#22411;&#36827;&#34892;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#39640;&#36136;&#37327;COT&#21512;&#29702;&#21270;&#36890;&#24120;&#26159;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#28041;&#21450;&#20887;&#20313;&#20449;&#24687;&#25110;&#20002;&#22833;&#37325;&#35201;&#20449;&#24687;&#65292;&#27880;&#37322;&#21512;&#29702;&#21270;&#36890;&#24120;&#19981;&#22826;&#20934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;T-SciQ&#65292;&#26088;&#22312;&#20351;&#29992;LLM&#20449;&#21495;&#25945;&#25480;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#12290;T-SciQ&#26041;&#27861;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;CoT&#21512;&#29702;&#21270;&#20449;&#21495;&#65292;&#24182;&#20808;&#36827;&#22320;&#35757;&#32451;&#36739;&#23567;&#30340;&#27169;&#22411;&#20197;&#22312;&#22797;&#26434;&#27169;&#24577;&#20013;&#25191;&#34892;CoT&#24605;&#32500;&#25512;&#29702;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#26174;&#30528;&#20943;&#23569;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21512;&#29702;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. They have also shown the ability to perform chain-of-thought (CoT) reasoning to solve complex problems. Recent studies have explored CoT reasoning in complex multimodal scenarios, such as the science question answering task, by fine-tuning multimodal models with high-quality human-annotated CoT rationales. However, collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the redundant information involved or the essential information missed. To address these issues, we propose a novel method termed \emph{T-SciQ} that aims at teaching science question answering with LLM signals. The T-SciQ approach generates high-quality CoT rationales as teaching signals and is advanced to train much smaller models to perform CoT reasoning in complex modalities. Additionally, we introduce a no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#36523;&#21270;&#31574;&#30053;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27604;&#21947;&#24615;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#34892;&#20026;&#26356;&#20855;&#20307;&#21270;&#30340;&#38544;&#21947;&#24615;&#21477;&#23376;&#26102;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.03445</link><description>&lt;p&gt;
LMs&#22266;&#23432;&#38453;&#22320;&#65306;&#25506;&#31350;&#20855;&#36523;&#21270;&#23545;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#27604;&#21947;&#24615;&#35821;&#35328;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models. (arXiv:2305.03445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#36523;&#21270;&#31574;&#30053;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27604;&#21947;&#24615;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#34892;&#20026;&#26356;&#20855;&#20307;&#21270;&#30340;&#38544;&#21947;&#24615;&#21477;&#23376;&#26102;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#21947;&#35821;&#35328;&#26159;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#35299;&#37322;&#22522;&#20110;&#21333;&#35789;&#30340;&#20351;&#29992;&#26041;&#24335;&#20559;&#31163;&#20102;&#23427;&#20204;&#30340;&#24120;&#35268;&#39034;&#24207;&#21644;&#21547;&#20041;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#21487;&#20197;&#36731;&#26494;&#29702;&#35299;&#21644;&#35808;&#37322;&#38544;&#21947;&#12289;&#27604;&#21947;&#25110;&#20064;&#35821;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20174;&#20855;&#36523;&#38544;&#21947;&#20013;&#25512;&#23548;&#20986;&#26469;&#12290;&#35821;&#35328;&#26159;&#20855;&#36523;&#21270;&#30340;&#20195;&#29702;&#65292;&#22914;&#26524;&#38544;&#21947;&#26159;&#20256;&#32479;&#30340;&#21644;&#35789;&#27719;&#21270;&#30340;&#65292;&#37027;&#20040;&#19968;&#20010;&#27809;&#26377;&#36523;&#20307;&#30340;&#31995;&#32479;&#23601;&#26356;&#23481;&#26131;&#29702;&#35299;&#20855;&#36523;&#27010;&#24565;&#12290;&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27604;&#21947;&#24615;&#21477;&#23376;&#30340;&#34892;&#21160;&#26356;&#20855;&#20307;&#21270;&#26102;&#65292;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#37322;&#38544;&#21947;&#21477;&#23376;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#25490;&#38500;&#20102;&#19982;&#20854;&#20182;&#29305;&#24449;&#65288;&#20363;&#22914;&#21333;&#35789;&#38271;&#24230;&#25110;&#20855;&#20307;&#24615;&#65289;&#30340;&#22810;&#37325;&#20849;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Figurative language is a challenge for language models since its interpretation is based on the use of words in a way that deviates from their conventional order and meaning. Yet, humans can easily understand and interpret metaphors, similes or idioms as they can be derived from embodied metaphors. Language is a proxy for embodiment and if a metaphor is conventional and lexicalised, it becomes easier for a system without a body to make sense of embodied concepts. Yet, the intricate relation between embodiment and features such as concreteness or age of acquisition has not been studied in the context of figurative language interpretation concerning language models. Hence, the presented study shows how larger language models perform better at interpreting metaphoric sentences when the action of the metaphorical sentence is more embodied. The analysis rules out multicollinearity with other features (e.g. word length or concreteness) and provides initial evidence that larger language model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Transformer&#26550;&#26500;&#23454;&#29616;&#22312;&#32447;&#25163;&#21183;&#35782;&#21035;&#65292;&#24182;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#22810;&#32423;&#20998;&#21106;&#38382;&#39064;&#30340;&#26426;&#22120;&#32763;&#35793;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35299;&#30721;&#31354;&#38388;&#25552;&#39640;&#23545;&#35821;&#27861;&#35268;&#21017;&#21644;&#32570;&#25439;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#33021;&#22815;&#29992;&#20110;&#22810;&#31181;&#35821;&#35328;&#21644;&#36890;&#29992;&#25163;&#20889;&#35782;&#21035;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.03407</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22312;&#32447;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Online Gesture Recognition using Transformer and Natural Language Processing. (arXiv:2305.03407v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Transformer&#26550;&#26500;&#23454;&#29616;&#22312;&#32447;&#25163;&#21183;&#35782;&#21035;&#65292;&#24182;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#22810;&#32423;&#20998;&#21106;&#38382;&#39064;&#30340;&#26426;&#22120;&#32763;&#35793;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35299;&#30721;&#31354;&#38388;&#25552;&#39640;&#23545;&#35821;&#27861;&#35268;&#21017;&#21644;&#32570;&#25439;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#33021;&#22815;&#29992;&#20110;&#22810;&#31181;&#35821;&#35328;&#21644;&#36890;&#29992;&#25163;&#20889;&#35782;&#21035;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Transformer&#26550;&#26500;&#20026;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#30340;&#23383;&#24418;&#31508;&#30011;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#22312;&#32447;&#25163;&#20889;&#25163;&#21183;&#26426;&#22120;&#32763;&#35793;&#26694;&#26550;&#12290;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#31471;&#21040;&#31471;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#22810;&#32423;&#20998;&#21106;&#38382;&#39064;&#65292;&#21516;&#26679;&#23398;&#20064;&#20102;&#19968;&#20123;&#35821;&#35328;&#29305;&#24449;&#21644;&#21477;&#27861;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20102;&#19968;&#20123;&#23398;&#20064;&#30340;Byte-Pair-Encoding&#65288;BPE&#65289;&#30340;&#22823;&#22411;&#35299;&#30721;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#23545;&#35821;&#27861;&#35268;&#21017;&#21644;&#32570;&#25439;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;&#32534;&#30721;&#22120;&#22534;&#26632;&#30452;&#25509;&#33719;&#21462;&#26102;&#31354;&#25968;&#25454;&#20196;&#29260;&#65292;&#28508;&#22312;&#22320;&#24418;&#25104;&#20102;&#19968;&#20010;&#26080;&#38480;&#22823;&#30340;&#36755;&#20837;&#35789;&#27719;&#34920;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#24212;&#29992;&#36229;&#20986;&#20102;&#26412;&#25991;&#30340;&#33539;&#22260;&#12290;&#25991;&#31456;&#36824;&#23637;&#31034;&#20102;&#32534;&#30721;&#22120;&#36801;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#21487;&#29992;&#20110;&#22810;&#31181;&#35821;&#35328;&#65292;&#32467;&#26524;&#26356;&#24555;&#30340;&#20248;&#21270;&#21644;&#20849;&#20139;&#21442;&#25968;&#12290;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#22312;&#32447;&#25163;&#20889;&#25163;&#21183;&#30340;&#30417;&#30563;&#25968;&#25454;&#38598;&#65292;&#36866;&#29992;&#20110;&#36890;&#29992;&#25163;&#20889;&#35782;&#21035;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Transformer architecture is shown to provide a powerful machine transduction framework for online handwritten gestures corresponding to glyph strokes of natural language sentences. The attention mechanism is successfully used to create latent representations of an end-to-end encoder-decoder model, solving multi-level segmentation while also learning some language features and syntax rules. The additional use of a large decoding space with some learned Byte-Pair-Encoding (BPE) is shown to provide robustness to ablated inputs and syntax rules. The encoder stack was directly fed with spatio-temporal data tokens potentially forming an infinitely large input vocabulary, an approach that finds applications beyond that of this work. Encoder transfer learning capabilities is also demonstrated on several languages resulting in faster optimisation and shared parameters. A new supervised dataset of online handwriting gestures suitable for generic handwriting recognition tasks was used to succ
&lt;/p&gt;</description></item><item><title>MuSe 2023&#26159;&#19968;&#32452;&#20849;&#20139;&#20219;&#21153;&#65292;&#28041;&#21450;&#19977;&#20010;&#24403;&#20195;&#22810;&#27169;&#24577;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#38382;&#39064;&#65306;&#27169;&#25311;&#24773;&#24863;&#12289;&#36328;&#25991;&#21270;&#24189;&#40664;&#21644;&#20010;&#24615;&#21270;&#12290;&#21442;&#19982;&#32773;&#38656;&#35201;&#22312;&#21508;&#33258;&#30340;&#23376;&#25361;&#25112;&#20013;&#39044;&#27979;&#24773;&#24863;&#30446;&#26631;&#12289;&#36328;&#25991;&#21270;&#24189;&#40664;&#21644;&#20010;&#24615;&#21270;&#30340;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2305.03369</link><description>&lt;p&gt;
MuSe 2023&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#25361;&#25112;&#36187;&#65306;&#27169;&#25311;&#24773;&#24863;&#12289;&#36328;&#25991;&#21270;&#24189;&#40664;&#21644;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
The MuSe 2023 Multimodal Sentiment Analysis Challenge: Mimicked Emotions, Cross-Cultural Humour, and Personalisation. (arXiv:2305.03369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03369
&lt;/p&gt;
&lt;p&gt;
MuSe 2023&#26159;&#19968;&#32452;&#20849;&#20139;&#20219;&#21153;&#65292;&#28041;&#21450;&#19977;&#20010;&#24403;&#20195;&#22810;&#27169;&#24577;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#38382;&#39064;&#65306;&#27169;&#25311;&#24773;&#24863;&#12289;&#36328;&#25991;&#21270;&#24189;&#40664;&#21644;&#20010;&#24615;&#21270;&#12290;&#21442;&#19982;&#32773;&#38656;&#35201;&#22312;&#21508;&#33258;&#30340;&#23376;&#25361;&#25112;&#20013;&#39044;&#27979;&#24773;&#24863;&#30446;&#26631;&#12289;&#36328;&#25991;&#21270;&#24189;&#40664;&#21644;&#20010;&#24615;&#21270;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MuSe 2023&#26159;&#19968;&#32452;&#20849;&#20139;&#20219;&#21153;&#65292;&#28041;&#21450;&#19977;&#20010;&#24403;&#20195;&#22810;&#27169;&#24577;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#38382;&#39064;&#65306;&#22312;&#27169;&#25311;&#24773;&#24863;&#23376;&#25361;&#25112;&#65288;MuSe-Mimic&#65289;&#20013;&#65292;&#21442;&#19982;&#32773;&#39044;&#27979;&#19977;&#20010;&#36830;&#32493;&#24773;&#24863;&#30446;&#26631;&#12290;&#36825;&#20010;&#23376;&#25361;&#25112;&#21033;&#29992;&#20102;Hume-Vidmimic&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#29983;&#25104;&#30340;&#35270;&#39057;&#12290;&#23545;&#20110;&#36328;&#25991;&#21270;&#24189;&#40664;&#26816;&#27979;&#23376;&#25361;&#25112;&#65288;MuSe-Humour&#65289;&#65292;&#25552;&#20379;&#20102;Passau Spontaneous Football Coach Humour&#65288;Passau-SFCH&#65289;&#25968;&#25454;&#38598;&#30340;&#25193;&#23637;&#12290;&#21442;&#19982;&#32773;&#38656;&#35201;&#39044;&#27979;&#36328;&#25991;&#21270;&#29615;&#22659;&#20013;&#33258;&#21457;&#24189;&#40664;&#30340;&#20986;&#29616;&#12290;&#20010;&#24615;&#21270;&#23376;&#25361;&#25112;&#65288;MuSe-Personalisation&#65289;&#22522;&#20110;Ulm-Trier Social Stress Test&#65288;Ulm-TSST&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22788;&#20110;&#32039;&#24352;&#29366;&#24577;&#19979;&#30340;&#34987;&#35797;&#24405;&#38899;&#12290;&#22312;&#36825;&#37324;&#65292;&#38656;&#35201;&#39044;&#27979;&#21796;&#37266;&#21644;&#20215;&#20540;&#20449;&#21495;&#65292;&#32780;&#37096;&#20998;&#27979;&#35797;&#26631;&#31614;&#21017;&#21487;&#29992;&#20110;&#20419;&#36827;&#20010;&#24615;&#21270;&#12290;MuSe 2023&#26088;&#22312;&#23558;&#26469;&#33258;&#19981;&#21516;&#30740;&#31350;&#31038;&#21306;&#30340;&#24191;&#27867;&#21463;&#20247;&#27719;&#32858;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
The MuSe 2023 is a set of shared tasks addressing three different contemporary multimodal affect and sentiment analysis problems: In the Mimicked Emotions Sub-Challenge (MuSe-Mimic), participants predict three continuous emotion targets. This sub-challenge utilises the Hume-Vidmimic dataset comprising of user-generated videos. For the Cross-Cultural Humour Detection Sub-Challenge (MuSe-Humour), an extension of the Passau Spontaneous Football Coach Humour (Passau-SFCH) dataset is provided. Participants predict the presence of spontaneous humour in a cross-cultural setting. The Personalisation Sub-Challenge (MuSe-Personalisation) is based on the Ulm-Trier Social Stress Test (Ulm-TSST) dataset, featuring recordings of subjects in a stressed situation. Here, arousal and valence signals are to be predicted, whereas parts of the test labels are made available in order to facilitate personalisation. MuSe 2023 seeks to bring together a broad audience from different research communities such as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35299;&#26512;-&#25191;&#34892;-&#20248;&#21270;&#65288;Parse-Execute-Refine&#65289;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#21521;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#27169;&#22411;&#28436;&#31034;&#25191;&#34892;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#21487;&#20197;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03356</link><description>&lt;p&gt;
&#20174;&#35299;&#26512;-&#25191;&#34892;&#21040;&#35299;&#26512;-&#25191;&#34892;-&#20248;&#21270;&#65306;&#25552;&#39640;&#22797;&#26434;&#38382;&#39064;&#31572;&#26696;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;
&lt;/p&gt;
&lt;p&gt;
From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base. (arXiv:2305.03356v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35299;&#26512;-&#25191;&#34892;-&#20248;&#21270;&#65288;Parse-Execute-Refine&#65289;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#21521;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#27169;&#22411;&#28436;&#31034;&#25191;&#34892;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#21487;&#20197;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25226;&#38382;&#39064;&#35299;&#26512;&#25104;&#21487;&#25191;&#34892;&#30340;&#36923;&#36753;&#24418;&#24335;&#23545;&#20110;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#26377;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#36827;&#34892;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KoPL&#30340;&#26032;&#22411;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#26088;&#22312;&#26174;&#24335;&#22320;&#27169;&#25311;&#25512;&#29702;&#36807;&#31243;&#65292;&#22312;&#22797;&#26434;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#39046;&#22495;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#26512;-&#25191;&#34892;-&#20248;&#21270;&#33539;&#24335;&#26469;&#24320;&#21457;&#35821;&#20041;&#35299;&#26512;&#22120;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#21521;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#27169;&#22411;&#28436;&#31034;&#25191;&#34892;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#23436;&#21892;&#21644;&#25913;&#36827;KoPL&#35299;&#26512;&#22120;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#26679;&#31616;&#21333;&#30340;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#35299;&#26512;&#38454;&#27573;&#65292;&#25191;&#34892;&#38454;&#27573;&#21644;&#20248;&#21270;&#38454;&#27573;&#65292;&#20197;&#22686;&#24378;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35299;&#26512;&#22120;&#20351;&#29992;KoPL&#29983;&#25104;&#36879;&#26126;&#30340;&#36923;&#36753;&#24418;&#24335;&#12290;&#28982;&#21518;&#65292;&#25191;&#34892;&#38454;&#27573;&#23545;&#40784;&#21644;&#25191;&#34892;&#36825;&#20123;&#36923;&#36753;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parsing questions into executable logical forms has showed impressive results for knowledge-base question answering (KBQA). However, complex KBQA is a more challenging task that requires to perform complex multi-step reasoning. Recently, a new semantic parser called KoPL has been proposed to explicitly model the reasoning processes, which achieved the state-of-the-art on complex KBQA. In this paper, we further explore how to unlock the reasoning ability of semantic parsers by a simple proposed parse-execute-refine paradigm. We refine and improve the KoPL parser by demonstrating the executed intermediate reasoning steps to the KBQA model. We show that such simple strategy can significantly improve the ability of complex reasoning. Specifically, we propose three components: a parsing stage, an execution stage and a refinement stage, to enhance the ability of complex reasoning. The parser uses the KoPL to generate the transparent logical forms. Then, the execution stage aligns and execute
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#25506;&#35752;&#20102;&#29702;&#35299;&#24515;&#26234;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;GPT-4&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#33021;&#21147;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.03353</link><description>&lt;p&gt;
MindGames&#65306;&#21033;&#29992;&#21160;&#24577;&#35748;&#30693;&#27169;&#24577;&#36923;&#36753;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38024;&#23545;&#24515;&#26234;&#29702;&#35770;&#36827;&#34892;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic. (arXiv:2305.03353v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#25506;&#35752;&#20102;&#29702;&#35299;&#24515;&#26234;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;GPT-4&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#33021;&#21147;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#26234;&#29702;&#35770;(ToM)&#26159;&#26234;&#33021;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#20934;&#30830;&#24230;&#37327;&#23427;&#20173;&#28982;&#26159;&#19968;&#20010;&#20105;&#35758;&#35805;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#23558;&#20154;&#31867;ToM&#35780;&#20272;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#20351;&#29992;&#20154;&#31867;&#21019;&#24314;&#30340;&#26631;&#20934;&#21270;&#27979;&#35797;&#25110;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#26495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#30340;&#25512;&#29702;&#19978;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#19982;ToM&#37325;&#21472;&#30340;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#26469;&#29983;&#25104;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#26032;&#30340;&#35821;&#35328;&#25216;&#24039;&#26469;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#65288;&#20174;70M&#21040;6B&#21644;350M&#21040;174B&#65289;&#24182;&#19981;&#19968;&#33268;&#22320;&#20135;&#29983;&#27604;&#38543;&#26426;&#32467;&#26524;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;GPT-4&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#35748;&#30693;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#20173;&#26377;&#25552;&#21319;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20844;&#24320;&#33719;&#21462;&#65306;https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileo
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM) is a critical component of intelligence, yet accurately measuring it continues to be a subject of debate. Prior research has attempted to apply human ToM assessments to natural language processing models using either human-created standardized tests or rule-based templates. However, these methods primarily focus on simplistic reasoning and require further validation. In this study, we utilize dynamic epistemic logic, which has established overlaps with ToM, to generate more intricate problems. We also introduce novel verbalization techniques to express these problems using natural language. Our findings indicate that certain language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance. While GPT-4 demonstrates improved epistemic reasoning capabilities, there is still room for enhancement. Our code and datasets are publicly available https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;QCRI&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#20351;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#25490;&#21517;&#21069;&#19977;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#21435;&#35299;&#20915;&#20998;&#26512;&#21644;&#39564;&#35777;&#22312;&#32447;&#26032;&#38395;&#20256;&#25773;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03336</link><description>&lt;p&gt;
QCRI&#22312;SemEval-2023&#20219;&#21153;3 &#20013;&#30340;&#34920;&#29616;&#65306;&#20351;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#26032;&#38395;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion Techniques Detection using Multilingual Models. (arXiv:2305.03336v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;QCRI&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#20351;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#25490;&#21517;&#21069;&#19977;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#21435;&#35299;&#20915;&#20998;&#26512;&#21644;&#39564;&#35777;&#22312;&#32447;&#26032;&#38395;&#20256;&#25773;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#27969;&#21644;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#19968;&#30452;&#20197;&#26469;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#35823;&#23548;&#29992;&#25143;&#12290;&#30001;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#21592;&#36827;&#34892;&#25163;&#21160;&#26816;&#27979;&#21644;&#39564;&#35777;&#30340;&#21162;&#21147;&#24050;&#32463;&#26080;&#27861;&#24212;&#23545;&#34394;&#20551;&#20449;&#24687;&#30340;&#24555;&#36895;&#25193;&#25955;&#21644;&#22823;&#35268;&#27169;&#20256;&#25773;&#12290;&#36825;&#28608;&#21169;&#20102;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#30340;&#21162;&#21147;&#65292;&#20197;&#24320;&#21457;&#29992;&#20110;&#20998;&#26512;&#21644;&#39564;&#35777;&#22312;&#32447;&#26032;&#38395;&#20256;&#25773;&#30340;&#31995;&#32479;&#12290; SemEval-2023&#20219;&#21153;3 &#23581;&#35797;&#35299;&#20915;&#22312;&#36825;&#20010;&#24635;&#20307;&#38382;&#39064;&#19979;&#30340;&#20960;&#20010;&#23376;&#20219;&#21153;&#65292;&#38024;&#23545;&#26032;&#38395;&#25991;&#31456;&#20013;&#20351;&#29992;&#30340;&#20889;&#20316;&#25216;&#24039;&#20197;&#24433;&#21709;&#35835;&#32773;&#30340;&#35266;&#28857;&#12290;&#35813;&#20219;&#21153;&#20351;&#29992;&#20845;&#31181;&#35821;&#35328;&#26469;&#35299;&#20915;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#27492;&#22806;&#36824;&#26377;&#19977;&#31181;&#8220;&#20986;&#20154;&#24847;&#26009;&#8221;&#30340;&#27979;&#35797;&#35821;&#35328;&#65292;&#20849;&#35745;27&#31181;&#19981;&#21516;&#30340;&#27979;&#35797;&#29615;&#22659;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#19982;&#35813;&#20219;&#21153;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#22242;&#38431;&#26159;6&#20010;&#25104;&#21151;&#25552;&#20132;&#25152;&#26377;&#29615;&#22659;&#36816;&#34892;&#30340;&#22242;&#38431;&#20043;&#19968;&#12290;&#23448;&#26041;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;27&#31181;&#27979;&#35797;&#29615;&#22659;&#20013;&#26377;10&#20010;&#25490;&#21517;&#22312;&#21069;&#19977;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation spreading in mainstream and social media has been misleading users in different ways. Manual detection and verification efforts by journalists and fact-checkers can no longer cope with the great scale and quick spread of misleading information. This motivated research and industry efforts to develop systems for analyzing and verifying news spreading online. The SemEval-2023 Task 3 is an attempt to address several subtasks under this overarching problem, targeting writing techniques used in news articles to affect readers' opinions. The task addressed three subtasks with six languages, in addition to three ``surprise'' test languages, resulting in 27 different test setups. This paper describes our participating system to this task. Our team is one of the 6 teams that successfully submitted runs for all setups. The official results show that our system is ranked among the top 3 systems for 10 out of the 27 setups.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#27169;&#25311;&#38271;&#25991;&#26723;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#38382;&#39064;&#65292;&#22312;&#26032;&#25552;&#20986;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03319</link><description>&lt;p&gt;
HiPool&#65306;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#38271;&#25991;&#26723;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
HiPool: Modeling Long Documents Using Graph Neural Networks. (arXiv:2305.03319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#27169;&#25311;&#38271;&#25991;&#26723;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#38382;&#39064;&#65292;&#22312;&#26032;&#25552;&#20986;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#32534;&#30721;&#38271;&#24207;&#21015;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;NLP&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#20173;&#21463;&#21040;&#39044;&#23450;&#20041;&#30340;&#26368;&#22823;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#25193;&#23637;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#26469;&#24314;&#27169;&#38271;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22823;&#22810;&#25968;&#26159;&#23545;&#19978;&#23618;&#20351;&#29992;&#39034;&#24207;&#27169;&#22411;&#65292;&#38754;&#20020;&#30528;&#38271;&#26399;&#20381;&#36182;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22266;&#23450;&#38271;&#24230;&#23545;&#24207;&#21015;&#36827;&#34892;&#20998;&#22359;&#65292;&#20197;&#27169;&#25311;&#21477;&#23376;&#32423;&#21035;&#30340;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#26469;&#27169;&#25311;&#21477;&#20869;&#21644;&#36328;&#21477;&#30340;&#20851;&#32852;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#38271;&#25991;&#26723;&#20998;&#31867;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#36739;&#23569;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20849;&#35745;&#20845;&#20010;&#25968;&#25454;&#38598;&#65292;&#26679;&#26412;&#24635;&#25968;&#36798;53000&#20010;&#65292;&#24179;&#22343;&#26631;&#35760;&#38271;&#24230;&#20026;4034&#20010;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Encoding long sequences in Natural Language Processing (NLP) is a challenging problem. Though recent pretraining language models achieve satisfying performances in many NLP tasks, they are still restricted by a pre-defined maximum length, making them challenging to be extended to longer sequences. So some recent works utilize hierarchies to model long sequences. However, most of them apply sequential models for upper hierarchies, suffering from long dependency issues. In this paper, we alleviate these issues through a graph-based method. We first chunk the sequence with a fixed length to model the sentence-level information. We then leverage graphs to model intra- and cross-sentence correlations with a new attention mechanism. Additionally, due to limited standard benchmarks for long document classification (LDC), we propose a new challenging benchmark, totaling six datasets with up to 53k samples and 4034 average tokens' length. Evaluation shows our model surpasses competitive baselin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LLM-RM&#22242;&#38431;&#21442;&#21152;SemEval2023&#20219;&#21153;2&#30340;&#25104;&#26524;&#65292;&#22312;&#36328;&#35821;&#35328;&#22810;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#65292;&#36890;&#36807;fine-tuning XLM-Roberta&#30340;&#20132;&#21449;&#35821;&#35328;&#34920;&#31034;&#65292;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03300</link><description>&lt;p&gt;
&#20351;&#29992;XLM-RoBERTa&#36827;&#34892;&#36328;&#35821;&#35328;&#22810;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;LLM-RM&#21442;&#21152;SemEval-2023&#20219;&#21153;2
&lt;/p&gt;
&lt;p&gt;
LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using XLM-RoBERTa. (arXiv:2305.03300v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LLM-RM&#22242;&#38431;&#21442;&#21152;SemEval2023&#20219;&#21153;2&#30340;&#25104;&#26524;&#65292;&#22312;&#36328;&#35821;&#35328;&#22810;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#65292;&#36890;&#36807;fine-tuning XLM-Roberta&#30340;&#20132;&#21449;&#35821;&#35328;&#34920;&#31034;&#65292;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#35782;&#21035;&#21477;&#23376;&#20013;&#26631;&#35760;&#32423;&#21035;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#35299;&#20915;&#22810;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#30340;NER&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;LLM-RM&#21442;&#21152;&#20102;&#26368;&#36817;&#20030;&#21150;&#30340;SemEval 2023&#20219;&#21153;2&#65306;MultiCoNER II&#65292;&#22810;&#35821;&#35328;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#23545;&#25152;&#26377;12&#31181;&#25552;&#20379;&#30340;&#35821;&#35328;&#65288;&#23391;&#21152;&#25289;&#35821;&#12289;&#20013;&#25991;&#12289;&#33521;&#35821;&#12289;&#27874;&#26031;&#35821;&#12289;&#27861;&#35821;&#12289;&#24503;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#29790;&#20856;&#35821;&#21644;&#20044;&#20811;&#20848;&#35821;&#65289;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;XLM-Roberta&#22522;&#30784;&#27169;&#22411;&#30340;&#20132;&#21449;&#35821;&#35328;&#34920;&#31034;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition(NER) is a task of recognizing entities at a token level in a sentence. This paper focuses on solving NER tasks in a multilingual setting for complex named entities. Our team, LLM-RM participated in the recently organized SemEval 2023 task, Task 2: MultiCoNER II,Multilingual Complex Named Entity Recognition. We approach the problem by leveraging cross-lingual representation provided by fine-tuning XLM-Roberta base model on datasets of all of the 12 languages provided -- Bangla, Chinese, English, Farsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish and Ukrainian
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#23376;&#20316;&#20026;&#22359;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#23558;&#22359;&#30340;&#36328;&#24230;&#35782;&#21035;&#20026;&#20803;&#32452;&#20851;&#31995;&#21644;&#21442;&#25968;&#65292;&#37319;&#29992;Chunk-OIE&#36827;&#34892;&#20803;&#32452;&#25552;&#21462;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03299</link><description>&lt;p&gt;
&#22522;&#20110;&#22359;&#30340;&#24320;&#25918;&#24335;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Open Information Extraction via Chunks. (arXiv:2305.03299v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#23376;&#20316;&#20026;&#22359;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#23558;&#22359;&#30340;&#36328;&#24230;&#35782;&#21035;&#20026;&#20803;&#32452;&#20851;&#31995;&#21644;&#21442;&#25968;&#65292;&#37319;&#29992;Chunk-OIE&#36827;&#34892;&#20803;&#32452;&#25552;&#21462;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#20449;&#24687;&#25277;&#21462;(OIE)&#33268;&#21147;&#20110;&#20174;&#24320;&#25918;&#39046;&#22495;&#30340;&#21477;&#23376;&#20013;&#25552;&#21462;&#20851;&#31995;&#20803;&#32452;&#12290;&#29616;&#26377;&#30340;OIE&#31995;&#32479;&#23558;&#21477;&#23376;&#25286;&#25104;&#26631;&#35760;&#21518;&#35782;&#21035;&#26631;&#35760;&#30340;&#36328;&#24230;&#20316;&#20026;&#20803;&#32452;&#20851;&#31995;&#21644;&#21442;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#21477;&#23376;&#20316;&#20026;&#22359;&#24207;&#21015;(SaC)&#65292;&#24182;&#23558;&#22359;&#30340;&#36328;&#24230;&#35782;&#21035;&#20026;&#20803;&#32452;&#20851;&#31995;&#21644;&#21442;&#25968;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;SaC&#23545;&#20110;OIE&#20855;&#26377;&#27604;&#26631;&#35760;&#24207;&#21015;&#26356;&#22909;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#29305;&#24615;&#65292;&#24182;&#38024;&#23545;&#22235;&#31181;&#22359;&#30340;&#36873;&#25321;&#65288;&#21363;CoNLL&#22359;&#12289;&#31616;&#21333;&#30701;&#35821;&#12289;NP&#22359;&#21644;SpanOIE&#30340;&#36328;&#24230;&#65289;&#38024;&#23545;gold OIE&#20803;&#32452;&#36827;&#34892;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#21477;&#23376;&#20998;&#22359;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;SaC&#20043;&#19978;&#36827;&#34892;&#30340;&#20803;&#32452;&#25552;&#21462;&#30340;Chunk-OIE&#12290;Chunk-OIE&#22312;&#22810;&#20010;OIE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65292;&#26174;&#31034;&#20102;SaC&#23545;OIE&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open Information Extraction (OIE) aims to extract relational tuples from open-domain sentences. Existing OIE systems split a sentence into tokens and recognize token spans as tuple relations and arguments. We instead propose Sentence as Chunk sequence (SaC) and recognize chunk spans as tuple relations and arguments. We argue that SaC has better quantitative and qualitative properties for OIE than sentence as token sequence, and evaluate four choices of chunks (i.e., CoNLL chunks, simple phrases, NP chunks, and spans from SpanOIE) against gold OIE tuples. Accordingly, we propose a simple BERT-based model for sentence chunking, and propose Chunk-OIE for tuple extraction on top of SaC. Chunk-OIE achieves state-of-the-art results on multiple OIE datasets, showing that SaC benefits OIE task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; Mix Prompt Tuning&#65288;MPT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25163;&#21160;&#25552;&#31034;&#27169;&#26495;&#19982;&#33258;&#21160;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#27169;&#26495;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#22810;&#31890;&#24230;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#36731;&#23545;&#27880;&#37322;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2305.03287</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20010;&#25552;&#31034;&#30693;&#35782;&#30340;&#20302;&#36164;&#28304;&#22810;&#31890;&#24230;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Multi-Granularity Academic Function Recognition Based on Multiple Prompt Knowledge. (arXiv:2305.03287v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; Mix Prompt Tuning&#65288;MPT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25163;&#21160;&#25552;&#31034;&#27169;&#26495;&#19982;&#33258;&#21160;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#27169;&#26495;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#22810;&#31890;&#24230;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#36731;&#23545;&#27880;&#37322;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;Fine-tuning &#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22914; SciBERT&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#25165;&#33021;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#33719;&#21462;&#31185;&#23398; NLP &#20219;&#21153;&#30340; fine-tune &#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26114;&#36149;&#24615;&#12290;&#21463;&#25552;&#31034;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; Mix Prompt Tuning&#65288;MPT&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#36731;&#23545;&#27880;&#37322;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#24182;&#20351;&#29992;&#24456;&#23569;&#25968;&#37327;&#30340;&#26631;&#35760;&#31034;&#20363;&#25552;&#39640;&#22810;&#31890;&#24230;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#25163;&#21160;&#25552;&#31034;&#27169;&#26495;&#19982;&#33258;&#21160;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#27169;&#26495;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#22810;&#26041;&#38754;&#30340;&#34920;&#31034;&#65292;&#20197;&#24110;&#21161;&#32473;&#23450;&#30340;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;&#20219;&#21153;&#20805;&#20998;&#21033;&#29992; PLMs &#20013;&#30340;&#30693;&#35782;&#12290;&#22522;&#20110;&#36825;&#20123;&#25552;&#31034;&#27169;&#26495;&#21644; fine-tuned PLM&#65292;&#22823;&#37327;&#30340;&#20266;&#26631;&#31614;&#34987;&#20998;&#37197;&#32473;&#26410;&#26631;&#35760;&#30340;&#23454;&#20363;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained language models (PLMs), e.g., SciBERT, generally requires large numbers of annotated data to achieve state-of-the-art performance on a range of NLP tasks in the scientific domain. However, obtaining the fine-tune data for scientific NLP task is still challenging and expensive. Inspired by recent advancement in prompt learning, in this paper, we propose the Mix Prompt Tuning (MPT), which is a semi-supervised method to alleviate the dependence on annotated data and improve the performance of multi-granularity academic function recognition tasks with a small number of labeled examples. Specifically, the proposed method provides multi-perspective representations by combining manual prompt templates with automatically learned continuous prompt templates to help the given academic function recognition task take full advantage of knowledge in PLMs. Based on these prompt templates and the fine-tuned PLM, a large number of pseudo labels are assigned to the unlabeled exam
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;&#8220;&#19987;&#19994;&#24615;&#25991;&#26412;&#29983;&#25104;&#8221;&#30340;&#20219;&#21153;&#65292;&#30446;&#30340;&#22312;&#20110;&#20174;&#30693;&#35782;&#26469;&#28304;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#12289;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26723;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#30340;IRP&#26694;&#26550;&#33021;&#22815;&#20811;&#26381;&#35821;&#35328;&#27169;&#22411;&#30340;&#32570;&#28857;&#65292;&#23454;&#29616;&#20102;&#20869;&#23481;&#35268;&#21010;&#12289;&#20107;&#23454;&#36873;&#25321; &#21644;&#25913;&#20889;&#27493;&#39588;&#20998;&#21035;&#22788;&#29702;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;&#25991;&#26723;&#20855;&#26377;&#36739;&#39640;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.03276</link><description>&lt;p&gt;
&#19987;&#19994;&#24615;&#25991;&#26412;&#29983;&#25104;&#65306;&#27169;&#20223;&#12289;&#26816;&#32034;&#12289;&#25913;&#20889;
&lt;/p&gt;
&lt;p&gt;
Expository Text Generation: Imitate, Retrieve, Paraphrase. (arXiv:2305.03276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03276
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;&#8220;&#19987;&#19994;&#24615;&#25991;&#26412;&#29983;&#25104;&#8221;&#30340;&#20219;&#21153;&#65292;&#30446;&#30340;&#22312;&#20110;&#20174;&#30693;&#35782;&#26469;&#28304;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#12289;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26723;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#30340;IRP&#26694;&#26550;&#33021;&#22815;&#20811;&#26381;&#35821;&#35328;&#27169;&#22411;&#30340;&#32570;&#28857;&#65292;&#23454;&#29616;&#20102;&#20869;&#23481;&#35268;&#21010;&#12289;&#20107;&#23454;&#36873;&#25321; &#21644;&#25913;&#20889;&#27493;&#39588;&#20998;&#21035;&#22788;&#29702;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;&#25991;&#26723;&#20855;&#26377;&#36739;&#39640;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#19994;&#24615;&#25991;&#26723;&#26159;&#21521;&#35835;&#32773;&#20256;&#36798;&#22797;&#26434;&#20449;&#24687;&#30340;&#37325;&#35201;&#36164;&#28304;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#26377;&#29992;&#65292;&#20294;&#25163;&#21160;&#32534;&#20889;&#19987;&#19994;&#24615;&#25991;&#29486;&#26159;&#32791;&#26102;&#19988;&#21171;&#21160;&#24378;&#24230;&#22823;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#23545;&#25152;&#28041;&#39046;&#22495;&#26377;&#28145;&#20837;&#20102;&#35299;&#12289;&#31934;&#24515;&#35268;&#21010;&#20869;&#23481;&#12289;&#20197;&#21450;&#33021;&#22815;&#20174;&#22810;&#20010;&#26469;&#28304;&#32508;&#21512;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#20943;&#36731;&#36127;&#25285;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19987;&#19994;&#24615;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#23427;&#26088;&#22312;&#20174;&#30693;&#35782;&#26469;&#28304;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#12289;&#20449;&#24687;&#20016;&#23500;&#30340;&#19987;&#19994;&#24615;&#25991;&#26723;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;IRP&#35299;&#20915;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;IRP&#26159;&#19968;&#20010;&#36845;&#20195;&#26694;&#26550;&#65292;&#20811;&#26381;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#65292;&#20998;&#21035;&#22788;&#29702;&#20102;&#20869;&#23481;&#35268;&#21010;&#12289;&#20107;&#23454;&#36873;&#25321;&#21644;&#25913;&#20889;&#30340;&#27493;&#39588;&#12290;&#36890;&#36807;&#23545;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;IRP&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#19987;&#19994;&#24615;&#25991;&#26723;&#65292;&#20934;&#30830;&#22320;&#21521;&#35835;&#32773;&#20256;&#36882;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expository documents are vital resources for conveying complex information to readers. Despite their usefulness, writing expository documents by hand is a time-consuming and labor-intensive process that requires knowledge of the domain of interest, careful content planning, and the ability to synthesize information from multiple sources. To ease these burdens, we introduce the task of expository text generation, which seeks to automatically generate an accurate and informative expository document from a knowledge source. We solve our task by developing IRP, an iterative framework that overcomes the limitations of language models and separately tackles the steps of content planning, fact selection, and rephrasing. Through experiments on three diverse datasets, we demonstrate that IRP produces high-quality expository documents that accurately inform readers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Verify-and-Edit&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26681;&#25454;&#22806;&#37096;&#30693;&#35782;&#21518;&#32534;&#36753;&#25512;&#29702;&#38142;&#65292;&#25552;&#39640;CoT&#25552;&#31034;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03268</link><description>&lt;p&gt;
Verify-and-Edit: &#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#24605;&#36335;&#38142;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework. (arXiv:2305.03268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Verify-and-Edit&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26681;&#25454;&#22806;&#37096;&#30693;&#35782;&#21518;&#32534;&#36753;&#25512;&#29702;&#38142;&#65292;&#25552;&#39640;CoT&#25552;&#31034;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;NLP&#20013;&#25104;&#20026;&#24120;&#24577;&#65292;&#22312;&#29983;&#25104;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20854;&#26368;&#33268;&#21629;&#30340;&#32570;&#28857;&#20043;&#19968;&#26159;&#32570;&#20047;&#20107;&#23454;&#27491;&#30830;&#24615;&#12290;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#25991;&#26412;&#19981;&#20165;&#23548;&#33268;&#34920;&#29616;&#19979;&#38477;&#65292;&#32780;&#19988;&#38477;&#20302;&#20102;&#20854;&#24212;&#29992;&#30340;&#20449;&#20219;&#21644;&#26377;&#25928;&#24615;&#12290;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#36890;&#36807;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#38142;&#65292;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#25552;&#39640;&#20449;&#20219;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#20173;&#23384;&#22312;&#20107;&#23454;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Verify-and-Edit&#26694;&#26550;&#65292;&#29992;&#20110;CoT&#25552;&#31034;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26681;&#25454;&#22806;&#37096;&#30693;&#35782;&#21518;&#32534;&#36753;&#25512;&#29702;&#38142;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;GPT-3&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#22810;&#20010;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) have become the norm in NLP, demonstrating good performance in generation and reasoning tasks, one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications. Chain-of-Thought (CoT) prompting improves trust and model performance on complex reasoning tasks by generating interpretable reasoning chains, but still suffers from factuality concerns in knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit framework for CoT prompting, which seeks to increase prediction factuality by post-editing reasoning chains according to external knowledge. Building on top of GPT-3, our framework lead to accuracy improvements in multiple open-domain question-answering tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27515;&#35282;&#22797;&#27963;&#65288;DDR&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#21450;&#26102;&#39640;&#25928;&#22320;&#26816;&#27979;&#36215;&#22987;&#27515;&#35282;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#25327;&#25937;&#34892;&#21160;&#20197;&#24341;&#23548;&#21644;&#32416;&#27491;&#23545;&#35805;&#25506;&#32034;&#26041;&#21521;&#65292;&#21516;&#26102;&#38450;&#27490;&#23545;&#35805;&#31574;&#30053;&#19981;&#26029;&#29359;&#21516;&#26679;&#30340;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2305.03262</link><description>&lt;p&gt;
&#20174;&#27515;&#35282;&#20013;&#35299;&#25937;&#23545;&#35805;&#65306;&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#31574;&#30053;&#20248;&#21270;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Rescue Conversations from Dead-ends: Efficient Exploration for Task-oriented Dialogue Policy Optimization. (arXiv:2305.03262v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27515;&#35282;&#22797;&#27963;&#65288;DDR&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#21450;&#26102;&#39640;&#25928;&#22320;&#26816;&#27979;&#36215;&#22987;&#27515;&#35282;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#25327;&#25937;&#34892;&#21160;&#20197;&#24341;&#23548;&#21644;&#32416;&#27491;&#23545;&#35805;&#25506;&#32034;&#26041;&#21521;&#65292;&#21516;&#26102;&#38450;&#27490;&#23545;&#35805;&#31574;&#30053;&#19981;&#26029;&#29359;&#21516;&#26679;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#23545;&#35805;&#31574;&#30053;&#38656;&#35201;&#23545;&#29615;&#22659;&#36827;&#34892;&#22823;&#37327;&#25506;&#32034;&#12290;&#26080;&#25928;&#30340;&#25506;&#32034;&#28010;&#36153;&#20102;&#24456;&#22810;&#26102;&#38388;&#65292;&#20351;&#24471;&#23398;&#20064;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#21457;&#29616;&#24182;&#23450;&#20041;&#20102;&#26080;&#25928;&#25506;&#32034;&#30340;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#65306;&#27515;&#35282;&#12290;&#24403;&#23545;&#35805;&#36827;&#20837;&#27515;&#35282;&#29366;&#24577;&#26102;&#65292;&#19981;&#31649;&#20043;&#21518;&#37319;&#21462;&#20160;&#20040;&#34892;&#21160;&#65292;&#23427;&#37117;&#23558;&#32487;&#32493;&#27839;&#30528;&#27515;&#35282;&#36712;&#36857;&#65292;&#30452;&#21040;&#26234;&#33021;&#20307;&#36798;&#21040;&#32456;&#27490;&#29366;&#24577;&#25110;&#26368;&#22823;&#27425;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27515;&#35282;&#22797;&#27963;&#31639;&#27861;&#65288;DDR&#65289;&#65292;&#21450;&#26102;&#39640;&#25928;&#22320;&#26816;&#27979;&#36215;&#22987;&#27515;&#35282;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#25327;&#25937;&#34892;&#21160;&#20197;&#24341;&#23548;&#21644;&#32416;&#27491;&#25506;&#32034;&#26041;&#21521;&#12290;&#20026;&#20102;&#38450;&#27490;&#23545;&#35805;&#31574;&#30053;&#19981;&#26029;&#29359;&#21516;&#26679;&#30340;&#38169;&#35823;&#65292;DDR &#36824;&#36890;&#36807;&#28155;&#21152;&#21253;&#21547;&#27515;&#35282;&#29366;&#24577;&#30340;&#30456;&#20851;&#32463;&#39564;&#36827;&#34892;&#23545;&#35805;&#25968;&#25454;&#25193;&#20805;&#12290;&#26412;&#25991;&#39318;&#20808;&#39564;&#35777;&#20102;&#27515;&#35282;&#26816;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;&#25253;&#21578;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a dialogue policy using deep reinforcement learning requires a lot of exploration of the environment. The amount of wasted invalid exploration makes their learning inefficient. In this paper, we find and define an important reason for the invalid exploration: dead-ends. When a conversation enters a dead-end state, regardless of the actions taken afterward, it will continue in a dead-end trajectory until the agent reaches a termination state or maximum turn. We propose a dead-end resurrection (DDR) algorithm that detects the initial dead-end state in a timely and efficient manner and provides a rescue action to guide and correct the exploration direction. To prevent dialogue policies from repeatedly making the same mistake, DDR also performs dialogue data augmentation by adding relevant experiences containing dead-end states. We first validate the dead-end detection reliability and then demonstrate the effectiveness and generality of the method by reporting experimental results
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#39118;&#26684;&#21270;&#25968;&#25454;&#25991;&#26412;&#29983;&#25104;&#65292;&#26088;&#22312;&#26681;&#25454;&#29305;&#23450;&#39118;&#26684;&#20026;&#32473;&#23450;&#30340;&#38750;&#35821;&#35328;&#25968;&#25454;&#29983;&#25104;&#36830;&#36143;&#30340;&#25991;&#26412;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#27169;&#22411;&#26469;&#35299;&#20915;&#35813;&#20219;&#21153;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03256</link><description>&lt;p&gt;
&#39118;&#26684;&#21270;&#25968;&#25454;&#25991;&#26412;&#29983;&#25104;&#65306;&#22522;&#20110;&#30005;&#21830;&#39046;&#22495;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain. (arXiv:2305.03256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03256
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#39118;&#26684;&#21270;&#25968;&#25454;&#25991;&#26412;&#29983;&#25104;&#65292;&#26088;&#22312;&#26681;&#25454;&#29305;&#23450;&#39118;&#26684;&#20026;&#32473;&#23450;&#30340;&#38750;&#35821;&#35328;&#25968;&#25454;&#29983;&#25104;&#36830;&#36143;&#30340;&#25991;&#26412;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#27169;&#22411;&#26469;&#35299;&#20915;&#35813;&#20219;&#21153;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25968;&#25454;&#25991;&#26412;&#29983;&#25104;&#20027;&#35201;&#38598;&#20013;&#20110;&#20174;&#38750;&#35821;&#35328;&#36755;&#20837;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#21644;&#23646;&#24615;&#20540;&#23545;&#65289;&#29983;&#25104;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#20294;&#24573;&#35270;&#20102;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#39118;&#26684;&#30340;&#25991;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#39118;&#26684;&#21270;&#25968;&#25454;&#25991;&#26412;&#29983;&#25104;&#65292;&#20854;&#20219;&#21153;&#26159;&#26681;&#25454;&#29305;&#23450;&#39118;&#26684;&#20026;&#32473;&#23450;&#30340;&#38750;&#35821;&#35328;&#25968;&#25454;&#29983;&#25104;&#36830;&#36143;&#30340;&#25991;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleD2T&#30340;&#26032;&#39062;&#27169;&#22411;&#65292;&#21253;&#25324;&#19977;&#20010;&#32452;&#20214;&#65306;&#36923;&#36753;&#35268;&#21010;&#22686;&#24378;&#30340;&#25968;&#25454;&#23884;&#20837;&#12289;&#22522;&#20110;&#25513;&#30721;&#30340;&#39118;&#26684;&#23884;&#20837;&#21644;&#38750;&#20559;&#32622;&#24335;&#39118;&#26684;&#21270;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing data-to-text generation efforts mainly focus on generating a coherent text from non-linguistic input data, such as tables and attribute-value pairs, but overlook that different application scenarios may require texts of different styles. Inspired by this, we define a new task, namely stylized data-to-text generation, whose aim is to generate coherent text for the given non-linguistic data according to a specific style. This task is non-trivial, due to three challenges: the logic of the generated text, unstructured style reference, and biased training samples. To address these challenges, we propose a novel stylized data-to-text generation model, named StyleD2T, comprising three components: logic planning-enhanced data embedding, mask-based style embedding, and unbiased stylized text generation. In the first component, we introduce a graph-guided logic planner for attribute organization to ensure the logic of generated text. In the second component, we devise feature-level mask
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Vicuna&#30340;&#38646;/&#23567;&#26679;&#26412;NER&#26694;&#26550;VicunaNER&#65292;&#20855;&#26377;&#20248;&#24322;&#30340;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.03253</link><description>&lt;p&gt;
VicunaNER: &#20351;&#29992;Vicuna&#36827;&#34892;&#38646;/&#23567;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna. (arXiv:2305.03253v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Vicuna&#30340;&#38646;/&#23567;&#26679;&#26412;NER&#26694;&#26550;VicunaNER&#65292;&#20855;&#26377;&#20248;&#24322;&#30340;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65292;&#20363;&#22914;ChatGPT&#65289;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;/&#23567;&#26679;&#26412;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#21482;&#33021;&#36890;&#36807;&#22312;&#32447;API&#35775;&#38382;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#25968;&#25454;&#27844;&#28431;&#21644;&#26080;&#27861;&#37325;&#29616;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26032;&#21457;&#24067;&#30340;&#24320;&#28304;LLM--Vicuna&#30340;&#38646;/&#23567;&#26679;&#26412;NER&#26694;&#26550;VicunaNER&#12290;VicunaNER&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20854;&#20013;&#27599;&#20010;&#38454;&#27573;&#21033;&#29992;&#22810;&#36718;&#19982;Vicuna&#30340;&#23545;&#35805;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#23454;&#20307;&#12290;&#25105;&#20204;&#23558;&#31532;&#20108;&#38454;&#27573;&#21629;&#21517;&#20026;&#37325;&#26032;&#35782;&#21035;&#65292;&#23427;&#35782;&#21035;&#31532;&#19968;&#38454;&#27573;&#65288;&#21363;&#35782;&#21035;&#65289;&#20013;&#26410;&#35782;&#21035;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#38454;&#27573;&#35774;&#32622;&#23454;&#20307;&#27491;&#30830;&#24615;&#26816;&#26597;&#23545;&#35805;&#26694;&#20197;&#36807;&#28388;&#38169;&#35823;&#23454;&#20307;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;5&#20010;&#39046;&#22495;&#30340;10&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;VicunaNER&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#21644;Few-NERD&#30340;&#23567;&#26679;&#26412;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;VicunaNER&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#22343;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;VicunaNER&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs, e.g., ChatGPT) have shown impressive zero- and few-shot capabilities in Named Entity Recognition (NER). However, these models can only be accessed via online APIs, which may cause data leak and non-reproducible problems. In this paper, we propose VicunaNER, a zero/few-shot NER framework based on the newly released open-source LLM -- Vicuna. VicunaNER is a two-phase framework, where each phase leverages multi-turn dialogues with Vicuna to recognize entities from texts. We name the second phase as Re-Recognition, which recognizes those entities not recognized in the first phase (a.k.a. Recognition). Moreover, we set entity correctness check dialogues in each phase to filter out wrong entities. We evaluate VicunaNER's zero-shot capacity on 10 datasets crossing 5 domains and few-shot capacity on Few-NERD. Experimental results demonstrate that VicunaNER achieves superior performance in both shot settings. Additionally, we conduct comprehensive investigations on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26694;&#26550;&#65288;Caro&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#26102;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;Caro&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20808;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03237</link><description>&lt;p&gt;
&#32771;&#34385;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#39046;&#22495;&#22806;&#24847;&#22270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts. (arXiv:2305.03237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26694;&#26550;&#65288;Caro&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#26102;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;Caro&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#24847;&#22270;&#26816;&#27979;&#23545;&#20110;&#23454;&#29992;&#30340;&#23545;&#35805;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#65292;&#36890;&#24120;&#38656;&#35201;&#32771;&#34385;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#20165;&#38480;&#20110;&#21333;&#36718;&#23545;&#35805;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#65288;Caro&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36981;&#24490;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#20174;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#12290;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#26500;&#24314;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#20351;&#29992;&#22810;&#35270;&#22270;&#20449;&#24687;&#29942;&#39048;&#25439;&#22833;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22312;Caro&#20013;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#26469;&#20174;&#36825;&#20123;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#25366;&#25496;OOD&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#33258;&#20030;&#26041;&#27861;&#29992;&#36825;&#20123;OOD&#26679;&#26412;&#26469;&#35757;&#32451;&#29983;&#25104;&#30340;&#27169;&#22411;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Caro&#22312;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#30340;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20165;&#32771;&#34385;&#21333;&#36718;&#19978;&#19979;&#25991;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain (OOD) intent detection is vital for practical dialogue systems, and it usually requires considering multi-turn dialogue contexts. However, most previous OOD intent detection approaches are limited to single dialogue turns. In this paper, we introduce a context-aware OOD intent detection (Caro) framework to model multi-turn contexts in OOD intent detection tasks. Specifically, we follow the information bottleneck principle to extract robust representations from multi-turn dialogue contexts. Two different views are constructed for each input sample and the superfluous information not related to intent detection is removed using a multi-view information bottleneck loss. Moreover, we also explore utilizing unlabeled data in Caro. A two-stage training process is introduced to mine OOD samples from these unlabeled data, and these OOD samples are used to train the resulting model with a bootstrapping approach. Comprehensive experiments demonstrate that Caro establishes state-of-
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#39318;&#27425;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26041;&#27861;&#20998;&#25104;&#19977;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03236</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22522;&#20110;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#39318;&#27425;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26041;&#27861;&#20998;&#25104;&#19977;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22522;&#20110;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#26816;&#27979;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#20960;&#24180;&#21462;&#24471;&#20102;&#26497;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#39318;&#27425;&#32508;&#36848;&#20102;OOD&#26816;&#27979;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;OOD&#26816;&#27979;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#20960;&#20010;&#30456;&#20851;&#39046;&#22495;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26368;&#36817;&#30340;&#31639;&#27861;&#20998;&#25104;&#19977;&#31867;&#65306;&#65288;1&#65289;&#21487;&#29992;OOD&#25968;&#25454;&#65292;&#65288;2&#65289;OOD&#25968;&#25454;&#19981;&#21487;&#29992;+&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#26631;&#31614;&#21487;&#29992;&#65292;&#65288;3&#65289;OOD&#25968;&#25454;&#19981;&#21487;&#29992;+ID&#26631;&#31614;&#19981;&#21487;&#29992;&#12290;&#31532;&#19977;&#65292;&#20171;&#32461;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#24635;&#32467;&#29616;&#26377;&#24037;&#20316;&#24182;&#25552;&#20986;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#35838;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is essential for the reliable and safe deployment of machine learning systems in the real world. Great progress has been made over the past years. This paper presents the first review of recent advances in OOD detection with a particular focus on natural language processing approaches. First, we provide a formal definition of OOD detection and discuss several related fields. We then categorize recent algorithms into three classes according to the data they used: (1) OOD data available, (2) OOD data unavailable + in-distribution (ID) label available, and (3) OOD data unavailable + ID label unavailable. Third, we introduce datasets, applications, and metrics. Finally, we summarize existing work and present potential future research topics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#35843;&#33410;&#38376;&#25511;Transformer&#26550;&#26500;&#65292;&#36890;&#36807;&#20056;&#27861;&#25928;&#24212;&#23454;&#29616;&#20102;&#31070;&#32463;&#35843;&#33410;&#65292;&#22312;SuperGLUE&#22522;&#20934;&#39564;&#35777;&#38598;&#19978;&#34920;&#29616;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.03232</link><description>&lt;p&gt;
&#31070;&#32463;&#35843;&#33410;&#38376;&#25511;Transformer
&lt;/p&gt;
&lt;p&gt;
Neuromodulation Gated Transformer. (arXiv:2305.03232v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#35843;&#33410;&#38376;&#25511;Transformer&#26550;&#26500;&#65292;&#36890;&#36807;&#20056;&#27861;&#25928;&#24212;&#23454;&#29616;&#20102;&#31070;&#32463;&#35843;&#33410;&#65292;&#22312;SuperGLUE&#22522;&#20934;&#39564;&#35777;&#38598;&#19978;&#34920;&#29616;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#21363;&#31070;&#32463;&#35843;&#33410;&#38376;&#25511;Transformer&#65288;NGT&#65289;&#65292;&#23427;&#36890;&#36807;&#19968;&#31181;&#20056;&#27861;&#25928;&#24212;&#65292;&#23454;&#29616;&#20102;Transformer&#20013;&#30340;&#31070;&#32463;&#35843;&#33410;&#30340;&#31616;&#21333;&#23454;&#29616;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;SuperGLUE&#22522;&#20934;&#39564;&#35777;&#38598;&#19978;&#36798;&#21040;&#26368;&#20339;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel architecture, the Neuromodulation Gated Transformer (NGT), which is a simple implementation of neuromodulation in transformers via a multiplicative effect. We compare it to baselines and show that it results in the best average performance on the SuperGLUE benchmark validation sets.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AttentionViz&#65292;&#19968;&#31181;&#20197;&#32852;&#21512;&#23884;&#20837;&#20026;&#22522;&#30784;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#20840;&#23616;&#20998;&#26512;&#22810;&#20010;&#36755;&#20837;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#25552;&#39640;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#24182;&#36890;&#36807;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#21644;&#19987;&#23478;&#21453;&#39304;&#25552;&#20379;&#26032;&#30340;&#20132;&#20114;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.03210</link><description>&lt;p&gt;
AttentionViz&#65306;Transformer Attention&#30340;&#20840;&#23616;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
AttentionViz: A Global View of Transformer Attention. (arXiv:2305.03210v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AttentionViz&#65292;&#19968;&#31181;&#20197;&#32852;&#21512;&#23884;&#20837;&#20026;&#22522;&#30784;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#20840;&#23616;&#20998;&#26512;&#22810;&#20010;&#36755;&#20837;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#25552;&#39640;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#24182;&#36890;&#36807;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#21644;&#19987;&#23478;&#21453;&#39304;&#25552;&#20379;&#26032;&#30340;&#20132;&#20114;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#27491;&#22312;&#38761;&#26032;&#26426;&#22120;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#30340;&#20869;&#37096;&#36816;&#20316;&#20173;&#28982;&#31070;&#31192;&#33707;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#24207;&#21015;&#20013;&#20803;&#32032;&#20043;&#38388;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#21487;&#35270;&#21270;Transformer&#27169;&#22411;&#29992;&#20110;&#35745;&#31639;&#27880;&#24847;&#21147;&#30340;&#26597;&#35810;&#21644;&#38190;&#21521;&#37327;&#30340;&#32852;&#21512;&#23884;&#20837;&#12290;&#19982;&#20197;&#21069;&#30340;&#27880;&#24847;&#21147;&#21487;&#35270;&#21270;&#25216;&#26415;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#26512;&#22810;&#20010;&#36755;&#20837;&#24207;&#21015;&#30340;&#20840;&#23616;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20110;&#36825;&#20123;&#32852;&#21512;&#26597;&#35810;-&#38190;&#23884;&#20837;&#21019;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;AttentionViz&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#30740;&#31350;&#35821;&#35328;&#21644;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#36890;&#36807;&#20960;&#20010;&#24212;&#29992;&#22330;&#26223;&#21644;&#19987;&#23478;&#21453;&#39304;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#21644;&#25552;&#20379;&#26377;&#20851;&#26597;&#35810;-&#38190;&#20132;&#20114;&#30340;&#26032;&#35265;&#35299;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we present a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an interactive visualization tool, AttentionViz, based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving model understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21360;&#24230;&#35821;&#31995;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#35789;&#27719;&#20849;&#20139;&#38382;&#39064;&#65292;&#25506;&#32034;&#20102;&#25968;&#25454;&#37319;&#26679;&#21644;&#35789;&#27719;&#37327;&#20043;&#38388;&#30340;&#32763;&#35793;&#24615;&#33021;&#26435;&#34913;&#65292;&#20197;&#21450;&#23383;&#27597;&#36716;&#20889;&#26159;&#21542;&#26377;&#21161;&#20110;&#20419;&#36827;&#36328;&#33050;&#26412;&#27010;&#25324;&#65292;&#21457;&#29616;&#23383;&#27597;&#36716;&#20889;&#23545;&#36328;&#33050;&#26412;&#32763;&#35793;&#25928;&#26524;&#27809;&#26377;&#26126;&#26174;&#25913;&#36827;&#65292;&#23545;&#20110;&#36739;&#20302;&#36164;&#28304;&#30340;&#35821;&#35328;&#65292;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#35757;&#32451;&#22312;&#21407;&#22987;&#33050;&#26412;&#19978;&#20284;&#20046;&#24050;&#32463;&#33021;&#22815;&#25269;&#25239;&#33050;&#26412;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.03207</link><description>&lt;p&gt;
&#30740;&#31350;&#21360;&#24230;&#35821;&#31995;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#35789;&#27719;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Investigating Lexical Sharing in Multilingual Machine Translation for Indian Languages. (arXiv:2305.03207v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21360;&#24230;&#35821;&#31995;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#35789;&#27719;&#20849;&#20139;&#38382;&#39064;&#65292;&#25506;&#32034;&#20102;&#25968;&#25454;&#37319;&#26679;&#21644;&#35789;&#27719;&#37327;&#20043;&#38388;&#30340;&#32763;&#35793;&#24615;&#33021;&#26435;&#34913;&#65292;&#20197;&#21450;&#23383;&#27597;&#36716;&#20889;&#26159;&#21542;&#26377;&#21161;&#20110;&#20419;&#36827;&#36328;&#33050;&#26412;&#27010;&#25324;&#65292;&#21457;&#29616;&#23383;&#27597;&#36716;&#20889;&#23545;&#36328;&#33050;&#26412;&#32763;&#35793;&#25928;&#26524;&#27809;&#26377;&#26126;&#26174;&#25913;&#36827;&#65292;&#23545;&#20110;&#36739;&#20302;&#36164;&#28304;&#30340;&#35821;&#35328;&#65292;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#35757;&#32451;&#22312;&#21407;&#22987;&#33050;&#26412;&#19978;&#20284;&#20046;&#24050;&#32463;&#33021;&#22815;&#25269;&#25239;&#33050;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#65292;&#19968;&#20123;&#31574;&#30053;&#21253;&#25324;&#25913;&#36827;&#23383;&#31526;&#32454;&#20998;&#32780;&#19981;&#26159;&#23376;&#35789;&#21644;&#23545;&#23383;&#27597;&#36716;&#20889;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#21360;&#22320;&#35821;&#12289;&#21476;&#21513;&#25289;&#29305;&#35821;&#12289;&#23612;&#27850;&#23572;&#35821;&#21040;&#33521;&#35821;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#35789;&#27719;&#20849;&#20139;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#37319;&#26679;&#21644;&#35789;&#27719;&#37327;&#20043;&#38388;&#23384;&#22312;&#30340;&#32763;&#35793;&#24615;&#33021;&#26435;&#34913;&#65292;&#24182;&#25506;&#35752;&#20102;&#23383;&#27597;&#36716;&#20889;&#26159;&#21542;&#26377;&#21161;&#20110;&#20419;&#36827;&#36328;&#33050;&#26412;&#27010;&#25324;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;&#19981;&#21516;&#35774;&#32622;&#22914;&#20309;&#25512;&#24191;&#21040;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#65288;&#39532;&#25289;&#22320;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#23383;&#27597;&#36716;&#20889;&#27809;&#26377;&#26126;&#26174;&#30340;&#25913;&#36827;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#21407;&#22987;&#33050;&#26412;&#19978;&#35757;&#32451;&#26102;&#65292;&#21363;&#20351;&#23545;&#20110;&#30456;&#23545;&#20302;&#36164;&#28304;&#30340;&#35821;&#35328;&#65292;&#20284;&#20046;&#24050;&#32463;&#33021;&#22815;&#25269;&#25239;&#33050;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual language models have shown impressive cross-lingual transfer ability across a diverse set of languages and tasks. To improve the cross-lingual ability of these models, some strategies include transliteration and finer-grained segmentation into characters as opposed to subwords. In this work, we investigate lexical sharing in multilingual machine translation (MT) from Hindi, Gujarati, Nepali into English. We explore the trade-offs that exist in translation performance between data sampling and vocabulary size, and we explore whether transliteration is useful in encouraging cross-script generalisation. We also verify how the different settings generalise to unseen languages (Marathi and Bengali). We find that transliteration does not give pronounced improvements and our analysis suggests that our multilingual MT models trained on original scripts seem to already be robust to cross-script differences even for relatively low-resource languages
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35270;&#39057;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#23398;&#20064;&#35270;&#39057;&#29305;&#23450;&#30340;&#25216;&#33021;&#65292;&#21462;&#24471;&#20102;&#22235;&#39033;&#35270;&#39057;&#23383;&#24149;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#24320;&#25918;&#24335;&#35270;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.03204</link><description>&lt;p&gt;
VideoOFA&#65306;&#29992;&#20110;&#35270;&#39057;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation. (arXiv:2305.03204v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35270;&#39057;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#23398;&#20064;&#35270;&#39057;&#29305;&#23450;&#30340;&#25216;&#33021;&#65292;&#21462;&#24471;&#20102;&#22235;&#39033;&#35270;&#39057;&#23383;&#24149;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#24320;&#25918;&#24335;&#35270;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35270;&#39057;&#23383;&#24149;&#21644;&#35270;&#39057;&#38382;&#31572;&#31561;&#20219;&#21153;&#30340;&#35270;&#39057;&#21040;&#25991;&#26412;&#29983;&#25104;&#65306;&#39318;&#20808;&#65292;&#19968;&#20010;&#29983;&#25104;&#24335;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#19978;&#36827;&#34892;&#32852;&#21512;&#39044;&#35757;&#32451;&#65292;&#20197;&#23398;&#20064;&#22522;&#26412;&#30340;&#35270;&#35273;&#35821;&#35328;&#27010;&#24565;&#65292;&#28982;&#21518;&#22312;&#19968;&#20010;&#20013;&#38388;&#30340;&#35270;&#39057;-&#25991;&#26412;&#39044;&#35757;&#32451;&#38454;&#27573;&#23545;&#35270;&#39057;&#25968;&#25454;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#23398;&#20064;&#35270;&#39057;&#29305;&#23450;&#30340;&#25216;&#33021;&#65292;&#22914;&#26102;&#31354;&#25512;&#29702;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;VideoOFA&#27169;&#22411;&#22312;&#22235;&#20010;&#35270;&#39057;&#23383;&#24149;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;CIDEr&#20998;&#25968;&#19978;&#24179;&#22343;&#20987;&#36133;&#20102;&#20808;&#21069;&#30340;&#24037;&#33402;&#27700;&#24179;9.7&#20010;&#25910;&#30410;&#28857;&#12290;&#23427;&#36824;&#22312;&#20004;&#20010;&#24320;&#25918;&#24335;&#35270;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#23637;&#31034;&#20102;&#23427;&#20316;&#20026;&#36890;&#29992;&#35270;&#39057;&#21040;&#25991;&#26412;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new two-stage pre-training framework for video-to-text generation tasks such as video captioning and video question answering: A generative encoder-decoder model is first jointly pre-trained on massive image-text data to learn fundamental vision-language concepts, and then adapted to video data in an intermediate video-text pre-training stage to learn video-specific skills such as spatio-temporal reasoning. As a result, our VideoOFA model achieves new state-of-the-art performance on four Video Captioning benchmarks, beating prior art by an average of 9.7 points in CIDEr score. It also outperforms existing models on two open-ended Video Question Answering datasets, showcasing its generalization capability as a universal video-to-text model.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#30340;&#33258;&#21160;&#20998;&#31867;&#31995;&#32479;&#65292;&#26368;&#32456;&#36890;&#36807;&#24212;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#20998;&#31867;&#22120;&#33719;&#24471;&#20102;94&#65285;&#30340;&#24179;&#22343;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03201</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22686;&#24378;&#26222;&#20160;&#22270;&#35821;&#30340;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Enhancing Pashto Text Classification using Language Processing Techniques for Single And Multi-Label Analysis. (arXiv:2305.03201v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03201
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#30340;&#33258;&#21160;&#20998;&#31867;&#31995;&#32479;&#65292;&#26368;&#32456;&#36890;&#36807;&#24212;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#20998;&#31867;&#22120;&#33719;&#24471;&#20102;94&#65285;&#30340;&#24179;&#22343;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#24050;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23548;&#33268;&#20102;&#22823;&#37327;&#30740;&#31350;&#24320;&#21457;&#33258;&#21160;&#21270;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#20197;&#25903;&#25345;&#22269;&#20869;&#22806;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#26469;&#22788;&#29702;&#26412;&#22320;&#35821;&#35328;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#22823;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#30340;&#33258;&#21160;&#20998;&#31867;&#31995;&#32479;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#25991;&#26723;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#20102;&#21508;&#31181;&#27169;&#22411;&#65292;&#21253;&#25324;&#32479;&#35745;&#21644;&#31070;&#32463;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;DistilBERT-base-multilingual-cased&#12289;&#22810;&#23618;&#24863;&#30693;&#22120;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;K&#26368;&#36817;&#37051;&#12289;&#20915;&#31574;&#26641;&#12289;&#39640;&#26031;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#36923;&#36753;&#22238;&#24402;&#65292;&#26469;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#21363;&#35789;&#34955;&#21644;&#35789;&#39057;&#36870;&#21521;&#25991;&#26723;&#39057;&#29575;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#20998;&#31867;&#22120;&#65292;&#27979;&#35797;&#20934;&#30830;&#29575;&#24179;&#22343;&#20540;&#36798;&#21040;&#20102;94&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification has become a crucial task in various fields, leading to a significant amount of research on developing automated text classification systems for national and international languages. However, there is a growing need for automated text classification systems that can handle local languages. This study aims to establish an automated classification system for Pashto text. To achieve this goal, we constructed a dataset of Pashto documents and applied various models, including statistical and neural machine learning models such as DistilBERT-base-multilingual-cased, Multilayer Perceptron, Support Vector Machine, K Nearest Neighbor, decision tree, Gaussian na\"ive Bayes, multinomial na\"ive Bayes, random forest, and logistic regression, to identify the most effective approach. We also evaluated two different feature extraction methods, bag of words and Term Frequency Inverse Document Frequency. The study achieved an average testing accuracy rate of 94% using the MLP class
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#23545;Dari&#35821;&#38899;&#20013;&#30340;&#21333;&#35789;&#36827;&#34892;&#35782;&#21035;&#65292;&#22312;&#23396;&#31435;Dari&#35789;&#35821;&#35821;&#26009;&#24211;&#19978;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.03200</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;Dari&#35821;&#35328;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Employing Hybrid Deep Neural Networks on Dari Speech. (arXiv:2305.03200v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#23545;Dari&#35821;&#38899;&#20013;&#30340;&#21333;&#35789;&#36827;&#34892;&#35782;&#21035;&#65292;&#22312;&#23396;&#31435;Dari&#35789;&#35821;&#35821;&#26009;&#24211;&#19978;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#25105;&#20204;&#20043;&#21069;&#20250;&#35758;&#35770;&#25991;&#30340;&#24310;&#20280;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#20110;&#24320;&#21457;&#21644;&#25913;&#36827;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20197;&#20419;&#36827;&#21644;&#22686;&#24378;&#20154;&#26426;&#20132;&#20114;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#22914;&#20170;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#24050;&#26222;&#21450;&#65292;&#24182;&#22312;&#28216;&#25103;&#12289;&#32763;&#35793;&#31995;&#32479;&#12289;&#26426;&#22120;&#20154;&#31561;&#26041;&#38754;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20173;&#38656;&#35201;&#22823;&#37327;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Mel&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65288;MFCCs&#65289;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#65292;&#20197;&#21450;&#20004;&#31181;&#32467;&#21512;CNN&#21644;RNN&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#26469;&#35782;&#21035;Dari&#35821;&#35328;&#20013;&#30340;&#21333;&#35789;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#24049;&#21019;&#24314;&#30340;&#23396;&#31435;Dari&#35789;&#35821;&#35821;&#26009;&#24211;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;20&#20010;&#30701;&#35821;Dari&#35789;&#27719;&#30340;1000&#20010;&#35805;&#35821;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21462;&#24471;&#20102;&#26497;&#39640;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is an extension of our previous conference paper. In recent years, there has been a growing interest among researchers in developing and improving speech recognition systems to facilitate and enhance human-computer interaction. Today, Automatic Speech Recognition (ASR) systems have become ubiquitous, used in everything from games to translation systems, robots, and more. However, much research is still needed on speech recognition systems for low-resource languages. This article focuses on the recognition of individual words in the Dari language using the Mel-frequency cepstral coefficients (MFCCs) feature extraction method and three different deep neural network models: Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and Multilayer Perceptron (MLP), as well as two hybrid models combining CNN and RNN. We evaluate these models using an isolated Dari word corpus that we have created, consisting of 1000 utterances for 20 short Dari terms. Our study achieved 
&lt;/p&gt;</description></item><item><title>GPT-4&#26159;OpenAI&#24320;&#21457;&#30340;&#31532;&#22235;&#20195;GPT&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#36229;&#36807;1&#19975;&#20159;&#30340;&#27169;&#22411;&#35268;&#27169;&#12289;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#25913;&#36827;&#30340;&#35821;&#22659;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#26377;&#26395;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#20010;&#20154;&#21161;&#29702;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#38382;&#31572;&#31995;&#32479;&#65292;&#20294;&#20063;&#23384;&#22312;&#35745;&#31639;&#35201;&#27714;&#12289;&#25968;&#25454;&#35201;&#27714;&#21644;&#36947;&#24503;&#38382;&#39064;&#31561;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.03195</link><description>&lt;p&gt;
Gpt-4&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#36827;&#23637;&#21644;&#26426;&#36935;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Gpt-4: A Review on Advancements and Opportunities in Natural Language Processing. (arXiv:2305.03195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03195
&lt;/p&gt;
&lt;p&gt;
GPT-4&#26159;OpenAI&#24320;&#21457;&#30340;&#31532;&#22235;&#20195;GPT&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#36229;&#36807;1&#19975;&#20159;&#30340;&#27169;&#22411;&#35268;&#27169;&#12289;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#25913;&#36827;&#30340;&#35821;&#22659;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#26377;&#26395;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#20010;&#20154;&#21161;&#29702;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#38382;&#31572;&#31995;&#32479;&#65292;&#20294;&#20063;&#23384;&#22312;&#35745;&#31639;&#35201;&#27714;&#12289;&#25968;&#25454;&#35201;&#27714;&#21644;&#36947;&#24503;&#38382;&#39064;&#31561;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#8221;&#65288;GPT&#65289;&#31995;&#21015;&#30340;&#31532;&#22235;&#20195;&#35821;&#35328;&#27169;&#22411;GPT-4&#30001;OpenAI&#24320;&#21457;&#65292;&#26377;&#26395;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23454;&#29616;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#35752;&#35770;&#20102;GPT-4&#30340;&#29305;&#28857;&#12289;&#28508;&#22312;&#24212;&#29992;&#20197;&#21450;&#21487;&#33021;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;GPT-4&#19982;&#20854;&#21069;&#36523;GPT-3&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30456;&#27604;GPT-3&#65292;GPT-4&#20855;&#26377;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#65288;&#36229;&#36807;1&#19975;&#20159;&#65289;&#12289;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12289;&#25913;&#36827;&#30340;&#35821;&#22659;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;GPT-4&#30340;&#19968;&#20123;&#28508;&#22312;&#24212;&#29992;&#21253;&#25324;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#20010;&#20154;&#21161;&#29702;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#38382;&#31572;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;GPT-4&#20063;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#22914;&#35745;&#31639;&#35201;&#27714;&#12289;&#25968;&#25454;&#35201;&#27714;&#21644;&#36947;&#24503;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer 4 (GPT-4) is the fourth-generation language model in the GPT series, developed by OpenAI, which promises significant advancements in the field of natural language processing (NLP). In this research article, we have discussed the features of GPT-4, its potential applications, and the challenges that it might face. We have also compared GPT-4 with its predecessor, GPT-3. GPT-4 has a larger model size (more than one trillion), better multilingual capabilities, improved contextual understanding, and reasoning capabilities than GPT-3. Some of the potential applications of GPT-4 include chatbots, personal assistants, language translation, text summarization, and question-answering. However, GPT-4 poses several challenges and limitations such as computational requirements, data requirements, and ethical concerns.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#21464;&#37327;&#65292;&#20197;&#20415;&#20415;&#20110;&#21435;&#36523;&#20221;&#21270;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#25968;&#25454;&#38598;PHI&#23383;&#27573;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03169</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#25935;&#24863;&#25968;&#25454;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sensitive Data Detection with High-Throughput Machine Learning Models in Electrical Health Records. (arXiv:2305.03169v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#21464;&#37327;&#65292;&#20197;&#20415;&#20415;&#20110;&#21435;&#36523;&#20221;&#21270;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#25968;&#25454;&#38598;PHI&#23383;&#27573;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#12289;&#31038;&#21306;&#21644;&#30740;&#31350;&#20154;&#21592;&#38656;&#35201;&#20998;&#20139;&#25968;&#25454;&#24182;&#21512;&#20316;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#12289;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#25512;&#36827;&#30740;&#31350;&#12290;1996&#24180;&#12298;&#20581;&#24247;&#20445;&#38505;&#27969;&#36890;&#19982;&#36131;&#20219;&#27861;&#26696;&#12299;(HIPAA)&#26159;&#19968;&#39033;&#32852;&#37030;&#27861;&#24459;&#65292;&#26088;&#22312;&#36890;&#36807;&#21046;&#23450;&#20445;&#25252;&#20581;&#24247;&#20449;&#24687;&#30340;&#35268;&#23450;&#26469;&#20445;&#25252;&#25935;&#24863;&#20581;&#24247;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#20849;&#20139;&#20043;&#21069;&#65292;HIPAA&#27809;&#26377;&#25552;&#20379;&#26377;&#25928;&#30340;&#26816;&#27979;&#25110;&#21024;&#38500;PHI&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#21464;&#37327;&#65292;&#20174;&#32780;&#20415;&#20110;&#21435;&#36523;&#20221;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data, there is an increasing need for healthcare providers, communities, and researchers to share data and collaborate to improve health outcomes, generate valuable insights, and advance research. The Health Insurance Portability and Accountability Act of 1996 (HIPAA) is a federal law designed to protect sensitive health information by defining regulations for protected health information (PHI). However, it does not provide efficient tools for detecting or removing PHI before data sharing. One of the challenges in this area of research is the heterogeneous nature of PHI fields in data across different parties. This variability makes rule-based sensitive variable identification systems that work on one database fail on another. To address this issue, our paper explores the use of machine learning algorithms to identify sensitive variables in structured data, thus facilitating the de-identification process. We made a key observation that the distributions of metadata of
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#23545;&#32858;&#31867;&#31639;&#27861;&#65288;KMeans&#12289;&#21333;&#38142;&#25509;&#32858;&#21512;&#31561;&#32423;&#12289;DBSCAN&#21644;HDBSCAN&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24212;&#29992;&#20110;&#35780;&#35770;&#32858;&#31867;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.03144</link><description>&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#23545;NLP&#32858;&#31867;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influence of various text embeddings on clustering performance in NLP. (arXiv:2305.03144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03144
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#23545;&#32858;&#31867;&#31639;&#27861;&#65288;KMeans&#12289;&#21333;&#38142;&#25509;&#32858;&#21512;&#31561;&#32423;&#12289;DBSCAN&#21644;HDBSCAN&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24212;&#29992;&#20110;&#35780;&#35770;&#32858;&#31867;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#30340;&#20986;&#29616;&#65292;&#35780;&#35770;&#23545;&#20110;&#39038;&#23458;&#35780;&#20272;&#20135;&#21697;&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#26143;&#32423;&#35780;&#20998;&#24182;&#19981;&#24635;&#26159;&#19982;&#39038;&#23458;&#32534;&#20889;&#30340;&#35780;&#35770;&#25991;&#26412;&#30456;&#21305;&#37197;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36873;&#25321;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#26469;&#34920;&#31034;&#36825;&#20123;&#35780;&#35770;&#30340;&#20219;&#21153;&#65292;&#24182;&#25506;&#31350;&#20102;&#23884;&#20837;&#36873;&#25321;&#23545;&#21508;&#31181;&#31867;&#22411;&#32858;&#31867;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#65288;BERT&#65289;&#21644;&#38750;&#19978;&#19979;&#25991;&#65288;Word2Vec&#65289;&#25991;&#26412;&#23884;&#20837;&#26469;&#34920;&#31034;&#25991;&#26412;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#23545;&#19977;&#31181;&#32858;&#31867;&#31639;&#27861;&#65288;&#22522;&#20110;&#20998;&#21306;&#30340;KMeans&#12289;&#21333;&#38142;&#25509;&#32858;&#21512;&#31561;&#32423;&#21644;&#23494;&#24230;&#22522;&#30784;&#30340;DBSCAN&#21644;HDBSCAN&#65289;&#22312;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#19979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of e-commerce platforms, reviews are crucial for customers to assess the credibility of a product. The star ratings do not always match the review text written by the customer. For example, a three star rating (out of five) may be incongruous with the review text, which may be more suitable for a five star review. A clustering approach can be used to relabel the correct star ratings by grouping the text reviews into individual groups. In this work, we explore the task of choosing different text embeddings to represent these reviews and also explore the impact the embedding choice has on the performance of various classes of clustering algorithms. We use contextual (BERT) and non-contextual (Word2Vec) text embeddings to represent the text and measure their impact of three classes on clustering algorithms - partitioning based (KMeans), single linkage agglomerative hierarchical, and density based (DBSCAN and HDBSCAN), each with various experimental settings. We use the sil
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#25506;&#35752;&#20102;&#20840;&#23616;&#25991;&#26723;&#19978;&#19979;&#25991;&#19982;&#23616;&#37096;&#19978;&#19979;&#25991;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#27491;&#30830;&#26816;&#32034;&#20840;&#23616;&#25991;&#26723;&#19978;&#19979;&#25991;&#23545;&#25552;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.03132</link><description>&lt;p&gt;
&#20840;&#23616;&#21644;&#23616;&#37096;&#25991;&#33033;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Role of Global and Local Context in Named Entity Recognition. (arXiv:2305.03132v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03132
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#25506;&#35752;&#20102;&#20840;&#23616;&#25991;&#26723;&#19978;&#19979;&#25991;&#19982;&#23616;&#37096;&#19978;&#19979;&#25991;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#27491;&#30830;&#26816;&#32034;&#20840;&#23616;&#25991;&#26723;&#19978;&#19979;&#25991;&#23545;&#25552;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#30001;&#20110;&#20854;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#19981;&#33021;&#19968;&#27425;&#22788;&#29702;&#38271;&#25991;&#26723;&#65292;&#22240;&#27492;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26159;&#25353;&#39034;&#24207;&#24212;&#29992;&#30340;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#24184;&#22320;&#21482;&#21253;&#21547;&#23616;&#37096;&#19978;&#19979;&#25991;&#65292;&#24182;&#38459;&#30861;&#20102;&#21033;&#29992;&#20840;&#23616;&#25991;&#26723;&#19978;&#19979;&#25991;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#22952;&#30861;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20840;&#23616;&#25991;&#26723;&#19978;&#19979;&#25991;&#30340;&#24433;&#21709;&#21450;&#20854;&#19982;&#23616;&#37096;&#19978;&#19979;&#25991;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27491;&#30830;&#26816;&#32034;&#20840;&#23616;&#25991;&#26723;&#19978;&#19979;&#25991;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#22823;&#20110;&#20165;&#21033;&#29992;&#23616;&#37096;&#25991;&#26412;&#12290;&#36825;&#20419;&#20351;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#26356;&#22909;&#22320;&#26816;&#32034;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformer-based models have recently shown great performance when applied to Named Entity Recognition (NER). As the complexity of their self-attention mechanism prevents them from processing long documents at once, these models are usually applied in a sequential fashion. Such an approach unfortunately only incorporates local context and prevents leveraging global document context in long documents such as novels, which might hinder performance. In this article, we explore the impact of global document context, and its relationships with local context. We find that correctly retrieving global document context has a greater impact on performance than only leveraging local context, prompting for further research on how to better retrieve that context.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26816;&#32034;&#22120;&#21487;&#20197;&#22312;&#25968;&#25454;&#38598;&#38388;&#37325;&#22797;&#20351;&#29992;&#65292;&#25903;&#25345;&#38024;&#23545;&#30446;&#26631;&#39046;&#22495;&#30340;&#28789;&#27963;&#25216;&#33021;&#37197;&#32622;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22810;&#20010; ODQA &#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#39062;&#30340;&#24494;&#35843;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03130</link><description>&lt;p&gt;
Chain-of-Skills: &#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Skills: A Configurable Model for Open-domain Question Answering. (arXiv:2305.03130v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26816;&#32034;&#22120;&#21487;&#20197;&#22312;&#25968;&#25454;&#38598;&#38388;&#37325;&#22797;&#20351;&#29992;&#65292;&#25903;&#25345;&#38024;&#23545;&#30446;&#26631;&#39046;&#22495;&#30340;&#28789;&#27963;&#25216;&#33021;&#37197;&#32622;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22810;&#20010; ODQA &#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#39062;&#30340;&#24494;&#35843;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#20013;&#65292;&#22914;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#65292;&#26816;&#32034;&#27169;&#22411;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#20214;&#12290;&#30001;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26377;&#30528;&#19981;&#21516;&#30340;&#26816;&#32034;&#25216;&#33021;&#65292;&#36817;&#26399;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#23450;&#21046;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26816;&#32034;&#22120;&#65292;&#20854;&#20013;&#21508;&#20010;&#27169;&#22359;&#23545;&#24212;&#20110;&#21487;&#20197;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#37325;&#22797;&#20351;&#29992;&#30340;&#20851;&#38190;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25903;&#25345;&#22522;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#28789;&#27963;&#25216;&#33021;&#37197;&#32622;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#36731;&#20219;&#21153;&#24178;&#25200;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21463;&#31232;&#30095; Transformer &#21551;&#21457;&#30340;&#26032;&#22411;&#27169;&#22359;&#21270;&#21442;&#25968;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#32500;&#22522;&#30334;&#31185;&#19978;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#20010; ODQA &#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20855;&#26377;&#22810;&#20219;&#21153;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38646;&#26679;&#20363;&#35780;&#20272;&#20013;&#20248;&#20110;&#26368;&#36817;&#30340;&#33258;&#25105;&#30417;&#30563;&#26816;&#32034;&#22120;&#65292;&#24182;&#22312; NQ&#12289;HotpotQA &#21644; OTT-QA &#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transferability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.03123</link><description>&lt;p&gt;
ChatGPT &#38656;&#35201;&#36827;&#34892;SPADE&#65288;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65289;&#35780;&#20272;&#65306;&#19968;&#39033;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#21478;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#24615;&#33021;&#21644;&#26377;&#25928;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#22312;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#20013;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21457;&#34920;&#65292;&#20197;&#23637;&#31034;ChatGPT&#21644;&#20854;&#20182;LLMs&#30340;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#12289;&#38598;&#25104;&#21644;&#24773;&#24863;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#22823;&#22810;&#25968;&#34987;&#24573;&#35270;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65292;&#24182;&#24314;&#35758;&#19981;&#20165;&#20165;&#26159;ChatGPT&#65292;&#32780;&#26159;&#22312;&#23545;&#35805;&#26426;&#22120;&#20154;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#21518;&#32493;&#20837;&#21475;&#37117;&#24212;&#35813;&#36827;&#34892;SPADE&#35780;&#20272;&#12290;&#26412;&#25991;&#35814;&#32454;&#35752;&#35770;&#20102;&#20851;&#20110;ChatGPT&#30340;&#38382;&#39064;&#21644;&#20851;&#27880;&#28857;&#19982;&#19978;&#36848;&#29305;&#24449;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#21021;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21487;&#35270;&#21270;&#20197;&#21450;&#20551;&#35774;&#30340;&#20107;&#23454;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#20026;&#27599;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21464;&#25442;&#22120;&#21644;AED&#26694;&#26550;&#30340;&#26032;&#26041;&#26696;&#65292;&#21033;&#29992;AED&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21464;&#25442;&#22120;&#30340;&#27969;&#24335;&#23646;&#24615;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03101</link><description>&lt;p&gt;
&#28151;&#21512;&#21464;&#25442;&#22120;&#21644;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks. (arXiv:2305.03101v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21464;&#25442;&#22120;&#21644;AED&#26694;&#26550;&#30340;&#26032;&#26041;&#26696;&#65292;&#21033;&#29992;AED&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21464;&#25442;&#22120;&#30340;&#27969;&#24335;&#23646;&#24615;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#25442;&#22120;&#21644;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;AED&#65289;&#26159;&#29992;&#20110;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#30340;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290; &#23427;&#20204;&#38024;&#23545;&#19981;&#21516;&#30340;&#30446;&#30340;&#36827;&#34892;&#35774;&#35745;&#65292;&#27599;&#31181;&#26041;&#27861;&#22312;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#37117;&#26377;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#20026;&#20102;&#21033;&#29992;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#21464;&#25442;&#22120;&#21644;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;TAED&#65289;&#22788;&#29702;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26032;&#26041;&#27861;&#21033;&#29992;&#20102;AED&#20013;&#20851;&#20110;&#38750;&#21333;&#35843;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21464;&#25442;&#22120;&#30340;&#27969;&#24335;&#23646;&#24615;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#21464;&#25442;&#22120;&#21644;AED&#20849;&#20139;&#21516;&#19968;&#20010;&#35821;&#38899;&#32534;&#30721;&#22120;&#12290;&#21464;&#25442;&#22120;&#20013;&#30340;&#39044;&#27979;&#22120;&#34987;AED&#27169;&#22411;&#20013;&#30340;&#35299;&#30721;&#22120;&#25152;&#21462;&#20195;&#65292;&#35299;&#30721;&#22120;&#30340;&#36755;&#20986;&#26159;&#22522;&#20110;&#35821;&#38899;&#36755;&#20837;&#32780;&#19981;&#26159;&#22522;&#20110;&#26080;&#26465;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#30830;&#20445;&#20102;&#36890;&#36807;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#30340;&#35835;&#21462;/&#20889;&#20837;&#22330;&#26223;&#26469;&#20248;&#21270;&#27169;&#22411;&#65292;&#24182;&#20026;&#27969;&#24335;&#24212;&#29992;&#31243;&#24207;&#21019;&#36896;&#20102;&#21305;&#37197;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transducer and Attention based Encoder-Decoder (AED) are two widely used frameworks for speech-to-text tasks. They are designed for different purposes and each has its own benefits and drawbacks for speech-to-text tasks. In order to leverage strengths of both modeling methods, we propose a solution by combining Transducer and Attention based Encoder-Decoder (TAED) for speech-to-text tasks. The new method leverages AED's strength in non-monotonic sequence to sequence learning while retaining Transducer's streaming property. In the proposed framework, Transducer and AED share the same speech encoder. The predictor in Transducer is replaced by the decoder in the AED model, and the outputs of the decoder are conditioned on the speech inputs instead of outputs from an unconditioned language model. The proposed solution ensures that the model is optimized by covering all possible read/write scenarios and creates a matched environment for streaming applications. We evaluate the proposed appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#20998;&#31867;&#22120;&#26469;&#24555;&#36895;&#36873;&#25321;&#26368;&#20339;&#30340;&#30456;&#20851;&#25991;&#26723;&#35821;&#26009;&#24211;&#36827;&#34892;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#25512;&#25991;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#34892;&#22312;&#32447;&#28165;&#27905;&#33021;&#28304;&#24773;&#24863;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.03092</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#31867;&#22120;&#26469;&#31579;&#36873;&#35821;&#26009;&#24211;&#65306;&#20197;&#22312;&#32447;&#28165;&#27905;&#33021;&#28304;&#24773;&#24863;&#20998;&#26512;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Curating corpora with classifiers: A case study of clean energy sentiment online. (arXiv:2305.03092v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#20998;&#31867;&#22120;&#26469;&#24555;&#36895;&#36873;&#25321;&#26368;&#20339;&#30340;&#30456;&#20851;&#25991;&#26723;&#35821;&#26009;&#24211;&#36827;&#34892;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#25512;&#25991;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#34892;&#22312;&#32447;&#28165;&#27905;&#33021;&#28304;&#24773;&#24863;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#24515;&#31574;&#21010;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#35821;&#26009;&#24211;&#26159;&#34917;&#20805;&#20256;&#32479;&#35843;&#26597;&#30340;&#26367;&#20195;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#20197;&#25552;&#20379;&#24191;&#27867;&#30340;&#20844;&#20247;&#24847;&#35265;&#12290;&#34429;&#28982;&#35843;&#26597;&#22312;&#25910;&#38598;&#20195;&#34920;&#24615;&#26679;&#26412;&#21644;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#26041;&#38754;&#24456;&#26377;&#25928;&#65292;&#20294;&#36816;&#34892;&#25104;&#26412;&#24456;&#39640;&#65292;&#32780;&#19988;&#20250;&#28382;&#21518;&#20110;&#20844;&#20247;&#24847;&#35265;&#25968;&#22825;&#25110;&#25968;&#21608;&#12290;&#36825;&#20004;&#20010;&#32570;&#28857;&#21487;&#20197;&#36890;&#36807;&#23454;&#26102;&#12289;&#39640;&#23481;&#37327;&#30340;&#25968;&#25454;&#27969;&#21644;&#24555;&#36895;&#30340;&#20998;&#26512;&#31649;&#36947;&#20811;&#26381;&#12290;&#22312;&#32452;&#32455;&#36825;&#26679;&#30340;&#25968;&#25454;&#31649;&#36947;&#26041;&#38754;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24555;&#36895;&#36873;&#25321;&#26368;&#20339;&#30340;&#30456;&#20851;&#25991;&#26723;&#35821;&#26009;&#24211;&#36827;&#34892;&#20998;&#26512;&#12290;&#20165;&#20165;&#36890;&#36807;&#20851;&#38190;&#35789;&#26597;&#35810;&#24448;&#24448;&#20250;&#21253;&#25324;&#19981;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#32780;&#36825;&#20123;&#25991;&#26723;&#24456;&#38590;&#29992;&#35789;&#34955;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#28040;&#27495;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25163;&#21160;&#26631;&#27880;&#30340;&#25512;&#25991;&#19978;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#25506;&#32034;&#20102;&#35821;&#26009;&#24211;&#31574;&#21010;&#30340;&#26041;&#27861;&#65292;&#20197;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;&#39640;&#36798;0.8&#20197;&#19978;&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Well curated, large-scale corpora of social media posts containing broad public opinion offer an alternative data source to complement traditional surveys. While surveys are effective at collecting representative samples and are capable of achieving high accuracy, they can be both expensive to run and lag public opinion by days or weeks. Both of these drawbacks could be overcome with a real-time, high volume data stream and fast analysis pipeline. A central challenge in orchestrating such a data pipeline is devising an effective method for rapidly selecting the best corpus of relevant documents for analysis. Querying with keywords alone often includes irrelevant documents that are not easily disambiguated with bag-of-words natural language processing methods. Here, we explore methods of corpus curation to filter irrelevant tweets using pre-trained transformer-based models, fine-tuned for our binary classification task on hand-labeled tweets. We are able to achieve F1 scores of up to 0.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20849;&#21516;&#24314;&#27169;&#8220;&#35201;&#38382;&#20160;&#20040;&#8221;&#21644;&#8220;&#22914;&#20309;&#38382;&#8221;&#30340;&#38382;&#39064;&#65292;&#22312;&#19981;&#26126;&#30830;&#31572;&#26696;&#30340;&#35774;&#32622;&#20013;&#38750;&#24120;&#23454;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.03088</link><description>&lt;p&gt;
&#38754;&#21521;&#26080;&#31572;&#26696;&#30340;&#20250;&#35805;&#38382;&#31572;&#29983;&#25104;&#30340;&#8220;&#35201;&#38382;&#20160;&#20040;&#8221;&#21644;&#8220;&#22914;&#20309;&#38382;&#8221;&#30340;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation. (arXiv:2305.03088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20849;&#21516;&#24314;&#27169;&#8220;&#35201;&#38382;&#20160;&#20040;&#8221;&#21644;&#8220;&#22914;&#20309;&#38382;&#8221;&#30340;&#38382;&#39064;&#65292;&#22312;&#19981;&#26126;&#30830;&#31572;&#26696;&#30340;&#35774;&#32622;&#20013;&#38750;&#24120;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#38382;&#31572;&#29983;&#25104;&#65288;CQG&#65289;&#26159;&#26426;&#22120;&#36890;&#36807;&#23545;&#35805;&#24110;&#21161;&#20154;&#31867;&#28385;&#36275;&#20449;&#24687;&#38656;&#27714;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#19968;&#33324;&#20998;&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#26126;&#30830;&#31572;&#26696;&#21644;&#19981;&#26126;&#30830;&#31572;&#26696;&#12290;&#34429;&#28982;&#21069;&#32773;&#36890;&#36807;&#26292;&#38706;&#39044;&#26399;&#31572;&#26696;&#26469;&#24110;&#21161;&#27169;&#22411;&#65292;&#20294;&#21518;&#32773;&#26356;&#21152;&#29616;&#23454;&#24182;&#26368;&#36817;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#19981;&#26126;&#30830;&#31572;&#26696;&#30340;&#35774;&#32622;&#20013;&#65292;&#8220;&#35201;&#38382;&#20160;&#20040;&#8221;&#21644;&#8220;&#22914;&#20309;&#38382;&#8221;&#26159;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#23558;&#19978;&#19979;&#25991;&#20013;&#30340;&#36830;&#32493;&#21477;&#23376;&#20316;&#20026;&#29702;&#30001;&#36873;&#25321;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20351;&#29992;&#36825;&#31181;&#26420;&#32032;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29983;&#25104;&#30340;&#23545;&#35805;&#21487;&#33021;&#19981;&#22815;&#33258;&#28982;&#65292;&#22240;&#20026;&#22312;&#29616;&#23454;&#20013;&#65292;&#23545;&#35805;&#32773;&#32463;&#24120;&#35848;&#35770;&#30456;&#20851;&#30340;&#20869;&#23481;&#65292;&#36825;&#20123;&#20869;&#23481;&#19981;&#19968;&#23450;&#26159;&#36830;&#32493;&#30340;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#38544;&#21547;&#20915;&#23450;&#35201;&#29983;&#25104;&#30340;&#38382;&#39064;&#31867;&#22411;&#65288;&#24067;&#23572;/&#36328;&#24230;&#65289;&#65292;&#26174;&#24335;&#24314;&#27169;&#38382;&#39064;&#31867;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#31572;&#26696;&#65288;&#25552;&#31034;&#27169;&#22411;&#29983;&#25104;&#26576;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65289;&#22312;&#26080;&#31572;&#26696;&#35774;&#32622;&#20013;&#19981;&#21487;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#31471;&#23545;&#31471;&#30340;&#26041;&#24335;&#20849;&#21516;&#27169;&#25311;&#8220;&#35201;&#38382;&#20160;&#20040;&#8221;&#21644;&#8220;&#22914;&#20309;&#38382;&#8221;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#30001;&#36873;&#25321;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#21644;&#20505;&#36873;&#21477;&#23376;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#26356;&#33258;&#28982;&#30340;&#23545;&#35805;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#38382;&#39064;&#31867;&#22411;&#39044;&#27979;&#22120;&#26469;&#26126;&#30830;&#27169;&#22411;&#20013;&#30340;&#30446;&#26631;&#38382;&#39064;&#31867;&#22411;&#12290;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational Question Generation (CQG) is a critical task for machines to assist humans in fulfilling their information needs through conversations. The task is generally cast into two different settings: answer-aware and answer-unaware. While the former facilitates the models by exposing the expected answer, the latter is more realistic and receiving growing attentions recently. What-to-ask and how-to-ask are the two main challenges in the answer-unaware setting. To address the first challenge, existing methods mainly select sequential sentences in context as the rationales. We argue that the conversation generated using such naive heuristics may not be natural enough as in reality, the interlocutors often talk about the relevant contents that are not necessarily sequential in context. Additionally, previous methods decide the type of question to be generated (boolean/span-based) implicitly. Modeling the question type explicitly is crucial as the answer, which hints the models to ge
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.02783</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25216;&#26415;&#20219;&#21153;&#20013;&#33258;&#21160;&#29983;&#25104;YAML&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#22312;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#26041;&#38754;&#30340;&#21463;&#30410;&#26368;&#22823;&#65292;&#32780;&#38024;&#23545;IT&#33258;&#21160;&#21270;&#31561;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;Ansible-YAML&#30340;&#29983;&#25104;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#21253;&#21547;Ansible-YAML&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25193;&#23637;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20004;&#20010;&#29992;&#20110;&#25429;&#25417;&#27492;&#39046;&#22495;&#29305;&#24449;&#30340;YAML&#21644;Ansible&#24615;&#33021;&#25351;&#26631;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Ansible Wisdom&#21487;&#20197;&#31934;&#30830;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#24182;&#19988;&#20854;&#24615;&#33021;&#21487;&#19982;&#29616;&#26377;&#25216;&#26415;&#30340;&#29366;&#24577;&#30456;&#23218;&#32654;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code generation tool, aimed at improving IT automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-YAML. We also develop two novel performance metrics for YAML and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#25506;&#31350;&#20102;&#22522;&#20110;&#36716;&#31227;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#21033;&#29992;&#22312;&#23494;&#20999;&#30456;&#20851;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35780;&#20272;&#33719;&#21462;&#31574;&#30053;&#26469;&#35299;&#20915;&#20849;&#25391;&#26816;&#27979;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;PRC&#30340;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#25351;&#23548;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.02459</link><description>&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#19982;&#20027;&#21160;&#23398;&#20064;&#29992;&#20110;&#20849;&#40483;&#26816;&#27979;&#65306;&#35299;&#20915;&#31232;&#26377;&#31867;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge. (arXiv:2305.02459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#25506;&#31350;&#20102;&#22522;&#20110;&#36716;&#31227;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#21033;&#29992;&#22312;&#23494;&#20999;&#30456;&#20851;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35780;&#20272;&#33719;&#21462;&#31574;&#30053;&#26469;&#35299;&#20915;&#20849;&#25391;&#26816;&#27979;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;PRC&#30340;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#25351;&#23548;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#31995;&#32479;&#20351;&#24471;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#26679;&#20363;&#33021;&#22815;&#24471;&#21040;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23545;&#20110;&#31232;&#26377;&#31867;&#20219;&#21153;&#65288;&#21363;&#31867;&#21035;&#26631;&#31614;&#38750;&#24120;&#23569;&#35265;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&lt;5%&#30340;&#26679;&#26412;&#65289;&#65292;&#25968;&#25454;&#37319;&#38598;&#38556;&#30861;&#20173;&#28982;&#23384;&#22312;&#12290;&#20027;&#21160;&#23398;&#20064;&#19968;&#33324;&#34987;&#25552;&#20986;&#29992;&#20110;&#32531;&#35299;&#36825;&#31181;&#25361;&#25112;&#65292;&#20294;&#36873;&#25321;&#31574;&#30053;&#65292;&#21363;&#36873;&#25321;&#31232;&#26377;&#31867;&#31034;&#20363;&#30340;&#26631;&#20934;&#65292;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#21464;&#21387;&#22120;&#21487;&#20197;&#23454;&#29616;&#36845;&#20195;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#36716;&#31227;&#21644;&#20027;&#21160;&#23398;&#20064;&#35299;&#20915;&#20102;&#36890;&#36807;&#21033;&#29992;&#22312;&#23494;&#20999;&#30456;&#20851;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35780;&#20272;&#33719;&#21462;&#31574;&#30053;&#26469;&#35299;&#20915;&#20849;&#25391;&#26816;&#27979;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#25552;&#20986;&#30340;&#31232;&#26377;&#31867;&#27010;&#29575;&#65288;PRC&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#38024;&#23545;&#29305;&#23450;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65288;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25910;&#38598;&#35748;&#30693;&#20849;&#25391;&#30340;&#35821;&#35328;&#26679;&#26412;&#65289;&#36827;&#34892;&#20102;&#36825;&#20123;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;PRC&#26159;&#25351;&#23548;&#27880;&#37322;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#26368;&#32456;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#36716;&#31227;&#23398;&#20064;&#21487;&#20197;&#22312;&#31232;&#32570;&#25968;&#25454;&#24773;&#20917;&#19979;&#25552;&#20379;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer-based systems have enabled greater accuracies with fewer training examples, data acquisition obstacles still persist for rare-class tasks -- when the class label is very infrequent (e.g. &lt; 5% of samples). Active learning has in general been proposed to alleviate such challenges, but choice of selection strategy, the criteria by which rare-class examples are chosen, has not been systematically evaluated. Further, transformers enable iterative transfer-learning approaches. We propose and investigate transfer- and active learning solutions to the rare class problem of dissonance detection through utilizing models trained on closely related tasks and the evaluation of acquisition strategies, including a proposed probability-of-rare-class (PRC) approach. We perform these experiments for a specific rare class problem: collecting language samples of cognitive dissonance from social media. We find that PRC is a simple and effective strategy to guide annotations and ultimately
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#20998;&#27835;&#25512;&#29702;&#26694;&#26550;NDCR&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#22797;&#26434;&#25991;&#26412;&#35270;&#20026;&#30001;&#22810;&#20010;&#31616;&#21333;&#21629;&#39064;&#21477;&#32452;&#25104;&#30340;&#22797;&#21512;&#21629;&#39064;&#25991;&#26412;&#65292;&#23558;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#20998;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;&#20998;&#27835;&#12289;&#24449;&#26381;&#21644;&#32452;&#21512;&#12290;&#35813;&#26694;&#26550;&#22312;&#35299;&#20915;&#35821;&#35328;&#22797;&#26434;&#25991;&#26412;&#38382;&#39064;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02265</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#35821;&#35328;&#22797;&#26434;&#30340;&#22270;&#20687;&#26816;&#32034;&#30340;&#31070;&#32463;&#20998;&#27835;&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text. (arXiv:2305.02265v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02265
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#20998;&#27835;&#25512;&#29702;&#26694;&#26550;NDCR&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#22797;&#26434;&#25991;&#26412;&#35270;&#20026;&#30001;&#22810;&#20010;&#31616;&#21333;&#21629;&#39064;&#21477;&#32452;&#25104;&#30340;&#22797;&#21512;&#21629;&#39064;&#25991;&#26412;&#65292;&#23558;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#20998;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;&#20998;&#27835;&#12289;&#24449;&#26381;&#21644;&#32452;&#21512;&#12290;&#35813;&#26694;&#26550;&#22312;&#35299;&#20915;&#35821;&#35328;&#22797;&#26434;&#25991;&#26412;&#38382;&#39064;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#22270;&#20687;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#24403;&#38754;&#23545;&#38590;&#20197;&#29702;&#35299;&#30340;&#35821;&#35328;&#22797;&#26434;&#25991;&#26412;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#26412;&#25991;&#21463;&#21040;&#20998;&#27835;&#31639;&#27861;&#21644;&#21452;&#36807;&#31243;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#23558;&#35821;&#35328;&#22797;&#26434;&#25991;&#26412;&#35270;&#20026;&#30001;&#22810;&#20010;&#31616;&#21333;&#21629;&#39064;&#21477;&#32452;&#25104;&#30340;&#22797;&#21512;&#21629;&#39064;&#25991;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#20998;&#27835;&#25512;&#29702;&#26694;&#26550;NDCR&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;1&#65289;&#20998;&#27835;&#65306;&#21629;&#39064;&#29983;&#25104;&#22120;&#23558;&#22797;&#21512;&#21629;&#39064;&#25991;&#26412;&#20998;&#20026;&#31616;&#21333;&#21629;&#39064;&#21477;&#65292;&#24182;&#29983;&#25104;&#23427;&#20204;&#30340;&#23545;&#24212;&#34920;&#31034;&#65292;2&#65289;&#24449;&#26381;&#65306;&#22522;&#20110;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#20132;&#20114;&#22120;&#23454;&#29616;&#20998;&#35299;&#21629;&#39064;&#21477;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;3&#65289;&#32452;&#21512;&#65306;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#22120;&#37319;&#29992;&#31070;&#32463;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#23558;&#19978;&#36848;&#25512;&#29702;&#29366;&#24577;&#32452;&#21512;&#65292;&#33719;&#24471;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Vision-Language Models (VLMs) have achieved remarkable performance in image retrieval from text. However, their performance drops drastically when confronted with linguistically complex texts that they struggle to comprehend. Inspired by the Divide-and-Conquer algorithm and dual-process theory, in this paper, we regard linguistically complex texts as compound proposition texts composed of multiple simple proposition sentences and propose an end-to-end Neural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains three main components: 1)Divide: a proposition generator divides the compound proposition text into simple proposition sentences and produces their corresponding representations, 2)Conquer: a pretrained VLMs-based visual-linguistic interactor achieves the interaction between decomposed proposition sentences and images, 3)Combine: a neural-symbolic reasoner combines the above reasoning states to obtain the final solution via a neural logic reasoning approach
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNTER&#30340;&#32479;&#19968;&#30693;&#35782;&#25509;&#21475;&#65292;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24615;&#33021;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#19981;&#26029;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.01624</link><description>&lt;p&gt;
UNTER: &#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#30693;&#35782;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
UNTER: A Unified Knowledge Interface for Enhancing Pre-trained Language Models. (arXiv:2305.01624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNTER&#30340;&#32479;&#19968;&#30693;&#35782;&#25509;&#21475;&#65292;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24615;&#33021;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#19981;&#26029;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22806;&#37096;&#30693;&#35782;&#27880;&#20837;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36866;&#29992;&#20110;&#32467;&#26500;&#21270;&#30693;&#35782;&#25110;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#20351;&#29992;&#26041;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNTER&#30340;&#32479;&#19968;&#30693;&#35782;&#25509;&#21475;&#65292;&#20197;&#25552;&#20379;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#22312;UNTER&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#35299;&#30721;&#22120;&#20316;&#20026;&#32479;&#19968;&#30340;&#30693;&#35782;&#25509;&#21475;&#65292;&#23558;&#20174;&#32534;&#30721;&#22120;&#33719;&#21462;&#30340;&#36328;&#24230;&#34920;&#31034;&#19982;&#20854;&#23545;&#24212;&#30340;&#30693;&#35782;&#36827;&#34892;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#32534;&#30721;&#22120;&#33021;&#22815;&#20174;&#20854;&#21442;&#25968;&#20013;&#32479;&#19968;&#35843;&#29992;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#30340;&#36328;&#24230;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#27880;&#20837;&#20004;&#31181;&#24418;&#24335;&#30340;&#30693;&#35782;&#65292;UNTER&#22312;&#19968;&#31995;&#21015;&#30693;&#35782;&#39537;&#21160;&#30340;NLP&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#19981;&#26029;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#23454;&#20307;&#31867;&#22411;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#25928;&#26524;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research demonstrates that external knowledge injection can advance pre-trained language models (PLMs) in a variety of downstream NLP tasks. However, existing knowledge injection methods are either applicable to structured knowledge or unstructured knowledge, lacking a unified usage. In this paper, we propose a UNified knowledge inTERface, UNTER, to provide a unified perspective to exploit both structured knowledge and unstructured knowledge. In UNTER, we adopt the decoder as a unified knowledge interface, aligning span representations obtained from the encoder with their corresponding knowledge. This approach enables the encoder to uniformly invoke span-related knowledge from its parameters for downstream applications. Experimental results show that, with both forms of knowledge injected, UNTER gains continuous improvements on a series of knowledge-driven NLP tasks, including entity typing, named entity recognition and relation extraction, especially in low-resource scenarios.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SearChain&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;SearChain&#36890;&#36807;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#23454;&#29616;&#65292;&#20854;&#24605;&#36335;&#26159;&#36890;&#36807;&#26500;&#36896;&#26597;&#35810;&#38142;&#65292;&#23558;&#22810;&#36339;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#65292;&#26368;&#32456;&#25351;&#23548;LLM&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.14732</link><description>&lt;p&gt;
&#22522;&#20110;SearChain&#30340;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#31934;&#30830;&#12289;&#21487;&#20449;&#21644;&#21487;&#36861;&#28335;&#20869;&#23481;&#29983;&#25104;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks. (arXiv:2304.14732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SearChain&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;SearChain&#36890;&#36807;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#23454;&#29616;&#65292;&#20854;&#24605;&#36335;&#26159;&#36890;&#36807;&#26500;&#36896;&#26597;&#35810;&#38142;&#65292;&#23558;&#22810;&#36339;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#65292;&#26368;&#32456;&#25351;&#23548;LLM&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22914;&#20309;&#20351;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#20934;&#30830;&#21487;&#20449;&#22312;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Search-in-the-Chain&#65288;SearChain&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#31561;&#20856;&#22411;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;SearChain&#26159;&#19968;&#20010;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#26694;&#26550;&#12290;&#22312;SearChain&#20013;&#65292;LLM&#26500;&#24314;&#26597;&#35810;&#38142;&#65292;&#20316;&#20026;&#22810;&#36339;&#38382;&#39064;&#30340;&#20998;&#35299;&#12290;&#38142;&#30340;&#27599;&#20010;&#33410;&#28857;&#37117;&#26159;&#30001;IR&#23548;&#21521;&#30340;&#26597;&#35810;-&#31572;&#26696;&#23545;&#65292;&#20197;&#21450;&#30001;LLM&#29983;&#25104;&#30340;&#35813;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;IR&#39564;&#35777;&#12289;&#23436;&#21892;&#21644;&#36319;&#36394;&#38142;&#20013;&#27599;&#20010;&#33410;&#28857;&#30340;&#20449;&#24687;&#65292;&#20197;&#25351;&#23548;LLM&#26500;&#24314;&#27491;&#30830;&#30340;&#26597;&#35810;&#38142;&#65292;&#24182;&#26368;&#32456;&#22238;&#31572;&#22810;&#36339;&#38382;&#39064;&#12290;SearChain&#20351;LLM&#20174;&#19968;&#27425;&#24615;&#31572;&#26696;&#36716;&#21464;&#20026;&#22810;&#27493;&#31572;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SearChain&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the wide application of Large Language Models (LLMs) such as ChatGPT, how to make the contents generated by LLM accurate and credible becomes very important, especially in complex knowledge-intensive tasks. In this paper, we propose a novel framework called Search-in-the-Chain (SearChain) to improve the accuracy, credibility and traceability of LLM-generated content for multi-hop question answering, which is a typical complex knowledge-intensive task. SearChain is a framework that deeply integrates LLM and information retrieval (IR). In SearChain, LLM constructs a chain-of-query, which is the decomposition of the multi-hop question. Each node of the chain is a query-answer pair consisting of an IR-oriented query and the answer generated by LLM for this query. IR verifies, completes, and traces the information of each node of the chain, so as to guide LLM to construct the correct chain-of-query, and finally answer the multi-hop question. SearChain makes LLM change from trying to gi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#65292;&#32780;GPT-4&#30340;&#34920;&#29616;&#21017;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.03439</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#21644;GPT-4&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. (arXiv:2304.03439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#65292;&#32780;GPT-4&#30340;&#34920;&#29616;&#21017;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;4&#65288;GPT-4&#65289;&#30340;&#21457;&#24067;&#65292;&#25105;&#20204;&#28212;&#26395;&#20102;&#35299;GPT-4&#22312;&#21508;&#31181;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;LogiQA&#21644;ReClor&#31561;&#24120;&#29992;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21450;&#20687;AR-LSAT&#36825;&#26679;&#30340;&#26032;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#38656;&#35201;&#36923;&#36753;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#27979;&#35797;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;ChatGPT&#21644;GPT-4&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;ChatGPT&#21644;GPT-4&#20043;&#38388;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#30340;&#34920;&#29616;&#36828;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#12290;GPT-4&#22312;&#25105;&#20204;&#30340;&#25163;&#21160;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#39640;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#21644;GPT-4&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#20026;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing logical reasoning ability is a comprehensive natural language understanding endeavor. With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks. This report analyses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehension and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. GPT-4 shows even higher performance on our manual tests. Among benchmarks, ChatGPT and GPT-4 do relatively w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DrBERT&#65292;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#21644;&#20020;&#24202;&#39046;&#22495;&#30340;&#20581;&#22766;&#30340;&#27861;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#20844;&#20849;&#25968;&#25454;&#21644;&#21307;&#30103;&#26426;&#26500;&#30340;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#19978;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;PLMs&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#26368;&#32456;&#65292;&#21457;&#24067;&#20102;&#39318;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#27861;&#35821;&#19987;&#29992;PLMs&#65292;&#20197;&#21450;&#26368;&#22823;&#30340;&#21307;&#23398;&#25968;&#25454;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2304.00958</link><description>&lt;p&gt;
DrBERT:&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#21644;&#20020;&#24202;&#39046;&#22495;&#30340;&#20581;&#22766;&#30340;&#27861;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains. (arXiv:2304.00958v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DrBERT&#65292;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#21644;&#20020;&#24202;&#39046;&#22495;&#30340;&#20581;&#22766;&#30340;&#27861;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#20844;&#20849;&#25968;&#25454;&#21644;&#21307;&#30103;&#26426;&#26500;&#30340;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#19978;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;PLMs&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#26368;&#32456;&#65292;&#21457;&#24067;&#20102;&#39318;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#27861;&#35821;&#19987;&#29992;PLMs&#65292;&#20197;&#21450;&#26368;&#22823;&#30340;&#21307;&#23398;&#25968;&#25454;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#26368;&#21021;&#30340;&#27169;&#22411;&#26159;&#20351;&#29992;&#36890;&#29992;&#39046;&#22495;&#25968;&#25454;&#35757;&#32451;&#30340;&#65292;&#20294;&#19987;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#27169;&#22411;&#24050;&#32463;&#20986;&#29616;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;PLMs&#30340;&#21407;&#22987;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#27425;&#27604;&#36739;&#20102;&#22312;&#20844;&#20849;&#32593;&#32476;&#25968;&#25454;&#21644;&#21307;&#30103;&#26426;&#26500;&#30340;&#31169;&#26377;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;PLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#23398;&#20064;&#31574;&#30053;&#22312;&#19968;&#32452;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#24050;&#32463;&#23384;&#22312;&#30340;&#22806;&#35821;&#29983;&#29289;&#21307;&#23398;PLMs&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#27861;&#35821;&#19987;&#29992;PLMs&#65292;&#31216;&#20026;DrBERT&#65292;&#20197;&#21450;&#36825;&#20123;&#27169;&#22411;&#25152;&#35757;&#32451;&#30340;&#26368;&#22823;&#30340;&#21307;&#23398;&#25968;&#25454;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pre-trained language models (PLMs) achieve the best performance on a wide range of natural language processing (NLP) tasks. While the first models were trained on general domain data, specialized ones have emerged to more effectively treat specific domains. In this paper, we propose an original study of PLMs in the medical domain on French language. We compare, for the first time, the performance of PLMs trained on both public data from the web and private data from healthcare establishments. We also evaluate different learning strategies on a set of biomedical tasks. In particular, we show that we can take advantage of already existing biomedical PLMs in a foreign language by further pre-train it on our targeted data. Finally, we release the first specialized PLMs for the biomedical field in French, called DrBERT, as well as the largest corpus of medical data under free license on which these models are trained.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20813;&#30123;&#27835;&#30103;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#20102;&#23567;&#26679;&#26412;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#23545;&#27604;&#20102;&#22522;&#32447;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#31361;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#25913;&#21892;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#19981;&#21516;&#30142;&#30149;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.12692</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Language Models are Few-shot Learners for Prognostic Prediction. (arXiv:2302.12692v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20813;&#30123;&#27835;&#30103;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#20102;&#23567;&#26679;&#26412;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#23545;&#27604;&#20102;&#22522;&#32447;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#31361;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#25913;&#21892;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#19981;&#21516;&#30142;&#30149;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39044;&#27979;&#26159;&#21307;&#30103;&#20445;&#20581;&#34892;&#19994;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#24314;&#31435;&#22312;&#29702;&#35770;&#26694;&#26550;Transformers&#20043;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#24182;&#26410;&#24310;&#20280;&#21040;&#36825;&#20010;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#20013;&#24739;&#32773;&#30340;&#20020;&#24202;&#25968;&#25454;&#21644;&#20998;&#23376;&#29305;&#24449;&#25506;&#32034;&#20102;Transformers&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#20813;&#30123;&#27835;&#30103;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Transformers&#30456;&#23545;&#20110;&#24120;&#35268;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20020;&#24202;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#39044;&#27979;&#32597;&#35265;&#30142;&#30149;&#39046;&#22495;&#19979;&#23567;&#26679;&#26412;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#23545;&#27604;&#20102;&#22522;&#32447;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#31181;&#30284;&#30151;&#31867;&#22411;&#30340;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#19981;&#21516;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#31361;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#25913;&#21892;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#19981;&#21516;&#30142;&#30149;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immunotherapy using real-world patients' clinical data and molecular profiles. This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas. The study benchmarks the efficacy of baselines and language models on prognostic prediction across multiple cancer types and investigates the impact of different pretrained language models under few-shot regimes. The results demonstrate significant improvements in accuracy and highlight the potential of NLP in clinical research to improve early detection and intervention for different diseases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#65306;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;&#21487;&#33021;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#20013;&#31574;&#30053;&#24615;&#22320;&#27880;&#20837;&#25552;&#31034;&#26469;&#36828;&#31243;&#21033;&#29992; LLM &#38598;&#25104;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#25915;&#20987;&#23545;&#25968;&#25454;&#30423;&#31363;&#12289;&#34837;&#34411;&#12289;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#27745;&#26579;&#31561;&#36896;&#25104;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2302.12173</link><description>&lt;p&gt;
&#19981;&#26159;&#20320;&#25152;&#31614;&#32626;&#30340;&#65306;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#22952;&#23475;&#29616;&#23454;&#19990;&#30028; LLM &#19968;&#20307;&#21270;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. (arXiv:2302.12173v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#65306;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;&#21487;&#33021;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#20013;&#31574;&#30053;&#24615;&#22320;&#27880;&#20837;&#25552;&#31034;&#26469;&#36828;&#31243;&#21033;&#29992; LLM &#38598;&#25104;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#25915;&#20987;&#23545;&#25968;&#25454;&#30423;&#31363;&#12289;&#34837;&#34411;&#12289;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#27745;&#26579;&#31561;&#36896;&#25104;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26085;&#30410;&#34987;&#25972;&#21512;&#21040;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#26368;&#36817;&#30340; LLM &#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#28789;&#27963;&#35843;&#33410;&#21151;&#33021;&#12290;&#36825;&#20351;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#65292;&#20363;&#22914; Prompt Injection (PI) &#25915;&#20987;&#20351;&#25915;&#20987;&#32773;&#33021;&#22815;&#35206;&#30422;&#21407;&#22987;&#25351;&#20196;&#21644;&#20351;&#29992;&#30340;&#25511;&#20214;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20154;&#20204;&#35748;&#20026;&#29992;&#25143;&#30452;&#25509;&#25552;&#31034; LLM&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#19981;&#26159;&#29992;&#25143;&#25552;&#31034;&#21602;&#65311;&#25105;&#20204;&#35748;&#20026; LLM &#19968;&#20307;&#21270;&#24212;&#29992;&#31243;&#24207;&#27169;&#31946;&#20102;&#25968;&#25454;&#21644;&#25351;&#20196;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20351;&#29992;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#30340;&#26032;&#25915;&#20987;&#21521;&#37327;&#65292;&#20351;&#25915;&#20987;&#32773;&#33021;&#22815;&#36890;&#36807;&#22312;&#21487;&#33021;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#20013;&#31574;&#30053;&#24615;&#22320;&#27880;&#20837;&#25552;&#31034;&#26469;&#36828;&#31243;&#65288;&#27809;&#26377;&#30452;&#25509;&#25509;&#21475;&#65289;&#21033;&#29992; LLM &#38598;&#25104;&#24212;&#29992;&#31243;&#24207;&#12290;&#25105;&#20204;&#20174;&#35745;&#31639;&#26426;&#23433;&#20840;&#30340;&#35282;&#24230;&#25512;&#23548;&#20986;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65292;&#20197;&#31995;&#32479;&#22320;&#35843;&#26597;&#24433;&#21709;&#21644;&#28431;&#27934;&#65292;&#21253;&#25324;&#25968;&#25454;&#30423;&#31363;&#12289;&#34837;&#34411;&#12289;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#27745;&#26579;&#20197;&#21450;&#20854;&#20182;&#21019;&#26032;&#24615;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other nove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#25991;&#26412;&#29983;&#25104;&#35780;&#20215;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#35780;&#20215;&#25351;&#26631;&#23384;&#22312;&#19968;&#20123;&#30450;&#28857;&#21644;&#20559;&#35265;&#65292;&#20363;&#22914;BERTScore&#23545;&#25688;&#35201;&#20013;&#30340;&#25130;&#26029;&#35823;&#24046;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#65292;MAUVE&#23545;&#20110;&#29983;&#25104;&#30340;&#24320;&#22836;&#25110;&#20013;&#38388;&#30340;&#35823;&#24046;&#19981;&#25935;&#24863;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20215;&#12290;</title><link>http://arxiv.org/abs/2212.10020</link><description>&lt;p&gt;
&#35770;&#22522;&#20110;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20215;&#25351;&#26631;&#30340;&#30450;&#28857;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Blind Spots of Model-Based Evaluation Metrics for Text Generation. (arXiv:2212.10020v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#25991;&#26412;&#29983;&#25104;&#35780;&#20215;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#35780;&#20215;&#25351;&#26631;&#23384;&#22312;&#19968;&#20123;&#30450;&#28857;&#21644;&#20559;&#35265;&#65292;&#20363;&#22914;BERTScore&#23545;&#25688;&#35201;&#20013;&#30340;&#25130;&#26029;&#35823;&#24046;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#65292;MAUVE&#23545;&#20110;&#29983;&#25104;&#30340;&#24320;&#22836;&#25110;&#20013;&#38388;&#30340;&#35823;&#24046;&#19981;&#25935;&#24863;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26377;&#29992;&#20294;&#24120;&#24120;&#34987;&#24573;&#30053;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20215;&#25351;&#26631;&#40065;&#26834;&#24615;&#20998;&#26512;&#26041;&#27861;&#65306;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#12290;&#25105;&#20204;&#38543;&#26426;&#35774;&#35745;&#24182;&#21512;&#25104;&#20102;&#21508;&#31181;&#21487;&#33021;&#30340;&#35823;&#24046;&#65292;&#24182;&#26816;&#26597;&#23427;&#20204;&#26159;&#21542;&#20250;&#23548;&#33268;&#35780;&#20215;&#25351;&#26631;&#20998;&#25968;&#30340;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#31995;&#21015;&#26368;&#26032;&#35780;&#20215;&#25351;&#26631;&#65292;&#29992;&#20110;&#24320;&#25918;&#24335;&#29983;&#25104;&#12289;&#32763;&#35793;&#21644;&#25688;&#35201;&#31561;&#20219;&#21153;&#12290;&#23454;&#39564;&#25581;&#31034;&#20102;&#29616;&#26377;&#35780;&#20215;&#25351;&#26631;&#20013;&#26377;&#36259;&#30340;&#19981;&#25935;&#24863;&#12289;&#20559;&#35265;&#12289;&#29978;&#33267;&#28431;&#27934;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;BERTScore&#23545;&#25688;&#35201;&#20013;&#30340;&#25130;&#26029;&#35823;&#24046;&#24863;&#21040;&#22256;&#24785;&#65292;&#32780;&#22312;&#29983;&#25104;&#30340;&#24320;&#22836;&#25110;&#20013;&#38388;&#23384;&#22312;&#35823;&#24046;&#26102;MAUVE&#65288;&#22522;&#20110;GPT-2&#65289;&#21017;&#19981;&#25935;&#24863;&#12290;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#30450;&#28857;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20215;&#12290;&#25105;&#20204;&#24050;&#22312;https://github.com/cloudygoose/blindspot_nlg &#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. We have released our code and data at https://github.com/cloudygoose/blindspot_nlg.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PVGRU&#30340;&#32452;&#20214;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#27719;&#24635;&#21464;&#37327;&#26469;&#32858;&#21512;&#23376;&#24207;&#21015;&#30340;&#32047;&#31215;&#20998;&#24067;&#21464;&#21270;&#65292;&#20174;&#32780;&#20248;&#21270;&#22522;&#20110;&#29983;&#25104;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22810;&#36718;&#23545;&#35805;&#22238;&#22797;&#65292;&#25552;&#39640;&#23545;&#35805;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.09086</link><description>&lt;p&gt;
PVGRU&#65306;&#36890;&#36807;Pseudo-Variational&#26426;&#21046;&#29983;&#25104;&#22810;&#26679;&#19988;&#30456;&#20851;&#30340;&#23545;&#35805;&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism. (arXiv:2212.09086v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PVGRU&#30340;&#32452;&#20214;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#27719;&#24635;&#21464;&#37327;&#26469;&#32858;&#21512;&#23376;&#24207;&#21015;&#30340;&#32047;&#31215;&#20998;&#24067;&#21464;&#21270;&#65292;&#20174;&#32780;&#20248;&#21270;&#22522;&#20110;&#29983;&#25104;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22810;&#36718;&#23545;&#35805;&#22238;&#22797;&#65292;&#25552;&#39640;&#23545;&#35805;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#29983;&#25104;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#30340;&#22238;&#22797;&#29983;&#25104;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;RNN&#65288;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#26368;&#21518;&#38544;&#34255;&#30340;&#29366;&#24577;&#26469;&#27719;&#24635;&#24207;&#21015;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#19981;&#21516;&#23545;&#35805;&#20013;&#35266;&#23519;&#21040;&#30340;&#24494;&#22937;&#21464;&#21270;&#65292;&#24182;&#19988;&#19981;&#33021;&#21306;&#20998;&#22312;&#26500;&#25104;&#26041;&#38754;&#30456;&#20284;&#30340;&#23545;&#35805;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Pseudo-Variational Gated Recurrent Unit&#65288;PVGRU&#65289;&#32452;&#20214;&#65292;&#26080;&#38656;&#21518;&#39564;&#30693;&#35782;&#21363;&#21487;&#23558;&#27719;&#24635;&#21464;&#37327;&#24341;&#20837;GRU&#65292;&#20854;&#21487;&#20197;&#32858;&#21512;&#23376;&#24207;&#21015;&#30340;&#32047;&#31215;&#20998;&#24067;&#21464;&#21270;&#12290; PVGRU&#21487;&#20197;&#36890;&#36807;&#24635;&#32467;&#21464;&#37327;&#24863;&#30693;&#24494;&#22937;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#36825;&#20123;&#21464;&#21270;&#26159;&#36890;&#36807;&#35774;&#35745;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#21644;&#37325;&#26500;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;PVGRU&#26500;&#24314;&#20102;Pseudo-Variational Hierarchical Dialogue&#65288;PVHD&#65289;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PVGRU&#21487;&#20197;&#24191;&#27867;&#25552;&#39640;&#23545;&#35805;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate response generation for multi-turn dialogue in generative-based chatbots. Existing generative models based on RNNs (Recurrent Neural Networks) usually employ the last hidden state to summarize the sequences, which makes models unable to capture the subtle variability observed in different dialogues and cannot distinguish the differences between dialogues that are similar in composition. In this paper, we propose a Pseudo-Variational Gated Recurrent Unit (PVGRU) component without posterior knowledge through introducing a recurrent summarizing variable into the GRU, which can aggregate the accumulated distribution variations of subsequences. PVGRU can perceive the subtle semantic variability through summarizing variables that are optimized by the devised distribution consistency and reconstruction objectives. In addition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model based on PVGRU. Experimental results demonstrate that PVGRU can broadly improve the dive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01692</link><description>&lt;p&gt;
&#22312;&#22330;&#23398;&#20064;&#32773;&#33021;&#21542;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#25512;&#29702;&#27010;&#24565;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22330;&#23398;&#20064;&#32773;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#20182;&#20204;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#22914;&#26631;&#31614;&#30340;&#24773;&#24863;&#65292;&#32780;&#19981;&#26159;&#22312;&#36755;&#20837;&#20013;&#25214;&#21040;&#26032;&#30340;&#20851;&#32852;&#24615;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#23569;&#26679;&#26412;&#35780;&#20272;&#35774;&#32622;&#20351;&#29992;&#38543;&#26426;&#36873;&#25321;&#30340;&#22312;&#22330;&#28436;&#31034;&#26080;&#27861;&#21306;&#20998;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#22823;&#37096;&#20998;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#24182;&#19981;&#21576;&#29616;&#36229;&#36234;&#26292;&#38706;&#20110;&#26032;&#20219;&#21153;&#20998;&#24067;&#30340;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#12290;&#25105;&#20204;&#20174;&#27880;&#37322;&#35299;&#37322;&#20013;&#25552;&#21462;&#20102;&#19968;&#32452;&#36825;&#26679;&#30340;&#27010;&#24565;&#65292;&#24182;&#27979;&#37327;&#20102;&#27169;&#22411;&#23637;&#31034;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#33719;&#24471;&#22810;&#23569;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations. However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input. However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models' ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.  To disentangle models' in-context learning ability independent of models' memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in f
&lt;/p&gt;</description></item><item><title>xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.00735</link><description>&lt;p&gt;
xTrimoABFold&#65306;&#26080;&#22810;&#24207;&#21015;&#27604;&#23545;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v2 [q-bio.QM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00735
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25239;&#20307;&#24037;&#31243;&#39046;&#22495;&#65292;&#35774;&#35745;&#19968;&#20010;&#26032;&#22411;&#25239;&#20307;&#20197;&#27491;&#30830;&#22320;&#32467;&#21512;&#29305;&#23450;&#25239;&#21407;&#30340;&#34920;&#20301;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20102;&#35299;&#25239;&#20307;&#32467;&#26500;&#21644;&#20854;&#34920;&#20301;&#21487;&#20197;&#20419;&#36827;&#23545;&#20854;&#21151;&#33021;&#30340;&#26426;&#21046;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#20174;&#20854;&#24207;&#21015;&#39044;&#27979;&#25239;&#20307;&#32467;&#26500;&#19968;&#30452;&#26159;&#19968;&#39033;&#39640;&#24230;&#26377;&#20215;&#20540;&#30340;&#20219;&#21153;&#65292;&#32780;AlphaFold2&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#34507;&#30333;&#36136;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23545;&#20110;&#25239;&#20307;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25239;&#20307;&#30340;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDRs&#65289;&#65292;&#20854;&#39044;&#27979;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of antibody engineering, an essential task is to design a novel antibody whose paratopes bind to a specific antigen with correct epitopes. Understanding antibody structure and its paratope can facilitate a mechanistic understanding of its function. Therefore, antibody structure prediction from its sequence alone has always been a highly valuable problem for de novo antibody design. AlphaFold2, a breakthrough in the field of structural biology, provides a solution to predict protein structure based on protein sequences and computationally expensive coevolutionary multiple sequence alignments (MSAs). However, the computational efficiency and undesirable prediction accuracy of antibodies, especially on the complementarity-determining regions (CDRs) of antibodies limit their applications in the industrially high-throughput drug design. To learn an informative representation of antibodies, we employed a deep antibody language model (ALM) on curated sequences from the observed a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PR-ENT&#30340;&#20107;&#20214;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#38750;&#32467;&#26500;&#21270;&#20840;&#25991;&#20107;&#20214;&#25551;&#36848;&#20013;&#25552;&#21462;&#20107;&#20214;&#31867;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22635;&#20805;&#20107;&#20214;&#25551;&#36848;&#20013;&#30340;&#27169;&#26495;&#65292;&#24182;&#22312;&#25991;&#26412;&#38544;&#21547;&#20851;&#31995;&#20219;&#21153;&#20013;&#36873;&#25321;&#31572;&#26696;&#20505;&#36873;&#39033;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#36164;&#28304;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.05257</link><description>&lt;p&gt;
&#29992;&#25552;&#31034;&#34164;&#28085;&#37325;&#26032;&#24605;&#32771;&#20107;&#20214;&#32534;&#30721;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Event Coding Pipeline with Prompt Entailment. (arXiv:2210.05257v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05257
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PR-ENT&#30340;&#20107;&#20214;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#38750;&#32467;&#26500;&#21270;&#20840;&#25991;&#20107;&#20214;&#25551;&#36848;&#20013;&#25552;&#21462;&#20107;&#20214;&#31867;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22635;&#20805;&#20107;&#20214;&#25551;&#36848;&#20013;&#30340;&#27169;&#26495;&#65292;&#24182;&#22312;&#25991;&#26412;&#38544;&#21547;&#20851;&#31995;&#20219;&#21153;&#20013;&#36873;&#25321;&#31572;&#26696;&#20505;&#36873;&#39033;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#36164;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30417;&#27979;&#21361;&#26426;&#65292;&#25919;&#27835;&#20107;&#20214;&#20174;&#26032;&#38395;&#20013;&#25552;&#21462;&#12290;&#24222;&#22823;&#30340;&#38750;&#32467;&#26500;&#21270;&#20840;&#25991;&#20107;&#20214;&#25551;&#36848;&#20351;&#24471;&#36880;&#26696;&#20998;&#26512;&#38590;&#20197;&#25511;&#21046;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#30340;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#32452;&#32455;&#26469;&#35828;&#12290;&#36825;&#23601;&#38656;&#35201;&#23545;&#20107;&#20214;&#36827;&#34892;&#20998;&#31867;&#65292;&#36825;&#19968;&#20219;&#21153;&#34987;&#31216;&#20026;&#20107;&#20214;&#32534;&#30721;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PR-ENT&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#20107;&#20214;&#32534;&#30721;&#26041;&#27861;&#65292;&#26356;&#21152;&#28789;&#27963;&#21644;&#36164;&#28304;&#39640;&#25928;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For monitoring crises, political events are extracted from the news. The large amount of unstructured full-text event descriptions makes a case-by-case analysis unmanageable, particularly for low-resource humanitarian aid organizations. This creates a demand to classify events into event types, a task referred to as event coding. Typically, domain experts craft an event type ontology, annotators label a large dataset and technical experts develop a supervised coding system. In this work, we propose PR-ENT, a new event coding approach that is more flexible and resource-efficient, while maintaining competitive accuracy: first, we extend an event description such as "Military injured two civilians'' by a template, e.g. "People were [Z]" and prompt a pre-trained (cloze) language model to fill the slot Z. Second, we select answer candidates Z* = {"injured'', "hurt"...} by treating the event description as premise and the filled templates as hypothesis in a textual entailment task. This allo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26126;&#30830;&#30340;&#33258;&#25105;&#20182;&#20154;&#24847;&#35782;&#29983;&#25104;&#20849;&#24773;&#22238;&#24212;&#65292;&#25552;&#39640;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#20849;&#24773;&#22238;&#24212;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.03884</link><description>&lt;p&gt;
&#19981;&#35201;&#22833;&#21435;&#33258;&#25105;&#65281;&#36890;&#36807;&#26126;&#30830;&#30340;&#33258;&#25105;&#20182;&#20154;&#24847;&#35782;&#29983;&#25104;&#20849;&#24773;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
Don't Lose Yourself! Empathetic Response Generation via Explicit Self-Other Awareness. (arXiv:2210.03884v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03884
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26126;&#30830;&#30340;&#33258;&#25105;&#20182;&#20154;&#24847;&#35782;&#29983;&#25104;&#20849;&#24773;&#22238;&#24212;&#65292;&#25552;&#39640;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#20849;&#24773;&#22238;&#24212;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#31867;&#20284;&#20110;&#20154;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20849;&#24773;&#22238;&#24212;&#30340;&#29983;&#25104;&#25104;&#20026;&#20102;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#20043;&#21069;&#30340;&#23581;&#35797;&#19981;&#22815;&#23436;&#25972;&#65292;&#19981;&#33021;&#24341;&#36215;&#20849;&#24773;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#20851;&#27880;&#20110;&#36890;&#36807;&#20854;&#20182;&#24863;&#30693;&#26469;&#33258;&#21160;&#27169;&#20223;&#29992;&#25143;&#30340;&#24773;&#32490;&#21644;&#24605;&#24819;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#30053;&#20102;&#32500;&#25345;&#21644;&#32771;&#34385;&#31995;&#32479;&#33258;&#36523;&#35266;&#28857;&#30340;&#37325;&#35201;&#36807;&#31243;&#65292;&#36825;&#26159;&#23454;&#29616;&#33258;&#25105;&#20182;&#20154;&#24847;&#35782;&#30340;&#20849;&#24773;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#26126;&#30830;&#30340;&#33258;&#25105;&#20182;&#20154;&#24847;&#35782;&#65288;EmpSOA&#65289;&#29983;&#25104;&#20849;&#24773;&#22238;&#24212;&#12290;&#20855;&#20307;&#22320;&#65292;&#35774;&#35745;&#20102;&#19977;&#20010;&#38454;&#27573;&#65306;&#33258;&#25105;&#20182;&#20154;&#21306;&#21035;&#12289;&#33258;&#25105;&#20182;&#20154;&#35843;&#33410;&#21644;&#33258;&#25105;&#20182;&#20154;&#29983;&#25104;&#65292;&#20197;&#28165;&#26224;&#22320;&#32500;&#25252;&#12289;&#35843;&#33410;&#21644;&#27880;&#20837;&#33258;&#25105;&#20182;&#20154;&#24863;&#30693;&#20449;&#24687;&#21040;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#22343;&#34920;&#26126;&#65292;EmpSOA&#29983;&#25104;&#20102;&#26356;&#20855;&#20849;&#24773;&#30340;&#22238;&#24212;&#65292;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a critical step to achieve human-like chatbots, empathetic response generation has attained increasing interests. Previous attempts are incomplete and not sufficient enough to elicit empathy because they only focus on the initial aspect of empathy to automatically mimic the feelings and thoughts of the user via other-awareness. However, they ignore to maintain and take the own views of the system into account, which is a crucial process to achieve the empathy called self-other awareness. To this end, we propose to generate Empathetic response with explicit Self-Other Awareness (EmpSOA). Specifically, three stages, self-other differentiation, self-other modulation and self-other generation, are devised to clearly maintain, regulate and inject the self-other aware information into the process of empathetic response generation. Both automatic and human evaluations on the benchmark dataset demonstrate the superiority of EmpSOA to generate more empathetic responses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#35789;&#34955;&#12289;&#24207;&#21015;&#12289;&#22270;&#24418;&#21644;&#20998;&#23618;&#26041;&#27861;&#65292;&#21457;&#29616;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#26080;&#27861;&#36229;&#36234;&#29616;&#20195;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#19988;&#29978;&#33267;&#26377;&#26102;&#34920;&#29616;&#19981;&#22914;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36136;&#30097;&#20102;&#36807;&#21435;&#20960;&#24180;&#20013;&#20026;&#24320;&#21457;&#26032;&#30340;&#22270;&#24418;&#26041;&#27861;&#25237;&#20837;&#30340;&#24040;&#22823;&#21162;&#21147;&#20197;&#21450;&#23427;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#24102;&#26469;&#30340;&#25215;&#35834;&#12290;</title><link>http://arxiv.org/abs/2204.03954</link><description>&lt;p&gt;
&#25105;&#20204;&#30495;&#30340;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#21527;&#65311;&#38024;&#23545;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#35789;&#34955;&#12289;&#24207;&#21015;&#12289;&#22270;&#21644;&#23618;&#27425;&#32467;&#26500;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Are We Really Making Much Progress? Bag-of-Words vs. Sequence vs. Graph vs. Hierarchy for Single- and Multi-Label Text Classification. (arXiv:2204.03954v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#35789;&#34955;&#12289;&#24207;&#21015;&#12289;&#22270;&#24418;&#21644;&#20998;&#23618;&#26041;&#27861;&#65292;&#21457;&#29616;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#26080;&#27861;&#36229;&#36234;&#29616;&#20195;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#19988;&#29978;&#33267;&#26377;&#26102;&#34920;&#29616;&#19981;&#22914;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36136;&#30097;&#20102;&#36807;&#21435;&#20960;&#24180;&#20013;&#20026;&#24320;&#21457;&#26032;&#30340;&#22270;&#24418;&#26041;&#27861;&#25237;&#20837;&#30340;&#24040;&#22823;&#21162;&#21147;&#20197;&#21450;&#23427;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#24102;&#26469;&#30340;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#24341;&#21457;&#20102;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#22270;&#24418;&#26041;&#27861;&#30340;&#22797;&#33487;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#26159;&#21542;&#27604;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#29616;&#20195;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26356;&#26377;&#30410;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#20016;&#23500;&#30340;&#35789;&#34955;&#12289;&#22522;&#20110;&#24207;&#21015;&#12289;&#22522;&#20110;&#22270;&#24418;&#21644;&#20998;&#23618;&#26041;&#27861;&#12290;&#25105;&#20204;&#32858;&#21512;&#20102;&#26469;&#33258;&#25991;&#29486;&#30340;&#32467;&#26524;&#65292;&#22312;5&#20010;&#21333;&#26631;&#31614;&#21644;7&#20010;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26126;&#30830;&#34920;&#26126;&#65292;&#22312;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#26080;&#27861;&#36229;&#36234;&#31934;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26377;&#26102;&#29978;&#33267;&#34920;&#29616;&#19981;&#22914;&#35789;&#34955;&#19978;&#30340;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#36136;&#30097;&#20102;&#36807;&#21435;&#20960;&#24180;&#20013;&#20026;&#24320;&#21457;&#26032;&#30340;&#22270;&#24418;&#26041;&#27861;&#25237;&#20837;&#30340;&#24040;&#22823;&#21162;&#21147;&#20197;&#21450;&#23427;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#24102;&#26469;&#30340;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of graph neural networks has triggered a resurgence of graph-based methods for single-label and multi-label text classification. However, it is unclear whether these graph-based methods are beneficial compared to standard machine learning methods and modern pretrained language models. We compare a rich selection of bag-of-words, sequence-based, graph-based, and hierarchical methods for text classification. We aggregate results from the literature over 5 single-label and 7 multi-label datasets and run our own experiments. Our findings unambiguously demonstrate that for single-label and multi-label classification tasks, the graph-based methods fail to outperform fine-tuned language models and sometimes even perform worse than standard machine learning methods like multilayer perceptron (MLP) on a bag-of-words. This questions the enormous amount of effort put into the development of new graph-based methods in the last years and the promises they make for text classification
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DA-ADB&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#36317;&#31163;&#24863;&#30693;&#24847;&#22270;&#34920;&#31034;&#21644;&#33258;&#36866;&#24212;&#20915;&#31574;&#36793;&#30028;&#26469;&#35299;&#20915;&#24320;&#25918;&#24335;&#24847;&#22270;&#26816;&#27979;&#20013;&#30340;&#20004;&#22823;&#25361;&#25112;&#12290;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.05823</link><description>&lt;p&gt;
&#23398;&#20064;&#21306;&#20998;&#24615;&#34920;&#31034;&#21644;&#36793;&#30028;&#20197;&#36827;&#34892;&#24320;&#25918;&#24335;&#24847;&#22270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Discriminative Representations and Decision Boundaries for Open Intent Detection. (arXiv:2203.05823v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DA-ADB&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#36317;&#31163;&#24863;&#30693;&#24847;&#22270;&#34920;&#31034;&#21644;&#33258;&#36866;&#24212;&#20915;&#31574;&#36793;&#30028;&#26469;&#35299;&#20915;&#24320;&#25918;&#24335;&#24847;&#22270;&#26816;&#27979;&#20013;&#30340;&#20004;&#22823;&#25361;&#25112;&#12290;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#24847;&#22270;&#26816;&#27979;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#26410;&#30693;&#30340;&#24320;&#25918;&#24335;&#24847;&#22270;&#65292;&#21516;&#26102;&#30830;&#20445;&#24050;&#30693;&#24847;&#22270;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#26080;&#27861;&#23398;&#20064;&#21451;&#22909;&#30340;&#34920;&#31034;&#20197;&#22312;&#21482;&#26377;&#24050;&#30693;&#24847;&#22270;&#30340;&#20808;&#39564;&#30693;&#35782;&#19979;&#26816;&#27979;&#24320;&#25918;&#24335;&#24847;&#22270;&#12290;&#20854;&#27425;&#65292;&#32570;&#20047;&#33719;&#24471;&#24050;&#30693;&#24847;&#22270;&#30340;&#29305;&#23450;&#32780;&#32039;&#20945;&#30340;&#20915;&#31574;&#36793;&#30028;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DA-ADB&#30340;&#21407;&#22987;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#29992;&#20110;&#24320;&#25918;&#24335;&#24847;&#22270;&#26816;&#27979;&#30340;&#36317;&#31163;&#24863;&#30693;&#24847;&#22270;&#34920;&#31034;&#21644;&#33258;&#36866;&#24212;&#20915;&#31574;&#36793;&#30028;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#36317;&#31163;&#20449;&#24687;&#22686;&#24378;&#20102;&#24847;&#22270;&#34920;&#31034;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#24179;&#34913;&#32463;&#39564;&#21644;&#24320;&#25918;&#31354;&#38388;&#39118;&#38505;&#26469;&#33719;&#24471;&#36866;&#24403;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open intent detection is a significant problem in natural language understanding, which aims to identify the unseen open intent while ensuring known intent identification performance. However, current methods face two major challenges. Firstly, they struggle to learn friendly representations to detect the open intent with prior knowledge of only known intents. Secondly, there is a lack of an effective approach to obtaining specific and compact decision boundaries for known intents. To address these issues, this paper presents an original framework called DA-ADB, which successively learns distance-aware intent representations and adaptive decision boundaries for open intent detection. Specifically, we first leverage distance information to enhance the distinguishing capability of the intent representations. Then, we design a novel loss function to obtain appropriate decision boundaries by balancing both empirical and open space risks. Extensive experiments demonstrate the effectiveness 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#26679;&#26412;&#30340;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;LOGEN&#65292;&#36890;&#36807;&#33258;&#35757;&#32451;&#21644;&#22522;&#20110;&#20869;&#23481;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#25277;&#26679;&#20266;&#36923;&#36753;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#37327;&#26679;&#26412;&#19979;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2112.01404</link><description>&lt;p&gt;
LOGEN&#65306;&#22522;&#20110;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#30340;&#33258;&#35757;&#32451;&#25991;&#26412;&#29983;&#25104;&#22312;&#23569;&#26679;&#26412;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with Self-training. (arXiv:2112.01404v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#26679;&#26412;&#30340;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;LOGEN&#65292;&#36890;&#36807;&#33258;&#35757;&#32451;&#21644;&#22522;&#20110;&#20869;&#23481;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#25277;&#26679;&#20266;&#36923;&#36753;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#37327;&#26679;&#26412;&#19979;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#34920;&#38754;&#23618;&#38754;&#25551;&#36848;&#65292;&#20854;&#23384;&#22312;&#25511;&#21046;&#20869;&#23481;&#36873;&#25321;&#22256;&#38590;&#21644;&#20302;&#20445;&#30495;&#24230;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#36923;&#36753;&#24418;&#24335;&#26469;&#20419;&#36827;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#37327;&#36739;&#22823;&#65292;&#36825;&#20351;&#24471;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#26679;&#26412;&#30340;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#23569;&#37327;&#31181;&#23376;&#36923;&#36753;&#24418;&#24335;&#65288;&#22914;20/100&#31181;&#23376;&#65289; &#65292;&#24182;&#21033;&#29992;&#33258;&#35757;&#32451;&#21644;&#22522;&#20110;&#20869;&#23481;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#25277;&#26679;&#20266;&#36923;&#36753;&#24418;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;&#22522;&#20934;&#26041;&#27861;&#33719;&#24471;&#26356;&#22909;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language generation from structured data mainly focuses on surface-level descriptions, suffering from uncontrollable content selection and low fidelity. Previous works leverage logical forms to facilitate logical knowledge-conditioned text generation. Though achieving remarkable progress, they are data-hungry, which makes the adoption for real-world applications challenging with limited data. To this end, this paper proposes a unified framework for logical knowledge-conditioned text generation in the few-shot setting. With only a few seeds logical forms (e.g., 20/100 shot), our approach leverages self-training and samples pseudo logical forms based on content and structure consistency. Experimental results demonstrate that our approach can obtain better few-shot performance than baselines.
&lt;/p&gt;</description></item><item><title>AVATAR&#26159;&#19968;&#20010; Java-Python&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547; 9,515&#20010;&#32534;&#31243;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#35780;&#20272;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#31243;&#24207;&#32763;&#35793;&#27169;&#22411;&#65292;&#24182;&#19988;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#32534;&#31243;&#35821;&#35328;&#32763;&#35793;&#30340;&#36136;&#37327;&#19978;&#20248;&#20110;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2108.11590</link><description>&lt;p&gt;
AVATAR&#65306;Java-Python&#31243;&#24207;&#32763;&#35793;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
AVATAR: A Parallel Corpus for Java-Python Program Translation. (arXiv:2108.11590v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11590
&lt;/p&gt;
&lt;p&gt;
AVATAR&#26159;&#19968;&#20010; Java-Python&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547; 9,515&#20010;&#32534;&#31243;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#35780;&#20272;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#31243;&#24207;&#32763;&#35793;&#27169;&#22411;&#65292;&#24182;&#19988;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#32534;&#31243;&#35821;&#35328;&#32763;&#35793;&#30340;&#36136;&#37327;&#19978;&#20248;&#20110;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#32763;&#35793;&#25351;&#23558;&#28304;&#20195;&#30721;&#20174;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#36801;&#31227;&#21040;&#21478;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#12290;&#23427;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#23454;&#29992;&#20215;&#20540;&#65292;&#22240;&#20026;&#36328;&#35821;&#35328;&#31227;&#26893;&#36719;&#20214;&#32791;&#26102;&#19988;&#25104;&#26412;&#39640;&#26114;&#12290;&#33258;&#21160;&#36827;&#34892;&#31243;&#24207;&#32763;&#35793;&#22312;&#36719;&#20214;&#36801;&#31227;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26368;&#36817;&#30740;&#31350;&#20154;&#21592;&#30001;&#20110;&#25214;&#19981;&#21040;&#24179;&#34892;&#35821;&#26009;&#24211;&#32780;&#25506;&#32034;&#20102;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#20351;&#24471;&#29992;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#25104;&#20026;&#21487;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AVATAR&#65292;&#21253;&#21547;9,515&#20010;&#20351;&#29992;&#20004;&#31181;&#27969;&#34892;&#35821;&#35328;&#65288;Java&#21644;Python&#65289;&#32534;&#20889;&#30340;&#32534;&#31243;&#38382;&#39064;&#21450;&#20854;&#35299;&#20915;&#26041;&#26696;&#30340;&#38598;&#21512;&#12290;AVATAR&#20174;&#31454;&#25216;&#32534;&#31243;&#32593;&#31449;&#12289;&#22312;&#32447;&#24179;&#21488;&#21644;&#24320;&#28304;&#23384;&#20648;&#24211;&#20013;&#25910;&#38598;&#12290;&#27492;&#22806;&#65292;AVATAR&#36824;&#21253;&#21547;&#20102;250&#20010;&#31034;&#20363;&#30340;&#21333;&#20803;&#27979;&#35797;&#65292;&#20197;&#20415;&#36827;&#34892;&#21151;&#33021;&#27491;&#30830;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;AVATAR&#24494;&#35843;&#20102;&#20960;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#31243;&#24207;&#32763;&#35793;&#36136;&#37327;&#19978;&#26126;&#26174;&#20248;&#20110;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;AVATAR&#21487;&#20197;&#20419;&#36827;&#31243;&#24207;&#32763;&#35793;&#27169;&#22411;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#20316;&#20026;&#35780;&#20272;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program translation refers to migrating source code from one programming language to another. It has tremendous practical value in software development, as porting software across languages is time-consuming and costly. Automating program translation is of paramount importance in software migration, and recently researchers explored unsupervised approaches due to the unavailability of parallel corpora. However, the availability of pre-trained language models for programming languages enables supervised fine-tuning with a small number of labeled examples. Therefore, we present AVATAR, a collection of 9,515 programming problems and their solutions written in two popular languages, Java and Python. AVATAR is collected from competitive programming sites, online platforms, and open-source repositories. Furthermore, AVATAR includes unit tests for 250 examples to facilitate functional correctness evaluation. We benchmark several pre-trained language models fine-tuned on AVATAR. Experiment res
&lt;/p&gt;</description></item></channel></rss>