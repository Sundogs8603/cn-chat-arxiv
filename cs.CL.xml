<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#25552;&#20986;IDGen&#65292;&#23558;&#27599;&#20010;&#25512;&#33616;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#25991;&#26412;ID&#65292;&#20174;&#32780;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26356;&#22909;&#22320;&#19982;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.19021</link><description>&lt;p&gt;
&#26397;&#21521;LLM-RecSys&#23545;&#40784;&#19982;&#25991;&#26412;ID&#23398;&#20064;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Towards LLM-RecSys Alignment with Textual ID Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19021
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;IDGen&#65292;&#23558;&#27599;&#20010;&#25512;&#33616;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#25991;&#26412;ID&#65292;&#20174;&#32780;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26356;&#22909;&#22320;&#19982;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#24050;&#32463;&#23558;&#20256;&#32479;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#25512;&#33616;&#26041;&#24335;&#36716;&#21464;&#20026;&#25991;&#26412;&#29983;&#25104;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#19982;&#22266;&#26377;&#25805;&#20316;&#20154;&#31867;&#35789;&#27719;&#30340;&#26631;&#20934;NLP&#20219;&#21153;&#30456;&#21453;&#65292;&#30446;&#21069;&#29983;&#25104;&#24335;&#25512;&#33616;&#39046;&#22495;&#30340;&#30740;&#31350;&#22312;&#22914;&#20309;&#22312;&#25991;&#26412;&#29983;&#25104;&#33539;&#24335;&#20013;&#20197;&#31616;&#27905;&#32780;&#26377;&#24847;&#20041;&#30340;ID&#34920;&#31034;&#26377;&#25928;&#32534;&#30721;&#25512;&#33616;&#39033;&#30446;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23545;&#40784;LLMs&#19982;&#25512;&#33616;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IDGen&#65292;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#26631;&#35760;&#23558;&#27599;&#20010;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#12289;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#25991;&#26412;ID&#12290;&#36825;&#36890;&#36807;&#22312;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#26049;&#35757;&#32451;&#25991;&#26412;ID&#29983;&#25104;&#22120;&#26469;&#23454;&#29616;&#65292;&#20351;&#20010;&#24615;&#21270;&#25512;&#33616;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21040;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#24182;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#35299;&#32806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19021v1 Announce Type: cross  Abstract: Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potenti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Counting-Stars&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;GPT-4 Turbo&#21644;Kimi Chat&#22312;&#27492;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.11802</link><description>&lt;p&gt;
Counting-Stars&#65306;&#19968;&#31181;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11802
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Counting-Stars&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;GPT-4 Turbo&#21644;Kimi Chat&#22312;&#27492;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#20855;&#26377;&#24378;&#22823;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#35780;&#20272;&#31574;&#30053;&#65292;&#23545;&#39046;&#20808;&#30340;LLMs&#65288;&#20363;&#22914;ChatGPT&#21644;KimiChat&#65289;&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#20102;&#35299;&#29978;&#23569;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#35780;&#20272;&#31574;&#30053;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;Counting-Stars&#12290;Counting-Stars&#26088;&#22312;&#35201;&#27714;LLMs&#20805;&#20998;&#29702;&#35299;&#21644;&#25429;&#25417;&#38271;&#19978;&#19979;&#25991;&#20013;&#30340;&#38271;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#33021;&#22815;&#25910;&#38598;&#36328;&#36234;&#25972;&#20010;&#19978;&#19979;&#25991;&#30340;&#22810;&#20010;&#35777;&#25454;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#22522;&#20110;Counting-Stars&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#20102;&#20004;&#20010;&#39046;&#20808;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#65292;&#21363;GPT-4 Turbo&#21644;Kimi Chat&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4 Turbo&#21644;Kimi Chat&#22312;Counting-Stars&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11802v1 Announce Type: new  Abstract: While recent research endeavors have concentrated on developing Large Language Models (LLMs) with robust long-context capabilities, due to the lack of appropriate evaluation strategies, relatively little is known about how well the long-context processing abilities and performance of leading LLMs (e.g., ChatGPT and KimiChat). To address this gap, we propose a simple, efficient, and reasonable strategy for evaluating long-context LLMs as a new benchmark, named Counting-Stars. The Counting-Stars is designed to require LLMs to fully understand and capture long dependencies in long contexts and be able to collect inter-dependency across multiple pieces of evidence spanning the entire context to finish the task. Based on the Counting-Stars, we conduct experiments to evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo and Kimi Chat. The experimental results indicate that GPT-4 Turbo and Kimi Chat achieve significant performance in th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ConspEmoLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38598;&#25104;&#24773;&#24863;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#38452;&#35851;&#29702;&#35770;&#25991;&#26412;&#30340;&#24773;&#24863;&#29305;&#24449;&#36827;&#34892;&#32508;&#21512;&#20998;&#26512;&#65292;&#33021;&#22815;&#25191;&#34892;&#22810;&#39033;&#20219;&#21153;&#65292;&#21253;&#25324;&#38452;&#35851;&#29702;&#35770;&#26816;&#27979;&#12289;&#29702;&#35770;&#31867;&#22411;&#20998;&#31867;&#21644;&#30456;&#20851;&#25991;&#26412;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.06765</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#24773;&#24863;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#38452;&#35851;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ConspEmoLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38598;&#25104;&#24773;&#24863;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#38452;&#35851;&#29702;&#35770;&#25991;&#26412;&#30340;&#24773;&#24863;&#29305;&#24449;&#36827;&#34892;&#32508;&#21512;&#20998;&#26512;&#65292;&#33021;&#22815;&#25191;&#34892;&#22810;&#39033;&#20219;&#21153;&#65292;&#21253;&#25324;&#38452;&#35851;&#29702;&#35770;&#26816;&#27979;&#12289;&#29702;&#35770;&#31867;&#22411;&#20998;&#31867;&#21644;&#30456;&#20851;&#25991;&#26412;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#22909;&#22788;&#21644;&#20260;&#23475;&#12290;&#21518;&#32773;&#30340;&#19968;&#20010;&#20027;&#35201;&#20363;&#23376;&#26159;&#35823;&#23548;&#20449;&#24687;&#65292;&#21253;&#25324;&#20805;&#26021;&#32593;&#32476;&#30340;&#38452;&#35851;&#29702;&#35770;&#12290; &#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#25552;&#39640;&#20102;&#20934;&#30830;&#26816;&#27979;&#35823;&#23548;&#20449;&#24687;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;LLM&#30340;&#38452;&#35851;&#29702;&#35770;&#26816;&#27979;&#26041;&#27861;&#20165;&#19987;&#27880;&#20110;&#20108;&#20803;&#20998;&#31867;&#65292;&#24182;&#26410;&#32771;&#34385;&#35823;&#23548;&#20449;&#24687;&#19982;&#24773;&#24863;&#29305;&#24449;&#65288;&#21363;&#24773;&#24863;&#21644;&#24773;&#32490;&#65289;&#20043;&#38388;&#30340;&#37325;&#35201;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#25581;&#31034;&#20854;&#29420;&#29305;&#24773;&#24863;&#29305;&#24449;&#30340;&#38452;&#35851;&#25991;&#26412;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConspEmoLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38598;&#25104;&#24773;&#24863;&#20449;&#24687;&#19988;&#33021;&#22815;&#25191;&#34892;&#28041;&#21450;&#38452;&#35851;&#29702;&#35770;&#30340;&#22810;&#26679;&#20219;&#21153;&#30340;&#24320;&#28304;LLM&#12290; &#36825;&#20123;&#20219;&#21153;&#19981;&#20165;&#21253;&#25324;&#38452;&#35851;&#29702;&#35770;&#26816;&#27979;&#65292;&#36824;&#21253;&#25324;&#29702;&#35770;&#31867;&#22411;&#20998;&#31867;&#21644;&#30456;&#20851;&#25991;&#26412;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06765v1 Announce Type: new  Abstract: The internet has brought both benefits and harms to society. A prime example of the latter is misinformation, including conspiracy theories, which flood the web. Recent advances in natural language processing, particularly the emergence of large language models (LLMs), have improved the prospects of accurate misinformation detection. However, most LLM-based approaches to conspiracy theory detection focus only on binary classification and fail to account for the important relationship between misinformation and affective features (i.e., sentiment and emotions). Driven by a comprehensive analysis of conspiracy text that reveals its distinctive affective features, we propose ConspEmoLLM, the first open-source LLM that integrates affective information and is able to perform diverse tasks relating to conspiracy theories. These tasks include not only conspiracy theory detection, but also classification of theory type and detection of related d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;BioELQA&#65292;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#30475;&#20316;&#26159;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65292;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23454;&#20307;&#38142;&#25509;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15189</link><description>&lt;p&gt;
&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#35270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Biomedical Entity Linking as Multiple Choice Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15189
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;BioELQA&#65292;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#30475;&#20316;&#26159;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65292;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23454;&#20307;&#38142;&#25509;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#65288;BioEL&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#32454;&#31890;&#24230;&#21644;&#38271;&#23614;&#23454;&#20307;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BioELQA&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#35270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;BioELQA&#39318;&#20808;&#21033;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23558;&#25552;&#21450;&#21644;&#20505;&#36873;&#23454;&#20307;&#20849;&#21516;&#21576;&#29616;&#32473;&#29983;&#25104;&#22120;&#65292;&#28982;&#21518;&#36755;&#20986;&#19982;&#20854;&#36873;&#23450;&#23454;&#20307;&#30456;&#20851;&#30340;&#39044;&#27979;&#31526;&#21495;&#12290;&#36825;&#31181;&#20844;&#24335;&#20351;&#24471;&#19981;&#21516;&#20505;&#36873;&#23454;&#20307;&#20043;&#38388;&#30340;&#26126;&#30830;&#27604;&#36739;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#25429;&#25417;&#20102;&#25552;&#21450;&#21644;&#23454;&#20307;&#20043;&#38388;&#20197;&#21450;&#23454;&#20307;&#20043;&#38388;&#30340;&#31934;&#32454;&#20132;&#20114;&#12290;&#20026;&#20102;&#25913;&#21892;&#38271;&#23614;&#23454;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#26816;&#32034;&#30456;&#20284;&#30340;&#24050;&#26631;&#35760;&#35757;&#32451;&#23454;&#20363;&#20316;&#20026;&#32447;&#32034;&#65292;&#24182;&#23558;&#36755;&#20837;&#19982;&#26816;&#32034;&#23454;&#20363;&#36830;&#25509;&#21040;&#29983;&#25104;&#22120;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BioELQA&#30340;&#34920;&#29616;&#20248;&#20110;&#32479;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15189v1 Announce Type: cross  Abstract: Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;TMPT&#26694;&#26550;&#22312;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14298</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#65306;&#26032;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Stance Detection: New Datasets and Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;TMPT&#26694;&#26550;&#22312;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20013;&#35782;&#21035;&#38024;&#23545;&#29305;&#23450;&#30446;&#26631;&#30340;&#20844;&#20247;&#24847;&#35265;&#12290;&#20197;&#24448;&#30340;&#31435;&#22330;&#26816;&#27979;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#32431;&#25991;&#26412;&#19978;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25512;&#25991;&#30340;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#65292;&#36825;&#22312;&#24403;&#20170;&#24555;&#36895;&#22686;&#38271;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#20154;&#20204;&#32463;&#24120;&#21457;&#24067;&#22810;&#27169;&#24335;&#28040;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;Twitter&#21019;&#24314;&#20102;&#20116;&#20010;&#26032;&#30340;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#31034;&#20363;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#30446;&#26631;&#22810;&#27169;&#24335;&#25552;&#31034;&#35843;&#25972;&#65288;TMPT&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21033;&#29992;&#30446;&#26631;&#20449;&#24687;&#20174;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#23398;&#20064;&#22810;&#27169;&#24335;&#31435;&#22330;&#29305;&#24449;&#12290;&#23545;&#25105;&#20204;&#30340;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;TMPT&#22312;&#22810;&#27169;&#24335;&#31435;&#22330;&#26816;&#27979;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14298v1 Announce Type: new  Abstract: Stance detection is a challenging task that aims to identify public opinion from social media platforms with respect to specific targets. Previous work on stance detection largely focused on pure texts. In this paper, we study multi-modal stance detection for tweets consisting of texts and images, which are prevalent in today's fast-growing social media platforms where people often post multi-modal messages. To this end, we create five new multi-modal stance detection datasets of different domains based on Twitter, in which each example consists of a text and an image. In addition, we propose a simple yet effective Targeted Multi-modal Prompt Tuning framework (TMPT), where target information is leveraged to learn multi-modal stance features from textual and visual modalities. Experimental results on our three benchmark datasets show that the proposed TMPT achieves state-of-the-art performance in multi-modal stance detection.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#35821;&#38899;&#32763;&#35793;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#20026;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20294;&#30446;&#21069;&#21508;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#35774;&#32622;&#22810;&#26679;&#24615;&#38459;&#30861;&#20102;&#30830;&#23450;&#27599;&#20010;&#26550;&#26500;&#26500;&#24314;&#22359;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.12025</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#38899;&#32763;&#35793;&#65306;&#23384;&#22312;&#21644;&#32570;&#22833;&#30340;&#20869;&#23481;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12025
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#35821;&#38899;&#32763;&#35793;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#20026;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20294;&#30446;&#21069;&#21508;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#35774;&#32622;&#22810;&#26679;&#24615;&#38459;&#30861;&#20102;&#30830;&#23450;&#27599;&#20010;&#26550;&#26500;&#26500;&#24314;&#22359;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#26368;&#36817;&#21457;&#29983;&#20102;&#19968;&#22330;&#21464;&#38761;&#24615;&#30340;&#36716;&#21464;&#65292;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#24443;&#24213;&#25913;&#21464;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;NLP&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#36825;&#31181;&#33539;&#24335;&#24050;&#32463;&#25193;&#23637;&#21040;&#20854;&#20182;&#24418;&#24335;&#65292;&#21253;&#25324;&#35821;&#38899;&#65292;&#22312;&#37027;&#37324;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#31215;&#26497;&#25506;&#32034;&#23558;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65288;SFMs&#65289;&#21644;LLMs&#32467;&#21512;&#25104;&#21333;&#19968;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#26412;&#25991;&#30528;&#37325;&#20110;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#65288;ST&#65289;&#12290;&#36890;&#36807;&#23457;&#26597;&#35813;&#20027;&#39064;&#19978;&#21457;&#34920;&#30340;&#35770;&#25991;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36804;&#20170;&#20026;&#27490;&#25552;&#20986;&#30340;&#26550;&#26500;&#35299;&#20915;&#26041;&#26696;&#21644;&#35757;&#32451;&#31574;&#30053;&#30340;&#32479;&#19968;&#35266;&#28857;&#65292;&#24378;&#35843;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#21644;&#24046;&#24322;&#20043;&#22788;&#12290;&#22522;&#20110;&#36825;&#19968;&#30740;&#31350;&#65292;&#25105;&#20204;&#19981;&#20165;&#25972;&#29702;&#20102;&#25152;&#23398;&#21040;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#36824;&#23637;&#31034;&#20102;&#22810;&#26679;&#21270;&#30340;&#35774;&#32622;&#21644;&#35780;&#20272;&#26041;&#27861;&#22914;&#20309;&#38459;&#30861;&#23545;&#27599;&#20010;&#26550;&#26500;&#26500;&#24314;&#22359;&#30340;&#26368;&#20339;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12025v1 Announce Type: new  Abstract: The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block 
&lt;/p&gt;</description></item><item><title>GeoEval&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;MMs&#22312;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;WizardMath&#27169;&#22411;&#22312;&#20027;&#35201;&#23376;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#19978;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.10104</link><description>&lt;p&gt;
GeoEval&#65306;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;&#22810;&#27169;&#22411;&#22312;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10104
&lt;/p&gt;
&lt;p&gt;
GeoEval&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;MMs&#22312;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;WizardMath&#27169;&#22411;&#22312;&#20027;&#35201;&#23376;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#19978;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22810;&#27169;&#22411;&#65288;MMs&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#20960;&#20309;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#21363;&#38656;&#35201;&#32508;&#21512;&#29702;&#35299;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#35780;&#20272;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;GeoEval&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#38598;&#21512;&#65292;&#21253;&#25324;&#19968;&#20010;&#20027;&#35201;&#23376;&#38598;&#21512;&#30340;2000&#20010;&#38382;&#39064;&#65292;&#19968;&#20010;&#37325;&#28857;&#20851;&#27880;&#21453;&#25512;&#29702;&#30340;750&#20010;&#38382;&#39064;&#23376;&#38598;&#21512;&#65292;&#19968;&#20010;&#22686;&#24378;&#23376;&#38598;&#21512;&#30340;2000&#20010;&#38382;&#39064;&#20197;&#21450;&#19968;&#20010;&#38590;&#39064;&#23376;&#38598;&#21512;&#30340;300&#20010;&#38382;&#39064;&#12290;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;LLMs&#21644;MMs&#22312;&#35299;&#20915;&#20960;&#20309;&#25968;&#23398;&#38382;&#39064;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#21313;&#20010;LLMs&#21644;MMs&#22312;&#36825;&#20123;&#19981;&#21516;&#23376;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;WizardMath&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#20027;&#35201;&#23376;&#38598;&#19978;&#36798;&#21040;55.67%&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#19978;&#21482;&#26377;6.00%&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#31361;&#20986;&#20102;&#20851;&#38190;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10104v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) and Multi-Modal Models (MMs) have demonstrated their remarkable capabilities in problem-solving. Yet, their proficiency in tackling geometry math problems, which necessitates an integrated understanding of both textual and visual information, has not been thoroughly evaluated. To address this gap, we introduce the GeoEval benchmark, a comprehensive collection that includes a main subset of 2000 problems, a 750 problem subset focusing on backward reasoning, an augmented subset of 2000 problems, and a hard subset of 300 problems. This benchmark facilitates a deeper investigation into the performance of LLMs and MMs on solving geometry math problems. Our evaluation of ten LLMs and MMs across these varied subsets reveals that the WizardMath model excels, achieving a 55.67\% accuracy rate on the main subset but only a 6.00\% accuracy on the challenging subset. This highlights the critical ne
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#27880;&#24847;&#21147;&#23618;&#20855;&#26377;&#36739;&#39640;&#30340;&#35789;&#25935;&#24863;&#24615;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;transformers&#30340;&#25104;&#21151;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2402.02969</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#23618;&#30340;&#35789;&#25935;&#24863;&#24615;&#30340;&#29702;&#35299;&#65306;&#36890;&#36807;&#38543;&#26426;&#29305;&#24449;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02969
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#27880;&#24847;&#21147;&#23618;&#20855;&#26377;&#36739;&#39640;&#30340;&#35789;&#25935;&#24863;&#24615;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;transformers&#30340;&#25104;&#21151;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;transformers&#24322;&#24120;&#25104;&#21151;&#32972;&#21518;&#21407;&#22240;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#20026;&#20160;&#20040;&#27880;&#24847;&#21147;&#23618;&#36866;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20123;&#20219;&#21153;&#35201;&#27714;&#39044;&#27979;&#27169;&#22411;&#25429;&#25417;&#19978;&#19979;&#25991;&#21547;&#20041;&#65292;&#21363;&#20351;&#21477;&#23376;&#24456;&#38271;&#65292;&#36825;&#24448;&#24448;&#21462;&#20915;&#20110;&#19968;&#20010;&#25110;&#20960;&#20010;&#35789;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#38543;&#26426;&#29305;&#24449;&#30340;&#20856;&#22411;&#35774;&#32622;&#20013;&#30740;&#31350;&#20102;&#36825;&#19968;&#20851;&#38190;&#23646;&#24615;&#65292;&#31216;&#20026;&#35789;&#25935;&#24863;&#24615;&#65288;WS&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27880;&#24847;&#21147;&#23618;&#20855;&#26377;&#36739;&#39640;&#30340;WS&#65292;&#21363;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23384;&#22312;&#19968;&#20010;&#21521;&#37327;&#65292;&#33021;&#22815;&#22823;&#24133;&#25200;&#21160;&#38543;&#26426;&#27880;&#24847;&#21147;&#29305;&#24449;&#26144;&#23556;&#12290;&#36825;&#20010;&#35770;&#28857;&#20851;&#38190;&#22320;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;softmax&#30340;&#20316;&#29992;&#65292;&#31361;&#26174;&#20102;&#23427;&#30456;&#23545;&#20110;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#65289;&#30340;&#20248;&#21183;&#12290;&#30456;&#21453;&#65292;&#26631;&#20934;&#38543;&#26426;&#29305;&#24449;&#30340;WS&#26159;$1/\sqrt{n}$&#38454;&#30340;&#65292;$n$&#26159;&#25991;&#26412;&#26679;&#26412;&#20013;&#30340;&#21333;&#35789;&#25968;&#65292;&#22240;&#27492;&#23427;&#38543;&#19978;&#19979;&#25991;&#30340;&#38271;&#24230;&#32780;&#34928;&#20943;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#20851;&#20110;&#35789;&#25935;&#24863;&#24615;&#30340;&#32467;&#26524;&#36716;&#21270;&#20026;&#27867;&#21270;&#30028;&#65306;&#30001;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Unveiling the reasons behind the exceptional success of transformers requires a better understanding of why attention layers are suitable for NLP tasks. In particular, such tasks require predictive models to capture contextual meaning which often depends on one or few words, even if the sentence is long. Our work studies this key property, dubbed word sensitivity (WS), in the prototypical setting of random features. We show that attention layers enjoy high WS, namely, there exists a vector in the space of embeddings that largely perturbs the random attention features map. The argument critically exploits the role of the softmax in the attention layer, highlighting its benefit compared to other activations (e.g., ReLU). In contrast, the WS of standard random features is of order $1/\sqrt{n}$, $n$ being the number of words in the textual sample, and thus it decays with the length of the context. We then translate these results on the word sensitivity into generalization bounds: due to th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PipeNet&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#20462;&#21098;&#25216;&#26415;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#32852;-&#20462;&#21098;-&#25512;&#29702;&#30340;&#27969;&#31243;&#26469;&#20462;&#21098;&#22122;&#22768;&#33410;&#28857;&#65292;&#20197;&#25552;&#39640;&#22270;&#25512;&#29702;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#33719;&#24471;&#33391;&#22909;&#30340;&#23376;&#22270;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.17536</link><description>&lt;p&gt;
PipeNet:&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#20351;&#29992;&#35821;&#20041;&#20462;&#21098;&#30340;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
PipeNet: Question Answering with Semantic Pruning over Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PipeNet&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#20462;&#21098;&#25216;&#26415;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#32852;-&#20462;&#21098;-&#25512;&#29702;&#30340;&#27969;&#31243;&#26469;&#20462;&#21098;&#22122;&#22768;&#33410;&#28857;&#65292;&#20197;&#25552;&#39640;&#22270;&#25512;&#29702;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#33719;&#24471;&#33391;&#22909;&#30340;&#23376;&#22270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#22312;&#38382;&#39064;&#22238;&#31572;&#20013;&#24341;&#20837;&#26174;&#24335;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#21487;&#20197;&#24102;&#26469;&#22909;&#22788;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#36981;&#24490;&#19968;&#20010;&#22522;&#30784;&#25512;&#29702;&#27969;&#31243;&#65292;&#22312;&#35813;&#27969;&#31243;&#20013;&#65292;&#39318;&#20808;&#23558;&#23454;&#20307;&#33410;&#28857;&#19982;&#26597;&#35810;(&#38382;&#39064;&#21644;&#20505;&#36873;&#31572;&#26696;)&#36827;&#34892;&#20851;&#32852;&#65292;&#28982;&#21518;&#20351;&#29992;&#25512;&#29702;&#27169;&#22359;&#23545;&#21305;&#37197;&#30340;&#22810;&#36339;&#23376;&#22270;&#36827;&#34892;&#25512;&#29702;&#65292;&#29992;&#20110;&#39044;&#27979;&#31572;&#26696;&#12290;&#34429;&#28982;&#36825;&#20010;&#27969;&#31243;&#22312;&#20174;&#24222;&#22823;&#30340;KG&#20013;&#25552;&#21462;&#24517;&#35201;&#20449;&#24687;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#21892;&#65292;&#20294;&#22312;&#25918;&#22823;&#20851;&#32852;&#30340;&#23376;&#22270;&#26102;&#65292;&#25928;&#29575;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#25214;&#21040;&#23376;&#22270;&#20013;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#23454;&#20307;&#33410;&#28857;&#65292;&#20197;&#25552;&#39640;&#20351;&#29992;KG&#36827;&#34892;&#22270;&#25512;&#29702;&#26102;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20851;&#32852;-&#20462;&#21098;-&#25512;&#29702;&#30340;&#27969;&#31243;&#26469;&#20462;&#21098;&#22122;&#22768;&#33410;&#28857;&#65292;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#33719;&#24471;&#33391;&#22909;&#30340;&#23376;&#22270;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20462;&#21098;&#27169;&#22359;&#39318;&#20808;&#26681;&#25454;&#21305;&#37197;&#33539;&#22260;&#20043;&#38388;&#30340;&#20381;&#36182;&#36317;&#31163;&#23545;&#27010;&#24565;&#33410;&#28857;&#36827;&#34892;&#35780;&#20998;&#65292;&#28982;&#21518;&#23545;&#20854;&#36827;&#34892;&#20462;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well acknowledged that incorporating explicit knowledge graphs (KGs) can benefit question answering. Existing approaches typically follow a grounding-reasoning pipeline in which entity nodes are first grounded for the query (question and candidate answers), and then a reasoning module reasons over the matched multi-hop subgraph for answer prediction. Although the pipeline largely alleviates the issue of extracting essential information from giant KGs, efficiency is still an open challenge when scaling up hops in grounding the subgraphs. In this paper, we target at finding semantically related entity nodes in the subgraph to improve the efficiency of graph reasoning with KG. We propose a grounding-pruning-reasoning pipeline to prune noisy nodes, remarkably reducing the computation cost and memory usage while also obtaining decent subgraph representation. In detail, the pruning module first scores concept nodes based on the dependency distance between matched spans and then prunes 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20026;&#20363;&#65292;&#21457;&#24067;&#22312;GitHub&#21644;Hugging Face&#19978;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.16640</link><description>&lt;p&gt;
TeenyTinyLlama&#65306;&#22522;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35757;&#32451;&#30340;&#24320;&#28304;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese. (arXiv:2401.16640v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16640
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20026;&#20363;&#65292;&#21457;&#24067;&#22312;GitHub&#21644;Hugging Face&#19978;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#30340;&#36827;&#23637;&#36824;&#19981;&#24179;&#34913;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;LLMs&#26159;&#22312;&#20687;&#33521;&#35821;&#36825;&#26679;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#65292;&#20294;&#22810;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#31245;&#24046;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#22810;&#35821;&#35328;&#22522;&#30784;&#26377;&#26102;&#20250;&#38480;&#21046;&#23427;&#20204;&#20135;&#29983;&#30340;&#21103;&#20135;&#21697;&#65292;&#22914;&#35745;&#31639;&#38656;&#27714;&#21644;&#35768;&#21487;&#21046;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#20026;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20351;&#29992;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#36807;&#31243;&#12289;&#20854;&#23616;&#38480;&#24615;&#21644;&#20248;&#21183;&#12290;&#36825;&#23601;&#26159;TeenyTinyLlama&#65306;&#20004;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#29983;&#25104;&#30340;&#32039;&#20945;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;GitHub&#21644;Hugging Face&#19978;&#20197;&#23485;&#26494;&#30340;Apache 2.0&#35768;&#21487;&#35777;&#21457;&#24067;&#23427;&#20204;&#65292;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;&#35814;&#35265;https://github.com/Nkluge-correa/TeenyTinyLlama
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced natural language processing, but their progress has yet to be equal across languages. While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes. In this study, we document the development of open-foundation models tailored for use in low-resource settings, their limitations, and their benefits. This is the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation. We release them under the permissive Apache 2.0 license on GitHub and Hugging Face for community use and further development. See https://github.com/Nkluge-correa/TeenyTinyLlama
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STG-LLM&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#26102;&#31354;&#25968;&#25454;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;STG-Tokenizer&#23558;&#22797;&#26434;&#30340;&#22270;&#24418;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#26631;&#35760;&#65292;&#20877;&#36890;&#36807;STG-Adapter&#23558;&#26631;&#35760;&#21270;&#25968;&#25454;&#19982;LLM&#30340;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#36830;&#25509;&#12290;&#36890;&#36807;&#24494;&#35843;&#21442;&#25968;&#65292;STG-LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#25226;&#25569;&#26631;&#35760;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#20445;&#30041;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;STG-LLM&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14192</link><description>&lt;p&gt;
&#22914;&#20309;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#26102;&#31354;&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Can Large Language Models Understand Spatial-Temporal Data?. (arXiv:2401.14192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STG-LLM&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#26102;&#31354;&#25968;&#25454;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;STG-Tokenizer&#23558;&#22797;&#26434;&#30340;&#22270;&#24418;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#26631;&#35760;&#65292;&#20877;&#36890;&#36807;STG-Adapter&#23558;&#26631;&#35760;&#21270;&#25968;&#25454;&#19982;LLM&#30340;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#36830;&#25509;&#12290;&#36890;&#36807;&#24494;&#35843;&#21442;&#25968;&#65292;STG-LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#25226;&#25569;&#26631;&#35760;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#20445;&#30041;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;STG-LLM&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#20219;&#21153;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#21033;&#29992;&#23427;&#20204;&#30340;&#33021;&#21147;&#36827;&#34892;&#26102;&#31354;&#39044;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26102;&#24207;&#25991;&#26412;&#19982;&#22797;&#26434;&#30340;&#26102;&#31354;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#38459;&#30861;&#20102;&#35813;&#24212;&#29992;&#30340;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;STG-LLM&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20026;LLM&#36171;&#20104;&#20102;&#26102;&#31354;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#35299;&#20915;&#25968;&#25454;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65306;1&#65289;STG-Tokenizer&#65306;&#36825;&#20010;&#26102;&#31354;&#22270;&#24418;&#26631;&#35760;&#22120;&#23558;&#22797;&#26434;&#30340;&#22270;&#24418;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#26631;&#35760;&#65292;&#25429;&#25417;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65307;2&#65289;STG-Adapter&#65306;&#36825;&#20010;&#31934;&#31616;&#30340;&#36866;&#37197;&#22120;&#30001;&#32447;&#24615;&#32534;&#30721;&#21644;&#35299;&#30721;&#23618;&#32452;&#25104;&#65292;&#22635;&#34917;&#20102;&#26631;&#35760;&#21270;&#25968;&#25454;&#21644;LLM&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#20165;&#24494;&#35843;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#25226;&#25569;STG-Tokenizer&#29983;&#25104;&#30340;&#26631;&#35760;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#20445;&#30041;LLM&#30340;&#21407;&#22987;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;STG-LLM&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) dominate tasks like natural language processing and computer vision, harnessing their power for spatial-temporal forecasting remains challenging. The disparity between sequential text and complex spatial-temporal data hinders this application. To address this issue, this paper introduces STG-LLM, an innovative approach empowering LLMs for spatial-temporal forecasting. We tackle the data mismatch by proposing: 1) STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph data into concise tokens capturing both spatial and temporal relationships; 2) STG-Adapter: This minimalistic adapter, consisting of linear encoding and decoding layers, bridges the gap between tokenized data and LLM comprehension. By fine-tuning only a small set of parameters, it can effectively grasp the semantics of tokens generated by STG-Tokenizer, while preserving the original natural language understanding capabilities of LLMs. Extensive experiments on diver
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.07927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;?
&lt;/p&gt;
&lt;p&gt;
Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07927
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#33021;&#22815;&#25552;&#20379;&#20854;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#23545;&#20844;&#20247;&#26159;&#30452;&#25509;&#21487;&#35775;&#38382;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#36825;&#26679;&#30340;&#39118;&#38505;&#65292;&#21363;&#20196;&#20154;&#20449;&#26381;&#20294;&#38169;&#35823;&#30340;&#35299;&#37322;&#21487;&#33021;&#23548;&#33268;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25903;&#25745;&#30340;&#33258;&#20449;&#12290;&#22240;&#27492;&#65292;&#35299;&#37322;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#26159;AI&#23433;&#20840;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#35780;&#20272;&#33258;&#25105;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#36807;&#20110;&#22797;&#26434;&#65292;&#26080;&#27861;&#27880;&#37322;&#20160;&#20040;&#26159;&#27491;&#30830;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#21487;&#38752;&#24615;&#30340;&#34913;&#37327;&#25351;&#26631;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35828;&#26576;&#32452;&#35789;&#23545;&#20110;&#20570;&#20986;&#39044;&#27979;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#22312;&#27809;&#26377;&#36825;&#20123;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#24212;&#35813;&#26080;&#27861;&#20570;&#20986;&#30456;&#21516;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#33258;&#27965;&#24615;&#26816;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21487;&#38752;&#24615;&#26041;&#27861;&#65292;&#20294;&#20043;&#21069;&#23578;&#26410;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#23558;&#33258;&#27965;&#24615;&#26816;&#27979;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SemCSR&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#33021;&#21147;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#23601;&#33021;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26679;&#24335;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.10962</link><description>&lt;p&gt;
&#24102;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#24863;&#30693;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models. (arXiv:2310.10962v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SemCSR&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#33021;&#21147;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#23601;&#33021;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26679;&#24335;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#23398;&#20064;&#26356;&#22909;&#30340;&#21477;&#23376;&#34920;&#31034;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#35757;&#32451;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#38656;&#35201;&#22823;&#37327;&#24102;&#26631;&#31614;&#30340;&#21477;&#23376;&#26469;&#26126;&#30830;&#26500;&#24314;&#27491;&#36127;&#23545;&#65288;&#20363;&#22914;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#65289;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#33719;&#21462;&#36275;&#22815;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#26082;&#36153;&#26102;&#21448;&#32791;&#36164;&#28304;&#65292;&#22240;&#27492;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20851;&#27880;&#24320;&#21457;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#30001;&#20110;&#36825;&#20123;&#38750;&#32467;&#26500;&#21270;&#30340;&#38543;&#26426;&#25277;&#26679;&#21477;&#23376;&#20043;&#38388;&#27809;&#26377;&#26126;&#30830;&#30340;&#20851;&#32852;&#65292;&#26500;&#24314;&#27491;&#36127;&#23545;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#21644;&#26377;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#26694;&#26550;&#65288;SemCSR&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#33021;&#21147;&#65292;&#25105;&#20204;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26679;&#24335;&#35821;&#26009;&#24211;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20154;&#24037;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has been proven to be effective in learning better sentence representations. However, to train a contrastive learning model, large numbers of labeled sentences are required to construct positive and negative pairs explicitly, such as those in natural language inference (NLI) datasets. Unfortunately, acquiring sufficient high-quality labeled data can be both time-consuming and resource-intensive, leading researchers to focus on developing methods for learning unsupervised sentence representations. As there is no clear relationship between these unstructured randomly-sampled sentences, building positive and negative pairs over them is tricky and problematic. To tackle these challenges, in this paper, we propose SemCSR, a semantic-aware contrastive sentence representation framework. By leveraging the generation and evaluation capabilities of large language models (LLMs), we can automatically construct a high-quality NLI-style corpus without any human annotation, and f
&lt;/p&gt;</description></item><item><title>EasyGen&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23481;&#26131;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;&#30456;&#27604;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;EasyGen&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292; &#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#65292;&#24182;&#19988;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#65292;&#36824;&#33021;&#22815;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.08949</link><description>&lt;p&gt;
Easier Multimodal Generation: Diffusion Models Meet LLMs
&lt;/p&gt;
&lt;p&gt;
Making Multimodal Generation Easier: When Diffusion Models Meet LLMs. (arXiv:2310.08949v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08949
&lt;/p&gt;
&lt;p&gt;
EasyGen&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23481;&#26131;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;&#30456;&#27604;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;EasyGen&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292; &#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#65292;&#24182;&#19988;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#65292;&#36824;&#33021;&#22815;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EasyGen&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#20027;&#35201;&#20381;&#36182;&#20110;&#32534;&#30721;&#22120;&#22914;CLIP&#25110;ImageBind&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#26469;&#26725;&#25509;&#27169;&#24577;&#20043;&#38388;&#24046;&#36317;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;EasyGen&#22522;&#20110;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#65292;&#20419;&#36827;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#12290;EasyGen&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#23618;&#23558;BiDiffuser&#21644;LLM&#36827;&#34892;&#38598;&#25104;&#65292;&#22788;&#29702;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38480;&#20110;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#19981;&#21516;&#65292;EasyGen&#36824;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;LLM&#21019;&#24314;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#30001;BiDiffuser&#35299;&#37322;&#29983;&#25104;&#36866;&#24403;&#30340;&#35270;&#35273;&#22238;&#22797;&#26469;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#20102;EasyGen&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#35757;&#32451;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge the gap between modalities, EasyGen is built upon a bidirectional conditional diffusion model named BiDiffuser, which promotes more efficient interactions between modalities. EasyGen handles image-to-text generation by integrating BiDiffuser and an LLM via a simple projection layer. Unlike most existing multimodal models that are limited to generating text responses, EasyGen can also facilitate text-to-image generation by leveraging the LLM to create textual descriptions, which can be interpreted by BiDiffuser to generate appropriate visual responses. Extensive quantitative and qualitative experiments demonstrate the effectiveness of EasyGen, whose training can b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;LM&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#21644;&#24320;&#21457;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27979;&#35797;LM&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#21644;&#23547;&#25214;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15817</link><description>&lt;p&gt;
&#20351;&#29992;LM&#27169;&#25311;&#27801;&#30418;&#35782;&#21035;LM&#20195;&#29702;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Identifying the Risks of LM Agents with an LM-Emulated Sandbox. (arXiv:2309.15817v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15817
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;LM&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#21644;&#24320;&#21457;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27979;&#35797;LM&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#21644;&#23547;&#25214;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20195;&#29702;&#21644;&#24037;&#20855;&#20351;&#29992;&#30340;&#25216;&#26415;&#36827;&#27493;&#65292;&#20363;&#22914;ChatGPT&#25554;&#20214;&#65292;&#20351;&#24471;&#20195;&#29702;&#20855;&#22791;&#20102;&#20016;&#23500;&#30340;&#21151;&#33021;&#65292;&#20294;&#20063;&#25918;&#22823;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#22914;&#27844;&#38706;&#31169;&#20154;&#25968;&#25454;&#25110;&#24341;&#21457;&#36130;&#21153;&#25439;&#22833;&#12290;&#35782;&#21035;&#36825;&#20123;&#39118;&#38505;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#24037;&#20316;&#65292;&#38656;&#35201;&#23454;&#26045;&#24037;&#20855;&#65292;&#25163;&#21160;&#35774;&#32622;&#27599;&#20010;&#27979;&#35797;&#22330;&#26223;&#30340;&#29615;&#22659;&#65292;&#24182;&#25214;&#21040;&#39118;&#38505;&#26696;&#20363;&#12290;&#38543;&#30528;&#24037;&#20855;&#21644;&#20195;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#27979;&#35797;&#36825;&#20123;&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#23558;&#20351;&#23547;&#25214;&#39640;&#39118;&#38505;&#12289;&#38271;&#23614;&#39118;&#38505;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ToolEmu&#65306;&#19968;&#20010;&#20351;&#29992;LM&#26469;&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#23454;&#20363;&#21270;&#30340;&#24773;&#20917;&#19979;&#23545;LM&#20195;&#29702;&#36827;&#34892;&#21508;&#31181;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#27979;&#35797;&#12290;&#38500;&#20102;&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#29992;&#20110;&#26816;&#26597;&#20195;&#29702;&#30340;&#22833;&#36133;&#24182;&#37327;&#21270;&#30456;&#20851;&#39118;&#38505;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#27979;&#35797;&#20102;&#24037;&#20855;&#27169;&#25311;&#22120;&#21644;&#35780;&#20272;&#22120;&#65292;&#24182;&#21457;&#29616;&#20102;6&#20010;...
&lt;/p&gt;
&lt;p&gt;
Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, manually setting up the environment for each test scenario, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios, without manual instantiation. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 6
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.10621</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10621
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24615;&#26631;&#31614;&#26159;&#35780;&#20272;&#21644;&#20248;&#21270;&#25628;&#32034;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#33719;&#21462;&#22823;&#37327;&#30456;&#20851;&#24615;&#26631;&#31614;&#36890;&#24120;&#38656;&#35201;&#31532;&#19977;&#26041;&#26631;&#27880;&#20154;&#21592;&#65292;&#20294;&#23384;&#22312;&#20302;&#36136;&#37327;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#26631;&#31614;&#36136;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#20180;&#32454;&#21453;&#39304;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.05516</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;LLMs&#37327;&#21270;&#20013;&#30340;&#26435;&#37325;&#33293;&#20837;
&lt;/p&gt;
&lt;p&gt;
Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25191;&#34892;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;3&#20301;&#21644;4&#20301;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#38543;&#30528;&#20301;&#25968;&#30340;&#20943;&#23569;&#65292;&#37327;&#21270;&#32593;&#26684;&#21464;&#24471;&#26356;&#21152;&#23485;&#27867;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19978;&#19979;&#33293;&#20837;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#32454;&#35843;&#19978;&#19979;&#33293;&#20837;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21463;&#21046;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#31934;&#30830;&#19988;&#26377;&#38480;&#30340;&#36793;&#30028;&#65292;&#21482;&#26377;&#25913;&#21464;&#33293;&#20837;&#20540;&#30340;&#38408;&#20540;&#25165;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#39640;&#25928;&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;SignRound&#65292;&#23427;&#28041;&#21450;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#30340;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ANALOGYKB&#65292;&#19968;&#31181;&#20351;&#29992;&#30334;&#19975;&#35268;&#27169;&#30693;&#35782;&#24211;&#30340;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05994</link><description>&lt;p&gt;
ANALOGYKB&#65306;&#20351;&#29992;&#30334;&#19975;&#35268;&#27169;&#30693;&#35782;&#24211;&#24320;&#21551;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base. (arXiv:2305.05994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ANALOGYKB&#65292;&#19968;&#31181;&#20351;&#29992;&#30334;&#19975;&#35268;&#27169;&#30693;&#35782;&#24211;&#30340;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#25512;&#29702;&#26159;&#20154;&#31867;&#30340;&#19968;&#39033;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#27169;&#22411;&#35757;&#32451;&#36164;&#28304;&#65292;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#22312;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ANALOGYKB&#65292;&#36825;&#26159;&#19968;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;&#31867;&#27604;&#30693;&#35782;&#24211;&#65292;&#23427;&#30001;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#23548;&#20986;&#12290;ANALOGYKB&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#35782;&#21035;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#31867;&#27604;&#65306;1&#65289;&#30456;&#21516;&#20851;&#31995;&#30340;&#31867;&#27604;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25552;&#21462;&#65307;2&#65289;&#31867;&#20284;&#20851;&#31995;&#30340;&#31867;&#27604;&#65292;&#21017;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;InstructGPT&#65289;&#21551;&#29992;&#30340;&#36873;&#25321;&#21644;&#36807;&#28388;&#31649;&#36947;&#36827;&#34892;&#35782;&#21035;&#65292;&#20877;&#32463;&#36807;&#23569;&#37327;&#20154;&#24037;&#36136;&#37327;&#25511;&#21046;&#12290;&#22312;&#20004;&#20010;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#65288;&#31867;&#27604;&#35782;&#21035;&#21644;&#29983;&#25104;&#65289;&#30340;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;ANALOGYKB&#25104;&#21151;&#22320;&#20351;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large LMs (InstructGPT), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables LMs to achieve much better results than previous state-of-the-art methods.
&lt;/p&gt;</description></item></channel></rss>