<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35270;&#21270;&#30740;&#31350;&#26463;&#25628;&#32034;&#26641;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25361;&#25112;&#12290;&#36890;&#36807;&#20840;&#38754;&#26816;&#26597;&#27169;&#22411;&#36755;&#20986;&#65292;&#21253;&#25324;&#20505;&#36873;&#36755;&#20986;&#21644;&#23545;&#24212;&#27010;&#29575;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19982;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#24182;&#32473;&#20986;&#20102;&#20116;&#20010;&#35814;&#32454;&#30340;&#20998;&#26512;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.11252</link><description>&lt;p&gt;
&#25581;&#31034;&#19981;&#21487;&#35328;&#35828;&#20043;&#20107;&#65306;&#36890;&#36807;&#21487;&#35270;&#21270;&#30740;&#31350;&#26463;&#25628;&#32034;&#26641;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges. (arXiv:2310.11252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35270;&#21270;&#30740;&#31350;&#26463;&#25628;&#32034;&#26641;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25361;&#25112;&#12290;&#36890;&#36807;&#20840;&#38754;&#26816;&#26597;&#27169;&#22411;&#36755;&#20986;&#65292;&#21253;&#25324;&#20505;&#36873;&#36755;&#20986;&#21644;&#23545;&#24212;&#27010;&#29575;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19982;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#24182;&#32473;&#20986;&#20102;&#20116;&#20010;&#35814;&#32454;&#30340;&#20998;&#26512;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#21450;&#22686;&#21152;&#20102;&#23545;&#20132;&#20114;&#24335;&#26041;&#27861;&#20197;&#24341;&#23548;&#27169;&#22411;&#36755;&#20986;&#30340;&#20852;&#36259;&#12290;&#25552;&#31034;&#32454;&#21270;&#34987;&#35748;&#20026;&#26159;&#36825;&#20123;&#26041;&#27861;&#20013;&#24433;&#21709;&#36755;&#20986;&#26368;&#26377;&#25928;&#30340;&#25163;&#27573;&#20043;&#19968;&#12290;&#25105;&#20204;&#35782;&#21035;&#20986;&#19982;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#21644;&#27169;&#22411;&#29305;&#23450;&#12289;&#35821;&#35328;&#21644;&#31038;&#20250;&#35821;&#35328;&#23398;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#38656;&#35201;&#23545;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#20840;&#38754;&#30340;&#26816;&#26597;&#65292;&#21253;&#25324;&#20505;&#36873;&#36755;&#20986;&#21644;&#23545;&#24212;&#27010;&#29575;&#12290;&#26463;&#25628;&#32034;&#26641;&#20316;&#20026;&#19968;&#31181;&#26222;&#36941;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#25552;&#20379;&#36825;&#20123;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#26469;&#30740;&#31350;&#26463;&#25628;&#32034;&#26641;&#65292;&#26041;&#20415;&#20998;&#26512;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25152;&#20570;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#23450;&#37327;&#22320;&#23637;&#31034;&#20102;&#25581;&#31034;&#26463;&#25628;&#32034;&#26641;&#30340;&#20215;&#20540;&#65292;&#24182;&#25552;&#20986;&#20102;&#20116;&#20010;&#35814;&#32454;&#30340;&#20998;&#26512;&#22330;&#26223;&#26469;&#35299;&#20915;&#25152;&#35782;&#21035;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing popularity of generative language models has amplified interest in interactive methods to guide model outputs. Prompt refinement is considered one of the most effective means to influence output among these methods. We identify several challenges associated with prompting large language models, categorized into data- and model-specific, linguistic, and socio-linguistic challenges. A comprehensive examination of model outputs, including runner-up candidates and their corresponding probabilities, is needed to address these issues. The beam search tree, the prevalent algorithm to sample model outputs, can inherently supply this information. Consequently, we introduce an interactive visual method for investigating the beam search tree, facilitating analysis of the decisions made by the model during generation. We quantitatively show the value of exposing the beam search tree and present five detailed analysis scenarios addressing the identified challenges. Our methodology valid
&lt;/p&gt;</description></item><item><title>CrossCodeEval&#26159;&#19968;&#20010;&#22810;&#20803;&#21270;&#21644;&#22810;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#36328;&#25991;&#20214;&#20195;&#30721;&#34917;&#20840;&#65292;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#29702;&#35299;&#25165;&#33021;&#20934;&#30830;&#23436;&#25104;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.11248</link><description>&lt;p&gt;
CrossCodeEval: &#19968;&#20010;&#22810;&#20803;&#21270;&#21644;&#22810;&#35821;&#35328;&#30340;&#29992;&#20110;&#36328;&#25991;&#20214;&#20195;&#30721;&#34917;&#20840;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion. (arXiv:2310.11248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11248
&lt;/p&gt;
&lt;p&gt;
CrossCodeEval&#26159;&#19968;&#20010;&#22810;&#20803;&#21270;&#21644;&#22810;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#36328;&#25991;&#20214;&#20195;&#30721;&#34917;&#20840;&#65292;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#29702;&#35299;&#25165;&#33021;&#20934;&#30830;&#23436;&#25104;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#34917;&#20840;&#27169;&#22411;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#24403;&#21069;&#27969;&#34892;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#22914;HumanEval&#21644;MBPP&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#25991;&#20214;&#20869;&#30340;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#19978;&#12290;&#36825;&#31181;&#36807;&#20110;&#31616;&#21270;&#30340;&#35774;&#32622;&#26080;&#27861;&#20934;&#30830;&#22320;&#20195;&#34920;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#36719;&#20214;&#24320;&#21457;&#22330;&#26223;&#65292;&#20854;&#20013;&#23384;&#20648;&#24211;&#36328;&#36234;&#22810;&#20010;&#25991;&#20214;&#65292;&#23384;&#22312;&#22823;&#37327;&#30340;&#36328;&#25991;&#20214;&#20381;&#36182;&#20851;&#31995;&#65292;&#38656;&#35201;&#35775;&#38382;&#21644;&#29702;&#35299;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#25165;&#33021;&#27491;&#30830;&#23436;&#25104;&#20195;&#30721;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CrossCodeEval&#65292;&#19968;&#20010;&#22810;&#20803;&#21270;&#21644;&#22810;&#35821;&#35328;&#30340;&#20195;&#30721;&#34917;&#20840;&#22522;&#20934;&#27979;&#35797;&#65292;&#38656;&#35201;&#28145;&#20837;&#30340;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#29702;&#35299;&#25165;&#33021;&#20934;&#30830;&#23436;&#25104;&#20195;&#30721;&#12290;CrossCodeEval&#22522;&#20110;&#22235;&#31181;&#27969;&#34892;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;Python&#65292;Java&#65292;TypeScript&#21644;C#&#65289;&#20013;&#30340;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#12289;&#24320;&#28304;&#12289;&#26435;&#38480;&#35768;&#21487;&#30340;&#23384;&#20648;&#24211;&#38598;&#21512;&#26500;&#24314;&#12290;&#20026;&#20102;&#21019;&#24314;&#20005;&#26684;&#35201;&#27714;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#36827;&#34892;&#20934;&#30830;&#23436;&#25104;&#30340;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#38745;&#24577;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly.  To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23454;&#20307;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11244</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Entity Matching using Large Language Models. (arXiv:2310.11244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23454;&#20307;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21305;&#37197;&#26159;&#21028;&#26029;&#20004;&#20010;&#23454;&#20307;&#25551;&#36848;&#26159;&#21542;&#25351;&#30340;&#26159;&#21516;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#23454;&#20307;&#21305;&#37197;&#26159;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#20013;&#30340;&#26680;&#24515;&#27493;&#39588;&#65292;&#20063;&#26159;&#35768;&#22810;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#24212;&#29992;&#38656;&#35201;&#23558;&#26469;&#33258;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;&#20135;&#21697;&#21305;&#37197;&#36215;&#26469;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#23454;&#20307;&#21305;&#37197;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22914;BERT&#25110;RoBERTa&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#20307;&#21305;&#37197;&#20013;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;i&#65289;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;ii&#65289;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23454;&#20307;&#19981;&#22815;&#20581;&#22766;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22522;&#20110;PLMs&#30340;&#21305;&#37197;&#22120;&#30340;&#22791;&#36873;&#26041;&#26696;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;LLMs&#23545;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#25176;&#31649;&#30340;LLMs&#65292;&#22914;GPT3.5&#21644;GPT4&#65292;&#20197;&#21450;&#22522;&#20110;Llama2&#30340;&#24320;&#28304;LLMs&#65292;&#21487;&#20197;&#22312;&#26412;&#22320;&#36816;&#34892;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#21644;&#8230;
&lt;/p&gt;
&lt;p&gt;
Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity. Entity Matching is a central step in most data integration pipelines and an enabler for many e-commerce applications which require to match products offers from different vendors. State-of-the-art entity matching methods often rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using large language models (LLMs) for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2 which can be run locally. We evaluate these models in a zero-shot scenario as well as a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27700;&#21360;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#36807;&#31243;&#20013;&#25554;&#20837;&#27700;&#21360;&#26469;&#20445;&#25252;&#27169;&#22411;&#26435;&#37325;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#36215;&#20316;&#29992;&#20294;&#22312;&#37327;&#21270;&#21518;&#20445;&#25345;&#38544;&#34255;&#65292;&#20026;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#26102;&#30340;&#27169;&#22411;&#26435;&#37325;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.11237</link><description>&lt;p&gt;
&#20351;&#29992;&#26435;&#37325;&#37327;&#21270;&#23545;LLMs&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Watermarking LLMs with Weight Quantization. (arXiv:2310.11237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27700;&#21360;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#36807;&#31243;&#20013;&#25554;&#20837;&#27700;&#21360;&#26469;&#20445;&#25252;&#27169;&#22411;&#26435;&#37325;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#36215;&#20316;&#29992;&#20294;&#22312;&#37327;&#21270;&#21518;&#20445;&#25345;&#38544;&#34255;&#65292;&#20026;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#26102;&#30340;&#27169;&#22411;&#26435;&#37325;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#28389;&#29992;&#20250;&#24102;&#26469;&#39640;&#39118;&#38505;&#65292;&#22240;&#27492;&#20445;&#25252;&#27169;&#22411;&#26435;&#37325;&#20197;&#36991;&#20813;&#20405;&#29359;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35768;&#21487;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27700;&#21360;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#36807;&#31243;&#20013;&#25554;&#20837;&#27700;&#21360;&#65292;&#32780;&#26080;&#38656;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20107;&#20808;&#23450;&#20041;&#35302;&#21457;&#22120;&#12290;&#36825;&#31181;&#27700;&#21360;&#22312;&#27169;&#22411;&#20197;fp32&#27169;&#24335;&#20351;&#29992;&#26102;&#36215;&#20316;&#29992;&#65292;&#22312;&#27169;&#22411;&#37327;&#21270;&#20026;int8&#21518;&#20445;&#25345;&#38544;&#34255;&#65292;&#36825;&#26679;&#29992;&#25143;&#21482;&#33021;&#36827;&#34892;&#27169;&#22411;&#30340;&#25512;&#29702;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#27169;&#22411;&#30417;&#30563;&#24494;&#35843;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#27700;&#21360;&#23884;&#20837;&#21040;&#21253;&#25324;GPT-Neo&#21644;LLaMA&#22312;&#20869;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#20013;&#12290;&#24076;&#26395;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20026;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#26102;&#30340;&#27169;&#22411;&#26435;&#37325;&#25552;&#20379;&#19968;&#20010;&#28508;&#22312;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications.
&lt;/p&gt;</description></item><item><title>RealBehavior&#26694;&#26550;&#26088;&#22312;&#20934;&#30830;&#25551;&#32472;&#22522;&#30784;&#27169;&#22411;&#30340;&#20154;&#31867;&#21270;&#34892;&#20026;&#65292;&#36890;&#36807;&#35780;&#20272;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#21270;&#23545;&#40784;&#30446;&#26631;&#65292;&#25552;&#39640;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.11227</link><description>&lt;p&gt;
RealBehavior: &#29992;&#20110;&#20934;&#30830;&#25551;&#32472;&#22522;&#30784;&#27169;&#22411;&#20154;&#31867;&#21270;&#34892;&#20026;&#26426;&#21046;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RealBehavior: A Framework for Faithfully Characterizing Foundation Models' Human-like Behavior Mechanisms. (arXiv:2310.11227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11227
&lt;/p&gt;
&lt;p&gt;
RealBehavior&#26694;&#26550;&#26088;&#22312;&#20934;&#30830;&#25551;&#32472;&#22522;&#30784;&#27169;&#22411;&#30340;&#20154;&#31867;&#21270;&#34892;&#20026;&#65292;&#36890;&#36807;&#35780;&#20272;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#21270;&#23545;&#40784;&#30446;&#26631;&#65292;&#25552;&#39640;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#20013;&#20986;&#29616;&#20154;&#31867;&#21270;&#34892;&#20026;&#30340;&#25253;&#36947;&#19982;&#26085;&#20465;&#22686;&#65292;&#24515;&#29702;&#23398;&#29702;&#35770;&#20026;&#30740;&#31350;&#36825;&#20123;&#34892;&#20026;&#25552;&#20379;&#20102;&#25345;&#20037;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#24448;&#24448;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#38754;&#21521;&#20154;&#31867;&#30340;&#24037;&#20855;&#65292;&#32780;&#27809;&#26377;&#39564;&#35777;&#20854;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RealBehavior&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20934;&#30830;&#25551;&#32472;&#27169;&#22411;&#30340;&#20154;&#31867;&#21270;&#34892;&#20026;&#12290;&#38500;&#20102;&#31616;&#21333;&#27979;&#37327;&#34892;&#20026;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#26681;&#25454;&#21487;&#37325;&#29616;&#24615;&#12289;&#20869;&#22806;&#19968;&#33268;&#24615;&#21644;&#27867;&#21270;&#24615;&#35780;&#20272;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21333;&#24212;&#29992;&#24515;&#29702;&#23398;&#24037;&#20855;&#26080;&#27861;&#20934;&#30830;&#25551;&#32472;&#25152;&#26377;&#30340;&#20154;&#31867;&#21270;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#27169;&#22411;&#19982;&#20154;&#31867;&#21644;&#31038;&#20250;&#20215;&#20540;&#23545;&#40784;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#26679;&#21270;&#23545;&#40784;&#30446;&#26631;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#38450;&#27490;&#21019;&#24314;&#20855;&#26377;&#21463;&#38480;&#29305;&#24449;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reports of human-like behaviors in foundation models are growing, with psychological theories providing enduring tools to investigate these behaviors. However, current research tends to directly apply these human-oriented tools without verifying the faithfulness of their outcomes. In this paper, we introduce a framework, RealBehavior, which is designed to characterize the humanoid behaviors of models faithfully. Beyond simply measuring behaviors, our framework assesses the faithfulness of results based on reproducibility, internal and external consistency, and generalizability. Our findings suggest that a simple application of psychological tools cannot faithfully characterize all human-like behaviors. Moreover, we discuss the impacts of aligning models with human and social values, arguing for the necessity of diversifying alignment objectives to prevent the creation of models with restricted characteristics.
&lt;/p&gt;</description></item><item><title>KG-GPT&#26159;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25512;&#29702;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#21477;&#23376;&#20998;&#21106;&#12289;&#22270;&#26816;&#32034;&#21644;&#25512;&#29702;&#19977;&#20010;&#27493;&#39588;&#23454;&#29616;&#20102;&#23545;&#30693;&#35782;&#22270;&#35889;&#30340;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#20107;&#23454;&#39564;&#35777;&#21644;KGQA&#22522;&#20934;&#19978;&#23637;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#21644;&#24378;&#20581;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11220</link><description>&lt;p&gt;
KG-GPT&#65306;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25512;&#29702;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models. (arXiv:2310.11220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11220
&lt;/p&gt;
&lt;p&gt;
KG-GPT&#26159;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25512;&#29702;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#21477;&#23376;&#20998;&#21106;&#12289;&#22270;&#26816;&#32034;&#21644;&#25512;&#29702;&#19977;&#20010;&#27493;&#39588;&#23454;&#29616;&#20102;&#23545;&#30693;&#35782;&#22270;&#35889;&#30340;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#20107;&#23454;&#39564;&#35777;&#21644;KGQA&#22522;&#20934;&#19978;&#23637;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#21644;&#24378;&#20581;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#19981;&#22815;&#20805;&#20998;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#19978;&#20351;&#29992;LLMs&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20960;&#20046;&#27809;&#26377;&#34987;&#35302;&#21450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KG-GPT&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#29992;&#36884;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#26469;&#25191;&#34892;&#28041;&#21450;KGs&#30340;&#20219;&#21153;&#12290;KG-GPT&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;&#21477;&#23376;&#20998;&#21106;&#12289;&#22270;&#26816;&#32034;&#21644;&#25512;&#29702;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#26088;&#22312;&#23558;&#21477;&#23376;&#20998;&#25104;&#37096;&#20998;&#12289;&#26816;&#32034;&#30456;&#20851;&#30340;&#22270;&#32452;&#20214;&#65292;&#24182;&#24471;&#20986;&#36923;&#36753;&#32467;&#35770;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;KG&#30340;&#20107;&#23454;&#39564;&#35777;&#21644;KGQA&#22522;&#20934;&#23545;KG-GPT&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20855;&#26377;&#26377;&#31454;&#20105;&#21147;&#21644;&#24378;&#20581;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#32988;&#36807;&#20102;&#20960;&#20010;&#23436;&#20840;&#30417;&#30563;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#25972;&#21512;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#22788;&#29702;&#26041;&#38754;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#35299;&#37322;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#29305;&#24449;&#24402;&#22240;&#35299;&#37322;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11207</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#33258;&#25105;&#35299;&#37322;&#65311;LLM&#29983;&#25104;&#30340;&#33258;&#35299;&#37322;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations. (arXiv:2310.11207v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#35299;&#37322;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#29305;&#24449;&#24402;&#22240;&#35299;&#37322;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#31867;&#23545;&#35805;&#20013;&#36827;&#34892;&#25351;&#23548;&#65292;&#20197;&#20135;&#29983;&#8220;&#26377;&#24110;&#21161;&#8221;&#30340;&#22238;&#31572;&#65292;&#23427;&#20204;&#36890;&#24120;&#20250;&#22312;&#22238;&#31572;&#20013;&#25552;&#20379;&#35299;&#37322;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#33258;&#35299;&#37322;&#12290;&#20363;&#22914;&#65292;&#22312;&#20998;&#26512;&#30005;&#24433;&#35780;&#35770;&#30340;&#24773;&#24863;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#36755;&#20986;&#24773;&#24863;&#30340;&#31215;&#26497;&#24615;&#65292;&#24182;&#21015;&#20986;&#35780;&#35770;&#20013;&#24102;&#26377;&#24773;&#24863;&#30340;&#35789;&#35821;&#65288;&#22914;&#8220;fantastic&#8221;&#21644;&#8220;memorable&#8221;&#65289;&#20316;&#20026;&#35299;&#37322;&#12290;&#36825;&#20123;&#33258;&#21160;&#29983;&#25104;&#30340;&#33258;&#35299;&#37322;&#26377;&#22810;&#22909;&#65311;&#26412;&#25991;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#29305;&#24449;&#24402;&#22240;&#35299;&#37322;&#30340;&#20219;&#21153;&#20013;&#23545;&#27492;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21518;&#32773;&#26159;&#21487;&#35299;&#37322;&#24615;&#25991;&#29486;&#20013;&#26368;&#24120;&#30740;&#31350;&#30340;&#35774;&#32622;&#65288;&#38024;&#23545;ChatGPT&#20043;&#21069;&#30340;&#27169;&#22411;&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#33258;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce "helpful" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as "fantastic" and "memorable" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#25991;&#26412;&#31616;&#21270;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#38750;&#20856;&#22411;&#25439;&#22833;&#21644;&#19968;&#31181;&#20248;&#21270;&#31616;&#21333;&#24615;&#30340;&#37325;&#26032;&#25490;&#24207;&#35299;&#30721;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11191</link><description>&lt;p&gt;
&#21307;&#23398;&#25991;&#26412;&#31616;&#21270;&#65306;&#36890;&#36807;&#38750;&#20856;&#22411;&#35757;&#32451;&#21644;&#37325;&#26032;&#25490;&#24207;&#30340;Beam Search&#35299;&#30721;&#20248;&#21270;&#21487;&#35835;&#24615;
&lt;/p&gt;
&lt;p&gt;
Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding. (arXiv:2310.11191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#25991;&#26412;&#31616;&#21270;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#38750;&#20856;&#22411;&#25439;&#22833;&#21644;&#19968;&#31181;&#20248;&#21270;&#31616;&#21333;&#24615;&#30340;&#37325;&#26032;&#25490;&#24207;&#35299;&#30721;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#31616;&#21270;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#22312;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#21307;&#23398;&#65289;&#20013;&#24357;&#21512;&#27807;&#36890;&#24046;&#36317;&#30340;&#36234;&#26469;&#36234;&#26377;&#29992;&#30340;&#24212;&#29992;&#65292;&#24050;&#36880;&#28176;&#23853;&#38706;&#22836;&#35282;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#31616;&#21270;&#26041;&#27861;&#26377;&#26102;&#20250;&#23548;&#33268;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#19979;&#38477;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#25991;&#26412;&#31616;&#21270;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#20856;&#22411;&#21507;&#20111;&#25439;&#22833;&#24178;&#22270;&#29255;&#21050;&#28608;&#29983;&#25104;&#26356;&#31616;&#21333;&#30340;&#26415;&#35821;&#65292;&#20197;&#21450;&#19968;&#31181;&#20248;&#21270;&#31616;&#21333;&#24615;&#30340;&#37325;&#26032;&#25490;&#24207;&#30340;Beam Search&#35299;&#30721;&#26041;&#27861;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#35835;&#24615;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#21457;&#29616;&#20026;&#25913;&#36827;&#21307;&#23398;&#39046;&#22495;&#30340;&#25991;&#26412;&#31616;&#21270;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text simplification has emerged as an increasingly useful application of AI for bridging the communication gap in specialized fields such as medicine, where the lexicon is often dominated by technical jargon and complex constructs. Despite notable progress, methods in medical simplification sometimes result in the generated text having lower quality and diversity. In this work, we explore ways to further improve the readability of text simplification in the medical domain. We propose (1) a new unlikelihood loss that encourages generation of simpler terms and (2) a reranked beam search decoding method that optimizes for simplicity, which achieve better performance on readability metrics on three datasets. This study's findings offer promising avenues for improving text simplification in the medical field.
&lt;/p&gt;</description></item><item><title>ViSoBERT&#26159;&#39318;&#20010;&#29992;&#20110;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#22788;&#29702;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#24773;&#24863;&#35782;&#21035;&#12289;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#22403;&#22334;&#35780;&#35770;&#26816;&#27979;&#21644;&#20167;&#24680;&#35328;&#35770;&#36328;&#24230;&#26816;&#27979;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.11166</link><description>&lt;p&gt;
ViSoBERT&#65306;&#38754;&#21521;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#22788;&#29702;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing. (arXiv:2310.11166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11166
&lt;/p&gt;
&lt;p&gt;
ViSoBERT&#26159;&#39318;&#20010;&#29992;&#20110;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#22788;&#29702;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#24773;&#24863;&#35782;&#21035;&#12289;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#22403;&#22334;&#35780;&#35770;&#26816;&#27979;&#21644;&#20167;&#24680;&#35328;&#35770;&#36328;&#24230;&#26816;&#27979;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#21644;&#20013;&#25991;&#20316;&#20026;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24050;&#32463;&#35265;&#35777;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#21457;&#23637;&#12290;&#23613;&#31649;&#36234;&#21335;&#26377;&#22823;&#32422;1&#20159;&#20351;&#29992;&#36234;&#21335;&#35821;&#30340;&#20154;&#21475;&#65292;&#20294;&#22312;&#19968;&#33324;&#30340;&#36234;&#21335;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#22914;&#35789;&#24615;&#26631;&#27880;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#24050;&#32463;&#23384;&#22312;&#19968;&#20123;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20363;&#22914;PhoBERT&#12289;ViBERT&#21644;vELECTRA&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#23616;&#38480;&#20110;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#29992;&#20110;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;ViSoBERT&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;XLM-R&#26550;&#26500;&#22312;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#22810;&#26679;&#21270;&#30340;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#19978;&#25506;&#32034;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20116;&#20010;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24212;&#29992;&#65306;&#24773;&#24863;&#35782;&#21035;&#12289;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#22403;&#22334;&#35780;&#35770;&#26816;&#27979;&#21644;&#20167;&#24680;&#35328;&#35770;&#36328;&#24230;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ViSoBERT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
English and Chinese, known as resource-rich languages, have witnessed the strong development of transformer-based language models for natural language processing tasks. Although Vietnam has approximately 100M people speaking Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA, performed well on general Vietnamese NLP tasks, including POS tagging and named entity recognition. These pre-trained language models are still limited to Vietnamese social media tasks. In this paper, we present the first monolingual pre-trained language model for Vietnamese social media texts, ViSoBERT, which is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese social media texts using XLM-R architecture. Moreover, we explored our pre-trained model on five important natural language downstream tasks on Vietnamese social media texts: emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. Our experime
&lt;/p&gt;</description></item><item><title>IMTLab&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#29992;&#20110;&#26500;&#24314;&#12289;&#35780;&#20272;&#21644;&#35786;&#26029;&#20132;&#20114;&#24335;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#25509;&#21475;&#65292;&#25903;&#25345;&#28789;&#27963;&#30340;&#26550;&#26500;&#21644;&#29992;&#25143;&#31574;&#30053;&#12290;&#21069;&#32512;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#22312;&#31471;&#21040;&#31471;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#26368;&#20302;&#30340;&#32534;&#36753;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.11163</link><description>&lt;p&gt;
IMTLab: &#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#12289;&#35780;&#20272;&#21644;&#35786;&#26029;&#20132;&#20114;&#24335;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#24320;&#28304;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing Interactive Machine Translation Systems. (arXiv:2310.11163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11163
&lt;/p&gt;
&lt;p&gt;
IMTLab&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#29992;&#20110;&#26500;&#24314;&#12289;&#35780;&#20272;&#21644;&#35786;&#26029;&#20132;&#20114;&#24335;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#25509;&#21475;&#65292;&#25903;&#25345;&#28789;&#27963;&#30340;&#26550;&#26500;&#21644;&#29992;&#25143;&#31574;&#30053;&#12290;&#21069;&#32512;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#22312;&#31471;&#21040;&#31471;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#26368;&#20302;&#30340;&#32534;&#36753;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;IMTLab&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#31471;&#21040;&#31471;&#20132;&#20114;&#24335;&#26426;&#22120;&#32763;&#35793;&#65288;IMT&#65289;&#31995;&#32479;&#24179;&#21488;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#26500;&#24314;&#20855;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;IMT&#31995;&#32479;&#65292;&#36827;&#34892;&#31471;&#21040;&#31471;&#35780;&#20272;&#65292;&#24182;&#35786;&#26029;&#31995;&#32479;&#30340;&#24369;&#28857;&#12290;IMTLab&#23558;&#25972;&#20010;&#20132;&#20114;&#32763;&#35793;&#36807;&#31243;&#35270;&#20026;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#65292;&#24182;&#22312;&#20854;&#20013;&#24341;&#20837;&#20154;&#26426;&#20132;&#20114;&#65292;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#12289;&#26080;&#38169;&#35823;&#30340;&#32763;&#35793;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36890;&#20449;&#25509;&#21475;&#65292;&#20197;&#25903;&#25345;&#28789;&#27963;&#30340;IMT&#26550;&#26500;&#21644;&#29992;&#25143;&#31574;&#30053;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#25311;&#21644;&#30495;&#23454;&#30340;&#20132;&#20114;&#29615;&#22659;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#35780;&#20272;&#65292;&#24182;&#21033;&#29992;&#35813;&#26694;&#26550;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#20043;&#21069;&#30340;IMT&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#21644;&#25163;&#21160;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#31471;&#21040;&#31471;&#35780;&#20272;&#20013;&#65292;&#21069;&#32512;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#20173;&#28982;&#20855;&#26377;&#26368;&#20302;&#30340;&#32534;&#36753;&#25104;&#26412;&#65292;&#32780;BiTIIMT&#36798;&#21040;&#20102;&#21487;&#27604;&#30340;&#32534;&#36753;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present IMTLab, an open-source end-to-end interactive machine translation (IMT) system platform that enables researchers to quickly build IMT systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. IMTLab treats the whole interactive translation process as a task-oriented dialogue with a human-in-the-loop setting, in which human interventions can be explicitly incorporated to produce high-quality, error-free translations. To this end, a general communication interface is designed to support the flexible IMT architectures and user policies. Based on the proposed design, we construct a simulated and real interactive environment to achieve end-to-end evaluation and leverage the framework to systematically evaluate previous IMT systems. Our simulated and manual experiments show that the prefix-constrained decoding approach still gains the lowest editing cost in the end-to-end evaluation, while BiTIIMT achieves comparable editing cost
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#25955;&#32852;&#24819;&#20219;&#21153;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#24605;&#32500;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20808;&#36827;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#35821;&#20041;&#20851;&#32852;&#65292;&#36229;&#36807;&#20102;&#22823;&#22810;&#25968;&#20154;&#31867;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.11158</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#21147;&#65306;&#27169;&#22411;&#33021;&#21542;&#20135;&#29983;&#19981;&#21516;&#30340;&#35821;&#20041;&#20851;&#32852;&#65311;
&lt;/p&gt;
&lt;p&gt;
Probing the Creativity of Large Language Models: Can models produce divergent semantic association?. (arXiv:2310.11158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#25955;&#32852;&#24819;&#20219;&#21153;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#24605;&#32500;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20808;&#36827;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#35821;&#20041;&#20851;&#32852;&#65292;&#36229;&#36807;&#20102;&#22823;&#22810;&#25968;&#20154;&#31867;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#35328;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#36827;&#19968;&#27493;&#29983;&#25104;&#21019;&#36896;&#24615;&#30340;&#20869;&#23481;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#35748;&#30693;&#35282;&#24230;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24605;&#32500;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#20998;&#25955;&#32852;&#24819;&#20219;&#21153;&#65288;DAT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23458;&#35266;&#34913;&#37327;&#21019;&#36896;&#21147;&#30340;&#26041;&#27861;&#65292;&#35201;&#27714;&#27169;&#22411;&#29983;&#25104;&#19981;&#30456;&#20851;&#30340;&#21333;&#35789;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65306;&#65288;1&#65289;&#22312;&#20351;&#29992;&#36138;&#23146;&#25628;&#32034;&#31574;&#30053;&#26102;&#65292;GPT-4&#36229;&#36807;&#20102;96&#65285;&#30340;&#20154;&#31867;&#65292;&#32780;GPT-3.5-turbo&#36229;&#36807;&#20102;&#24179;&#22343;&#27700;&#24179;&#65307;&#65288;2&#65289;&#23545;&#20110;&#27169;&#22411;&#32780;&#35328;&#65292;&#38543;&#26426;&#25277;&#26679;&#21644;&#28201;&#24230;&#35843;&#33410;&#26159;&#33719;&#24471;&#26356;&#39640;DAT&#20998;&#25968;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20294;&#22312;&#21019;&#36896;&#21147;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#30340;&#35821;&#20041;&#20851;&#32852;&#65292;&#36825;&#26159;&#21019;&#36896;&#21147;&#30340;&#22522;&#26412;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models possess remarkable capacity for processing language, but it remains unclear whether these models can further generate creative content. The present study aims to investigate the creative thinking of large language models through a cognitive perspective. We utilize the divergent association task (DAT), an objective measurement of creativity that asks models to generate unrelated words and calculates the semantic distance between them. We compare the results across different models and decoding strategies. Our findings indicate that: (1) When using the greedy search strategy, GPT-4 outperforms 96% of humans, while GPT-3.5-turbo exceeds the average human level. (2) Stochastic sampling and temperature scaling are effective to obtain higher DAT scores for models except GPT-4, but face a trade-off between creativity and stability. These results imply that advanced large language models have divergent semantic associations, which is a fundamental process underlying creat
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#31185;&#23398;&#27169;&#22411;&#20013;&#30340;&#22320;&#20301;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#36879;&#26126;&#24230;&#21644;&#20449;&#24687;&#25552;&#20379;&#33021;&#21147;&#30340;&#35752;&#35770;&#65292;&#24403;&#21069;&#38454;&#27573;&#30340;LLMs&#20960;&#20046;&#23545;&#35821;&#35328;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#35299;&#37322;&#65292;&#26410;&#26469;&#30740;&#31350;&#38656;&#35201;&#25506;&#32034;&#26356;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.11146</link><description>&lt;p&gt;
&#35821;&#35328;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21435;&#24448;&#20309;&#26041;
&lt;/p&gt;
&lt;p&gt;
The Quo Vadis of the Relationship between Language and Large Language Models. (arXiv:2310.11146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11146
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#31185;&#23398;&#27169;&#22411;&#20013;&#30340;&#22320;&#20301;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#36879;&#26126;&#24230;&#21644;&#20449;&#24687;&#25552;&#20379;&#33021;&#21147;&#30340;&#35752;&#35770;&#65292;&#24403;&#21069;&#38454;&#27573;&#30340;LLMs&#20960;&#20046;&#23545;&#35821;&#35328;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#35299;&#37322;&#65292;&#26410;&#26469;&#30740;&#31350;&#38656;&#35201;&#25506;&#32034;&#26356;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#65288;&#36890;&#29992;&#65289;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#20013;&#65292;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27963;&#21160;&#30340;&#20960;&#20010;&#26368;&#36817;&#30340;&#36827;&#23637;&#40723;&#21169;&#23558;LLMs&#20316;&#20026;&#35821;&#35328;&#31185;&#23398;&#27169;&#22411;&#12290;&#34429;&#28982;&#29992;&#20110;&#25551;&#36848;LLMs&#30340;&#26415;&#35821;&#20542;&#21521;&#20110;&#23558;&#20854;&#20316;&#20026;&#31185;&#23398;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#20026;&#20182;&#20204;&#35797;&#22270;&#34920;&#31034;&#30340;&#30446;&#26631;&#31995;&#32479;&#25552;&#20379;&#27934;&#35265;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#22312;&#30830;&#23450;&#32570;&#20047;&#36879;&#26126;&#24230;&#30340;&#31185;&#23398;&#27169;&#22411;&#24102;&#26469;&#30340;&#26368;&#37325;&#35201;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#39118;&#38505;&#20043;&#21518;&#65292;&#35770;&#36848;&#20102;LLMs&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#31185;&#23398;&#27169;&#22411;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65288;&#23545;&#35937;&#12289;&#23186;&#20171;&#12289;&#21547;&#20041;&#21644;&#29992;&#25143;&#65289;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#30446;&#21069;&#21457;&#23637;&#38454;&#27573;&#30340;LLMs&#20960;&#20046;&#23545;&#35821;&#35328;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#35299;&#37322;&#65292;&#28982;&#21518;&#23545;&#36825;&#20010;&#20027;&#39064;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of Artificial (General) Intelligence (AI), the several recent advancements in Natural language processing (NLP) activities relying on Large Language Models (LLMs) have come to encourage the adoption of LLMs as scientific models of language. While the terminology employed for the characterization of LLMs favors their embracing as such, it is not clear that they are in a place to offer insights into the target system they seek to represent. After identifying the most important theoretical and empirical risks brought about by the adoption of scientific models that lack transparency, we discuss LLMs relating them to every scientific model's fundamental components: the object, the medium, the meaning and the user. We conclude that, at their current stage of development, LLMs hardly offer any explanations for language, and then we provide an outlook for more informative future research directions on this topic.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#26696;&#30740;&#31350;&#20102;&#31471;&#21040;&#31471;&#38271;&#31687;&#21516;&#20256;&#65292;&#21363;&#22312;&#27809;&#26377;&#39044;&#20808;&#20998;&#21106;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#21475;&#35821;&#32763;&#35793;&#12290;&#23427;&#35843;&#26597;&#20102;&#26368;&#26032;&#30340;&#31471;&#21040;&#31471;&#21516;&#20256;&#36827;&#23637;&#65292;&#35780;&#20272;&#20102;&#21516;&#20256;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#21644;&#19982;&#38271;&#31687;&#22330;&#26223;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11141</link><description>&lt;p&gt;
Simultaneous Speech Translation in Long-Form Setting: Thesis Proposal
&lt;/p&gt;
&lt;p&gt;
Long-form Simultaneous Speech Translation: Thesis Proposal. (arXiv:2310.11141v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11141
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#26696;&#30740;&#31350;&#20102;&#31471;&#21040;&#31471;&#38271;&#31687;&#21516;&#20256;&#65292;&#21363;&#22312;&#27809;&#26377;&#39044;&#20808;&#20998;&#21106;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#21475;&#35821;&#32763;&#35793;&#12290;&#23427;&#35843;&#26597;&#20102;&#26368;&#26032;&#30340;&#31471;&#21040;&#31471;&#21516;&#20256;&#36827;&#23637;&#65292;&#35780;&#20272;&#20102;&#21516;&#20256;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#21644;&#19982;&#38271;&#31687;&#22330;&#26223;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#20256;&#25216;&#26415;&#26088;&#22312;&#23454;&#26102;&#23558;&#21475;&#35821;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#65292;&#29978;&#33267;&#22312;&#35828;&#35805;&#20154;&#26410;&#23436;&#25104;&#21477;&#23376;&#20043;&#21069;&#21363;&#21487;&#23454;&#29616;&#32763;&#35793;&#12290;&#20256;&#32479;&#19978;&#65292;&#21516;&#20256;&#20027;&#35201;&#36890;&#36807;&#32423;&#32852;&#31995;&#32479;&#26469;&#22788;&#29702;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#35821;&#38899;&#35782;&#21035;&#12289;&#20998;&#21106;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#23376;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23545;&#31471;&#21040;&#31471;&#31995;&#32479;&#30340;&#24191;&#27867;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;&#31471;&#21040;&#31471;&#21516;&#20256;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#23427;&#20204;&#20551;&#35774;&#28304;&#35821;&#38899;&#24050;&#32463;&#34987;&#39044;&#20808;&#20998;&#21106;&#25104;&#21477;&#23376;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22330;&#26223;&#26159;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#36825;&#20010;&#35770;&#25991;&#25552;&#26696;&#30528;&#37325;&#20110;&#22788;&#29702;&#31471;&#21040;&#31471;&#38271;&#31687;&#21516;&#20256;&#38382;&#39064;&#65292;&#21363;&#22312;&#27809;&#26377;&#39044;&#20808;&#20998;&#21106;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32763;&#35793;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#31471;&#21040;&#31471;&#21516;&#20256;&#36827;&#23637;&#30340;&#35843;&#26597;&#65292;&#35780;&#20272;&#20102;&#21516;&#20256;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#20197;&#21450;&#19982;&#38271;&#31687;&#22330;&#26223;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous speech translation (SST) aims to provide real-time translation of spoken language, even before the speaker finishes their sentence. Traditionally, SST has been addressed primarily by cascaded systems that decompose the task into subtasks, including speech recognition, segmentation, and machine translation. However, the advent of deep learning has sparked significant interest in end-to-end (E2E) systems. Nevertheless, a major limitation of most approaches to E2E SST reported in the current literature is that they assume that the source speech is pre-segmented into sentences, which is a significant obstacle for practical, real-world applications. This thesis proposal addresses end-to-end simultaneous speech translation, particularly in the long-form setting, i.e., without pre-segmentation. We present a survey of the latest advancements in E2E SST, assess the primary obstacles in SST and its relevance to long-form scenarios, and suggest approaches to tackle these challenges.
&lt;/p&gt;</description></item><item><title>IDMO&#39033;&#30446;&#26088;&#22312;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#65292;&#20854;&#36129;&#29486;&#21253;&#25324;&#21019;&#24314;&#26032;&#22411;&#25968;&#25454;&#38598;&#12289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#12289;&#35780;&#20272;GPT-4&#31561;&#12290;</title><link>http://arxiv.org/abs/2310.11097</link><description>&lt;p&gt;
&#29992;&#20110;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#23454;&#39564;&#65306;IDMO&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
Experimenting AI Technologies for Disinformation Combat: the IDMO Project. (arXiv:2310.11097v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11097
&lt;/p&gt;
&lt;p&gt;
IDMO&#39033;&#30446;&#26088;&#22312;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#65292;&#20854;&#36129;&#29486;&#21253;&#25324;&#21019;&#24314;&#26032;&#22411;&#25968;&#25454;&#38598;&#12289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#12289;&#35780;&#20272;GPT-4&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22823;&#21033;&#25968;&#23383;&#23186;&#20307;&#35266;&#23519;&#39033;&#30446;&#65288;IDMO&#65289;&#26159;&#27431;&#27954;&#19968;&#39033;&#20513;&#35758;&#30340;&#19968;&#37096;&#20998;&#65292;&#19987;&#27880;&#20110;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#12290;&#26412;&#25253;&#21578;&#27010;&#36848;&#20102;Rai-CRITS&#22312;&#35813;&#39033;&#30446;&#20013;&#30340;&#36129;&#29486;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#21019;&#24314;&#29992;&#20110;&#27979;&#35797;&#25216;&#26415;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#65292;&#65288;ii&#65289;&#24320;&#21457;&#33258;&#21160;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#31867;Pagella Politica&#30340;&#35009;&#20915;&#20197;&#20415;&#20110;&#26356;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#65288;iii&#65289;&#21019;&#24314;&#33258;&#21160;&#27169;&#22411;&#65292;&#23545;FEVER&#25968;&#25454;&#38598;&#19978;&#30340;&#25991;&#26412;&#34164;&#21547;&#20855;&#26377;&#24322;&#24120;&#31934;&#24230;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#65288;iv&#65289;&#20351;&#29992;GPT-4&#35780;&#20272;&#25991;&#26412;&#34164;&#21547;&#65292; &#65288;v&#65289;&#22312;&#22269;&#23478;&#27963;&#21160;&#20013;&#24320;&#23637;&#25552;&#39640;&#23545;&#20551;&#26032;&#38395;&#24847;&#35782;&#30340;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Italian Digital Media Observatory (IDMO) project, part of a European initiative, focuses on countering disinformation and fake news. This report outlines contributions from Rai-CRITS to the project, including: (i) the creation of novel datasets for testing technologies (ii) development of an automatic model for categorizing Pagella Politica verdicts to facilitate broader analysis (iii) creation of an automatic model for recognizing textual entailment with exceptional accuracy on the FEVER dataset (iv) assessment using GPT-4 to identify textual entailmen (v) a game to raise awareness about fake news at national events.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11085</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25512;&#26029;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#26377;&#20004;&#20010;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#35201;&#27714;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#25110;&#25512;&#26029;&#23427;&#20204;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22122;&#22768;&#65292;(2)&#23427;&#20204;&#38656;&#35201;&#20154;&#24037;&#23545;&#25991;&#26723;&#36827;&#34892;&#27880;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#30340;&#30740;&#31350;&#32773;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#22312;&#28040;&#38500;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20851;&#38190;&#24615;&#30340;&#20248;&#21183;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;DocRED&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#30417;&#30563;&#23545;&#27604;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;(Supervised Contrastively Pre-trained Transformer)&#26469;&#29702;&#35299;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#20889;&#20316;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#20869;&#23481;&#19982;&#20854;&#20316;&#32773;&#30456;&#20851;&#32852;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#26377;&#23475;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.11081</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#30417;&#30563;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#29702;&#35299;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#20889;&#20316;&#39118;&#26684;
&lt;/p&gt;
&lt;p&gt;
Understanding writing style in social media with a supervised contrastively pre-trained transformer. (arXiv:2310.11081v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#30417;&#30563;&#23545;&#27604;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;(Supervised Contrastively Pre-trained Transformer)&#26469;&#29702;&#35299;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#20889;&#20316;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#20869;&#23481;&#19982;&#20854;&#20316;&#32773;&#30456;&#20851;&#32852;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#26377;&#23475;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#25104;&#20026;&#26377;&#23475;&#34892;&#20026;&#30340;&#32933;&#27779;&#22303;&#22756;&#65292;&#20174;&#20167;&#24680;&#35328;&#35770;&#21040;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#24694;&#24847;&#34892;&#20026;&#32773;&#29616;&#22312;&#26377;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33258;&#30001;&#26469;&#34892;&#20351;&#19981;&#24403;&#34892;&#20026;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#31038;&#20250;&#21160;&#33633;&#21644;&#20005;&#37325;&#21518;&#26524;&#65292;&#22914;&#32654;&#22269;&#24635;&#32479;&#36873;&#20030;&#26399;&#38388;&#30340;&#22269;&#20250;&#23665;&#34989;&#20987;&#20107;&#20214;&#21644;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#30340;&#21453;&#30123;&#33495;&#36816;&#21160;&#12290;&#29702;&#35299;&#22312;&#32447;&#35821;&#35328;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#21152;&#36843;&#20999;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20869;&#23481;&#20998;&#26512;&#65292;&#20294;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#37325;&#28857;&#36716;&#31227;&#21040;&#36890;&#36807;&#23558;&#20869;&#23481;&#19982;&#20854;&#21508;&#33258;&#30340;&#20316;&#32773;&#30456;&#20851;&#32852;&#26469;&#29702;&#35299;&#26377;&#23475;&#34892;&#20026;&#12290;&#35768;&#22810;&#26032;&#39062;&#30340;&#26041;&#27861;&#23581;&#35797;&#23398;&#20064;&#25991;&#26412;&#20013;&#20316;&#32773;&#30340;&#39118;&#26684;&#29305;&#24449;&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#26041;&#27861;&#21463;&#21040;&#23567;&#25968;&#25454;&#38598;&#25110;&#27425;&#20248;&#35757;&#32451;&#25439;&#22833;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26469;&#33258;&#20844;&#20849;&#26469;&#28304;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#30340;Style Transformer for Authorship Representations (STAR)&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#35821;&#26009;&#24211;&#35268;&#27169;&#36798;&#21040;4.5 x 10^6 auth&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Social Networks serve as fertile ground for harmful behavior, ranging from hate speech to the dissemination of disinformation. Malicious actors now have unprecedented freedom to misbehave, leading to severe societal unrest and dire consequences, as exemplified by events such as the Capitol assault during the US presidential election and the Antivaxx movement during the COVID-19 pandemic. Understanding online language has become more pressing than ever. While existing works predominantly focus on content analysis, we aim to shift the focus towards understanding harmful behaviors by relating content to their respective authors. Numerous novel approaches attempt to learn the stylistic features of authors in texts, but many of these approaches are constrained by small datasets or sub-optimal training losses. To overcome these limitations, we introduce the Style Transformer for Authorship Representations (STAR), trained on a large corpus derived from public sources of 4.5 x 10^6 auth
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#20197;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#28508;&#22312;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#26469;&#32531;&#35299;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#21518;&#65292;LLMs&#33021;&#22815;&#29983;&#25104;&#26356;&#20844;&#24179;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.11079</link><description>&lt;p&gt;
&#20174;&#32418;&#38431;&#25805;&#20316;&#20013;&#23398;&#20064;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#25361;&#34885;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models. (arXiv:2310.11079v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11079
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#20197;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#28508;&#22312;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#26469;&#32531;&#35299;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#21518;&#65292;LLMs&#33021;&#22815;&#29983;&#25104;&#26356;&#20844;&#24179;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#23545;&#35805;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#25913;&#36827;&#12290;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20445;&#30041;&#21487;&#33021;&#20260;&#23475;&#20154;&#31867;&#30340;&#19981;&#24179;&#31561;&#30340;&#21516;&#26102;&#65292;&#32534;&#30721;&#20102;&#28508;&#22312;&#30340;&#20559;&#35265;&#12290;&#20256;&#32479;&#30340;&#20559;&#35265;&#35843;&#26597;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27979;&#35797;&#29992;&#20363;&#36890;&#24120;&#25104;&#26412;&#39640;&#26114;&#19988;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#21019;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#29992;&#20110;&#26816;&#27979;LLMs&#28508;&#22312;&#24615;&#21035;&#20559;&#35265;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19977;&#31181;&#33879;&#21517;&#30340;LLM&#65292;&#24182;&#21457;&#29616;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#20102;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#38024;&#23545;&#25152;&#21457;&#29616;&#30340;&#20559;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#31574;&#30053;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#65292;&#26469;&#35268;&#36991;&#21442;&#25968;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#36890;&#36807;&#36825;&#31181;&#25552;&#35758;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#26356;&#20844;&#24179;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, researchers have made considerable improvements in dialogue systems with the progress of large language models (LLMs) such as ChatGPT and GPT-4. These LLM-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions. The traditional biases investigation methods often rely on human-written test cases. However, these test cases are usually expensive and limited. In this work, we propose a first-of-its-kind method that automatically generates test cases to detect LLMs' potential gender bias. We apply our method to three well-known LLMs and find that the generated test cases effectively identify the presence of biases. To address the biases identified, we propose a mitigation strategy that uses the generated test cases as demonstrations for in-context learning to circumvent the need for parameter fine-tuning. The experimental results show that LLMs generate fairer responses with the proposed approach.
&lt;/p&gt;</description></item><item><title>VoxArabica&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#35328;&#24863;&#30693;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#24320;&#21457;&#21644;&#28436;&#31034;&#65292;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35821;&#26041;&#35328;&#35782;&#21035;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#35813;&#31995;&#32479;&#35757;&#32451;&#20102;&#21508;&#31181;&#27169;&#22411;&#29992;&#20110;&#19981;&#21516;&#26041;&#35328;&#30340;&#35782;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#21151;&#33021;&#30340;&#32593;&#32476;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.11069</link><description>&lt;p&gt;
VoxArabica&#65306;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#35328;&#24863;&#30693;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System. (arXiv:2310.11069v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11069
&lt;/p&gt;
&lt;p&gt;
VoxArabica&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#35328;&#24863;&#30693;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#24320;&#21457;&#21644;&#28436;&#31034;&#65292;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35821;&#26041;&#35328;&#35782;&#21035;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#35813;&#31995;&#32479;&#35757;&#32451;&#20102;&#21508;&#31181;&#27169;&#22411;&#29992;&#20110;&#19981;&#21516;&#26041;&#35328;&#30340;&#35782;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#21151;&#33021;&#30340;&#32593;&#32476;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#25289;&#20271;&#35821;&#26159;&#19968;&#31181;&#22797;&#26434;&#30340;&#35821;&#35328;&#65292;&#20840;&#29699;&#26377;&#36229;&#36807;4.5&#20159;&#20154;&#21475;&#20351;&#29992;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#35328;&#21644;&#21475;&#38899;&#12290;&#30001;&#20110;&#35821;&#35328;&#30340;&#22810;&#26679;&#24615;&#21644;&#21464;&#21270;&#65292;&#20026;&#38463;&#25289;&#20271;&#35821;&#26500;&#24314;&#19968;&#20010;&#31283;&#20581;&#19988;&#36890;&#29992;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#21644;&#28436;&#31034;&#19968;&#20010;&#21517;&#20026;VoxArabica&#30340;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#29992;&#20110;&#26041;&#35328;&#35782;&#21035;(DID)&#21644;&#38463;&#25289;&#20271;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#12290;&#25105;&#20204;&#22312;&#30417;&#30563;&#29615;&#22659;&#19979;&#35757;&#32451;&#20102;&#21508;&#31181;&#27169;&#22411;&#65292;&#20363;&#22914;HuBERT(DID)&#12289;Whisper&#21644;XLS-R(ASR)&#65292;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#30340;DID&#21644;ASR&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;DID&#27169;&#22411;&#34987;&#35757;&#32451;&#29992;&#20110;&#35782;&#21035;&#38500;&#20102;&#26631;&#20934;&#38463;&#25289;&#20271;&#20043;&#22806;&#30340;17&#31181;&#19981;&#21516;&#30340;&#26041;&#35328;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;(MSA)&#12289;&#22467;&#21450;&#35821;&#12289;&#25705;&#27931;&#21733;&#35821;&#21644;&#28151;&#21512;&#25968;&#25454;&#19978;&#24494;&#35843;&#25105;&#20204;&#30340;ASR&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;ASR&#20013;&#30340;&#20854;&#20182;&#26041;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Whisper&#21644;MMS&#31561;&#19981;&#21516;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#38598;&#25104;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#32593;&#32476;&#30028;&#38754;&#20013;&#65292;&#20855;&#26377;&#22810;&#26679;&#30340;&#21151;&#33021;&#65292;&#22914;&#38899;&#39057;&#24405;&#21046;&#12289;&#19978;&#20256;&#25991;&#20214;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#20986;&#38382;&#39064;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Arabic is a complex language with many varieties and dialects spoken by over 450 millions all around the world. Due to the linguistic diversity and variations, it is challenging to build a robust and generalized ASR system for Arabic. In this work, we address this gap by developing and demoing a system, dubbed VoxArabica, for dialect identification (DID) as well as automatic speech recognition (ASR) of Arabic. We train a wide range of models such as HuBERT (DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR tasks. Our DID models are trained to identify 17 different dialects in addition to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data. Additionally, for the remaining dialects in ASR, we provide the option to choose various models such as Whisper and MMS in a zero-shot setting. We integrate these models into a single web interface with diverse features such as audio recording, file upload, model selection, and the option to raise fl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Moral Foundation Theory&#21644;DeNEVIL&#31639;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20215;&#20540;&#65292;&#24182;&#26500;&#24314;&#20102;MoralPrompt&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20869;&#22312;&#20215;&#20540;&#12290;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#19981;&#23545;&#40784;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.11053</link><description>&lt;p&gt;
Denevil: &#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#26469;&#35299;&#35835;&#21644;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning. (arXiv:2310.11053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11053
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Moral Foundation Theory&#21644;DeNEVIL&#31639;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20215;&#20540;&#65292;&#24182;&#26500;&#24314;&#20102;MoralPrompt&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20869;&#22312;&#20215;&#20540;&#12290;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#19981;&#23545;&#40784;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#31361;&#30772;&#65292;&#28982;&#32780;&#23427;&#20204;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#26085;&#24120;&#29983;&#27963;&#20013;&#21487;&#33021;&#20250;&#24102;&#26469;&#30001;&#29983;&#25104;&#30340;&#19981;&#36947;&#24503;&#20869;&#23481;&#24341;&#36215;&#30340;&#31038;&#20250;&#39118;&#38505;&#12290;&#23613;&#31649;&#24050;&#32463;&#23545;&#29305;&#23450;&#38382;&#39064;&#22914;&#20559;&#35265;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#20174;&#36947;&#24503;&#21746;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;LLM&#30340;&#20869;&#22312;&#20215;&#20540;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#36947;&#24503;&#22522;&#30784;&#29702;&#35770;&#28145;&#20837;&#25506;&#35752;&#36947;&#24503;&#20215;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeNEVIL&#65292;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#65292;&#26088;&#22312;&#21160;&#24577;&#21033;&#29992;LLM&#30340;&#20215;&#20540;&#33030;&#24369;&#24615;&#24182;&#20197;&#29983;&#25104;&#26041;&#24335;&#25581;&#31034;&#20262;&#29702;&#36829;&#35268;&#34892;&#20026;&#65292;&#25581;&#31034;&#20854;&#28508;&#22312;&#30340;&#20215;&#20540;&#20542;&#21521;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MoralPrompt&#65292;&#19968;&#20010;&#21253;&#21547;2,397&#20010;&#25552;&#31034;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;500&#22810;&#20010;&#20215;&#20540;&#21407;&#21017;&#65292;&#24182;&#23545;&#19968;&#31995;&#21015;LLM&#30340;&#20869;&#22312;&#20215;&#20540;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23454;&#36136;&#19978;&#26159;&#19981;&#23545;&#40784;&#30340;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values utilizing Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs. We discovered that most models are essentially misaligned, necessitating further ethical value alignment. In r
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#27861;&#24459;&#35780;&#20272;&#20219;&#21153;6&#19978;&#30340;&#25552;&#20132;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21644;&#24102;&#35299;&#37322;&#30340;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#31561;&#23376;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#21508;&#20010;&#23376;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2310.11049</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;6&#20013;&#30340;&#38750;&#32435;&#20219;&#21153;:&#27861;&#24459;&#35780;&#20272;&#26041;&#27861;&#35770;&#12290;(arXiv:2310.11049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation. (arXiv:2310.11049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#27861;&#24459;&#35780;&#20272;&#20219;&#21153;6&#19978;&#30340;&#25552;&#20132;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21644;&#24102;&#35299;&#37322;&#30340;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#31561;&#23376;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#21508;&#20010;&#23376;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#27861;&#24459;&#35780;&#20272;&#20219;&#21153;6&#19978;&#30340;&#25552;&#20132;&#12290;&#25105;&#20204;&#30340;&#25552;&#20132;&#20027;&#35201;&#38598;&#20013;&#22312;&#19977;&#20010;&#23376;&#20219;&#21153;&#19978;&#65306;&#20219;&#21153;B&#30340;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(L-NER)&#65292;&#20219;&#21153;C1&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;(LJP)&#21644;&#20219;&#21153;C2&#30340;&#24102;&#35299;&#37322;&#30340;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;(CJPE)&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#23376;&#20219;&#21153;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#65292;&#24182;&#35814;&#32454;&#21576;&#29616;&#20102;&#32467;&#26524;&#65292;&#21253;&#25324;&#25968;&#25454;&#32479;&#35745;&#21644;&#26041;&#27861;&#35770;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20687;&#26412;&#30740;&#31350;&#20013;&#25152;&#28041;&#21450;&#30340;&#27861;&#24459;&#20219;&#21153;&#27491;&#22312;&#22240;&#33258;&#21160;&#21270;&#27861;&#24459;&#20998;&#26512;&#21644;&#25903;&#25345;&#30340;&#38656;&#27714;&#22686;&#21152;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#25490;&#34892;&#27036;&#19978;&#25253;&#21578;&#30340;&#20219;&#21153;B&#12289;&#20219;&#21153;C1&#21644;&#20219;&#21153;C2&#20013;&#20998;&#21035;&#33719;&#24471;&#20102;15th&#12289;11th&#21644;1st&#30340;&#31454;&#20105;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to the SemEval-2023 for Task 6 on LegalEval: Understanding Legal Texts. Our submission concentrated on three subtasks: Legal Named Entity Recognition (L-NER) for Task-B, Legal Judgment Prediction (LJP) for Task-C1, and Court Judgment Prediction with Explanation (CJPE) for Task-C2. We conducted various experiments on these subtasks and presented the results in detail, including data statistics and methodology. It is worth noting that legal tasks, such as those tackled in this research, have been gaining importance due to the increasing need to automate legal analysis and support. Our team obtained competitive rankings of 15$^{th}$, 11$^{th}$, and 1$^{st}$ in Task-B, Task-C1, and Task-C2, respectively, as reported on the leaderboard.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27468;&#35789;&#21019;&#20316;&#32773;&#21644;&#28436;&#21809;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#27468;&#35789;-&#21019;&#20316;&#32773;&#20998;&#31867;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#28436;&#21809;&#32773;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11035</link><description>&lt;p&gt;
&#27468;&#35789;&#21019;&#20316;&#32773;&#27468;&#25163;&#29109;&#24433;&#21709;&#27468;&#35789;-&#21019;&#20316;&#32773;&#20998;&#31867;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Lyricist-Singer Entropy Affects Lyric-Lyricist Classification Performance. (arXiv:2310.11035v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27468;&#35789;&#21019;&#20316;&#32773;&#21644;&#28436;&#21809;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#27468;&#35789;-&#21019;&#20316;&#32773;&#20998;&#31867;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#28436;&#21809;&#32773;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#27468;&#35789;&#26159;&#38899;&#20048;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23545;&#27468;&#35789;&#21019;&#20316;&#32773;&#30340;&#29305;&#24449;&#36827;&#34892;&#38899;&#20048;&#20449;&#24687;&#22788;&#29702;&#30740;&#31350;&#30340;&#25968;&#37327;&#24456;&#23569;&#12290;&#30001;&#20110;&#36825;&#20123;&#29305;&#24449;&#22312;&#38899;&#20048;&#24212;&#29992;&#20013;&#21487;&#33021;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#20174;&#27468;&#35789;&#20013;&#25552;&#21462;&#20195;&#34920;&#27468;&#35789;&#21019;&#20316;&#32773;&#29305;&#24449;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#30001;&#20110;&#36825;&#20123;&#29305;&#24449;&#24517;&#39035;&#22312;&#25552;&#21462;&#20043;&#21069;&#36827;&#34892;&#35782;&#21035;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20855;&#26377;&#26131;&#20110;&#35782;&#21035;&#29305;&#24449;&#30340;&#27468;&#35789;&#21019;&#20316;&#32773;&#19978;&#12290;&#25105;&#20204;&#35748;&#20026;&#27468;&#25163;&#24212;&#35813;&#28436;&#21809;&#20855;&#26377;&#29305;&#23450;&#20110;&#27468;&#25163;&#30340;&#26576;&#20123;&#29305;&#24449;&#30340;&#29420;&#29305;&#27468;&#26354;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#27468;&#35789;&#21019;&#20316;&#32773;&#23545;&#20182;&#20204;&#20026;&#20854;&#20889;&#27468;&#27468;&#25163;&#30340;&#29420;&#29305;&#29305;&#24449;&#36127;&#26377;&#36131;&#20219;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#27468;&#35789;-&#21019;&#20316;&#32773;&#20998;&#31867;&#30340;&#24615;&#33021;&#25110;&#20174;&#27468;&#35789;&#20013;&#25429;&#25417;&#21040;&#27468;&#35789;&#21019;&#20316;&#32773;&#29305;&#24449;&#30340;&#38590;&#26131;&#31243;&#24230;&#21487;&#33021;&#21462;&#20915;&#20110;&#27468;&#25163;&#30340;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27468;&#35789;&#21019;&#20316;&#32773;&#21644;&#27468;&#25163;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although lyrics represent an essential component of music, few music information processing studies have been conducted on the characteristics of lyricists. Because these characteristics may be valuable for musical applications, such as recommendations, they warrant further study. We considered a potential method that extracts features representing the characteristics of lyricists from lyrics. Because these features must be identified prior to extraction, we focused on lyricists with easily identifiable features. We believe that it is desirable for singers to perform unique songs that share certain characteristics specific to the singer. Accordingly, we hypothesized that lyricists account for the unique characteristics of the singers they write lyrics for. In other words, lyric-lyricist classification performance or the ease of capturing the features of a lyricist from the lyrics may depend on the variety of singers. In this study, we observed a relationship between lyricist-singer ent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30456;&#27604;&#35843;&#20248;&#21518;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#35843;&#20248;&#21518;&#30340;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#21407;&#22240;&#26159;&#35299;&#30721;&#22120;&#27169;&#22411;&#26356;&#20851;&#27880;&#34920;&#38754;&#35789;&#24207;&#21015;&#32780;&#24573;&#30053;&#20102;&#21547;&#20041;&#65292;&#24182;&#19988;&#38750;&#24120;&#22823;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20351;&#24471;&#38590;&#20197;&#35782;&#21035;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.11026</link><description>&lt;p&gt;
&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation. (arXiv:2310.11026v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30456;&#27604;&#35843;&#20248;&#21518;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#35843;&#20248;&#21518;&#30340;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#21407;&#22240;&#26159;&#35299;&#30721;&#22120;&#27169;&#22411;&#26356;&#20851;&#27880;&#34920;&#38754;&#35789;&#24207;&#21015;&#32780;&#24573;&#30053;&#20102;&#21547;&#20041;&#65292;&#24182;&#19988;&#38750;&#24120;&#22823;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20351;&#24471;&#38590;&#20197;&#35782;&#21035;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25552;&#39640;&#29983;&#25104;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;&#24403;&#21069;&#36235;&#21183;&#26159;&#20351;&#29992;&#36234;&#26469;&#36234;&#22823;&#30340;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30740;&#31350;&#22522;&#20110;&#36825;&#31181;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#20248;&#65292;&#20998;&#21035;&#22312;&#20004;&#31181;&#19981;&#21516;&#20219;&#21153;&#65288;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65289;&#20197;&#21450;&#20004;&#31181;&#35821;&#35328;&#65288;&#26085;&#35821;&#21644;&#33521;&#35821;&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#35843;&#20248;&#21518;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#35843;&#20248;&#21518;&#30340;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#34920;&#29616;&#36739;&#24046;&#12290;&#23545;&#20110;&#36896;&#25104;&#36825;&#31181;&#24773;&#20917;&#30340;&#21407;&#22240;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#26356;&#20851;&#27880;&#34920;&#38754;&#35789;&#24207;&#21015;&#32780;&#24573;&#30053;&#20102;&#21547;&#20041;&#12290;&#30740;&#31350;&#36824;&#26174;&#31034;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#38750;&#24120;&#22823;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20351;&#24471;&#38590;&#20197;&#35782;&#21035;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic evaluation of text generation is essential for improving the accuracy of generation tasks. In light of the current trend towards increasingly larger decoder-based language models, we investigate automatic evaluation methods based on such models for text generation. This paper compares various methods, including tuning with encoder-based models and large language models under equal conditions, on two different tasks, machine translation evaluation and semantic textual similarity, in two languages, Japanese and English. Experimental results show that compared to the tuned encoder-based models, the tuned decoder-based models perform poorly. The analysis of the causes for this suggests that the decoder-based models focus on surface word sequences and do not capture meaning. It is also revealed that in-context learning of very large decoder-based models such as ChatGPT makes it difficult to identify fine-grained semantic differences.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20196;&#29260;&#36335;&#24452;&#39044;&#27979;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23454;&#20307;&#26631;&#35760;&#30340;&#38405;&#35835;&#39034;&#24207;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#25991;&#26723;&#24067;&#23616;&#24314;&#27169;&#20026;&#20196;&#29260;&#30340;&#26377;&#21521;&#22270;&#65292;&#24182;&#39044;&#27979;&#23454;&#20307;&#30340;&#20196;&#29260;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.11016</link><description>&lt;p&gt;
&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#38405;&#35835;&#39034;&#24207;&#30340;&#37325;&#35201;&#24615;&#65306;&#36890;&#36807;&#20196;&#29260;&#36335;&#24452;&#39044;&#27979;&#26469;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction. (arXiv:2310.11016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20196;&#29260;&#36335;&#24452;&#39044;&#27979;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23454;&#20307;&#26631;&#35760;&#30340;&#38405;&#35835;&#39034;&#24207;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#25991;&#26723;&#24067;&#23616;&#24314;&#27169;&#20026;&#20196;&#29260;&#30340;&#26377;&#21521;&#22270;&#65292;&#24182;&#39044;&#27979;&#23454;&#20307;&#30340;&#20196;&#29260;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#65288;VrDs&#65289;&#20013;&#30340;&#20449;&#24687;&#25552;&#21462;&#65292;&#20854;&#20013;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#34987;&#35270;&#20026;&#39044;&#27979;&#20196;&#29260;&#30340;BIO&#23454;&#20307;&#26631;&#31614;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#65292;&#36981;&#24490;NLP&#30340;&#20856;&#22411;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;BIO&#26631;&#35760;&#26041;&#26696;&#20381;&#36182;&#20110;&#27169;&#22411;&#36755;&#20837;&#30340;&#27491;&#30830;&#39034;&#24207;&#65292;&#22312;OCR&#31995;&#32479;&#35782;&#21035;&#21644;&#23433;&#25490;&#25991;&#26412;&#30340;&#25195;&#25551;VrDs&#20013;&#26080;&#27861;&#20445;&#35777;&#12290;&#36825;&#31181;&#38405;&#35835;&#39034;&#24207;&#38382;&#39064;&#38459;&#30861;&#20102;&#36890;&#36807;BIO&#26631;&#35760;&#26041;&#26696;&#20934;&#30830;&#26631;&#35760;&#23454;&#20307;&#65292;&#20351;&#24471;&#24207;&#21015;&#26631;&#27880;&#26041;&#27861;&#26080;&#27861;&#39044;&#27979;&#27491;&#30830;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#20026;&#20102;&#35299;&#20915;&#38405;&#35835;&#39034;&#24207;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20196;&#29260;&#36335;&#24452;&#39044;&#27979;&#65288;TPP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#39044;&#27979;&#22836;&#65292;&#29992;&#20110;&#22312;&#25991;&#26723;&#20013;&#39044;&#27979;&#20316;&#20026;&#20196;&#29260;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21450;&#12290;&#19982;&#20196;&#29260;&#20998;&#31867;&#19981;&#21516;&#65292;TPP&#23558;&#25991;&#26723;&#24067;&#23616;&#24314;&#27169;&#20026;&#20196;&#29260;&#30340;&#23436;&#25972;&#26377;&#21521;&#22270;&#65292;&#24182;&#39044;&#27979;&#23454;&#20307;&#22312;&#22270;&#20013;&#30340;&#20196;&#29260;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in multimodal pre-trained models have significantly improved information extraction from visually-rich documents (VrDs), in which named entity recognition (NER) is treated as a sequence-labeling task of predicting the BIO entity tags for tokens, following the typical setting of NLP. However, BIO-tagging scheme relies on the correct order of model inputs, which is not guaranteed in real-world NER on scanned VrDs where text are recognized and arranged by OCR systems. Such reading order issue hinders the accurate marking of entities by BIO-tagging scheme, making it impossible for sequence-labeling methods to predict correct named entities. To address the reading order issue, we introduce Token Path Prediction (TPP), a simple prediction head to predict entity mentions as token sequences within documents. Alternative to token classification, TPP models the document layout as a complete directed graph of tokens, and predicts token paths within the graph as entities. For bette
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#35821;&#38899;&#35782;&#21035;&#36827;&#34892;&#32416;&#38169;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#26131;&#20986;&#38169;&#21333;&#35789;&#30340;&#20998;&#25968;&#24182;&#23558;&#20854;&#29992;&#20316;&#20808;&#39564;&#20998;&#24067;&#26469;&#25351;&#23548;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32416;&#38169;&#22411;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#20013;&#26377;&#25928;&#65292;&#30456;&#23545;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#21333;&#35789;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.11003</link><description>&lt;p&gt;
&#20026;&#35821;&#38899;&#35782;&#21035;&#36827;&#34892;&#32416;&#38169;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Correction Focused Language Model Training for Speech Recognition. (arXiv:2310.11003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#35821;&#38899;&#35782;&#21035;&#36827;&#34892;&#32416;&#38169;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#26131;&#20986;&#38169;&#21333;&#35789;&#30340;&#20998;&#25968;&#24182;&#23558;&#20854;&#29992;&#20316;&#20808;&#39564;&#20998;&#24067;&#26469;&#25351;&#23548;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32416;&#38169;&#22411;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#20013;&#26377;&#25928;&#65292;&#30456;&#23545;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#21333;&#35789;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36890;&#24120;&#34987;&#29992;&#20110;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#20013;&#12290;&#20256;&#32479;&#30340;LM&#35757;&#32451;&#26041;&#24335;&#23558;&#35821;&#26009;&#24211;&#20013;&#30340;&#25152;&#26377;&#21333;&#35789;&#24179;&#31561;&#23545;&#24453;&#65292;&#23548;&#33268;ASR&#24615;&#33021;&#30340;&#25913;&#36827;&#25928;&#26524;&#19981;&#20339;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32416;&#38169;&#22411;LM&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#20248;&#20808;&#22788;&#29702;ASR&#26131;&#20986;&#38169;&#30340;&#21333;&#35789;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#21333;&#35789;&#32423;ASR&#26131;&#20986;&#38169;&#20998;&#25968;&#65292;&#34920;&#31034;ASR&#38169;&#35823;&#35782;&#21035;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#23558;&#20854;&#24418;&#25104;&#19968;&#20010;&#20808;&#39564;&#21333;&#35789;&#20998;&#24067;&#20197;&#25351;&#23548;LM&#35757;&#32451;&#12290;&#20026;&#20102;&#22312;&#20165;&#26377;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#32416;&#38169;&#22411;&#35757;&#32451;&#65292;&#25105;&#20204;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#26131;&#20986;&#38169;&#20998;&#25968;&#39044;&#27979;&#22120;&#21644;&#25991;&#26412;&#29983;&#25104;&#22120;&#65292;&#24182;&#36827;&#34892;&#22810;&#20219;&#21153;&#24494;&#35843;&#12290;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;LM&#30456;&#27604;&#65292;&#32416;&#38169;&#22411;&#35757;&#32451;&#22312;&#36275;&#22815;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#36798;&#21040;&#30456;&#23545;5.5%&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have been commonly adopted to boost the performance of automatic speech recognition (ASR) particularly in domain adaptation tasks. Conventional way of LM training treats all the words in corpora equally, resulting in suboptimal improvements in ASR performance. In this work, we introduce a novel correction focused LM training approach which aims to prioritize ASR fallible words. The word-level ASR fallibility score, representing the likelihood of ASR mis-recognition, is defined and shaped as a prior word distribution to guide the LM training. To enable correction focused training with text-only corpora, large language models (LLMs) are employed as fallibility score predictors and text generators through multi-task fine-tuning. Experimental results for domain adaptation tasks demonstrate the effectiveness of our proposed method. Compared with conventional LMs, correction focused training achieves up to relatively 5.5% word error rate (WER) reduction in sufficient te
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#23545;&#35805;&#25688;&#35201;&#26041;&#27861;&#26080;&#27861;&#32771;&#34385;&#29992;&#25143;&#30340;&#29305;&#23450;&#20852;&#36259;&#65292;&#32780;&#25351;&#23548;&#23545;&#35805;&#25688;&#35201;&#30340;&#24341;&#20837;&#21487;&#20197;&#24110;&#21161;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#26469;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26597;&#35810;&#25688;&#35201;&#19977;&#20803;&#32452;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#26469;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10981</link><description>&lt;p&gt;
&#20351;&#29992;&#26597;&#35810;&#32858;&#21512;&#30340;&#25351;&#23548;&#24615;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Instructive Dialogue Summarization with Query Aggregations. (arXiv:2310.10981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10981
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#23545;&#35805;&#25688;&#35201;&#26041;&#27861;&#26080;&#27861;&#32771;&#34385;&#29992;&#25143;&#30340;&#29305;&#23450;&#20852;&#36259;&#65292;&#32780;&#25351;&#23548;&#23545;&#35805;&#25688;&#35201;&#30340;&#24341;&#20837;&#21487;&#20197;&#24110;&#21161;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#26469;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26597;&#35810;&#25688;&#35201;&#19977;&#20803;&#32452;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#26469;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#23545;&#35805;&#25688;&#35201;&#26041;&#27861;&#30452;&#25509;&#29983;&#25104;&#25688;&#35201;&#65292;&#19981;&#32771;&#34385;&#29992;&#25143;&#30340;&#29305;&#23450;&#20852;&#36259;&#12290;&#36825;&#22312;&#29992;&#25143;&#26356;&#21152;&#20851;&#27880;&#29305;&#23450;&#20027;&#39064;&#25110;&#26041;&#38754;&#30340;&#24773;&#20917;&#19979;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#38543;&#30528;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25351;&#23548;&#23545;&#35805;&#26469;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#38598;&#12290;&#20026;&#20102;&#20811;&#26381;&#25351;&#23548;&#24615;&#23545;&#35805;&#25688;&#35201;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#26041;&#27861;&#26469;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#25688;&#35201;&#19977;&#20803;&#32452;&#12290;&#36825;&#20010;&#36807;&#31243;&#21253;&#25324;&#20197;&#25688;&#35201;&#20026;&#38170;&#28857;&#30340;&#26597;&#35810;&#29983;&#25104;&#12289;&#26597;&#35810;&#36807;&#28388;&#21644;&#22522;&#20110;&#26597;&#35810;&#30340;&#25688;&#35201;&#29983;&#25104;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;InstructDS&#65288;&#25351;&#23548;&#24615;&#23545;&#35805;&#25688;&#35201;&#65289;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#23545;&#35805;&#25688;&#35201;&#21644;&#23545;&#35805;&#38405;&#35835;&#29702;&#35299;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional dialogue summarization methods directly generate summaries and do not consider user's specific interests. This poses challenges in cases where the users are more focused on particular topics or aspects. With the advancement of instruction-finetuned language models, we introduce instruction-tuning to dialogues to expand the capability set of dialogue summarization models. To overcome the scarcity of instructive dialogue summarization data, we propose a three-step approach to synthesize high-quality query-based summarization triples. This process involves summary-anchored query generation, query filtering, and query-based summary generation. By training a unified model called InstructDS (Instructive Dialogue Summarization) on three summarization datasets with multi-purpose instructive triples, we expand the capability of dialogue summarization models. We evaluate our method on four datasets, including dialogue summarization and dialogue reading comprehension. Experimental re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#22810;&#27169;&#24577;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#26500;&#24314;&#26694;&#26550;(MDCF)&#35774;&#35745;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#27169;&#22411;&#23545;&#20110;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#32570;&#20047;&#12289;&#29983;&#25104;&#20869;&#23481;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.10967</link><description>&lt;p&gt;
EXMODD:&#19968;&#31181;&#35299;&#37322;&#24615;&#22810;&#27169;&#24577;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset. (arXiv:2310.10967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10967
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#22810;&#27169;&#24577;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#26500;&#24314;&#26694;&#26550;(MDCF)&#35774;&#35745;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#27169;&#22411;&#23545;&#20110;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#32570;&#20047;&#12289;&#29983;&#25104;&#20869;&#23481;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#38656;&#27714;&#19968;&#30452;&#26159;&#38459;&#30861;&#23545;&#35805;&#20219;&#21153;&#30740;&#31350;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#25163;&#24037;&#12289;&#32593;&#32476;&#29228;&#34411;&#21644;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#65292;&#20174;&#20114;&#32852;&#32593;&#25910;&#38598;&#30340;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#36890;&#20439;&#22238;&#31572;&#12289;&#26080;&#24847;&#20041;&#30340;&#38472;&#36848;&#21644;&#26377;&#23475;&#23545;&#35805;&#12290;&#36890;&#36807;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#25968;&#25454;&#29983;&#25104;&#26159;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22810;&#27169;&#24577;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20219;&#21153;&#65292;&#20173;&#23384;&#22312;&#19977;&#20010;&#32570;&#28857;&#65306;1) &#30446;&#21069;&#36824;&#27809;&#26377;&#33021;&#25509;&#21463;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#24320;&#28304;&#22823;&#22411;&#27169;&#22411;&#65307;2) &#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65307;3) &#29983;&#25104;&#30340;&#25968;&#25454;&#36890;&#24120;&#38590;&#20197;&#36827;&#34892;&#36136;&#37327;&#25511;&#21046;&#24182;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#36827;&#34892;&#25910;&#38598;&#12290;&#20026;&#20102;&#20943;&#36731;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#37325;&#35201;&#20154;&#21147;&#21644;&#36164;&#28304;&#24320;&#25903;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#25968;&#25454;&#26500;&#24314;&#26694;&#26550;(MDCF)&#12290;MDCF&#35774;&#35745;&#36866;&#24403;&#30340;&#25552;&#31034;&#26469;&#25512;&#21160;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24418;&#24335;&#33391;&#22909;&#19988;&#20196;&#20154;&#28385;&#24847;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for high-quality data has been a key issue hindering the research of dialogue tasks. Recent studies try to build datasets through manual, web crawling, and large pre-trained models. However, man-made data is expensive and data collected from the internet often includes generic responses, meaningless statements, and toxic dialogues. Automatic data generation through large models is a cost-effective method, but for open-domain multimodal dialogue tasks, there are still three drawbacks: 1) There is currently no open-source large model that can accept multimodal input; 2) The content generated by the model lacks interpretability; 3) The generated data is usually difficult to quality control and require extensive resource to collect. To alleviate the significant human and resource expenditure in data collection, we propose a Multimodal Data Construction Framework (MDCF). MDCF designs proper prompts to spur the large-scale pre-trained language model to generate well-formed and satis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SemCSR&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#33021;&#21147;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#23601;&#33021;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26679;&#24335;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.10962</link><description>&lt;p&gt;
&#24102;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#24863;&#30693;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models. (arXiv:2310.10962v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SemCSR&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#33021;&#21147;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#23601;&#33021;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26679;&#24335;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#23398;&#20064;&#26356;&#22909;&#30340;&#21477;&#23376;&#34920;&#31034;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#35757;&#32451;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#38656;&#35201;&#22823;&#37327;&#24102;&#26631;&#31614;&#30340;&#21477;&#23376;&#26469;&#26126;&#30830;&#26500;&#24314;&#27491;&#36127;&#23545;&#65288;&#20363;&#22914;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#65289;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#33719;&#21462;&#36275;&#22815;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#26082;&#36153;&#26102;&#21448;&#32791;&#36164;&#28304;&#65292;&#22240;&#27492;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20851;&#27880;&#24320;&#21457;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#30001;&#20110;&#36825;&#20123;&#38750;&#32467;&#26500;&#21270;&#30340;&#38543;&#26426;&#25277;&#26679;&#21477;&#23376;&#20043;&#38388;&#27809;&#26377;&#26126;&#30830;&#30340;&#20851;&#32852;&#65292;&#26500;&#24314;&#27491;&#36127;&#23545;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#21644;&#26377;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#26694;&#26550;&#65288;SemCSR&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#33021;&#21147;&#65292;&#25105;&#20204;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26679;&#24335;&#35821;&#26009;&#24211;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20154;&#24037;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has been proven to be effective in learning better sentence representations. However, to train a contrastive learning model, large numbers of labeled sentences are required to construct positive and negative pairs explicitly, such as those in natural language inference (NLI) datasets. Unfortunately, acquiring sufficient high-quality labeled data can be both time-consuming and resource-intensive, leading researchers to focus on developing methods for learning unsupervised sentence representations. As there is no clear relationship between these unstructured randomly-sampled sentences, building positive and negative pairs over them is tricky and problematic. To tackle these challenges, in this paper, we propose SemCSR, a semantic-aware contrastive sentence representation framework. By leveraging the generation and evaluation capabilities of large language models (LLMs), we can automatically construct a high-quality NLI-style corpus without any human annotation, and f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20960;&#20309;&#20998;&#26512;&#65292;&#25105;&#20204;&#35745;&#31639;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#38190;&#30424;&#35774;&#35745;&#65292;&#25552;&#20379;&#26356;&#24555;&#30340;&#25171;&#23383;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.10956</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#33521;&#35821;&#35821;&#35328;&#30340;&#20960;&#20309;&#20998;&#26512;&#26469;&#35745;&#31639;&#26368;&#20339;&#38190;&#30424;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Computing the optimal keyboard through a geometric analysis of the English language. (arXiv:2310.10956v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10956
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20960;&#20309;&#20998;&#26512;&#65292;&#25105;&#20204;&#35745;&#31639;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#38190;&#30424;&#35774;&#35745;&#65292;&#25552;&#20379;&#26356;&#24555;&#30340;&#25171;&#23383;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;COMSW4995 002 - &#20960;&#20309;&#25968;&#25454;&#20998;&#26512;&#35838;&#31243;&#30340;&#23567;&#32452;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#23558;&#20851;&#27880;&#28857;&#25918;&#22312;&#35774;&#35745;&#24555;&#36895;&#25171;&#23383;&#38190;&#30424;&#19978;&#12290;&#21033;&#29992;&#19968;&#20123;&#20960;&#20309;&#24037;&#20855;&#22312;&#20248;&#21270;&#26694;&#26550;&#20013;&#25552;&#20379;&#20102;&#26356;&#24555;&#25171;&#23383;&#30340;&#21019;&#26032;&#38190;&#30424;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of a group project for the course COMSW4995 002 - Geometric Data Analysis, we bring our attention to the design of fast-typing keyboards. Leveraging some geometric tools in an optimization framework allowed us to propose novel keyboard layouts that offer a faster typing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#30740;&#31350;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#24120;&#29992;&#30340;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#36825;&#20123;&#25928;&#26524;&#38598;&#20013;&#22312;&#20960;&#20010;&#35821;&#35328;&#32500;&#24230;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#38750;&#30456;&#20851;&#32500;&#24230;&#20135;&#29983;"&#28322;&#20986;"&#25928;&#24212;&#12290;&#36825;&#20010;&#26694;&#26550;&#20026;&#36127;&#36131;&#20219;&#21644;&#40065;&#26834;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#25968;&#25454;&#38598;&#25928;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.10955</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#38598;&#25928;&#26524;&#30340;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A State-Vector Framework for Dataset Effects. (arXiv:2310.10955v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#30740;&#31350;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#24120;&#29992;&#30340;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#36825;&#20123;&#25928;&#26524;&#38598;&#20013;&#22312;&#20960;&#20010;&#35821;&#35328;&#32500;&#24230;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#38750;&#30456;&#20851;&#32500;&#24230;&#20135;&#29983;"&#28322;&#20986;"&#25928;&#24212;&#12290;&#36825;&#20010;&#26694;&#26550;&#20026;&#36127;&#36131;&#20219;&#21644;&#40065;&#26834;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#25968;&#25454;&#38598;&#25928;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#31995;&#32479;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20173;&#28982;&#19981;&#22815;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;&#65292;&#20197;&#20415;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20005;&#26684;&#30340;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#23558;&#29702;&#24819;&#21270;&#25506;&#27979;&#27979;&#35797;&#32467;&#26524;&#20316;&#20026;&#21521;&#37327;&#31354;&#38388;&#30340;&#22522;&#30784;&#12290;&#35813;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#29420;&#31435;&#21644;&#20114;&#21160;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#24120;&#29992;&#30340;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#26174;&#33879;&#25928;&#26524;&#26159;&#29305;&#24449;&#24615;&#30340;&#65292;&#24182;&#19988;&#38598;&#20013;&#22312;&#20960;&#20010;&#35821;&#35328;&#32500;&#24230;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#20123;"&#28322;&#20986;"&#25928;&#24212;&#65306;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#22312;&#30475;&#20284;&#19982;&#39044;&#26399;&#20219;&#21153;&#26080;&#20851;&#30340;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;&#20026;&#31995;&#32479;&#22320;&#29702;&#35299;&#25968;&#25454;&#38598;&#25928;&#26524;&#65292;&#36825;&#26159;&#36127;&#36131;&#20219;&#21644;&#40065;&#26834;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive success of recent deep neural network (DNN)-based systems is significantly influenced by the high-quality datasets used in training. However, the effects of the datasets, especially how they interact with each other, remain underexplored. We propose a state-vector framework to enable rigorous studies in this direction. This framework uses idealized probing test results as the bases of a vector space. This framework allows us to quantify the effects of both standalone and interacting datasets. We show that the significant effects of some commonly-used language understanding datasets are characteristic and are concentrated on a few linguistic dimensions. Additionally, we observe some ``spill-over'' effects: the datasets could impact the models along dimensions that may seem unrelated to the intended tasks. Our state-vector framework paves the way for a systematic understanding of the dataset effects, a crucial component in responsible and robust model development.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TEQ&#65292;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#31561;&#25928;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#20302;&#31934;&#24230;&#30340;&#37327;&#21270;&#26041;&#24335;&#28385;&#36275;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#35201;&#27714;&#12290;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20856;&#22411;&#30340;LLMs&#19978;&#20855;&#26377;&#30456;&#20284;&#30340;&#34920;&#29616;&#65292;&#19988;&#19981;&#22686;&#21152;&#25512;&#29702;&#26102;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2310.10944</link><description>&lt;p&gt;
TEQ&#65306;&#29992;&#20110;LLM&#37327;&#21270;&#30340;&#21487;&#35757;&#32451;&#31561;&#25928;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
TEQ: Trainable Equivalent Transformation for Quantization of LLMs. (arXiv:2310.10944v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TEQ&#65292;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#31561;&#25928;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#20302;&#31934;&#24230;&#30340;&#37327;&#21270;&#26041;&#24335;&#28385;&#36275;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#35201;&#27714;&#12290;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20856;&#22411;&#30340;LLMs&#19978;&#20855;&#26377;&#30456;&#20284;&#30340;&#34920;&#29616;&#65292;&#19988;&#19981;&#22686;&#21152;&#25512;&#29702;&#26102;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#38656;&#35201;&#26032;&#30340;&#12289;&#25913;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#20197;&#28385;&#36275;&#36825;&#20123;&#29616;&#20195;&#26550;&#26500;&#30340;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TEQ&#65292;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#31561;&#25928;&#36716;&#25442;&#65292;&#21487;&#20197;&#20445;&#30041;&#27169;&#22411;&#36755;&#20986;&#30340;FP32&#31934;&#24230;&#65292;&#21516;&#26102;&#21033;&#29992;&#20302;&#31934;&#24230;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;3&#20301;&#21644;4&#20301;&#30340;&#20165;&#26435;&#37325;&#37327;&#21270;&#12290;&#35757;&#32451;&#36807;&#31243;&#36731;&#37327;&#21270;&#65292;&#21482;&#38656;1K&#27493;&#39588;&#21644;&#23569;&#20110;&#21407;&#27169;&#22411;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;0.1%&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#35813;&#36716;&#25442;&#19981;&#20250;&#22686;&#21152;&#20219;&#20309;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#20856;&#22411;LLMs&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#20854;&#20182;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/intel/neural-compressor&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more prevalent, there is a growing need for new and improved quantization methods that can meet the computationalast layer demands of these modern architectures while maintaining the accuracy. In this paper, we present TEQ, a trainable equivalent transformation that preserves the FP32 precision of the model output while taking advantage of low-precision quantization, especially 3 and 4 bits weight-only quantization. The training process is lightweight, requiring only 1K steps and fewer than 0.1 percent of the original model's trainable parameters. Furthermore, the transformation does not add any computational overhead during inference. Our results are on-par with the state-of-the-art (SOTA) methods on typical LLMs. Our approach can be combined with other methods to achieve even better performance. The code is available at https://github.com/intel/neural-compressor.
&lt;/p&gt;</description></item><item><title>MASON-NLP&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#26816;&#27979;&#25233;&#37057;&#30151;&#29366;&#12290;&#20219;&#21153;1&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#19981;&#21516;&#26465;&#20214;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10941</link><description>&lt;p&gt;
MASON-NLP&#22312;eRisk 2023&#19978;&#30340;&#36129;&#29486;&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#26816;&#27979;&#25233;&#37057;&#30151;&#29366;
&lt;/p&gt;
&lt;p&gt;
MASON-NLP at eRisk 2023: Deep Learning-Based Detection of Depression Symptoms from Social Media Texts. (arXiv:2310.10941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10941
&lt;/p&gt;
&lt;p&gt;
MASON-NLP&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#26816;&#27979;&#25233;&#37057;&#30151;&#29366;&#12290;&#20219;&#21153;1&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#19981;&#21516;&#26465;&#20214;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#19968;&#31181;&#23545;&#20154;&#20204;&#29983;&#27963;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#30340;&#24515;&#29702;&#20581;&#24247;&#38556;&#30861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#20010;&#20307;&#30340;&#20132;&#27969;&#26041;&#24335;&#65288;&#21253;&#25324;&#21475;&#22836;&#21644;&#20070;&#38754;&#25991;&#23383;&#65289;&#26816;&#27979;&#21040;&#25233;&#37057;&#30151;&#30340;&#36857;&#35937;&#12290;&#29305;&#21035;&#26159;&#65292;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#19968;&#20010;&#21487;&#20379;&#25105;&#20204;&#26816;&#26597;&#25233;&#37057;&#30151;&#29366;&#30340;&#20016;&#23500;&#32780;&#26041;&#20415;&#30340;&#25991;&#26412;&#26469;&#28304;&#12290;&#36125;&#20811;&#25233;&#37057;&#37327;&#34920;&#65288;BDI&#65289;&#38382;&#21367;&#36890;&#24120;&#29992;&#20110;&#34913;&#37327;&#25233;&#37057;&#30151;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#23427;&#26159;&#26412;&#30740;&#31350;&#20013;&#21487;&#20197;&#36741;&#21161;&#20351;&#29992;&#30340;&#24037;&#20855;&#20043;&#19968;&#12290;&#25105;&#20204;&#21487;&#20197;&#23558;&#30740;&#31350;&#33539;&#22260;&#32553;&#23567;&#21040;&#36825;&#20123;&#30151;&#29366;&#65292;&#22240;&#20026;&#27599;&#20010;BDI&#38382;&#39064;&#37117;&#19982;&#29305;&#23450;&#30340;&#25233;&#37057;&#30151;&#29366;&#30456;&#20851;&#32852;&#12290;&#38656;&#35201;&#35760;&#20303;&#65292;&#24182;&#38750;&#27599;&#20010;&#25233;&#37057;&#30151;&#24739;&#32773;&#21516;&#26102;&#34920;&#29616;&#20986;&#25152;&#26377;&#30151;&#29366;&#65292;&#32780;&#26159;&#20854;&#20013;&#30340;&#19968;&#20123;&#32508;&#21512;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#33021;&#30830;&#23450;&#19968;&#20010;&#21477;&#23376;&#25110;&#19968;&#27573;&#29992;&#25143;&#29983;&#25104;&#30340;&#20869;&#23481;&#19982;&#29305;&#23450;&#26465;&#20214;&#30456;&#20851;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#28857;&#65292;eRisk 2023&#20219;&#21153;1&#30340;&#35774;&#35745;&#30446;&#30340;&#27491;&#26159;&#20026;&#20102;&#35780;&#20272;&#19981;&#21516;&#26465;&#20214;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is a mental health disorder that has a profound impact on people's lives. Recent research suggests that signs of depression can be detected in the way individuals communicate, both through spoken words and written texts. In particular, social media posts are a rich and convenient text source that we may examine for depressive symptoms. The Beck Depression Inventory (BDI) Questionnaire, which is frequently used to gauge the severity of depression, is one instrument that can aid in this study. We can narrow our study to only those symptoms since each BDI question is linked to a particular depressive symptom. It's important to remember that not everyone with depression exhibits all symptoms at once, but rather a combination of them. Therefore, it is extremely useful to be able to determine if a sentence or a piece of user-generated content is pertinent to a certain condition. With this in mind, the eRisk 2023 Task 1 was designed to do exactly that: assess the relevance of diffe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#23391;&#21152;&#25289;&#35821;&#21644;&#38177;&#23572;&#36203;&#33922;&#35821;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#27133;&#20301;&#22635;&#20805;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#19981;&#20805;&#36275;&#25968;&#25454;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10935</link><description>&lt;p&gt;
&#22522;&#20110;&#23478;&#24237;&#21161;&#25163;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#27133;&#20301;&#22635;&#20805;&#65306;&#23391;&#21152;&#25289;&#35821;&#21644;&#38177;&#23572;&#36203;&#33922;&#35821;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for Bangla and Sylheti. (arXiv:2310.10935v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10935
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#23391;&#21152;&#25289;&#35821;&#21644;&#38177;&#23572;&#36203;&#33922;&#35821;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#27133;&#20301;&#22635;&#20805;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#19981;&#20805;&#36275;&#25968;&#25454;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#38899;&#21161;&#25163;&#22312;&#25105;&#20204;&#39640;&#24230;&#25216;&#26415;&#21270;&#30340;&#31038;&#20250;&#20013;&#30340;&#22320;&#20301;&#24471;&#21040;&#24041;&#22266;&#65292;&#26377;&#24517;&#35201;&#36866;&#24212;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#29615;&#22659;&#65292;&#21253;&#25324;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21475;&#35821;&#24418;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#39318;&#20010;&#29992;&#20110;&#27491;&#24335;&#23391;&#21152;&#25289;&#35821;&#12289;&#21475;&#35821;&#23391;&#21152;&#25289;&#35821;&#21644;&#38177;&#23572;&#36203;&#33922;&#35821;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#27133;&#20301;&#22635;&#20805;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#24635;&#20849;&#21253;&#21547;10&#20010;&#21807;&#19968;&#24847;&#22270;&#30340;984&#20010;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#19981;&#20805;&#36275;&#25968;&#25454;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#22312;&#21475;&#35821;&#23391;&#21152;&#25289;&#35821;&#30340;&#24847;&#22270;&#26816;&#27979;&#20013;&#65292;GPT-3.5&#27169;&#22411;&#36798;&#21040;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;0.94&#30340;F1&#24471;&#20998;&#65292;&#22312;&#27133;&#20301;&#22635;&#20805;&#20013;&#36798;&#21040;&#20102;0.51&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
As voice assistants cement their place in our technologically advanced society, there remains a need to cater to the diverse linguistic landscape, including colloquial forms of low-resource languages. Our study introduces the first-ever comprehensive dataset for intent detection and slot filling in formal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples across 10 unique intents. Our analysis reveals the robustness of large language models for tackling downstream tasks with inadequate data. The GPT-3.5 model achieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot filling for colloquial Bangla.
&lt;/p&gt;</description></item><item><title>&#22686;&#24378;&#30340;Transformer&#24341;&#20837;&#20102;&#20840;&#23618;&#26631;&#20934;&#21270;&#12289;&#21152;&#26435;&#27531;&#24046;&#36830;&#25509;&#12289;&#24378;&#21270;&#23398;&#20064;&#20301;&#32622;&#32534;&#30721;&#21644;&#38646;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#31561;&#21019;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;Multi30k&#32763;&#35793;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#65292;&#30456;&#27604;&#20110;&#21407;&#22987;Transformer&#65292;&#23454;&#29616;&#20102;202.96%&#30340;BLEU&#20998;&#25968;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.10930</link><description>&lt;p&gt;
&#22686;&#24378;&#30340;Transformer&#26550;&#26500;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhanced Transformer Architecture for Natural Language Processing. (arXiv:2310.10930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10930
&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#30340;Transformer&#24341;&#20837;&#20102;&#20840;&#23618;&#26631;&#20934;&#21270;&#12289;&#21152;&#26435;&#27531;&#24046;&#36830;&#25509;&#12289;&#24378;&#21270;&#23398;&#20064;&#20301;&#32622;&#32534;&#30721;&#21644;&#38646;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#31561;&#21019;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;Multi30k&#32763;&#35793;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#65292;&#30456;&#27604;&#20110;&#21407;&#22987;Transformer&#65292;&#23454;&#29616;&#20102;202.96%&#30340;BLEU&#20998;&#25968;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#24403;&#21069;&#30340;NLP&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;&#22686;&#21152;transformer&#30340;&#25968;&#37327;&#26469;&#25552;&#39640;&#22788;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25216;&#26415;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#36164;&#28304;&#65292;&#22914;&#35745;&#31639;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#32467;&#26500;&#65292;&#20855;&#26377;&#20840;&#23618;&#26631;&#20934;&#21270;&#12289;&#21152;&#26435;&#27531;&#24046;&#36830;&#25509;&#12289;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#38646;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#31561;&#29305;&#28857;&#12290;&#25552;&#20986;&#30340;&#22686;&#24378;Transformer&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;Multi30k&#32763;&#35793;&#25968;&#25454;&#38598;&#30340;&#21452;&#35821;&#35780;&#20272;&#23454;&#39564;&#24471;&#21040;&#30340;BLEU&#20998;&#25968;&#36827;&#34892;&#39564;&#35777;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21407;&#22987;Transformer&#30456;&#27604;&#65292;&#22686;&#24378;Transformer&#30340;BLEU&#20998;&#25968;&#25552;&#39640;&#20102;202.96%&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer is a state-of-the-art model in the field of natural language processing (NLP). Current NLP models primarily increase the number of transformers to improve processing performance. However, this technique requires a lot of training resources such as computing capacity. In this paper, a novel structure of Transformer is proposed. It is featured by full layer normalization, weighted residual connection, positional encoding exploiting reinforcement learning, and zero masked self-attention. The proposed Transformer model, which is called Enhanced Transformer, is validated by the bilingual evaluation understudy (BLEU) score obtained with the Multi30k translation dataset. As a result, the Enhanced Transformer achieves 202.96% higher BLEU score as compared to the original transformer with the translation dataset.
&lt;/p&gt;</description></item><item><title>Spatial HuBERT&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#36890;&#36947;&#38899;&#39057;&#36755;&#20837;&#20013;&#30340;&#21333;&#20010;&#35828;&#35805;&#32773;&#30340;&#22768;&#23398;&#21644;&#31354;&#38388;&#20449;&#24687;&#65292;&#20854;&#34920;&#31034;&#22312;&#21508;&#31181;&#31354;&#38388;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21333;&#22768;&#36947;&#35821;&#38899;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#22312;&#22024;&#26434;&#21644;&#28151;&#21709;&#29615;&#22659;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.10922</link><description>&lt;p&gt;
Spatial HuBERT: &#21333;&#20010;&#35828;&#35805;&#32773;&#22810;&#36890;&#36947;&#38899;&#39057;&#30340;&#33258;&#30417;&#30563;&#31354;&#38388;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio. (arXiv:2310.10922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10922
&lt;/p&gt;
&lt;p&gt;
Spatial HuBERT&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#36890;&#36947;&#38899;&#39057;&#36755;&#20837;&#20013;&#30340;&#21333;&#20010;&#35828;&#35805;&#32773;&#30340;&#22768;&#23398;&#21644;&#31354;&#38388;&#20449;&#24687;&#65292;&#20854;&#34920;&#31034;&#22312;&#21508;&#31181;&#31354;&#38388;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21333;&#22768;&#36947;&#35821;&#38899;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#22312;&#22024;&#26434;&#21644;&#28151;&#21709;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#34987;&#29992;&#26469;&#21033;&#29992;&#26080;&#26631;&#27880;&#25968;&#25454;&#65292;&#36890;&#36807;&#35757;&#32451;&#34920;&#31034;&#27169;&#22411;&#25913;&#21892;&#35821;&#38899;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#22312;&#21508;&#31181;&#22768;&#23398;&#39046;&#22495;&#12289;&#35821;&#35328;&#12289;&#27169;&#24577;&#29978;&#33267;&#21516;&#26102;&#35828;&#35805;&#32773;&#20013;&#29983;&#25104;&#26377;&#25928;&#30340;&#34920;&#31034;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#37117;&#38480;&#21046;&#22312;&#21333;&#22768;&#36947;&#38899;&#39057;&#35760;&#24405;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Spatial HuBERT&#65292;&#23427;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#36890;&#36947;&#38899;&#39057;&#36755;&#20837;&#23398;&#20064;&#21333;&#20010;&#35828;&#35805;&#32773;&#22312;&#28508;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#30340;&#22768;&#23398;&#21644;&#31354;&#38388;&#20449;&#24687;&#12290;Spatial HuBERT&#23398;&#20064;&#30340;&#34920;&#31034;&#22312;&#21508;&#31181;&#31354;&#38388;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21333;&#22768;&#36947;&#35821;&#38899;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#22312;&#28151;&#21709;&#21644;&#22024;&#26434;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Spatial HuBERT&#23398;&#20064;&#30340;&#34920;&#31034;&#22312;&#35821;&#38899;&#23450;&#20301;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publi
&lt;/p&gt;</description></item><item><title>NuclearQA&#26159;&#19968;&#20010;&#20154;&#24037;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26680;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#21253;&#21547;100&#20010;&#19987;&#23478;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#35813;&#22522;&#20934;&#19982;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#19981;&#21516;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#26680;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10920</link><description>&lt;p&gt;
NuclearQA: &#29992;&#20110;&#26680;&#39046;&#22495;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain. (arXiv:2310.10920v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10920
&lt;/p&gt;
&lt;p&gt;
NuclearQA&#26159;&#19968;&#20010;&#20154;&#24037;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26680;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#21253;&#21547;100&#20010;&#19987;&#23478;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#35813;&#22522;&#20934;&#19982;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#19981;&#21516;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#26680;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#34892;&#65292;&#23427;&#20204;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#12290;&#20294;&#26159;&#38543;&#30528;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25193;&#22823;&#65292;&#35780;&#20272;&#20854;&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#26085;&#30410;&#32570;&#20047;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#22823;&#37096;&#20998;&#19987;&#27880;&#20110;&#19981;&#38656;&#35201;&#23545;&#25152;&#28041;&#21450;&#20027;&#39064;&#36827;&#34892;&#27491;&#30830;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NuclearQA&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#19987;&#23478;&#35774;&#35745;&#30340;&#29992;&#20110;&#35780;&#20272;&#26680;&#39046;&#22495;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;100&#20010;&#38382;&#39064;&#65292;&#29992;&#20110;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#30001;&#20110;&#29616;&#26377;&#35780;&#20272;&#25351;&#26631;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#22522;&#20934;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#24049;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#26680;&#39046;&#22495;&#65292;NuclearQA&#20063;&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As LLMs have become increasingly popular, they have been used in almost every field. But as the application for LLMs expands from generic fields to narrow, focused science domains, there exists an ever-increasing gap in ways to evaluate their efficacy in those fields. For the benchmarks that do exist, a lot of them focus on questions that don't require proper understanding of the subject in question. In this paper, we present NuclearQA, a human-made benchmark of 100 questions to evaluate language models in the nuclear domain, consisting of a varying collection of questions that have been specifically designed by experts to test the abilities of language models. We detail our approach and show how the mix of several types of questions makes our benchmark uniquely capable of evaluating models in the nuclear domain. We also present our own evaluation metric for assessing LLM's performances due to the limitations of existing ones. Our experiments on state-of-the-art models suggest that eve
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#31532;&#20108;&#35821;&#35328;&#20889;&#20316;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#19982;&#35821;&#35328;&#23398;&#20064;&#32773;&#21512;&#20316;&#65292;&#22312;&#20445;&#25345;&#20010;&#20154;&#34920;&#36798;&#22768;&#38899;&#30340;&#21516;&#26102;&#25552;&#21319;&#20889;&#20316;&#33021;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;ChatGPT&#22312;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2310.10903</link><description>&lt;p&gt;
&#26032;&#20852;&#30340;AI&#36741;&#21161;&#35805;&#35821;&#65306;ChatGPT&#19982;&#31532;&#20108;&#35821;&#35328;&#20889;&#20316;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT. (arXiv:2310.10903v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10903
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#31532;&#20108;&#35821;&#35328;&#20889;&#20316;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#19982;&#35821;&#35328;&#23398;&#20064;&#32773;&#21512;&#20316;&#65292;&#22312;&#20445;&#25345;&#20010;&#20154;&#34920;&#36798;&#22768;&#38899;&#30340;&#21516;&#26102;&#25552;&#21319;&#20889;&#20316;&#33021;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;ChatGPT&#22312;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24555;&#36895;&#26222;&#21450;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#23545;&#20154;&#31867;&#20889;&#20316;&#30340;&#24433;&#21709;&#30340;&#20105;&#35758;&#12290;&#22312;&#23545;&#20889;&#20316;&#26631;&#20934;&#19979;&#38477;&#30340;&#25285;&#24551;&#20013;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#22312;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#35821;&#35328;&#23398;&#20064;&#32773;&#20013;&#30340;&#20316;&#29992;&#12290;&#37319;&#29992;&#26696;&#20363;&#30740;&#31350;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#21338;&#22763;&#29983;&#20975;&#28789;&#65288;Kailing&#65289;&#22312;&#25972;&#20010;&#23398;&#26415;&#20889;&#20316;&#36807;&#31243;&#20013;&#22914;&#20309;&#20351;&#29992;ChatGPT&#30340;&#32463;&#39564;&#12290;&#30740;&#31350;&#37319;&#29992;&#27963;&#21160;&#29702;&#35770;&#20316;&#20026;&#29702;&#35770;&#26694;&#26550;&#26469;&#29702;&#35299;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#36827;&#34892;&#20889;&#20316;&#30340;&#36807;&#31243;&#65292;&#30740;&#31350;&#25968;&#25454;&#21253;&#25324;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#12289;&#20889;&#20316;&#26679;&#26412;&#21644;GPT&#26085;&#24535;&#30340;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20975;&#28789;&#22312;&#21508;&#20010;&#20889;&#20316;&#38454;&#27573;&#26377;&#25928;&#22320;&#19982;ChatGPT&#21512;&#20316;&#65292;&#21516;&#26102;&#20445;&#30041;&#33258;&#24049;&#29420;&#29305;&#30340;&#20316;&#32773;&#22768;&#38899;&#21644;&#20027;&#20307;&#24615;&#12290;&#36825;&#31361;&#26174;&#20102;ChatGPT&#31561;AI&#24037;&#20855;&#22312;&#22686;&#24378;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#23398;&#26415;&#20889;&#20316;&#33021;&#21147;&#26102;&#19981;&#20250;&#25513;&#30422;&#20010;&#20154;&#30340;&#30495;&#23454;&#24615;&#12290;&#26412;&#26696;&#20363;&#30740;&#31350;&#23545;ChatGPT&#22312;&#23398;&#26415;&#30028;&#20013;&#30340;&#20351;&#29992;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid proliferation of ChatGPT has incited debates regarding its impact on human writing. Amid concerns about declining writing standards, this study investigates the role of ChatGPT in facilitating academic writing, especially among language learners. Using a case study approach, this study examines the experiences of Kailing, a doctoral student, who integrates ChatGPT throughout their academic writing process. The study employs activity theory as a lens for understanding writing with generative AI tools and data analyzed includes semi-structured interviews, writing samples, and GPT logs. Results indicate that Kailing effectively collaborates with ChatGPT across various writing stages while preserving her distinct authorial voice and agency. This underscores the potential of AI tools such as ChatGPT to enhance academic writing for language learners without overshadowing individual authenticity. This case study offers a critical exploration of how ChatGPT is utilized in the academi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24433;&#21709;&#39537;&#21160;&#30340;&#36873;&#25321;&#24615;&#27880;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#21892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#20851;&#38190;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#22312;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.10873</link><description>&lt;p&gt;
IDEAL: &#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#39537;&#21160;&#36873;&#25321;&#24615;&#27880;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models. (arXiv:2310.10873v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24433;&#21709;&#39537;&#21160;&#30340;&#36873;&#25321;&#24615;&#27880;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#21892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#20851;&#38190;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#22312;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#65292;&#23427;&#21033;&#29992;&#19978;&#19979;&#25991;&#31034;&#20363;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#30340;&#25552;&#31034;&#12290;&#36825;&#20123;&#25552;&#31034;&#23545;&#20110;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#25552;&#31034;&#38656;&#35201;&#20174;&#22823;&#37327;&#27880;&#37322;&#30340;&#31034;&#20363;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#25214;&#21040;&#27491;&#30830;&#30340;&#25552;&#31034;&#21487;&#33021;&#23548;&#33268;&#39640;&#26114;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24433;&#21709;&#39537;&#21160;&#30340;&#36873;&#25321;&#24615;&#27880;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25913;&#21892;&#19978;&#19979;&#25991;&#31034;&#20363;&#36136;&#37327;&#30340;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20174;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#27744;&#20013;&#36873;&#25321;&#19968;&#20010;&#20851;&#38190;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#29992;&#20110;&#21518;&#32493;&#30340;&#25552;&#31034;&#37319;&#26679;&#12290;&#20855;&#20307;&#22320;&#65292;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#26377;&#21521;&#22270;&#26469;&#34920;&#31034;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#28982;&#21518;&#21033;&#29992;&#25193;&#25955;&#36807;&#31243;&#37327;&#21270;&#20505;&#36873;&#26410;&#26631;&#35760;&#23376;&#38598;&#30340;&#24433;&#21709;&#21147;&#65292;&#26368;&#21518;&#24341;&#20837;&#19968;&#20010;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;&#36138;&#24515;&#31639;&#27861;&#26469;&#36873;&#25321;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#22914;&#26524;&#25968;&#25454;&#25552;&#20379;&#20102;&#26368;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#31639;&#27861;&#23601;&#20250;&#36845;&#20195;&#22320;&#36873;&#25321;&#36825;&#20123;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a ma
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#24615;&#21035;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24110;&#21161;&#20943;&#36731;&#20256;&#32479;&#31461;&#35805;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#26469;&#20943;&#36731;&#23398;&#20064;&#21040;&#30340;&#20559;&#35265;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#23545;&#24615;&#21035;&#25200;&#21160;&#25935;&#24863;&#65292;&#20294;&#22312;&#21453;&#20107;&#23454;&#35757;&#32451;&#21518;&#23545;&#21518;&#32493;&#24341;&#20837;&#30340;&#21453;&#24615;&#21035;&#20559;&#35265;&#26356;&#19981;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2310.10865</link><description>&lt;p&gt;
&#29579;&#23376;&#20250;&#24471;&#21040;&#30495;&#29233;&#20043;&#21563;&#21527;&#65311;&#20851;&#20110;&#31461;&#35805;&#25991;&#26412;&#20013;&#24615;&#21035;&#25200;&#21160;&#23545;&#27169;&#22411;&#25935;&#24863;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Will the Prince Get True Love's Kiss? On the Model Sensitivity to Gender Perturbation over Fairytale Texts. (arXiv:2310.10865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#24615;&#21035;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24110;&#21161;&#20943;&#36731;&#20256;&#32479;&#31461;&#35805;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#26469;&#20943;&#36731;&#23398;&#20064;&#21040;&#30340;&#20559;&#35265;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#23545;&#24615;&#21035;&#25200;&#21160;&#25935;&#24863;&#65292;&#20294;&#22312;&#21453;&#20107;&#23454;&#35757;&#32451;&#21518;&#23545;&#21518;&#32493;&#24341;&#20837;&#30340;&#21453;&#24615;&#21035;&#20559;&#35265;&#26356;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#20256;&#32479;&#30340;&#31461;&#35805;&#25925;&#20107;&#20013;&#23384;&#22312;&#22823;&#37327;&#26377;&#23475;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#20026;&#20102;&#20943;&#36731;&#31461;&#35805;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#20559;&#35265;&#23545;&#24615;&#21035;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#31461;&#35805;&#25925;&#20107;&#20013;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;FairytaleQA&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#27169;&#22411;&#23545;&#20132;&#25442;&#24615;&#21035;&#35282;&#33394;&#20449;&#24687;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#35757;&#32451;&#26102;&#24341;&#20837;&#21453;&#20107;&#23454;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#26469;&#20943;&#36731;&#23398;&#20064;&#21040;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#24222;&#22823;&#35789;&#27719;&#37327;&#26469;&#25903;&#25345;&#36229;&#36234;&#31461;&#35805;&#25925;&#20107;&#30340;&#25991;&#26412;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#23545;&#24615;&#21035;&#25200;&#21160;&#25935;&#24863;&#65292;&#24615;&#33021;&#19982;&#21407;&#22987;&#27979;&#35797;&#38598;&#30456;&#27604;&#26174;&#33879;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#24403;&#39318;&#20808;&#22312;&#21453;&#20107;&#23454;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#27169;&#22411;&#23545;&#21518;&#32493;&#24341;&#20837;&#30340;&#21453;&#24615;&#21035;&#20559;&#35265;&#26356;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that traditional fairytales are rife with harmful gender biases. To help mitigate these gender biases in fairytales, this work aims to assess learned biases of language models by evaluating their robustness against gender perturbations. Specifically, we focus on Question Answering (QA) tasks in fairytales. Using counterfactual data augmentation to the FairytaleQA dataset, we evaluate model robustness against swapped gender character information, and then mitigate learned biases by introducing counterfactual gender stereotypes during training time. We additionally introduce a novel approach that utilizes the massive vocabulary of language models to support text genres beyond fairytales. Our experimental results suggest that models are sensitive to gender perturbations, with significant performance drops compared to the original testing set. However, when first fine-tuned on a counterfactual training dataset, models are less sensitive to the later introduced anti-gend
&lt;/p&gt;</description></item><item><title>CoTFormer&#26159;&#19968;&#31181;transformer&#21464;&#20307;&#65292;&#36890;&#36807;&#20351;&#29992;&#38544;&#21547;&#30340;&#38142;&#24605;&#32771;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#19982;&#26356;&#28145;&#27169;&#22411;&#30456;&#24403;&#30340;&#23481;&#37327;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#20013;&#26174;&#33879;&#20248;&#20110;&#26356;&#22823;&#30340;&#26631;&#20934;transformers&#12290;</title><link>http://arxiv.org/abs/2310.10845</link><description>&lt;p&gt;
CoTFormer&#65306;&#26356;&#22810;&#30340;&#20851;&#27880;&#20196;&#29260;&#24357;&#34917;&#20102;&#26356;&#23569;&#30340;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
CoTFormer: More Tokens With Attention Make Up For Less Depth. (arXiv:2310.10845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10845
&lt;/p&gt;
&lt;p&gt;
CoTFormer&#26159;&#19968;&#31181;transformer&#21464;&#20307;&#65292;&#36890;&#36807;&#20351;&#29992;&#38544;&#21547;&#30340;&#38142;&#24605;&#32771;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#19982;&#26356;&#28145;&#27169;&#22411;&#30456;&#24403;&#30340;&#23481;&#37327;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#20013;&#26174;&#33879;&#20248;&#20110;&#26356;&#22823;&#30340;&#26631;&#20934;transformers&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#21457;&#23637;&#36234;&#26469;&#36234;&#22823;&#21644;&#26356;&#28145;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#31454;&#36187;&#27491;&#22312;&#36827;&#34892;&#20013;&#12290;&#28982;&#32780;&#65292;&#20687;&#38142;&#24605;&#32771;&#65288;CoT&#65289;&#26041;&#27861;&#36825;&#26679;&#30340;&#25216;&#26415;&#22312;&#23454;&#29616;&#26368;&#20339;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20173;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20351;&#29992;&#38142;&#24605;&#32771;&#21644;&#20351;&#29992;&#26356;&#28145;&#30340;transformer&#20043;&#38388;&#30340;&#36817;&#20284;&#24179;&#34892;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoTFormer&#65292;&#19968;&#31181;&#20351;&#29992;&#38544;&#21547;&#38142;&#24605;&#32771;&#26426;&#21046;&#26469;&#23454;&#29616;&#19982;&#26356;&#28145;&#27169;&#22411;&#30456;&#24403;&#23481;&#37327;&#30340;transformer&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#35777;&#26126;&#20102;CoTFormer&#30340;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26126;&#26174;&#20248;&#20110;&#26356;&#22823;&#30340;&#26631;&#20934;transformers&#12290;
&lt;/p&gt;
&lt;p&gt;
The race to continually develop ever larger and deeper foundational models is underway. However, techniques like the Chain-of-Thought (CoT) method continue to play a pivotal role in achieving optimal downstream performance. In this work, we establish an approximate parallel between using chain-of-thought and employing a deeper transformer. Building on this insight, we introduce CoTFormer, a transformer variant that employs an implicit CoT-like mechanism to achieve capacity comparable to a deeper model. Our empirical findings demonstrate the effectiveness of CoTFormers, as they significantly outperform larger standard transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24694;&#24847;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#21363;&#20351;&#32463;&#36807;&#23433;&#20840;&#35843;&#25972;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#36825;&#20123;&#25915;&#20987;&#21033;&#29992;&#24369;&#28857;&#24182;&#35823;&#23548;AI&#31995;&#32479;&#65292;&#23545;&#20110;&#22797;&#26434;&#31995;&#32479;&#30340;&#25915;&#20987;&#23588;&#20026;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2310.10844</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24694;&#24847;&#25915;&#20987;&#25152;&#25581;&#31034;&#30340;&#28431;&#27934;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks. (arXiv:2310.10844v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24694;&#24847;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#21363;&#20351;&#32463;&#36807;&#23433;&#20840;&#35843;&#25972;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#36825;&#20123;&#25915;&#20987;&#21033;&#29992;&#24369;&#28857;&#24182;&#35823;&#23548;AI&#31995;&#32479;&#65292;&#23545;&#20110;&#22797;&#26434;&#31995;&#32479;&#30340;&#25915;&#20987;&#23588;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20307;&#31995;&#32467;&#26500;&#21644;&#33021;&#21147;&#26041;&#38754;&#36805;&#36895;&#21457;&#23637;&#65292;&#38543;&#30528;&#23427;&#20204;&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#28145;&#20837;&#25972;&#21512;&#65292;&#23457;&#26597;&#20854;&#23433;&#20840;&#24615;&#21464;&#24471;&#26356;&#21152;&#32039;&#36843;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#23545;LLMs&#36827;&#34892;&#24694;&#24847;&#25915;&#20987;&#30340;&#26032;&#20852;&#36328;&#23398;&#31185;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#35813;&#39046;&#22495;&#26159;&#21487;&#20449;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#23433;&#20840;&#24615;&#30340;&#35266;&#28857;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#32463;&#36807;&#23433;&#20840;&#35843;&#25972;&#30340;LLMs&#65288;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#21644;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#21152;&#24378;&#23398;&#20064;&#65289;&#20063;&#21487;&#33021;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#21033;&#29992;&#24369;&#28857;&#24182;&#35823;&#23548;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#22914;ChatGPT&#21644;Bard&#31561;&#27169;&#22411;&#30340;&#8220;&#36234;&#29425;&#8221;&#25915;&#20987;&#30340;&#26222;&#36941;&#23384;&#22312;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25551;&#36848;&#20102;&#23427;&#20204;&#30340;&#23433;&#20840;&#23545;&#40784;&#65292;&#24182;&#26681;&#25454;&#21508;&#31181;&#23398;&#20064;&#32467;&#26500;&#23545;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20998;&#31867;&#65306;&#20165;&#25991;&#26412;&#25915;&#20987;&#12289;&#22810;&#27169;&#24577;&#25915;&#20987;&#20197;&#21450;&#19987;&#38376;&#38024;&#23545;&#22797;&#26434;&#31995;&#32479;&#30340;&#20854;&#20182;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26080;&#39118;&#26684;&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#65292;&#33021;&#22815;&#23545;&#25239;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39118;&#26684;&#25915;&#20987;&#30340;&#20551;&#26032;&#38395;&#12290;&#36890;&#36807;LLM&#22686;&#24378;&#30340;&#26032;&#38395;&#37325;&#26500;&#65292;&#35813;&#26816;&#27979;&#22120;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#25552;&#39640;&#20102;&#23545;&#20266;&#35013;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10830</link><description>&lt;p&gt;
&#20551;&#26032;&#38395;&#22312;&#32501;&#32650;&#30340;&#22806;&#34915;&#20013;&#65306;&#23545;&#25239;LLM&#22686;&#24378;&#39118;&#26684;&#25915;&#20987;&#30340;&#40065;&#26834;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks. (arXiv:2310.10830v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10830
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26080;&#39118;&#26684;&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#65292;&#33021;&#22815;&#23545;&#25239;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39118;&#26684;&#25915;&#20987;&#30340;&#20551;&#26032;&#38395;&#12290;&#36890;&#36807;LLM&#22686;&#24378;&#30340;&#26032;&#38395;&#37325;&#26500;&#65292;&#35813;&#26816;&#27979;&#22120;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#25552;&#39640;&#20102;&#23545;&#20266;&#35013;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24120;&#24120;&#35748;&#20026;&#22312;&#32447;&#20551;&#26032;&#38395;&#21644;&#21487;&#38752;&#26032;&#38395;&#22312;&#20889;&#20316;&#39118;&#26684;&#19978;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#22914;&#20351;&#29992;&#32824;&#20154;&#21548;&#38395;&#30340;&#35821;&#35328;&#19982;&#23458;&#35266;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24378;&#35843;&#39118;&#26684;&#30456;&#20851;&#29305;&#24449;&#20063;&#21487;&#20197;&#29992;&#20110;&#39118;&#26684;&#25915;&#20987;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23835;&#36215;&#20351;&#24694;&#24847;&#29992;&#25143;&#33021;&#22815;&#20197;&#26368;&#20302;&#25104;&#26412;&#27169;&#20223;&#20540;&#24471;&#20449;&#36182;&#30340;&#26032;&#38395;&#23186;&#20307;&#30340;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#20197;LLM&#20266;&#35013;&#30340;&#20551;&#26032;&#38395;&#20869;&#23481;&#23548;&#33268;&#20808;&#36827;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65288;F1&#20998;&#25968;&#20943;&#23569;&#39640;&#36798;38%&#65289;&#65292;&#32473;&#22312;&#32447;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#33258;&#21160;&#26816;&#27979;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SheepDog&#65292;&#19968;&#31181;&#23545;&#26032;&#38395;&#20889;&#20316;&#39118;&#26684;&#40065;&#26834;&#30340;&#26080;&#39118;&#26684;&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#12290;SheepDog&#36890;&#36807;LLM&#22686;&#24378;&#30340;&#26032;&#38395;&#37325;&#26500;&#23454;&#29616;&#20102;&#36825;&#31181;&#36866;&#24212;&#24615;&#65292;&#36890;&#36807;&#39118;&#26684;&#23548;&#21521;&#30340;&#37325;&#26500;&#25552;&#31034;&#26469;&#23450;&#21046;&#27599;&#31687;&#25991;&#31456;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#20889;&#20316;&#39118;&#26684;&#12290;&#36890;&#36807;&#37319;&#29992;&#26080;&#39118;&#26684;&#35757;&#32451;&#65292;SheepDog&#21487;&#20197;&#22312;&#19981;&#21516;&#39118;&#26684;&#30340;&#26032;&#38395;&#20013;&#26816;&#27979;&#20551;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is commonly perceived that online fake news and reliable news exhibit stark differences in writing styles, such as the use of sensationalist versus objective language. However, we emphasize that style-related features can also be exploited for style-based attacks. Notably, the rise of powerful Large Language Models (LLMs) has enabled malicious users to mimic the style of trustworthy news outlets at minimal cost. Our analysis reveals that LLM-camouflaged fake news content leads to substantial performance degradation of state-of-the-art text-based detectors (up to 38% decrease in F1 Score), posing a significant challenge for automated detection in online ecosystems. To address this, we introduce SheepDog, a style-agnostic fake news detector robust to news writing styles. SheepDog achieves this adaptability through LLM-empowered news reframing, which customizes each article to match different writing styles using style-oriented reframing prompts. By employing style-agnostic training, S
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SD-HuBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#25105;&#33976;&#39311;&#30446;&#26631;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#23398;&#20064;&#35821;&#38899;&#21477;&#23376;&#32423;&#34920;&#31034;&#26102;&#38899;&#33410;&#32452;&#32455;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#35821;&#38899;&#20013;&#21010;&#23450;&#26126;&#30830;&#30340;&#36793;&#30028;&#65292;&#24182;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#38899;&#33410;&#32467;&#26500;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#20219;&#21153;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#30340;&#21477;&#23376;&#32423;&#34920;&#31034;&#65292;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#26080;&#30417;&#30563;&#38899;&#33410;&#21457;&#29616;&#21644;&#23398;&#20064;&#21477;&#23376;&#32423;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.10803</link><description>&lt;p&gt;
SD-HuBERT: &#33258;&#25105;&#33976;&#39311;&#35825;&#23548;HuBERT&#20013;&#30340;&#38899;&#33410;&#32452;&#32455;
&lt;/p&gt;
&lt;p&gt;
SD-HuBERT: Self-Distillation Induces Syllabic Organization in HuBERT. (arXiv:2310.10803v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SD-HuBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#25105;&#33976;&#39311;&#30446;&#26631;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#23398;&#20064;&#35821;&#38899;&#21477;&#23376;&#32423;&#34920;&#31034;&#26102;&#38899;&#33410;&#32452;&#32455;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#35821;&#38899;&#20013;&#21010;&#23450;&#26126;&#30830;&#30340;&#36793;&#30028;&#65292;&#24182;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#38899;&#33410;&#32467;&#26500;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#20219;&#21153;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#30340;&#21477;&#23376;&#32423;&#34920;&#31034;&#65292;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#26080;&#30417;&#30563;&#38899;&#33410;&#21457;&#29616;&#21644;&#23398;&#20064;&#21477;&#23376;&#32423;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#21333;&#20803;&#21457;&#29616;&#24320;&#21551;&#20102;&#21475;&#35821;&#35821;&#35328;&#22788;&#29702;&#30340;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#30340;&#21333;&#20803;&#24448;&#24448;&#20173;&#22788;&#20110;&#38899;&#32032;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;SSL&#34920;&#31034;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23398;&#20064;&#35821;&#38899;&#30340;&#21477;&#23376;&#32423;&#34920;&#31034;&#26102;&#65292;&#38899;&#33410;&#32452;&#32455;&#30340;&#20986;&#29616;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#8220;&#33258;&#25105;&#33976;&#39311;&#8221;&#30446;&#26631;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;HuBERT&#65292;&#24182;&#21152;&#20837;&#19968;&#20010;&#27719;&#32858;&#26631;&#35760;&#26469;&#24635;&#32467;&#25972;&#20010;&#21477;&#23376;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#35821;&#38899;&#20013;&#21010;&#23450;&#20102;&#26126;&#30830;&#30340;&#36793;&#30028;&#65292;&#24182;&#19988;&#24103;&#38388;&#30340;&#34920;&#31034;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#38899;&#33410;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#20986;&#29616;&#30340;&#32467;&#26500;&#24456;&#22823;&#31243;&#24230;&#19978;&#19982;&#30495;&#23454;&#38899;&#33410;&#23545;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;Spoken Speech ABX&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#30340;&#21477;&#23376;&#32423;&#34920;&#31034;&#12290;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#38899;&#33410;&#21457;&#29616;&#21644;&#23398;&#20064;&#21477;&#23376;&#32423;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space, limiting the utility of SSL representations. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt "self-distillation" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames show salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#33258;&#25105;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#20855;&#26377;&#25512;&#26029;&#35821;&#38899;&#21457;&#38899;&#36816;&#21160;&#23398;&#30340;&#33021;&#21147;&#65292;&#24182;&#26174;&#31034;&#20986;&#36825;&#19968;&#23646;&#24615;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#20855;&#26377;&#37325;&#21472;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31616;&#21333;&#21464;&#25442;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#21516;&#35828;&#35805;&#32773;&#12289;&#24615;&#21035;&#12289;&#35821;&#35328;&#21644;&#26041;&#35328;&#20043;&#38388;&#36716;&#25442;&#65292;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#25299;&#23485;&#20102;&#25105;&#20204;&#23545;&#35821;&#38899;&#22788;&#29702;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.10788</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#25512;&#26029;&#20986;&#26222;&#36866;&#30340;&#21457;&#38899;&#36816;&#21160;&#23398;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Models of Speech Infer Universal Articulatory Kinematics. (arXiv:2310.10788v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#33258;&#25105;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#20855;&#26377;&#25512;&#26029;&#35821;&#38899;&#21457;&#38899;&#36816;&#21160;&#23398;&#30340;&#33021;&#21147;&#65292;&#24182;&#26174;&#31034;&#20986;&#36825;&#19968;&#23646;&#24615;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#20855;&#26377;&#37325;&#21472;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31616;&#21333;&#21464;&#25442;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#21516;&#35828;&#35805;&#32773;&#12289;&#24615;&#21035;&#12289;&#35821;&#35328;&#21644;&#26041;&#35328;&#20043;&#38388;&#36716;&#25442;&#65292;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#25299;&#23485;&#20102;&#25105;&#20204;&#23545;&#35821;&#38899;&#22788;&#29702;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#19968;&#30452;&#26159;&#40657;&#21283;&#23376;&#65292;&#20294;&#26368;&#36817;&#30340;&#35768;&#22810;&#30740;&#31350;&#24320;&#22987;&#23545;&#20687;HuBERT&#36825;&#26679;&#30340;&#27169;&#22411;&#36827;&#34892;"&#25506;&#27979;"&#65292;&#20197;&#23558;&#20854;&#20869;&#37096;&#34920;&#31034;&#19982;&#35821;&#38899;&#30340;&#19981;&#21516;&#26041;&#38754;&#30456;&#20851;&#32852;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#25105;&#30417;&#30563;&#27169;&#22411;&#30340;&#19968;&#20010;&#22522;&#26412;&#23646;&#24615;&#65292;&#21363;"&#25512;&#26029;&#21457;&#38899;&#36816;&#21160;&#23398;"&#65292;&#21363;&#36825;&#20123;&#27169;&#22411;&#23558;&#22768;&#23398;&#36716;&#21270;&#20026;&#35821;&#38899;&#20449;&#21495;&#24213;&#23618;&#30340;&#22240;&#26524;&#24615;&#21457;&#38899;&#21160;&#24577;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#26174;&#31034;&#20986;&#65292;&#35813;&#25277;&#35937;&#22312;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#25454;&#30340;&#35821;&#35328;&#20043;&#38388;&#26377;&#36739;&#22823;&#30340;&#37325;&#21472;&#65292;&#22312;&#31867;&#20284;&#38899;&#31995;&#30340;&#35821;&#35328;&#20013;&#26377;&#26356;&#39640;&#30340;&#20559;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26174;&#31034;&#20986;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#20223;&#23556;&#21464;&#25442;&#65292;&#22768;&#23398;&#21040;&#21457;&#38899;&#30340;&#36870;&#36716;&#25442;&#65288;AAI&#65289;&#22312;&#35828;&#35805;&#32773;&#20043;&#38388;&#20855;&#26377;&#21487;&#20256;&#36882;&#24615;&#65292;&#29978;&#33267;&#22312;&#24615;&#21035;&#12289;&#35821;&#35328;&#21644;&#26041;&#35328;&#20043;&#38388;&#20063;&#20855;&#26377;&#21487;&#20256;&#36882;&#24615;&#65292;&#26174;&#31034;&#20102;&#35813;&#23646;&#24615;&#30340;&#26222;&#36866;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#20123;&#32467;&#26524;&#20026;&#25105;&#20204;&#23545;&#35821;&#38899;&#22788;&#29702;&#30340;&#35748;&#35782;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning (SSL) based models of speech have shown remarkable performance on a range of downstream tasks. These state-of-the-art models have remained blackboxes, but many recent studies have begun "probing" models like HuBERT, to correlate their internal representations to different aspects of speech. In this paper, we show "inference of articulatory kinematics" as fundamental property of SSL models, i.e., the ability of these models to transform acoustics into the causal articulatory dynamics underlying the speech signal. We also show that this abstraction is largely overlapping across the language of the data used to train the model, with preference to the language with similar phonological system. Furthermore, we show that with simple affine transformations, Acoustic-to-Articulatory inversion (AAI) is transferrable across speakers, even across genders, languages, and dialects, showing the generalizability of this property. Together, these results shed new light on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BanglaNLP&#24320;&#21457;&#30340;&#31995;&#32479;&#22312;&#35299;&#20915;&#23391;&#21152;&#25289;&#25991;&#20013;&#26292;&#21147;&#29053;&#21160;&#25991;&#23383;&#26816;&#27979;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#30740;&#31350;&#25968;&#25454;&#22686;&#24378;&#21644;&#20351;&#29992;&#22810;&#35821;&#35328;-e5-base&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#22312;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;68.11%&#30340;&#23439;F1&#20540;&#65292;&#22312;&#25490;&#34892;&#27036;&#19978;&#25490;&#21517;&#31532;23&#20301;&#12290;</title><link>http://arxiv.org/abs/2310.10781</link><description>&lt;p&gt;
BanglaNLP&#22312;BLP-2023&#20219;&#21153;1&#20013;&#30340;&#34920;&#29616;&#65306;&#22312;&#23391;&#21152;&#25289;&#25991;&#20013;&#26292;&#21147;&#29053;&#21160;&#25991;&#23383;&#26816;&#27979;&#20013;&#23545;&#19981;&#21516;Transformer&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models for Violence Inciting Text Detection in Bengali. (arXiv:2310.10781v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BanglaNLP&#24320;&#21457;&#30340;&#31995;&#32479;&#22312;&#35299;&#20915;&#23391;&#21152;&#25289;&#25991;&#20013;&#26292;&#21147;&#29053;&#21160;&#25991;&#23383;&#26816;&#27979;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#30740;&#31350;&#25968;&#25454;&#22686;&#24378;&#21644;&#20351;&#29992;&#22810;&#35821;&#35328;-e5-base&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#22312;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;68.11%&#30340;&#23439;F1&#20540;&#65292;&#22312;&#25490;&#34892;&#27036;&#19978;&#25490;&#21517;&#31532;23&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#23391;&#21152;&#25289;&#25991;&#20013;&#26292;&#21147;&#29053;&#21160;&#25991;&#23383;&#26816;&#27979;&#30340;&#20849;&#20139;&#20219;&#21153;&#20013;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25105;&#20204;&#20351;&#29992;&#30340;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#26041;&#27861;&#26469;&#35753;&#25105;&#20204;&#30340;&#27169;&#22411;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#26377;&#21161;&#20110;&#20998;&#31867;&#21028;&#26029;&#32473;&#23450;&#30340;&#25991;&#26412;&#26159;&#21542;&#21253;&#21547;&#23041;&#32961;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#38598;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25105;&#20204;&#30340;&#20219;&#21153;&#20013;&#65292;&#23545;&#22810;&#35821;&#35328;-e5-base&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20854;&#20182;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#27979;&#35797;&#38598;&#20013;&#33719;&#24471;&#20102;68.11%&#30340;&#23439;F1&#20540;&#65292;&#24182;&#22312;&#36825;&#20010;&#20849;&#20139;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;23&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the system that we have developed while solving this shared task on violence inciting text detection in Bangla. We explain both the traditional and the recent approaches that we have used to make our models learn. Our proposed system helps to classify if the given text contains any threat. We studied the impact of data augmentation when there is a limited dataset available. Our quantitative results show that finetuning a multilingual-e5-base model performed the best in our task compared to other transformer-based architectures. We obtained a macro F1 of 68.11\% in the test set and our performance in this shared task is ranked at 23 in the leaderboard.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#20351;&#29992;GPT-4&#22788;&#29702;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10765</link><description>&lt;p&gt;
BiomedJourney: &#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#20013;&#30340;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys. (arXiv:2310.10765v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#20351;&#29992;GPT-4&#22788;&#29702;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22270;&#20687;&#32534;&#36753;&#30340;&#25351;&#23548;&#23398;&#20064;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#22914;InstructPix2Pix&#65292;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21487;&#20197;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#21453;&#20107;&#23454;&#22270;&#20687;&#29983;&#25104;&#65292;&#20174;&#32780;&#24110;&#21161;&#21306;&#20998;&#22240;&#26524;&#32467;&#26500;&#21644;&#20266;&#30456;&#20851;&#65292;&#24182;&#20419;&#36827;&#30142;&#30149;&#36827;&#23637;&#24314;&#27169;&#30340;&#31283;&#20581;&#22270;&#20687;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#30340;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#24182;&#19981;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#36824;&#36828;&#26410;&#28145;&#20837;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#32473;&#23450;&#19968;&#20010;&#25293;&#25668;&#20110;&#19981;&#21516;&#26102;&#38388;&#28857;&#30340;&#20004;&#20010;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#30340;&#24739;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#22788;&#29702;&#30456;&#24212;&#30340;&#22270;&#20687;&#25253;&#21578;&#65292;&#24182;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#19977;&#20803;&#32452;&#65288;&#20808;&#21069;&#22270;&#20687;&#12289;&#36827;&#23637;&#25551;&#36848;&#12289;&#26032;&#22270;&#20687;&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such methods can be applied to counterfactual image generation, which helps differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual biomedical image generation is largely underexplored. In this paper, we present BiomedJourney, a novel method for counterfactual biomedical image generation by instruction-learning from multimodal patient journeys. Given a patient with two biomedical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate a natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20943;&#23569;&#20174;&#36130;&#21153;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#26102;&#30340;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#21644;&#20803;&#25968;&#25454;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#20449;&#24687;&#25552;&#21462;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.10760</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20943;&#23569;&#20174;&#36130;&#21153;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#20013;&#30340;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Towards reducing hallucination in extracting information from financial reports using Large Language Models. (arXiv:2310.10760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20943;&#23569;&#20174;&#36130;&#21153;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#26102;&#30340;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#21644;&#20803;&#25968;&#25454;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#20449;&#24687;&#25552;&#21462;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36130;&#21153;&#20998;&#26512;&#24072;&#26469;&#35828;&#65292;&#20844;&#21496;&#36130;&#21153;&#25253;&#21578;&#30340;&#38382;&#31572;&#65288;Q&amp;A&#65289;&#37096;&#20998;&#26159;&#36827;&#34892;&#21508;&#31181;&#20998;&#26512;&#21644;&#25237;&#36164;&#20915;&#31574;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20174;&#38382;&#31572;&#37096;&#20998;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#38754;&#20020;&#30528;&#24456;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#26041;&#27861;&#22914;&#35814;&#32454;&#38405;&#35835;&#21644;&#35760;&#31508;&#35760;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#20154;&#20026;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#32780;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#21644;&#31867;&#20284;&#25216;&#26415;&#22312;&#20934;&#30830;&#22788;&#29702;&#26080;&#32467;&#26500;&#21270;&#30340;&#25991;&#23383;&#36716;&#24405;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#24120;&#24120;&#38169;&#36807;&#39537;&#21160;&#25237;&#36164;&#32773;&#20915;&#31574;&#30340;&#24494;&#22937;&#30340;&#35821;&#35328;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#25928;&#24555;&#36895;&#22320;&#20174;&#25910;&#30410;&#25253;&#21578;&#36716;&#24405;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#21516;&#26102;&#36890;&#36807;&#32467;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#20197;&#21450;&#20803;&#25968;&#25454;&#26469;&#30830;&#20445;&#39640;&#31934;&#24230;&#30340;&#36716;&#21270;&#36807;&#31243;&#65292;&#24182;&#20943;&#23569;&#20102;&#38169;&#35823;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a financial analyst, the question and answer (Q\&amp;A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q\&amp;A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24265;&#20215;&#22320;&#22312;&#29616;&#26377;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#24182;&#23545;&#29305;&#23450;&#21457;&#35328;&#36827;&#34892;&#22870;&#24809;&#65292;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10735</link><description>&lt;p&gt;
&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26500;&#24314;&#20010;&#24615;&#19968;&#33268;&#30340;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning. (arXiv:2310.10735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24265;&#20215;&#22320;&#22312;&#29616;&#26377;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#24182;&#23545;&#29305;&#23450;&#21457;&#35328;&#36827;&#34892;&#22870;&#24809;&#65292;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25345;&#19968;&#33268;&#30340;&#20010;&#24615;&#26159;&#20219;&#20309;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#21697;&#36136;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#25110;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#20195;&#29702;&#65292;&#20197;&#23454;&#29616;&#20010;&#24615;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#31995;&#32479;&#24448;&#24448;&#32570;&#20047;&#19968;&#33268;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20174;&#26469;&#27809;&#26377;&#22240;&#35328;&#20043;&#19981;&#20934;&#32780;&#21463;&#21040;&#24809;&#32602;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#39069;&#22806;&#35757;&#32451;&#21487;&#20197;&#32531;&#35299;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#20294;&#26159;&#35757;&#32451;&#36807;&#31243;&#24456;&#26114;&#36149;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#25913;&#21892;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#20197;&#21069;&#30340;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#26082;&#21487;&#20197;&#20687;&#30417;&#30563;&#23398;&#20064;&#37027;&#26679;&#24265;&#20215;&#22320;&#22312;&#29616;&#26377;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#21448;&#21487;&#20197;&#20687;&#24378;&#21270;&#23398;&#20064;&#37027;&#26679;&#23545;&#29305;&#23450;&#21457;&#35328;&#36827;&#34892;&#22870;&#24809;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#26041;&#24046;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#26041;&#24046;&#20943;&#23569;MLE&#21021;&#22987;&#21270;&#65288;VaRMI&#65289;&#37325;&#35201;&#24615;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Maintaining a consistent persona is a key quality for any open domain dialogue system. Current state-of-the-art systems do this by training agents with supervised learning or online reinforcement learning (RL). However, systems trained with supervised learning often lack consistency as they are never punished for uttering contradictions. Additional training with RL can alleviate some of these issues, however the training process is expensive. Instead, we propose an offline RL framework to improve the persona consistency of dialogue systems. Our framework allows us to combine the advantages of previous methods as we can inexpensively train our model on existing data as in supervised learning, while punishing and rewarding specific utterances as in RL. We also introduce a simple importance sampling method to reduce the variance of importance weights in offline RL training which we call Variance-Reducing MLE-Initialized (VaRMI) importance sampling. Our automatic and human evaluations show
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#25915;&#20987;&#24615;&#20869;&#23481;&#25913;&#20889;&#26041;&#38754;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#36755;&#20837;-&#26631;&#31614;&#28436;&#31034;&#23545;&#26469;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#26597;&#35810;&#30340;&#25152;&#38656;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#21487;&#29992;&#24615;&#21644;&#20943;&#23569;&#25915;&#20987;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10707</link><description>&lt;p&gt;
&#28436;&#31034;&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65306;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25512;&#36827;&#25915;&#20987;&#24615;&#20869;&#23481;&#25913;&#20889;
&lt;/p&gt;
&lt;p&gt;
Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning. (arXiv:2310.10707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10707
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#25915;&#20987;&#24615;&#20869;&#23481;&#25913;&#20889;&#26041;&#38754;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#36755;&#20837;-&#26631;&#31614;&#28436;&#31034;&#23545;&#26469;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#26597;&#35810;&#30340;&#25152;&#38656;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#21487;&#29992;&#24615;&#21644;&#20943;&#23569;&#25915;&#20987;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#20889;&#25915;&#20987;&#24615;&#20869;&#23481;&#26159;&#19968;&#31181;&#26356;&#22909;&#30340;&#26367;&#20195;&#20869;&#23481;&#21024;&#38500;&#30340;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#27807;&#36890;&#29615;&#22659;&#30340;&#25991;&#26126;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#24335;&#30340;&#25913;&#20889;&#22120;&#22312;&#20445;&#30041;&#24847;&#20041;&#21644;&#24847;&#22270;&#30340;&#21516;&#26102;&#65292;&#23545;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#20381;&#36182;&#24615;&#36739;&#39640;&#12290;&#23427;&#20204;&#20063;&#20445;&#30041;&#20102;&#21407;&#22987;&#20869;&#23481;&#30340;&#22823;&#37096;&#20998;&#25915;&#20987;&#24615;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#25972;&#20307;&#21487;&#29992;&#24615;&#30340;&#30097;&#38382;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#20174;&#19994;&#32773;&#24320;&#21457;&#21487;&#29992;&#30340;&#25913;&#20889;&#22120;&#65292;&#21363;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#36755;&#20837;-&#26631;&#31614;&#28436;&#31034;&#23545;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#26597;&#35810;&#30340;&#25152;&#38656;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20851;&#38190;&#22240;&#32032;&#65292;&#22914;&#28436;&#31034;&#30340;&#25968;&#37327;&#21644;&#39034;&#24207;&#65292;&#25490;&#38500;&#25552;&#31034;&#25351;&#20196;&#65292;&#20197;&#21450;&#38477;&#20302;&#27979;&#37327;&#27602;&#24615;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#25105;&#20204;&#25552;&#20986;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#31036;&#35980;&#25913;&#20889;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#21407;&#21017;&#24615;&#35780;&#20272;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#35805;&#24335;&#30340;&#31895;&#40065;&#21457;&#35328;&#12289;&#31036;&#35980;&#25913;&#20889;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. Supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. They also retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. In this paper we aim to assist practitioners in developing usable paraphrasers by exploring In-Context Learning (ICL) with large language models (LLMs), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. Our study focuses on key factors such as -- number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. We perform principled evaluation on three datasets, including our proposed Context-Aware Polite Paraphrase dataset, comprising of dialogue-style rude utterances, polite paraphr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#36741;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#24341;&#23548;&#21644;&#36873;&#25321;&#27169;&#22411;&#36755;&#20986;&#33021;&#22815;&#24102;&#26469;&#26368;&#22823;&#30340;&#25928;&#30410;&#65292;&#24182;&#19988;&#19982;&#33258;&#30001;&#32534;&#36753;&#30456;&#27604;&#24182;&#19981;&#25439;&#23475;&#21442;&#19982;&#32773;&#23545;&#25511;&#21046;&#30340;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2310.10706</link><description>&lt;p&gt;
&#21457;&#25381;LLMs&#30340;&#33021;&#37327;&#65306;&#36890;&#36807;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#35270;&#35282;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of LLMs: Evaluating Human-AI text Co-Creation through the Lens of News Headline Generation. (arXiv:2310.10706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10706
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#36741;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#24341;&#23548;&#21644;&#36873;&#25321;&#27169;&#22411;&#36755;&#20986;&#33021;&#22815;&#24102;&#26469;&#26368;&#22823;&#30340;&#25928;&#30410;&#65292;&#24182;&#19988;&#19982;&#33258;&#30001;&#32534;&#36753;&#30456;&#27604;&#24182;&#19981;&#25439;&#23475;&#21442;&#19982;&#32773;&#23545;&#25511;&#21046;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25506;&#32034;&#20154;&#31867;&#22914;&#20309;&#26368;&#22909;&#22320;&#21033;&#29992;LLMs&#36827;&#34892;&#20889;&#20316;&#65292;&#24182;&#20102;&#35299;&#19982;&#36825;&#20123;&#27169;&#22411;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#20889;&#20316;&#36807;&#31243;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#21644;&#20449;&#20219;&#24230;&#65292;&#25105;&#20204;&#22312;LLM&#36741;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#27604;&#36739;&#20102;&#24120;&#35265;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#31867;&#22411;&#65288;&#20363;&#22914;&#65292;&#24341;&#23548;&#31995;&#32479;&#65292;&#20174;&#31995;&#32479;&#36755;&#20986;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#21518;&#26399;&#32534;&#36753;&#36755;&#20986;&#65289;&#12290;&#23613;&#31649;LLMs&#21333;&#29420;&#21487;&#20197;&#29983;&#25104;&#20196;&#20154;&#28385;&#24847;&#30340;&#26032;&#38395;&#26631;&#39064;&#65292;&#20294;&#24179;&#22343;&#32780;&#35328;&#65292;&#20154;&#31867;&#30340;&#25511;&#21046;&#26159;&#38656;&#35201;&#30340;&#65292;&#20197;&#20462;&#22797;&#19981;&#21487;&#21462;&#30340;&#27169;&#22411;&#36755;&#20986;&#12290;&#22312;&#21508;&#31181;&#20132;&#20114;&#26041;&#27861;&#20013;&#65292;&#24341;&#23548;&#21644;&#36873;&#25321;&#27169;&#22411;&#36755;&#20986;&#22686;&#21152;&#20102;&#26368;&#22810;&#30340;&#25928;&#30410;&#65292;&#20195;&#20215;&#26368;&#20302;&#65288;&#26102;&#38388;&#21644;&#31934;&#21147;&#65289;&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#26234;&#33021;&#21327;&#21161;&#24182;&#27809;&#26377;&#25439;&#23475;&#21442;&#19982;&#32773;&#23545;&#25511;&#21046;&#30340;&#24863;&#30693;&#65292;&#19982;&#33258;&#30001;&#32534;&#36753;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
To explore how humans can best leverage LLMs for writing and how interacting with these models affects feelings of ownership and trust in the writing process, we compared common human-AI interaction types (e.g., guiding system, selecting from system outputs, post-editing outputs) in the context of LLM-assisted news headline generation. While LLMs alone can generate satisfactory news headlines, on average, human control is needed to fix undesirable model outputs. Of the interaction methods, guiding and selecting model output added the most benefit with the lowest cost (in time and effort). Further, AI assistance did not harm participants' perception of control compared to freeform editing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#22312;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#20351;&#29992;&#20174;&#36716;&#24405;&#25968;&#25454;&#20013;&#24471;&#21040;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#35821;&#35328;&#29305;&#23450;&#30340;&#35789;&#27719;&#35843;&#25972;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#32416;&#27491;&#37325;&#22797;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2310.10704</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;&#20013;&#30340;&#20998;&#35789;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Optimized Tokenization for Transcribed Error Correction. (arXiv:2310.10704v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#22312;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#20351;&#29992;&#20174;&#36716;&#24405;&#25968;&#25454;&#20013;&#24471;&#21040;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#35821;&#35328;&#29305;&#23450;&#30340;&#35789;&#27719;&#35843;&#25972;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#32416;&#27491;&#37325;&#22797;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#22914;&#21457;&#38899;&#21464;&#21270;&#12289;&#19981;&#33391;&#38899;&#39057;&#26465;&#20214;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#24378;&#35843;&#20102;&#21518;&#22788;&#29702;&#27493;&#39588;&#32416;&#27491;&#37325;&#22797;&#38169;&#35823;&#30340;&#24517;&#35201;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37319;&#29992;&#19987;&#29992;&#30340;&#38169;&#35823;&#20462;&#27491;&#27169;&#22411;&#20855;&#26377;&#20248;&#21183;&#65292;&#28982;&#32780;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#24182;&#19981;&#23481;&#26131;&#33719;&#24471;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#36890;&#24120;&#20351;&#29992;&#21512;&#25104;&#30340;&#31867;&#20284;&#36716;&#24405;&#30340;&#25968;&#25454;&#65292;&#28982;&#32780;&#65292;&#24357;&#21512;&#36716;&#24405;&#38169;&#35823;&#21644;&#21512;&#25104;&#22122;&#22768;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#36317;&#24182;&#19981;&#31616;&#21333;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32416;&#27491;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#23637;&#31034;&#65306;&#65288;1&#65289;&#20351;&#29992;&#20174;&#19968;&#32452;&#36716;&#24405;&#25968;&#25454;&#20013;&#24471;&#21040;&#30340;&#38169;&#35823;&#20998;&#24067;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#20248;&#20110;&#24212;&#29992;&#38543;&#26426;&#25200;&#21160;&#30340;&#24120;&#35265;&#26041;&#27861;&#65307;&#65288;2&#65289;&#24212;&#29992;&#35821;&#35328;&#29305;&#23450;&#30340;&#35789;&#27719;&#35843;&#25972;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#32416;&#27491;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenges facing speech recognition systems, such as variations in pronunciations, adverse audio conditions, and the scarcity of labeled data, emphasize the necessity for a post-processing step that corrects recurring errors. Previous research has shown the advantages of employing dedicated error correction models, yet training such models requires large amounts of labeled data which is not easily obtained. To overcome this limitation, synthetic transcribed-like data is often utilized, however, bridging the distribution gap between transcribed errors and synthetic noise is not trivial. In this paper, we demonstrate that the performance of correction models can be significantly increased by training solely using synthetic data. Specifically, we empirically show that: (1) synthetic data generated using the error distribution derived from a set of transcribed data outperforms the common approach of applying random perturbations; (2) applying language-specific adjustments to the vocab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#28216;&#25103;&#20013;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#29616;&#20986;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10701</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind for Multi-Agent Collaboration via Large Language Models. (arXiv:2310.10701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#28216;&#25103;&#20013;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#29616;&#20986;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20294;&#23427;&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35268;&#21010;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#25991;&#26412;&#28216;&#25103;&#20013;&#35780;&#20272;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#22312;&#29702;&#35770;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#20986;&#29616;&#20102;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#22312;&#38271;&#26399;&#35268;&#21010;&#19978;&#23384;&#22312;&#20248;&#21270;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#23545;&#20219;&#21153;&#29366;&#24577;&#30340;&#38169;&#35823;&#35748;&#30693;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#35821;&#20041;&#24605;&#32500;&#38142;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#24341;&#20837;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20197;&#27492;&#26469;&#23454;&#29616;&#26356;&#31934;&#32454;&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.10698</link><description>&lt;p&gt;
&#23558;&#20195;&#30721;&#35821;&#20041;&#19982;LLMs&#30456;&#32467;&#21512;&#65306;&#20195;&#30721;&#29983;&#25104;&#30340;&#35821;&#20041;&#24605;&#32500;&#38142;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation. (arXiv:2310.10698v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#35821;&#20041;&#24605;&#32500;&#38142;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#24341;&#20837;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20197;&#27492;&#26469;&#23454;&#29616;&#26356;&#31934;&#32454;&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#20195;&#30721;&#29983;&#25104;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23545;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#21644;&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#39640;&#32423;&#35821;&#20041;&#26144;&#23556;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#22522;&#20110;LLMs&#30340;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#20381;&#36182;&#20110;&#20165;&#20351;&#29992;&#35299;&#30721;&#22120;&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#24120;&#23558;&#20195;&#30721;&#20165;&#35270;&#20026;&#32431;&#25991;&#26412;&#26631;&#35760;&#65292;&#21363;&#23558;&#38656;&#27714;&#20316;&#20026;&#25552;&#31034;&#36755;&#20837;&#65292;&#24182;&#23558;&#20195;&#30721;&#20316;&#20026;&#19968;&#31995;&#21015;&#24179;&#38754;&#26631;&#35760;&#36755;&#20986;&#65292;&#21487;&#33021;&#20250;&#24573;&#30053;&#28304;&#20195;&#30721;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#35821;&#20041;&#29305;&#24449;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#35821;&#20041;&#24605;&#32500;&#38142;&#8221;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#20195;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#31216;&#20026;SeCoT&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#28304;&#20195;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#65288;&#20363;&#22914;&#25968;&#25454;&#27969;&#21644;&#25511;&#21046;&#27969;&#65289;&#26356;&#31934;&#30830;&#22320;&#25551;&#36848;&#20102;&#31243;&#24207;&#25191;&#34892;&#34892;&#20026;&#12289;&#24847;&#22270;&#21644;&#21151;&#33021;&#12290;&#36890;&#36807;&#24341;&#23548;LLMs&#32771;&#34385;&#21644;&#25972;&#21512;&#35821;&#20041;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#20195;&#30721;&#26356;&#31934;&#32454;&#30340;&#29702;&#35299;&#21644;&#34920;&#31034;&#65292;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have showcased remarkable prowess in code generation. However, automated code generation is still challenging since it requires a high-level semantic mapping between natural language requirements and codes. Most existing LLMs-based approaches for code generation rely on decoder-only causal language models often treate codes merely as plain text tokens \ie feeding the requirements as a prompt input, and outputing code as flat sequence of tokens, potentially missing the rich semantic features inherent in source code. To bridge this gap, this paper proposes the "Semantic Chain-of-Thought" approach to intruduce semantic information of code, named SeCoT. Our motivation is that the semantic information of the source code (\eg data flow and control flow) describes more precise program execution behavior, intention and function. By guiding LLM consider and integrate semantic information, we can achieve a more granular understanding and representation of code, enhan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;LLM-SS&#65292;&#36890;&#36807;&#21512;&#25104;&#23398;&#29983;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#65292;&#20026;&#23398;&#29983;&#24314;&#27169;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#25945;&#23398;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.10690</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#29983;&#24314;&#27169;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20174;&#19968;&#27425;&#24615;&#35266;&#23519;&#20013;&#21512;&#25104;&#35270;&#35273;&#32534;&#31243;&#20013;&#23398;&#29983;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation. (arXiv:2310.10690v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;LLM-SS&#65292;&#36890;&#36807;&#21512;&#25104;&#23398;&#29983;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#65292;&#20026;&#23398;&#29983;&#24314;&#27169;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#24314;&#27169;&#23545;&#20110;&#35768;&#22810;&#25945;&#32946;&#25216;&#26415;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#23398;&#20064;&#32467;&#26524;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#22240;&#20026;&#23398;&#29983;&#34920;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#19988;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#30340;&#23398;&#20064;&#25216;&#33021;&#38598;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LLM-SS&#65292;&#21033;&#29992;LLMs&#21512;&#25104;&#23398;&#29983;&#30340;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#29305;&#23450;&#23398;&#29983;&#22312;&#21442;&#32771;&#20219;&#21153;&#19978;&#30340;&#35299;&#20915;&#23581;&#35797;&#20316;&#20026;&#35266;&#23519;&#65292;&#30446;&#26631;&#26159;&#21512;&#25104;&#35813;&#23398;&#29983;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#19982;&#19981;&#21516;&#30340;LLMs&#32467;&#21512;&#20351;&#29992;&#65307;&#32780;&#19988;&#65292;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#23427;&#20204;&#23545;&#39046;&#22495;&#32972;&#26223;&#21644;&#23398;&#29983;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#20855;&#20307;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Student modeling is central to many educational technologies as it enables the prediction of future learning outcomes and targeted instructional strategies. However, open-ended learning environments pose challenges for accurately modeling students due to the diverse behaviors exhibited by students and the absence of a well-defined set of learning skills. To approach these challenges, we explore the application of Large Language Models (LLMs) for in-context student modeling in open-ended learning environments. We introduce a novel framework, LLM-SS, that leverages LLMs for synthesizing student's behavior. More concretely, given a particular student's solving attempt on a reference task as observation, the goal is to synthesize the student's attempt on a target task. Our framework can be combined with different LLMs; moreover, we fine-tune LLMs using domain-specific expertise to boost their understanding of domain background and student behaviors. We evaluate several concrete methods bas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10688</link><description>&lt;p&gt;
&#19968;&#31181;&#20165;&#35299;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24320;&#31665;&#21363;&#29992;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#25509;&#36817;&#27599;&#20010;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#22312;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#21382;&#21490;&#38271;&#24230;&#12289;&#39044;&#27979;&#38271;&#24230;&#21644;&#26102;&#38388;&#31890;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#22806;&#37096;&#31243;&#24207;&#30340;&#36741;&#21161;&#19979;&#32500;&#25345;&#26641;&#25628;&#32034;&#33021;&#21147;&#65292;&#24182;&#20135;&#29983;&#23637;&#31034;&#26641;&#32467;&#26500;&#25628;&#32034;&#36807;&#31243;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.10686</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#26641;&#25628;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Autonomous Tree-search Ability of Large Language Models. (arXiv:2310.10686v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10686
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#22806;&#37096;&#31243;&#24207;&#30340;&#36741;&#21161;&#19979;&#32500;&#25345;&#26641;&#25628;&#32034;&#33021;&#21147;&#65292;&#24182;&#20135;&#29983;&#23637;&#31034;&#26641;&#32467;&#26500;&#25628;&#32034;&#36807;&#31243;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#19979;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#38656;&#35201;&#25506;&#32034;&#12289;&#25112;&#30053;&#39044;&#35265;&#21644;&#39034;&#24207;&#20915;&#31574;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#36275;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#22806;&#37096;&#31243;&#24207;&#23450;&#20041;&#25628;&#32034;&#36923;&#36753;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25191;&#34892;&#34987;&#21160;&#30340;&#26641;&#25628;&#32034;&#26469;&#35299;&#20915;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#33509;&#24178;&#22522;&#26412;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#34987;&#21160;&#30340;&#26641;&#25628;&#32034;&#19981;&#39640;&#25928;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#36718;&#35821;&#35328;&#27169;&#22411;API&#35843;&#29992;&#26469;&#35299;&#20915;&#21333;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#34987;&#21160;&#25628;&#32034;&#26041;&#27861;&#19981;&#28789;&#27963;&#65292;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#30340;&#31243;&#24207;&#35774;&#35745;&#12290;&#28982;&#21518;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#20986;&#29616;&#65306;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#22312;&#27809;&#26377;&#22806;&#37096;&#31243;&#24207;&#30340;&#36741;&#21161;&#19979;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#30340;&#26641;&#25628;&#32034;&#33021;&#21147;&#65292;&#20173;&#28982;&#33021;&#22815;&#29983;&#25104;&#28165;&#26224;&#23637;&#31034;&#26641;&#32467;&#26500;&#25628;&#32034;&#36807;&#31243;&#30340;&#21709;&#24212;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;&#31216;&#20026;&#33258;&#20027;&#26641;&#25628;&#32034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have excelled in remarkable reasoning capabilities with advanced prompting techniques, but they fall short on tasks that require exploration, strategic foresight, and sequential decision-making. Recent works propose to utilize external programs to define search logic, such that LLMs can perform passive tree search to solve more challenging reasoning tasks. Though impressive results have been achieved, there are several fundamental limitations of these approaches. First, passive tree searches are not efficient as they usually require multiple rounds of LLM API calls to solve one single problem. Moreover, passive search methods are not flexible since they need task-specific program designs. Then a natural question arises: can we maintain the tree-search capability of LLMs without the aid of external programs, and can still generate responses that clearly demonstrate the process of a tree-structure search? To this end, we propose a new concept called autonomous tree-
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#26159;&#19968;&#20010;&#30740;&#31350;&#30340;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#20010;&#22330;&#26223;&#65292;&#21487;&#20197;&#36890;&#36807;&#21435;&#23398;&#20064;&#35753;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65292;&#21482;&#38656;&#35201;&#36127;&#38754;&#31034;&#20363;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#29305;&#21035;&#23545;&#20110;&#30693;&#36947;&#20855;&#20307;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#30340;&#35757;&#32451;&#26679;&#26412;&#26356;&#20026;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.10683</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Unlearning. (arXiv:2310.10683v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10683
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#26159;&#19968;&#20010;&#30740;&#31350;&#30340;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#20010;&#22330;&#26223;&#65292;&#21487;&#20197;&#36890;&#36807;&#21435;&#23398;&#20064;&#35753;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65292;&#21482;&#38656;&#35201;&#36127;&#38754;&#31034;&#20363;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#29305;&#21035;&#23545;&#20110;&#30693;&#36947;&#20855;&#20307;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#30340;&#35757;&#32451;&#26679;&#26412;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#23398;&#20064;&#65292;&#21363;&#24536;&#35760;&#19981;&#21463;&#27426;&#36814;&#30340;&#65288;&#38750;&#65289;&#34892;&#20026;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33267;&#23569;&#19977;&#31181;&#24773;&#22659;&#21487;&#20197;&#20174;&#21435;&#23398;&#20064;&#20013;&#20351;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65306;&#65288;1&#65289;&#21024;&#38500;&#26377;&#23475;&#22238;&#22797;&#65292;&#65288;2&#65289;&#25353;&#35201;&#27714;&#21024;&#38500;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#65288;3&#65289;&#28040;&#38500;&#24187;&#35273;&#12290;&#20316;&#20026;&#23545;&#40784;&#25216;&#26415;&#30340;&#19968;&#31181;&#65292;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;&#21482;&#38656;&#35201;&#36127;&#38754;&#65288;&#20363;&#22914;&#26377;&#23475;&#65289;&#31034;&#20363;&#65292;&#36825;&#27604;&#22312;RLHF&#65288;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#20013;&#25152;&#38656;&#30340;&#27491;&#38754;&#65288;&#20363;&#22914;&#26377;&#24110;&#21161;&#19988;&#36890;&#24120;&#30001;&#20154;&#31867;&#32534;&#20889;&#65289;&#31034;&#20363;&#26356;&#23481;&#26131;&#21644;&#26356;&#20415;&#23452;&#22320;&#25910;&#38598;&#65288;&#20363;&#22914;&#36890;&#36807;&#32418;&#38431;&#27979;&#35797;&#25110;&#29992;&#25143;&#25253;&#21578;&#65289;&#65307;&#65288;2&#65289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65307;&#65288;3&#65289;&#24403;&#25105;&#20204;&#30693;&#36947;&#21738;&#20123;&#35757;&#32451;&#26679;&#26412;&#23548;&#33268;&#20102;&#19981;&#33391;&#34892;&#20026;&#26102;&#65292;&#23427;&#29305;&#21035;&#26377;&#25928;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;LLM&#21435;&#23398;&#20064;&#30340;&#24037;&#20316;&#20043;&#19968;&#12290;&#25105;&#20204;&#20063;&#26159;&#39318;&#27425;&#22312;LLM&#21435;&#23398;&#20064;&#20013;&#21046;&#23450;&#20102;&#35774;&#32622;&#12289;&#30446;&#26631;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#20174;&#19994;&#32773;&#21482;&#26377;&#26377;&#38480;&#30340;
&lt;/p&gt;
&lt;p&gt;
We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25104;&#21151;&#22797;&#21046;&#20102;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#65292;&#20294;&#20854;&#32467;&#26524;&#34920;&#26126;&#24179;&#22343;&#35780;&#32423;&#26377;&#19978;&#21319;&#20559;&#24046;&#21644;&#36739;&#20302;&#30340;&#21464;&#24322;&#24615;&#19982;&#32467;&#26500;&#25928;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.10679</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#36328;&#25991;&#21270;&#20010;&#24615;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Large language models can replicate cross-cultural differences in personality. (arXiv:2310.10679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10679
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25104;&#21151;&#22797;&#21046;&#20102;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#65292;&#20294;&#20854;&#32467;&#26524;&#34920;&#26126;&#24179;&#22343;&#35780;&#32423;&#26377;&#19978;&#21319;&#20559;&#24046;&#21644;&#36739;&#20302;&#30340;&#21464;&#24322;&#24615;&#19982;&#32467;&#26500;&#25928;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#39564;(N=8000)&#26469;&#30830;&#23450;GPT-4&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#12290;&#25105;&#20204;&#36873;&#25321;&#32654;&#22269;&#21644;&#38889;&#22269;&#20316;&#20026;&#25991;&#21270;&#23545;&#27604;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20004;&#20010;&#22269;&#23478;&#30340;&#20154;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#20154;&#26684;&#24046;&#24322;&#12290;&#25105;&#20204;&#25805;&#32437;&#20102;&#27169;&#25311;&#30340;&#30446;&#26631;&#65288;&#32654;&#22269; vs. &#38889;&#22269;&#65289;&#65292;&#38382;&#21367;&#30340;&#35821;&#35328;&#65288;&#33521;&#35821; vs. &#38889;&#35821;&#65289;&#20197;&#21450;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4 vs. GPT-3.5&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#22797;&#21046;&#20102;&#27599;&#20010;&#22240;&#23376;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#24179;&#22343;&#35780;&#32423;&#20855;&#26377;&#19978;&#21319;&#20559;&#24046;&#65292;&#24182;&#19988;&#27604;&#20154;&#31867;&#26679;&#26412;&#30340;&#21464;&#24322;&#24615;&#26356;&#20302;&#65292;&#20197;&#21450;&#32467;&#26500;&#25928;&#24230;&#36739;&#20302;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35777;&#25454;&#35828;&#26126;LLMs&#21487;&#20197;&#20419;&#36827;&#36328;&#25991;&#21270;&#24515;&#29702;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. Overall, we provide preliminary evidence that LLMs can aid cross-cultural psychological research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#19982;&#20154;&#31867;&#30340;&#38598;&#20307;&#22836;&#33041;&#39118;&#26292;&#20013;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#20026;&#25968;&#23398;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#21457;&#29616;&#21644;&#35299;&#20915;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2310.10677</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#25968;&#23398;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#28508;&#22312;&#22836;&#33041;&#39118;&#26292;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;
LLMs as Potential Brainstorming Partners for Math and Science Problems. (arXiv:2310.10677v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#19982;&#20154;&#31867;&#30340;&#38598;&#20307;&#22836;&#33041;&#39118;&#26292;&#20013;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#20026;&#25968;&#23398;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#21457;&#29616;&#21644;&#35299;&#20915;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24191;&#27867;&#25104;&#21151;&#65292;&#21508;&#31181;&#25968;&#23398;&#21644;&#31185;&#23398;&#31038;&#21306;&#30340;&#19987;&#19994;&#20154;&#22763;&#23545;&#35780;&#20272;&#26368;&#26032;&#27169;&#22411;&#22312;&#21457;&#29616;&#25110;&#35299;&#20915;&#36890;&#24120;&#38656;&#35201;&#21019;&#36896;&#21147;&#21644;&#22836;&#33041;&#39118;&#26292;&#30340;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#34429;&#28982;&#24403;&#21069;&#20154;&#26426;&#26234;&#33021;&#21327;&#20316;&#19982;&#35299;&#20915;&#22797;&#26434;&#30340;&#25968;&#23398;&#21644;&#31185;&#23398;&#38382;&#39064;&#20043;&#38388;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#65292;&#20363;&#22914;&#20845;&#20010;&#26410;&#35299;&#20915;&#30340;&#21315;&#31143;&#24180;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#21021;&#27493;&#35843;&#26597;&#26174;&#31034;&#20986;&#20102;&#36808;&#21521;&#24357;&#21512;&#40511;&#27807;&#30340;&#26377;&#24076;&#26395;&#30340;&#19968;&#27493;&#12290;&#36825;&#24471;&#30410;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#32034;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;LLM&#65288;&#29305;&#21035;&#26159;GPT-4&#65289;&#22312;&#19982;&#20154;&#31867;&#30340;&#38598;&#20307;&#22836;&#33041;&#39118;&#26292;&#20013;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent rise of widely successful deep learning models, there is emerging interest among professionals in various math and science communities to see and evaluate the state-of-the-art models' abilities to collaborate on finding or solving problems that often require creativity and thus brainstorming. While a significant chasm still exists between current human-machine intellectual collaborations and the resolution of complex math and science problems, such as the six unsolved Millennium Prize Problems, our initial investigation into this matter reveals a promising step towards bridging the divide. This is due to the recent advancements in Large Language Models (LLMs). More specifically, we conduct comprehensive case studies to explore both the capabilities and limitations of the current state-of-the-art LLM, notably GPT-4, in collective brainstorming with humans.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;WhatsApp&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20197;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#24182;&#36890;&#36807;WhatsApp&#25552;&#20379;&#26356;&#22909;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;&#36825;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#25928;&#12289;&#26377;&#25928;&#22320;&#22788;&#29702;&#29992;&#25143;&#30340;&#26597;&#35810;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.10675</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;WhatsApp&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21019;&#24314;
&lt;/p&gt;
&lt;p&gt;
Creation Of A ChatBot Based On Natural Language Proccesing For Whatsapp. (arXiv:2310.10675v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;WhatsApp&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20197;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#24182;&#36890;&#36807;WhatsApp&#25552;&#20379;&#26356;&#22909;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;&#36825;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#25928;&#12289;&#26377;&#25928;&#22320;&#22788;&#29702;&#29992;&#25143;&#30340;&#26597;&#35810;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#36716;&#22411;&#26102;&#20195;&#65292;&#23458;&#25143;&#26381;&#21153;&#23545;&#32452;&#32455;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#65292;&#20026;&#20102;&#28385;&#36275;&#23545;&#21363;&#26102;&#22238;&#22797;&#21644;&#20010;&#24615;&#21270;&#24110;&#21161;24&#23567;&#26102;&#19981;&#38388;&#26029;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#38656;&#27714;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#24037;&#20855;&#12290;&#30446;&#21069;&#65292;&#35768;&#22810;&#20844;&#21496;&#38656;&#35201;&#20026;&#20854;&#23458;&#25143;&#25552;&#20379;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#20379;&#21512;&#36866;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24320;&#21457;&#19968;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20197;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#24182;&#36890;&#36807;WhatsApp&#25552;&#20379;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#28857;&#26159;&#21019;&#24314;&#19968;&#20010;&#39640;&#25928;&#12289;&#26377;&#25928;&#22788;&#29702;&#29992;&#25143;&#26597;&#35810;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#24050;&#36827;&#34892;&#20102;&#19982;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#30456;&#20851;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20998;&#26512;&#20102;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#23454;&#26045;&#20013;&#20351;&#29992;&#30340;&#26041;&#27861;&#35770;&#26041;&#27861;&#12289;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#36136;&#37327;&#23646;&#24615;&#12290;&#25152;&#24471;&#21040;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of digital transformation, customer service is of paramount importance to the success of organizations, and to meet the growing demand for immediate responses and personalized assistance 24 hours a day, chatbots have become a promising tool to solve these problems. Currently, there are many companies that need to provide these solutions to their customers, which motivates us to study this problem and offer a suitable solution. The objective of this study is to develop a chatbot based on natural language processing to improve customer satisfaction and improve the quality of service provided by the company through WhatsApp. The solution focuses on creating a chatbot that efficiently and effectively handles user queries. A literature review related to existing chatbots has been conducted, analyzing methodological approaches, artificial intelligence techniques and quality attributes used in the implementation of chatbots. The results found highlight that chatbots based on natura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;LLMs&#26469;&#20272;&#35745;&#25991;&#26412;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#21644;PCA&#26144;&#23556;&#25552;&#20986;&#20102;&#25913;&#21892;&#25991;&#26412;&#25551;&#36848;&#29366;&#24577;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#20294;&#23454;&#39564;&#34920;&#26126;&#36825;&#19968;&#26041;&#27861;&#30446;&#21069;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.10673</link><description>&lt;p&gt;
&#26397;&#30528;&#22522;&#20110;&#24773;&#24863;&#30340;&#21512;&#25104;&#24847;&#35782;&#65306;&#20351;&#29992;LLMs&#20272;&#35745;&#24773;&#24863;&#27010;&#29575;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Towards Emotion-Based Synthetic Consciousness: Using LLMs to Estimate Emotion Probability Vectors. (arXiv:2310.10673v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;LLMs&#26469;&#20272;&#35745;&#25991;&#26412;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#21644;PCA&#26144;&#23556;&#25552;&#20986;&#20102;&#25913;&#21892;&#25991;&#26412;&#25551;&#36848;&#29366;&#24577;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#20294;&#23454;&#39564;&#34920;&#26126;&#36825;&#19968;&#26041;&#27861;&#30446;&#21069;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;LLMs&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#26469;&#20272;&#35745;&#19982;&#25991;&#26412;&#30456;&#20851;&#30340;&#24773;&#24863;&#29366;&#24577;&#30340;&#25688;&#35201;&#12290;&#24773;&#24863;&#29366;&#24577;&#30340;&#25688;&#35201;&#26159;&#19968;&#20010;&#35789;&#20856;&#65292;&#29992;&#20110;&#25551;&#36848;&#24773;&#24863;&#65292;&#24182;&#32473;&#20986;&#22312;&#21407;&#22987;&#25991;&#26412;&#21644;&#24773;&#24863;&#35302;&#21457;&#23614;&#37096;&#32452;&#25104;&#30340;&#25552;&#31034;&#21518;&#65292;&#35813;&#35789;&#20986;&#29616;&#30340;&#27010;&#29575;&#12290;&#36890;&#36807;&#23545;&#20122;&#39532;&#36874;&#20135;&#21697;&#35780;&#35770;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24773;&#24863;&#25551;&#36848;&#35789;&#21487;&#20197;&#26144;&#23556;&#21040;&#31867;&#20284;PCA&#30340;&#31354;&#38388;&#20013;&#12290;&#24076;&#26395;&#36890;&#36807;&#23614;&#37096;&#25552;&#31034;&#20063;&#21487;&#20197;&#24341;&#21457;&#20851;&#20110;&#25913;&#21892;&#24403;&#21069;&#25991;&#26412;&#25152;&#25551;&#36848;&#29366;&#24577;&#30340;&#21160;&#20316;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#23454;&#39564;&#20284;&#20046;&#34920;&#26126;&#65292;&#36825;&#31181;&#24037;&#20316;&#24182;&#19981;&#23481;&#26131;&#23454;&#29616;&#12290;&#36825;&#31181;&#22833;&#36133;&#20351;&#24471;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#27604;&#36739;&#24773;&#24863;&#21453;&#24212;&#26469;&#36873;&#25321;&#26368;&#20339;&#39044;&#27979;&#32467;&#26524;&#30340;&#34892;&#21160;&#30446;&#21069;&#26080;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper shows how LLMs (Large Language Models) may be used to estimate a summary of the emotional state associated with piece of text. The summary of emotional state is a dictionary of words used to describe emotion together with the probability of the word appearing after a prompt comprising the original text and an emotion eliciting tail. Through emotion analysis of Amazon product reviews we demonstrate emotion descriptors can be mapped into a PCA type space. It was hoped that text descriptions of actions to improve a current text described state could also be elicited through a tail prompt. Experiment seemed to indicate that this is not straightforward to make work. This failure put our hoped for selection of action via choosing the best predict ed outcome via comparing emotional responses out of reach for the moment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#37327;&#23376;&#26680;&#26041;&#27861;&#21644;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#32467;&#21512;&#32463;&#20856;&#30340;&#38477;&#32500;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#34920;&#36798;&#30340;&#20154;&#31867;&#24773;&#24863;&#21644;&#35266;&#28857;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#21462;&#24471;&#20102;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10672</link><description>&lt;p&gt;
&#34701;&#21512;&#37327;&#23376;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid Quantum-Classical Machine Learning for Sentiment Analysis. (arXiv:2310.10672v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#37327;&#23376;&#26680;&#26041;&#27861;&#21644;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#32467;&#21512;&#32463;&#20856;&#30340;&#38477;&#32500;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#34920;&#36798;&#30340;&#20154;&#31867;&#24773;&#24863;&#21644;&#35266;&#28857;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#21462;&#24471;&#20102;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#19982;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#30340;&#21512;&#20316;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#34920;&#36798;&#30340;&#20154;&#31867;&#24773;&#24863;&#21644;&#35266;&#28857;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#37327;&#23376;&#26680;&#26041;&#27861;&#21644;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#32463;&#20856;&#30340;&#38477;&#32500;&#25216;&#26415;&#65288;&#22914;PCA&#21644;Haar&#23567;&#27874;&#21464;&#25442;&#65289;&#32467;&#21512;&#36215;&#26469;&#12290;&#35813;&#26041;&#27861;&#22312;&#22522;&#20110;&#33521;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#30340;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23545;&#25968;&#25454;&#36827;&#34892;&#38477;&#32500;&#22788;&#29702;&#21518;&#65292;&#22522;&#20110;&#37327;&#23376;&#30340;&#28151;&#21512;&#31639;&#27861;&#30340;&#24615;&#33021;&#19968;&#33268;&#19988;&#20248;&#20110;&#32463;&#20856;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The collaboration between quantum computing and classical machine learning offers potential advantages in natural language processing, particularly in the sentiment analysis of human emotions and opinions expressed in large-scale datasets. In this work, we propose a methodology for sentiment analysis using hybrid quantum-classical machine learning algorithms. We investigate quantum kernel approaches and variational quantum circuit-based classifiers and integrate them with classical dimension reduction techniques such as PCA and Haar wavelet transform. The proposed methodology is evaluated using two distinct datasets, based on English and Bengali languages. Experimental results show that after dimensionality reduction of the data, performance of the quantum-based hybrid algorithms were consistent and better than classical methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#21152;&#23494;&#36164;&#20135;&#30417;&#31649;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#35843;&#26597;&#20102;&#23545;&#26410;&#21463;&#30417;&#31649;&#30340;&#21152;&#23494;&#36164;&#20135;&#30333;&#30382;&#20070;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#30340;&#29616;&#26377;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#20102;&#30740;&#31350;&#31354;&#30333;&#12290;&#28982;&#21518;&#65292;&#20998;&#26512;&#20102;&#27431;&#30431;&#30340;&#21152;&#23494;&#36164;&#20135;&#24066;&#22330;&#27861;&#35268;&#24341;&#20837;&#30340;&#21464;&#21270;&#65292;&#25506;&#35752;&#20102;&#22312;&#26032;&#30340;&#30417;&#31649;&#26694;&#26550;&#20869;&#25972;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#26377;&#28508;&#21147;&#20351;&#30417;&#31649;&#26426;&#26500;&#12289;&#21152;&#23494;&#36164;&#20135;&#21457;&#34892;&#32773;&#21644;&#25237;&#36164;&#32773;&#21463;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.10333</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#36164;&#20135;&#30417;&#31649;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65306;&#19968;&#20221;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
NLP for Crypto-Asset Regulation: A Roadmap. (arXiv:2310.10333v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10333
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#21152;&#23494;&#36164;&#20135;&#30417;&#31649;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#35843;&#26597;&#20102;&#23545;&#26410;&#21463;&#30417;&#31649;&#30340;&#21152;&#23494;&#36164;&#20135;&#30333;&#30382;&#20070;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#30340;&#29616;&#26377;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#20102;&#30740;&#31350;&#31354;&#30333;&#12290;&#28982;&#21518;&#65292;&#20998;&#26512;&#20102;&#27431;&#30431;&#30340;&#21152;&#23494;&#36164;&#20135;&#24066;&#22330;&#27861;&#35268;&#24341;&#20837;&#30340;&#21464;&#21270;&#65292;&#25506;&#35752;&#20102;&#22312;&#26032;&#30340;&#30417;&#31649;&#26694;&#26550;&#20869;&#25972;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#26377;&#28508;&#21147;&#20351;&#30417;&#31649;&#26426;&#26500;&#12289;&#21152;&#23494;&#36164;&#20135;&#21457;&#34892;&#32773;&#21644;&#25237;&#36164;&#32773;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#21306;&#22359;&#38142;&#36164;&#20135;&#39046;&#22495;&#65292;&#30333;&#30382;&#20070;&#26159;&#25237;&#36164;&#32773;&#25351;&#23548;&#30340;&#37325;&#35201;&#25991;&#20214;&#65292;&#22312;&#27431;&#30431;&#30340;&#21152;&#23494;&#36164;&#20135;&#24066;&#22330;&#27861;&#35268;&#65288;MiCAR&#65289;&#19979;&#65292;&#23427;&#20204;&#29616;&#22312;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#20869;&#23481;&#35201;&#27714;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#20316;&#20026;&#20998;&#26512;&#36825;&#20123;&#25991;&#20214;&#21644;&#21327;&#21161;&#30417;&#31649;&#21512;&#35268;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26412;&#25991;&#20026;&#35813;&#20027;&#39064;&#25552;&#20379;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23545;&#26410;&#21463;&#30417;&#31649;&#30340;&#21306;&#22359;&#38142;&#36164;&#20135;&#30333;&#30382;&#20070;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#30340;&#29616;&#26377;&#24212;&#29992;&#65292;&#21457;&#29616;&#20102;&#21487;&#20197;&#36890;&#36807;&#36328;&#23398;&#31185;&#21512;&#20316;&#26469;&#22635;&#34917;&#30740;&#31350;&#31354;&#30333;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;MiCAR&#24341;&#20837;&#30340;&#21464;&#21270;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#22312;&#26032;&#30340;&#30417;&#31649;&#26694;&#26550;&#20869;&#25972;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#26377;&#28508;&#21147;&#20351;&#30417;&#31649;&#26426;&#26500;&#12289;&#21306;&#22359;&#38142;&#36164;&#20135;&#21457;&#34892;&#32773;&#21644;&#25237;&#36164;&#32773;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of crypto-assets, white papers are essential documents for investor guidance, and are now subject to unprecedented content requirements under the EU's Markets in Crypto-Assets Regulation (MiCAR). Natural Language Processing can serve as a powerful tool for both analyzing these documents and assisting in regulatory compliance. This paper delivers two contributions to the topic. First, we survey existing applications of textual analysis to unregulated crypto-asset white papers, uncovering a research gap that could be bridged with interdisciplinary collaboration. We then conduct an analysis of the changes introduced by MiCAR, highlighting the opportunities and challenges of integrating NLP within the new regulatory framework. Our findings set the stage for further research, with the potential to benefit regulators, crypto-asset issuers, and investors.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;OpenAI&#30340;&#26368;&#26032;&#27169;&#22411;GPT-4V&#22312;&#22810;&#27169;&#24577;&#21307;&#23398;&#35786;&#26029;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25104;&#20687;&#27169;&#24577;&#21644;&#35299;&#21078;&#23398;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.09909</link><description>&lt;p&gt;
GPT-4V(ision)&#33021;&#22815;&#29992;&#20110;&#21307;&#30103;&#24212;&#29992;&#21527;&#65311;GPT-4V&#22312;&#22810;&#27169;&#24577;&#21307;&#23398;&#35786;&#26029;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis. (arXiv:2310.09909v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;OpenAI&#30340;&#26368;&#26032;&#27169;&#22411;GPT-4V&#22312;&#22810;&#27169;&#24577;&#21307;&#23398;&#35786;&#26029;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25104;&#20687;&#27169;&#24577;&#21644;&#35299;&#21078;&#23398;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#30001;&#20110;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#25512;&#21160;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#27493;&#65292;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;OpenAI&#26368;&#26032;&#27169;&#22411;GPT-4V(ision)&#22312;&#22810;&#27169;&#24577;&#21307;&#23398;&#35786;&#26029;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;17&#20010;&#20154;&#20307;&#31995;&#32479;&#65292;&#21253;&#25324;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#12289;&#22836;&#39048;&#37096;&#12289;&#24515;&#33039;&#12289;&#33016;&#37096;&#12289;&#34880;&#28082;&#23398;&#12289;&#32925;&#32966;&#12289;&#32963;&#32928;&#12289;&#27852;&#23615;&#29983;&#27542;&#12289;&#22919;&#31185;&#12289;&#20135;&#31185;&#12289;&#20083;&#33146;&#12289;&#32908;&#32905;&#39592;&#39612;&#12289;&#33034;&#26609;&#12289;&#34880;&#31649;&#12289;&#32959;&#30244;&#12289;&#21019;&#20260;&#12289;&#20799;&#31185;&#65292;&#20197;&#21450;&#20174;8&#31181;&#26085;&#24120;&#20020;&#24202;&#24120;&#29992;&#30340;&#25104;&#20687;&#27169;&#24577;&#33719;&#24471;&#30340;&#22270;&#20687;&#65292;&#20363;&#22914;X&#20809;&#12289;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;(CT)&#12289;&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#12289;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;(PET)&#12289;&#25968;&#23383;&#20943;&#24433;&#34880;&#31649;&#36896;&#24433;(DSA)&#12289;&#20083;&#33146;X&#20809;&#25668;&#24433;&#26415;&#12289;&#36229;&#22768;&#21644;&#30149;&#29702;&#23398;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;GPT-4V&#22312;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25104;&#20687;&#27169;&#24577;&#21644;&#35299;&#21078;&#23398;&#35782;&#21035;&#65292;&#26080;&#35770;&#26159;&#21542;&#25552;&#20379;&#19987;&#21033;&#21382;&#21490;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by the large foundation models, the development of artificial intelligence has witnessed tremendous progress lately, leading to a surge of general interest from the public. In this study, we aim to assess the performance of OpenAI's newest model, GPT-4V(ision), specifically in the realm of multimodal medical diagnosis. Our evaluation encompasses 17 human body systems, including Central Nervous System, Head and Neck, Cardiac, Chest, Hematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology, Obstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma, Pediatrics, with images taken from 8 modalities used in daily clinic routine, e.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Digital Subtraction Angiography (DSA), Mammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on multiple clinical tasks with or without patent history provided, including imaging modality and anatomy recognition, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;transformer&#30340;&#37325;&#26032;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#26684;&#37325;&#25490;&#24207;&#26469;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09680</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#26684;&#37325;&#25490;&#24207;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improved Contextual Recognition In Automatic Speech Recognition Systems By Semantic Lattice Rescoring. (arXiv:2310.09680v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;transformer&#30340;&#37325;&#26032;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#26684;&#37325;&#25490;&#24207;&#26469;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#31361;&#30772;&#20351;&#24471;ASR&#31995;&#32479;&#22312;&#20934;&#30830;&#36716;&#24405;&#21475;&#35821;&#30340;&#33021;&#21147;&#19978;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36825;&#26159;&#26500;&#24314;&#23545;&#35805;&#20195;&#29702;&#30340;&#20851;&#38190;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#36776;&#21035;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#21333;&#35789;&#21644;&#30701;&#35821;&#20173;&#28982;&#26159;&#19968;&#39033;&#36843;&#20999;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#26684;&#22788;&#29702;&#26469;&#22686;&#24378;ASR&#31995;&#32479;&#20013;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#20132;&#20184;&#21508;&#31181;&#35789;&#27719;&#21644;&#35828;&#35805;&#39118;&#26684;&#30340;&#36716;&#24405;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;HMM-GMM&#65289;&#65292;&#20197;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#21644;&#22768;&#23398;&#24314;&#27169;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#37325;&#26032;&#35780;&#20998;&#21333;&#35789;&#26684;&#65292;&#20351;&#25105;&#20204;&#30340;&#32593;&#32476;&#20855;&#22791;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) has witnessed a profound research interest. Recent breakthroughs have given ASR systems different prospects such as faithfully transcribing spoken language, which is a pivotal advancement in building conversational agents. However, there is still an imminent challenge of accurately discerning context-dependent words and phrases. In this work, we propose a novel approach for enhancing contextual recognition within ASR systems via semantic lattice processing leveraging the power of deep learning models in accurately delivering spot-on transcriptions across a wide variety of vocabularies and speaking styles. Our solution consists of using Hidden Markov Models and Gaussian Mixture Models (HMM-GMM) along with Deep Neural Networks (DNN) models integrating both language and acoustic modeling for better accuracy. We infused our network with the use of a transformer-based model to properly rescore the word lattice achieving remarkable capabilities with a palpa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#36798;&#26641;&#35299;&#30721;&#31574;&#30053;&#65292;&#23558;&#26641;&#32467;&#26500;&#25972;&#21512;&#21040;&#25968;&#23398;&#26041;&#31243;&#29983;&#25104;&#20013;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#39034;&#24207;&#26041;&#27861;&#24573;&#35270;&#24182;&#34892;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09619</link><description>&lt;p&gt;
&#25968;&#23398;&#26041;&#31243;&#29983;&#25104;&#30340;&#34920;&#36798;&#26641;&#35299;&#30721;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
An Expression Tree Decoding Strategy for Mathematical Equation Generation. (arXiv:2310.09619v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#36798;&#26641;&#35299;&#30721;&#31574;&#30053;&#65292;&#23558;&#26641;&#32467;&#26500;&#25972;&#21512;&#21040;&#25968;&#23398;&#26041;&#31243;&#29983;&#25104;&#20013;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#39034;&#24207;&#26041;&#27861;&#24573;&#35270;&#24182;&#34892;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25968;&#23398;&#26041;&#31243;&#38656;&#35201;&#20934;&#30830;&#29702;&#35299;&#25968;&#23398;&#34920;&#36798;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#26631;&#35760;&#32423;&#29983;&#25104;&#21644;&#34920;&#36798;&#24335;&#32423;&#29983;&#25104;&#20004;&#31867;&#12290;&#21069;&#32773;&#23558;&#26041;&#31243;&#35270;&#20026;&#25968;&#23398;&#35821;&#35328;&#65292;&#25353;&#39034;&#24207;&#29983;&#25104;&#25968;&#23398;&#26631;&#35760;&#12290;&#34920;&#36798;&#24335;&#32423;&#26041;&#27861;&#36880;&#20010;&#29983;&#25104;&#27599;&#20010;&#34920;&#36798;&#24335;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#34920;&#36798;&#24335;&#34920;&#31034;&#19968;&#20010;&#27714;&#35299;&#27493;&#39588;&#65292;&#36825;&#20123;&#27493;&#39588;&#20043;&#38388;&#33258;&#28982;&#23384;&#22312;&#24182;&#34892;&#25110;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#24403;&#21069;&#30340;&#39034;&#24207;&#26041;&#27861;&#24573;&#35270;&#20102;&#36825;&#20123;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26641;&#32467;&#26500;&#25972;&#21512;&#21040;&#34920;&#36798;&#24335;&#32423;&#29983;&#25104;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#36798;&#26641;&#35299;&#30721;&#31574;&#30053;&#12290;&#20026;&#20102;&#29983;&#25104;&#19968;&#20010;&#20197;&#34920;&#36798;&#24335;&#20026;&#33410;&#28857;&#30340;&#26641;&#65292;&#25105;&#20204;&#37319;&#29992;&#36880;&#23618;&#24182;&#34892;&#35299;&#30721;&#31574;&#30053;&#65306;&#22312;&#27599;&#19968;&#23618;&#21516;&#26102;&#35299;&#30721;&#22810;&#20010;&#29420;&#31435;&#34920;&#36798;&#24335;&#65288;&#21494;&#33410;&#28857;&#65289;&#65292;&#24182;&#37325;&#22797;&#36880;&#23618;&#36827;&#34892;&#24182;&#34892;&#35299;&#30721;&#65292;&#20197;&#39034;&#24207;&#29983;&#25104;&#20381;&#36182;&#20110;&#20854;&#20182;&#34920;&#36798;&#24335;&#30340;&#29238;&#33410;&#28857;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating mathematical equations from natural language requires an accurate understanding of the relations among math expressions. Existing approaches can be broadly categorized into token-level and expression-level generation. The former treats equations as a mathematical language, sequentially generating math tokens. Expression-level methods generate each expression one by one. However, each expression represents a solving step, and there naturally exist parallel or dependent relations between these steps, which are ignored by current sequential methods. Therefore, we integrate tree structure into the expression-level generation and advocate an expression tree decoding strategy. To generate a tree with expression as its node, we employ a layer-wise parallel decoding strategy: we decode multiple independent expressions (leaf nodes) in parallel at each layer and repeat parallel decoding layer by layer to sequentially generate these parent node expressions that depend on others. Beside
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Reward-Augmented Decoding (RAD)&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#30340;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#26469;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25991;&#26412;&#12290;RAD&#22312;&#29983;&#25104;&#38750;&#26377;&#23475;&#21644;&#24773;&#24863;&#21463;&#25511;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20063;&#24456;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.09520</link><description>&lt;p&gt;
Reward-Augmented Decoding: &#20351;&#29992;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#21463;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model. (arXiv:2310.09520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Reward-Augmented Decoding (RAD)&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#30340;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#26469;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25991;&#26412;&#12290;RAD&#22312;&#29983;&#25104;&#38750;&#26377;&#23475;&#21644;&#24773;&#24863;&#21463;&#25511;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20063;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#29983;&#25104;&#30340;&#25991;&#26412;&#23384;&#22312;&#38382;&#39064;&#25110;&#32773;&#32570;&#20047;&#25152;&#38656;&#30340;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reward-Augmented Decoding (RAD)&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#23567;&#22411;&#30340;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#26469;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25991;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;RAD&#21033;&#29992;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#37319;&#26679;&#27010;&#29575;&#26469;&#26356;&#20542;&#21521;&#20110;&#39640;&#22870;&#21169;&#30340;&#26631;&#35760;&#12290;&#36890;&#36807;&#20351;&#29992;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#65292;RAD&#33021;&#22815;&#32531;&#23384;&#20808;&#21069;&#29983;&#25104;&#27493;&#39588;&#30340;&#28608;&#27963;&#20540;&#65292;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#38750;&#26377;&#23475;&#21644;&#24773;&#24863;&#21463;&#25511;&#25991;&#26412;&#26041;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;RAD&#22312;&#20165;&#25913;&#21464;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#27861;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#19982;&#28041;&#21450;&#37325;&#26032;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;RAD&#22312;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09430</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#24050;&#32463;&#23558;&#20154;&#24037;&#31995;&#32479;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#21040;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#35780;&#20272;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#21517;&#20026;"ReClor-plus"&#12289;"LogiQA-plus"&#21644;"LogiQAv2-plus"&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#19977;&#20010;&#23376;&#38598;&#65306;&#31532;&#19968;&#20010;&#26159;&#36873;&#39033;&#38543;&#26426;&#25171;&#20081;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#27491;&#30830;&#36873;&#39033;&#26367;&#25442;&#20026;"&#27809;&#26377;&#20854;&#20182;&#36873;&#39033;&#26159;&#27491;&#30830;&#30340;"&#65292;&#31532;&#19977;&#20010;&#26159;&#21069;&#20004;&#20010;&#23376;&#38598;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#37492;&#21035;&#21644;&#29983;&#25104;&#22411;&#30340;LLMs&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#31616;&#21333;&#30340;&#25216;&#24039;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#21407;&#22987;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#24456;&#38590;&#22238;&#31572;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25200;&#21160;&#24341;&#20837;&#20219;&#21153;&#21464;&#21270;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options are correct", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#30693;&#35782;&#24341;&#23548;&#23545;&#35805;&#29983;&#25104;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#33258;&#36866;&#24212;&#23545;&#27604;&#23398;&#20064;&#65288;MACL&#65289;&#26694;&#26550;&#65292;&#24182;&#22312;&#20196;&#29260;&#32423;&#21644;&#24207;&#21015;&#32423;&#19978;&#21160;&#24577;&#37319;&#26679;&#36127;&#20363;&#26469;&#35299;&#20915;&#27169;&#22411;&#31616;&#21333;&#25554;&#20837;&#30693;&#35782;&#29255;&#27573;&#23548;&#33268;&#30340;&#36864;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08943</link><description>&lt;p&gt;
&#22810;&#23618;&#33258;&#36866;&#24212;&#23545;&#27604;&#23398;&#20064;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#30693;&#35782;&#20869;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation. (arXiv:2310.08943v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08943
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#30693;&#35782;&#24341;&#23548;&#23545;&#35805;&#29983;&#25104;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#33258;&#36866;&#24212;&#23545;&#27604;&#23398;&#20064;&#65288;MACL&#65289;&#26694;&#26550;&#65292;&#24182;&#22312;&#20196;&#29260;&#32423;&#21644;&#24207;&#21015;&#32423;&#19978;&#21160;&#24577;&#37319;&#26679;&#36127;&#20363;&#26469;&#35299;&#20915;&#27169;&#22411;&#31616;&#21333;&#25554;&#20837;&#30693;&#35782;&#29255;&#27573;&#23548;&#33268;&#30340;&#36864;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24341;&#23548;&#30340;&#23545;&#35805;&#29983;&#25104;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#26469;&#34917;&#20805;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#32531;&#35299;&#25991;&#26412;&#36864;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#24448;&#24448;&#26080;&#27861;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#26041;&#24335;&#23558;&#27492;&#20449;&#24687;&#20869;&#21270;&#21040;&#22238;&#31572;&#20013;&#12290;&#30456;&#21453;&#65292;&#23427;&#21482;&#26159;&#31616;&#21333;&#22320;&#23558;&#25552;&#20379;&#30340;&#30693;&#35782;&#29255;&#27573;&#25554;&#20837;&#21040;&#26222;&#36890;&#30340;&#22238;&#31572;&#20013;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#30340;&#22238;&#31572;&#24448;&#24448;&#20047;&#21619;&#12289;&#19981;&#36830;&#36143;&#65292;&#24182;&#19988;&#32570;&#20047;&#20114;&#21160;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#36864;&#21270;&#38382;&#39064;&#20173;&#26410;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#65292;&#36825;&#31181;&#22797;&#21046;&#24335;&#36864;&#21270;&#20027;&#35201;&#26159;&#30001;&#20110;&#24369;&#27010;&#29575;&#30446;&#26631;&#36896;&#25104;&#30340;&#65292;&#23427;&#20801;&#35768;&#27169;&#22411;&#36890;&#36807;&#20165;&#22522;&#20110;&#37325;&#21472;&#30340;&#34920;&#38754;&#27169;&#24335;&#21305;&#37197;&#26469;&#8220;&#27450;&#39575;&#8221;&#30446;&#26631;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#33258;&#36866;&#24212;&#23545;&#27604;&#23398;&#20064;&#65288;MACL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20196;&#29260;&#32423;&#21644;&#24207;&#21015;&#32423;&#19978;&#21160;&#24577;&#37319;&#26679;&#36127;&#20363;&#65292;&#24182;&#38543;&#21518;&#24809;&#32602;&#36864;&#21270;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-grounded dialogue generation aims to mitigate the issue of text degeneration by incorporating external knowledge to supplement the context. However, the model often fails to internalize this information into responses in a human-like manner. Instead, it simply inserts segments of the provided knowledge into generic responses. As a result, the generated responses tend to be tedious, incoherent, and in lack of interactivity which means the degeneration problem is still unsolved. In this work, we first find that such copying-style degeneration is primarily due to the weak likelihood objective, which allows the model to "cheat" the objective by merely duplicating knowledge segments in a superficial pattern matching based on overlap. To overcome this challenge, we then propose a Multi-level Adaptive Contrastive Learning (MACL) framework that dynamically samples negative examples and subsequently penalizes degeneration behaviors at both the token-level and sequence-level. Extensive
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08659</link><description>&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#65292;&#24182;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;LoRA&#31934;&#35843;&#20013;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#21516;&#26102;&#24212;&#29992;&#37327;&#21270;&#21644;LoRA&#31934;&#35843;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#35266;&#23519;&#21040;&#23436;&#25972;&#31934;&#35843;&#21644;&#37327;&#21270;&#21152;LoRA&#31934;&#35843;&#26041;&#27861;&#20043;&#38388;&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#19978;&#23384;&#22312;&#19968;&#33268;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoftQ&#65288;LoRA-Fine-Tuning-aware Quantization&#65289;&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#26469;&#36827;&#34892;LoRA&#31934;&#35843;&#12290;&#36825;&#31181;&#21021;&#22987;&#21270;&#20943;&#36731;&#20102;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
&lt;/p&gt;</description></item><item><title>BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07276</link><description>&lt;p&gt;
BioT5&#65306;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#36328;&#27169;&#24577;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07276
&lt;/p&gt;
&lt;p&gt;
BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#29983;&#29289;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#21033;&#29992;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#25972;&#21512;&#26469;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#29983;&#25104;&#26080;&#25928;&#30340;&#20998;&#23376;SMILES&#12289;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21033;&#29992;&#19981;&#36275;&#20197;&#21450;&#23545;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#31561;&#37327;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;BioT5&#65292;&#23427;&#36890;&#36807;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#29983;&#29289;&#23398;&#20013;&#30340;&#36328;&#27169;&#24577;&#25972;&#21512;&#12290;BioT5&#21033;&#29992;SELFIES&#36827;&#34892;100%&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#24182;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#29983;&#29289;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#29289;&#23454;&#20307;&#21608;&#22260;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;BioT5&#21306;&#20998;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20449;&#24687;&#12290;&#22312;&#24494;&#35843;&#21518;&#65292;BioT5&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
&lt;/p&gt;</description></item><item><title>Whispering LLaMA&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#38169;&#35823;&#26657;&#27491;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22768;&#23398;&#20449;&#24687;&#21644;&#22806;&#37096;&#35821;&#35328;&#34920;&#31034;&#65292;&#29983;&#25104;&#20934;&#30830;&#30340;&#35821;&#38899;&#36716;&#24405;&#19978;&#19979;&#25991;&#65292;&#30456;&#23545;&#20110;n-best&#20551;&#35774;&#65292;&#35789;&#38169;&#35823;&#29575;&#24615;&#33021;&#25552;&#21319;&#20102;37.66%&#12290;</title><link>http://arxiv.org/abs/2310.06434</link><description>&lt;p&gt;
Whispering LLaMA&#65306;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#38169;&#35823;&#26657;&#27491;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition. (arXiv:2310.06434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06434
&lt;/p&gt;
&lt;p&gt;
Whispering LLaMA&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#38169;&#35823;&#26657;&#27491;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22768;&#23398;&#20449;&#24687;&#21644;&#22806;&#37096;&#35821;&#35328;&#34920;&#31034;&#65292;&#29983;&#25104;&#20934;&#30830;&#30340;&#35821;&#38899;&#36716;&#24405;&#19978;&#19979;&#25991;&#65292;&#30456;&#23545;&#20110;n-best&#20551;&#35774;&#65292;&#35789;&#38169;&#35823;&#29575;&#24615;&#33021;&#25552;&#21319;&#20102;37.66%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#25216;&#26415;&#65292;&#29992;&#20110;&#29983;&#25104;&#20934;&#30830;&#30340;&#35821;&#38899;&#36716;&#24405;&#19978;&#19979;&#25991;&#65292;&#20197;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035; (ASR) &#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#26657;&#27491;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28789;&#27963;&#36816;&#29992;&#29420;&#29305;&#30340;&#21021;&#22987;&#21270;&#25216;&#26415;&#21644;&#21442;&#25968;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#22411;&#25552;&#21319;&#20102;ASR&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#22810;&#26679;&#21270;&#30340;ASR&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#34701;&#21512;&#25216;&#26415;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22797;&#29616;&#24615;&#65292;&#30456;&#23545;&#20110;n-best&#20551;&#35774;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35789;&#38169;&#35823;&#29575;&#24615;&#33021;&#25552;&#21319;&#20102;37.66%&#12290;&#20026;&#20102;&#40723;&#21169;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#24320;&#28304;&#22312;https://github.com/Srijith-rkr/Whispering-LLaMA&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a fresh paradigm in generative error correction within the realm of n-best hypotheses. Unlike the existing ranking-based rescoring methods, our approach adeptly uses distinct initialization techniques and parameter-efficient algorithms to boost ASR performance derived from pre-trained speech and text models. Through evaluation across diverse ASR datasets, we evaluate the stability and reproducibility of our fusion technique, demonstrating its improved word error rate relative (WERR) performance in comparison to n-best hypotheses by relatively 37.66%. To encourage future research, we have made our code and pre-trained models open source at https://github.com/Srijith-rkr/Whispering-LLaMA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;OpenAI GPT-3.5&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39550;&#39542;&#36712;&#36857;&#65292;&#25552;&#39640;&#20102;&#36816;&#21160;&#35268;&#21010;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01415</link><description>&lt;p&gt;
GPT-Driver: &#20351;&#29992;GPT&#23398;&#20064;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
GPT-Driver: Learning to Drive with GPT. (arXiv:2310.01415v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;OpenAI GPT-3.5&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39550;&#39542;&#36712;&#36857;&#65292;&#25552;&#39640;&#20102;&#36816;&#21160;&#35268;&#21010;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;OpenAI GPT-3.5&#27169;&#22411;&#36716;&#21270;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#21487;&#38752;&#36816;&#21160;&#35268;&#21010;&#22120;&#12290;&#36816;&#21160;&#35268;&#21010;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#26680;&#24515;&#25361;&#25112;&#65292;&#26088;&#22312;&#35268;&#21010;&#19968;&#20010;&#23433;&#20840;&#33298;&#36866;&#30340;&#39550;&#39542;&#36712;&#36857;&#12290;&#29616;&#26377;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#20027;&#35201;&#21033;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#22312;&#38754;&#23545;&#26032;&#39062;&#21644;&#26410;&#30693;&#30340;&#39550;&#39542;&#22330;&#26223;&#26102;&#23637;&#29616;&#20986;&#19981;&#36275;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22266;&#26377;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#21644;&#27867;&#21270;&#28508;&#21147;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#22522;&#26412;&#35265;&#35299;&#26159;&#23558;&#36816;&#21160;&#35268;&#21010;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20043;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#35270;&#35282;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35268;&#21010;&#22120;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#34920;&#31034;&#20026;&#35821;&#35328;&#35760;&#21495;&#65292;&#24182;&#21033;&#29992;LLM&#36890;&#36807;&#23545;&#22352;&#26631;&#30340;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#39550;&#39542;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple yet effective approach that can transform the OpenAI GPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion planning is a core challenge in autonomous driving, aiming to plan a driving trajectory that is safe and comfortable. Existing motion planners predominantly leverage heuristic methods to forecast driving trajectories, yet these approaches demonstrate insufficient generalization capabilities in the face of novel and unseen driving scenarios. In this paper, we propose a novel approach to motion planning that capitalizes on the strong reasoning capabilities and generalization potential inherent to Large Language Models (LLMs). The fundamental insight of our approach is the reformulation of motion planning as a language modeling problem, a perspective not previously explored. Specifically, we represent the planner inputs and outputs as language tokens, and leverage the LLM to generate driving trajectories through a language description of coo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Google Bard&#36825;&#20010;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;Bard&#22312;&#23558;&#35270;&#35273;&#21644;&#35821;&#35328;&#20998;&#26512;&#30456;&#32467;&#21512;&#26041;&#38754;&#20381;&#36182;&#20110;&#23545;&#22270;&#20687;&#36827;&#34892;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#19978;&#26377;&#25361;&#25112;&#30340;&#38382;&#39064;&#20294;&#26080;&#27861;&#20462;&#25913;&#21407;&#22987;&#35270;&#35273;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2309.16705</link><description>&lt;p&gt;
&#35299;&#30721;&#22270;&#20687;&#65306;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding Imagery: Unleashing Large Language Models. (arXiv:2309.16705v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Google Bard&#36825;&#20010;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;Bard&#22312;&#23558;&#35270;&#35273;&#21644;&#35821;&#35328;&#20998;&#26512;&#30456;&#32467;&#21512;&#26041;&#38754;&#20381;&#36182;&#20110;&#23545;&#22270;&#20687;&#36827;&#34892;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#19978;&#26377;&#25361;&#25112;&#30340;&#38382;&#39064;&#20294;&#26080;&#27861;&#20462;&#25913;&#21407;&#22987;&#35270;&#35273;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#25361;&#25112;-&#21709;&#24212;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;Google Bard&#36827;&#34892;&#20102;64&#20010;&#35270;&#35273;&#25361;&#25112;&#65292;&#26088;&#22312;&#25506;&#31350;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#25361;&#25112;&#28085;&#30422;&#20102;&#21508;&#31181;&#31867;&#21035;&#65292;&#21253;&#25324;&#8220;&#35270;&#35273;&#24773;&#22659;&#25512;&#29702;&#8221;&#65292;&#8220;&#35270;&#35273;&#25991;&#26412;&#25512;&#29702;&#8221;&#21644;&#8220;&#19979;&#19968;&#22330;&#26223;&#39044;&#27979;&#8221;&#31561;&#65292;&#20197;&#30830;&#23450;Bard&#22312;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#20998;&#26512;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;Bard&#20542;&#21521;&#20110;&#26681;&#25454;&#22270;&#29255;&#20570;&#20986;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#65292;&#29305;&#21035;&#26159;&#22312;&#30830;&#23450;&#22270;&#29255;&#20013;&#30340;&#32447;&#32034;&#26102;&#12290;&#19982;GPT4&#31561;&#20854;&#20182;&#27169;&#22411;&#19981;&#21516;&#65292;Bard&#20284;&#20046;&#19981;&#20381;&#36182;&#20110;&#20687;Tesseract&#36825;&#26679;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#24211;&#65292;&#32780;&#26159;&#20687;Google Lens&#21644;Visual API&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#35782;&#21035;&#22797;&#26434;&#22270;&#29255;&#20013;&#30340;&#25991;&#26412;&#12290;&#26174;&#30528;&#30340;&#26159;&#65292;Bard&#21487;&#20197;&#36890;&#36807;&#35270;&#35273;&#26041;&#24335;&#35299;&#20915;ChatGPT&#26080;&#27861;&#29702;&#35299;&#30340;&#39564;&#35777;&#30721;&#65292;&#25512;&#33616;&#20351;&#29992;Tesseract&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;Bard&#27169;&#22411;&#22522;&#20110;&#35270;&#35273;&#36755;&#20837;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#26080;&#27861;&#37325;&#24314;&#25110;&#20462;&#25913;&#21407;&#22987;&#30340;&#35270;&#35273;&#23545;&#35937;&#26469;&#25903;&#25345;&#20854;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a challenge-response study, we subjected Google Bard to 64 visual challenges designed to probe multimodal Large Language Models (LLMs). The challenges spanned diverse categories, including "Visual Situational Reasoning," "Visual Text Reasoning," and "Next Scene Prediction," among others, to discern Bard's competence in melding visual and linguistic analyses. Our findings indicate that Bard tends to rely on making educated guesses about visuals, especially when determining cues from images. Unlike other models like GPT4, Bard does not appear to rely on optical character recognition libraries like Tesseract but recognizes text in complex images like deep learning models such as Google Lens and Visual API. Significantly Bard can solve CAPTCHAs visually that ChatGPT fails to understand, recommending Tesseract solutions. Moreover, while the Bard model proposes solutions based on visual input, it cannot recreate or modify the original visual objects to support its conclusions. Bard fails 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25903;&#25345;&#38271;&#19978;&#19979;&#25991;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#23454;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#30740;&#31350;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#30340;&#25351;&#23548;&#35843;&#25972;&#31243;&#24207;&#65292;&#24050;&#32463;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16039</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#26377;&#25928;&#38271;&#19978;&#19979;&#25991;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Effective Long-Context Scaling of Foundation Models. (arXiv:2309.16039v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16039
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25903;&#25345;&#38271;&#19978;&#19979;&#25991;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#23454;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#30740;&#31350;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#30340;&#25351;&#23548;&#35843;&#25972;&#31243;&#24207;&#65292;&#24050;&#32463;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25903;&#25345;&#26368;&#22810;32768&#20010;&#26631;&#35760;&#30340;&#26377;&#25928;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#19978;&#19979;&#25991;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;Llama 2&#36830;&#32493;&#39044;&#35757;&#32451;&#12289;&#22312;&#38271;&#25991;&#26412;&#19978;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26500;&#24314;&#25105;&#20204;&#30340;&#27169;&#22411;&#31995;&#21015;&#12290;&#25105;&#20204;&#23545;&#35821;&#35328;&#24314;&#27169;&#12289;&#21512;&#25104;&#19978;&#19979;&#25991;&#25506;&#27979;&#20219;&#21153;&#21644;&#21508;&#31181;&#30740;&#31350;&#22522;&#20934;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#22312;&#30740;&#31350;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#24120;&#35268;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#30456;&#23545;&#20110;Llama 2&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#25351;&#23548;&#35843;&#25972;&#31243;&#24207;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#30340;&#38271;&#25351;&#23548;&#25968;&#25454;&#65292;70B&#29256;&#26412;&#24050;&#32463;&#22312;&#19968;&#22871;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;gpt-3.5-turbo-16k&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#38500;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;Llama&#30340;&#20301;&#32622;&#32534;&#30721;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#24314;&#27169;&#38271;&#20381;&#36182;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a series of long-context LLMs that support effective context windows of up to 32,768 tokens. Our model series are built through continual pretraining from Llama 2 with longer training sequences and on a dataset where long texts are upsampled. We perform extensive evaluation on language modeling, synthetic context probing tasks, and a wide range of research benchmarks. On research benchmarks, our models achieve consistent improvements on most regular tasks and significant improvements on long-context tasks over Llama 2. Notably, with a cost-effective instruction tuning procedure that does not require human-annotated long instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks. Alongside these results, we provide an in-depth analysis on the individual components of our method. We delve into Llama's position encodings and discuss its limitation in modeling long dependencies. We also examine the impact of var
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12289;&#30524;&#21160;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;&#22823;&#33041;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#22788;&#29702;&#19982;&#20851;&#38190;&#23383;&#30456;&#20851;&#24230;&#19981;&#21516;&#30340;&#21333;&#35789;&#30340;&#31070;&#32463;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#35821;&#20041;&#25512;&#29702;&#38405;&#35835;&#29702;&#35299;&#20013;&#31070;&#32463;&#29366;&#24577;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2309.15714</link><description>&lt;p&gt;
ChatGPT-BCI&#65306;&#20351;&#29992;GPT&#12289;EEG&#21644;&#30524;&#21160;&#29983;&#29289;&#26631;&#35760;&#22120;&#22312;&#35821;&#20041;&#25512;&#29702;&#38405;&#35835;&#29702;&#35299;&#20013;&#36827;&#34892;&#21333;&#35789;&#32423;&#31070;&#32463;&#29366;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-BCI: Word-Level Neural State Classification Using GPT, EEG, and Eye-Tracking Biomarkers in Semantic Inference Reading Comprehension. (arXiv:2309.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12289;&#30524;&#21160;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;&#22823;&#33041;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#22788;&#29702;&#19982;&#20851;&#38190;&#23383;&#30456;&#20851;&#24230;&#19981;&#21516;&#30340;&#21333;&#35789;&#30340;&#31070;&#32463;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#35821;&#20041;&#25512;&#29702;&#38405;&#35835;&#29702;&#35299;&#20013;&#31070;&#32463;&#29366;&#24577;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65288;&#22914;GPT&#65289;&#30340;&#36805;&#29467;&#21457;&#23637;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#29702;&#35299;&#35821;&#20041;&#35821;&#35328;&#24847;&#20041;&#30340;&#33021;&#21147;&#24050;&#32463;&#36827;&#20837;&#20102;&#19968;&#20010;&#26032;&#38454;&#27573;&#12290;&#36825;&#38656;&#35201;&#36328;&#35748;&#30693;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;LLMs&#12289;&#30524;&#21160;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#30740;&#31350;&#22823;&#33041;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#22914;&#20309;&#22788;&#29702;&#19982;&#20851;&#38190;&#23383;&#30456;&#20851;&#31243;&#24230;&#19981;&#21516;&#30340;&#21333;&#35789;&#65292;&#20174;&#32780;&#25552;&#20379;&#20851;&#20110;&#20010;&#20307;&#31070;&#32463;&#29366;&#24577;&#22312;&#35821;&#20041;&#20851;&#31995;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#25913;&#36827;&#20102;&#19982;&#20851;&#38190;&#23383;&#39640;&#30456;&#20851;&#24230;&#21644;&#20302;&#30456;&#20851;&#24230;&#30340;&#21333;&#35789;&#38405;&#35835;&#36807;&#31243;&#20013;&#19982;&#27880;&#35270;&#30456;&#20851;&#30340;EEG&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;&#22312;12&#21517;&#21463;&#35797;&#32773;&#20013;&#65292;&#27492;&#21333;&#35789;&#32423;&#21035;&#20998;&#31867;&#30340;&#26368;&#20339;&#39564;&#35777;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;60&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent explosion of large language models (LLMs), such as Generative Pretrained Transformers (GPT), the need to understand the ability of humans and machines to comprehend semantic language meaning has entered a new phase. This requires interdisciplinary research that bridges the fields of cognitive science and natural language processing (NLP). This pilot study aims to provide insights into individuals' neural states during a semantic relation reading-comprehension task. We propose jointly analyzing LLMs, eye-gaze, and electroencephalographic (EEG) data to study how the brain processes words with varying degrees of relevance to a keyword during reading. We also use a feature engineering approach to improve the fixation-related EEG data classification while participants read words with high versus low relevance to the keyword. The best validation accuracy in this word-level classification is over 60\% across 12 subjects. Words of high relevance to the inference keyword had sig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;wav2vec&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35328;&#35821;&#22833;&#35821;&#30151;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#32423;&#21035;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.14107</link><description>&lt;p&gt;
&#22522;&#20110;Wav2vec&#30340;&#35328;&#35821;&#22833;&#35821;&#30151;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#32423;&#21035;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Wav2vec-based Detection and Severity Level Classification of Dysarthria from Speech. (arXiv:2309.14107v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;wav2vec&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35328;&#35821;&#22833;&#35821;&#30151;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#32423;&#21035;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22768;&#23398;&#35821;&#38899;&#20449;&#21495;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#32423;&#21035;&#20998;&#31867;&#65292;&#21487;&#20197;&#20316;&#20026;&#21307;&#23398;&#35786;&#26029;&#20013;&#30340;&#24037;&#20855;&#29992;&#20110;&#35328;&#35821;&#22833;&#35821;&#30151;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;wav2vec 2.0&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#26500;&#24314;&#20102;&#35328;&#35821;&#22833;&#35821;&#30151;&#30340;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#32423;&#21035;&#20998;&#31867;&#31995;&#32479;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;UA-speech&#25968;&#25454;&#24211;&#12290;&#22312;&#26816;&#27979;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;wav2vec&#27169;&#22411;&#30340;&#31532;&#19968;&#23618;&#23884;&#20837;&#25928;&#26524;&#26368;&#22909;&#65292;&#30456;&#27604;&#20110;&#26368;&#20339;&#22522;&#20934;&#29305;&#24449;&#65288;&#22768;&#35889;&#22270;&#65289;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.23%&#12290;&#22312;&#30740;&#31350;&#30340;&#20005;&#37325;&#31243;&#24230;&#32423;&#21035;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;&#26368;&#32456;&#23618;&#30340;&#23884;&#20837;&#30456;&#27604;&#20110;&#26368;&#20339;&#22522;&#20934;&#29305;&#24449;&#65288;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65289;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;10.62%&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic detection and severity level classification of dysarthria directly from acoustic speech signals can be used as a tool in medical diagnosis. In this work, the pre-trained wav2vec 2.0 model is studied as a feature extractor to build detection and severity level classification systems for dysarthric speech. The experiments were carried out with the popularly used UA-speech database. In the detection experiments, the results revealed that the best performance was obtained using the embeddings from the first layer of the wav2vec model that yielded an absolute improvement of 1.23% in accuracy compared to the best performing baseline feature (spectrogram). In the studied severity level classification task, the results revealed that the embeddings from the final layer gave an absolute improvement of 10.62% in accuracy compared to the best baseline features (mel-frequency cepstral coefficients).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22768;&#38376;&#28304;&#29305;&#24449;&#30340;&#31995;&#32479;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#22768;&#38899;&#30149;&#29702;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22768;&#38376;&#28304;&#21253;&#21547;&#30340;&#20449;&#24687;&#23545;&#20110;&#30149;&#29702;&#24615;&#22768;&#38899;&#30340;&#21028;&#21035;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.14080</link><description>&lt;p&gt;
&#20351;&#29992;&#22768;&#38376;&#28304;&#29305;&#24449;&#36827;&#34892;&#30149;&#29702;&#24615;&#22768;&#38899;&#30340;&#20998;&#26512;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Analysis and Detection of Pathological Voice using Glottal Source Features. (arXiv:2309.14080v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22768;&#38376;&#28304;&#29305;&#24449;&#30340;&#31995;&#32479;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#22768;&#38899;&#30149;&#29702;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22768;&#38376;&#28304;&#21253;&#21547;&#30340;&#20449;&#24687;&#23545;&#20110;&#30149;&#29702;&#24615;&#22768;&#38899;&#30340;&#21028;&#21035;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#22768;&#38899;&#30149;&#29702;&#33021;&#22815;&#23454;&#29616;&#23458;&#35266;&#35780;&#20272;&#21644;&#26089;&#26399;&#24178;&#39044;&#35786;&#26029;&#12290;&#26412;&#30740;&#31350;&#23545;&#22768;&#38376;&#28304;&#29305;&#24449;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#22768;&#38899;&#30149;&#29702;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#20934;&#23553;&#38381;&#30456;&#20301;&#65288;QCP&#65289;&#30340;&#22768;&#38376;&#36870;&#28388;&#27874;&#26041;&#27861;&#20272;&#35745;&#24471;&#21040;&#30340;&#22768;&#38376;&#27969;&#21160;&#12289;&#20351;&#29992;&#38646;&#39057;&#29575;&#28388;&#27874;&#65288;ZFF&#65289;&#26041;&#27861;&#35745;&#31639;&#24471;&#21040;&#30340;&#36817;&#20284;&#22768;&#38376;&#28304;&#20449;&#21495;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;&#22768;&#23398;&#22768;&#38899;&#20449;&#21495;&#26469;&#25552;&#21462;&#22768;&#38376;&#28304;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20174;&#30001;QCP&#21644;ZFF&#35745;&#31639;&#20986;&#30340;&#22768;&#38376;&#28304;&#27874;&#24418;&#20013;&#25512;&#23548;&#20986;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65288;MFCCs&#65289;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#30149;&#29702;&#24615;&#22768;&#38899;&#30340;&#22768;&#38376;&#28304;&#39057;&#35889;&#21464;&#21270;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#24211;&#65292;&#21363;&#21307;&#38498;Universitario Principe de Asturias&#65288;HUPA&#65289;&#25968;&#25454;&#24211;&#21644;Saarbrucken Voice Disorders&#65288;SVD&#65289;&#25968;&#25454;&#24211;&#12290;&#29305;&#24449;&#20998;&#26512;&#25581;&#31034;&#20102;&#22768;&#38376;&#28304;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#30340;&#21028;&#21035;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic detection of voice pathology enables objective assessment and earlier intervention for the diagnosis. This study provides a systematic analysis of glottal source features and investigates their effectiveness in voice pathology detection. Glottal source features are extracted using glottal flows estimated with the quasi-closed phase (QCP) glottal inverse filtering method, using approximate glottal source signals computed with the zero frequency filtering (ZFF) method, and using acoustic voice signals directly. In addition, we propose to derive mel-frequency cepstral coefficients (MFCCs) from the glottal source waveforms computed by QCP and ZFF to effectively capture the variations in glottal source spectra of pathological voice. Experiments were carried out using two databases, the Hospital Universitario Principe de Asturias (HUPA) database and the Saarbrucken Voice Disorders (SVD) database. Analysis of features revealed that the glottal source contains information that discri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(AceGPT)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#26469;&#22521;&#20859;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;AceGPT&#22312;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#26159;&#26368;&#20808;&#36827;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.12053</link><description>&lt;p&gt;
AceGPT&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26412;&#22320;&#21270;&#20026;&#38463;&#25289;&#20271;&#25991;
&lt;/p&gt;
&lt;p&gt;
AceGPT, Localizing Large Language Models in Arabic. (arXiv:2309.12053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(AceGPT)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#26469;&#22521;&#20859;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;AceGPT&#22312;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#26159;&#26368;&#20808;&#36827;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21457;&#36866;&#29992;&#20110;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36843;&#20999;&#38656;&#27714;&#21644;&#26041;&#27861;&#35770;&#65292;&#38463;&#25289;&#20271;&#25991;&#20855;&#26377;&#29420;&#29305;&#30340;&#25991;&#21270;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#30446;&#21069;&#30340;&#20027;&#27969;&#27169;&#22411;&#22914;ChatGPT&#24182;&#26410;&#20805;&#20998;&#35299;&#20915;&#12290;&#22312;&#32771;&#34385;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#26412;&#22320;&#20215;&#20540;&#35266;&#26102;&#36824;&#23384;&#22312;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25171;&#21253;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#36827;&#19968;&#27493;&#20351;&#29992;&#38463;&#25289;&#20271;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#12289;&#20351;&#29992;&#26412;&#22320;&#38463;&#25289;&#20271;&#25351;&#20196;&#21644;&#38463;&#25289;&#20271;&#35821;GPT-4&#22238;&#24212;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;(SFT)&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#26412;&#22320;&#25991;&#21270;&#21644;&#20215;&#20540;&#35266;&#25935;&#24863;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;(RLAIF)&#12290;&#30446;&#26631;&#26159;&#35757;&#32451;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#19982;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;LLM&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#22810;&#26679;&#21270;&#30340;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#24471;&#21040;&#30340;&#21517;&#20026;AceGPT&#30340;&#38463;&#25289;&#20271;&#25991;LLM&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#26159;&#26368;&#20808;&#36827;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the imperative need and methodology for developing a localized Large Language Model (LLM) tailored for Arabic, a language with unique cultural characteristics that are not adequately addressed by current mainstream models like ChatGPT. Key concerns additionally arise when considering cultural sensitivity and local values. To this end, the paper outlines a packaged solution, including further pre-training with Arabic texts, supervised fine-tuning (SFT) using native Arabic instructions and GPT-4 responses in Arabic, and reinforcement learning with AI feedback (RLAIF) using a reward model that is sensitive to local culture and values. The objective is to train culturally aware and value-aligned Arabic LLMs that can serve the diverse application-specific needs of Arabic-speaking communities.  Extensive evaluations demonstrated that the resulting LLM called `\textbf{AceGPT}' is the SOTA open Arabic LLM in various benchmarks, including instruction-following benchmark (i.e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;fine-tuned&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#20010;&#24615;&#21270;&#29983;&#25104;&#20840;&#36523;PET&#25253;&#21578;&#30340;&#20934;&#30830;&#21360;&#35937;&#12290;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#24341;&#20837;&#38405;&#35835;&#21307;&#29983;&#30340;&#36523;&#20221;&#20449;&#24687;&#65292;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21307;&#29983;&#29305;&#23450;&#30340;&#25253;&#21578;&#39118;&#26684;&#12290;&#30740;&#31350;&#32467;&#26524;&#32463;&#36807;&#19987;&#23478;&#35780;&#20272;&#21644;&#26680;&#21307;&#23398;&#21307;&#29983;&#30340;&#36136;&#37327;&#35780;&#20998;&#35748;&#21487;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10066</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#20010;&#24615;&#21270;&#21360;&#35937;&#29983;&#25104;PET&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Automatic Personalized Impression Generation for PET Reports Using Large Language Models. (arXiv:2309.10066v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;fine-tuned&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#20010;&#24615;&#21270;&#29983;&#25104;&#20840;&#36523;PET&#25253;&#21578;&#30340;&#20934;&#30830;&#21360;&#35937;&#12290;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#24341;&#20837;&#38405;&#35835;&#21307;&#29983;&#30340;&#36523;&#20221;&#20449;&#24687;&#65292;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21307;&#29983;&#29305;&#23450;&#30340;&#25253;&#21578;&#39118;&#26684;&#12290;&#30740;&#31350;&#32467;&#26524;&#32463;&#36807;&#19987;&#23478;&#35780;&#20272;&#21644;&#26680;&#21307;&#23398;&#21307;&#29983;&#30340;&#36136;&#37327;&#35780;&#20998;&#35748;&#21487;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#30830;&#23450;&#36890;&#36807;fine-tuned&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#21487;&#20197;&#20026;&#20840;&#36523;PET&#25253;&#21578;&#29983;&#25104;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#21360;&#35937;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#20351;&#29992;teacher-forcing&#31639;&#27861;&#22312;PET&#25253;&#21578;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;12&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#36755;&#20837;&#26159;&#25253;&#21578;&#21457;&#29616;&#65292;&#21442;&#32771;&#26159;&#20020;&#24202;&#21360;&#35937;&#12290;&#39069;&#22806;&#30340;&#36755;&#20837;&#26631;&#35760;&#32534;&#30721;&#20102;&#38405;&#35835;&#21307;&#29983;&#30340;&#36523;&#20221;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21307;&#29983;&#29305;&#23450;&#30340;&#25253;&#21578;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324;2010&#24180;&#33267;2022&#24180;&#38388;&#20174;&#25105;&#20204;&#26426;&#26500;&#25910;&#38598;&#30340;37,370&#20221;&#22238;&#39038;&#24615;PET&#25253;&#21578;&#12290;&#36890;&#36807;&#19982;&#20004;&#21517;&#26680;&#21307;&#23398;&#65288;NM&#65289;&#21307;&#29983;&#30340;&#36136;&#37327;&#35780;&#20998;&#36827;&#34892;30&#20010;&#35780;&#20272;&#25351;&#26631;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26368;&#21305;&#37197;&#30340;&#25351;&#26631;&#36873;&#25321;&#20102;&#29992;&#20110;&#19987;&#23478;&#35780;&#20272;&#30340;&#27169;&#22411;&#12290;&#22312;&#37096;&#20998;&#25968;&#25454;&#23376;&#38598;&#20013;&#65292;&#26681;&#25454;6&#20010;&#36136;&#37327;&#32500;&#24230;&#21644;&#19968;&#20010;&#24635;&#20307;&#23454;&#29992;&#24615;&#35780;&#20998;&#65288;5&#20998;&#21046;&#65289;&#65292;&#19977;&#21517;&#26680;&#21307;&#23398;&#21307;&#29983;&#35780;&#20272;&#20102;&#27169;&#22411;&#29983;&#25104;&#30340;&#21360;&#35937;&#21644;&#21407;&#22987;&#20020;&#24202;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To determine if fine-tuned large language models (LLMs) can generate accurate, personalized impressions for whole-body PET reports. Materials and Methods: Twelve language models were trained on a corpus of PET reports using the teacher-forcing algorithm, with the report findings as input and the clinical impressions as reference. An extra input token encodes the reading physician's identity, allowing models to learn physician-specific reporting styles. Our corpus comprised 37,370 retrospective PET reports collected from our institution between 2010 and 2022. To identify the best LLM, 30 evaluation metrics were benchmarked against quality scores from two nuclear medicine (NM) physicians, with the most aligned metrics selecting the model for expert evaluation. In a subset of data, model-generated impressions and original clinical impressions were assessed by three NM physicians according to 6 quality dimensions and an overall utility score (5-point scale). Each physician reviewe
&lt;/p&gt;</description></item><item><title>Prefix-diffusion&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#27880;&#20837;&#21069;&#32512;&#22270;&#20687;&#23884;&#20837;&#26469;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39069;&#22806;&#30340;&#26144;&#23556;&#32593;&#32476;&#26469;&#20943;&#23569;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04965</link><description>&lt;p&gt;
&#21069;&#32512;&#25193;&#25955;&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#26679;&#21270;&#22270;&#20687;&#23383;&#24149;&#30340;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image Captioning. (arXiv:2309.04965v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04965
&lt;/p&gt;
&lt;p&gt;
Prefix-diffusion&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#27880;&#20837;&#21069;&#32512;&#22270;&#20687;&#23884;&#20837;&#26469;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39069;&#22806;&#30340;&#26144;&#23556;&#32593;&#32476;&#26469;&#20943;&#23569;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#30340;&#23383;&#24149;&#30340;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#21442;&#25968;&#35268;&#27169;&#36739;&#22823;&#20173;&#28982;&#26159;&#36825;&#20123;&#31995;&#32479;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#32593;&#32476;&#65292;&#32467;&#21512;&#20102;&#36830;&#32493;&#25193;&#25955;&#65292;&#31216;&#20026;&#21069;&#32512;&#25193;&#25955;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#21069;&#32512;&#22270;&#20687;&#23884;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#20943;&#23569;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#39069;&#22806;&#30340;&#26144;&#23556;&#32593;&#32476;&#12290;&#21069;&#32512;&#25193;&#25955;&#33021;&#22815;&#20197;&#30456;&#23545;&#36739;&#23569;&#30340;&#21442;&#25968;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#20445;&#25345;&#23383;&#24149;&#30340;&#27969;&#30021;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#25193;&#23637;&#22270;&#20687;&#23383;&#24149;&#30340;&#25193;&#25955;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24182;&#19982;&#26368;&#36817;&#30340;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While impressive performance has been achieved in image captioning, the limited diversity of the generated captions and the large parameter scale remain major barriers to the real-word application of these systems. In this work, we propose a lightweight image captioning network in combination with continuous diffusion, called Prefix-diffusion. To achieve diversity, we design an efficient method that injects prefix image embeddings into the denoising process of the diffusion model. In order to reduce trainable parameters, we employ a pre-trained model to extract image features and further design an extra mapping network. Prefix-diffusion is able to generate diverse captions with relatively less parameters, while maintaining the fluency and relevance of the captions benefiting from the generative capabilities of the diffusion model. Our work paves the way for scaling up diffusion models for image captioning, and achieves promising performance compared with recent approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#21644;&#40065;&#26834;&#65292;&#28389;&#29992;API&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#12290;&#36825;&#23545;&#21021;&#32423;&#24320;&#21457;&#32773;&#26469;&#35828;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;&#20195;&#30721;&#20013;&#30340;API&#28389;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.10335</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#30721;&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Robustness and Reliability of Large Language Model Code Generation. (arXiv:2308.10335v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#21644;&#40065;&#26834;&#65292;&#28389;&#29992;API&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#12290;&#36825;&#23545;&#21021;&#32423;&#24320;&#21457;&#32773;&#26469;&#35828;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;&#20195;&#30721;&#20013;&#30340;API&#28389;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#21644;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#38750;&#20961;&#33021;&#21147;&#12290;&#24403;&#36935;&#21040;&#32534;&#30721;&#38382;&#39064;&#26102;&#65292;&#36719;&#20214;&#24037;&#31243;&#24072;&#24120;&#24120;&#20250;&#21672;&#35810;LLMs&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#19968;&#20123;&#21162;&#21147;&#26469;&#36991;&#20813;&#35821;&#27861;&#38169;&#35823;&#24182;&#20351;&#20195;&#30721;&#19982;&#39044;&#26399;&#30340;&#35821;&#20041;&#23545;&#40784;&#65292;&#20294;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#23578;&#26410;&#34987;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#29615;&#22659;&#20013;&#65292;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#31561;&#21516;&#20110;&#21487;&#38752;&#21644;&#40065;&#26834;&#30340;&#20195;&#30721;&#12290;&#22312;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#28389;&#29992;API&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#22914;&#36164;&#28304;&#27844;&#28431;&#12289;&#31243;&#24207;&#23849;&#28291;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;LLM&#20195;&#30721;&#29983;&#25104;&#26381;&#21153;&#30340;&#29992;&#25143;&#23454;&#38469;&#19978;&#26159;&#26368;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#30475;&#20284;&#27491;&#30830;&#30340;&#20195;&#30721;&#24433;&#21709;&#30340;&#24320;&#21457;&#32773;&#8212;&#8212;&#20182;&#20204;&#36890;&#24120;&#26159;&#19981;&#29087;&#24713;LLMs&#20026;&#20182;&#20204;&#29983;&#25104;&#20195;&#30721;&#30340;API&#30340;&#21021;&#32423;&#24320;&#21457;&#32773;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;API&#30340;&#28389;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the large language models (LLMs) have shown extraordinary ability in understanding natural language and generating programming code. It has been a common practice of software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability and robustness of the code generationfrom LLMs have not yet been thoroughly studied. The executable code is not equivalent to the reliable and robust code, especially in the context of real-world software development. The misuse of APIs in the generated code could lead to severe problem, such as resource leaks, program crashes. To make things worse, the users of LLM code generation services are actually the developers that are most vulnerable to these code that seems right -- They are always novice developers that are not familiar with the APIs that LLMs generate code for them. Therefore, they could hardly tell the misuse in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#35813;&#22871;&#20214;&#30001;200&#20010;&#23433;&#20840;&#25552;&#31034;&#32452;&#25104;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#26088;&#22312;&#24341;&#20986;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01263</link><description>&lt;p&gt;
XSTest: &#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#23433;&#20840;&#34892;&#20026;&#30340;&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. (arXiv:2308.01263v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#35813;&#22871;&#20214;&#30001;200&#20010;&#23433;&#20840;&#25552;&#31034;&#32452;&#25104;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#26088;&#22312;&#24341;&#20986;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#36866;&#24403;&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24456;&#23481;&#26131;&#36981;&#24490;&#24694;&#24847;&#25351;&#20196;&#24182;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#36825;&#28608;&#21457;&#20102;&#23433;&#20840;&#24037;&#20316;&#65292;&#22914;&#32418;&#38431;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#21453;&#39304;&#23398;&#20064;&#65292;&#26088;&#22312;&#20351;&#27169;&#22411;&#26082;&#26377;&#29992;&#21448;&#26080;&#23475;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#32039;&#24352;&#20851;&#31995;&#65292;&#22240;&#20026;&#26080;&#23475;&#24615;&#35201;&#27714;&#27169;&#22411;&#25298;&#32477;&#36981;&#20174;&#19981;&#23433;&#20840;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26080;&#27861;&#25552;&#20379;&#24110;&#21161;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#21487;&#33021;&#22312;&#24179;&#34913;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#20197;&#33267;&#20110;&#21363;&#20351;&#20351;&#29992;&#31867;&#20284;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#35821;&#35328;&#25110;&#25552;&#21450;&#25935;&#24863;&#20027;&#39064;&#30340;&#26126;&#26174;&#23433;&#20840;&#25552;&#31034;&#20063;&#20250;&#34987;&#25298;&#32477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#26032;&#27979;&#35797;&#22871;&#20214;&#65292;&#20197;&#31995;&#32479;&#21270;&#21644;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35782;&#21035;&#36825;&#31181;&#22840;&#24352;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#30446;&#21069;&#65292;XSTest&#21253;&#25324;200&#20010;&#23433;&#20840;&#25552;&#31034;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#33391;&#22909;&#26657;&#20934;&#30340;&#27169;&#22411;&#19981;&#24212;&#35813;&#25298;&#32477;&#36981;&#24490;&#36825;&#20123;&#25552;&#31034;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;XSTest&#30340;&#21019;&#24314;&#21644;&#32452;&#25104;&#65292;&#24182;&#20351;&#29992;&#27979;&#35797;&#22871;&#20214;&#31361;&#26174;&#31995;&#32479;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse complying with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a structured and systematic way. In its current form, XSTest comprises 200 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with. We describe XSTest's creation and composition, and use the test suite to highlight systematic f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10236</link><description>&lt;p&gt;
&#19977;&#24605;&#32780;&#21518;&#34892;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models. (arXiv:2307.10236v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#24615;&#33021;&#31361;&#30772;&#20026;&#20247;&#22810;&#24037;&#19994;&#24212;&#29992;&#21644;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#38169;&#35823;&#29983;&#25104;&#65292;&#22914;&#34394;&#20551;&#39044;&#27979;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#24187;&#35273;&#65292;&#20063;&#24341;&#21457;&#20102;&#23545;LLMs&#21487;&#38752;&#24615;&#30340;&#20005;&#37325;&#20851;&#27880;&#65292;&#23588;&#20854;&#22312;&#23545;&#23433;&#20840;&#12289;&#21487;&#38752;&#24615;&#26377;&#25935;&#24863;&#30340;&#22330;&#26223;&#20013;&#65292;&#21487;&#33021;&#38459;&#30861;&#20854;&#22312;&#23454;&#38469;&#20013;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24050;&#32463;&#26174;&#31034;&#20986;&#20854;&#22312;&#35299;&#37322;&#19968;&#33324;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#39118;&#38505;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20294;&#20851;&#20110;&#23427;&#26159;&#21542;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26377;&#21161;&#20110;&#25506;&#32034;LLMs&#30340;&#33021;&#21147;&#21644;&#25269;&#21046;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#24320;&#23637;&#20102;&#20851;&#20110;LLMs&#39118;&#38505;&#35780;&#20272;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;12&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;4&#20010;LLMs&#22312;4&#20010;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;&#19981;&#30830;&#23450;&#24615;&#22312;&#25506;&#32034;LLMs&#33021;&#21147;&#21644;&#23545;&#25239;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent unc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.17181</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#26080;&#30417;&#30563;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29992;&#20110;&#25991;&#26412;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#21512;&#25104;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#31454;&#20105;&#26469;&#21019;&#24314;&#36924;&#30495;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;GAN&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#12290;&#22240;&#20026;&#33258;&#28982;&#35821;&#35328;&#30001;&#31163;&#25955;&#30340;&#26631;&#35760;&#32452;&#25104;&#65292;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#36935;&#21040;&#22256;&#38590;&#65307;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#25991;&#26412;-GAN&#30740;&#31350;&#20351;&#29992;&#22870;&#21169;&#31995;&#32479;&#20197;&#38543;&#26426;&#26631;&#35760;&#20026;&#22522;&#30784;&#29983;&#25104;&#21477;&#23376;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#30340;&#29983;&#25104;&#22120;&#22312;&#23545;&#25239;&#35757;&#32451;&#20043;&#21069;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23548;&#33268;&#21512;&#25104;&#30340;&#21477;&#23376;&#37325;&#22797;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#21407;&#22987;GAN&#30340;&#26694;&#26550;&#26469;&#21512;&#25104;&#21477;&#23376;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;TESGAN&#65289;&#65292;&#23427;&#29983;&#25104;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#26469;&#35299;&#20915;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;REFLECT&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22833;&#36133;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.15724</link><description>&lt;p&gt;
REFLECT:&#23545;&#26426;&#22120;&#20154;&#32463;&#21382;&#36827;&#34892;&#24635;&#32467;&#65292;&#20197;&#29992;&#20110;&#22833;&#36133;&#35299;&#37322;&#21644;&#32416;&#27491;
&lt;/p&gt;
&lt;p&gt;
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;REFLECT&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22833;&#36133;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#22833;&#36133;&#25191;&#34892;&#26159;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#31283;&#20581;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#36755;&#20837;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#21033;&#29992;LLM&#30340;&#21147;&#37327;&#36827;&#34892;&#26426;&#22120;&#20154;&#22833;&#36133;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;REFLECT&#65292;&#23558;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#36807;&#21435;&#32463;&#39564;&#30340;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#36880;&#27493;&#22833;&#36133;&#35299;&#37322;&#31639;&#27861;&#26597;&#35810;LLM&#12290;&#22522;&#20110;&#35299;&#37322;&#65292;&#22833;&#36133;&#32416;&#27491;&#35268;&#21010;&#22120;&#29983;&#25104;&#19968;&#20010;&#21487;&#25191;&#34892;&#35745;&#21010;&#65292;&#20197;&#32416;&#27491;&#22833;&#36133;&#24182;&#23436;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;RoboFail&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#20174;&#32780;&#24110;&#21161;&#25104;&#21151;&#30340;&#32416;&#27491;&#35268;&#21010;&#12290;&#39033;&#30446;&#32593;&#31449;&#65306;https://roboreflect.github.io/
&lt;/p&gt;
&lt;p&gt;
The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong common sense reasoning skills on textual inputs. To leverage the power of LLM for robot failure explanation, we propose a framework REFLECT, which converts multi-sensory data into a hierarchical summary of robot past experiences and queries LLM with a progressive failure explanation algorithm. Conditioned on the explanation, a failure correction planner generates an executable plan for the robot to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset and show that our LLM-based framework is able to generate informative failure explanations that assist successful correction planning. Project website: https://roboreflect.github.io/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#22810;&#26679;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#24212;&#29992;&#36827;&#23637;&#32531;&#24930;&#65292;LLMs&#36824;&#27809;&#26377;&#30495;&#27491;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.10070</link><description>&lt;p&gt;
ChatGPT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health. (arXiv:2306.10070v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#22810;&#26679;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#24212;&#29992;&#36827;&#23637;&#32531;&#24930;&#65292;LLMs&#36824;&#27809;&#26377;&#30495;&#27491;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#20844;&#20247;&#21644;&#39046;&#22495;&#19987;&#23478;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#20135;&#29983;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#22810;&#26679;&#24212;&#29992;&#65292;&#20855;&#20307;&#25506;&#35752;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#12289;&#21307;&#30103;&#25991;&#26412;&#25688;&#35201;&#12289;&#20449;&#24687;&#25277;&#21462;&#21644;&#21307;&#23398;&#25945;&#32946;&#31561;&#39046;&#22495;&#65292;&#24182;&#30740;&#31350;LLMs&#26159;&#21542;&#20855;&#26377;&#30495;&#27491;&#30340;&#36716;&#22411;&#21147;&#37327;&#20197;&#24443;&#24213;&#25913;&#21464;&#36825;&#20123;&#20219;&#21153;&#25110;&#32773;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29420;&#29305;&#22797;&#26434;&#24615;&#26159;&#21542;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25991;&#29486;&#35843;&#30740;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#23545;&#20110;&#20854;&#20182;&#24212;&#29992;&#65292;&#36827;&#23637;&#36824;&#27604;&#36739;&#32531;&#24930;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;LLMs&#36824;&#27809;&#26377;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has drawn considerable attention from both the general public and domain experts with its remarkable text generation capabilities. This has subsequently led to the emergence of diverse applications in the field of biomedicine and health. In this work, we examine the diverse applications of large language models (LLMs), such as ChatGPT, in biomedicine and health. Specifically we explore the areas of biomedical information retrieval, question answering, medical text summarization, information extraction, and medical education, and investigate whether LLMs possess the transformative power to revolutionize these tasks or whether the distinct complexities of biomedical domain presents unique challenges. Following an extensive literature survey, we find that significant advances have been made in the field of text generation tasks, surpassing the previous state-of-the-art methods. For other applications, the advances have been modest. Overall, LLMs have not yet revolutionized the bio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09896</link><description>&lt;p&gt;
&#25581;&#31192; GPT &#33258;&#25105;&#20462;&#22797;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#19978;&#20173;&#38754;&#20020;&#22256;&#38590;&#12290;&#33258;&#25105;&#20462;&#22797;&#8212;&#8212;&#21363;&#27169;&#22411;&#35843;&#35797;&#24182;&#20462;&#22797;&#33258;&#24049;&#30340;&#20195;&#30721;&#8212;&#8212;&#26368;&#36817;&#25104;&#20026;&#25552;&#39640;&#24615;&#33021;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33258;&#25105;&#20462;&#22797;&#22914;&#20309;&#26377;&#25928;&#22320;&#21457;&#25381;&#20316;&#29992;&#30340;&#30740;&#31350;&#36824;&#38750;&#24120;&#26377;&#38480;&#12290;&#26377;&#20154;&#20250;&#24819;&#30693;&#36947;&#65292;&#24403;&#21516;&#19968;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#26102;&#65292;&#27169;&#22411;&#31350;&#31455;&#33021;&#21542;&#25552;&#20379;&#20934;&#30830;&#30340;&#21453;&#39304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22810;&#31181;&#32534;&#30721;&#25361;&#25112;&#32452;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#31574;&#30053; pass@t&#65292;&#35813;&#31574;&#30053;&#34913;&#37327;&#20102;&#20219;&#21153;&#36890;&#36807;&#29575;&#19982;&#20174;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#24635;&#26631;&#35760;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20165;&#22522;&#20110;&#25277;&#26679;&#30340;&#26041;&#27861;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#35780;&#20272;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#24433;&#21709;&#33258;&#25105;&#20462;&#22797;&#34920;&#29616;&#30340;&#20960;&#20010;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36755;&#20837;&#22122;&#22768;&#36739;&#23569;&#19988;&#27169;&#22411;&#23545;&#21021;&#22987;&#36755;&#20986;&#19981;&#22826;&#33258;&#20449;&#30340;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#65292;&#33258;&#25105;&#20462;&#22797;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#21453;&#39304;&#26469;&#22686;&#24378; GPT &#27169;&#22411;&#30340;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#65292;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#21313;&#20998;&#26377;&#38480;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19555</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#33021;&#20316;&#20026;&#25277;&#35937;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Not Abstract Reasoners. (arXiv:2305.19555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#21313;&#20998;&#26377;&#38480;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25991;&#26412;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25104;&#21151;&#30340;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#65292;LLMs&#26159;&#21542;&#33021;&#22815;&#36798;&#21040;&#20154;&#31867;&#30340;&#35748;&#30693;&#33021;&#21147;&#25110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36824;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#23616;&#38480;&#24615;&#20063;&#19981;&#30830;&#23450;&#12290;&#25277;&#35937;&#25512;&#29702;&#26159;&#35748;&#30693;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#25214;&#21040;&#21644;&#24212;&#29992;&#19968;&#33324;&#27169;&#24335;&#12290;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32467;&#26500;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#21487;&#20197;&#25581;&#31034;&#23427;&#20204;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#28508;&#22312;&#23616;&#38480;&#24615;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30446;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#23545;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#25506;&#31350;&#20102;&#36896;&#25104;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown tremendous performance on a large variety of natural language processing tasks, ranging from text comprehension to common sense reasoning. However, the mechanisms responsible for this success remain unknown, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally limited. Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data. Evaluating deep neural architectures on this task could give insight into their potential limitations regarding reasoning and their broad generalisation abilities, yet this is currently an under-explored area. In this paper, we perform extensive evaluations of state-of-the-art LLMs on abstract reasoning tasks, showing that they achieve very limited performance in contrast with other natural language tasks, and we investigate the reasons for this difference. We apply techniques that have been show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#36827;&#34892;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#24615;&#33021;&#19981;&#19968;&#33268;&#65292;&#24378;&#35843;&#20102;&#20998;&#31867;&#21644;&#29983;&#25104;&#22330;&#26223;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.15040</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Natural Language Generation. (arXiv:2305.15040v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#36827;&#34892;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#24615;&#33021;&#19981;&#19968;&#33268;&#65292;&#24378;&#35843;&#20102;&#20998;&#31867;&#21644;&#29983;&#25104;&#22330;&#26223;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#39046;&#22495;&#30001;&#20110;&#25163;&#21160;&#26631;&#27880;&#28041;&#21450;&#30340;&#36807;&#31243;&#26497;&#20854;&#26114;&#36149;&#19988;&#32791;&#26102;&#65292;&#23548;&#33268;&#26631;&#35760;&#25968;&#25454;&#20005;&#37325;&#21294;&#20047;&#12290;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#27861;&#26159;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#31034;&#20363;&#36827;&#34892;&#26631;&#35760;&#26469;&#25552;&#39640;&#27880;&#37322;&#25928;&#29575;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22312;&#25991;&#26412;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#65292;AL&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20854;&#22312;NLG&#20013;&#30340;&#24212;&#29992;&#36824;&#27809;&#26377;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;NLG&#36827;&#34892;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#32771;&#34385;&#20102;&#22810;&#31181;&#20219;&#21153;&#21644;&#22810;&#31181;&#20027;&#35201;&#36873;&#25321;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#24378;&#22823;&#30340;&#25351;&#23548;&#35843;&#20248;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;AL&#31574;&#30053;&#30340;&#24615;&#33021;&#19981;&#19968;&#33268;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36807;&#20102;&#38543;&#26426;&#31034;&#20363;&#36873;&#25321;&#30340;&#22522;&#20934;&#32447;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#27809;&#26377;&#36798;&#21040;&#35813;&#22522;&#20934;&#32447;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20998;&#31867;&#21644;&#29983;&#25104;&#22330;&#26223;&#20043;&#38388;&#30340;&#19968;&#20123;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Natural Language Generation (NLG) suffers from a severe shortage of labeled data due to the extremely expensive and time-consuming process involved in manual annotation. A natural approach for coping with this problem is active learning (AL), a well-known machine learning technique for improving annotation efficiency by selectively choosing the most informative examples to label. However, while AL has been well-researched in the context of text classification, its application to NLG remains largely unexplored. In this paper, we present a first systematic study of active learning for NLG, considering a diverse set of tasks and multiple leading selection strategies, and harnessing a strong instruction-tuned model. Our results indicate that the performance of existing AL strategies is inconsistent, surpassing the baseline of random example selection in some cases but not in others. We highlight some notable differences between the classification and generation scenarios, and 
&lt;/p&gt;</description></item><item><title>Sophia&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#65292;&#24182;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#25511;&#21046;&#26356;&#26032;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.14342</link><description>&lt;p&gt;
Sophia&#65306;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#38543;&#26426;&#20108;&#38454;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. (arXiv:2305.14342v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14342
&lt;/p&gt;
&lt;p&gt;
Sophia&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#65292;&#24182;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#25511;&#21046;&#26356;&#26032;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#24040;&#22823;&#25104;&#26412;&#65292;&#20248;&#21270;&#31639;&#27861;&#30340;&#24494;&#23567;&#25913;&#36827;&#23558;&#20250;&#22823;&#22823;&#38477;&#20302;&#35757;&#32451;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;Adam&#21450;&#20854;&#21464;&#31181;&#19968;&#30452;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#20108;&#38454;&#65288;&#22522;&#20110;Hessian&#30340;&#65289;&#20248;&#21270;&#22120;&#24448;&#24448;&#20250;&#24102;&#26469;&#22826;&#22810;&#30340;&#27599;&#27493;&#24320;&#38144;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sophia&#65292;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#23427;&#20351;&#29992;&#36731;&#37327;&#32423;&#20272;&#35745;&#30340;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#12290;&#26356;&#26032;&#27493;&#39588;&#26159;&#26799;&#24230;&#30340;&#31227;&#21160;&#24179;&#22343;&#20540;&#38500;&#20197;&#20272;&#35745;Hessian&#30340;&#31227;&#21160;&#24179;&#22343;&#20540;&#65292;&#28982;&#21518;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#12290;&#35009;&#21098;&#25511;&#21046;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26356;&#26032;&#22823;&#23567;&#65292;&#24182;&#25511;&#21046;&#20102;Hessian&#22312;&#36712;&#36857;&#19978;&#30340;&#38750;&#20984;&#24615;&#21644;&#24555;&#36895;&#21464;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;Sophia&#21482;&#22312;&#27599;&#20960;&#27425;&#36845;&#20195;&#20013;&#20272;&#35745;&#23545;&#35282;Hessian&#65292;&#36825;&#20960;&#20046;&#27809;&#26377;&#24179;&#22343;&#27599;&#27493;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;&#22312;&#20351;&#29992;GPT m&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#26102;&#65292;
&lt;/p&gt;
&lt;p&gt;
Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#25104;&#23545;&#20934;&#30830;&#24615;&#21644;&#20851;&#32852;&#24615;&#26657;&#20934;&#30340;&#26041;&#27861;&#23545;&#29616;&#20195;&#25351;&#26631;&#36827;&#34892;&#20803;&#35780;&#20272;&#65292;&#20197;&#26356;&#20844;&#24179;&#22320;&#35780;&#20272;&#25351;&#26631;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14324</link><description>&lt;p&gt;
Ties Matter: &#29992;&#25104;&#23545;&#20934;&#30830;&#24615;&#21644;&#20851;&#32852;&#24615;&#26657;&#20934;&#20803;&#35780;&#20272;&#29616;&#20195;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration. (arXiv:2305.14324v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14324
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#25104;&#23545;&#20934;&#30830;&#24615;&#21644;&#20851;&#32852;&#24615;&#26657;&#20934;&#30340;&#26041;&#27861;&#23545;&#29616;&#20195;&#25351;&#26631;&#36827;&#34892;&#20803;&#35780;&#20272;&#65292;&#20197;&#26356;&#20844;&#24179;&#22320;&#35780;&#20272;&#25351;&#26631;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kendall's tau&#32463;&#24120;&#34987;&#29992;&#26469;&#20803;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#23545;&#20010;&#21035;&#32763;&#35793;&#30340;&#35780;&#20998;&#12290;&#23427;&#23545;&#25104;&#23545;&#20998;&#25968;&#27604;&#36739;&#30340;&#37325;&#28857;&#24456;&#30452;&#35266;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#22914;&#20309;&#22788;&#29702;&#20851;&#32852;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#39046;&#22495;&#65292;&#28608;&#21457;&#20102;&#25991;&#29486;&#20013;&#19981;&#21516;&#21464;&#31181;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#29616;&#20195;&#26426;&#22120;&#32763;&#35793;&#20803;&#35780;&#20272;&#31561;&#29615;&#22659;&#20013;&#65292;&#29616;&#26377;&#30340;&#21464;&#31181;&#30001;&#20110;&#20854;&#23545;&#20851;&#32852;&#24615;&#30340;&#22788;&#29702;&#32780;&#23384;&#22312;&#32570;&#38519;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#21487;&#20197;&#34987;&#25805;&#32437;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#19968;&#31181;&#25104;&#23545;&#20934;&#30830;&#24615;&#30340;&#29256;&#26412;&#23545;&#25351;&#26631;&#36827;&#34892;&#20803;&#35780;&#20272;&#65292;&#35813;&#29256;&#26412;&#32473;&#20104;&#25351;&#26631;&#27491;&#30830;&#39044;&#27979;&#20851;&#32852;&#24615;&#30340;&#20449;&#29992;&#65292;&#24182;&#32467;&#21512;&#33258;&#21160;&#24341;&#20837;&#20851;&#32852;&#24615;&#21040;&#25351;&#26631;&#35780;&#20998;&#30340;&#20851;&#32852;&#24615;&#26657;&#20934;&#36807;&#31243;&#65292;&#20351;&#24471;&#21487;&#20197;&#20844;&#24179;&#27604;&#36739;&#37027;&#20123;&#39044;&#27979;&#21644;&#19981;&#39044;&#27979;&#20851;&#32852;&#24615;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#35770;&#35777;&#24182;&#25552;&#20379;&#23454;&#39564;&#35777;&#25454;&#65292;&#34920;&#26126;&#36825;&#20123;&#25913;&#21160;&#21487;&#20197;&#26356;&#20844;&#24179;&#22320;&#35780;&#20272;&#25351;&#26631;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kendall's tau is frequently used to meta-evaluate how well machine translation (MT) evaluation metrics score individual translations. Its focus on pairwise score comparisons is intuitive but raises the question of how ties should be handled, a gray area that has motivated different variants in the literature. We demonstrate that, in settings like modern MT meta-evaluation, existing variants have weaknesses arising from their handling of ties, and in some situations can even be gamed. We propose instead to meta-evaluate metrics with a version of pairwise accuracy that gives metrics credit for correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores, enabling fair comparison between metrics that do and do not predict ties. We argue and provide experimental evidence that these modifications lead to fairer ranking-based assessments of metric performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;R2H&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#26500;&#24314;&#33021;&#22815;&#22238;&#24212;&#24110;&#21161;&#35831;&#27714;&#30340;&#22810;&#27169;&#24577;&#23548;&#33322;&#21161;&#25163;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#21253;&#25324;&#20004;&#20010;&#20219;&#21153;&#65292;&#21363;&#26681;&#25454;&#23545;&#35805;&#21382;&#21490;&#29983;&#25104;&#21709;&#24212;&#21644;&#22312;&#19982;&#20219;&#21153;&#25191;&#34892;&#32773;&#21512;&#20316;&#26102;&#36827;&#34892;&#21709;&#24212;&#35780;&#20272;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20219;&#21153;&#23548;&#21521;&#22810;&#27169;&#24577;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#26469;&#26500;&#24314;&#23548;&#33322;&#21161;&#25163;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.14260</link><description>&lt;p&gt;
R2H: &#26500;&#24314;&#33021;&#22815;&#22238;&#24212;&#24110;&#21161;&#35831;&#27714;&#30340;&#22810;&#27169;&#24577;&#23548;&#33322;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
R2H: Building Multimodal Navigation Helpers that Respond to Help Requests. (arXiv:2305.14260v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;R2H&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#26500;&#24314;&#33021;&#22815;&#22238;&#24212;&#24110;&#21161;&#35831;&#27714;&#30340;&#22810;&#27169;&#24577;&#23548;&#33322;&#21161;&#25163;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#21253;&#25324;&#20004;&#20010;&#20219;&#21153;&#65292;&#21363;&#26681;&#25454;&#23545;&#35805;&#21382;&#21490;&#29983;&#25104;&#21709;&#24212;&#21644;&#22312;&#19982;&#20219;&#21153;&#25191;&#34892;&#32773;&#21512;&#20316;&#26102;&#36827;&#34892;&#21709;&#24212;&#35780;&#20272;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20219;&#21153;&#23548;&#21521;&#22810;&#27169;&#24577;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#26469;&#26500;&#24314;&#23548;&#33322;&#21161;&#25163;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#23548;&#33322;&#21161;&#25163;&#20195;&#29702;&#20154;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#29615;&#22659;&#24863;&#30693;&#21644;&#23545;&#35805;&#33021;&#21147;&#65292;&#24341;&#23548;&#29992;&#25143;&#22312;&#26410;&#30693;&#21306;&#22495;&#20013;&#36827;&#34892;&#23548;&#33322;&#65292;&#24182;&#25104;&#20026;&#27531;&#38556;&#20154;&#22763;&#30340;&#28508;&#22312;&#36741;&#21161;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;Respond to Help Requests (R2H)&#65292;&#20197;&#20419;&#36827;&#24320;&#21457;&#33021;&#22815;&#22238;&#24212;&#24110;&#21161;&#35831;&#27714;&#30340;&#22810;&#27169;&#24577;&#23548;&#33322;&#21161;&#25163;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;&#20307;&#39564;&#25968;&#25454;&#38598;&#12290;R2H&#20027;&#35201;&#21253;&#25324;&#20004;&#20010;&#20219;&#21153;&#65306;(1) Respond to Dialog History (RDH)&#65292;&#29992;&#20110;&#35780;&#20272;&#21161;&#25163;&#20195;&#29702;&#20154;&#26681;&#25454;&#32473;&#23450;&#30340;&#23545;&#35805;&#21382;&#21490;&#29983;&#25104;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#21709;&#24212;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;(2) Respond during Interaction (RdI)&#65292;&#29992;&#20110;&#35780;&#20272;&#21709;&#24212;&#22312;&#19982;&#20219;&#21153;&#25191;&#34892;&#32773;&#25345;&#32493;&#21512;&#20316;&#26102;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20004;&#31181;&#26500;&#24314;&#23548;&#33322;&#21161;&#25163;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#23548;&#21521;&#30340;&#22810;&#27169;&#24577;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35266;&#23519;&#24182;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent navigation-helper agents are critical as they can navigate users in unknown areas through environmental awareness and conversational ability, serving as potential accessibility tools for individuals with disabilities. In this work, we first introduce a novel benchmark, Respond to Help Requests (R2H), to promote the development of multi-modal navigation helpers capable of responding to requests for help, utilizing existing dialog-based embodied datasets. R2H mainly includes two tasks: (1) Respond to Dialog History (RDH), which assesses the helper agent's ability to generate informative responses based on a given dialog history, and (2) Respond during Interaction (RdI), which evaluates the effectiveness and efficiency of the response during consistent cooperation with a task performer. Furthermore, we explore two approaches to construct the navigation-helper agent, including fine-tuning a novel task-oriented multi-modal response generation model that can see and respond, name
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#25991;&#26412;&#20013;&#35299;&#26512;&#20986;&#30340;&#22330;&#26223;&#22270;&#35270;&#20026;&#22270;&#20687;&#22330;&#26223;&#22270;&#30340;&#20195;&#29702;&#65292;&#24182;&#23545;&#22270;&#36827;&#34892;&#20998;&#35299;&#21644;&#22686;&#24378;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#23545;&#27604;&#23398;&#20064;&#20197;&#23558;&#21508;&#31181;&#22797;&#26434;&#24230;&#30340;&#21477;&#23376;&#23545;&#40784;&#21040;&#21516;&#19968;&#24133;&#22270;&#20687;&#19978;&#65292;&#21516;&#26102;&#22312;&#22330;&#26223;&#22270;&#31354;&#38388;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#36127;&#26679;&#26412;&#25366;&#25496;&#25216;&#26415;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13812</link><description>&lt;p&gt;
&#20174;&#22270;&#20687;-&#25991;&#26412;-&#22270;&#35889;&#31354;&#38388;&#20013;&#36827;&#34892;&#31895;&#21040;&#32454;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#25552;&#39640;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality. (arXiv:2305.13812v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#25991;&#26412;&#20013;&#35299;&#26512;&#20986;&#30340;&#22330;&#26223;&#22270;&#35270;&#20026;&#22270;&#20687;&#22330;&#26223;&#22270;&#30340;&#20195;&#29702;&#65292;&#24182;&#23545;&#22270;&#36827;&#34892;&#20998;&#35299;&#21644;&#22686;&#24378;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#23545;&#27604;&#23398;&#20064;&#20197;&#23558;&#21508;&#31181;&#22797;&#26434;&#24230;&#30340;&#21477;&#23376;&#23545;&#40784;&#21040;&#21516;&#19968;&#24133;&#22270;&#20687;&#19978;&#65292;&#21516;&#26102;&#22312;&#22330;&#26223;&#22270;&#31354;&#38388;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#36127;&#26679;&#26412;&#25366;&#25496;&#25216;&#26415;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20174;&#32780;&#20026;&#21508;&#31181;&#19979;&#28216;&#22810;&#27169;&#24577;&#20219;&#21153;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20984;&#26174;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23545;&#35937;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#30340;&#32452;&#25104;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#20005;&#37325;&#38480;&#21046;&#12290;&#22330;&#26223;&#22270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#29702;&#35299;&#22270;&#20687;&#32452;&#25104;&#30340;&#26377;&#25928;&#26041;&#24335;&#12290;&#36825;&#20123;&#26159;&#22270;&#20687;&#30340;&#22270;&#24418;&#32467;&#26500;&#21270;&#35821;&#20041;&#34920;&#31034;&#65292;&#21253;&#25324;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#12289;&#23427;&#20204;&#30340;&#23646;&#24615;&#21644;&#19982;&#22330;&#26223;&#20013;&#20854;&#20182;&#23545;&#35937;&#30340;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20174;&#25991;&#26412;&#20013;&#35299;&#26512;&#20986;&#30340;&#22330;&#26223;&#22270;&#35270;&#20026;&#22270;&#20687;&#22330;&#26223;&#22270;&#30340;&#20195;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20998;&#35299;&#21644;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#21450;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#23558;&#21508;&#31181;&#22797;&#26434;&#24230;&#30340;&#21477;&#23376;&#23545;&#40784;&#21040;&#21516;&#19968;&#24133;&#22270;&#20687;&#19978;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#22312;&#22330;&#26223;&#22270;&#31354;&#38388;&#25552;&#20986;&#20102;&#26032;&#30340;&#36127;&#26679;&#26412;&#25366;&#25496;&#25216;&#26415;&#65292;&#29992;&#20110;&#25913;&#21892;&#23545;&#27604;&#23398;&#20064;&#12290;&#22312;&#19977;&#20010;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24378;&#22823;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#32447;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#35937;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#30340;&#32452;&#25104;&#25512;&#29702;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning, leading to state-of-the-art models for various downstream multimodal tasks. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations. Scene graphs have emerged as an effective way to understand images compositionally. These are graph-structured semantic representations of images that contain objects, their attributes, and relations with other objects in a scene. In this work, we consider the scene graph parsed from text as a proxy for the image scene graph and propose a graph decomposition and augmentation framework along with a coarse-to-fine contrastive learning objective between images and text that aligns sentences of various complexities to the same image. Along with this, we propose novel negative mining techniques in the scene graph space for im
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#35782;&#21035;&#35821;&#20041;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#19977;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#21333;&#35789;&#23545;&#40784;&#21644;&#21477;&#32423;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#19982;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#31283;&#20581;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.13303</link><description>&lt;p&gt;
&#12298;&#20851;&#20110;&#30456;&#20851;&#25991;&#26723;&#20013;&#35821;&#20041;&#24046;&#24322;&#30340;&#26080;&#30417;&#30563;&#35782;&#21035;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Unsupervised Recognition of Semantic Differences in Related Documents. (arXiv:2305.13303v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#35782;&#21035;&#35821;&#20041;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#19977;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#21333;&#35789;&#23545;&#40784;&#21644;&#21477;&#32423;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#19982;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#31283;&#20581;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31361;&#20986;&#23548;&#33268;&#20004;&#20010;&#25991;&#26723;&#20043;&#38388;&#35821;&#20041;&#24046;&#24322;&#30340;&#21333;&#35789;&#21487;&#33021;&#23545;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#26377;&#29992;&#12290;&#25105;&#20204;&#23558;&#35821;&#20041;&#24046;&#24322;&#35782;&#21035;&#65288;RSD&#65289;&#23450;&#20041;&#20026;&#19968;&#20010;&#26631;&#35760;&#32423;&#22238;&#24402;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#19977;&#20010;&#20381;&#36182;&#20110;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#22522;&#26412;&#30340;&#33521;&#35821;&#21477;&#23376;&#24320;&#22987;&#65292;&#24182;&#36880;&#28176;&#36716;&#21521;&#26356;&#22797;&#26434;&#30340;&#36328;&#35821;&#35328;&#25991;&#26723;&#23545;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#21333;&#35789;&#23545;&#40784;&#21644;&#21477;&#32423;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#19982;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#31283;&#20581;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#21487;&#20197;&#22312;https://github.com/ZurichNLP/recognizing-semantic-differences&#25214;&#21040;&#37325;&#29616;&#25105;&#20204;&#23454;&#39564;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically highlighting words that cause semantic differences between two documents could be useful for a wide range of applications. We formulate recognizing semantic differences (RSD) as a token-level regression task and study three unsupervised approaches that rely on a masked language model. To assess the approaches, we begin with basic English sentences and gradually move to more complex, cross-lingual document pairs. Our results show that an approach based on word alignment and sentence-level contrastive learning has a robust correlation to gold labels. However, all unsupervised approaches still leave a large margin of improvement. Code to reproduce our experiments is available at https://github.com/ZurichNLP/recognizing-semantic-differences
&lt;/p&gt;</description></item><item><title>MacLaSa&#26159;&#19968;&#31181;&#36890;&#36807;&#20174;&#32039;&#33268;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#22810;&#26041;&#38754;&#21487;&#25511;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#20272;&#35745;&#32039;&#33268;&#28508;&#22312;&#31354;&#38388;&#21644;&#20351;&#29992;&#22522;&#20110;ODE&#30340;&#40065;&#26834;&#37319;&#26679;&#22120;&#26469;&#26377;&#25928;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#26399;&#26395;&#23646;&#24615;&#30340;&#21477;&#23376;&#12290;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#32593;&#32476;&#26469;&#28040;&#38500;&#19981;&#21516;&#26041;&#38754;&#20043;&#38388;&#30340;&#22495;&#24046;&#36317;&#65292;&#24182;&#33021;&#22815;&#21046;&#23450;&#32852;&#21512;&#33021;&#37327;&#27169;&#22411;&#21644;&#25554;&#20837;&#20219;&#24847;&#23646;&#24615;&#37492;&#21035;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.12785</link><description>&lt;p&gt;
MacLaSa: &#36890;&#36807;&#20174;&#32039;&#33268;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#39640;&#25928;&#37319;&#26679;&#23454;&#29616;&#22810;&#26041;&#38754;&#21487;&#25511;&#30340;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space. (arXiv:2305.12785v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12785
&lt;/p&gt;
&lt;p&gt;
MacLaSa&#26159;&#19968;&#31181;&#36890;&#36807;&#20174;&#32039;&#33268;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#22810;&#26041;&#38754;&#21487;&#25511;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#20272;&#35745;&#32039;&#33268;&#28508;&#22312;&#31354;&#38388;&#21644;&#20351;&#29992;&#22522;&#20110;ODE&#30340;&#40065;&#26834;&#37319;&#26679;&#22120;&#26469;&#26377;&#25928;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#26399;&#26395;&#23646;&#24615;&#30340;&#21477;&#23376;&#12290;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#32593;&#32476;&#26469;&#28040;&#38500;&#19981;&#21516;&#26041;&#38754;&#20043;&#38388;&#30340;&#22495;&#24046;&#36317;&#65292;&#24182;&#33021;&#22815;&#21046;&#23450;&#32852;&#21512;&#33021;&#37327;&#27169;&#22411;&#21644;&#25554;&#20837;&#20219;&#24847;&#23646;&#24615;&#37492;&#21035;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26041;&#38754;&#21487;&#25511;&#30340;&#25991;&#26412;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#21516;&#26102;&#20855;&#26377;&#22810;&#20010;&#26399;&#26395;&#23646;&#24615;&#30340;&#27969;&#30021;&#21477;&#23376;&#12290;&#20256;&#32479;&#26041;&#27861;&#35201;&#20040;&#22312;&#35299;&#30721;&#38454;&#27573;&#32467;&#21512;&#22810;&#20010;&#25805;&#20316;&#31526;&#65292;&#36890;&#24120;&#22312;&#31163;&#25955;&#25991;&#26412;&#31354;&#38388;&#20013;&#36827;&#34892;&#26114;&#36149;&#30340;&#36845;&#20195;&#25110;&#25628;&#32034;&#65292;&#35201;&#20040;&#20026;&#27599;&#20010;&#26041;&#38754;&#21333;&#29420;&#35757;&#32451;&#25511;&#21046;&#22120;&#65292;&#30001;&#20110;&#19981;&#21516;&#26041;&#38754;&#20043;&#38388;&#30340;&#24046;&#24322;&#32780;&#23548;&#33268;&#25991;&#26412;&#36136;&#37327;&#30340;&#36864;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26041;&#38754;&#25511;&#21046;&#26041;&#27861;&#65292;&#31216;&#20026;MacLaSa&#65292;&#23427;&#36890;&#36807;&#20272;&#35745;&#22810;&#20010;&#26041;&#38754;&#30340;&#32039;&#33268;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#30340;&#40065;&#26834;&#37319;&#26679;&#22120;&#36827;&#34892;&#39640;&#25928;&#37319;&#26679;&#12290;&#20026;&#20102;&#28040;&#38500;&#19981;&#21516;&#26041;&#38754;&#20043;&#38388;&#30340;&#22495;&#24046;&#36317;&#65292;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#32593;&#32476;&#23558;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#25991;&#26412;&#24207;&#21015;&#26144;&#23556;&#21040;&#25509;&#36817;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20272;&#35745;&#30340;&#28508;&#22312;&#31354;&#38388;&#20351;&#24471;&#33021;&#22815;&#21046;&#23450;&#32852;&#21512;&#33021;&#37327;&#27169;&#22411;&#65288;EBMs&#65289;&#24182;&#25554;&#20837;&#20219;&#24847;&#23646;&#24615;&#37492;&#21035;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-aspect controllable text generation aims to generate fluent sentences that possess multiple desired attributes simultaneously. Traditional methods either combine many operators in the decoding stage, often with costly iteration or search in the discrete text space, or train separate controllers for each aspect, resulting in a degeneration of text quality due to the discrepancy between different aspects. To address these limitations, we introduce a novel approach for multi-aspect control, namely MacLaSa, that estimates compact latent space for multiple aspects and performs efficient sampling with a robust sampler based on ordinary differential equations (ODEs). To eliminate the domain gaps between different aspects, we utilize a Variational Autoencoder (VAE) network to map text sequences from varying data sources into close latent representations. The estimated latent space enables the formulation of joint energy-based models (EBMs) and the plugging in of arbitrary attribute discr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;MultiCochrane&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#31532;&#19968;&#20010;&#21477;&#23376;&#23545;&#40784;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#31616;&#21270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22810;&#35821;&#35328;&#31616;&#21270;&#30452;&#25509;&#23558;&#22797;&#26434;&#25991;&#26412;&#31616;&#21270;&#20026;&#22810;&#31181;&#35821;&#35328;&#30340;&#31616;&#21270;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.12532</link><description>&lt;p&gt;
&#21307;&#23398;&#25991;&#26412;&#30340;&#22810;&#35821;&#35328;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multilingual Simplification of Medical Texts. (arXiv:2305.12532v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;MultiCochrane&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#31532;&#19968;&#20010;&#21477;&#23376;&#23545;&#40784;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#31616;&#21270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22810;&#35821;&#35328;&#31616;&#21270;&#30452;&#25509;&#23558;&#22797;&#26434;&#25991;&#26412;&#31616;&#21270;&#20026;&#22810;&#31181;&#35821;&#35328;&#30340;&#31616;&#21270;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#25991;&#26412;&#31616;&#21270;&#26088;&#22312;&#20135;&#29983;&#22797;&#26434;&#25991;&#26412;&#30340;&#31616;&#21270;&#29256;&#26412;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#36825;&#39033;&#20219;&#21153;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#26368;&#26032;&#30340;&#21307;&#23398;&#21457;&#29616;&#36890;&#24120;&#36890;&#36807;&#22797;&#26434;&#21644;&#25216;&#26415;&#24615;&#30340;&#25991;&#31456;&#36827;&#34892;&#20256;&#25773;&#12290;&#36825;&#20026;&#23547;&#27714;&#26368;&#26032;&#21307;&#23398;&#21457;&#29616;&#20449;&#24687;&#30340;&#26222;&#36890;&#20154;&#36896;&#25104;&#20102;&#38556;&#30861;&#65292;&#36827;&#32780;&#38459;&#30861;&#20102;&#20581;&#24247;&#32032;&#20859;&#30340;&#25552;&#39640;&#12290;&#22823;&#37096;&#20998;&#21307;&#23398;&#25991;&#26412;&#31616;&#21270;&#30340;&#29616;&#26377;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#21333;&#35821;&#35328;&#29615;&#22659;&#20013;&#65292;&#23548;&#33268;&#36825;&#20123;&#35777;&#25454;&#21482;&#33021;&#29992;&#19968;&#31181;&#35821;&#35328;&#65288;&#36890;&#24120;&#26159;&#33521;&#35821;&#65289;&#25552;&#20379;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#35821;&#35328;&#31616;&#21270;&#30452;&#25509;&#23558;&#22797;&#26434;&#25991;&#26412;&#31616;&#21270;&#20026;&#22810;&#31181;&#35821;&#35328;&#30340;&#31616;&#21270;&#25991;&#26412;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MultiCochrane&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#31532;&#19968;&#20010;&#22235;&#31181;&#35821;&#35328;&#65288;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#27861;&#35821;&#21644;&#27874;&#26031;&#35821;&#65289;&#30340;&#21477;&#23376;&#23545;&#40784;&#22810;&#35821;&#35328;&#25991;&#26412;&#31616;&#21270;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#24037;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#22312;&#36825;&#20123;&#35821;&#35328;&#20043;&#38388;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#38646;&#26679;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated text simplification aims to produce simple versions of complex texts. This task is especially useful in the medical domain, where the latest medical findings are typically communicated via complex and technical articles. This creates barriers for laypeople seeking access to up-to-date medical findings, consequently impeding progress on health literacy. Most existing work on medical text simplification has focused on monolingual settings, with the result that such evidence would be available only in just one language (most often, English). This work addresses this limitation via multilingual simplification, i.e., directly simplifying complex texts into simplified texts in multiple languages. We introduce MultiCochrane, the first sentence-aligned multilingual text simplification dataset for the medical domain in four languages: English, Spanish, French, and Farsi. We evaluate fine-tuned and zero-shot models across these languages, with extensive human assessments and analyses. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>TrueTeacher&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#36827;&#34892;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;TrueTeacher&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25688;&#35201;&#65292;&#22810;&#35821;&#35328;&#29305;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11171</link><description>&lt;p&gt;
TrueTeacher: &#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models. (arXiv:2305.11171v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11171
&lt;/p&gt;
&lt;p&gt;
TrueTeacher&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#36827;&#34892;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;TrueTeacher&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25688;&#35201;&#65292;&#22810;&#35821;&#35328;&#29305;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#36890;&#24120;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#36827;&#34892;&#65292;&#28982;&#32780;&#36825;&#20123;&#27169;&#22411;&#22312;&#35780;&#20272;&#25688;&#35201;&#26102;&#21462;&#24471;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#36807;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#25913;&#36827;&#20102;&#27492;&#31867;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#22522;&#20110;&#25200;&#21160;&#30340;&#20154;&#24037;&#32534;&#20889;&#25688;&#35201;&#65292;&#19982;&#30495;&#23454;&#30340;&#27169;&#22411;&#29983;&#25104;&#25688;&#35201;&#22312;&#29305;&#24615;&#19978;&#24120;&#24120;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#23545;&#21487;&#33021;&#23384;&#22312;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26368;&#36817;&#22312;&#30452;&#25509;&#35780;&#20272;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#35745;&#31639;&#24320;&#38144;&#36807;&#22823;&#65292;&#26080;&#27861;&#23454;&#38469;&#24212;&#29992;&#12290;&#20986;&#20110;&#23545;&#36825;&#20123;&#38480;&#21046;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TrueTeacher&#65292;&#19968;&#31181;&#20351;&#29992;LLM&#27880;&#37322;&#22810;&#26679;&#30340;&#27169;&#22411;&#29983;&#25104;&#25688;&#35201;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;TrueTeacher&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25688;&#35201;&#65292;&#24182;&#19988;&#20855;&#26377;&#22810;&#35821;&#35328;&#29305;&#24615;&#12290;&#22312;TRUE&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36828;&#36828;&#36229;&#36807;&#20102;NLI&#27169;&#22411;&#21644;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the st
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Chunk-LEvel Multi-reference Evaluation (CLEME)&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#22810;&#21442;&#32771;&#29615;&#22659;&#19979;&#35780;&#20272;GEC&#31995;&#32479;&#26102;&#23384;&#22312;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#20854;&#36890;&#36807;&#28040;&#38500;&#30001;&#19981;&#19968;&#33268;&#30340;&#32534;&#36753;&#36793;&#30028;&#24341;&#36215;&#30340;&#20559;&#24046;&#21644;&#33258;&#21160;&#30830;&#23450;&#35821;&#27861;&#38169;&#35823;&#30340;&#36793;&#30028;&#26469;&#25552;&#39640;&#20102;GEC&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10819</link><description>&lt;p&gt;
CLEME: &#38024;&#23545;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#30340;&#21435;&#20559;&#32622;&#22810;&#21442;&#32771;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction. (arXiv:2305.10819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10819
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Chunk-LEvel Multi-reference Evaluation (CLEME)&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#22810;&#21442;&#32771;&#29615;&#22659;&#19979;&#35780;&#20272;GEC&#31995;&#32479;&#26102;&#23384;&#22312;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#20854;&#36890;&#36807;&#28040;&#38500;&#30001;&#19981;&#19968;&#33268;&#30340;&#32534;&#36753;&#36793;&#30028;&#24341;&#36215;&#30340;&#20559;&#24046;&#21644;&#33258;&#21160;&#30830;&#23450;&#35821;&#27861;&#38169;&#35823;&#30340;&#36793;&#30028;&#26469;&#25552;&#39640;&#20102;GEC&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;Grammatical Error Correction (GEC)&#26159;&#19968;&#39033;&#39640;&#24230;&#20027;&#35266;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#35780;&#20272;&#20854;&#24615;&#33021;&#21464;&#24471;&#22256;&#38590;&#12290;&#35774;&#35745;&#23613;&#21487;&#33021;&#23458;&#35266;&#30340;&#35780;&#20272;&#25351;&#26631;&#23545;&#20110;GEC&#20219;&#21153;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#20027;&#27969;&#35780;&#20272;&#25351;&#26631;&#65292;&#21363;&#22522;&#20110;&#21442;&#32771;&#30340;&#25351;&#26631;&#65292;&#22312;&#25552;&#21462;&#32534;&#36753;&#26102;&#26410;&#32771;&#34385;&#22810;&#20010;&#21442;&#32771;&#30340;&#23384;&#22312;&#65292;&#20174;&#32780;&#24341;&#20837;&#20559;&#35265;&#21040;&#22810;&#21442;&#32771;&#35780;&#20272;&#20013;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Chunk-LEvel Multi-reference Evaluation (CLEME)&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#22810;&#21442;&#32771;&#29615;&#22659;&#20013;&#35780;&#20272;GEC&#31995;&#32479;&#12290;&#39318;&#20808;&#65292;CLEME&#20026;&#28304;&#12289;&#20551;&#35774;&#21644;&#25152;&#26377;&#21442;&#32771;&#24314;&#31435;&#20855;&#26377;&#19968;&#33268;&#36793;&#30028;&#30340;&#22359;&#24207;&#21015;&#65292;&#20174;&#32780;&#28040;&#38500;&#30001;&#19981;&#19968;&#33268;&#30340;&#32534;&#36753;&#36793;&#30028;&#24341;&#36215;&#30340;&#20559;&#24046;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#21457;&#29616;&#23384;&#22312;&#19981;&#21516;&#35821;&#27861;&#38169;&#35823;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#25105;&#20204;&#33258;&#21160;&#30830;&#23450;&#20102;&#35821;&#27861;&#38169;&#35823;&#30340;&#36793;&#30028;&#65292;&#24182;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#35745;&#31639;&#20102;F$_{0.5}$&#24471;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;CLEME&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#21435;&#20559;&#32622;&#22810;&#21442;&#32771;&#35780;&#20272;GEC&#31995;&#32479;&#65292;&#24182;&#25552;&#39640;GEC&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is intractable to evaluate the performance of Grammatical Error Correction (GEC) systems since GEC is a highly subjective task. Designing an evaluation metric that is as objective as possible is crucial to the development of GEC task. Previous mainstream evaluation metrics, i.e., reference-based metrics, introduce bias into the multi-reference evaluation because they extract edits without considering the presence of multiple references. To overcome the problem, we propose Chunk-LEvel Multi-reference Evaluation (CLEME) designed to evaluate GEC systems in multi-reference settings. First, CLEME builds chunk sequences with consistent boundaries for the source, the hypothesis and all the references, thus eliminating the bias caused by inconsistent edit boundaries. Then, based on the discovery that there exist boundaries between different grammatical errors, we automatically determine the grammatical error boundaries and compute F$_{0.5}$ scores in a novel way. Our proposed CLEME approach
&lt;/p&gt;</description></item><item><title>&#35821;&#38899;&#32763;&#35793;(ST)&#26159;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#30340;&#33391;&#22909;&#25163;&#27573;&#12290;&#36890;&#36807;&#24341;&#20837;ST&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21333;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#22330;&#26223;&#19979;&#34920;&#29616;&#22343;&#22909;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09652</link><description>&lt;p&gt;
&#35299;&#37322;&#22120;&#29702;&#35299;&#24744;&#30340;&#24847;&#24605;: &#30001;&#35821;&#38899;&#32763;&#35793;&#21327;&#21161;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation. (arXiv:2305.09652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09652
&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#32763;&#35793;(ST)&#26159;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#30340;&#33391;&#22909;&#25163;&#27573;&#12290;&#36890;&#36807;&#24341;&#20837;ST&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21333;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#22330;&#26223;&#19979;&#34920;&#29616;&#22343;&#22909;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#22312;&#25991;&#26412;&#21644;&#35821;&#38899;&#19978;&#26377;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#12290;&#26426;&#22120;&#32763;&#35793;&#24050;&#34987;&#30830;&#23450;&#20026;&#24378;&#22823;&#30340;&#25991;&#26412;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#22240;&#20026;&#23427;&#20351;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#36755;&#20837;&#35821;&#21477;&#30340;&#39640;&#32423;&#35821;&#20041;&#21644;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#36825;&#23545;&#20110;&#22788;&#29702;&#26356;&#20302;&#32423;&#21035;&#30340;&#22768;&#23398;&#24103;&#30340;&#35821;&#38899;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#29305;&#21035;&#38024;&#23545;&#36328;&#35821;&#35328;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#35821;&#38899;&#32763;&#35793;(ST)&#26159;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#30340;&#33391;&#22909;&#25163;&#27573;&#65292;&#26080;&#35770;&#26159;&#22312;&#21333;&#35821;&#35328;&#22330;&#26223;&#36824;&#26159;&#36328;&#35821;&#35328;&#22330;&#26223;&#19979;&#12290;&#36890;&#36807;&#24341;&#20837;ST&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20351;&#29992;SLURP&#12289;MINDS-14&#21644;NMSQA&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#21333;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#24847;&#22270;&#20998;&#31867;&#20197;&#21450;&#21475;&#35821;&#38382;&#31572;&#26102;&#65292;&#30456;&#27604;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#22343;&#20855;&#26377;&#26356;&#39640;&#24615;&#33021;&#12290;&#20026;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26469;&#33258;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end spoken language understanding (SLU) remains elusive even with current large pretrained language models on text and speech, especially in multilingual cases. Machine translation has been established as a powerful pretraining objective on text as it enables the model to capture high-level semantics of the input utterance and associations between different languages, which is desired for speech models that work on lower-level acoustic frames. Motivated particularly by the task of cross-lingual SLU, we demonstrate that the task of speech translation (ST) is a good means of pretraining speech models for end-to-end SLU on both monolingual and cross-lingual scenarios.  By introducing ST, our models give higher performance over current baselines on monolingual and multilingual intent classification as well as spoken question answering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the effectiveness of our methods, we also release two new benchmark datasets from both syntheti
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#33539;&#24335;&#8212;&#8212;&#36890;&#36807;LLM&#36827;&#34892;&#25512;&#33616;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;RecLLM&#25152;&#20570;&#25512;&#33616;&#30340;&#20844;&#27491;&#24615;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#8212;&#8212;FaiRLLM&#65292;&#24182;&#38024;&#23545;&#38899;&#20048;&#21644;&#30005;&#24433;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#20843;&#20010;&#25935;&#24863;&#23646;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.07609</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#20844;&#24179;&#21487;&#38752;&#65311;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation. (arXiv:2305.07609v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07609
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#33539;&#24335;&#8212;&#8212;&#36890;&#36807;LLM&#36827;&#34892;&#25512;&#33616;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;RecLLM&#25152;&#20570;&#25512;&#33616;&#30340;&#20844;&#27491;&#24615;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#8212;&#8212;FaiRLLM&#65292;&#24182;&#38024;&#23545;&#38899;&#20048;&#21644;&#30005;&#24433;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#20843;&#20010;&#25935;&#24863;&#23646;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26174;&#30528;&#25104;&#23601;&#23548;&#33268;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#33539;&#24335;&#8212;&#8212;&#36890;&#36807;LLM&#36827;&#34892;&#25512;&#33616;&#65288;RecLLM&#65289;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;LLMs&#21487;&#33021;&#21253;&#21547;&#31038;&#20250;&#20559;&#35265;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;RecLLM&#25152;&#20570;&#25512;&#33616;&#30340;&#20844;&#27491;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;RecLLM&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#26377;&#24517;&#35201;&#20174;&#29992;&#25143;&#30340;&#21508;&#31181;&#25935;&#24863;&#23646;&#24615;&#35282;&#24230;&#35780;&#20272;RecLLM&#30340;&#20844;&#24179;&#24615;&#12290;&#30001;&#20110;RecLLM&#33539;&#24335;&#19982;&#20256;&#32479;&#25512;&#33616;&#33539;&#24335;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#22240;&#27492;&#30452;&#25509;&#20351;&#29992;&#20256;&#32479;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;&#8220;&#36890;&#36807;LLM&#30340;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#8221;&#65288;FaiRLLM&#65289;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#31934;&#24515;&#35774;&#35745;&#30340;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20004;&#20010;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#20843;&#20010;&#25935;&#24863;&#23646;&#24615;&#65306;&#38899;&#20048;&#21644;&#30005;&#24433;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;FaiRLLM&#22522;&#20934;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable achievements of Large Language Models (LLMs) have led to the emergence of a novel recommendation paradigm -- Recommendation via LLM (RecLLM). Nevertheless, it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation. To avoid the potential risks of RecLLM, it is imperative to evaluate the fairness of RecLLM with respect to various sensitive attributes on the user side. Due to the differences between the RecLLM paradigm and the traditional recommendation paradigm, it is problematic to directly use the fairness benchmark of traditional recommendation. To address the dilemma, we propose a novel benchmark called Fairness of Recommendation via LLM (FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset that accounts for eight sensitive attributes1 in two recommendation scenarios: music and movies. By utilizing our FaiRLLM benchmark, we conducted an evaluation 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#26377;&#21033;&#20110;&#25552;&#39640;LLM&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13734</link><description>&lt;p&gt;
&#19968;&#20010;LLM&#30693;&#36947;&#33258;&#24049;&#22312;&#25746;&#35854;&#30340;&#20869;&#37096;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
The Internal State of an LLM Knows When its Lying. (arXiv:2304.13734v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#26377;&#21033;&#20110;&#25552;&#39640;LLM&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#65288;&#21487;&#33021;&#65289;&#26368;&#20026;&#31361;&#20986;&#30340;&#32570;&#28857;&#26159;&#20197;&#33258;&#20449;&#30340;&#35821;&#27668;&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20551;&#35774;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#21487;&#20197;&#29992;&#20110;&#25581;&#31034;&#19968;&#20010;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;LLM&#25152;&#29983;&#25104;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20845;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30495;&#23454;&#21644;&#34394;&#20551;&#30340;&#35821;&#21477;&#12290;&#19968;&#20010;&#20998;&#31867;&#22120;&#34987;&#35757;&#32451;&#20986;&#26469;&#65292;&#26681;&#25454;LLM&#30340;&#28608;&#27963;&#20540;&#26469;&#26816;&#27979;&#21738;&#20010;&#35821;&#21477;&#26159;&#30495;&#23454;&#30340;&#25110;&#34394;&#20551;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20998;&#31867;&#22120;&#25509;&#25910;LLM&#20026;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#35821;&#21477;&#29983;&#25104;&#30340;&#28608;&#27963;&#20540;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#26816;&#27979;&#35821;&#21477;&#30495;&#23454;&#24615;&#30340;&#26041;&#27861;&#29978;&#33267;&#27604;&#23569;&#37327;&#25552;&#31034;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20984;&#26174;&#20102;&#21033;&#29992;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#26469;&#25552;&#39640;&#20854;&#21487;&#20449;&#24230;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have shown exceptional performance in various tasks, their (arguably) most prominent drawback is generating inaccurate or false information with a confident tone. In this paper, we hypothesize that the LLM's internal state can be used to reveal the truthfulness of a statement. Therefore, we introduce a simple yet effective method to detect the truthfulness of LLM-generated statements, which utilizes the LLM's hidden layer activations to determine the veracity of statements. To train and evaluate our method, we compose a dataset of true and false statements in six different topics. A classifier is trained to detect which statement is true or false based on an LLM's activation values. Specifically, the classifier receives as input the activation values from the LLM for each of the statements in the dataset. Our experiments demonstrate that our method for detecting statement veracity significantly outperforms even few-shot prompting methods, highlighting
&lt;/p&gt;</description></item><item><title>PAXQA&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29616;&#26377;&#24179;&#34892;&#35821;&#26009;&#24211;&#29983;&#25104;&#36328;&#35821;&#35328;&#38382;&#31572;&#26679;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#21644;&#27880;&#37322;&#25237;&#24433;&#30456;&#32467;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#36827;&#34892;&#38382;&#39064;&#32763;&#35793;&#21644;&#22238;&#31572;&#32763;&#35793;&#12290;&#35813;&#26041;&#27861;&#22312;4&#31181;&#35821;&#35328;&#20013;&#29983;&#25104;&#20102;662K&#20010;&#20363;&#23376;&#12290;</title><link>http://arxiv.org/abs/2304.12206</link><description>&lt;p&gt;
PAXQA: &#22312;&#35757;&#32451;&#35268;&#27169;&#19978;&#29983;&#25104;&#36328;&#35821;&#35328;&#38382;&#31572;&#26679;&#20363;
&lt;/p&gt;
&lt;p&gt;
PAXQA: Generating Cross-lingual Question Answering Examples at Training Scale. (arXiv:2304.12206v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12206
&lt;/p&gt;
&lt;p&gt;
PAXQA&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29616;&#26377;&#24179;&#34892;&#35821;&#26009;&#24211;&#29983;&#25104;&#36328;&#35821;&#35328;&#38382;&#31572;&#26679;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#21644;&#27880;&#37322;&#25237;&#24433;&#30456;&#32467;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#36827;&#34892;&#38382;&#39064;&#32763;&#35793;&#21644;&#22238;&#31572;&#32763;&#35793;&#12290;&#35813;&#26041;&#27861;&#22312;4&#31181;&#35821;&#35328;&#20013;&#29983;&#25104;&#20102;662K&#20010;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38382;&#31572;&#31995;&#32479;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290; &#36825;&#31181;&#26631;&#27880;&#24037;&#20316;&#25104;&#26412;&#39640;&#65292;&#24182;&#19988;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#38590;&#24230;&#22686;&#21152;&#12290; &#22240;&#27492;&#65292;&#20043;&#21069;&#30340;&#36328;&#35821;&#35328;&#38382;&#31572;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21457;&#24067;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#23558;&#38646;-shot&#26041;&#27861;&#20316;&#20026;&#22522;&#20934;&#24212;&#29992;&#12290; &#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36328;&#35821;&#35328;&#38382;&#31572;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#30340;&#38388;&#25509;&#30417;&#30563;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;PAXQA&#65288;&#36328;&#35821;&#35328;(x) QA&#30340;&#25237;&#24433;&#27880;&#37322;&#65289;&#65292;&#23558;&#36328;&#35821;&#35328;&#38382;&#31572;&#20998;&#35299;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290; &#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#29983;&#25104;&#65288;QG&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;&#33521;&#25991;&#20391;&#38754;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#27880;&#37322;&#25237;&#24433;&#26469;&#32763;&#35793;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290; &#20026;&#20102;&#26356;&#22909;&#22320;&#32763;&#35793;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35789;&#27719;&#32422;&#26463;&#26426;&#22120;&#32763;&#35793;&#30340;&#29992;&#27861;&#65292;&#20854;&#20013;&#20174;&#24179;&#34892;&#21452;&#25991;&#26412;&#20013;&#25552;&#21462;&#32422;&#26463;&#23454;&#20307;&#12290; &#25105;&#20204;&#23558;PAXQA&#24212;&#29992;&#20110;4&#31181;&#35821;&#35328;&#65288;662K ex&#65289;&#30340;&#36328;&#35821;&#35328;&#38382;&#31572;&#26679;&#20363;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing question answering (QA) systems owe much of their success to large, high-quality training data. Such annotation efforts are costly, and the difficulty compounds in the cross-lingual setting. Therefore, prior cross-lingual QA work has focused on releasing evaluation datasets, and then applying zero-shot methods as baselines. This work proposes a synthetic data generation method for cross-lingual QA which leverages indirect supervision from existing parallel corpora. Our method termed PAXQA (Projecting annotations for cross-lingual (x) QA) decomposes cross-lingual QA into two stages. First, we apply a question generation (QG) model to the English side. Second, we apply annotation projection to translate both the questions and answers. To better translate questions, we propose a novel use of lexically-constrained machine translation, in which constrained entities are extracted from the parallel bitexts.  We apply PAXQA to generate cross-lingual QA examples in 4 languages (662K ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04250</link><description>&lt;p&gt;
&#21487;&#32534;&#36753;&#29992;&#25143;&#26723;&#26696;&#30340;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#39640;&#36136;&#37327;&#25512;&#33616;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20174;&#20132;&#20114;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#25552;&#20379;&#32473;&#29992;&#25143;&#25511;&#21046;&#25152;&#25509;&#25910;&#30340;&#25512;&#33616;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LACE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;LACE&#22522;&#20110;&#29992;&#25143;&#20132;&#20114;&#30340;&#25991;&#26723;&#26816;&#32034;&#65292;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#31616;&#27905;&#30340;&#21487;&#35835;&#30340;&#27010;&#24565;&#38598;&#65292;&#24182;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#27010;&#24565;&#30340;&#20010;&#24615;&#21270;&#34920;&#31034;&#12290;&#35813;&#22522;&#20110;&#27010;&#24565;&#30340;&#29992;&#25143;&#26723;&#26696;&#34987;&#21033;&#29992;&#26469;&#20570;&#20986;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#35774;&#35745;&#36890;&#36807;&#36879;&#26126;&#30340;&#29992;&#25143;&#26723;&#26696;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#22810;&#31181;&#30452;&#35266;&#20132;&#20114;&#26041;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19977;&#20010;&#25512;&#33616;&#20219;&#21153;&#65288;&#28201;&#21551;&#21160;&#12289;&#20919;&#21551;&#21160;&#21644;&#38646;&#26679;&#26412;&#65289;&#30340;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#31163;&#32447;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;&#20174;LACE&#33719;&#24471;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#22312;&#32447;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;LACE&#30340;&#26377;&#25928;&#24615;&#21644;&#29992;&#25143;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for making high-quality recommendations often rely on learning latent representations from interaction data. These methods, while performant, do not provide ready mechanisms for users to control the recommendation they receive. Our work tackles this problem by proposing LACE, a novel concept value bottleneck model for controllable text recommendations. LACE represents each user with a succinct set of human-readable concepts through retrieval given user-interacted documents and learns personalized representations of the concepts based on user documents. This concept based user profile is then leveraged to make recommendations. The design of our model affords control over the recommendations through a number of intuitive interactions with a transparent user profile. We first establish the quality of recommendations obtained from LACE in an offline evaluation on three recommendation tasks spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we validate the 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#21644;&#20256;&#25773;&#32467;&#26500;&#65292;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#36328;&#36234;&#35821;&#35328;&#21644;&#39046;&#22495;&#30028;&#38480;&#30340;&#35875;&#35328;&#12290;</title><link>http://arxiv.org/abs/2304.01492</link><description>&lt;p&gt;
&#32479;&#19968;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#19982;&#20256;&#25773;&#32467;&#26500;&#29992;&#20110;&#25552;&#39640;&#20302;&#36164;&#28304;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#21644;&#20256;&#25773;&#32467;&#26500;&#65292;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#36328;&#36234;&#35821;&#35328;&#21644;&#39046;&#22495;&#30028;&#38480;&#30340;&#35875;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#35875;&#35328;&#20276;&#38543;&#30528;&#31361;&#21457;&#26032;&#38395;&#25110;&#28909;&#38376;&#35805;&#39064;&#32780;&#20256;&#25773;&#65292;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#30495;&#30456;&#30340;&#20256;&#25773;&#12290;&#29616;&#26377;&#30340;&#35875;&#35328;&#26816;&#27979;&#31639;&#27861;&#23637;&#31034;&#20102;&#22312;&#21069;&#20960;&#22825;&#26032;&#38395;&#19978;&#33391;&#22909;&#24615;&#33021;&#30340;&#21069;&#26223;&#65292;&#20294;&#26159;&#30001;&#20110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#21644;&#20808;&#21069;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23427;&#20204;&#24456;&#38590;&#21457;&#29616;&#19982;&#39044;&#26399;&#20107;&#20214;&#26377;&#20851;&#30340;&#35875;&#35328;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21516;&#35821;&#35328;&#65288;&#21363;&#20302;&#36164;&#28304;&#29615;&#22659;&#65289;&#20013;&#20256;&#25773;&#30340;&#35875;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#29305;&#24449;&#26469;&#26816;&#27979;&#35875;&#35328;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#25773;&#30340;&#35875;&#35328;&#34920;&#31034;&#20026;&#26080;&#21521;&#25299;&#25169;&#32467;&#26500;&#65292;&#28982;&#21518;&#36890;&#36807;&#32479;&#19968;&#23545;&#27604;&#33539;&#24335;&#36827;&#34892;Multi-scale&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#30830;&#22320;&#31361;&#30772;&#20102;&#39046;&#22495;&#21644;/&#25110;&#35821;&#35328;&#38382;&#39064;&#30340;&#38556;&#30861;&#65292;&#36890;&#36807;&#35821;&#35328;&#23545;&#40784;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The truth is significantly hampered by massive rumors that spread along with breaking news or popular topics. Since there is sufficient corpus gathered from the same domain for model training, existing rumor detection algorithms show promising performance on yesterday's news. However, due to a lack of training data and prior expert knowledge, they are poor at spotting rumors concerning unforeseen events, especially those propagated in different languages (i.e., low-resource regimes). In this paper, we propose a unified contrastive transfer framework to detect rumors by adapting the features learned from well-resourced rumor data to that of the low-resourced. More specifically, we first represent rumor circulated on social media as an undirected topology, and then train a Multi-scale Graph Convolutional Network via a unified contrastive paradigm. Our model explicitly breaks the barriers of the domain and/or language issues, via language alignment and a novel domain-adaptive contrastive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20219;&#21153;&#21644;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#26469;&#20248;&#21270;ChatGPT&#22312;&#22797;&#26434;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#28201;&#24230;&#35774;&#32622;&#21644;&#20219;&#21153;&#20449;&#24687;&#23545;ChatGPT&#34920;&#29616;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.13780</link><description>&lt;p&gt;
&#20248;&#21270;ChatGPT&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Towards Making the Most of ChatGPT for Machine Translation. (arXiv:2303.13780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20219;&#21153;&#21644;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#26469;&#20248;&#21270;ChatGPT&#22312;&#22797;&#26434;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#28201;&#24230;&#35774;&#32622;&#21644;&#20219;&#21153;&#20449;&#24687;&#23545;ChatGPT&#34920;&#29616;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#26041;&#38754;&#21487;&#20197;&#36798;&#21040;&#21830;&#19994;&#31995;&#32479;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#65288;&#20363;&#22914;&#20302;&#36164;&#28304;&#21644;&#36828;&#31243;&#35821;&#35328;&#23545;&#32763;&#35793;&#65289;&#33853;&#21518;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#37319;&#29992;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#26080;&#27861;&#20805;&#20998;&#21457;&#25381;ChatGPT&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#28201;&#24230;&#12289;&#20219;&#21153;&#20449;&#24687;&#21644;&#39046;&#22495;&#20449;&#24687;&#31561;&#20960;&#20010;&#26041;&#38754;&#65292;&#36827;&#19968;&#27493;&#25366;&#25496;ChatGPT&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#24182;&#30456;&#24212;&#22320;&#25552;&#20986;&#20004;&#31181;&#65288;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#65289;&#25552;&#31034;&#65306;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#65288;TSP&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#65288;DSP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT shows remarkable capabilities for machine translation (MT). Several prior studies have shown that it achieves comparable results to commercial systems for high-resource languages, but lags behind in complex tasks, e.g, low-resource and distant-language-pairs translation. However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT. In this report, we aim to further mine ChatGPT's translation ability by revisiting several aspects: temperature, task information, and domain information, and correspondingly propose two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The performance of ChatGPT depends largely on temperature, and a lower temperature usually can achieve better performance; 2) Emphasizing the task information further improves ChatGPT's performance, particularly in complex MT tasks; 3) Introducing domain information can elicit ChatGPT's generalization ability and improve i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;NLP&#25968;&#25454;&#38598;&#20013;&#33258;&#25105;&#24433;&#21709;&#30340;&#31283;&#23450;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#25506;&#35752;&#20102;&#33258;&#25105;&#24433;&#21709;&#20998;&#25968;&#22312;&#25968;&#25454;&#28165;&#29702;&#20013;&#30340;&#36866;&#24212;&#24615;&#65292;&#20197;&#21450;&#20854;&#23545;&#26426;&#22120;&#32763;&#35793;&#12289;&#38382;&#31572;&#21644;&#25991;&#26412;&#20998;&#31867;&#31561;&#20219;&#21153;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.13959</link><description>&lt;p&gt;
&#35753;&#27599;&#20010;&#26679;&#26412;&#37117;&#26377;&#29992;&#65306;&#20851;&#20110;&#22312;&#21547;&#22122;NLP&#25968;&#25454;&#38598;&#20013;&#33258;&#25105;&#24433;&#21709;&#30340;&#31283;&#23450;&#24615;&#21644;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets. (arXiv:2302.13959v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;NLP&#25968;&#25454;&#38598;&#20013;&#33258;&#25105;&#24433;&#21709;&#30340;&#31283;&#23450;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#25506;&#35752;&#20102;&#33258;&#25105;&#24433;&#21709;&#20998;&#25968;&#22312;&#25968;&#25454;&#28165;&#29702;&#20013;&#30340;&#36866;&#24212;&#24615;&#65292;&#20197;&#21450;&#20854;&#23545;&#26426;&#22120;&#32763;&#35793;&#12289;&#38382;&#31572;&#21644;&#25991;&#26412;&#20998;&#31867;&#31561;&#20219;&#21153;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;NLP&#39046;&#22495;&#65292;&#20351;&#29992;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#24050;&#25104;&#20026;&#25512;&#21160;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#30340;&#26631;&#20934;&#25104;&#20998;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#36136;&#37327;&#21487;&#33021;&#24050;&#25104;&#20026;&#35299;&#38145;&#26356;&#22823;&#36827;&#23637;&#30340;&#29942;&#39048;&#12290;&#30001;&#20110;&#29616;&#20195;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#35268;&#27169;&#65292;&#26631;&#20934;&#30340;&#25968;&#25454;&#36807;&#28388;&#19981;&#23481;&#26131;&#24212;&#29992;&#65292;&#22240;&#20026;&#26377;&#23475;&#25968;&#25454;&#30340;&#22810;&#38754;&#24615;&#21644;&#36807;&#28388;&#35268;&#21017;&#30340;&#38544;&#26214;&#24615;&#20351;&#20854;&#38590;&#20197;&#25512;&#24191;&#21040;&#22810;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#26679;&#26412;&#30340;&#36890;&#29992;&#33258;&#25105;&#24433;&#21709;&#20998;&#25968;&#22312;&#25968;&#25454;&#28165;&#29702;&#20013;&#30340;&#36866;&#24212;&#24615;&#65292;&#20998;&#26512;&#20102;&#20854;&#25429;&#25417;&#33258;&#28982;&#24322;&#24120;&#20540;&#30340;&#21151;&#25928;&#65292;&#24182;&#30740;&#31350;&#20102;&#22522;&#20110;&#33258;&#25105;&#24433;&#21709;&#30340;&#25968;&#25454;&#28165;&#29702;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#38382;&#31572;&#21644;&#25991;&#26412;&#20998;&#31867;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#24615;&#33021;&#30340;&#25913;&#36827;&#31243;&#24230;&#12290;&#36825;&#20123;&#30740;&#31350;&#22522;&#20110;&#26368;&#36817;&#30340;&#33258;&#25105;&#24433;&#21709;&#35745;&#31639;&#21644;&#33258;&#21160;&#21270;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increasingly larger datasets have become a standard ingredient to advancing the state-of-the-art in NLP. However, data quality might have already become the bottleneck to unlock further gains. Given the diversity and the sizes of modern datasets, standard data filtering is not straight-forward to apply, because of the multifacetedness of the harmful data and elusiveness of filtering rules that would generalize across multiple tasks. We study the fitness of task-agnostic self-influence scores of training examples for data cleaning, analyze their efficacy in capturing naturally occurring outliers, and investigate to what extent self-influence based data cleaning can improve downstream performance in machine translation, question answering and text classification, building up on recent approaches to self-influence calculation and automated curriculum learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;InfoSeek&#12290;&#20351;&#29992;InfoSeek&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2302.11713</link><description>&lt;p&gt;
Pre-trained Vision and Language Models&#33021;&#21542;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;InfoSeek&#12290;&#20351;&#29992;InfoSeek&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models&#22312;&#28041;&#21450;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#39046;&#20808;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#35270;&#35273;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20855;&#22791;&#22238;&#31572;&#19981;&#20165;&#20165;&#26597;&#35810;&#35270;&#35273;&#20869;&#23481;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#30693;&#35782;&#23494;&#38598;&#21644;&#20449;&#24687;&#23547;&#27714;&#24615;&#36136;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InfoSeek&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;InfoSeek&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#24182;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;PaLI-X&#65292;BLIP2&#31561;&#65289;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;InfoSeek&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#22815;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#20182;&#20204;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#21040;&#30340;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20934;&#30830;&#30340;&#35270;&#35273;&#23454;&#20307;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual content but knowledge-intensive and information-seeking. In this study, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training. Furthermore, we show that accurate visual entit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11916</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#65306;&#35299;&#37322;&#21644;&#23547;&#25214;&#22909;&#30340;&#31034;&#33539;&#20197;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#22312;&#25512;&#29702;&#26102;&#23454;&#29616;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#30340;&#26174;&#33879;&#25928;&#29575;&#65292;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290; &#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#24378;&#35843;&#36825;&#31181;&#33021;&#21147;&#23545;&#23569;&#37327;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#24456;&#25935;&#24863;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#36125;&#21494;&#26031;&#35270;&#35282;&#30740;&#31350;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#20174;&#31034;&#33539;&#20013;&#38544;&#21547;&#22320;&#25512;&#26029;&#20986;&#30456;&#20851;&#20449;&#24687;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;&#22312;&#27492;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#19968;&#32452;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#38543;&#26426;&#36873;&#25321;&#22522;&#32447;&#30340;&#24179;&#22343;&#20540;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#27599;&#20010; GPT2 &#21644; GPT3 &#27169;&#22411;&#26377;&#26174;&#30528;&#30340; 12.5% &#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that la
&lt;/p&gt;</description></item><item><title>ConvLab-3&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#23545;&#35805;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#22522;&#20110;&#32479;&#19968;&#25968;&#25454;&#26684;&#24335;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#25903;&#25345;&#36801;&#31227;&#23398;&#20064;&#21644;RL&#65292;&#24182;&#25552;&#20379;&#24555;&#36895;&#24320;&#21457;&#21644;&#35780;&#20272;&#40065;&#26834;&#30340;&#23545;&#35805;&#31574;&#30053;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.17148</link><description>&lt;p&gt;
ConvLab-3&#65306;&#22522;&#20110;&#32479;&#19968;&#25968;&#25454;&#26684;&#24335;&#30340;&#28789;&#27963;&#23545;&#35805;&#31995;&#32479;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data Format. (arXiv:2211.17148v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17148
&lt;/p&gt;
&lt;p&gt;
ConvLab-3&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#23545;&#35805;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#22522;&#20110;&#32479;&#19968;&#25968;&#25454;&#26684;&#24335;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#25903;&#25345;&#36801;&#31227;&#23398;&#20064;&#21644;RL&#65292;&#24182;&#25552;&#20379;&#24555;&#36895;&#24320;&#21457;&#21644;&#35780;&#20272;&#40065;&#26834;&#30340;&#23545;&#35805;&#31574;&#30053;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#65288;TOD&#65289;&#20316;&#20026;&#25968;&#23383;&#21161;&#25163;&#65292;&#21487;&#20197;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#35832;&#22914;&#39044;&#35746;&#33322;&#29677;&#25110;&#23547;&#25214;&#39184;&#21381;&#31561;&#21508;&#31181;&#20219;&#21153;&#12290;&#30446;&#21069;&#29992;&#20110;&#26500;&#24314;TOD&#31995;&#32479;&#30340;&#24037;&#20855;&#21253;&#36890;&#24120;&#22312;&#25552;&#20379;&#20840;&#38754;&#30340;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#23454;&#39564;&#29615;&#22659;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#29992;&#25143;&#20307;&#39564;&#20063;&#19981;&#21451;&#22909;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ConvLab-3&#65306;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#23545;&#35805;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#32479;&#19968;&#25968;&#25454;&#26684;&#24335;&#31616;&#21270;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#30740;&#31350;&#24191;&#27867;&#27867;&#21270;&#21644;&#36801;&#31227;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24037;&#20855;&#30340;&#22686;&#24378;&#65292;&#21253;&#25324;&#31616;&#21270;&#30340;&#35757;&#32451;&#36807;&#31243;&#12289;&#28145;&#20837;&#30340;&#35780;&#20272;&#24037;&#20855;&#20197;&#21450;&#22810;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#36873;&#25321;&#65292;ConvLab-3&#25903;&#25345;&#24555;&#36895;&#24320;&#21457;&#21644;&#35780;&#20272;&#40065;&#26834;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;RL&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;ConvLab-3&#19981;&#20165;&#26159;&#19968;&#27454;&#24378;&#22823;&#30340;&#24037;&#20855;&#20379;&#32463;&#39564;&#20016;&#23500;&#30340;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue (TOD) systems function as digital assistants, guiding users through various tasks such as booking flights or finding restaurants. Existing toolkits for building TOD systems often fall short of in delivering comprehensive arrays of data, models, and experimental environments with a user-friendly experience. We introduce ConvLab-3: a multifaceted dialogue system toolkit crafted to bridge this gap. Our unified data format simplifies the integration of diverse datasets and models, significantly reducing complexity and cost for studying generalization and transfer. Enhanced with robust reinforcement learning (RL) tools, featuring a streamlined training process, in-depth evaluation tools, and a selection of user simulators, ConvLab-3 supports the rapid development and evaluation of robust dialogue policies. Through an extensive study, we demonstrate the efficacy of transfer learning and RL and showcase that ConvLab-3 is not only a powerful tool for seasoned researchers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24182;&#34892;BERT&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#20551;&#26032;&#38395;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#24182;&#34892;&#30340;BERT&#32593;&#32476;&#20998;&#21035;&#32534;&#30721;&#26032;&#38395;&#26631;&#39064;&#21644;&#26032;&#38395;&#27491;&#25991;&#65292;&#24182;&#19988;&#32771;&#34385;&#21040;BERT&#32593;&#32476;&#30340;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20551;&#26032;&#38395;&#30340;&#30495;&#23454;&#24615;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2204.04793</link><description>&lt;p&gt;
&#20351;&#29992;&#24182;&#34892;BERT&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fake news detection using parallel BERT deep neural networks. (arXiv:2204.04793v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24182;&#34892;BERT&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#20551;&#26032;&#38395;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#24182;&#34892;&#30340;BERT&#32593;&#32476;&#20998;&#21035;&#32534;&#30721;&#26032;&#38395;&#26631;&#39064;&#21644;&#26032;&#38395;&#27491;&#25991;&#65292;&#24182;&#19988;&#32771;&#34385;&#21040;BERT&#32593;&#32476;&#30340;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20551;&#26032;&#38395;&#30340;&#30495;&#23454;&#24615;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26159;&#31038;&#20132;&#32593;&#32476;&#21644;&#23186;&#20307;&#38754;&#20020;&#30340;&#19968;&#20010;&#26085;&#30410;&#20005;&#37325;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#22810;&#24180;&#26469;&#26816;&#27979;&#20551;&#26032;&#38395;&#19968;&#30452;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#26368;&#36817;&#20960;&#24180;&#31038;&#20132;&#32593;&#32476;&#30340;&#21457;&#23637;&#21644;&#26032;&#38395;&#20256;&#25773;&#36895;&#24230;&#30340;&#22686;&#21152;&#20043;&#21518;&#65292;&#36825;&#20010;&#38382;&#39064;&#20877;&#27425;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20960;&#31181;&#26041;&#27861;&#20043;&#19968;&#26159;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26681;&#25454;&#25991;&#26412;&#26679;&#24335;&#26469;&#26816;&#27979;&#20551;&#26032;&#38395;&#12290;&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#24120;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#20043;&#19968;&#26159;&#20351;&#29992;transformers&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;BERT&#26159;&#26368;&#26377;&#24076;&#26395;&#30340;transformer&#20043;&#19968;&#65292;&#23427;&#22312;&#35768;&#22810;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MWPBert&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#24182;&#34892;&#30340;BERT&#32593;&#32476;&#23545;&#20840;&#25991;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#30495;&#23454;&#24615;&#26816;&#27979;&#12290;&#20854;&#20013;&#19968;&#20010;BERT&#32593;&#32476;&#32534;&#30721;&#26032;&#38395;&#26631;&#39064;&#65292;&#21478;&#19968;&#20010;BERT&#32593;&#32476;&#32534;&#30721;&#26032;&#38395;&#27491;&#25991;&#12290;&#30001;&#20110;BERT&#32593;&#32476;&#30340;&#36755;&#20837;&#38271;&#24230;&#26159;&#26377;&#38480;&#19988;&#24120;&#25968;&#65292;&#32780;&#26032;&#38395;&#27491;&#25991;&#36890;&#24120;&#26159;&#19968;&#27573;&#38271;&#25991;&#26412;&#65292;&#25105;&#20204;&#26080;&#27861;&#23558;&#25972;&#20010;&#26032;&#38395;&#25991;&#26412;&#36755;&#20837;&#21040;&#32593;&#32476;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fake news is a growing challenge for social networks and media. Detection of fake news always has been a problem for many years, but after the evolution of social networks and increasing speed of news dissemination in recent years has been considered again. There are several approaches to solving this problem, one of which is to detect fake news based on its text style using deep neural networks. In recent years, one of the most used forms of deep neural networks for natural language processing is transfer learning with transformers. BERT is one of the most promising transformers who outperforms other models in many NLP benchmarks. This article, we introduce MWPBert, which uses two parallel BERT networks to perform veracity detection on full-text news articles. One of the BERT networks encodes news headline, and another encodes news body. Since the input length of the BERT network is limited and constant and the news body is usually a long text, we cannot fed the whole news text into t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#26816;&#27979;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;BLM-17m&#65292;&#28085;&#30422;&#20102;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#26399;&#38388;&#30340;17&#30334;&#19975;&#25512;&#25991;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;TF-IDF&#21644;LDA&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2105.01331</link><description>&lt;p&gt;
BLM-17m: &#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#26816;&#27979;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter. (arXiv:2105.01331v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.01331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#26816;&#27979;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;BLM-17m&#65292;&#28085;&#30422;&#20102;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#26399;&#38388;&#30340;17&#30334;&#19975;&#25512;&#25991;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;TF-IDF&#21644;LDA&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26435;&#20445;&#25252;&#26159;&#19990;&#30028;&#19978;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#28085;&#30422;&#26368;&#36817;&#20960;&#20010;&#26376;&#20840;&#29699;&#24433;&#21709;&#28145;&#36828;&#30340;&#20154;&#26435;&#30683;&#30462;&#20043;&#19968;&#8212;&#8212;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;17&#30334;&#19975;&#25512;&#25991;&#30340;&#20027;&#39064;&#26816;&#27979;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25512;&#25991;&#26159;&#20174;2020&#24180;5&#26376;25&#26085;&#33267;2020&#24180;8&#26376;21&#26085;&#25910;&#38598;&#30340;&#65292;&#28085;&#30422;&#20102;&#36825;&#19968;&#20107;&#20214;&#24320;&#22987;&#21518;&#30340;89&#22825;&#12290;&#25105;&#20204;&#36890;&#36807;&#30417;&#27979;&#20840;&#29699;&#21644;&#26412;&#22320;&#25253;&#32440;&#30340;&#26368;&#28909;&#38376;&#26032;&#38395;&#20027;&#39064;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26631;&#35760;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;TF-IDF&#21644;LDA&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;k&#20540;&#23545;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;https://github.com/MeysamAsgariC/BLMT &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protection of human rights is one of the most important problems of our world. In this paper, our aim is to provide a dataset which covers one of the most significant human rights contradiction in recent months affected the whole world, George Floyd incident. We propose a labeled dataset for topic detection that contains 17 million tweets. These Tweets are collected from 25 May 2020 to 21 August 2020 that covers 89 days from start of this incident. We labeled the dataset by monitoring most trending news topics from global and local newspapers. Apart from that, we present two baselines, TF-IDF and LDA. We evaluated the results of these two methods with three different k values for metrics of precision, recall and f1-score. The collected dataset is available at https://github.com/MeysamAsgariC/BLMT.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#32463;&#20856;&#36923;&#36753;&#20013;&#21435;&#38500;&#19981;&#27491;&#30830;&#30340;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#36923;&#36753;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26080;&#27861;&#25512;&#23548;&#20986;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#25110;&#20854;&#21542;&#23450;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#39044;&#26399;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/1312.7832</link><description>&lt;p&gt;
&#23450;&#20041;&#32463;&#20856;&#36923;&#36753;&#30340;&#34164;&#28085;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Defining implication relation for classical logic. (arXiv:1312.7832v10 [math.LO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1312.7832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#32463;&#20856;&#36923;&#36753;&#20013;&#21435;&#38500;&#19981;&#27491;&#30830;&#30340;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#36923;&#36753;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26080;&#27861;&#25512;&#23548;&#20986;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#25110;&#20854;&#21542;&#23450;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#39044;&#26399;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32463;&#20856;&#36923;&#36753;&#20013;&#65292;&#8220;P&#34164;&#28085;Q&#8221;&#31561;&#20215;&#20110;&#8220;&#38750;P&#25110;Q&#8221;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#31181;&#31561;&#20215;&#20851;&#31995;&#23384;&#22312;&#38382;&#39064;&#12290;&#23454;&#38469;&#19978;&#65292;&#20174;&#8220;P&#34164;&#28085;Q&#8221;&#21487;&#20197;&#25512;&#20986;&#8220;&#38750;P&#25110;Q&#8221;&#65288;&#8220;&#34164;&#28085;&#21040;&#26512;&#21462;&#8221;&#26159;&#27491;&#30830;&#30340;&#65289;&#65292;&#32780;&#20174;&#8220;&#38750;P&#25110;Q&#8221;&#36890;&#24120;&#19981;&#33021;&#25512;&#20986;&#8220;P&#34164;&#28085;Q&#8221;&#65288;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#36890;&#24120;&#26159;&#19981;&#25104;&#31435;&#30340;&#65289;&#65292;&#25152;&#20197;&#23427;&#20204;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#36890;&#24120;&#26159;&#26080;&#25928;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#32463;&#20856;&#36923;&#36753;&#65288;CL&#65289;&#20013;&#21435;&#38500;&#19981;&#27491;&#30830;&#30340;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25152;&#26399;&#26395;&#23646;&#24615;&#30340;&#36923;&#36753;&#31995;&#32479;&#65288;IRL&#65289;&#65306;(1) &#36890;&#36807;&#23558;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#28155;&#21152;&#21040;IRL&#20013;&#21487;&#20197;&#31616;&#21333;&#22320;&#24471;&#21040;CL&#65307;(2) &#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#19982;IRL&#26080;&#20851;&#65288;&#26080;&#27861;&#22312;IRL&#20013;&#25512;&#23548;&#20986;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#25110;&#20854;&#21542;&#23450;&#65289;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;IRL&#23601;&#26159;&#36890;&#36807;&#20174;CL&#20013;&#20934;&#30830;&#22320;&#21435;&#38500;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#32780;&#24471;&#21040;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classical logic, "P implies Q" is equivalent to "not-P or Q". It is well known that the equivalence is problematic. Actually, from "P implies Q", "not-P or Q" can be inferred ("Implication-to-disjunction" is valid), while from "not-P or Q", "P implies Q" cannot be inferred in general ("Disjunction-to-implication" is not generally valid), so the equivalence between them is invalid in general. This work aims to remove exactly the incorrect Disjunction-to-implication from classical logic (CL). The paper proposes a logical system (IRL) with the expected properties: (1) CL is simply obtained by adding Disjunction-to-implication to IRL, and (2) Disjunction-to-implication is independent of IRL (either Disjunction-to-implication or its negation cannot be derived in IRL) in the general case. In other words, IRL is just the system obtained by exactly removing Disjunction-to-implication from CL.
&lt;/p&gt;</description></item></channel></rss>