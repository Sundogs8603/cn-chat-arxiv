<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>ChatGPT&#22312;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#32771;&#35797;&#20013;&#21462;&#24471;20.5&#20998;&#30340;&#25104;&#32489;&#65292;&#34920;&#29616;&#20986;&#33021;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22823;&#23398;&#32771;&#35797;&#20013;&#25104;&#21151;&#65292;&#20294;&#19981;&#33021;&#35828;&#26126;&#20854;&#23545;&#35745;&#31639;&#26426;&#31185;&#23398;&#26377;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.09461</link><description>&lt;p&gt;
ChatGPT&#21442;&#21152;&#35745;&#31639;&#26426;&#31185;&#23398;&#32771;&#35797;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Participates in a Computer Science Exam. (arXiv:2303.09461v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09461
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#32771;&#35797;&#20013;&#21462;&#24471;20.5&#20998;&#30340;&#25104;&#32489;&#65292;&#34920;&#29616;&#20986;&#33021;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22823;&#23398;&#32771;&#35797;&#20013;&#25104;&#21151;&#65292;&#20294;&#19981;&#33021;&#35828;&#26126;&#20854;&#23545;&#35745;&#31639;&#26426;&#31185;&#23398;&#26377;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35201;&#27714;ChatGPT&#21442;&#21152;&#8220;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#8221;&#30340;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#32771;&#35797;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#31243;&#24207;&#22312;&#25972;&#20010;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#23558;&#20854;&#31572;&#26696;&#25163;&#21160;&#22797;&#21046;&#21040;&#32771;&#35797;&#31572;&#39064;&#32440;&#19978;&#65292;&#19982;&#20854;&#20182;200&#21517;&#23398;&#29983;&#19968;&#36215;&#36827;&#34892;&#21311;&#21517;&#35780;&#20998;&#12290;&#25105;&#20204;&#21457;&#29616;ChatGPT&#21193;&#24378;&#36890;&#36807;&#20102;&#32771;&#35797;&#65292;&#33719;&#24471;&#20102;40&#20998;&#20013;&#30340;20.5&#20998;&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#30830;&#23454;&#21487;&#20197;&#22312;&#20687;&#22823;&#23398;&#32771;&#35797;&#36825;&#26679;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#25104;&#21151;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#32771;&#35797;&#20013;&#30340;&#20219;&#21153;&#22312;&#32467;&#26500;&#19978;&#19982;&#20854;&#20182;&#22312;&#32447;&#21487;&#25214;&#21040;&#30340;&#32771;&#35797;&#21367;&#12289;&#23436;&#25104;&#30340;&#20316;&#19994;&#38382;&#39064;&#21644;&#25945;&#23398;&#26448;&#26009;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#20174;&#36825;&#20010;&#23454;&#39564;&#20013;&#24471;&#20986;ChatGPT&#26377;&#20219;&#20309;&#35745;&#31639;&#26426;&#31185;&#23398;&#29702;&#35299;&#30340;&#32467;&#35770;&#26159;&#20026;&#26102;&#36807;&#26089;&#30340;&#12290;&#25105;&#20204;&#19982;ChatGPT&#30340;&#35848;&#35805;&#35760;&#24405;&#21487;&#20197;&#22312;\url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}&#19978;&#25214;&#21040;&#65292;&#25972;&#20010;&#35780;&#20998;&#32771;&#35797;&#22312;&#26412;&#25991;&#30340;&#38468;&#24405;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We asked ChatGPT to participate in an undergraduate computer science exam on ''Algorithms and Data Structures''. We evaluated the program on the entire exam as posed to the students. We hand-copied its answers onto an exam sheet, which was subsequently graded in a blind setup alongside those of 200 participating students. We find that ChatGPT narrowly passed the exam, obtaining 20.5 out of 40 points. This impressive performance indicates that ChatGPT can indeed succeed in challenging tasks like university exams. At the same time, the tasks in our exam are structurally similar to those on other exams, solved homework problems, and teaching materials that can be found online. Therefore, it would be premature to conclude from this experiment that ChatGPT has any understanding of computer science. The transcript of our conversation with ChatGPT is available at \url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}, and the entire graded exam is in the appendix of this paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#36328;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22810;&#35821;&#31181;&#25968;&#25454;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26631;&#35760;&#30340;&#36716;&#24405;&#19978;&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#35777;&#26126;&#22810;&#35821;&#31181;&#27169;&#22411;&#24615;&#33021;&#20248;&#36234;&#65292;&#20351;&#29992;&#26356;&#30456;&#20284;&#30340;&#35821;&#35328;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#19978;&#36827;&#34892;&#24494;&#35843;&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2303.09455</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#35270;&#35273;&#35821;&#38899;&#34920;&#31034;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Cross-lingual Visual Speech Representations. (arXiv:2303.09455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#36328;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22810;&#35821;&#31181;&#25968;&#25454;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26631;&#35760;&#30340;&#36716;&#24405;&#19978;&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#35777;&#26126;&#22810;&#35821;&#31181;&#27169;&#22411;&#24615;&#33021;&#20248;&#36234;&#65292;&#20351;&#29992;&#26356;&#30456;&#20284;&#30340;&#35821;&#35328;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#19978;&#36827;&#34892;&#24494;&#35843;&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#36817;&#24180;&#26469;&#36880;&#28176;&#27969;&#34892;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#24403;&#21069;&#30456;&#20851;&#24037;&#20316;&#20165;&#38480;&#20110;&#21033;&#29992;&#38899;&#39057;&#20449;&#21495;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#36328;&#35821;&#35328;&#33258;&#30417;&#30563;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;RAVEn&#26694;&#26550;&#65292;&#23545;&#26410;&#26631;&#35760;&#30340;&#22810;&#35821;&#31181;&#25968;&#25454;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26631;&#35760;&#30340;&#36716;&#24405;&#19978;&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;1&#65289;&#20855;&#26377;&#26356;&#22810;&#25968;&#25454;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#31181;&#27169;&#22411;&#65292;&#20294;&#24403;&#25968;&#25454;&#37327;&#22266;&#23450;&#26102;&#65292;&#21333;&#35821;&#31181;&#27169;&#22411;&#24448;&#24448;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#65307;&#65288;2&#65289;&#22810;&#35821;&#31181;&#20248;&#20110;&#20165;&#33521;&#35821;&#39044;&#35757;&#32451;&#65307;&#65288;3&#65289;&#20351;&#29992;&#26356;&#30456;&#20284;&#30340;&#35821;&#35328;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#65307;&#65288;4&#65289;&#22312;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#19978;&#36827;&#34892;&#24494;&#35843;&#19982;&#22312;&#39044;&#35757;&#32451;&#38598;&#20013;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#30456;&#24403;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#33021;&#21551;&#21457;&#26410;&#26469;&#20851;&#20110;&#38750;&#33521;&#35821;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual self-supervised learning has been a growing research topic in the last few years. However, current works only explored the use of audio signals to create representations. In this work, we study cross-lingual self-supervised visual representation learning. We use the recently-proposed Raw Audio-Visual Speech Encoders (RAVEn) framework to pre-train an audio-visual model with unlabelled multilingual data, and then fine-tune the visual model on labelled transcriptions. Our experiments show that: (1) multi-lingual models with more data outperform monolingual ones, but, when keeping the amount of data fixed, monolingual models tend to reach better performance; (2) multi-lingual outperforms English-only pre-training; (3) using languages which are more similar yields better results; and (4) fine-tuning on unseen languages is competitive to using the target language in the pre-training set. We hope our study inspires future research on non-English-only speech representation learni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#21363;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#27492;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.09446</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#36755;&#20837;&#25511;&#21046;&#39640;&#32500;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Controlling High-Dimensional Data With Sparse Input. (arXiv:2303.09446v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#21363;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#27492;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#20154;&#22312;&#29615;&#36335;&#25511;&#21046;&#29983;&#25104;&#39640;&#24230;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#32570;&#20047;&#26377;&#25928;&#30340;&#25509;&#21475;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#36755;&#20986;&#65292;&#36825;&#20010;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29992;&#25143;&#25110;&#25163;&#21160;&#25506;&#32034;&#19981;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25110;&#32773;&#36153;&#21147;&#22320;&#27880;&#37322;&#25968;&#25454;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#20110;&#25511;&#21046;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#20013;&#30340;&#38901;&#24459;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#31216;&#20026;&#22810;&#23454;&#20363;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(MICVAE)&#65292;&#23427;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#32534;&#30721;&#31232;&#30095;&#30340;&#38901;&#24459;&#29305;&#24449;&#24182;&#36755;&#20986;&#23436;&#25972;&#30340;&#27874;&#24418;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;MICVAE&#34920;&#29616;&#20986;&#20102;&#31232;&#30095;&#30340;&#20154;&#22312;&#29615;&#36335;&#25511;&#21046;&#26426;&#21046;&#25152;&#38656;&#30340;&#33391;&#22909;&#21697;&#36136;&#65306;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#12290;&#21363;&#20351;&#21482;&#26377;&#38750;&#24120;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;(~4)&#65292;MICVAE&#20063;&#33021;&#35753;&#29992;&#25143;&#23454;&#29616;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of human-in-the-loop control for generating highly-structured data. This task is challenging because existing generative models lack an efficient interface through which users can modify the output. Users have the option to either manually explore a non-interpretable latent space, or to laboriously annotate the data with conditioning labels. To solve this, we introduce a novel framework whereby an encoder maps a sparse, human interpretable control space onto the latent space of a generative model. We apply this framework to the task of controlling prosody in text-to-speech synthesis. We propose a model, called Multiple-Instance CVAE (MICVAE), that is specifically designed to encode sparse prosodic features and output complete waveforms. We show empirically that MICVAE displays desirable qualities of a sparse human-in-the-loop control mechanism: efficiency, robustness, and faithfulness. With even a very small number of input values (~4), MICVAE enables users to im
&lt;/p&gt;</description></item><item><title>Trustera&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#23458;&#25143;&#21644;&#20195;&#29702;&#21830;&#30340;&#23454;&#26102;&#23545;&#35805;&#20013;&#23631;&#34109;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#30340;&#31995;&#32479;&#65292;&#20197;&#20445;&#25252;&#25935;&#24863;&#20449;&#24687;&#19981;&#34987;&#27844;&#38706;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#35805;&#33258;&#28982;&#24615;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#23454;&#26102;&#38899;&#39057;&#23631;&#34109;&#27169;&#22359;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#21487;&#20197;&#20943;&#23569;PII&#34987;&#25318;&#25130;&#25110;&#23384;&#20648;&#22312;&#19981;&#23433;&#20840;&#25968;&#25454;&#23384;&#20648;&#20013;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2303.09438</link><description>&lt;p&gt;
Trustera&#65306;&#19968;&#31181;&#23454;&#26102;&#23631;&#34109;&#35848;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Trustera: A Live Conversation Redaction System. (arXiv:2303.09438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09438
&lt;/p&gt;
&lt;p&gt;
Trustera&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#23458;&#25143;&#21644;&#20195;&#29702;&#21830;&#30340;&#23454;&#26102;&#23545;&#35805;&#20013;&#23631;&#34109;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#30340;&#31995;&#32479;&#65292;&#20197;&#20445;&#25252;&#25935;&#24863;&#20449;&#24687;&#19981;&#34987;&#27844;&#38706;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#35805;&#33258;&#28982;&#24615;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#23454;&#26102;&#38899;&#39057;&#23631;&#34109;&#27169;&#22359;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#21487;&#20197;&#20943;&#23569;PII&#34987;&#25318;&#25130;&#25110;&#23384;&#20648;&#22312;&#19981;&#23433;&#20840;&#25968;&#25454;&#23384;&#20648;&#20013;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Trustera&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#22312;&#23454;&#26102;&#35848;&#35805;&#20013;&#23631;&#34109;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#65288;PII&#65289;&#20197;&#28040;&#38500;&#20195;&#29702;&#21830;&#38656;&#35201;&#21548;&#21040;&#25935;&#24863;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#29616;&#22330;&#23458;&#25143;-&#20195;&#29702;&#21830;&#23545;&#35805;&#33258;&#28982;&#24615;&#30340;&#21151;&#33021;&#31995;&#32479;&#12290;&#19982;&#36890;&#35805;&#21518;&#30340;&#23631;&#34109;&#30456;&#27604;&#65292;&#38899;&#39057;&#23631;&#34109;&#24320;&#22987;&#20110;&#23458;&#25143;&#24320;&#22987;&#19982;PII&#23454;&#20307;&#20132;&#35848;&#26102;&#12290;&#36825;&#26174;&#30528;&#38477;&#20302;&#20102;PII&#34987;&#25318;&#25130;&#25110;&#23384;&#20648;&#22312;&#19981;&#23433;&#20840;&#25968;&#25454;&#23384;&#20648;&#20013;&#30340;&#39118;&#38505;&#12290;Trustera&#30340;&#26550;&#26500;&#30001;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#23454;&#26102;&#38899;&#39057;&#23631;&#34109;&#27169;&#22359;&#30340;&#31649;&#36947;&#32452;&#25104;&#12290;&#31995;&#32479;&#30340;&#30446;&#26631;&#26159;&#19977;&#37325;&#30340;&#65306;&#23631;&#34109;PII&#23454;&#20307;&#12289;&#25513;&#30422;&#21457;&#36865;&#32473;&#20195;&#29702;&#21830;&#30340;&#38899;&#39057;&#65292;&#21516;&#26102;&#25429;&#33719;&#23454;&#20307;&#65292;&#20197;&#20415;&#25429;&#33719;&#30340;PII&#21487;&#29992;&#20110;&#20184;&#27454;&#20132;&#26131;&#25110;&#26469;&#30005;&#32773;&#35782;&#21035;&#12290;Trustera&#30446;&#21069;&#27491;&#22312;&#34987;&#25968;&#21315;&#21517;&#20195;&#29702;&#21830;&#20351;&#29992;&#65292;&#20197;&#20445;&#25252;&#23458;&#25143;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustera, the first functional system that redacts personally identifiable information (PII) in real-time spoken conversations to remove agents' need to hear sensitive information while preserving the naturalness of live customer-agent conversations. As opposed to post-call redaction, audio masking starts as soon as the customer begins speaking to a PII entity. This significantly reduces the risk of PII being intercepted or stored in insecure data storage. Trustera's architecture consists of a pipeline of automatic speech recognition, natural language understanding, and a live audio redactor module. The system's goal is three-fold: redact entities that are PII, mask the audio that goes to the agent, and at the same time capture the entity, so that the captured PII can be used for a payment transaction or caller identification. Trustera is currently being used by thousands of agents to secure customers' sensitive information.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#23558;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#20026;&#26368;&#32456;&#34920;&#31034;&#65292;&#32469;&#36807;&#20013;&#38388;&#30340;Transformer&#35745;&#31639;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20013;&#21487;&#8220;&#31397;&#35270;&#8221;GPT-2&#21644;BERT&#30340;&#26089;&#26399;&#23618;&#34920;&#31034;&#65292;&#26174;&#31034;&#32463;&#24120;&#22312;&#26089;&#26399;&#23618;&#20013;LMs&#24050;&#32463;&#39044;&#27979;&#26368;&#32456;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.09435</link><description>&lt;p&gt;
&#36339;&#36291;&#21040;&#32467;&#35770;&#65306;&#29992;&#32447;&#24615;&#21464;&#25442;&#31616;&#21270;Transformers
&lt;/p&gt;
&lt;p&gt;
Jump to Conclusions: Short-Cutting Transformers With Linear Transformations. (arXiv:2303.09435v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#23558;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#20026;&#26368;&#32456;&#34920;&#31034;&#65292;&#32469;&#36807;&#20013;&#38388;&#30340;Transformer&#35745;&#31639;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20013;&#21487;&#8220;&#31397;&#35270;&#8221;GPT-2&#21644;BERT&#30340;&#26089;&#26399;&#23618;&#34920;&#31034;&#65292;&#26174;&#31034;&#32463;&#24120;&#22312;&#26089;&#26399;&#23618;&#20013;LMs&#24050;&#32463;&#39044;&#27979;&#26368;&#32456;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;(LMs)&#22312;&#27599;&#20010;&#23618;&#27425;&#19978;&#37117;&#21019;&#24314;&#20854;&#36755;&#20837;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#20294;&#21482;&#20351;&#29992;&#26368;&#32456;&#23618;&#30340;&#34920;&#31034;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#30340;&#20869;&#37096;&#20915;&#31574;&#36807;&#31243;&#21644;&#20013;&#38388;&#34920;&#31034;&#30340;&#23454;&#29992;&#24615;&#21464;&#24471;&#27169;&#31946;&#19981;&#28165;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#19968;&#28857;&#65292;&#21487;&#20197;&#23558;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#20026;&#26368;&#32456;&#34920;&#31034;&#65292;&#32469;&#36807;&#20013;&#38388;&#30340;Transformer&#35745;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#26469;&#36827;&#34892;&#36825;&#31181;&#36716;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#27604;&#30446;&#21069;&#27969;&#34892;&#30340;&#22312;&#26368;&#32456;&#23618;&#31354;&#38388;&#20013;&#26816;&#26597;&#25152;&#26377;&#23618;&#30340;&#38544;&#34255;&#34920;&#31034;&#30340;&#26041;&#27861;&#26356;&#20934;&#30830;&#30340;&#36817;&#20284;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#8220;&#31397;&#35270;&#8221;GPT-2&#21644;BERT&#30340;&#26089;&#26399;&#23618;&#34920;&#31034;&#65292;&#26174;&#31034;&#32463;&#24120;&#22312;&#26089;&#26399;&#23618;&#20013;LMs&#24050;&#32463;&#39044;&#27979;&#26368;&#32456;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#26368;&#36817;&#30340;&#26089;&#26399;&#36864;&#20986;&#31574;&#30053;&#30340;&#23454;&#29992;&#24615;&#65292;&#34920;&#26126;&#24403;&#26088;&#22312;&#8230;&#8230;(&#21407;&#25991;&#25130;&#27490;)
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models (LMs) create hidden representations of their inputs at every layer, but only use final-layer representations for prediction. This obscures the internal decision-making process of the model and the utility of its intermediate representations. One way to elucidate this is to cast the hidden representations as final representations, bypassing the transformer computation in-between. In this work, we suggest a simple method for such casting, by using linear transformations. We show that our approach produces more accurate approximations than the prevailing practice of inspecting hidden representations from all layers in the space of the final layer. Moreover, in the context of language modeling, our method allows "peeking" into early layer representations of GPT-2 and BERT, showing that often LMs already predict the final output in early layers. We then demonstrate the practicality of our method to recent early exit strategies, showing that when aiming, for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Team SheffieldVeraAI&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#30340;&#34920;&#29616;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#26032;&#38395;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#20998;&#31867;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#26041;&#27861;&#12290;&#35813;&#22242;&#38431;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#65292;&#21462;&#24471;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#19979;&#30340;&#22909;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2303.09421</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;3&#20013;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#26041;&#27861;&#65306;Team SheffieldVeraAI&#22312;&#26032;&#38395;&#31867;&#22411;&#12289;&#20027;&#39064;&#21644;&#35828;&#26381;&#25216;&#24039;&#20998;&#31867;&#26041;&#38754;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Team SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches for news genre, topic and persuasion technique classification. (arXiv:2303.09421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Team SheffieldVeraAI&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#30340;&#34920;&#29616;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#26032;&#38395;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#20998;&#31867;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#26041;&#27861;&#12290;&#35813;&#22242;&#38431;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#65292;&#21462;&#24471;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#19979;&#30340;&#22909;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;SemEval-2023&#20219;&#21153;3&#65306;&#22312;&#22312;&#32447;&#26032;&#38395;&#20013;&#26816;&#27979;&#31867;&#21035;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#12290; &#23545;&#20110;&#23376;&#20219;&#21153;1&#65288;&#26032;&#38395;&#31867;&#22411;&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#35757;&#32451;&#21644;&#36866;&#37197;&#22120;mBERT&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#20854;&#22312;&#24503;&#35821;&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#24182;&#19988;&#20855;&#26377;&#22810;&#35821;&#35328;&#22242;&#38431;&#20013;&#26368;&#39640;&#30340;&#24179;&#22343;&#25490;&#21517;&#12290; &#23545;&#20110;&#23376;&#20219;&#21153;2&#65288;&#26694;&#26550;&#65289;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#21333;&#29420;&#30340;&#38598;&#25104;&#65306;&#19968;&#20010;&#21333;&#35821;RoBERTa-MUPPETLARGE&#21644;&#19968;&#20010;XLM-RoBERTaLARGE&#30340;&#38598;&#25104;&#65292;&#20998;&#21035;&#20351;&#29992;&#36866;&#37197;&#22120;&#21644;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65292;&#22312;3&#31181;&#35821;&#35328;&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#65292;&#24182;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#33719;&#24471;&#26368;&#20339;&#24179;&#22343;&#25490;&#21517;&#12290; &#23545;&#20110;&#23376;&#20219;&#21153;3&#65288;&#35828;&#26381;&#25216;&#24039;&#65289;&#65292;&#25105;&#20204;&#20026;&#33521;&#35821;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#35821;&#35328;RoBERTa-Base&#27169;&#22411;&#21644;&#19968;&#20010;&#36866;&#29992;&#20110;&#21097;&#20313;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;mBERT&#27169;&#22411;&#65292;&#20854;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#22343;&#25490;&#21517;&#21069;10&#65292;&#20854;&#20013;&#33521;&#35821;&#25490;&#21517;&#31532;&#20108;&#12290; &#23545;&#20110;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our approach for SemEval-2023 Task 3: Detecting the category, the framing, and the persuasion techniques in online news in a multi-lingual setup. For Subtask 1 (News Genre), we propose an ensemble of fully trained and adapter mBERT models which was ranked joint-first for German, and had the highest mean rank of multi-language teams. For Subtask 2 (Framing), we achieved first place in 3 languages, and the best average rank across all the languages, by using two separate ensembles: a monolingual RoBERTa-MUPPETLARGE and an ensemble of XLM-RoBERTaLARGE with adapters and task adaptive pretraining. For Subtask 3 (Persuasion Techniques), we train a monolingual RoBERTa-Base model for English and a multilingual mBERT model for the remaining languages, which achieved top 10 for all languages, including 2nd for English. For each subtask, we compare monolingual and multilingual approaches, and consider class imbalance techniques.
&lt;/p&gt;</description></item><item><title>ToxVis&#26159;&#19968;&#20010;&#21487;&#35270;&#21270;&#20114;&#21160;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#23558;&#22312;&#32447;&#20869;&#23481;&#20998;&#20026;&#38544;&#24335;&#12289;&#26174;&#24335;&#21644;&#38750;&#20196;&#20154;&#21453;&#24863;&#30340;&#19977;&#31867;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#37322;&#25216;&#26415;&#25552;&#20379;&#20998;&#31867;&#32467;&#26524;&#35299;&#37322;&#65292;&#24182;&#20026;&#20102;&#35299;&#20196;&#20154;&#21453;&#24863;&#20869;&#23481;&#21644;&#25903;&#25345;&#26356;&#26377;&#25928;&#30340;&#20869;&#23481;&#23457;&#26680;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2303.09402</link><description>&lt;p&gt;
ToxVis&#65306;&#36890;&#36807;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#22686;&#24378;&#38544;&#24335;&#19982;&#26174;&#24335;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
ToxVis: Enabling Interpretability of Implicit vs. Explicit Toxicity Detection Models with Interactive Visualization. (arXiv:2303.09402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09402
&lt;/p&gt;
&lt;p&gt;
ToxVis&#26159;&#19968;&#20010;&#21487;&#35270;&#21270;&#20114;&#21160;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#23558;&#22312;&#32447;&#20869;&#23481;&#20998;&#20026;&#38544;&#24335;&#12289;&#26174;&#24335;&#21644;&#38750;&#20196;&#20154;&#21453;&#24863;&#30340;&#19977;&#31867;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#37322;&#25216;&#26415;&#25552;&#20379;&#20998;&#31867;&#32467;&#26524;&#35299;&#37322;&#65292;&#24182;&#20026;&#20102;&#35299;&#20196;&#20154;&#21453;&#24863;&#20869;&#23481;&#21644;&#25903;&#25345;&#26356;&#26377;&#25928;&#30340;&#20869;&#23481;&#23457;&#26680;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24179;&#21488;&#19978;&#20167;&#24680;&#35328;&#35770;&#30340;&#23835;&#36215;&#23548;&#33268;&#20102;&#26377;&#25928;&#20869;&#23481;&#23457;&#26680;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#21253;&#25324;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#22312;&#20869;&#30340;&#20196;&#20154;&#21453;&#24863;&#30340;&#22312;&#32447;&#20869;&#23481;&#20855;&#26377;&#20027;&#35266;&#24615;&#21644;&#22810;&#26041;&#38754;&#24615;&#65292;&#23545;&#20110;&#20154;&#31867;&#23457;&#26680;&#21592;&#21644;&#20869;&#23481;&#23457;&#26680;&#31995;&#32479;&#37117;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ToxVis&#65292;&#19968;&#20010;&#21487;&#35270;&#21270;&#20114;&#21160;&#21644;&#21487;&#35299;&#37322;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#23558;&#20196;&#20154;&#21453;&#24863;&#30340;&#35328;&#35770;&#20998;&#20026;&#19977;&#31867;&#65306;&#38544;&#24335;&#30340;&#12289;&#26174;&#24335;&#30340;&#21644;&#38750;&#20196;&#20154;&#21453;&#24863;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;RoBERTa&#12289;XLNET&#21644;GPT-3 Fine-tune&#20102;&#20004;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#37322;&#25216;&#26415;&#26469;&#25552;&#20379;&#20998;&#31867;&#32467;&#26524;&#30340;&#35299;&#37322;&#12290;ToxVis&#20351;&#29992;&#25143;&#33021;&#22815;&#36755;&#20837;&#21487;&#33021;&#30340;&#20196;&#20154;&#21453;&#24863;&#30340;&#25991;&#26412;&#65292;&#24182;&#33719;&#24471;&#20998;&#31867;&#32467;&#26524;&#20197;&#21450;&#21738;&#20123;&#21333;&#35789;&#23545;&#35813;&#20915;&#31574;&#20570;&#20986;&#20102;&#26368;&#22823;&#30340;&#36129;&#29486;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#12290;&#36890;&#36807;&#20351;&#20998;&#31867;&#36807;&#31243;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;ToxVis&#20026;&#20102;&#35299;&#20196;&#20154;&#21453;&#24863;&#30340;&#20869;&#23481;&#30340;&#24494;&#22937;&#20043;&#22788;&#20197;&#21450;&#25903;&#25345;&#26356;&#26377;&#25928;&#30340;&#20869;&#23481;&#23457;&#26680;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of hate speech on online platforms has led to an urgent need for effective content moderation. However, the subjective and multi-faceted nature of hateful online content, including implicit hate speech, poses significant challenges to human moderators and content moderation systems. To address this issue, we developed ToxVis, a visually interactive and explainable tool for classifying hate speech into three categories: implicit, explicit, and non-hateful. We fine-tuned two transformer-based models using RoBERTa, XLNET, and GPT-3 and used deep learning interpretation techniques to provide explanations for the classification results. ToxVis enables users to input potentially hateful text and receive a classification result along with a visual explanation of which words contributed most to the decision. By making the classification process explainable, ToxVis provides a valuable tool for understanding the nuances of hateful content and supporting more effective content moderation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21382;&#21490;&#20215;&#26684;&#21644;Twitter&#24773;&#24863;&#20998;&#26512;&#39044;&#27979;&#27604;&#29305;&#24065;&#20215;&#26684;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25928;&#26524;&#12290;&#24773;&#24863;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;9.45&#65285;&#65292;&#20215;&#26684;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;3.6&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.09397</link><description>&lt;p&gt;
&#21033;&#29992;Twitter&#24773;&#24863;&#20998;&#26512;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;
&lt;/p&gt;
&lt;p&gt;
Cryptocurrency Price Prediction using Twitter Sentiment Analysis. (arXiv:2303.09397v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21382;&#21490;&#20215;&#26684;&#21644;Twitter&#24773;&#24863;&#20998;&#26512;&#39044;&#27979;&#27604;&#29305;&#24065;&#20215;&#26684;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25928;&#26524;&#12290;&#24773;&#24863;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;9.45&#65285;&#65292;&#20215;&#26684;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;3.6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21152;&#23494;&#36135;&#24065;&#30340;&#27874;&#21160;&#24615;&#21450;&#22810;&#26679;&#21270;&#30340;&#24847;&#35265;&#65292;&#22312;&#35768;&#22810;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#37117;&#25104;&#20026;&#20102;&#35752;&#35770;&#30340;&#20013;&#24515;&#35805;&#39064;&#12290;Twitter &#36805;&#36895;&#25104;&#20026;&#26032;&#38395;&#26469;&#28304;&#21644;&#27604;&#29305;&#24065;&#35752;&#35770;&#30340;&#23186;&#20171;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26088;&#22312;&#21033;&#29992;&#21382;&#21490;&#20215;&#26684;&#21644;&#25512;&#25991;&#24773;&#24863;&#26469;&#39044;&#27979;&#27604;&#29305;&#24065;&#30340;&#20215;&#26684;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#21487;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#36716;&#25442;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#25512;&#25991;&#38598;&#30340;&#24773;&#24863;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#30340;&#24773;&#24863;&#20197;&#21450;&#21382;&#21490;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#25968;&#25454;&#12289;&#25512;&#25991;&#25968;&#37327;&#12289;&#29992;&#25143;&#30340;&#36861;&#38543;&#32773;&#25968;&#37327;&#20197;&#21450;&#29992;&#25143;&#26159;&#21542;&#36890;&#36807;&#39564;&#35777;&#26469;&#39044;&#27979;&#27604;&#29305;&#24065;&#30340;&#20215;&#26684;&#12290;&#24773;&#24863;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;9.45&#65285;&#65292;&#21453;&#24212;&#20102;&#23454;&#26102;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#24179;&#22343;&#35823;&#24046;&#12290;&#32780;&#20215;&#26684;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#21017;&#20026;3.6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cryptocurrency ecosystem has been the centre of discussion on many social media platforms, following its noted volatility and varied opinions. Twitter is rapidly being utilised as a news source and a medium for bitcoin discussion. Our algorithm seeks to use historical prices and sentiment of tweets to forecast the price of Bitcoin. In this study, we develop an end-to-end model that can forecast the sentiment of a set of tweets (using a Bidirectional Encoder Representations from Transformers - based Neural Network Model) and forecast the price of Bitcoin (using Gated Recurrent Unit) using the predicted sentiment and other metrics like historical cryptocurrency price data, tweet volume, a user's following, and whether or not a user is verified. The sentiment prediction gave a Mean Absolute Percentage Error of 9.45%, an average of real-time data, and test data. The mean absolute percent error for the price prediction was 3.6%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#25991;&#26412;&#25253;&#21578;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;Auto-TTE&#65292;&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#30340;12&#23548;&#32852;&#24515;&#30005;&#22270;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#24515;&#30005;&#22270;&#29983;&#25104;&#27169;&#22411;&#24182;&#19981;&#21482;&#32771;&#34385;&#21333;&#20010;&#24515;&#30005;&#20449;&#21495;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#26356;&#20855;&#22797;&#26434;&#24615;&#12289;&#26356;&#21152;&#30495;&#23454;&#30340;&#29983;&#25104;&#65292;&#22312;&#25991;&#26412;&#21040;&#24515;&#30005;&#22270;&#21512;&#25104;&#39046;&#22495;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.09395</link><description>&lt;p&gt;
&#22522;&#20110;&#20020;&#24202;&#25991;&#26412;&#25253;&#21578;&#30340;&#25991;&#26412;&#21040;&#24515;&#30005;&#22270;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text-to-ECG: 12-Lead Electrocardiogram Synthesis conditioned on Clinical Text Reports. (arXiv:2303.09395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#25991;&#26412;&#25253;&#21578;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;Auto-TTE&#65292;&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#30340;12&#23548;&#32852;&#24515;&#30005;&#22270;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#24515;&#30005;&#22270;&#29983;&#25104;&#27169;&#22411;&#24182;&#19981;&#21482;&#32771;&#34385;&#21333;&#20010;&#24515;&#30005;&#20449;&#21495;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#26356;&#20855;&#22797;&#26434;&#24615;&#12289;&#26356;&#21152;&#30495;&#23454;&#30340;&#29983;&#25104;&#65292;&#22312;&#25991;&#26412;&#21040;&#24515;&#30005;&#22270;&#21512;&#25104;&#39046;&#22495;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#21512;&#25104;&#26159;&#19968;&#31181;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#29983;&#25104;&#36924;&#30495;&#30340;&#21512;&#25104;&#24515;&#30005;&#20449;&#21495;&#20197;&#20379;&#21307;&#30103;&#20351;&#29992;&#65292;&#26080;&#38656;&#25285;&#24515;&#27880;&#37322;&#25104;&#26412;&#25110;&#20020;&#24202;&#25968;&#25454;&#38544;&#31169;&#38480;&#21046;&#12290;&#20256;&#32479;&#30340;&#24515;&#30005;&#22270;&#29983;&#25104;&#27169;&#22411;&#21482;&#32771;&#34385;&#21333;&#20010;&#24515;&#30005;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;GAN&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21482;&#33021;&#29983;&#25104;&#21333;&#23548;&#32852;&#26679;&#26412;&#65292;&#24182;&#35201;&#27714;&#27599;&#20010;&#35786;&#26029;&#20998;&#31867;&#36827;&#34892;&#21333;&#29420;&#30340;&#22521;&#35757;&#12290;&#24515;&#30005;&#22270;&#30340;&#35786;&#26029;&#20998;&#31867;&#26080;&#27861;&#25429;&#25417;&#24515;&#30005;&#22270;&#20043;&#38388;&#30340;&#22797;&#26434;&#24046;&#24322;&#65292;&#36825;&#20123;&#24046;&#24322;&#21462;&#20915;&#20110;&#21508;&#31181;&#29305;&#24449;&#65288;&#20363;&#22914;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#23398;&#32454;&#33410;&#65292;&#20849;&#23384;&#30340;&#35786;&#26029;&#20998;&#31867;&#31561;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25991;&#26412;&#21040;&#24515;&#30005;&#22270;&#20219;&#21153;&#65292;&#20854;&#20013;&#20351;&#29992;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#24515;&#30005;&#22270;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;Auto-TTE&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#20020;&#24202;&#25991;&#26412;&#25253;&#21578;&#26465;&#20214;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#21512;&#25104;12&#23548;&#32852;&#24515;&#30005;&#22270;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#25991;&#26412;&#21040;&#24515;&#30005;&#22270;&#39046;&#22495;&#30340;&#20854;&#20182;&#20195;&#34920;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25991;&#26412;&#21040;&#24515;&#30005;&#22270;&#21512;&#25104;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram (ECG) synthesis is the area of research focused on generating realistic synthetic ECG signals for medical use without concerns over annotation costs or clinical data privacy restrictions. Traditional ECG generation models consider a single ECG lead and utilize GAN-based generative models. These models can only generate single lead samples and require separate training for each diagnosis class. The diagnosis classes of ECGs are insufficient to capture the intricate differences between ECGs depending on various features (e.g. patient demographic details, co-existing diagnosis classes, etc.). To alleviate these challenges, we present a text-to-ECG task, in which textual inputs are used to produce ECG outputs. Then we propose Auto-TTE, an autoregressive generative model conditioned on clinical text reports to synthesize 12-lead ECGs, for the first time to our knowledge. We compare the performance of our model with other representative models in text-to-speech and text-to-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23450;&#20041;&#20102;&#19968;&#31181;MTC&#20998;&#31867;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;CFG&#27169;&#22411;&#30340;&#25277;&#21462;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;ICL&#33258;&#21160;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;DUGs&#20013;&#30340;MTC&#65292;&#26377;&#26395;&#36890;&#36807;&#23450;&#20041;&#23433;&#20840;&#30340;&#24739;&#32773;&#27963;&#21160;&#27169;&#24335;&#26469;&#25512;&#36827;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.09366</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21307;&#23398;&#26102;&#38388;&#32422;&#26463;&#25277;&#21462;&#33539;&#22260;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Scope of In-Context Learning for the Extraction of Medical Temporal Constraints. (arXiv:2303.09366v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23450;&#20041;&#20102;&#19968;&#31181;MTC&#20998;&#31867;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;CFG&#27169;&#22411;&#30340;&#25277;&#21462;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;ICL&#33258;&#21160;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;DUGs&#20013;&#30340;MTC&#65292;&#26377;&#26395;&#36890;&#36807;&#23450;&#20041;&#23433;&#20840;&#30340;&#24739;&#32773;&#27963;&#21160;&#27169;&#24335;&#26469;&#25512;&#36827;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#27835;&#30103;&#36890;&#24120;&#23545;&#24739;&#32773;&#30340;&#26085;&#24120;&#27963;&#21160;&#26045;&#21152;&#26102;&#38388;&#32422;&#26463;&#12290;&#36829;&#21453;&#21307;&#23398;&#26102;&#38388;&#32422;&#26463;&#65288;MTC&#65289;&#20250;&#23548;&#33268;&#32570;&#20047;&#27835;&#30103;&#20381;&#20174;&#24615;&#65292;&#20197;&#21450;&#19981;&#33391;&#30340;&#20581;&#24247;&#32467;&#26524;&#21644;&#22686;&#21152;&#30340;&#21307;&#30103;&#36153;&#29992;&#12290;&#36825;&#20123;MTC&#22312;&#24739;&#32773;&#25945;&#32946;&#26448;&#26009;&#21644;&#20020;&#24202;&#25991;&#26412;&#20013;&#30340;&#33647;&#29289;&#20351;&#29992;&#25351;&#21335;&#65288;DUGs&#65289;&#20013;&#34987;&#21457;&#29616;&#12290;&#36890;&#36807;&#22312;&#35745;&#31639;&#19978;&#34920;&#31034;DUGs&#20013;&#30340;MTC&#65292;&#23558;&#26377;&#21161;&#20110;&#36890;&#36807;&#24110;&#21161;&#23450;&#20041;&#23433;&#20840;&#30340;&#24739;&#32773;&#27963;&#21160;&#27169;&#24335;&#26469;&#25512;&#36827;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#24212;&#29992;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;DUGs&#20013;&#21457;&#29616;&#30340;MTC&#20998;&#31867;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#30340;&#27169;&#22411;&#26469;&#35745;&#31639;&#22320;&#34920;&#31034;MTC&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19977;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20849;&#35745;N = 836&#20010;&#24102;&#26631;&#20934;&#21270;&#30340;MTC&#26631;&#35760;&#30340;DUGs&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#33258;&#21160;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;DUGs&#20013;&#21457;&#29616;&#30340;MTC&#65292;&#36328;&#25152;&#26377;&#25968;&#25454;&#38598;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;0.62&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;ICL&#27169;&#22411;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medications often impose temporal constraints on everyday patient activity. Violations of such medical temporal constraints (MTCs) lead to a lack of treatment adherence, in addition to poor health outcomes and increased healthcare expenses. These MTCs are found in drug usage guidelines (DUGs) in both patient education materials and clinical texts. Computationally representing MTCs in DUGs will advance patient-centric healthcare applications by helping to define safe patient activity patterns. We define a novel taxonomy of MTCs found in DUGs and develop a novel context-free grammar (CFG) based model to computationally represent MTCs from unstructured DUGs. Additionally, we release three new datasets with a combined total of N = 836 DUGs labeled with normalized MTCs. We develop an in-context learning (ICL) solution for automatically extracting and normalizing MTCs found in DUGs, achieving an average F1 score of 0.62 across all datasets. Finally, we rigorously investigate ICL model perfor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;Telugu&#27468;&#26354;&#30340;&#27468;&#35789;&#24773;&#24863;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38899;&#20048;&#24773;&#24863;&#35782;&#21035;&#65292;&#30740;&#31350;&#32773;&#20351;&#29992;&#20102;&#20004;&#31181;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#24494;&#35843;&#21518;&#30340;XLMRoBERTa&#27169;&#22411;&#21487;&#20197;&#27604;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#26356;&#22909;&#30340;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09364</link><description>&lt;p&gt;
Tollywood&#24773;&#24863;&#65306;&#27888;&#21346;&#22266;&#35821;&#27468;&#35789;&#20013;&#20215;&#20540;-&#21796;&#36215;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Tollywood Emotions: Annotation of Valence-Arousal in Telugu Song Lyrics. (arXiv:2303.09364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;Telugu&#27468;&#26354;&#30340;&#27468;&#35789;&#24773;&#24863;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38899;&#20048;&#24773;&#24863;&#35782;&#21035;&#65292;&#30740;&#31350;&#32773;&#20351;&#29992;&#20102;&#20004;&#31181;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#24494;&#35843;&#21518;&#30340;XLMRoBERTa&#27169;&#22411;&#21487;&#20197;&#27604;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#26356;&#22909;&#30340;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35782;&#21035;&#38899;&#20048;&#24773;&#24863;&#26041;&#38754;&#65292;&#20154;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#22768;&#23398;&#29305;&#24449;&#12289;&#31038;&#20250;&#26631;&#31614;&#21644;&#20803;&#25968;&#25454;&#65292;&#20294;&#24456;&#23569;&#20851;&#27880;&#27468;&#35789;&#12290;&#30446;&#21069;&#27809;&#26377;&#21253;&#21547;&#20215;&#20540;&#21644;&#21796;&#36215;&#25163;&#21160;&#35780;&#20998;&#30340;&#21360;&#24230;&#35821;&#27468;&#26354;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;Spotify&#19978;&#25910;&#38598;&#20102;Telugu&#27468;&#26354;&#27468;&#35789;&#30340;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#35780;&#27880;&#20102;&#31163;&#25955;&#23610;&#24230;&#19978;&#30340;&#20215;&#20540;&#21644;&#21796;&#36215;&#12290;&#23545;&#20110;&#20215;&#20540;&#21644;&#21796;&#36215;&#65292;&#35266;&#23519;&#21040;&#30456;&#24403;&#39640;&#30340;&#20154;&#38469;&#38388;&#19968;&#33268;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#20998;&#31867;&#25216;&#26415;&#21019;&#24314;&#20102;&#20004;&#20010;&#38899;&#20048;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#20174;&#27468;&#35789;&#20013;&#35782;&#21035;&#20215;&#20540;&#12289;&#21796;&#36215;&#21644;&#30456;&#24212;&#30340;&#24773;&#24863;&#35937;&#38480;&#12290;&#20351;&#29992;&#26415;&#35821;&#39057;&#29575;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#21151;&#33021;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;XLMRoBERTa&#65288;XLM-R&#65289;&#27169;&#22411;&#29992;&#20110;&#20215;&#20540;&#65292;&#21796;&#36215;&#21644;&#35937;&#38480;&#20998;&#31867;&#20219;&#21153;&#12290;&#24494;&#35843;&#21518;&#30340;XLMRoBERTa&#20248;&#20110;SVM&#65292;&#23558;&#20215;&#20540;&#12289;&#21796;&#36215;&#21644;&#35937;&#38480;&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#20998;&#21035;&#25552;&#39640;&#20102;54.69&#65285;&#12289;67.61&#65285;&#21644;34.13&#65285;&#65292;&#36798;&#21040;77.90&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition from a given music track has heavily relied on acoustic features, social tags, and metadata but is seldom focused on lyrics. There are no datasets of Indian language songs that contain both valence and arousal manual ratings of lyrics. We present a new manually annotated dataset of Telugu songs' lyrics collected from Spotify with valence and arousal annotated on a discrete scale. A fairly high inter-annotator agreement was observed for both valence and arousal. Subsequently, we create two music emotion recognition models by using two classification techniques to identify valence, arousal and respective emotion quadrant from lyrics. Support vector machine (SVM) with term frequency-inverse document frequency (TF-IDF) features and fine-tuning the pre-trained XLMRoBERTa (XLM-R) model were used for valence, arousal and quadrant classification tasks. Fine-tuned XLMRoBERTa performs better than the SVM by improving macro-averaged F1-scores of 54.69%, 67.61%, 34.13% to 77.90
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#22312;&#39640;&#31561;&#25945;&#32946;Python&#32534;&#31243;&#35838;&#31243;&#30340;&#21021;&#32423;&#21644;&#20013;&#32423;&#35780;&#20272;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#32534;&#31243;&#38382;&#39064;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.09325</link><description>&lt;p&gt;
GPT&#26159;&#21542;&#33021;&#36890;&#36807;&#39640;&#31561;&#25945;&#32946;&#32534;&#31243;&#35838;&#31243;&#30340;&#35780;&#20272;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?. (arXiv:2303.09325v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09325
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#22312;&#39640;&#31561;&#25945;&#32946;Python&#32534;&#31243;&#35838;&#31243;&#30340;&#21021;&#32423;&#21644;&#20013;&#32423;&#35780;&#20272;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#32534;&#31243;&#38382;&#39064;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#22312;&#39640;&#31561;&#25945;&#32946;Python&#32534;&#31243;&#35838;&#31243;&#30340;&#21021;&#32423;&#21644;&#20013;&#32423;&#35780;&#20272;&#20013;&#30340;&#33021;&#21147;&#12290;&#20154;&#20204;&#23545;&#36825;&#31181;&#26032;&#20852;&#25216;&#26415;&#22312;&#32534;&#31243;&#25945;&#32946;&#26041;&#38754;&#30340;&#28508;&#22312;&#29992;&#36884;&#65288;&#20363;&#22914;&#65292;&#32451;&#20064;&#29983;&#25104;&#65292;&#20195;&#30721;&#35299;&#37322;&#65289;&#21644;&#19981;&#33391;&#29992;&#36884;&#65288;&#20363;&#22914;&#65292;&#20316;&#24330;&#65289;&#30340;&#35752;&#35770;&#24050;&#32463;&#21152; intens &#20102;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#22810;&#31181;&#35780;&#20272;&#24037;&#20855;&#30340;&#24191;&#27867;&#32534;&#31243;&#35838;&#31243;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#20005;&#26684;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#35780;&#20272;&#20174;&#31616;&#21333;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;&#19981;&#28041;&#21450;&#20195;&#30721;&#65289;&#21040;&#20195;&#30721;&#20998;&#24067;&#22312;&#22810;&#20010;&#25991;&#20214;&#20013;&#30340;&#22797;&#26434;&#32534;&#31243;&#39033;&#30446;&#30340;&#19977;&#20010;Python&#35838;&#31243;&#19978;&#35780;&#20272;&#20102;GPT&#65288;&#24635;&#20849;599&#20010;&#32451;&#20064;&#39064;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;GPT&#27169;&#22411;&#22914;&#20309;&#25104;&#21151;&#22320;&#21033;&#29992;&#33258;&#21160;&#35780;&#20998;&#22120;&#25552;&#20379;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#19981;&#33021;&#36890;&#36807;&#22312;&#39640;&#31561;&#25945;&#32946;Python&#32534;&#31243;&#35838;&#31243;&#20013;&#36890;&#24120;&#28041;&#21450;&#30340;&#23436;&#25972;&#35780;&#20272;&#24037;&#20855;&#30340;&#20840;&#35889;&#12290;&#34429;&#28982;GPT&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#22320;&#29983;&#25104;&#31616;&#21333;&#32451;&#20064;&#30340;&#35821;&#27861;&#27491;&#30830;&#20195;&#30721;&#65292;&#20294;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#32451;&#20064;&#65292;&#20182;&#20204;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#24182;&#19988;&#26080;&#27861;&#29983;&#25104;&#28385;&#36275;&#22810;&#25991;&#20214;&#32534;&#31243;&#39033;&#30446;&#35201;&#27714;&#30340;&#20195;&#30721;&#12290;&#27492;&#22806;&#65292;GPT&#27169;&#22411;&#26174;&#31034;&#20102;&#26377;&#38480;&#30340;&#21033;&#29992;&#33258;&#21160;&#35780;&#20998;&#22120;&#25552;&#20379;&#30340;&#21453;&#39304;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;GPT&#27169;&#22411;&#21487;&#33021;&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#24182;&#24378;&#35843;&#38656;&#35201;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#26356;&#22810;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluated the capability of generative pre-trained transformers (GPT), to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. Discussions of potential uses (e.g., exercise generation, code explanation) and misuses (e.g., cheating) of this emerging technology in programming education have intensified, but to date there has not been a rigorous analysis of the models' capabilities in the realistic context of a full-fledged programming course with diverse set of assessment instruments. We evaluated GPT on three Python courses that employ assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Further, we studied if and how successfully GPT models leverage feedback provided by an auto-grader. We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Pyth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38544;&#24335;&#21361;&#23475;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#19968;&#31181;&#38754;&#21521;&#34920;&#24773;&#21253;&#24773;&#22659;&#30340;&#25299;&#25169;&#24863;&#30693;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;TOT&#65292;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26680;&#26041;&#27861;&#20174;&#22810;&#20010;&#27169;&#24577;&#20013;&#25429;&#25417;&#20114;&#34917;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.09314</link><description>&lt;p&gt;
TOT&#65306;&#38754;&#21521;&#22810;&#27169;&#24577;&#20167;&#24680;&#26816;&#27979;&#30340;&#25299;&#25169;&#24863;&#30693;&#26368;&#20248;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
TOT: Topology-Aware Optimal Transport For Multimodal Hate Detection. (arXiv:2303.09314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38544;&#24335;&#21361;&#23475;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#19968;&#31181;&#38754;&#21521;&#34920;&#24773;&#21253;&#24773;&#22659;&#30340;&#25299;&#25169;&#24863;&#30693;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;TOT&#65292;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26680;&#26041;&#27861;&#20174;&#22810;&#20010;&#27169;&#24577;&#20013;&#25429;&#25417;&#20114;&#34917;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20167;&#24680;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#22312;&#32447;&#26377;&#23475;&#20869;&#23481;&#65288;&#22914;&#34920;&#24773;&#21253;&#31561;&#65289;&#65292;&#26159;&#26500;&#24314;&#20581;&#24247;&#30340;&#20114;&#32852;&#32593;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#26174;&#24335;&#20167;&#24680;&#35328;&#35770;&#30340;&#26816;&#27979;&#65292;&#32780;&#24573;&#30053;&#20102;&#38544;&#24335;&#21361;&#23475;&#30340;&#20998;&#26512;&#65292;&#36825;&#22312;&#23384;&#22312;&#30528;&#25197;&#26354;&#25110;&#32570;&#20047;&#26126;&#26174;&#25991;&#26412;&#26631;&#35760;&#21644;&#20154;&#21475;&#32479;&#35745;&#35270;&#35273;&#32447;&#32034;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30528;&#29305;&#21035;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TOT&#65306;&#19968;&#31181;&#38754;&#21521;&#34920;&#24773;&#21253;&#24773;&#22659;&#30340;&#25299;&#25169;&#24863;&#30693;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#65292;&#23558;&#36328;&#27169;&#24577;&#23545;&#40784;&#38382;&#39064;&#36716;&#21270;&#20026;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#30340;&#27714;&#35299;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26680;&#26041;&#27861;&#20174;&#22810;&#20010;&#27169;&#24577;&#20013;&#25429;&#25417;&#20114;&#34917;&#20449;&#24687;&#12290;&#26680;&#23884;&#20837;&#25552;&#20379;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#36716;&#25442;&#33021;&#21147;&#65292;&#20197;&#37325;&#29616;&#36755;&#20837;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal hate detection, which aims to identify harmful content online such as memes, is crucial for building a wholesome internet environment. Previous work has made enlightening exploration in detecting explicit hate remarks. However, most of their approaches neglect the analysis of implicit harm, which is particularly challenging as explicit text markers and demographic visual cues are often twisted or missing. The leveraged cross-modal attention mechanisms also suffer from the distributional modality gap and lack logical interpretability. To address these semantic gaps issues, we propose TOT: a topology-aware optimal transport framework to decipher the implicit harm in memes scenario, which formulates the cross-modal aligning problem as solutions for optimal transportation plans. Specifically, we leverage an optimal transport kernel method to capture complementary information from multiple modalities. The kernel embedding provides a non-linear transformation ability to reproduce 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#22522;&#20110; CRF &#21644;&#28145;&#24230;&#23398;&#20064;&#65288;&#22914; BanglaBERT&#65289;&#30340;&#40065;&#26834;&#23391;&#21152;&#25289;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102; CNER &#20219;&#21153;&#65292;&#22635;&#34917;&#20102;&#23391;&#21152;&#25289;&#35821;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2303.09306</link><description>&lt;p&gt;
&#26500;&#24314;&#40065;&#26834;&#30340;&#23391;&#21152;&#25289;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Bangla Complex Named Entity Recognition. (arXiv:2303.09306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#22522;&#20110; CRF &#21644;&#28145;&#24230;&#23398;&#20064;&#65288;&#22914; BanglaBERT&#65289;&#30340;&#40065;&#26834;&#23391;&#21152;&#25289;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102; CNER &#20219;&#21153;&#65292;&#22635;&#34917;&#20102;&#23391;&#21152;&#25289;&#35821;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035; (NER) &#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#21253;&#25324;&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#21629;&#21517;&#23454;&#20307;&#12290;&#23613;&#31649;&#23391;&#21152;&#25289;&#35821;&#26159;&#20840;&#29699;&#31532;&#19971;&#22823;&#20351;&#29992;&#35821;&#35328;&#65292;&#20294;&#38024;&#23545;&#23391;&#21152;&#25289;&#35821;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#24037;&#20316;&#36824;&#24456;&#23569;&#12290;CNER &#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#35782;&#21035;&#21644;&#20998;&#31867;&#22797;&#26434;&#21644;&#22797;&#21512;&#23454;&#20307;&#65292;&#32780;&#36825;&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#19981;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915; BanglaCoNER &#25968;&#25454;&#38598;&#19978;&#30340; CNER &#20219;&#21153;&#30340;&#33719;&#32988;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21363;&#26465;&#20214;&#38543;&#26426;&#22330; (CRF) &#21644;&#22522;&#20110; finetuning transformer &#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914; BanglaBERT&#65289;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324; 15300 &#20010;&#29992;&#20110;&#35757;&#32451;&#30340;&#21477;&#23376;&#21644; 800 &#20010;&#29992;&#20110;&#39564;&#35777;&#30340;&#21477;&#23376;&#65292;&#26684;&#24335;&#20026; .conll&#12290;&#23545;&#25968;&#25454;&#38598;&#30340;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512; (EDA) &#25581;&#31034;&#20986;&#25968;&#25454;&#38598;&#26377; 7 &#31181;&#19981;&#21516;&#30340; NER &#26631;&#31614;&#65292;&#20854;&#20013;&#26377;&#33521;&#35821;&#21333;&#35789;&#30340;&#26126;&#26174;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) is a fundamental task in natural language processing that involves identifying and classifying named entities in text. But much work hasn't been done for complex named entity recognition in Bangla, despite being the seventh most spoken language globally. CNER is a more challenging task than traditional NER as it involves identifying and classifying complex and compound entities, which are not common in Bangla language. In this paper, we present the winning solution of Bangla Complex Named Entity Recognition Challenge - addressing the CNER task on BanglaCoNER dataset using two different approaches, namely Conditional Random Fields (CRF) and finetuning transformer based Deep Learning models such as BanglaBERT.  The dataset consisted of 15300 sentences for training and 800 sentences for validation, in the .conll format. Exploratory Data Analysis (EDA) on the dataset revealed that the dataset had 7 different NER tags, with notable presence of English words, s
&lt;/p&gt;</description></item><item><title>SmartBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#65292;&#20197;&#21152;&#36895;BERT&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.09266</link><description>&lt;p&gt;
SmartBERT&#65306;&#29992;&#20110;&#21152;&#36895;BERT&#25512;&#29702;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#26426;&#21046;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference. (arXiv:2303.09266v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09266
&lt;/p&gt;
&lt;p&gt;
SmartBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#65292;&#20197;&#21152;&#36895;BERT&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#26679;&#26412;&#22312;&#26089;&#26399;&#36864;&#20986;&#20043;&#21069;&#37117;&#24517;&#39035;&#32463;&#36807;&#25152;&#26377;&#36830;&#32493;&#23618;&#65292;&#36739;&#22797;&#26434;&#30340;&#26679;&#26412;&#36890;&#24120;&#20250;&#32463;&#21382;&#26356;&#22810;&#30340;&#23618;&#65292;&#20173;&#28982;&#23384;&#22312;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SmartBERT&#30340;Bert&#25512;&#29702;&#30340;&#26032;&#22411;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#30456;&#32467;&#21512;&#30340;&#26426;&#21046;&#65292;&#23427;&#23558;&#36339;&#36807;&#38376;&#21644;&#36864;&#20986;&#31639;&#23376;&#21152;&#20837;&#21040;BERT&#30340;&#27599;&#19968;&#23618;&#20013;&#12290;SmartBERT&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#23618;&#23545;&#27604;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#32467;&#21512;&#21040;&#25105;&#20204;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#20197;&#25552;&#39640;&#20013;&#38388;&#23618;&#21644;&#20998;&#31867;&#22120;&#65292;&#36825;&#23545;&#20110;&#26089;&#26399;&#36864;&#20986;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#20445;&#25345;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#36339;&#36807;&#38376;&#30340;&#19968;&#33268;&#20351;&#29992;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#38454;&#27573;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#26435;&#37325;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#20843;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic early exiting has been proven to improve the inference speed of the pre-trained language model like BERT. However, all samples must go through all consecutive layers before early exiting and more complex samples usually go through more layers, which still exists redundant computation. In this paper, we propose a novel dynamic early exiting combined with layer skipping for BERT inference named SmartBERT, which adds a skipping gate and an exiting operator into each layer of BERT. SmartBERT can adaptively skip some layers and adaptively choose whether to exit. Besides, we propose cross-layer contrastive learning and combine it into our training phases to boost the intermediate layers and classifiers which would be beneficial for early exiting. To keep the consistent usage of skipping gates between training and inference phases, we propose a hard weight mechanism during training phase. We conduct experiments on eight classification datasets of the GLUE benchmark. Experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#22359;&#20301;&#21387;&#32553;&#26041;&#27861;&#65292;&#31216;&#20026;BBCT&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#21387;&#32553;&#25972;&#20010;Transformer&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;softmax&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#22312;GLUE&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;BBCT&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#23454;&#29616;&#23569;&#20110;1&#65285;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2303.09184</link><description>&lt;p&gt;
&#22522;&#20110;&#22359;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#20301;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Block-wise Bit-Compression of Transformer-based Models. (arXiv:2303.09184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#22359;&#20301;&#21387;&#32553;&#26041;&#27861;&#65292;&#31216;&#20026;BBCT&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#21387;&#32553;&#25972;&#20010;Transformer&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;softmax&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#22312;GLUE&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;BBCT&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#23454;&#29616;&#23569;&#20110;1&#65285;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;BERT&#12289;GPT-3&#21644;ChatGPT&#31561;&#36817;&#26399;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27969;&#34892;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22411;&#30340;&#24040;&#22823;&#35745;&#31639;&#37327;&#12289;&#24040;&#22823;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#39640;&#24310;&#36831;&#26159;&#20113;&#35745;&#31639;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BBCT&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#22359;&#20301;&#21387;&#32553;&#26041;&#27861;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;Transformer&#30340;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;softmax&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#25105;&#20204;&#20197;&#39640;&#25928;BERT&#20026;&#26696;&#20363;&#65292;&#20351;&#29992;BBCT&#26041;&#27861;&#36827;&#34892;&#21387;&#32553;&#12290;&#25105;&#20204;&#22312;General Language Understanding Evaluation(GLUE)&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;BBCT&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24230;&#19979;&#38477;&#23567;&#20110;1&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the popularity of the recent Transformer-based models represented by BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range of natural language processing tasks. However, the massive computations, huge memory footprint, and thus high latency of Transformer-based models is an inevitable challenge for the cloud with high real-time requirement. To tackle the issue, we propose BBCT, a method of block-wise bit-compression for transformer without retraining. Our method achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient BERT with the method of BBCT. Our benchmark test results on General Language Understanding Evaluation (GLUE) show that BBCT can achieve less than 1% accuracy drop in most tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#24212;&#29992;&#20110;&#27861;&#24459;&#20219;&#21153;&#26102;&#25152;&#38754;&#20020;&#30340;&#27861;&#24459;&#38382;&#39064;&#65292;&#20197;&#21450;&#20351;&#29992;&#25968;&#25454;&#36164;&#28304;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#19987;&#38376;&#21270;LLMs&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09136</link><description>&lt;p&gt;
&#26597;&#30475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#26041;&#38754;&#30340;&#31616;&#35201;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Short Survey of Viewing Large Language Models in Legal Aspect. (arXiv:2303.09136v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#24212;&#29992;&#20110;&#27861;&#24459;&#20219;&#21153;&#26102;&#25152;&#38754;&#20020;&#30340;&#27861;&#24459;&#38382;&#39064;&#65292;&#20197;&#21450;&#20351;&#29992;&#25968;&#25454;&#36164;&#28304;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#19987;&#38376;&#21270;LLMs&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25913;&#21464;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#20123;&#27169;&#22411;&#20063;&#22312;&#27861;&#24459;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#35201;&#30340;&#24433;&#21709;&#65292;&#23427;&#20204;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#33258;&#21160;&#21270;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#65292;&#20363;&#22914;&#27861;&#24459;&#21028;&#26029;&#39044;&#27979;&#12289;&#27861;&#24459;&#25991;&#20214;&#20998;&#26512;&#21644;&#27861;&#24459;&#25991;&#20214;&#25776;&#20889;&#12290;&#28982;&#32780;&#65292;&#23558;LLMs&#25972;&#21512;&#21040;&#27861;&#24459;&#39046;&#22495;&#20013;&#20063;&#24341;&#21457;&#20102;&#19968;&#20123;&#27861;&#24459;&#38382;&#39064;&#65292;&#21253;&#25324;&#38544;&#31169;&#38382;&#39064;&#12289;&#20559;&#35265;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#27425;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;LLMs&#34701;&#20837;&#27861;&#24459;&#39046;&#22495;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;LLMs&#22312;&#27861;&#24459;&#20219;&#21153;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#20351;&#29992;&#26102;&#20986;&#29616;&#30340;&#27861;&#24459;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#21487;&#20197;&#29992;&#20110;&#22312;&#27861;&#24459;&#39046;&#22495;&#19987;&#38376;&#21270;LLMs&#30340;&#25968;&#25454;&#36164;&#28304;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20960;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#24182;&#24635;&#32467;&#20102;&#26412;&#25991;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#24076;&#26395;&#25552;&#20379;LLMs&#22312;&#27861;&#24459;&#19978;&#30340;&#24403;&#21069;&#29366;&#24577;&#27010;&#36848;&#65292;&#24182;&#31361;&#20986;&#20854;&#28508;&#22312;&#30340;&#22909;&#22788;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have transformed many fields, including natural language processing, computer vision, and reinforcement learning. These models have also made a significant impact in the field of law, where they are being increasingly utilized to automate various legal tasks, such as legal judgement prediction, legal document analysis, and legal document writing. However, the integration of LLMs into the legal field has also raised several legal problems, including privacy concerns, bias, and explainability. In this survey, we explore the integration of LLMs into the field of law. We discuss the various applications of LLMs in legal tasks, examine the legal challenges that arise from their use, and explore the data resources that can be used to specialize LLMs in the legal domain. Finally, we discuss several promising directions and conclude this paper. By doing so, we hope to provide an overview of the current state of LLMs in law and highlight the potential benefits and c
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20004;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20998;&#26512;&#20013;&#22788;&#29702;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32452;&#32455;&#12289;&#39033;&#30446;&#21644;&#27169;&#22359;&#30340;&#33258;&#28982;&#36793;&#30028;&#20998;&#21106;&#26041;&#27861;&#65292;&#21457;&#29616;&#27599;&#20010;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#37117;&#20250;&#20135;&#29983;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#19982;&#23569;&#37327;&#24494;&#35843;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.09128</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#20195;&#30721;&#20998;&#26512;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Exploring Distributional Shifts in Large Language Models for Code Analysis. (arXiv:2303.09128v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09128
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20004;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20998;&#26512;&#20013;&#22788;&#29702;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32452;&#32455;&#12289;&#39033;&#30446;&#21644;&#27169;&#22359;&#30340;&#33258;&#28982;&#36793;&#30028;&#20998;&#21106;&#26041;&#27861;&#65292;&#21457;&#29616;&#27599;&#20010;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#37117;&#20250;&#20135;&#29983;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#19982;&#23569;&#37327;&#24494;&#35843;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20004;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; CodeT5 &#21644; Codex &#30340;&#33021;&#21147;&#65292;&#20197;&#20415;&#25512;&#24191;&#21040;&#39046;&#22495;&#22806;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#22522;&#26412;&#24212;&#29992;&#65306;&#20195;&#30721;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#12290;&#25105;&#20204;&#25353;&#29031;&#20854;&#33258;&#28982;&#36793;&#30028;&#65288;&#25353;&#32452;&#32455;&#12289;&#25353;&#39033;&#30446;&#21644;&#25353;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#27169;&#22359;&#65289;&#23558;&#25968;&#25454;&#20998;&#20026;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#36825;&#26679;&#65292;&#22312;&#37096;&#32626;&#26102;&#65292;&#35782;&#21035;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#25968;&#25454;&#21464;&#24471;&#31616;&#21333;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26469;&#33258;&#27599;&#20010;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#37117;&#20250;&#32473;&#36825;&#20004;&#20010;&#27169;&#22411;&#24102;&#26469;&#20998;&#24067;&#20559;&#31227;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22914;&#20309;&#36866;&#24212;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#34429;&#28982;&#22810;&#20219;&#21153;&#23398;&#20064;&#26412;&#36523;&#26159;&#19968;&#20010;&#21512;&#29702;&#30340;&#22522;&#32447;&#65292;&#20294;&#23558;&#20854;&#19982;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#26816;&#32034;&#30340;&#31034;&#20363;&#30340;&#23569;&#37327;&#24494;&#35843;&#30456;&#32467;&#21512;&#21487;&#20197;&#23454;&#29616;&#38750;&#24120;&#24378;&#30340;&#24615;&#33021;&#12290;&#20107;&#23454;&#19978;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#38750;&#24120;&#20302;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#20248;&#20110;&#30452;&#25509;&#35843;&#25972;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
We systematically study the capacity of two large language models for code CodeT5 and Codex - to generalize to out-of-domain data. In this study, we consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. This makes recognition of in-domain vs out-of-domain data at the time of deployment trivial. We establish that samples from each new domain present both models with a significant challenge of distribution shift. We study how well different established methods can adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. In fact, according to our experiments, this solution can outperform direct finetuning for very low-data scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27010;&#29575;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#26631;&#35760;&#25512;&#21521;&#24544;&#23454;&#25429;&#25417;&#26631;&#31614;&#29305;&#23450;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#31867;&#21035;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25552;&#31034;&#24037;&#31243;&#30340;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.09100</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#34917;&#19969;-&#20196;&#29260;&#23545;&#40784;&#30340;&#36125;&#21494;&#26031;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models. (arXiv:2303.09100v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27010;&#29575;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#26631;&#35760;&#25512;&#21521;&#24544;&#23454;&#25429;&#25417;&#26631;&#31614;&#29305;&#23450;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#31867;&#21035;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25552;&#31034;&#24037;&#31243;&#30340;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#26500;&#24314;&#26377;&#25928;&#25552;&#31034;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#36153;&#26102;&#36153;&#21147;&#30340;&#25163;&#21160;&#35774;&#35745;&#65292;&#35201;&#20040;&#23558;&#25552;&#31034;&#35843;&#20248;&#20316;&#20026;&#28857;&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25551;&#36848;&#31867;&#21035;&#30340;&#22810;&#26679;&#29305;&#24449;&#24182;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27010;&#29575;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#20174;&#28508;&#22312;&#20998;&#24067;&#20013;&#39318;&#20808;&#37319;&#26679;&#38544;&#21521;&#37327;&#65292;&#28982;&#21518;&#37319;&#29992;&#36731;&#37327;&#32423;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#26631;&#31614;&#29305;&#23450;&#30340;&#38543;&#26426;&#25552;&#31034;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#35270;&#35273;&#30693;&#35782;&#19982;&#22270;&#20687;&#30340;&#35821;&#20041;&#35268;&#21017;&#21270;&#65292;&#24182;&#23558;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#25552;&#31034;&#35270;&#20026;&#34917;&#19969;&#21644;&#20196;&#29260;&#38598;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23558;&#25552;&#31034;&#26631;&#35760;&#25512;&#21521;&#24544;&#23454;&#25429;&#25417;&#26631;&#31614;&#29305;&#23450;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#20855;&#20449;&#24687;&#37327;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#31034;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34917;&#19969;-&#20196;&#29260;&#23545;&#40784;&#30340;&#36125;&#21494;&#26031;&#25552;&#31034;&#23398;&#20064;&#65288;PTBPL&#65289;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt learning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize prompt learning with the visual knowledge and view images and the corresponding prompts as patch and token sets under optimal transport, which pushes the prompt tokens to faithfully capture the label-specific visual concepts, instead of overfitting the training categories. Moreover, the proposed model can also be straightforwar
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;GLEN&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;3,465&#31181;&#19981;&#21516;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#26631;&#27880;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#38454;&#27573;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#22823;&#26412;&#20307;&#22823;&#23567;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09093</link><description>&lt;p&gt;
GLEN&#65306;&#38754;&#21521;&#25968;&#21315;&#31181;&#31867;&#22411;&#30340;&#36890;&#29992;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GLEN: General-Purpose Event Detection for Thousands of Types. (arXiv:2303.09093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09093
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;GLEN&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;3,465&#31181;&#19981;&#21516;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#26631;&#27880;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#38454;&#27573;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#22823;&#26412;&#20307;&#22823;&#23567;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25277;&#21462;&#31995;&#32479;&#30340;&#21457;&#23637;&#19968;&#30452;&#21463;&#38480;&#20110;&#32570;&#20047;&#24191;&#27867;&#35206;&#30422;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20351;&#20107;&#20214;&#25277;&#21462;&#31995;&#32479;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;GLEN&#65292;&#28085;&#30422;&#20102;3,465&#31181;&#19981;&#21516;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#26412;&#20307;&#27604;&#20219;&#20309;&#24403;&#21069;&#25968;&#25454;&#38598;&#37117;&#22823;20&#20493;&#20197;&#19978;&#12290;GLEN&#21033;&#29992;DWD&#21472;&#21152;&#25216;&#26415;&#21019;&#24314;&#65292;&#36890;&#36807;&#25552;&#20379;&#32500;&#22522;&#30334;&#31185;Qnode&#21644;PropBank&#35282;&#33394;&#38598;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20351;&#29992;PropBank&#30340;&#29616;&#26377;&#26631;&#27880;&#20316;&#20026;&#38388;&#25509;&#30417;&#30563;&#26469;&#23436;&#25104;&#21019;&#24314;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#38454;&#27573;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;GLEN&#30340;&#22823;&#26412;&#20307;&#22823;&#23567;&#21644;&#37096;&#20998;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65288;F1&#20998;&#25968;&#25552;&#39640;&#20102;&#32422;10%&#65289;&#65292;&#19982;&#20256;&#32479;&#30340;&#20998;&#31867;&#22522;&#32447;&#21644;&#36739;&#26032;&#30340;&#22522;&#20110;&#23450;&#20041;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#26174;&#31034;&#26631;&#31614;&#22122;&#22768;&#20173;&#28982;&#26159;&#25552;&#39640;&#24615;&#33021;&#30340;&#26368;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of event extraction systems has been hindered by the absence of wide-coverage, large-scale datasets. To make event extraction systems more accessible, we build a general-purpose event detection dataset GLEN, which covers 3,465 different event types, making it over 20x larger in ontology than any current dataset. GLEN is created by utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and PropBank rolesets. This enables us to use the abundant existing annotation for PropBank as distant supervision. In addition, we also propose a new multi-stage event detection model specifically designed to handle the large ontology size and partial labels in GLEN. We show that our model exhibits superior performance (~10% F1 gain) compared to both conventional classification baselines and newer definition-based models. Finally, we perform error analysis and show that label noise is still the largest challenge for improving performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#21463;&#21040;&#25968;&#25454;&#38598;&#19981;&#21516;&#30340;&#25805;&#20316;&#21270;&#26041;&#24335;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09092</link><description>&lt;p&gt;
&#25506;&#31350;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#25512;&#24191;&#22833;&#36133;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Investigating Failures to Generalize for Coreference Resolution Models. (arXiv:2303.09092v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#21463;&#21040;&#25968;&#25454;&#38598;&#19981;&#21516;&#30340;&#25805;&#20316;&#21270;&#26041;&#24335;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#36890;&#24120;&#20250;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#22312;&#22914;&#20309;&#23454;&#29616;&#25351;&#20195;&#28040;&#35299;&#26041;&#38754;&#65288;&#21363;&#29702;&#35770;&#27010;&#24565;&#22312;&#25968;&#25454;&#38598;&#20013;&#30340;&#25805;&#20316;&#21270;&#26041;&#24335;&#65289;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#26159;&#30001;&#20110;&#36873;&#25321;&#35821;&#26009;&#24211;&#21644;&#27880;&#37322;&#25351;&#21335;&#31561;&#22240;&#32032;&#25152;&#33268;&#12290;&#26412;&#25991;&#26088;&#22312;&#35843;&#26597;&#24403;&#21069;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#30340;&#38169;&#35823;&#31243;&#24230;&#19982;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#23454;&#29616;&#24046;&#24322;&#20043;&#38388;&#30340;&#20851;&#32852;&#31243;&#24230;&#65288;OntoNotes&#12289;PreCo&#21644;Winogrande&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#24615;&#33021;&#20998;&#20026;&#22810;&#20010;&#31867;&#21035;&#65292;&#23545;&#24212;&#20110;&#22810;&#31181;&#25351;&#20195;&#65292;&#21253;&#25324;&#19968;&#33324;&#24615;&#25552;&#21450;&#12289;&#22797;&#21512;&#20462;&#39280;&#31526;&#21644;&#36830;&#31995;&#35859;&#35789;&#31561;&#12290;&#36825;&#31181;&#20998;&#31867;&#26377;&#21161;&#20110;&#25105;&#20204;&#35843;&#26597;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#36328;&#36234;&#19981;&#21516;&#25351;&#20195;&#31867;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21487;&#33021;&#20250;&#20986;&#29616;&#21738;&#20123;&#24046;&#24322;&#12290;&#20363;&#22914;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#22312;OntoNotes&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;PreCo&#20013;&#19968;&#33324;&#24615;&#25552;&#21450;&#21644;&#36830;&#31995;&#35859;&#35789;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#21644;&#25351;&#20195;&#28040;&#35299;&#25805;&#20316;&#21270;&#26041;&#38754;&#35780;&#20272;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coreference resolution models are often evaluated on multiple datasets. Datasets vary, however, in how coreference is realized -- i.e., how the theoretical concept of coreference is operationalized in the dataset -- due to factors such as the choice of corpora and annotation guidelines. We investigate the extent to which errors of current coreference resolution models are associated with existing differences in operationalization across datasets (OntoNotes, PreCo, and Winogrande). Specifically, we distinguish between and break down model performance into categories corresponding to several types of coreference, including coreferring generic mentions, compound modifiers, and copula predicates, among others. This break down helps us investigate how state-of-the-art models might vary in their ability to generalize across different coreference types. In our experiments, for example, models trained on OntoNotes perform poorly on generic mentions and copula predicates in PreCo. Our findings 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#19968;&#33268;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#21512;&#20316;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;GAN&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#26679;&#26412;&#23481;&#26131;&#20559;&#31163;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#12289;&#37492;&#21035;&#27169;&#22411;&#25913;&#36827;&#39281;&#21644;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GAN&#65292;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#20063;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.09075</link><description>&lt;p&gt;
&#33258;&#19968;&#33268;&#23398;&#20064;&#65306;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#30340;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Self-Consistent Learning: Cooperation between Generators and Discriminators. (arXiv:2303.09075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#19968;&#33268;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#21512;&#20316;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;GAN&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#26679;&#26412;&#23481;&#26131;&#20559;&#31163;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#12289;&#37492;&#21035;&#27169;&#22411;&#25913;&#36827;&#39281;&#21644;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GAN&#65292;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#20063;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#19979;&#28216;&#37492;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#24050;&#32463;&#22240;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#21457;&#23637;&#32780;&#24191;&#21463;&#27426;&#36814;&#12290;&#22312;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#29983;&#25104;&#27169;&#22411;&#21644;&#37492;&#21035;&#27169;&#22411;&#26159;&#20998;&#21035;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#23427;&#20204;&#19981;&#33021;&#36866;&#24212;&#24444;&#27492;&#30340;&#20219;&#20309;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#30340;&#26679;&#26412;&#24456;&#23481;&#26131;&#20559;&#31163;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#65292;&#32780;&#37492;&#21035;&#27169;&#22411;&#30340;&#25913;&#36827;&#24456;&#24555;&#23601;&#20250;&#36798;&#21040;&#39281;&#21644;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36890;&#36807;&#19968;&#31181;&#23545;&#25239;&#24615;&#36807;&#31243;&#19982;&#37492;&#21035;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#20197;&#23454;&#29616;&#32852;&#21512;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;GAN&#30340;&#35757;&#32451;&#26497;&#19981;&#31283;&#23450;&#65292;&#24448;&#24448;&#38590;&#20197;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#19968;&#33268;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#19968;&#20010;&#37492;&#21035;&#22120;&#21644;&#19968;&#20010;&#29983;&#25104;&#22120;&#20197;&#38381;&#29615;&#24418;&#24335;&#21512;&#20316;&#35757;&#32451;&#12290;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#22312;&#22810;&#36718;&#26356;&#26032;&#20013;&#30456;&#20114;&#22686;&#24378;&#65292;&#29983;&#25104;&#30340;&#26679;&#26412;&#36880;&#28176;&#25509;&#36817;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#65292;&#32780;&#37492;&#21035;&#27169;&#22411;&#19981;&#26029;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GAN&#65292;&#32780;&#19988;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using generated data to improve the performance of downstream discriminative models has recently gained popularity due to the great development of pre-trained language models. In most previous studies, generative models and discriminative models are trained separately and thus could not adapt to any changes in each other. As a result, the generated samples can easily deviate from the real data distribution, while the improvement of the discriminative model quickly reaches saturation. Generative adversarial networks (GANs) train generative models via an adversarial process with discriminative models to achieve joint training. However, the training of standard GANs is notoriously unstable and often falls short of convergence. In this paper, to address these issues, we propose a $\textit{self-consistent learning}$ framework, in which a discriminator and a generator are cooperatively trained in a closed-loop form. The discriminator and the generator enhance each other during multiple round
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#31995;&#32479;&#8220;&#19981;&#24212;&#35813;&#8221;&#22238;&#31572;&#30340;&#38382;&#39064;&#25552;&#20986;&#20102;&#38382;&#31572;&#20013;&#30340;&#20445;&#23494;&#22788;&#29702;&#65292;&#26088;&#22312;&#20445;&#25252;&#25935;&#24863;&#29992;&#25143;&#25110;&#25935;&#24863;&#20449;&#24687;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#26550;&#26500;&#65292;&#21487;&#20197;&#25945;&#20250;&#38382;&#31572;&#31995;&#32479;&#20445;&#23432;&#29305;&#23450;&#30340;&#26426;&#23494;&#12290;</title><link>http://arxiv.org/abs/2303.09067</link><description>&lt;p&gt;
&#38382;&#31572;&#20013;&#30340;&#20445;&#23494;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Secret-Keeping in Question Answering. (arXiv:2303.09067v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#31995;&#32479;&#8220;&#19981;&#24212;&#35813;&#8221;&#22238;&#31572;&#30340;&#38382;&#39064;&#25552;&#20986;&#20102;&#38382;&#31572;&#20013;&#30340;&#20445;&#23494;&#22788;&#29702;&#65292;&#26088;&#22312;&#20445;&#25252;&#25935;&#24863;&#29992;&#25143;&#25110;&#25935;&#24863;&#20449;&#24687;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#26550;&#26500;&#65292;&#21487;&#20197;&#25945;&#20250;&#38382;&#31572;&#31995;&#32479;&#20445;&#23432;&#29305;&#23450;&#30340;&#26426;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#38382;&#31572;&#30740;&#31350;&#20391;&#37325;&#20110;&#22987;&#32456;&#25552;&#20379;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#38750;&#21487;&#22238;&#31572;&#38382;&#39064;&#30340;&#22788;&#29702;&#65292;&#20294;&#26159;&#22914;&#20309;&#24212;&#23545;&#37027;&#20123;&#31995;&#32479;&#8220;&#19981;&#24212;&#35813;&#8221;&#22238;&#31572;&#30340;&#38382;&#39064;&#21602;&#65311;&#36825;&#21487;&#33021;&#26159;&#20026;&#20102;&#20445;&#25252;&#25935;&#24863;&#29992;&#25143;&#25110;&#25935;&#24863;&#20449;&#24687;&#12290;&#35768;&#22810;&#27169;&#22411;&#22312;&#36973;&#21463;&#23545;&#25239;&#24615;&#29992;&#25143;&#30340;&#23457;&#38382;&#26102;&#20250;&#26292;&#38706;&#25935;&#24863;&#20449;&#24687;&#12290;&#25105;&#20204;&#35797;&#22270;&#25214;&#20986;&#26159;&#21542;&#21487;&#33021;&#25945;&#20250;&#38382;&#31572;&#31995;&#32479;&#20445;&#23432;&#29305;&#23450;&#30340;&#26426;&#23494;&#12290;&#25105;&#20204;&#35774;&#35745;&#21644;&#23454;&#29616;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#25105;&#20204;&#30340;&#35780;&#20272;&#30830;&#23450;&#23613;&#31649;&#21487;&#33021;&#65292;&#20294;&#26377;&#35768;&#22810;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20197;&#20943;&#23569;&#31995;&#32479;&#30340;&#20559;&#25191;&#30151;&#65288;&#35823;&#25253;&#65289;&#65292;&#20449;&#24687;&#27844;&#28431;&#65288;&#28431;&#25253;&#65289;&#24182;&#23558;&#35813;&#24037;&#20316;&#30340;&#23454;&#29616;&#25193;&#23637;&#21040;&#22312;&#20449;&#24687;&#27719;&#32858;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#26426;&#23494;&#30340;&#26356;&#22797;&#26434;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing question-answering research focuses on unanswerable questions in the context of always providing an answer when a system can\dots but what about cases where a system {\bf should not} answer a question. This can either be to protect sensitive users or sensitive information. Many models expose sensitive information under interrogation by an adversarial user. We seek to determine if it is possible to teach a question-answering system to keep a specific fact secret. We design and implement a proof-of-concept architecture and through our evaluation determine that while possible, there are numerous directions for future research to reduce system paranoia (false positives), information leakage (false negatives) and extend the implementation of the work to more complex problems with preserving secrecy in the presence of information aggregation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;&#29575;&#21644;&#20449;&#24687;&#38169;&#35823;&#29575;&#22343;&#36739;&#20302;&#65292;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#22823;&#37117;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2303.09038</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#21644;Prompt Learning&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65306;&#32467;&#26524;&#12289;&#38480;&#21046;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. (arXiv:2303.09038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;&#29575;&#21644;&#20449;&#24687;&#38169;&#35823;&#29575;&#22343;&#36739;&#20302;&#65292;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#22823;&#37117;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20854;&#31867;&#20284;&#20154;&#31867;&#34920;&#36798;&#21644;&#25512;&#29702;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20415;&#24739;&#32773;&#21644;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#24471;&#21040;&#26356;&#22909;&#30340;&#21307;&#30103;&#25945;&#32946;&#12290;&#30740;&#31350;&#37319;&#38598;&#20102;62&#20221;&#20302;&#21058;&#37327;&#33016;&#37096;CT&#32954;&#30284;&#31579;&#26597;&#25195;&#25551;&#21644;76&#20221;&#33041;MRI&#36716;&#31227;&#24615;&#31579;&#26597;&#25195;&#25551;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#26681;&#25454;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#35780;&#20215;&#65292;ChatGPT&#21487;&#20197;&#25104;&#21151;&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;0.07&#22788;&#65292;&#20449;&#24687;&#38169;&#35823;0.11&#22788;&#12290;&#23601;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#32780;&#35328;&#65292;&#23427;&#20204;&#26159;&#19968;&#33324;&#24615;&#30340;&#30456;&#20851;&#24314;&#35758;&#65292;&#20363;&#22914;&#20445;&#25345;&#19982;&#21307;&#29983;&#30340;&#38543;&#35775;&#21644;&#23494;&#20999;&#30417;&#27979;&#20219;&#20309;&#30151;&#29366;&#65292;&#23545;&#20110;&#20849;138&#20010;&#30149;&#20363;&#20013;&#30340;&#32422;37&#65285;&#65292;ChatGPT&#25552;&#20379;&#20102;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#26377;&#20851;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large language model called ChatGPT has drawn extensively attention because of its human-like expression and reasoning abilities. In this study, we investigate the feasibility of using ChatGPT in experiments on using ChatGPT to translate radiology reports into plain language for patients and healthcare providers so that they are educated for improved healthcare. Radiology reports from 62 low-dose chest CT lung cancer screening scans and 76 brain MRI metastases screening scans were collected in the first half of February for this study. According to the evaluation by radiologists, ChatGPT can successfully translate radiology reports into plain language with an average score of 4.1 in the five-point system with 0.07 places of information missing and 0.11 places of misinformation. In terms of the suggestions provided by ChatGPT, they are general relevant such as keeping following-up with doctors and closely monitoring any symptoms, and for about 37% of 138 cases in total ChatGPT offer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20174;&#20687;&#32032;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;ALFWorld&#21644;VirtualHome&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.09031</link><description>&lt;p&gt;
&#19968;&#24133;&#22270;&#32988;&#36807;&#21315;&#35328;&#19975;&#35821;&#65306;&#35821;&#35328;&#27169;&#22411;&#20174;&#20687;&#32032;&#20013;&#35268;&#21010;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
A Picture is Worth a Thousand Words: Language Models Plan from Pixels. (arXiv:2303.09031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20174;&#20687;&#32032;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;ALFWorld&#21644;VirtualHome&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#26159;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#25191;&#34892;&#23454;&#38469;&#29615;&#22659;&#20013;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#30340;&#37325;&#35201;&#33021;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#26469;&#20174;&#25991;&#26412;&#25351;&#20196;&#20013;&#25512;&#29702;&#20986;&#35268;&#21010;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#20043;&#21069;&#36890;&#36807;PLM&#36827;&#34892;&#35268;&#21010;&#30340;&#26041;&#27861;&#35201;&#20040;&#20551;&#23450;&#35266;&#23519;&#32467;&#26524;&#20197;&#25991;&#26412;&#24418;&#24335;&#21487;&#33719;&#24471;&#65288;&#20363;&#22914;&#30001;&#23383;&#24149;&#27169;&#22411;&#25552;&#20379;&#65289;&#65292;&#35201;&#20040;&#20165;&#20174;&#25351;&#20196;&#20013;&#29702;&#35299;&#35268;&#21010;&#65292;&#25110;&#32773;&#21482;&#26377;&#26377;&#38480;&#26041;&#24335;&#22320;&#25972;&#21512;&#20102;&#26377;&#20851;&#35270;&#35273;&#29615;&#22659;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#39044;&#35757;&#32451;&#30340;&#21487;&#20379;&#24615;&#20989;&#25968;&#65289;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#35266;&#23519;&#32467;&#26524;&#30452;&#25509;&#32534;&#30721;&#20026;PLM&#30340;&#36755;&#20837;&#25552;&#31034;&#65292;PLM&#20063;&#33021;&#22815;&#20934;&#30830;&#36827;&#34892;&#35268;&#21010;&#12290;&#25105;&#20204;&#22312;ALFWorld&#21644;VirtualHome&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#31181;&#31616;&#21333;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning is an important capability of artificial agents that perform long-horizon tasks in real-world environments. In this work, we explore the use of pre-trained language models (PLMs) to reason about plan sequences from text instructions in embodied visual environments. Prior PLM based approaches for planning either assume observations are available in the form of text (e.g., provided by a captioning model), reason about plans from the instruction alone, or incorporate information about the visual environment in limited ways (such as a pre-trained affordance function). In contrast, we show that PLMs can accurately plan even when observations are directly encoded as input prompts for the PLM. We show that this simple approach outperforms prior approaches in experiments on the ALFWorld and VirtualHome benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#25512;&#29702;&#21644;&#24037;&#20855;&#20351;&#29992;&#65288;ART&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#22797;&#26434;&#22810;&#27493;&#25512;&#29702;&#27493;&#39588;&#65292;&#21487;&#26080;&#32541;&#38598;&#25104;&#22806;&#37096;&#24037;&#20855;&#25903;&#25345;&#12290;&#35813;&#26694;&#26550;&#33719;&#24471;&#20102;&#22312;&#26032;&#20219;&#21153;&#19978;&#26174;&#30528;&#25913;&#36827;&#65292;&#19988;&#26080;&#38656;&#25163;&#24037;&#21046;&#20316;&#28436;&#31034;&#25110;&#31934;&#24515;&#32534;&#20889;&#33050;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.09014</link><description>&lt;p&gt;
ART: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#22810;&#27493;&#25512;&#29702;&#21644;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
ART: Automatic multi-step reasoning and tool-use for large language models. (arXiv:2303.09014v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#25512;&#29702;&#21644;&#24037;&#20855;&#20351;&#29992;&#65288;ART&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#22797;&#26434;&#22810;&#27493;&#25512;&#29702;&#27493;&#39588;&#65292;&#21487;&#26080;&#32541;&#38598;&#25104;&#22806;&#37096;&#24037;&#20855;&#25903;&#25345;&#12290;&#35813;&#26694;&#26550;&#33719;&#24471;&#20102;&#22312;&#26032;&#20219;&#21153;&#19978;&#26174;&#30528;&#25913;&#36827;&#65292;&#19988;&#26080;&#38656;&#25163;&#24037;&#21046;&#20316;&#28436;&#31034;&#25110;&#31934;&#24515;&#32534;&#20889;&#33050;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22312;&#23569;&#37327;&#21644;&#38646;-shot&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#65292;&#29983;&#25104;&#20013;&#38388;&#30340;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#27493;&#39588;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#21487;&#20197;&#20381;&#36182;&#22806;&#37096;&#24037;&#20855;&#26469;&#25903;&#25345;&#36229;&#20986;&#26680;&#24515;LLM&#21151;&#33021;&#30340;&#35745;&#31639;&#65288;&#20363;&#22914;&#25628;&#32034;/&#36816;&#34892;&#20195;&#30721;&#65289;&#12290;&#22312;CoT&#25552;&#31034;&#21644;&#24037;&#20855;&#20351;&#29992;&#26041;&#38754;&#30340;&#20808;&#21069;&#24037;&#20316;&#36890;&#24120;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#29305;&#23450;&#20219;&#21153;&#30340;&#28436;&#31034;&#65292;&#24182;&#31934;&#24515;&#32534;&#20889;&#27169;&#22411;&#29983;&#25104;&#19982;&#24037;&#20855;&#20351;&#29992;&#20132;&#38169;&#30340;&#33050;&#26412;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#21160;&#25512;&#29702;&#21644;&#24037;&#20855;&#20351;&#29992;&#65288;ART&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#20923;&#32467;LLMs&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#20316;&#20026;&#31243;&#24207;&#12290;&#32473;&#23450;&#26032;&#30340;&#20219;&#21153;&#26469;&#35299;&#20915;&#65292;ART&#20174;&#20219;&#21153;&#24211;&#20013;&#36873;&#25321;&#22810;&#27493;&#25512;&#29702;&#21644;&#24037;&#20855;&#20351;&#29992;&#30340;&#28436;&#31034;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;ART&#22312;&#35843;&#29992;&#22806;&#37096;&#24037;&#20855;&#26102;&#26080;&#32541;&#22320;&#26242;&#20572;&#29983;&#25104;&#65292;&#24182;&#22312;&#24674;&#22797;&#29983;&#25104;&#20043;&#21069;&#38598;&#25104;&#20854;&#36755;&#20986;&#12290;ART&#22312;BigBench&#21644;MMLU&#19978;&#30475;&#21040;&#20102;&#22312;&#26410;&#35265;&#20219;&#21153;&#30340;&#23569;&#37327;&#25552;&#31034;&#21644;&#33258;&#21160;CoT&#19978;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multi-step reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU
&lt;/p&gt;</description></item><item><title>DeltaScore&#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#26469;&#35780;&#20272;&#25925;&#20107;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#26041;&#38754;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#25200;&#21160;&#21069;&#21518;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#26469;&#34913;&#37327;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.08991</link><description>&lt;p&gt;
DeltaScore: &#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#35780;&#20215;&#25925;&#20107;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DeltaScore: Evaluating Story Generation with Differentiating Perturbations. (arXiv:2303.08991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08991
&lt;/p&gt;
&lt;p&gt;
DeltaScore&#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#26469;&#35780;&#20272;&#25925;&#20107;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#26041;&#38754;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#25200;&#21160;&#21069;&#21518;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#26469;&#34913;&#37327;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#21508;&#31181;&#35780;&#20215;&#25351;&#26631;&#23384;&#22312;&#65292;&#20294;&#23545;&#20110;&#25925;&#20107;&#29983;&#25104;&#30340;&#23454;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#19981;&#24378;&#65292;&#20063;&#19981;&#33021;&#27979;&#37327;&#32454;&#31890;&#24230;&#30340;&#25925;&#20107;&#26041;&#38754;&#65292;&#20363;&#22914;&#27969;&#30021;&#24230;&#19982;&#30456;&#20851;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26088;&#22312;&#35780;&#20272;&#25972;&#20307;&#29983;&#25104;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;DeltaScore&#65292;&#19968;&#31181;&#21033;&#29992;&#25200;&#21160;&#26469;&#35780;&#20272;&#32454;&#31890;&#24230;&#25925;&#20107;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22522;&#20110;&#36825;&#26679;&#30340;&#20551;&#35774;&#65306;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#34920;&#29616;&#24471;&#36234;&#22909;&#65288;&#20363;&#22914;&#27969;&#30021;&#24230;&#65289;&#65292;&#23427;&#23601;&#20250;&#21463;&#21040;&#29305;&#23450;&#25200;&#21160;&#65288;&#20363;&#22914;&#24341;&#20837;&#38169;&#21035;&#23383;&#65289;&#30340;&#24433;&#21709;&#36234;&#22823;&#12290;&#20026;&#20102;&#34913;&#37327;&#24433;&#21709;&#65292;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#25200;&#21160;&#21069;&#21518;&#25925;&#20107;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#20351;&#29992;DeltaScore&#35780;&#20272;&#20102;&#22522;&#20110;&#29366;&#24577;&#30340;&#26368;&#26032;&#27169;&#22411;&#21644;&#20256;&#32479;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25351;&#26631;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#19982;&#20154;&#31867;&#22312;&#20116;&#20010;&#32454;&#31890;&#24230;&#25925;&#20107;&#26041;&#38754;&#30340;&#21028;&#26029;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various evaluation metrics exist for natural language generation tasks, but they have limited utility for story generation since they generally do not correlate well with human judgments and do not measure fine-grained story aspects, such as fluency versus relatedness, as they are intended to assess overall generation quality. In this paper, we propose deltascore, an approach that utilizes perturbation to evaluate fine-grained story aspects. Our core idea is based on the hypothesis that the better the story performs in a specific aspect (e.g., fluency), the more it will be affected by a particular perturbation (e.g., introducing typos). To measure the impact, we calculate the likelihood difference between the pre- and post-perturbation stories using a language model. We evaluate deltascore against state-of-the-art model-based and traditional similarity-based metrics across multiple story domains, and investigate its correlation with human judgments on five fine-grained story aspects: f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#20351;&#29992;&#36328;&#39046;&#22495;&#25216;&#26415;&#23558;&#22823;&#22411;&#20135;&#21697;&#35780;&#35770;&#25968;&#25454;&#24211;&#30340;&#20998;&#31867;&#31995;&#32479;&#35757;&#32451;&#65292;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35199;&#29677;&#29273;&#35821;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08985</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#35199;&#29677;&#29273;&#35821;&#24773;&#24863;&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Sentiment Classification in Spanish. (arXiv:2303.08985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#20351;&#29992;&#36328;&#39046;&#22495;&#25216;&#26415;&#23558;&#22823;&#22411;&#20135;&#21697;&#35780;&#35770;&#25968;&#25454;&#24211;&#30340;&#20998;&#31867;&#31995;&#32479;&#35757;&#32451;&#65292;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35199;&#29677;&#29273;&#35821;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23398;&#26415;&#21644;&#21830;&#19994;&#24212;&#29992;&#12290;&#20854;&#30446;&#30340;&#26159;&#33258;&#21160;&#39044;&#27979;&#21253;&#21547;&#35266;&#28857;&#21644;&#20027;&#35266;&#24615;&#30340;&#25991;&#26412;&#20013;&#23384;&#22312;&#30340;&#24773;&#24863;&#31243;&#24230;&#65292;&#20363;&#22914;&#20135;&#21697;&#21644;&#30005;&#24433;&#35780;&#35770;&#25110;&#25512;&#25991;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#25991;&#26412;&#21253;&#21547;&#19981;&#21516;&#30340;&#35789;&#35821;&#21644;&#34920;&#36798;&#65292;&#22240;&#27492;&#36328;&#39046;&#22495;&#21644;&#36328;&#35821;&#35328;&#25216;&#26415;&#32463;&#24120;&#34987;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#20197;&#25913;&#21892;&#32467;&#26524;&#65292;&#32780;&#19988;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#24211;&#21644;&#36164;&#28304;&#65292;&#20351;&#29992;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#25991;&#26412;&#38590;&#24230;&#26356;&#22823;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#23545;&#20351;&#29992;&#22823;&#22411;&#20135;&#21697;&#35780;&#35770;&#25968;&#25454;&#24211;&#35757;&#32451;&#30340;&#20998;&#31867;&#31995;&#32479;&#22312;&#19981;&#21516;&#30340;&#35199;&#29677;&#29273;&#35821;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment Classification is a fundamental task in the field of Natural Language Processing, and has very important academic and commercial applications. It aims to automatically predict the degree of sentiment present in a text that contains opinions and subjectivity at some level, like product and movie reviews, or tweets. This can be really difficult to accomplish, in part, because different domains of text contains different words and expressions. In addition, this difficulty increases when text is written in a non-English language due to the lack of databases and resources. As a consequence, several cross-domain and cross-language techniques are often applied to this task in order to improve the results. In this work we perform a study on the ability of a classification system trained with a large database of product reviews to generalize to different Spanish domains. Reviews were collected from the MercadoLibre website from seven Latin American countries, allowing the creation of 
&lt;/p&gt;</description></item><item><title>PRESTO&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;55&#19975;&#20010;&#20154;&#19982;&#34394;&#25311;&#21161;&#25163;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#22810;&#35821;&#35328;&#23545;&#35805;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#22914;&#35828;&#35805;&#19981;&#36830;&#36143;&#12289;&#20195;&#30721;&#20999;&#25442;&#21644;&#20462;&#27491;&#31561;&#30495;&#23454;NLU&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.08954</link><description>&lt;p&gt;
PRESTO&#65306;&#29992;&#20110;&#35299;&#26512;&#36924;&#30495;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs. (arXiv:2303.08954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08954
&lt;/p&gt;
&lt;p&gt;
PRESTO&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;55&#19975;&#20010;&#20154;&#19982;&#34394;&#25311;&#21161;&#25163;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#22810;&#35821;&#35328;&#23545;&#35805;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#22914;&#35828;&#35805;&#19981;&#36830;&#36143;&#12289;&#20195;&#30721;&#20999;&#25442;&#21644;&#20462;&#27491;&#31561;&#30495;&#23454;NLU&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Google Assistant&#12289;Alexa&#21644;Siri&#31561;&#31995;&#32479;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#21464;&#24471;&#26222;&#36941;&#65292;&#20154;&#20204;&#23545;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#30340;&#30740;&#31350;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#22312;&#25429;&#25417;&#21508;&#31181;&#29992;&#25143;&#30171;&#28857;&#30340;&#23454;&#38469;&#24773;&#20917;&#26041;&#38754;&#32570;&#20047;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#36825;&#38480;&#21046;&#20102;&#23398;&#26415;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20351;&#20851;&#20110;&#35299;&#26512;&#36924;&#30495;&#23545;&#35805;&#30340;&#19968;&#20123;&#25361;&#25112;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PRESTO&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;55&#19975;&#20010;&#20154;&#19982;&#34394;&#25311;&#21161;&#25163;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#22810;&#35821;&#35328;&#23545;&#35805;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;PRESTO&#21253;&#21547;&#20102;&#30495;&#23454;NLU&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#22810;&#26679;&#21270;&#25361;&#25112;&#65292;&#22914;&#35828;&#35805;&#19981;&#36830;&#36143;&#12289;&#20195;&#30721;&#20999;&#25442;&#21644;&#20462;&#27491;&#12290;&#23427;&#26159;&#21807;&#19968;&#19968;&#20010;&#25552;&#20379;&#27599;&#20010;&#31034;&#20363;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#22914;&#29992;&#25143;&#30340;&#32852;&#31995;&#20154;&#21644;&#21015;&#34920;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#29983;&#25104;&#20250;&#35805;&#35299;&#26512;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22522;&#20110;mT5&#27169;&#22411;&#30340;&#22522;&#32447;&#34920;&#26126;&#65292;PRESTO&#20013;&#23384;&#22312;&#30340;&#23545;&#35805;&#29616;&#35937;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#22312;&#20302;&#36164;&#28304;&#35774;&#32622;&#20013;&#26356;&#21152;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research interest in task-oriented dialogs has increased as systems such as Google Assistant, Alexa and Siri have become ubiquitous in everyday life. However, the impact of academic research in this area has been limited by the lack of datasets that realistically capture the wide array of user pain points. To enable research on some of the more challenging aspects of parsing realistic conversations, we introduce PRESTO, a public dataset of over 550K contextual multilingual conversations between humans and virtual assistants. PRESTO contains a diverse array of challenges that occur in real-world NLU tasks such as disfluencies, code-switching, and revisions. It is the only large scale human generated conversational parsing dataset that provides structured context such as a user's contacts and lists for each example. Our mT5 model based baselines demonstrate that the conversational phenomenon present in PRESTO are challenging to model, which is further pronounced in a low-resource setup.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#20986;&#38498;&#21333;&#20013;&#25552;&#21462;&#27010;&#24565;&#65292;&#24182;&#24212;&#29992;&#26080;&#30417;&#30563;&#20851;&#38190;&#35789;&#26041;&#27861;&#35782;&#21035;&#37325;&#35201;&#27010;&#24565;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#20020;&#24202;&#25991;&#26412;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.08928</link><description>&lt;p&gt;
&#24212;&#29992;&#26080;&#30417;&#30563;&#20851;&#38190;&#35789;&#26041;&#27861;&#23545;&#20986;&#38498;&#21333;&#20013;&#25552;&#21462;&#30340;&#27010;&#24565;&#36827;&#34892;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Applying unsupervised keyphrase methods on concepts extracted from discharge sheets. (arXiv:2303.08928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#20986;&#38498;&#21333;&#20013;&#25552;&#21462;&#27010;&#24565;&#65292;&#24182;&#24212;&#29992;&#26080;&#30417;&#30563;&#20851;&#38190;&#35789;&#26041;&#27861;&#35782;&#21035;&#37325;&#35201;&#27010;&#24565;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#20020;&#24202;&#25991;&#26412;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#29992;&#21508;&#31181;&#31185;&#23398;&#27700;&#24179;&#21644;&#20889;&#20316;&#39118;&#26684;&#32534;&#20889;&#21253;&#21547;&#26377;&#20215;&#20540;&#24739;&#32773;&#20449;&#24687;&#30340;&#20020;&#24202;&#35760;&#24405;&#12290;&#23545;&#20110;&#22788;&#29702;&#24191;&#27867;&#30340;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65292;&#29702;&#35299;&#20160;&#20040;&#20449;&#24687;&#26159;&#24517;&#35201;&#30340;&#21487;&#33021;&#23545;&#20020;&#24202;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#26377;&#25152;&#24110;&#21161;&#12290;&#23454;&#20307;&#35782;&#21035;&#21644;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#20934;&#26415;&#35821;&#26159;&#20943;&#23569;&#22788;&#29702;&#20020;&#24202;&#35760;&#24405;&#20013;&#30340;&#27495;&#20041;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23613;&#31649;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#23454;&#20307;&#38142;&#25509;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#20063;&#21487;&#33021;&#23548;&#33268;&#20135;&#29983;&#37325;&#22797;&#21644;&#20302;&#20215;&#20540;&#30340;&#27010;&#24565;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#30830;&#23450;&#27599;&#20010;&#20869;&#23481;&#35760;&#24405;&#30340;&#37096;&#20998;&#24182;&#35782;&#21035;&#20851;&#38190;&#27010;&#24565;&#20197;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#21547;&#20041;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#20986;&#38498;&#21333;&#20013;&#25552;&#21462;&#27010;&#24565;&#65292;&#24182;&#24212;&#29992;&#26080;&#30417;&#30563;&#20851;&#38190;&#35789;&#26041;&#27861;&#26469;&#35782;&#21035;&#37325;&#35201;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#35782;&#21035;&#20986;&#20851;&#38190;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical notes containing valuable patient information are written by different health care providers with various scientific levels and writing styles. It might be helpful for clinicians and researchers to understand what information is essential when dealing with extensive electronic medical records. Entities recognizing and mapping them to standard terminologies is crucial in reducing ambiguity in processing clinical notes. Although named entity recognition and entity linking are critical steps in clinical natural language processing, they can also result in the production of repetitive and low-value concepts. In other hand, all parts of a clinical text do not share the same importance or content in predicting the patient's condition. As a result, it is necessary to identify the section in which each content is recorded and also to identify key concepts to extract meaning from clinical texts. In this study, these challenges have been addressed by using clinical natural language proc
&lt;/p&gt;</description></item><item><title>SelfCheckGPT&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#38646;&#36164;&#28304;&#30340;&#26041;&#24335;&#26816;&#26597;&#40657;&#30418;&#27169;&#22411;&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.08896</link><description>&lt;p&gt;
SelfCheckGPT: &#38646;&#36164;&#28304;&#40657;&#30418;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#29992;&#20110;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. (arXiv:2303.08896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08896
&lt;/p&gt;
&lt;p&gt;
SelfCheckGPT&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#38646;&#36164;&#28304;&#30340;&#26041;&#24335;&#26816;&#26597;&#40657;&#30418;&#27169;&#22411;&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20363;&#22914;GPT-3&#65292;&#33021;&#22815;&#23545;&#21508;&#31181;&#29992;&#25143;&#25552;&#31034;&#36827;&#34892;&#39640;&#24230;&#27969;&#30021;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;LLM&#24050;&#30693;&#20250;&#20135;&#29983;&#24187;&#35273;&#20107;&#23454;&#21644;&#38750;&#20107;&#23454;&#38472;&#36848;&#65292;&#36825;&#21487;&#33021;&#20250;&#21066;&#24369;&#23545;&#23427;&#20204;&#30340;&#36755;&#20986;&#30340;&#20449;&#20219;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#26816;&#26597;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#35775;&#38382;&#20196;&#29260;&#32423;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#65288;&#36825;&#21487;&#33021;&#23545;&#20110;ChatGPT&#31561;&#31995;&#32479;&#26469;&#35828;&#19981;&#21487;&#29992;&#65289;&#65292;&#35201;&#20040;&#38656;&#35201;&#36890;&#36807;&#21333;&#29420;&#30340;&#36890;&#24120;&#22797;&#26434;&#30340;&#27169;&#22359;&#25509;&#21475;&#30340;&#22806;&#37096;&#25968;&#25454;&#24211;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;SelfCheckGPT&#8221;&#65292;&#21487;&#20197;&#20197;&#38646;&#36164;&#28304;&#30340;&#26041;&#24335;&#26816;&#26597;&#40657;&#30418;&#27169;&#22411;&#65292;&#21363;&#19981;&#38656;&#35201;&#22806;&#37096;&#25968;&#25454;&#24211;&#12290; SelfCheckGPT&#21033;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#24605;&#24819;&#65306;&#22914;&#26524;LLM&#20855;&#26377;&#29305;&#23450;&#27010;&#24565;&#30340;&#30693;&#35782;&#65292;&#21017;&#37319;&#26679;&#30340;&#21709;&#24212;&#21487;&#33021;&#31867;&#20284;&#24182;&#21253;&#21547;&#19968;&#33268;&#30340;&#20107;&#23454;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#24187;&#35273;&#30340;&#20107;&#23454;&#65292;&#38543;&#26426;&#37319;&#26679;&#30340;&#21709;&#24212;&#21487;&#33021;&#20250;&#21457;&#25955;&#24182;&#30456;&#20114;&#30683;&#30462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;GP-T-3&#27169;&#22411;&#20026;&#20363;&#26469;&#30740;&#31350;&#27492;&#26041;&#27861;&#65292;&#24182;&#22312;&#24120;&#35265;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SelfCheckGPT&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#27169;&#22411;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#19988;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to token-level output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if a LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312; Coq &#20013;&#23545; Operad &#30340;&#24418;&#24335;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#21033;&#29992; operad &#26469;&#24418;&#24335;&#21270;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#20041;&#23398;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2303.08894</link><description>&lt;p&gt;
Coq &#20013; Operad &#30340;&#24418;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Formalization of Operads in Coq. (arXiv:2303.08894v1 [math.CT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312; Coq &#20013;&#23545; Operad &#30340;&#24418;&#24335;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#21033;&#29992; operad &#26469;&#24418;&#24335;&#21270;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#20041;&#23398;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#21738;&#31181;&#29305;&#24615;&#33021;&#22815;&#25552;&#20379;&#26368;&#39640;&#30340;&#27491;&#30830;&#24615;&#20445;&#35777;&#65311;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#31572;&#26696;&#65292;&#20063;&#26159;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#26159;&#22312;&#32534;&#31243;&#35821;&#35328;&#20013;&#25552;&#20379;&#19968;&#20010;&#24418;&#24335;&#21270;&#29256;&#26412;&#65292;&#29305;&#21035;&#26159;&#22914;&#26524;&#23384;&#22312;&#21487;&#34920;&#31034;&#30340;&#35821;&#20041;&#30340;&#35805;&#12290;&#36798;&#21040;&#36825;&#26679;&#30340;&#24418;&#24335;&#21270;&#29256;&#26412;&#20026;&#30830;&#20445;&#26500;&#36896;&#27491;&#30830;&#24615;&#25552;&#20379;&#20102;&#37329;&#26631;&#20934;&#12290;&#22312; DARPA V-SPELLS &#35745;&#21010;&#20013;&#65292;&#25105;&#20204;&#21162;&#21147;&#25552;&#20379;&#20102;&#20803;&#35821;&#35328;&#35821;&#20041;&#23398;&#22522;&#30784;&#30340;&#19968;&#20010;&#24418;&#24335;&#21270;&#29256;&#26412;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21483;&#20570; operad &#30340;&#25968;&#23398;&#23545;&#35937;&#12290;&#36825;&#20010;&#23545;&#35937;&#20855;&#26377;&#32452;&#21512;&#23646;&#24615;&#65292;&#23545;&#20110;&#29992;&#26356;&#23567;&#30340;&#20195;&#30721;&#22359;&#26500;&#24314;&#35821;&#35328;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102; operad &#22312; Coq &#35777;&#26126;&#21161;&#25163;&#20013;&#30340;&#24418;&#24335;&#21270;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312; Coq &#20013;&#23450;&#20041;&#30340; operad &#33021;&#22815;&#25552;&#20379;&#25351;&#23450;&#22312; Coq &#20013;&#30340;&#29289;&#20307;&#26159; operad &#30340;&#35777;&#26126;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312; Coq &#20013;&#20026;&#25105;&#20204;&#22312; V-SPELLS &#20013;&#24320;&#21457;&#20803;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#30340;&#25968;&#23398;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#20026;&#32534;&#31243;&#35821;&#35328;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#21033;&#29992; operad &#26469;&#24418;&#24335;&#21270;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#20041;&#23398;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
What provides the highest level of assurance for correctness of execution within a programming language? One answer, and our solution in particular, to this problem is to provide a formalization for, if it exists, the denotational semantics of a programming language. Achieving such a formalization provides a gold standard for ensuring a programming language is correct-by-construction. In our effort on the DARPA V-SPELLS program, we worked to provide a foundation for the denotational semantics of a meta-language using a mathematical object known as an operad. This object has compositional properties which are vital to building languages from smaller pieces. In this paper, we discuss our formalization of an operad in the proof assistant Coq. Moreover, our definition within Coq is capable of providing proofs that objects specified within Coq are operads. This work within Coq provides a formal mathematical basis for our meta-language development within V-SPELLS. Our work also provides, to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;ROSE&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#27861;&#30340;&#31070;&#32463;&#35745;&#31639;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#31070;&#32463;&#22797;&#26434;&#24230;&#27700;&#24179;&#19978;&#36827;&#34892;&#32467;&#26500;&#26500;&#24314;&#24182;&#36827;&#34892;&#22522;&#26412;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2303.08877</link><description>&lt;p&gt;
ROSE: &#19968;&#31181;&#29992;&#20110;&#35821;&#27861;&#30340;&#31070;&#32463;&#35745;&#31639;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
ROSE: A Neurocomputational Architecture for Syntax. (arXiv:2303.08877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;ROSE&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#27861;&#30340;&#31070;&#32463;&#35745;&#31639;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#31070;&#32463;&#22797;&#26434;&#24230;&#27700;&#24179;&#19978;&#36827;&#34892;&#32467;&#26500;&#26500;&#24314;&#24182;&#36827;&#34892;&#22522;&#26412;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#20013;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#32508;&#21512;&#27169;&#22411;&#24517;&#39035;&#21253;&#21547;&#22235;&#20010;&#32452;&#20214;&#65306;&#34920;&#31034;&#12289;&#25805;&#20316;&#12289;&#32467;&#26500;&#21644;&#32534;&#30721;&#12290;&#23427;&#36824;&#38656;&#35201;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#35299;&#37322;&#65292;&#38416;&#36848;&#36825;&#20123;&#32452;&#20214;&#22914;&#20309;&#22312;&#26426;&#21046;&#19978;&#21644;&#22240;&#26524;&#19978;&#30456;&#20114;&#20851;&#32852;&#12290;&#26412;&#25991;&#36890;&#36807;&#25193;&#23637;&#31070;&#32463;&#25391;&#33633;&#22914;&#20309;&#25351;&#31034;&#21508;&#31181;&#35821;&#35328;&#36807;&#31243;&#30340;&#29616;&#26377;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#27861;&#30340;&#31070;&#32463;&#35745;&#31639;&#26550;&#26500;&#65292;&#31216;&#20026;ROSE&#27169;&#22411;&#65288;&#34920;&#31034;&#12289;&#25805;&#20316;&#12289;&#32467;&#26500;&#12289;&#32534;&#30721;&#65289;&#12290;&#22312;ROSE&#19979;&#65292;&#35821;&#27861;&#30340;&#22522;&#26412;&#25968;&#25454;&#32467;&#26500;&#26159;&#21407;&#23376;&#29305;&#24449;&#12289;&#21508;&#31181;&#24515;&#29702;&#34920;&#31034;&#31867;&#22411;&#65288;R&#65289;&#65292;&#24182;&#20197;&#21333;&#20803;&#21644;&#38598;&#21512;&#32423;&#21035;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#20123;&#21333;&#20301;&#32463;&#36807;&#22522;&#26412;&#35745;&#31639;&#65288;0&#65289;&#36716;&#21270;&#20026;&#21487;&#20379;&#21518;&#32493;&#32467;&#26500;&#26500;&#24314;&#32423;&#21035;&#35775;&#38382;&#30340;&#21487;&#25805;&#20316;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
A comprehensive model of natural language processing in the brain must accommodate four components: representations, operations, structures and encoding. It further requires a principled account of how these components mechanistically, and causally, relate to each another. While previous models have isolated regions of interest for structure-building and lexical access, many gaps remain with respect to bridging distinct scales of neural complexity. By expanding existing accounts of how neural oscillations can index various linguistic processes, this article proposes a neurocomputational architecture for syntax, termed the ROSE model (Representation, Operation, Structure, Encoding). Under ROSE, the basic data structures of syntax are atomic features, types of mental representations (R), and are coded at the single-unit and ensemble level. Elementary computations (O) that transform these units into manipulable objects accessible to subsequent structure-building levels are coded via high 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;Anchors&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.08806</link><description>&lt;p&gt;
&#29702;&#35299;&#20107;&#21518;&#35299;&#37322;&#22120;&#65306;&#20197;Anchors&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Understanding Post-hoc Explainers: The Case of Anchors. (arXiv:2303.08806v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Anchors&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#39640;&#24230;&#35201;&#27714;&#20294;&#38590;&#20197;&#23454;&#29616;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#30340;&#20010;&#20307;&#39044;&#27979;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#26412;&#22320;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#35299;&#37322;&#30340;&#36807;&#31243;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#21487;&#33021;&#19982;&#35201;&#35299;&#37322;&#30340;&#39044;&#27979;&#19968;&#26679;&#31070;&#31192;&#12290;&#27492;&#22806;&#65292;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#32463;&#24120;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#23427;&#20204;&#22312;&#31616;&#21333;&#27169;&#22411;&#19978;&#30340;&#34892;&#20026;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#23545;Anchors&#65288;Ribeiro&#31561;&#20154;&#65292;2018&#65289;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65306;&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#24378;&#35843;&#19968;&#23567;&#32452;&#21333;&#35789;&#20197;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many scenarios, the interpretability of machine learning models is a highly required but difficult task. To explain the individual predictions of such models, local model-agnostic approaches have been proposed. However, the process generating the explanations can be, for a user, as mysterious as the prediction to be explained. Furthermore, interpretability methods frequently lack theoretical guarantees, and their behavior on simple models is frequently unknown. While it is difficult, if not impossible, to ensure that an explainer behaves as expected on a cutting-edge model, we can at least ensure that everything works on simple, already interpretable models. In this paper, we present a theoretical analysis of Anchors (Ribeiro et al., 2018): a popular rule-based interpretability method that highlights a small set of words to explain a text classifier's decision. After formalizing its algorithm and providing useful insights, we demonstrate mathematically that Anchors produces meaningf
&lt;/p&gt;</description></item><item><title>GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08774</link><description>&lt;p&gt;
GPT-4&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Technical Report. (arXiv:2303.08774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08774
&lt;/p&gt;
&lt;p&gt;
GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;GPT-4&#30340;&#24320;&#21457;&#65292;&#23427;&#26159;&#19968;&#20010;&#21487;&#20197;&#25509;&#21463;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#22914;&#20154;&#31867;&#65292;&#20294;GPT-4&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#65292;&#25104;&#32489;&#25490;&#21517;&#22312;&#21069;10&#65285;&#24038;&#21491;&#12290;GPT-4&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#25991;&#26723;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#21518;&#35757;&#32451;&#23545;&#40784;&#36807;&#31243;&#25552;&#39640;&#20102;&#20107;&#23454;&#24615;&#21644;&#31526;&#21512;&#26399;&#26395;&#34892;&#20026;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;GPT-4&#30340;&#26576;&#20123;&#24615;&#33021;&#26041;&#38754;&#65292;&#32780;&#36825;&#20123;&#24615;&#33021;&#26159;&#22522;&#20110;&#20351;&#29992;&#19981;&#36229;&#36807;GPT-4&#35745;&#31639;&#33021;&#21147;&#30340;1/1,000&#30340;&#27169;&#22411;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
&lt;/p&gt;</description></item><item><title>FactReranker&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#35780;&#20272;&#22120;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#25688;&#35201;&#19982;&#25918;&#23556;&#23398;&#21457;&#29616;&#23454;&#20917;&#19968;&#33268;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20107;&#23454;&#24341;&#23548;&#26469;&#26377;&#25928;&#22320;&#36873;&#25321;&#26368;&#20339;&#30340;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2303.08335</link><description>&lt;p&gt;
FactReranker&#65306;&#22522;&#20110;&#20107;&#23454;&#24341;&#23548;&#30340;&#36741;&#21161;&#35780;&#20272;&#22120;&#29992;&#20110;&#24544;&#23454;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
FactReranker: Fact-guided Reranker for Faithful Radiology Report Summarization. (arXiv:2303.08335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08335
&lt;/p&gt;
&lt;p&gt;
FactReranker&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#35780;&#20272;&#22120;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#25688;&#35201;&#19982;&#25918;&#23556;&#23398;&#21457;&#29616;&#23454;&#20917;&#19968;&#33268;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20107;&#23454;&#24341;&#23548;&#26469;&#26377;&#25928;&#22320;&#36873;&#25321;&#26368;&#20339;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#20854;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#20445;&#25345;&#25152;&#20135;&#29983;&#30340;&#25688;&#35201;&#21644;&#22320;&#38754;&#23454;&#20917;&#25918;&#23556;&#23398;&#21457;&#29616;&#20043;&#38388;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30740;&#31350;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#30452;&#25509;&#20248;&#21270;&#27491;&#30830;&#35748;&#30693;&#24230;&#37327;&#25351;&#26631;&#65292;&#22914;CheXBert&#25110;RadGraph&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20351;&#29992;&#36138;&#23146;&#25628;&#32034;&#25110;&#26463;&#25628;&#32034;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#22312;&#36873;&#25321;&#26368;&#20339;&#20505;&#36873;&#39033;&#26102;&#27809;&#26377;&#32771;&#34385;&#20107;&#23454;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#23454;&#38469;&#19968;&#33268;&#24615;&#30340;&#25913;&#21892;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31532;&#20108;&#38454;&#27573;&#25688;&#35201;&#26041;&#27861;FactReranker&#65292;&#23427;&#26159;&#31532;&#19968;&#27425;&#23581;&#35797;&#22522;&#20110;&#23427;&#20204;&#20272;&#35745;&#30340;&#23454;&#38469;&#19968;&#33268;&#24615;&#24471;&#20998;&#26469;&#23398;&#20064;&#20174;&#25152;&#26377;&#20505;&#36873;&#39033;&#20013;&#36873;&#25321;&#26368;&#20339;&#25688;&#35201;&#12290;&#25105;&#20204;&#24314;&#35758;&#22522;&#20110;RadGraph&#27169;&#24335;&#25552;&#21462;&#36755;&#20837;&#21307;&#30103;&#25253;&#21578;&#12289;&#20854;&#40644;&#37329;&#25688;&#35201;&#21644;&#20505;&#36873;&#25688;&#35201;&#30340;&#21307;&#30103;&#20107;&#23454;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#20107;&#23454;&#24341;&#23548;&#30340;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#20197;&#26377;&#25928;&#22320;&#32467;&#21512;&#25552;&#21462;&#30340;&#21307;&#30103;&#20107;&#23454;&#26469;&#36873;&#25321;&#26368;&#20339;&#25688;&#35201;&#12290;&#25105;&#20204;&#20998;&#35299;&#20102;&#20107;&#23454;-
&lt;/p&gt;
&lt;p&gt;
Automatic radiology report summarization is a crucial clinical task, whose key challenge is to maintain factual accuracy between produced summaries and ground truth radiology findings. Existing research adopts reinforcement learning to directly optimize factual consistency metrics such as CheXBert or RadGraph score. However, their decoding method using greedy search or beam search considers no factual consistency when picking the optimal candidate, leading to limited factual consistency improvement. To address it, we propose a novel second-stage summarizing approach FactReranker, the first attempt that learns to choose the best summary from all candidates based on their estimated factual consistency score. We propose to extract medical facts of the input medical report, its gold summary, and candidate summaries based on the RadGraph schema and design the fact-guided reranker to efficiently incorporate the extracted medical facts for selecting the optimal summary. We decompose the fact-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.08302</link><description>&lt;p&gt;
&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#23384;&#28040;&#32791;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#37327;&#21270;&#26041;&#26696;&#12289;&#19981;&#21516;&#27169;&#22411;&#26063;&#12289;&#19981;&#21516;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12289;&#19981;&#21516;&#37327;&#21270;&#20301;&#31934;&#24230;&#31561;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#30740;&#31350;&#20173;&#32570;&#22833;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#36825;&#20123;&#32452;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(1)&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;(&#32780;&#19981;&#26159;&#26420;&#32032;&#30340;&#26368;&#36817;&#33293;&#20837;&#37327;&#21270;)&#26159;&#23454;&#29616;&#33391;&#22909;&#31934;&#24230;&#30340;&#24517;&#35201;&#26465;&#20214;&#65307;(2) &#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#65288;&#22914;5&#20301;&#65289;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#65288;&#22914;4&#20301;&#65289;&#65288;&#20854;&#26377;&#25928;&#20301;&#25968;&#19982;5&#20301;&#30456;&#20284;&#65289;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#65292;&#24182;&#30041;&#19979;&#26410;&#26469;&#26426;&#20250;&#21644;&#31995;&#32479;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (\ptq) had been recently shown as a compromising method to reduce the memory consumption and/or compute cost for large language models. However, a comprehensive study about the effect of different quantization schemes, different model families, different \ptq methods, different quantization bit precision, etc, is still missing. In this work, we provide an extensive study on those components over tens of thousands of zero-shot experiments. Our results show that (1) Fine-grained quantization and \ptq methods (instead of naive round-to-nearest quantization) are necessary to achieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained quantization is more powerful than lower bits (e.g., 4 bits) with very fine-grained quantization (whose effective bits is similar to 5-bits). We also present recommendations about how to utilize quantization for \llms with different sizes, and leave suggestions of future opportunities and system work that are not res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#32929;&#31080;&#20215;&#26684;&#30340;&#30456;&#20851;&#24615;&#24314;&#31435;&#20851;&#31995;&#65292;&#23454;&#29616;&#37329;&#34701;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#20102;&#35299;&#24773;&#24863;&#20998;&#26512;&#19982;&#32929;&#31080;&#20215;&#26684;&#20043;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#20998;&#26512;&#24072;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.02563</link><description>&lt;p&gt;
FinXABSA: &#36890;&#36807;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#37329;&#34701;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis. (arXiv:2303.02563v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#32929;&#31080;&#20215;&#26684;&#30340;&#30456;&#20851;&#24615;&#24314;&#31435;&#20851;&#31995;&#65292;&#23454;&#29616;&#37329;&#34701;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#20102;&#35299;&#24773;&#24863;&#20998;&#26512;&#19982;&#32929;&#31080;&#20215;&#26684;&#20043;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#20998;&#26512;&#24072;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an aspect-based sentiment analysis approach to achieve explainability in financial analysis by establishing a relationship with stock prices using the Pearson correlation coefficient. The proposed methodology provides a more detailed and accurate understanding of the relationship between sentiment analysis and stock prices, which can be useful for investors and financial analysts in making informed decisions.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Pearson&#30456;&#20851;&#31995;&#25968;&#24314;&#31435;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#19982;&#32929;&#31080;&#20215;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#37329;&#34701;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#20174;&#37329;&#34701;&#26032;&#38395;&#25991;&#31456;&#20013;&#26500;&#24314;&#26041;&#38754;&#21015;&#34920;&#65292;&#24182;&#20998;&#26512;&#27599;&#20010;&#26041;&#38754;&#30340;&#24773;&#24863;&#24378;&#24230;&#24471;&#20998;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;Pearson&#31995;&#25968;&#23558;&#36825;&#20123;&#24471;&#20998;&#19982;&#30456;&#20851;&#20844;&#21496;&#30340;&#32929;&#31080;&#20215;&#26684;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#30830;&#23450;&#20219;&#20309;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#20102;&#35299;&#24773;&#24863;&#20998;&#26512;&#19982;&#32929;&#31080;&#20215;&#26684;&#20043;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#20998;&#26512;&#24072;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26469;&#35299;&#37322;&#24773;&#24863;&#20998;&#26512;&#32467;&#26524;&#21450;&#20854;&#23545;&#32929;&#31080;&#20215;&#26684;&#30340;&#24433;&#21709;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#25991;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#35299;&#37322;&#24615;&#22312;&#37329;&#34701;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach for explainability in financial analysis by utilizing the Pearson correlation coefficient to establish a relationship between aspect-based sentiment analysis and stock prices. The proposed methodology involves constructing an aspect list from financial news articles and analyzing sentiment intensity scores for each aspect. These scores are then compared to the stock prices for the relevant companies using the Pearson coefficient to determine any significant correlations. The results indicate that the proposed approach provides a more detailed and accurate understanding of the relationship between sentiment analysis and stock prices, which can be useful for investors and financial analysts in making informed decisions. Additionally, this methodology offers a transparent and interpretable way to explain the sentiment analysis results and their impact on stock prices. Overall, the findings of this paper demonstrate the importance of explainability in f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Prophet&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#22312;&#29305;&#23450;&#30340;&#30693;&#35782;&#22411;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20986;&#31572;&#26696;&#21551;&#21457;&#24335;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.01903</link><description>&lt;p&gt;
&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering. (arXiv:2303.01903v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Prophet&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#22312;&#29305;&#23450;&#30340;&#30693;&#35782;&#22411;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20986;&#31572;&#26696;&#21551;&#21457;&#24335;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38656;&#35201;&#36229;&#20986;&#22270;&#20687;&#33539;&#22260;&#30340;&#22806;&#37096;&#30693;&#35782;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#20174;&#26174;&#24335;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#26816;&#32034;&#25152;&#38656;&#30340;&#30693;&#35782;&#65292;&#36825;&#32463;&#24120;&#20250;&#24341;&#20837;&#19982;&#38382;&#39064;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;GPT-3&#65289;&#20316;&#20026;&#38544;&#21547;&#24335;&#30693;&#35782;&#24341;&#25806;&#26469;&#33719;&#21462;&#22238;&#31572;&#25152;&#38656;&#30340;&#24517;&#35201;&#30693;&#35782;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23427;&#20204;&#36824;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;GPT-3&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#25552;&#20379;&#30340;&#36755;&#20837;&#20449;&#24687;&#20173;&#28982;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prophet&#8212;&#8212;&#19968;&#20010;&#27010;&#24565;&#19978;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22238;&#31572;&#21551;&#21457;&#24335;&#26041;&#24335;&#65292;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;VQA&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#29305;&#23450;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#32780;&#19981;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#20013;&#25552;&#21462;&#20102;&#20004;&#31181;&#20114;&#34917;&#30340;&#31572;&#26696;&#21551;&#21457;&#24335;&#65306;&#31572;&#26696;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have sought to use a large language model (i.e., GPT-3) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of GPT-3 as the provided input information is insufficient. In this paper, we present Prophet -- a conceptually simple framework designed to prompt GPT-3 with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the model: answer candidates 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#25361;&#25112;&#24615;&#20219;&#21153;&#8212;&#8212;&#35270;&#35273;&#24773;&#24863;&#35299;&#35835;&#20219;&#21153;(VEIT)&#65292;&#26088;&#22312;&#36890;&#36807;AI&#23545;&#21019;&#20316;&#32773;&#30340;&#24515;&#29702;&#29366;&#24577;&#36827;&#34892;&#21512;&#29702;&#30340;&#35299;&#37322;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#24471;&#21040;&#20102;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#25903;&#25345;&#21644;&#19987;&#19994;&#30340;&#27880;&#37322;&#12290;&#25454;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#25968;&#25454;&#38598;&#19981;&#20165;&#33021;&#22815;&#25903;&#25345;VEIT&#65292;&#32780;&#19988;&#30456;&#23545;&#20110;&#20854;&#20182;&#23383;&#24149;&#25968;&#25454;&#38598;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10276</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#21019;&#20316;&#35299;&#35835;&#24515;&#29702;&#29366;&#24577;&#65306;&#12298;&#30475;&#35265;&#20320;&#30340;&#20869;&#24515;&#12299;
&lt;/p&gt;
&lt;p&gt;
See Your Heart: Psychological states Interpretation through Visual Creations. (arXiv:2302.10276v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#25361;&#25112;&#24615;&#20219;&#21153;&#8212;&#8212;&#35270;&#35273;&#24773;&#24863;&#35299;&#35835;&#20219;&#21153;(VEIT)&#65292;&#26088;&#22312;&#36890;&#36807;AI&#23545;&#21019;&#20316;&#32773;&#30340;&#24515;&#29702;&#29366;&#24577;&#36827;&#34892;&#21512;&#29702;&#30340;&#35299;&#37322;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#24471;&#21040;&#20102;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#25903;&#25345;&#21644;&#19987;&#19994;&#30340;&#27880;&#37322;&#12290;&#25454;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#25968;&#25454;&#38598;&#19981;&#20165;&#33021;&#22815;&#25903;&#25345;VEIT&#65292;&#32780;&#19988;&#30456;&#23545;&#20110;&#20854;&#20182;&#23383;&#24149;&#25968;&#25454;&#38598;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31934;&#31070;&#20998;&#26512;&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#35270;&#35273;&#21019;&#20316;&#29983;&#25104;&#23545;&#20010;&#20307;&#24515;&#29702;&#29366;&#24577;&#30340;&#35299;&#37322;&#27491;&#38754;&#20020;&#30528;&#24456;&#22823;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#20027;&#35201;&#21253;&#25324;&#24773;&#32490;&#20998;&#31867;&#21644;&#24433;&#21709;&#26631;&#27880;&#20004;&#20010;&#20219;&#21153;&#65292;&#38590;&#20197;&#28385;&#36275;&#24515;&#29702;&#35299;&#37322;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#28385;&#36275;&#31934;&#31070;&#20998;&#26512;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#39033;&#25361;&#25112;&#24615;&#20219;&#21153;&#8212;&#8212;&#35270;&#35273;&#24773;&#24863;&#35299;&#35835;&#20219;&#21153;(VEIT)&#12290;VEIT&#35201;&#27714;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#35270;&#35273;&#21019;&#20316;&#29983;&#25104;&#21512;&#29702;&#30340;&#21019;&#20316;&#32773;&#24515;&#29702;&#29366;&#24577;&#30340;&#35299;&#37322;&#12290;&#20026;&#25903;&#25345;&#35813;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;Sandplay Interpretation Dataset (SpyIn)&#65292;&#35813;&#25968;&#25454;&#38598;&#24471;&#21040;&#20102;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#25903;&#25345;&#21644;&#19987;&#19994;&#30340;&#27880;&#37322;&#12290;&#25968;&#25454;&#38598;&#20998;&#26512;&#34920;&#26126;&#65292;SpyIn&#19981;&#20165;&#33021;&#22815;&#25903;&#25345;VEIT&#65292;&#32780;&#19988;&#30456;&#23545;&#20110;&#20854;&#20182;&#23383;&#24149;&#25968;&#25454;&#38598;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22522;&#20110;SpyIn&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#39033;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In psychoanalysis, generating interpretations to one's psychological state through visual creations is facing significant demands. The two main tasks of existing studies in the field of computer vision, sentiment/emotion classification and affective captioning, can hardly satisfy the requirement of psychological interpreting. To meet the demands for psychoanalysis, we introduce a challenging task, \textbf{V}isual \textbf{E}motion \textbf{I}nterpretation \textbf{T}ask (VEIT). VEIT requires AI to generate reasonable interpretations of creator's psychological state through visual creations. To support the task, we present a multimodal dataset termed SpyIn (\textbf{S}and\textbf{p}la\textbf{y} \textbf{In}terpretation Dataset), which is psychological theory supported and professional annotated. Dataset analysis illustrates that SpyIn is not only able to support VEIT, but also more challenging compared with other captioning datasets. Building on SpyIn, we conduct experiments of several image 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102; ChatGPT &#26159;&#21542;&#21487;&#29992;&#20110;&#25552;&#20379;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;(NLE), &#25968;&#25454;&#34920;&#26126;ChatGPT&#29983;&#25104;&#30340;NLEs&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#36136;&#37327;&#26356;&#22909;&#65292;&#20294;&#20173;&#26377;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.07736</link><description>&lt;p&gt;
ChatGPT&#27604;&#20154;&#31867;&#26631;&#27880;&#21592;&#26356;&#22909;&#21527;? ChatGPT&#22312;&#35299;&#37322;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;(arXiv:2302.07736v2 [cs.CL]&#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech. (arXiv:2302.07736v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102; ChatGPT &#26159;&#21542;&#21487;&#29992;&#20110;&#25552;&#20379;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;(NLE), &#25968;&#25454;&#34920;&#26126;ChatGPT&#29983;&#25104;&#30340;NLEs&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#36136;&#37327;&#26356;&#22909;&#65292;&#20294;&#20173;&#26377;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24456;&#22810;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#37117;&#26159;&#38544;&#21547;&#30340;&#12290;&#30001;&#20110;&#20854;&#24494;&#22937;&#30340;&#24615;&#36136;&#65292;&#26816;&#27979;&#36825;&#31181;&#20167;&#24680;&#35328;&#35770;&#30340;&#21487;&#35299;&#37322;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102; ChatGPT &#26159;&#21542;&#21487;&#29992;&#20110;&#25552;&#20379;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;(NLE)&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#25552;&#31034;&#35821;&#20197;&#24341;&#20986;&#31616;&#27905;&#30340; ChatGPT &#29983;&#25104;&#30340; NLE&#65292;&#24182;&#36890;&#36807;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340; NLE &#36827;&#34892;&#27604;&#36739;&#65292;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#20197;&#35780;&#20272;&#20854;&#36136;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102; ChatGPT &#22312;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have alarmed that many online hate speeches are implicit. With its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. In this work, we examine whether ChatGPT can be used for providing natural language explanations (NLEs) for implicit hateful speech detection. We design our prompt to elicit concise ChatGPT-generated NLEs and conduct user studies to evaluate their qualities by comparison with human-written NLEs. We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#21487;&#20197;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#22312;&#32447;&#23545;&#35805;&#12290;&#20182;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#20379;&#22522;&#20110;&#35777;&#25454;&#30340;&#24314;&#35758;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#23545;&#35805;&#20013;&#24863;&#21463;&#21040;&#29702;&#35299;&#30340;&#24863;&#35273;&#12290;</title><link>http://arxiv.org/abs/2302.07268</link><description>&lt;p&gt;
AI&#32842;&#22825;&#21161;&#25163;&#21487;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
AI Chat Assistants can Improve Conversations about Divisive Topics. (arXiv:2302.07268v4 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#21487;&#20197;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#22312;&#32447;&#23545;&#35805;&#12290;&#20182;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#20379;&#22522;&#20110;&#35777;&#25454;&#30340;&#24314;&#35758;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#23545;&#35805;&#20013;&#24863;&#21463;&#21040;&#29702;&#35299;&#30340;&#24863;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#32447;&#20132;&#27969;&#25968;&#37327;&#27491;&#22312;&#36805;&#36895;&#22686;&#38271;&#12290;&#20294;&#26159;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#12289;&#28040;&#24687;&#24212;&#29992;&#31243;&#24207;&#21644;&#20854;&#20182;&#25968;&#23383;&#35770;&#22363;&#19978;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20114;&#21160;&#21487;&#33021;&#20250;&#20135;&#29983;&#20998;&#35010;&#21644;&#20914;&#31361;&#12290;&#36825;&#31181;&#26377;&#27602;&#24615;&#22686;&#21152;&#20102;&#26497;&#21270;&#30340;&#31243;&#24230;&#65292;&#24182;&#19988;&#37325;&#35201;&#30340;&#26159;&#65292;&#20405;&#34432;&#20102;&#22810;&#20803;&#21270;&#31038;&#20250;&#21457;&#23637;&#35299;&#20915;&#24433;&#21709;&#25152;&#26377;&#20154;&#30340;&#22797;&#26434;&#31038;&#20250;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23398;&#32773;&#21644;&#27665;&#38388;&#31038;&#20250;&#32452;&#32455;&#25512;&#21160;&#24178;&#39044;&#25514;&#26045;&#65292;&#20351;&#38754;&#23545;&#38754;&#30340;&#23545;&#35805;&#19981;&#37027;&#20040;&#20855;&#26377;&#20998;&#35010;&#24615;&#25110;&#26356;&#20855;&#29983;&#20135;&#21147;&#65292;&#20294;&#23558;&#36825;&#20123;&#21162;&#21147;&#25193;&#23637;&#33267;&#22312;&#32447;&#21457;&#29983;&#30340;&#35768;&#22810;&#35805;&#35821;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#35813;&#23454;&#39564;&#35777;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;&#20309;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#22312;&#32447;&#23545;&#35805;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#20379;&#22522;&#20110;&#35777;&#25454;&#30340;&#24314;&#35758;&#65292;&#20197;&#25913;&#21892;&#21442;&#19982;&#32773;&#22312;&#23545;&#35805;&#20013;&#24863;&#21463;&#21040;&#29702;&#35299;&#30340;&#24863;&#35273;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#24314;&#35758;&#30830;&#23454;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#25913;&#21892;&#23545;&#35805;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A rapidly increasing amount of human conversation occurs online. But divisiveness and conflict can fester in text-based interactions on social media platforms, in messaging apps, and on other digital forums. Such toxicity increases polarization and, importantly, corrodes the capacity of diverse societies to develop efficient solutions to complex social problems that impact everyone. Scholars and civil society groups promote interventions that can make interpersonal conversations less divisive or more productive in offline settings, but scaling these efforts to the amount of discourse that occurs online is extremely challenging. We present results of a large-scale experiment that demonstrates how online conversations about divisive topics can be improved with artificial intelligence tools. Specifically, we employ a large language model to make real-time, evidence-based recommendations intended to improve participants' perception of feeling understood in conversations. We find that these
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Meta-SN&#65292;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#36830;&#23545;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20046;&#27809;&#26377;&#26631;&#31614;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;Meta-SN&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#32534;&#30721;&#31867;&#21035;&#26631;&#31614;&#30340;&#20302;&#32500;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#20811;&#26381;&#20102;&#20449;&#24565;&#32593;&#32476;&#31639;&#27861;&#20013;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.03507</link><description>&lt;p&gt;
&#20960;&#20046;&#27809;&#26377;&#26631;&#31614;&#30340;&#25991;&#26412;&#20998;&#31867;&#30340;&#20803;&#23398;&#20064;&#36830;&#23545;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Siamese Network for Few-Shot Text Classification. (arXiv:2302.03507v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Meta-SN&#65292;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#36830;&#23545;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20046;&#27809;&#26377;&#26631;&#31614;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;Meta-SN&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#32534;&#30721;&#31867;&#21035;&#26631;&#31614;&#30340;&#20302;&#32500;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#20811;&#26381;&#20102;&#20449;&#24565;&#32593;&#32476;&#31639;&#27861;&#20013;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20046;&#27809;&#26377;&#26631;&#31614;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#35757;&#32451;&#25968;&#25454;&#26469;&#35299;&#20915;&#65292;&#20854;&#20013;&#20803;&#23398;&#20064;&#26041;&#27861;&#65288;&#20363;&#22914;PROTO&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#36830;&#23545;&#32593;&#32476;&#65292;Meta-SN&#65292;&#26469;&#35299;&#20915;PROTO&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#19977;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#24573;&#30053;&#20102;&#35745;&#31639;&#21407;&#22411;&#21521;&#37327;&#26102;&#37319;&#26679;&#25903;&#25345;&#38598;&#30340;&#38543;&#26426;&#24615;&#65307;&#65288;2&#65289;&#24573;&#30053;&#20102;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65307;&#65288;3&#65289;&#20197;&#32431;&#38543;&#26426;&#26041;&#24335;&#26500;&#24314;&#20803;&#20219;&#21153;&#12290;Meta-SN&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65288;&#20363;&#22914;&#31867;&#21035;&#21517;&#31216;&#21644;&#25551;&#36848;&#25991;&#26412;&#65289;&#26469;&#32534;&#30721;&#31867;&#21035;&#26631;&#31614;&#30340;&#20302;&#32500;&#23884;&#20837;&#21521;&#37327;&#65292;&#32780;&#19981;&#26159;&#20174;&#37319;&#26679;&#25903;&#25345;&#38598;&#20013;&#35745;&#31639;&#21407;&#22411;&#21521;&#37327;&#12290;&#27492;&#22806;&#65292;Meta-SN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#21363;&#22686;&#21152;&#20102;&#23545;&#38590;&#20197;&#20998;&#31867;&#26679;&#26412;&#30340;&#37319;&#26679;&#27010;&#29575;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;Meta-SN&#20248;&#20110;&#21253;&#25324;PROTO&#22312;&#20869;&#30340;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning has been used to tackle the problem of label scarcity in text classification, of which meta-learning based methods have shown to be effective, such as the prototypical networks (PROTO). Despite the success of PROTO, there still exist three main problems: (1) ignore the randomness of the sampled support sets when computing prototype vectors; (2) disregard the importance of labeled samples; (3) construct meta-tasks in a purely random manner. In this paper, we propose a Meta-Learning Siamese Network, namely, Meta-SN, to address these issues. Specifically, instead of computing prototype vectors from the sampled support sets, Meta-SN utilizes external knowledge (e.g. class names and descriptive texts) for class labels, which is encoded as the low-dimensional embeddings of prototype vectors. In addition, Meta-SN presents a novel sampling strategy for constructing meta-tasks, which gives higher sampling probabilities to hard-to-classify samples. Extensive experiments are con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DSL&#32452;&#21512;&#30340;&#19968;&#20010;&#24418;&#24335;&#21270;&#20195;&#25968;&#26694;&#26550;&#65292;&#20351;&#29992;&#20195;&#25968;&#32467;&#26500;&#26469;&#27169;&#25311;&#20803;&#35821;&#35328;&#65292;&#23454;&#29616;DSL&#25277;&#35937;&#30340;&#32534;&#20889;&#12289;&#32452;&#21512;&#21644;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#39564;&#35777;&#31649;&#36947;&#26469;&#39564;&#35777;&#35813;&#26694;&#26550;&#30340;&#32452;&#21512;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.00744</link><description>&lt;p&gt;
DSL&#32452;&#21512;&#30340;&#19968;&#20010;&#24418;&#24335;&#21270;&#20195;&#25968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Formal Algebraic Framework for DSL Composition. (arXiv:2302.00744v1 [math.CT] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DSL&#32452;&#21512;&#30340;&#19968;&#20010;&#24418;&#24335;&#21270;&#20195;&#25968;&#26694;&#26550;&#65292;&#20351;&#29992;&#20195;&#25968;&#32467;&#26500;&#26469;&#27169;&#25311;&#20803;&#35821;&#35328;&#65292;&#23454;&#29616;DSL&#25277;&#35937;&#30340;&#32534;&#20889;&#12289;&#32452;&#21512;&#21644;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#39564;&#35777;&#31649;&#36947;&#26469;&#39564;&#35777;&#35813;&#26694;&#26550;&#30340;&#32452;&#21512;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#20195;&#25968;&#32467;&#26500;&#26469;&#24314;&#27169;&#20803;&#35821;&#35328;&#65292;&#20197;&#20415;&#32534;&#20889;&#12289;&#32452;&#21512;DSL&#25277;&#35937;&#24182;&#25552;&#20379;&#20114;&#25805;&#20316;&#24615;&#30340;&#24418;&#24335;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#39564;&#35777;&#20803;&#35821;&#35328;&#30340;&#32452;&#21512;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35813;&#24418;&#24335;&#21270;&#26694;&#26550;&#30340;&#26500;&#24314;&#20197;&#21450;&#19982;&#25105;&#20204;&#22242;&#38431;&#22312;DARPA V-SPELLS&#39033;&#30446;&#20013;&#30340;&#24037;&#20316;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#29992;&#20110;&#23436;&#25104;&#23545;V-SPELLS&#39564;&#35777;&#20219;&#21153;&#30340;&#31649;&#36947;&#12290;&#25105;&#20204;&#26088;&#22312;&#22312;&#25991;&#31456;&#20013;&#23545;&#27492;&#39564;&#35777;&#31649;&#36947;&#36827;&#34892;&#27010;&#36848;&#12290;&#35813;&#31649;&#36947;&#21487;&#20998;&#20026;&#22235;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#31532;&#19968;&#20010;&#26159;&#22312;Coq&#20013;&#25552;&#20379;&#20803;&#35821;&#35328;&#30340;&#24418;&#24335;&#27169;&#22411;&#65307;&#31532;&#20108;&#20010;&#26159;&#22312;Coq&#20013;&#25552;&#20379;&#25152;&#36873;&#20195;&#25968;&#32467;&#26500;&#30340;&#35268;&#33539;&#65307;&#31532;&#19977;&#20010;&#26159;&#38656;&#35201;&#22312;Coq&#20013;&#23454;&#29616;&#29305;&#23450;&#23454;&#20363;&#30340;&#20195;&#25968;&#32467;&#26500;&#65292;&#24182;&#22312;Coq&#20013;&#32473;&#20986;&#35777;&#26126;&#65292;&#35777;&#26126;&#35813;&#23454;&#29616;&#26159;&#26681;&#25454;&#25105;&#20204;&#22312;&#31532;&#20108;&#20010;&#32452;&#20214;&#20013;&#35268;&#23450;&#30340;&#20195;&#25968;&#32467;&#26500;&#65307;
&lt;/p&gt;
&lt;p&gt;
We discuss a formal framework for using algebraic structures to model a meta-language that can write, compose, and provide interoperability between abstractions of DSLs. The purpose of this formal framework is to provide a verification of compositional properties of the meta-language. Throughout our paper we discuss the construction of this formal framework, as well its relation to our team's work on the DARPA V-SPELLS program via the pipeline we have developed for completing our verification tasking on V-SPELLS. We aim to give a broad overview of this verification pipeline in our paper. The pipeline can be split into four main components: the first is providing a formal model of the meta-language in Coq; the second is to give a specification in Coq of our chosen algebraic structures; third, we need to implement specific instances of our algebraic structures in Coq, as well as give a proof in Coq that this implementation is an algebraic structure according to our specification in the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#26497;&#22823;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;VLPLMs&#65289;&#21019;&#20316;&#25925;&#20107;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;SOTA&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;VLPLMs&#29983;&#25104;&#30340;&#25925;&#20107;&#36136;&#37327;&#26356;&#39640;&#65292;&#24182;&#23637;&#31034;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#19982;&#20154;&#31867;&#20316;&#32773;&#30456;&#25239;&#34913;&#65292;&#23613;&#31649;&#21021;&#27493;&#35843;&#26597;&#25581;&#31034;&#20102;&#23427;&#20204;&#20542;&#21521;&#20110;&#8220;&#25220;&#34989;&#8221;&#30495;&#23454;&#30340;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2301.09790</link><description>&lt;p&gt;
&#26497;&#22823;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#24456;&#23569;&#30340;&#26679;&#20363;&#19979;&#23398;&#20064;&#25925;&#20107;&#21019;&#20316;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Very Large Pretrained Language Models Learn Storytelling With A Few Examples?. (arXiv:2301.09790v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#26497;&#22823;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;VLPLMs&#65289;&#21019;&#20316;&#25925;&#20107;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;SOTA&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;VLPLMs&#29983;&#25104;&#30340;&#25925;&#20107;&#36136;&#37327;&#26356;&#39640;&#65292;&#24182;&#23637;&#31034;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#19982;&#20154;&#31867;&#20316;&#32773;&#30456;&#25239;&#34913;&#65292;&#23613;&#31649;&#21021;&#27493;&#35843;&#26597;&#25581;&#31034;&#20102;&#23427;&#20204;&#20542;&#21521;&#20110;&#8220;&#25220;&#34989;&#8221;&#30495;&#23454;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#35821;&#27861;&#36890;&#39034;&#30340;&#21477;&#23376;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25925;&#20107;&#65292;&#20294;&#26159;&#23427;&#20204;&#38590;&#20197;&#29983;&#25104;&#36830;&#36143;&#12289;&#26377;&#24847;&#20041;&#21644;&#26377;&#36259;&#30340;&#25925;&#20107;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25925;&#20107;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#25506;&#32034;&#26356;&#39640;&#32423;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#24773;&#33410;&#25110;&#24120;&#35782;&#30693;&#35782;&#20197;&#25552;&#39640;&#29983;&#25104;&#25925;&#20107;&#30340;&#36136;&#37327;&#12290;&#20351;&#29992;&#26497;&#22823;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;VLPLMs&#65289;&#22914;GPT3&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;&#24050;&#32463;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#33258;&#21160;&#21644;&#20154;&#31867;&#35780;&#20272;&#26469;&#27604;&#36739;VLPLMs&#19982;&#37027;&#20123;&#22312;&#39118;&#26684;&#12289;&#35821;&#35328;&#21644;&#38271;&#24230;&#31561;&#26041;&#38754;&#19981;&#21516;&#30340;SOTA&#27169;&#22411;&#22312;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#25925;&#20107;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;VLPLMs&#29983;&#25104;&#30340;&#25925;&#20107;&#36136;&#37327;&#36828;&#36828;&#39640;&#20110;&#20854;&#20182;&#25925;&#20107;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#20197;&#19982;&#20154;&#31867;&#20316;&#32773;&#30456;&#25239;&#34913;&#65292;&#23613;&#31649;&#21021;&#27493;&#35843;&#26597;&#20063;&#25581;&#31034;&#20102;&#23427;&#20204;&#20542;&#21521;&#20110;&#8220;&#25220;&#34989;&#8221;&#30495;&#23454;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
While pre-trained language models can generate individually fluent sentences for automatic story generation, they struggle to generate stories that are coherent, sensible and interesting. Current state-of-the-art (SOTA) story generation models explore using higher-level features such as plots or commonsense knowledge to improve the quality of generated stories. Prompt-based learning using very large pre-trained language models (VLPLMs) such as GPT3 has demonstrated impressive performance even across various NLP tasks. In this paper, we present an extensive study using automatic and human evaluation to compare the story generation capability of VLPLMs to those SOTA models in three different datasets where stories differ in style, register and length. Our results show that VLPLMs generate much higher quality stories than other story generation models, and to a certain extent rival human authors, although preliminary investigation also reveals that they tend to ``plagiarise'' real stories
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HEAR&#30340;&#28151;&#21512;&#32534;&#30721;&#22120;&#19982;&#33258;&#36866;&#24212;&#37325;&#21551;&#65288;HEAR&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#27969;&#24335;&#36755;&#20837;&#20013;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#30340;&#19981;&#24517;&#35201;&#28014;&#28857;&#25805;&#20316;&#21644;&#19981;&#24517;&#35201;&#26631;&#31614;&#32763;&#36716;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27969;&#24335;&#36755;&#20837;&#19978;&#30340;&#26631;&#35760;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.09244</link><description>&lt;p&gt;
&#29992;&#20110;&#27969;&#24335;&#24207;&#21015;&#26631;&#35760;&#30340;&#39640;&#25928;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient Encoders for Streaming Sequence Tagging. (arXiv:2301.09244v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HEAR&#30340;&#28151;&#21512;&#32534;&#30721;&#22120;&#19982;&#33258;&#36866;&#24212;&#37325;&#21551;&#65288;HEAR&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#27969;&#24335;&#36755;&#20837;&#20013;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#30340;&#19981;&#24517;&#35201;&#28014;&#28857;&#25805;&#20316;&#21644;&#19981;&#24517;&#35201;&#26631;&#31614;&#32763;&#36716;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27969;&#24335;&#36755;&#20837;&#19978;&#30340;&#26631;&#35760;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22686;&#37327;&#27969;&#36755;&#20837;&#65288;&#20363;&#22914;&#36716;&#24405;&#35821;&#38899;&#65289;&#20013;&#65292;&#20026;&#20102;&#23545;&#27969;&#24335;&#24207;&#21015;&#26631;&#35760;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#65292;&#38656;&#35201;&#20026;&#27599;&#20010;&#26032;&#26631;&#35760;&#20174;&#22836;&#24320;&#22987;&#23545;&#27599;&#20010;&#26631;&#35760;&#36827;&#34892;&#32534;&#30721;&#12290;&#20808;&#21069;&#35745;&#31639;&#30340;&#19981;&#21487;&#37325;&#29992;&#24615;&#23548;&#33268;&#20102;&#26356;&#39640;&#25968;&#37327;&#30340;&#28014;&#28857;&#25805;&#20316;&#65288;&#25110;FLOP&#65289;&#21644;&#26356;&#39640;&#25968;&#37327;&#30340;&#19981;&#24517;&#35201;&#26631;&#31614;&#32763;&#36716;&#12290;&#22686;&#21152;&#30340;FLOP&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#25346;&#38047;&#26102;&#38388;&#65292;&#32780;&#22686;&#21152;&#30340;&#26631;&#31614;&#32763;&#36716;&#20250;&#23548;&#33268;&#27969;&#24335;&#24615;&#33021;&#26356;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HEAR&#30340;&#28151;&#21512;&#32534;&#30721;&#22120;&#19982;&#33258;&#36866;&#24212;&#37325;&#21551;&#65288;HEAR&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21452;&#21521;&#32534;&#30721;&#22120;&#22312;&#31163;&#32447;&#65288;&#25110;&#23436;&#25972;&#65289;&#36755;&#20837;&#19978;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27969;&#36755;&#20837;&#65288;&#25110;&#19981;&#23436;&#25972;&#65289;&#19978;&#30340;&#24615;&#33021;&#12290;HEAR&#20855;&#26377;&#28151;&#21512;&#21333;&#21521; - &#21452;&#21521;&#32534;&#30721;&#22120;&#26550;&#26500;&#26469;&#25191;&#34892;&#24207;&#21015;&#26631;&#35760;&#65292;&#20197;&#21450;&#33258;&#36866;&#24212;&#37325;&#21551;&#27169;&#22359;&#65288;ARM&#65289;&#20197;&#26377;&#36873;&#25321;&#22320;&#24341;&#23548;&#32534;&#30721;&#22120;&#30340;&#21452;&#21521;&#37096;&#20998;&#30340;&#37325;&#26032;&#21551;&#21160;&#12290;&#22312;&#22235;&#20010;&#24207;&#21015;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;HEAR&#26174;&#33879;&#22320;&#25552;&#39640;&#20102;&#27969;&#24335;&#34920;&#29616;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31163;&#32447;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A naive application of state-of-the-art bidirectional encoders for streaming sequence tagging would require encoding each token from scratch for each new token in an incremental streaming input (like transcribed speech). The lack of re-usability of previous computation leads to a higher number of Floating Point Operations (or FLOPs) and higher number of unnecessary label flips. Increased FLOPs consequently lead to higher wall-clock time and increased label flipping leads to poorer streaming performance. In this work, we present a Hybrid Encoder with Adaptive Restart (HEAR) that addresses these issues while maintaining the performance of bidirectional encoders over the offline (or complete) inputs while improving performance on streaming (or incomplete) inputs. HEAR has a Hybrid unidirectional-bidirectional encoder architecture to perform sequence tagging, along with an Adaptive Restart Module (ARM) to selectively guide the restart of bidirectional portion of the encoder. Across four se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#20869;&#23481;&#20016;&#23500;&#20102;&#29983;&#29289;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#22788;&#29702;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#20043;&#21069;&#30340;&#22270;&#20687;&#21644;&#25253;&#21578;&#65292;&#20351;&#29992;&#20102;CNN-Transformer&#28151;&#21512;&#22810;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#20010;&#21644;&#22810;&#22270;&#20687;&#35774;&#32622;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.04558</link><description>&lt;p&gt;
&#23398;&#20064;&#21033;&#29992;&#26102;&#38388;&#32467;&#26500;&#36827;&#34892;&#29983;&#29289;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing. (arXiv:2301.04558v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04558
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#20869;&#23481;&#20016;&#23500;&#20102;&#29983;&#29289;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#22788;&#29702;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#20043;&#21069;&#30340;&#22270;&#20687;&#21644;&#25253;&#21578;&#65292;&#20351;&#29992;&#20102;CNN-Transformer&#28151;&#21512;&#22810;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#20010;&#21644;&#22810;&#22270;&#20687;&#35774;&#32622;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#20102;&#25104;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#29983;&#29289;&#21307;&#23398;VLP&#30340;&#20808;&#21069;&#24037;&#20316;&#22823;&#22810;&#20381;&#36182;&#20110;&#21333;&#20010;&#22270;&#20687;&#21644;&#25253;&#21578;&#23545;&#30340;&#23545;&#40784;&#65292;&#21363;&#20351;&#20020;&#24202;&#35760;&#24405;&#36890;&#24120;&#20250;&#28041;&#21450;&#20197;&#21069;&#30340;&#22270;&#20687;&#12290;&#36825;&#19981;&#20165;&#24341;&#20837;&#20102;&#27169;&#24577;&#20043;&#38388;&#24046;&#21170;&#30340;&#23545;&#40784;&#65292;&#32780;&#19988;&#38169;&#36807;&#20102;&#21033;&#29992;&#25968;&#25454;&#20013;&#29616;&#26377;&#26102;&#38388;&#20869;&#23481;&#30340;&#20016;&#23500;&#33258;&#30417;&#30563;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#21644;&#24494;&#35843;&#26399;&#38388;&#26126;&#30830;&#32771;&#34385;&#20102;&#20043;&#21069;&#30340;&#22270;&#20687;&#21644;&#25253;&#21578;&#65288;&#22914;&#26524;&#26377;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;BioViL-T&#65292;&#20351;&#29992;&#20102;CNN-Transformer&#28151;&#21512;&#22810;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#19982;&#25991;&#26412;&#27169;&#22411;&#19968;&#36215;&#32852;&#21512;&#35757;&#32451;&#12290;&#23427;&#34987;&#35774;&#35745;&#25104;&#36866;&#29992;&#20110;&#20986;&#29616;&#30340;&#25361;&#25112;&#65292;&#22914;&#23039;&#24577;&#21464;&#21270;&#21644;&#32570;&#22833;&#30340;&#26102;&#38388;&#20869;&#36755;&#20837;&#22270;&#20687;&#12290;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#21333;&#20010;&#21644;&#22810;&#22270;&#20687;&#35774;&#32622;&#20013;&#22343;&#34920;&#29616;&#20248;&#24322;&#65292;&#22312;&#20998;&#31867;&#12289;&#30701;&#35821;&#23450;&#20301;&#21644;&#25253;&#21578;&#21487;&#35270;&#21270;&#19977;&#20010;downstream&#20219;&#21153;&#19978;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning in vision-language processing exploits semantic alignment between imaging and text modalities. Prior work in biomedical VLP has mostly relied on the alignment of single image and report pairs even though clinical notes commonly refer to prior images. This does not only introduce poor alignment between the modalities but also a missed opportunity to exploit rich self-supervision through existing temporal content in the data. In this work, we explicitly account for prior images and reports when available during both training and fine-tuning. Our approach, named BioViL-T, uses a CNN-Transformer hybrid multi-image encoder trained jointly with a text model. It is designed to be versatile to arising challenges such as pose variations and missing input images across time. The resulting model excels on downstream tasks both in single- and multi-image setups, achieving state-of-the-art performance on (I) progression classification, (II) phrase grounding, and (III) repor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#34920;&#36798;&#20016;&#23500;&#30340;&#20505;&#36873;&#21477;&#23376;&#23545;&#24182;&#32467;&#21512;&#32676;&#20247;&#22806;&#21253;&#30340;&#25104;&#23545;&#20154;&#31867;&#21028;&#26029;&#65292;&#35757;&#32451;&#19968;&#20010;&#26144;&#23556;&#35821;&#20041;&#30456;&#20284;&#24615;&#21040;&#32479;&#35745;&#20195;&#29702;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#24357;&#21512;&#20154;&#31867;&#30452;&#35273;&#21644;&#20844;&#24179;&#20998;&#31867;&#35268;&#33539;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#26041;&#27861;&#22312;&#34920;&#29616;&#33021;&#21147;&#12289;&#19982;&#20154;&#31867;&#30452;&#35273;&#30340;&#19968;&#33268;&#24615;&#21644;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10154</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#31867;&#23548;&#21521;&#30340;&#20844;&#24179;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Human-Guided Fair Classification for Natural Language Processing. (arXiv:2212.10154v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#34920;&#36798;&#20016;&#23500;&#30340;&#20505;&#36873;&#21477;&#23376;&#23545;&#24182;&#32467;&#21512;&#32676;&#20247;&#22806;&#21253;&#30340;&#25104;&#23545;&#20154;&#31867;&#21028;&#26029;&#65292;&#35757;&#32451;&#19968;&#20010;&#26144;&#23556;&#35821;&#20041;&#30456;&#20284;&#24615;&#21040;&#32479;&#35745;&#20195;&#29702;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#24357;&#21512;&#20154;&#31867;&#30452;&#35273;&#21644;&#20844;&#24179;&#20998;&#31867;&#35268;&#33539;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#26041;&#27861;&#22312;&#34920;&#29616;&#33021;&#21147;&#12289;&#19982;&#20154;&#31867;&#30452;&#35273;&#30340;&#19968;&#33268;&#24615;&#21644;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#31616;&#21382;&#31579;&#36873;&#21644;&#20869;&#23481;&#23457;&#26680;&#31561;&#39640;&#39118;&#38505;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#24517;&#39035;&#26159;&#20844;&#24179;&#30340;&#65292;&#24182;&#36890;&#36807;&#23545;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#30340;&#25200;&#21160;&#19981;&#21464;&#26469;&#36991;&#20813;&#27495;&#35270;&#24615;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#23545;&#36825;&#20123;&#25200;&#21160;&#30340;&#30452;&#35273;&#19982;&#25429;&#25417;&#23427;&#20204;&#30340;&#24418;&#24335;&#30456;&#20284;&#24230;&#35268;&#33539;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#30828;&#32534;&#30721;&#21333;&#35789;&#26367;&#25442;&#65292;&#23548;&#33268;&#35268;&#33539;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#25110;&#32773;&#26080;&#27861;&#20805;&#20998;&#22320;&#19982;&#20154;&#31867;&#30452;&#35273;&#30456;&#19968;&#33268;&#65288;&#20363;&#22914;&#65292;&#22312;&#19981;&#23545;&#31216;&#30340;&#21453;&#20107;&#23454;&#24773;&#20917;&#19979;&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#21457;&#29616;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#30452;&#35273;&#20844;&#24179;&#35268;&#33539;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26080;&#30417;&#30563;&#24335;&#36716;&#25442;&#21644;GPT-3&#30340;&#38646;-shot&#33021;&#21147;&#33258;&#21160;&#29983;&#25104;&#35821;&#20041;&#19978;&#31867;&#20284;&#20294;&#22312;&#25935;&#24863;&#23646;&#24615;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#34920;&#36798;&#20016;&#23500;&#30340;&#20505;&#36873;&#21477;&#23376;&#23545;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#32676;&#20247;&#22806;&#21253;&#33719;&#24471;&#36825;&#20123;&#20505;&#36873;&#20154;&#30340;&#25104;&#23545;&#20154;&#31867;&#21028;&#26029;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#23558;&#35821;&#20041;&#30456;&#20284;&#24615;&#26144;&#23556;&#21040;&#32479;&#35745;&#20195;&#29702;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#34920;&#29616;&#33021;&#21147;&#12289;&#19982;&#20154;&#31867;&#30452;&#35273;&#30340;&#19968;&#33268;&#24615;&#21644;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. These classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. However, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). This work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. We show how to leverage unsupervised style transfer and GPT-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensit
&lt;/p&gt;</description></item><item><title>LUNA&#26694;&#26550;&#36890;&#36807;&#25968;&#23383;&#25554;&#20214;&#21644;&#39044;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24456;&#22909;&#29702;&#35299;&#25968;&#23383;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.02691</link><description>&lt;p&gt;
LUNA&#65306;&#20351;&#29992;&#25968;&#23383;&#25554;&#20214;&#21644;&#39044;&#35757;&#32451;&#30340;Transformers&#23454;&#29616;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training. (arXiv:2212.02691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02691
&lt;/p&gt;
&lt;p&gt;
LUNA&#26694;&#26550;&#36890;&#36807;&#25968;&#23383;&#25554;&#20214;&#21644;&#39044;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24456;&#22909;&#29702;&#35299;&#25968;&#23383;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;NLP&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21033;&#29992;transformers&#29702;&#35299;&#35821;&#35328;&#30340;&#26041;&#27861;&#23384;&#22312;&#19968;&#20010;&#24369;&#28857;&#65306;&#25968;&#23383;&#29702;&#35299;&#12290;&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#65292;&#25968;&#23383;&#32463;&#24120;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#34920;&#26684;&#65289;&#20013;&#12290;&#20294;&#26159;&#21033;&#29992;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23383;&#20219;&#21153;&#30340;&#24403;&#21069;&#26041;&#27861;&#20250;&#20002;&#22833;&#19968;&#20123;&#25968;&#23383;&#20449;&#24687;&#65292;&#20363;&#22914;&#23558;&#25968;&#23383;&#20998;&#35299;&#20026;&#23376;&#35789;&#26631;&#35760;&#31561;&#65292;&#23548;&#33268;&#35768;&#22810;&#19982;&#25968;&#23383;&#30456;&#20851;&#30340;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LUNA&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23383;&#25512;&#29702;&#21644;&#35745;&#31639;&#33021;&#21147;&#12290;LUNA&#20351;&#29992;NumTok&#21644;NumBed&#30340;&#25968;&#23383;&#25554;&#20214;&#23558;&#27599;&#20010;&#25968;&#23383;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#26469;&#34920;&#31034;&#36755;&#20837;&#12290;&#36890;&#36807;&#25968;&#23383;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#22238;&#24402;&#25439;&#22833;&#21644;&#27169;&#22411;&#33976;&#39311;&#65292;LUNA&#24357;&#21512;&#20102;&#25968;&#23383;&#21644;&#35789;&#27719;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26126;&#30830;&#23558;&#25968;&#23383;&#33021;&#21147;&#27880;&#20837;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#25968;&#23383;&#25554;&#20214;&#21644;&#39044;&#35757;&#32451;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are widely used in NLP tasks. However, current approaches to leveraging transformers to understand language expose one weak spot: Number understanding. In some scenarios, numbers frequently occur, especially in semi-structured data like tables. But current approaches to rich-number tasks with transformer-based language models abandon or lose some of the numeracy information - e.g., breaking numbers into sub-word tokens - which leads to many number-related errors. In this paper, we propose the LUNA framework which improves the numerical reasoning and calculation capabilities of transformer-based language models. With the number plugin of NumTok and NumBed, LUNA represents each number as a whole to model input. With number pre-training, including regression loss and model distillation, LUNA bridges the gap between number and vocabulary embeddings. To the best of our knowledge, this is the first work that explicitly injects numeracy capability into language models using Numbe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;GLM2FSA&#65292;&#33021;&#22815;&#33258;&#21160;&#20174;&#20219;&#21153;&#30446;&#26631;&#30340;&#31616;&#30701;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20219;&#21153;&#30693;&#35782;&#24182;&#26500;&#24314;&#19968;&#20010;&#32534;&#30721;&#39640;&#23618;&#27425;&#20219;&#21153;&#30693;&#35782;&#30340;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65292;&#26500;&#24314;&#30340;&#33258;&#21160;&#26426;&#21487;&#20197;&#34987;&#27491;&#24335;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2212.01944</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#26426;&#34920;&#31034;&#20219;&#21153;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Automaton-Based Representations of Task Knowledge from Generative Language Models. (arXiv:2212.01944v3 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01944
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;GLM2FSA&#65292;&#33021;&#22815;&#33258;&#21160;&#20174;&#20219;&#21153;&#30446;&#26631;&#30340;&#31616;&#30701;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20219;&#21153;&#30693;&#35782;&#24182;&#26500;&#24314;&#19968;&#20010;&#32534;&#30721;&#39640;&#23618;&#27425;&#20219;&#21153;&#30693;&#35782;&#30340;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65292;&#26500;&#24314;&#30340;&#33258;&#21160;&#26426;&#21487;&#20197;&#34987;&#27491;&#24335;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#26426;&#30340;&#20219;&#21153;&#30693;&#35782;&#34920;&#31034;&#22312;&#25511;&#21046;&#21644;&#35268;&#21010;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#26500;&#24314;&#27492;&#31867;&#33258;&#21160;&#26426;&#25152;&#38656;&#30340;&#39640;&#23618;&#27425;&#20219;&#21153;&#30693;&#35782;&#36890;&#24120;&#24456;&#22256;&#38590;&#12290;&#21516;&#26102;&#65292;&#22823;&#35268;&#27169;&#33258;&#21160;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#30456;&#20851;&#20219;&#21153;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#36755;&#20986;&#19981;&#33021;&#27491;&#24335;&#39564;&#35777;&#25110;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GLM2FSA&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#20174;&#20219;&#21153;&#30446;&#26631;&#30340;&#31616;&#30701;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#26500;&#24314;&#19968;&#20010;&#32534;&#30721;&#39640;&#23618;&#27425;&#20219;&#21153;&#30693;&#35782;&#30340;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;FSA&#65289;&#12290;GLM2FSA&#39318;&#20808;&#21521;GLM&#21457;&#36865;&#26597;&#35810;&#20197;&#25552;&#21462;&#25991;&#26412;&#24418;&#24335;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#28982;&#21518;&#23427;&#24314;&#31435;&#19968;&#20010;FSA&#26469;&#34920;&#31034;&#36825;&#31181;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#21644;&#33258;&#21160;&#26426;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#26500;&#24314;&#30340;FSA&#21487;&#20197;&#38024;&#23545;&#29992;&#25143;&#23450;&#20041;&#30340;&#35268;&#26684;&#36827;&#34892;&#27491;&#24335;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automaton-based representations of task knowledge play an important role in control and planning for sequential decision-making problems. However, obtaining the high-level task knowledge required to build such automata is often difficult. Meanwhile, large-scale generative language models (GLMs) can automatically generate relevant task knowledge. However, the textual outputs from GLMs cannot be formally verified or used for sequential decision-making. We propose a novel algorithm named GLM2FSA, which constructs a finite state automaton (FSA) encoding high-level task knowledge from a brief natural-language description of the task goal. GLM2FSA first sends queries to a GLM to extract task knowledge in textual form, and then it builds an FSA to represent this text-based knowledge. The proposed algorithm thus fills the gap between natural-language task descriptions and automaton-based representations, and the constructed FSA can be formally verified against user-defined specifications. We a
&lt;/p&gt;</description></item><item><title>DreamArtist&#37319;&#29992;&#27491;&#36127;prompt-tuning&#23398;&#20064;&#31574;&#30053;&#26469;&#29983;&#25104;&#21487;&#25511;&#30340;&#19968;&#27425;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.11337</link><description>&lt;p&gt;
DreamArtist: &#36890;&#36807;&#23545;&#27604;prompt-tuning&#23454;&#29616;&#21487;&#25511;&#30340;&#19968;&#27425;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via Contrastive Prompt-Tuning. (arXiv:2211.11337v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11337
&lt;/p&gt;
&lt;p&gt;
DreamArtist&#37319;&#29992;&#27491;&#36127;prompt-tuning&#23398;&#20064;&#31574;&#30053;&#26469;&#29983;&#25104;&#21487;&#25511;&#30340;&#19968;&#27425;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#25991;&#26412;&#25351;&#23548;&#21512;&#25104;&#39640;&#36136;&#37327;&#12289;&#29305;&#24449;&#20016;&#23500;&#12289;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#26032;&#27010;&#24565;&#65288;&#20363;&#22914;&#26032;&#39118;&#26684;&#12289;&#29289;&#20307;&#23454;&#20307;&#31561;&#65289;&#26102;&#24120;&#24120;&#38754;&#20020;&#22256;&#38590;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#23581;&#35797;&#37319;&#29992;&#24494;&#35843;&#25110;prompt-tuning&#31574;&#30053;&#26469;&#25945;&#25480;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20174;&#21442;&#32771;&#22270;&#20687;&#38598;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#36807;&#24230;&#25311;&#21512;&#32473;&#23450;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#22312;&#21333;&#27425;&#24212;&#29992;&#20013;&#65292;&#36825;&#23545;&#20110;&#20445;&#25345;&#29983;&#25104;&#21487;&#25511;&#24615;&#24182;&#20135;&#29983;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;DreamArtist&#65292;&#23427;&#37319;&#29992;&#20102;&#27491;&#36127;prompt-tuning&#23398;&#20064;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DreamArtist&#32467;&#21512;&#20102;&#27491;&#36127;&#23884;&#20837;&#24182;&#32852;&#21512;&#35757;&#32451;&#23427;&#20204;&#12290;&#27491;&#23884;&#20837;&#31215;&#26497;&#22320;&#25429;&#25417;&#21442;&#32771;&#22270;&#20687;&#30340;&#26174;&#30528;&#29305;&#24449;&#26469;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#65292;&#32780;&#36127;&#23884;&#20837;&#21017;&#24378;&#21046;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#24615;&#22270;&#20687;&#20197;&#38477;&#20302;&#36807;&#24230;&#25311;&#21512;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text-to-image generation models have achieved remarkable progress in synthesizing high-quality, feature-rich images with high resolution guided by texts. However, these models often struggle with novel concepts, eg, new styles, object entities, etc. Although recent attempts have employed fine-tuning or prompt-tuning strategies to teach the pre-trained diffusion model novel concepts from a reference image set,they have the drawback of overfitting to the given reference images, particularly in one-shot applications, which is harmful to generate diverse and high-quality images while maintaining generation controllability.  To tackle this challenge, we present a simple yet effective method called DreamArtist, which employs a positive-negative prompt-tuning learning strategy. Specifically, DreamArtist incorporates both positive and negative embeddings and jointly trains them. The positive embedding aggressively captures the salient characteristics of the reference image to drive
&lt;/p&gt;</description></item><item><title>&#35299;&#30721;&#31639;&#27861;&#30340;&#36873;&#25321;&#38656;&#35201;&#32771;&#34385;&#27169;&#22411;&#20284;&#28982;&#24230;&#21644;&#20219;&#21153;&#25928;&#29992;&#30340;&#21305;&#37197;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#31867;&#19981;&#21305;&#37197;&#32531;&#35299;&#31574;&#30053;&#65288;MMS&#65289;&#30340;&#35270;&#35282;&#65292;&#21487;&#20197;&#25552;&#39640;&#35299;&#30721;&#31639;&#27861;&#30340;&#36890;&#29992;&#24615;</title><link>http://arxiv.org/abs/2210.07228</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#20316;&#20026;&#20284;&#28982;&#24230;-&#25928;&#29992;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Language Model Decoding as Likelihood-Utility Alignment. (arXiv:2210.07228v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07228
&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#31639;&#27861;&#30340;&#36873;&#25321;&#38656;&#35201;&#32771;&#34385;&#27169;&#22411;&#20284;&#28982;&#24230;&#21644;&#20219;&#21153;&#25928;&#29992;&#30340;&#21305;&#37197;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#31867;&#19981;&#21305;&#37197;&#32531;&#35299;&#31574;&#30053;&#65288;MMS&#65289;&#30340;&#35270;&#35282;&#65292;&#21487;&#20197;&#25552;&#39640;&#35299;&#30721;&#31639;&#27861;&#30340;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#35821;&#35328;&#29983;&#25104;&#27969;&#31243;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#26159;&#35299;&#30721;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#24212;&#35813;&#25351;&#23548;&#36873;&#25321;&#35299;&#30721;&#31639;&#27861;&#30340;&#19968;&#33324;&#21407;&#21017;&#20173;&#19981;&#28165;&#26970;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#20165;&#22312;&#29421;&#31364;&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#35299;&#30721;&#31639;&#27861;&#65292;&#20182;&#20204;&#30340;&#21457;&#29616;&#19981;&#33021;&#25512;&#24191;&#21040;&#36328;&#20219;&#21153;&#12290;&#25105;&#20204;&#35748;&#20026;&#27169;&#22411;&#30340;&#20284;&#28982;&#21644;&#20219;&#21153;&#29305;&#23450;&#25928;&#29992;&#30340;&#19981;&#21305;&#37197;&#26159;&#29702;&#35299;&#35299;&#30721;&#31639;&#27861;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20026;&#20102;&#32467;&#26500;&#21270;&#35752;&#35770;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#19981;&#21305;&#37197;&#32531;&#35299;&#31574;&#30053;&#65288;MMS&#65289;&#30340;&#20998;&#31867;&#27861;&#65292;&#25552;&#20379;&#35299;&#30721;&#20316;&#20026;&#23545;&#40784;&#24037;&#20855;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#36825;&#20010;MMS&#20998;&#31867;&#27861;&#26681;&#25454;&#35299;&#30721;&#31639;&#27861;&#23545;&#20284;&#28982;&#24230;-&#25928;&#29992;&#19981;&#21305;&#37197;&#30340;&#38544;&#21547;&#20551;&#35774;&#23545;&#20854;&#36827;&#34892;&#20998;&#32452;&#65292;&#20135;&#29983;&#20851;&#20110;&#23427;&#20204;&#36328;&#20219;&#21153;&#36866;&#29992;&#24615;&#30340;&#19968;&#33324;&#24615;&#22768;&#26126;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#20998;&#26512;&#36328;&#22810;&#20010;&#20219;&#21153;&#30340;&#39044;&#27979;&#30340;&#20284;&#28982;&#24230;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
A critical component of a successful language generation pipeline is the decoding algorithm. However, the general principles that should guide the choice of a decoding algorithm remain unclear. Previous works only compare decoding algorithms in narrow scenarios, and their findings do not generalize across tasks. We argue that the misalignment between the model's likelihood and the task-specific notion of utility is the key factor to understanding the effectiveness of decoding algorithms. To structure the discussion, we introduce a taxonomy of misalignment mitigation strategies (MMSs), providing a unifying view of decoding as a tool for alignment. The MMS taxonomy groups decoding algorithms based on their implicit assumptions about likelihood--utility misalignment, yielding general statements about their applicability across tasks. Specifically, by analyzing the correlation between the likelihood and the utility of predictions across a diverse set of tasks, we provide empirical evidence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35782;&#21035;&#21270;&#30103;&#21518;&#30284;&#30151;&#24739;&#32773;&#24613;&#25937;&#25252;&#29702;&#39118;&#38505;&#30340;&#38382;&#39064;&#65292;&#19982;&#20197;&#24448;&#20351;&#29992;&#32467;&#26500;&#21270;&#21355;&#29983;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#30340;&#27169;&#22411;&#30456;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20004;&#32773;&#24046;&#24322;&#19981;&#22823;&#65292;&#20174;&#32780;&#35828;&#26126;&#20102;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#19981;&#21516;&#24739;&#32773;&#32676;&#20307;&#39118;&#38505;&#20559;&#24046;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2209.13860</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35782;&#21035;&#22312;&#20020;&#24202;&#35760;&#24405;&#20013;&#39640;&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing Methods to Identify Oncology Patients at High Risk for Acute Care with Clinical Notes. (arXiv:2209.13860v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35782;&#21035;&#21270;&#30103;&#21518;&#30284;&#30151;&#24739;&#32773;&#24613;&#25937;&#25252;&#29702;&#39118;&#38505;&#30340;&#38382;&#39064;&#65292;&#19982;&#20197;&#24448;&#20351;&#29992;&#32467;&#26500;&#21270;&#21355;&#29983;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#30340;&#27169;&#22411;&#30456;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20004;&#32773;&#24046;&#24322;&#19981;&#22823;&#65292;&#20174;&#32780;&#35828;&#26126;&#20102;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#19981;&#21516;&#24739;&#32773;&#32676;&#20307;&#39118;&#38505;&#20559;&#24046;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35760;&#24405;&#26159;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#22914;&#20309;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#35782;&#21035;&#21270;&#30103;&#24320;&#22987;&#21518;&#30284;&#30151;&#24739;&#32773;&#24613;&#25937;&#25252;&#29702;&#65288;ACU&#65289;&#30340;&#39118;&#38505;&#12290;&#20351;&#29992;&#32467;&#26500;&#21270;&#21355;&#29983;&#25968;&#25454;&#65288;SHD&#65289;&#36827;&#34892;&#39118;&#38505;&#39044;&#27979;&#24050;&#25104;&#20026;&#26631;&#20934;&#65292;&#20294;&#20351;&#29992;&#33258;&#30001;&#25991;&#26412;&#26684;&#24335;&#36827;&#34892;&#39044;&#27979;&#26356;&#20026;&#22797;&#26434;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#30001;&#25991;&#26412;&#31508;&#35760;&#32780;&#38750;SHD&#36827;&#34892;ACU&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#25163;&#21160;&#26500;&#24314;&#30340;&#35821;&#35328;&#29305;&#24449;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SHD&#27169;&#22411;&#30053;&#32988;&#20110;NLP&#27169;&#22411;&#65307; SHD&#30340;l1-&#32602;&#39033;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#23454;&#29616;&#30340;C&#32479;&#35745;&#37327;&#20026;0.748&#65288;95&#65285;-CI&#65306;0.735&#65292;0.762&#65289;&#65292;&#32780;&#20855;&#26377;&#35821;&#35328;&#29305;&#24449;&#30340;&#30456;&#21516;&#27169;&#22411;&#23454;&#29616;&#30340;C&#32479;&#35745;&#37327;&#20026;0.730&#65288;95&#65285;-CI&#65306;0.717&#65292;0.745&#65289;&#65292;&#32780;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#23454;&#29616;&#30340;C&#32479;&#35745;&#37327;&#20026;0.702&#65288;95&#65285;-CI&#65306;0.688&#65292;0.717&#65289;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#19981;&#21516;&#24739;&#32773;&#32676;&#20307;&#30340;&#39118;&#38505;&#20559;&#24046;&#26159;&#19981;&#21516;&#30340;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#33258;&#30001;&#25991;&#26412;&#25968;&#25454;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical notes are an essential component of a health record. This paper evaluates how natural language processing (NLP) can be used to identify the risk of acute care use (ACU) in oncology patients, once chemotherapy starts. Risk prediction using structured health data (SHD) is now standard, but predictions using free-text formats are complex. This paper explores the use of free-text notes for the prediction of ACU instead of SHD. Deep Learning models were compared to manually engineered language features. Results show that SHD models minimally outperform NLP models; an l1-penalised logistic regression with SHD achieved a C-statistic of 0.748 (95%-CI: 0.735, 0.762), while the same model with language features achieved 0.730 (95%-CI: 0.717, 0.745) and a transformer-based model achieved 0.702 (95%-CI: 0.688, 0.717). This paper shows how language models can be used in clinical applications and underlines how risk bias is different for diverse patient groups, even using only free-text dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#37322;&#38142;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#24110;&#21161;&#29702;&#35299;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;&#30446;&#26631;&#20449;&#24687;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;BLUE&#20998;&#25968;&#20174;44.0&#25552;&#39640;&#21040;&#20102;62.3&#65292;&#24050;&#36890;&#36807;&#22810;&#31181;&#24230;&#37327;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#36136;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2209.04889</link><description>&lt;p&gt;
&#35299;&#37322;&#38142;&#65306;&#29983;&#25104;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#26356;&#39640;&#36136;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#26032;&#25552;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chain of Explanation: New Prompting Method to Generate Higher Quality Natural Language Explanation for Implicit Hate Speech. (arXiv:2209.04889v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#37322;&#38142;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#24110;&#21161;&#29702;&#35299;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;&#30446;&#26631;&#20449;&#24687;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;BLUE&#20998;&#25968;&#20174;44.0&#25552;&#39640;&#21040;&#20102;62.3&#65292;&#24050;&#36890;&#36807;&#22810;&#31181;&#24230;&#37327;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#39640;&#32423;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#35299;&#37322;&#20026;&#20160;&#20040;&#26576;&#20123;&#25991;&#26412;&#21487;&#33021;&#26159;&#20855;&#26377;&#20167;&#24680;&#33394;&#24425;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#37322;&#38142; (CoE) &#25552;&#31034;&#26041;&#27861;&#65292; &#20351;&#29992;&#21551;&#21457;&#24335;&#35789;&#35821;&#21644;&#30446;&#26631;&#32676;&#20307;&#65292;&#20026;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;&#30446;&#26631;&#20449;&#24687;&#65292;&#25105;&#20204;&#23558; NLE &#29983;&#25104;&#30340; BLUE &#20998;&#25968;&#20174; 44.0 &#25552;&#39640;&#21040;&#20102; 62.3&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#22810;&#31181;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#20449;&#24687;&#37327;&#21644;&#28165;&#26224;&#24230;&#35780;&#20998;&#35780;&#20272;&#20102;&#29983;&#25104;&#30340; NLE &#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have exploited advanced generative language models to generate Natural Language Explanations (NLE) for why a certain text could be hateful. We propose the Chain of Explanation (CoE) Prompting method, using the heuristic words and target group, to generate high-quality NLE for implicit hate speech. We improved the BLUE score from 44.0 to 62.3 for NLE generation by providing accurate target information. We then evaluate the quality of generated NLE using various automatic metrics and human annotations of informativeness and clarity scores.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#25991;&#26412;&#25968;&#25454;&#19978;&#20004;&#31867;&#26041;&#27861;&#65306;&#35745;&#31639;&#27599;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#21644;&#25552;&#21462;&#31616;&#21333;&#36923;&#36753;&#35268;&#21017;&#65292;&#21457;&#29616;&#22312;&#30456;&#21516;&#27169;&#22411;&#19979;&#20135;&#29983;&#30340;&#35299;&#37322;&#20063;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35299;&#37322;&#24046;&#24322;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.01420</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#27604;&#36739;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#35268;&#21017;&#25552;&#21462;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Comparing Feature Importance and Rule Extraction for Interpretability on Text Data. (arXiv:2207.01420v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#25991;&#26412;&#25968;&#25454;&#19978;&#20004;&#31867;&#26041;&#27861;&#65306;&#35745;&#31639;&#27599;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#21644;&#25552;&#21462;&#31616;&#21333;&#36923;&#36753;&#35268;&#21017;&#65292;&#21457;&#29616;&#22312;&#30456;&#21516;&#27169;&#22411;&#19979;&#20135;&#29983;&#30340;&#35299;&#37322;&#20063;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35299;&#37322;&#24046;&#24322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#28041;&#21450;&#25991;&#26412;&#25968;&#25454;&#30340;&#20851;&#38190;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#65292;&#36825;&#23548;&#33268;&#20102;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#22312;&#23616;&#37096;&#26041;&#27861;&#20013;&#65292;&#20986;&#29616;&#20102;&#20004;&#31181;&#26063;&#32676;&#65306;&#19968;&#31181;&#35745;&#31639;&#27599;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#21478;&#19968;&#31181;&#21017;&#25552;&#21462;&#31616;&#21333;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#19981;&#21516;&#26041;&#27861;&#21487;&#20197;&#23548;&#33268;&#24847;&#22806;&#19981;&#21516;&#30340;&#35299;&#37322;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#37027;&#20123;&#25105;&#20204;&#39044;&#35745;&#20250;&#26377;&#23450;&#24615;&#24039;&#21512;&#30340;&#31616;&#21333;&#27169;&#22411;&#19978;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#20135;&#29983;&#30340;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex machine learning algorithms are used more and more often in critical tasks involving text data, leading to the development of interpretability methods. Among local methods, two families have emerged: those computing importance scores for each feature and those extracting simple logical rules. In this paper we show that using different methods can lead to unexpectedly different explanations, even when applied to simple models for which we would expect qualitative coincidence. To quantify this effect, we propose a new approach to compare explanations produced by different methods.
&lt;/p&gt;</description></item><item><title>&#25104;&#20154;&#22914;&#20309;&#29702;&#35299;&#24188;&#20799;&#30340;&#35821;&#35328;&#20173;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#25104;&#20154;&#33021;&#22815;&#29702;&#35299;&#24188;&#20799;&#35821;&#35328;&#65292;&#26159;&#22240;&#20026;&#20182;&#20204;&#26377;&#20851;&#20110;&#24188;&#20799;&#35797;&#22270;&#20256;&#36798;&#20449;&#24687;&#30340;&#29305;&#23450;&#20808;&#39564;&#26399;&#26395;&#65292;&#24182;&#25581;&#31034;&#20102;&#25104;&#20154;&#22312;&#26089;&#26399;&#27807;&#36890;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2206.07807</link><description>&lt;p&gt;
&#25104;&#20154;&#22914;&#20309;&#29702;&#35299;&#24188;&#20799;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
How Adults Understand What Young Children Say. (arXiv:2206.07807v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07807
&lt;/p&gt;
&lt;p&gt;
&#25104;&#20154;&#22914;&#20309;&#29702;&#35299;&#24188;&#20799;&#30340;&#35821;&#35328;&#20173;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#25104;&#20154;&#33021;&#22815;&#29702;&#35299;&#24188;&#20799;&#35821;&#35328;&#65292;&#26159;&#22240;&#20026;&#20182;&#20204;&#26377;&#20851;&#20110;&#24188;&#20799;&#35797;&#22270;&#20256;&#36798;&#20449;&#24687;&#30340;&#29305;&#23450;&#20808;&#39564;&#26399;&#26395;&#65292;&#24182;&#25581;&#31034;&#20102;&#25104;&#20154;&#22312;&#26089;&#26399;&#27807;&#36890;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24188;&#20799;&#26399;&#30340;&#35821;&#35328;&#24120;&#24120;&#19982;&#25104;&#20154;&#30340;&#35821;&#35328;&#19981;&#22826;&#30456;&#20284;&#65292;&#20294;&#26159;&#29238;&#27597;&#21644;&#20854;&#20182;&#29031;&#39038;&#32773;&#33021;&#22815;&#29702;&#35299;&#36825;&#20123;&#35821;&#35328;&#24182;&#25454;&#27492;&#20570;&#20986;&#21453;&#24212;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25104;&#20154;&#20316;&#20026;&#21548;&#20247;&#22914;&#20309;&#21453;&#26144;&#20182;&#20204;&#23545;&#20799;&#31461;&#35797;&#22270;&#20256;&#36798;&#20160;&#20040;&#20197;&#21450;&#20799;&#31461;&#21457;&#38899;&#30340;&#22797;&#26434;&#20449;&#24565;&#12290;&#20351;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#24314;&#27169;&#21475;&#35821;&#35782;&#21035;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#24403;&#35745;&#31639;&#27169;&#22411;&#21253;&#21547;&#20851;&#20110;&#20799;&#31461;&#24819;&#35201;&#20256;&#36798;&#30340;&#20449;&#24687;&#30340;&#24378;&#28872;&#30340;&#12289;&#29305;&#23450;&#20110;&#35821;&#22659;&#30340;&#20808;&#39564;&#26399;&#26395;&#26102;&#65292;&#23427;&#20204;&#25165;&#33021;&#22797;&#21046;&#25104;&#20154;&#23545;&#24188;&#20799;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;&#36825;&#25351;&#20986;&#20102;&#25104;&#20154;&#35748;&#30693;&#36807;&#31243;&#22312;&#25903;&#25345;&#26089;&#26399;&#27807;&#36890;&#20013;&#21457;&#25381;&#30340;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#20799;&#31461;&#22914;&#20309;&#22312;&#20165;&#26377;&#21021;&#27493;&#30340;&#25104;&#20154;&#35821;&#35328;&#29702;&#35299;&#26102;&#65292;&#31215;&#26497;&#24341;&#23548;&#25104;&#20154;&#20195;&#34920;&#33258;&#24049;&#37319;&#21462;&#34892;&#21160;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25104;&#20154;&#24378;&#22823;&#30340;&#21548;&#21147;&#33021;&#21147;&#23545;&#20110;&#23156;&#24188;&#20799;&#21457;&#23637;&#29702;&#35770;&#30340;&#24191;&#27867;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children's early speech often bears little resemblance to that of adults, and yet parents and other caregivers are able to interpret that speech and react accordingly. Here we investigate how these adult inferences as listeners reflect sophisticated beliefs about what children are trying to communicate, as well as how children are likely to pronounce words. Using a Bayesian framework for modeling spoken word recognition, we find that computational models can replicate adult interpretations of children's speech only when they include strong, context-specific prior expectations about the messages that children will want to communicate. This points to a critical role of adult cognitive processes in supporting early communication and reveals how children can actively prompt adults to take actions on their behalf even when they have only a nascent understanding of the adult language. We discuss the wide-ranging implications of the powerful listening capabilities of adults for theories of fi
&lt;/p&gt;</description></item><item><title>Anchors &#26159;&#19968;&#31181;&#21518;&#22788;&#29702;&#30340;&#35268;&#21017;&#24615;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20984;&#26174;&#20986;&#19968;&#20010;&#23567;&#32452;&#35789;&#35821;&#65288;&#38170;&#28857;&#65289;&#26469;&#24378;&#35843;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545; Anchors &#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#32771;&#34385;&#21040;&#23547;&#25214;&#26368;&#20339;&#38170;&#28857;&#26159;&#35814;&#23613;&#30340;&#65292;&#24182;&#36890;&#36807; TF-IDF &#21521;&#37327;&#21270;&#27493;&#39588;&#20197;&#21450;&#27169;&#22411;&#23618;&#27425;&#30340;&#26174;&#24335;&#32467;&#26524;&#65292;&#25506;&#31350;&#20854;&#22312;&#19981;&#21516;&#31867;&#21035;&#27169;&#22411;&#20013;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26368;&#39640;&#20559;&#23548;&#25968;&#25152;&#23545;&#24212;&#30340;&#35789;&#27719;&#21487;&#20197;&#37325;&#26032;&#21152;&#26435;&#29992;&#20316; Anchors &#35789;&#27719;&#12290;</title><link>http://arxiv.org/abs/2205.13789</link><description>&lt;p&gt;
&#19968;&#29255;&#25991;&#23383;&#28023;&#65306;&#38024;&#23545;&#25991;&#26412;&#25968;&#25454;&#30340; Anchors &#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Sea of Words: An In-Depth Analysis of Anchors for Text Data. (arXiv:2205.13789v2 [stat.ML] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13789
&lt;/p&gt;
&lt;p&gt;
Anchors &#26159;&#19968;&#31181;&#21518;&#22788;&#29702;&#30340;&#35268;&#21017;&#24615;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20984;&#26174;&#20986;&#19968;&#20010;&#23567;&#32452;&#35789;&#35821;&#65288;&#38170;&#28857;&#65289;&#26469;&#24378;&#35843;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545; Anchors &#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#32771;&#34385;&#21040;&#23547;&#25214;&#26368;&#20339;&#38170;&#28857;&#26159;&#35814;&#23613;&#30340;&#65292;&#24182;&#36890;&#36807; TF-IDF &#21521;&#37327;&#21270;&#27493;&#39588;&#20197;&#21450;&#27169;&#22411;&#23618;&#27425;&#30340;&#26174;&#24335;&#32467;&#26524;&#65292;&#25506;&#31350;&#20854;&#22312;&#19981;&#21516;&#31867;&#21035;&#27169;&#22411;&#20013;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26368;&#39640;&#20559;&#23548;&#25968;&#25152;&#23545;&#24212;&#30340;&#35789;&#27719;&#21487;&#20197;&#37325;&#26032;&#21152;&#26435;&#29992;&#20316; Anchors &#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Anchors &#26159;&#19968;&#31181;&#22522;&#20110;&#21518;&#22788;&#29702;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#24182;&#24378;&#35843;&#19968;&#23567;&#32452;&#35789;&#35821;&#65288;&#38170;&#28857;&#65289;&#65292;&#36825;&#20123;&#35789;&#35821;&#23384;&#22312;&#20110;&#25991;&#26723;&#20013;&#26102;&#65292;&#27169;&#22411;&#36755;&#20986;&#31867;&#20284;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545; Anchors &#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#32771;&#34385;&#21040;&#23547;&#25214;&#26368;&#20339;&#38170;&#28857;&#26159;&#35814;&#23613;&#30340;&#12290;&#25105;&#20204;&#23558;&#25991;&#26412;&#20998;&#31867;&#30340;&#31639;&#27861;&#24418;&#24335;&#21270;&#21518;&#65292;&#32467;&#21512;&#19981;&#21516;&#31867;&#21035;&#27169;&#22411;&#30340;&#26174;&#24335;&#32467;&#26524;&#65292;&#25506;&#31350;&#20102; Anchors &#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#25105;&#20204;&#20998;&#21035;&#35206;&#30422;&#20102;&#22522;&#26412; if-then &#35268;&#21017;&#21644;&#32447;&#24615;&#20998;&#31867;&#22120;&#36825;&#20004;&#31181;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#36825;&#39033;&#20998;&#26512;&#65292;&#27934;&#35265;&#20219;&#20309;&#21487;&#24494;&#20998;&#20998;&#31867;&#22120;&#30340; Anchors &#34892;&#20026;&#29305;&#24449;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#26368;&#39640;&#20559;&#23548;&#25968;&#25152;&#23545;&#24212;&#30340;&#35789;&#35821;&#65292;&#36890;&#36807;&#21453;&#21521;&#25991;&#20214;&#37325;&#26032;&#21152;&#26435;&#65292;&#21487;&#20197;&#20316;&#20026; Anchors &#35789;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anchors (Ribeiro et al., 2018) is a post-hoc, rule-based interpretability method. For text data, it proposes to explain a decision by highlighting a small set of words (an anchor) such that the model to explain has similar outputs when they are present in a document. In this paper, we present the first theoretical analysis of Anchors, considering that the search for the best anchor is exhaustive. After formalizing the algorithm for text classification, we present explicit results on different classes of models when the vectorization step is TF-IDF, and words are replaced by a fixed out-of-dictionary token when removed. Our inquiry covers models such as elementary if-then rules and linear classifiers. We then leverage this analysis to gain insights on the behavior of Anchors for any differentiable classifiers. For neural networks, we empirically show that the words corresponding to the highest partial derivatives of the model with respect to the input, reweighted by the inverse document
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#20998;&#26512;&#35777;&#23454;&#65292;&#26085;&#23572;&#26364;&#35821;&#35328;&#21644;&#26031;&#25289;&#22827;&#35821;&#35328;&#20013;&#30340;&#24615;&#21035;&#19968;&#33268;&#25928;&#24212;&#27604;&#32599;&#26364;&#35821;&#35328;&#26356;&#21152;&#31283;&#20581;&#65292;&#20294;&#25928;&#24212;&#22823;&#23567;&#36866;&#20013;&#65292;&#24182;&#19988;&#23384;&#22312;&#30740;&#31350;&#38388;&#30340;&#21464;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2109.03490</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#24615;&#21035;&#19968;&#33268;&#24615;&#24433;&#21709;&#30340;&#36328;&#35821;&#35328;&#24046;&#24322;&#65306;&#22522;&#20110;&#20803;&#20998;&#26512;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-linguistic differences in gender congruency effects: Evidence from meta-analyses. (arXiv:2109.03490v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03490
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#20998;&#26512;&#35777;&#23454;&#65292;&#26085;&#23572;&#26364;&#35821;&#35328;&#21644;&#26031;&#25289;&#22827;&#35821;&#35328;&#20013;&#30340;&#24615;&#21035;&#19968;&#33268;&#25928;&#24212;&#27604;&#32599;&#26364;&#35821;&#35328;&#26356;&#21152;&#31283;&#20581;&#65292;&#20294;&#25928;&#24212;&#22823;&#23567;&#36866;&#20013;&#65292;&#24182;&#19988;&#23384;&#22312;&#30740;&#31350;&#38388;&#30340;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20154;&#35748;&#20026;&#65292;&#21333;&#35789;&#21046;&#20316;&#30340;&#20934;&#22791;&#39034;&#24207;&#21462;&#20915;&#20110;&#35828;&#35805;&#32773;&#25152;&#29992;&#30340;&#35821;&#35328;&#12290;&#24403;&#24503;&#35821;&#25110;&#33655;&#20848;&#35821;&#30340;&#35828;&#35805;&#32773;&#21046;&#20316;&#23567;&#29483;&#30340;&#32763;&#35793;&#26102;&#65292;&#20250;&#22312;&#21046;&#20316;&#36807;&#31243;&#30340;&#36739;&#26089;&#38454;&#27573;&#36873;&#25321;&#26631;&#35760;&#24615;&#21035;&#30340;&#38480;&#23450;&#35789;&#12290;&#32780;&#27861;&#35821;&#25110;&#24847;&#22823;&#21033;&#35821;&#30340;&#35828;&#35805;&#32773;&#20250;&#23558;&#38480;&#23450;&#35789;&#25110;&#24418;&#23481;&#35789;&#30340;&#32534;&#30721;&#25512;&#36831;&#21040;&#21487;&#29992;&#21517;&#35789;&#30340;&#38899;&#38901;&#24418;&#24335;&#20043;&#21518;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#21333;&#35789;&#30340;&#39034;&#24207;&#30456;&#21516;&#65288;&#20363;&#22914;&#22312;&#24503;&#35821;&#20013;&#26159;&#8220;die kleine Katze&#8221;&#65292;&#22312;&#27861;&#35821;&#20013;&#26159;&#8220;le petit chat&#8221;&#65289;&#65292;&#23427;&#20204;&#30340;&#35745;&#21010;&#39034;&#24207;&#19981;&#21516;&#65292;&#21487;&#33021;&#38656;&#35201;&#22312;&#21046;&#20316;&#24320;&#22987;&#21069;&#36827;&#34892;&#19981;&#21516;&#31243;&#24230;&#30340;&#25552;&#21069;&#35745;&#21010;&#12290;&#36825;&#31181;&#26089;&#26399;&#21644;&#26202;&#26399;&#36873;&#25321;&#35821;&#35328;&#20043;&#38388;&#30340;&#21306;&#21035;&#26159;&#20026;&#20102;&#35299;&#37322;&#35266;&#23519;&#21040;&#30340;&#25351;&#20986;&#24503;&#26085;&#31561;&#35821;&#35328;&#20294;&#19981;&#21253;&#25324;&#32599;&#26364;&#35821;&#31995;&#35821;&#35328;&#20013;&#30340;&#22270;&#29255;&#21629;&#21517;&#36895;&#24230;&#36739;&#24930;&#30340;&#24615;&#21035;&#24178;&#25200;&#25928;&#24212;&#12290;&#36827;&#34892;&#20803;&#20998;&#26512;&#20197;&#30452;&#25509;&#27979;&#35797;&#36825;&#19968;&#36328;&#35821;&#35328;&#24615;&#20551;&#35828;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#22270;&#29255;&#21629;&#21517;&#21453;&#24212;&#26102;&#38388;&#20013;&#65292;&#24615;&#21035;&#19968;&#33268;&#25928;&#24212;&#22312;&#26085;&#23572;&#26364;&#35821;&#35328;&#21644;&#26031;&#25289;&#22827;&#35821;&#35328;&#20013;&#30340;&#31283;&#20581;&#24615;&#27604;&#32599;&#26364;&#35821;&#31995;&#35821;&#35328;&#26356;&#39640;&#12290;&#20294;&#26159;&#65292;&#25928;&#24212;&#22823;&#23567;&#36866;&#20013;&#65292;&#24182;&#19988;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;&#25928;&#24212;&#22823;&#23567;&#22312;&#30740;&#31350;&#20043;&#38388;&#23384;&#22312;&#30456;&#24403;&#30340;&#21464;&#24322;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#25351;&#20986;&#20102;&#24615;&#21035;&#26631;&#35760;&#24433;&#21709;&#35821;&#35328;&#21046;&#20316;&#30340;&#36328;&#35821;&#35328;&#24046;&#24322;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#32771;&#34385;&#24433;&#21709;&#24615;&#21035;&#19968;&#33268;&#24615;&#25928;&#24212;&#22823;&#23567;&#30340;&#26041;&#27861;&#21644;&#35821;&#22659;&#22240;&#32032;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been proposed that the order in which words are prepared for production depends on the speaker's language. When producing the translation equivalent of the small cat, speakers of German or Dutch select the gender-marked determiner at a relatively early stage of production. Speakers of French or Italian postpone the encoding of a determiner or adjective until the phonological form of the noun is available. Hence, even though the words are produced in the same order (e.g., die kleine Katze in German, le petit chat in French), they are not planned in the same order and might require different amounts of advanced planning prior to production onset. This distinction between early and late selection languages was proposed to account for the observation that speakers of Germanic and Slavic languages, but not of Romance languages, are slower to name pictures in the context of a distractor word of a different gender. Meta-analyses are conducted to provide the first direct test of this cr
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#23454;&#39564;&#34920;&#26126;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;NER&#26041;&#27861;&#21487;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2109.01636</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01636
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#23454;&#39564;&#34920;&#26126;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;NER&#26041;&#27861;&#21487;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;NER&#20219;&#21153;&#38754;&#20020;&#30340;&#26368;&#22823;&#22256;&#38590;&#26159;&#21363;&#20351;&#22312;NE&#31867;&#22411;&#21644;&#25991;&#26723;&#19981;&#29087;&#24713;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#38656;&#35201;&#20445;&#25345;&#21487;&#26816;&#27979;&#24615;&#12290;&#24847;&#35782;&#21040;&#29305;&#23450;&#24615;&#20449;&#24687;&#21487;&#33021;&#21253;&#21547;&#21333;&#35789;&#30340;&#28508;&#22312;&#21547;&#20041;&#24182;&#29983;&#25104;&#35789;&#23884;&#20837;&#30340;&#35821;&#20041;&#30456;&#20851;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;&#29616;&#26377;&#30340;NER&#26041;&#27861;&#20013;&#65292;NER&#30340;&#24615;&#33021;&#23558;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the fast development of Deep Learning techniques, Named Entity Recognition (NER) is becoming more and more important in the information extraction task. The greatest difficulty that the NER task faces is to keep the detectability even when types of NE and documents are unfamiliar. Realizing that the specificity information may contain potential meanings of a word and generate semantic-related features for word embedding, we develop a distribution-aware word embedding and implement three different methods to make use of the distribution information in a NER framework. And the result shows that the performance of NER will be improved if the word specificity is incorporated into existing NER methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2108.08614</link><description>&lt;p&gt;
UNIQORN&#65306;&#32479;&#19968;&#30340;RDF&#30693;&#35782;&#22270;&#35889;&#19982;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.08614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#22312;&#30693;&#35782;&#22270;&#35889;&#21644;&#20854;&#20182;RDF&#25968;&#25454;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#35768;&#22810;&#20248;&#31168;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#25110;&#30005;&#25253;&#26597;&#35810;&#25552;&#20379;&#28165;&#26224;&#30340;&#31572;&#26696;&#12290;&#20854;&#20013;&#19968;&#20123;&#31995;&#32479;&#23558;&#25991;&#26412;&#28304;&#20316;&#20026;&#38468;&#21152;&#35777;&#25454;&#32435;&#20837;&#22238;&#31572;&#36807;&#31243;&#65292;&#20294;&#19981;&#33021;&#35745;&#31639;&#20165;&#23384;&#22312;&#20110;&#25991;&#26412;&#20013;&#30340;&#31572;&#26696;&#12290;&#30456;&#21453;&#65292;IR&#21644;NLP&#31038;&#21306;&#30340;&#31995;&#32479;&#24050;&#32463;&#35299;&#20915;&#20102;&#26377;&#20851;&#25991;&#26412;&#30340;QA&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20123;&#31995;&#32479;&#20960;&#20046;&#19981;&#21033;&#29992;&#35821;&#20041;&#25968;&#25454;&#21644;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20197;&#26080;&#32541;&#25805;&#20316;&#28151;&#21512;RDF&#25968;&#25454;&#38598;&#21644;&#25991;&#26412;&#35821;&#26009;&#24211;&#25110;&#21333;&#20010;&#26469;&#28304;&#30340;&#22797;&#26434;&#38382;&#39064;&#30340;&#31995;&#32479;&#65292;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;UNIQORN&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;BERT&#27169;&#22411;&#20174;RDF&#25968;&#25454;&#21644;/&#25110;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35777;&#25454;&#26469;&#21160;&#24577;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#12290;&#32467;&#26524;&#22270;&#36890;&#24120;&#38750;&#24120;&#20016;&#23500;&#20294;&#39640;&#24230;&#22024;&#26434;&#12290;UNIQORN&#36890;&#36807;&#29992;&#20110;&#32452;Steiner&#26641;&#30340;&#22270;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#36755;&#20837;&#65292;&#20174;&#32780;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#65292;&#36827;&#32780;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge graphs and other RDF data has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, systems from the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents the first system for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph is typically rich but highly noisy. UNIQORN copes with this input by a graph algorithm for Group Steiner Trees, that identifi
&lt;/p&gt;</description></item></channel></rss>