<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>CLIP&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#20854;&#25968;&#25454;&#32780;&#38750;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20803;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#24341;&#20837;&#20102;MetaCLIP&#65292;&#35813;&#26041;&#27861;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#20013;&#29983;&#25104;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#35814;&#32454;&#30340;&#25968;&#25454;&#20449;&#24687;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MetaCLIP&#22312;&#22788;&#29702;400M&#20010;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16671</link><description>&lt;p&gt;
&#25581;&#31192;CLIP&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Demystifying CLIP Data. (arXiv:2309.16671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16671
&lt;/p&gt;
&lt;p&gt;
CLIP&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#20854;&#25968;&#25454;&#32780;&#38750;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20803;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#24341;&#20837;&#20102;MetaCLIP&#65292;&#35813;&#26041;&#27861;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#20013;&#29983;&#25104;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#35814;&#32454;&#30340;&#25968;&#25454;&#20449;&#24687;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MetaCLIP&#22312;&#22788;&#29702;400M&#20010;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26159;&#19968;&#31181;&#25512;&#21160;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#20026;&#29616;&#20195;&#35782;&#21035;&#31995;&#32479;&#21644;&#29983;&#25104;&#27169;&#22411;&#27880;&#20837;&#20102;&#27963;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;CLIP&#25104;&#21151;&#30340;&#20027;&#35201;&#22240;&#32032;&#26159;&#20854;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;CLIP&#21482;&#25552;&#20379;&#20102;&#20851;&#20110;&#20854;&#25968;&#25454;&#21644;&#22914;&#20309;&#25910;&#38598;&#25968;&#25454;&#30340;&#38750;&#24120;&#26377;&#38480;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#20854;&#20182;&#30740;&#31350;&#21162;&#21147;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#36807;&#28388;&#26469;&#37325;&#29616;CLIP&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24847;&#22312;&#25581;&#31034;CLIP&#30340;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#65292;&#24182;&#22312;&#20844;&#24320;&#32473;&#31038;&#21306;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#20803;&#25968;&#25454;&#25972;&#29702;&#30340;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;MetaCLIP&#65289;&#12290;MetaCLIP&#36890;&#36807;&#23545;&#20803;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24179;&#34913;&#65292;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#65288;&#20174;CLIP&#30340;&#27010;&#24565;&#20013;&#24471;&#20986;&#65289;&#20013;&#20135;&#29983;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#20005;&#26684;&#38548;&#31163;&#20102;&#27169;&#22411;&#21644;&#35757;&#32451;&#35774;&#32622;&#65292;&#20165;&#19987;&#27880;&#20110;&#25968;&#25454;&#12290;MetaCLIP&#24212;&#29992;&#20110;&#21253;&#21547;400M&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#30340;CommonCrawl&#65292;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP's data by filtering with its model parameters. In this work, we intend to reveal CLIP's data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outper
&lt;/p&gt;</description></item><item><title>MindShift&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#22522;&#20110;&#24515;&#24577;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#36866;&#24212;&#29992;&#25143;&#29615;&#22659;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#39640;&#36136;&#37327;&#35828;&#26381;&#20869;&#23481;&#26469;&#24110;&#21161;&#29992;&#25143;&#35299;&#20915;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#30340;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2309.16639</link><description>&lt;p&gt;
MindShift: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#24515;&#24577;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention. (arXiv:2309.16639v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16639
&lt;/p&gt;
&lt;p&gt;
MindShift&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#22522;&#20110;&#24515;&#24577;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#36866;&#24212;&#29992;&#25143;&#29615;&#22659;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#39640;&#36136;&#37327;&#35828;&#26381;&#20869;&#23481;&#26469;&#24110;&#21161;&#29992;&#25143;&#35299;&#20915;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#30340;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#23545;&#36523;&#20307;&#21644;&#24515;&#29702;&#20581;&#24247;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#20808;&#21069;&#30740;&#31350;&#65292;&#29616;&#26377;&#30340;&#35828;&#26381;&#25216;&#24039;&#19981;&#36275;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#36523;&#20307;&#29615;&#22659;&#21644;&#24515;&#29702;&#29366;&#24577;&#25552;&#20379;&#21160;&#24577;&#35828;&#26381;&#20869;&#23481;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#20026;&#25805;&#20316;&#30740;&#31350;&#65288;N = 12&#65289;&#21644;&#19968;&#39033;&#35775;&#35848;&#30740;&#31350;&#65288;N = 10&#65289;&#65292;&#24635;&#32467;&#20102;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#32972;&#21518;&#30340;&#24515;&#24577;&#65306;&#26080;&#32842;&#12289;&#21387;&#21147;&#21644;&#24815;&#24615;&#12290;&#36825;&#20026;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#31181;&#35828;&#26381;&#31574;&#30053;&#65306;&#29702;&#35299;&#12289;&#23433;&#25242;&#12289;&#21796;&#36215;&#21644;&#25903;&#25345;&#20064;&#24815;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23454;&#29616;&#20102;&#26377;&#25928;&#35828;&#26381;&#20869;&#23481;&#30340;&#33258;&#21160;&#21644;&#21160;&#24577;&#29983;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#25216;&#26415;&#39537;&#21160;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;&#25216;&#26415;MindShift&#12290;MindShift&#26681;&#25454;&#29992;&#25143;&#24403;&#19979;&#30340;&#36523;&#20307;&#29615;&#22659;&#12289;&#24515;&#24577;&#12289;&#24212;&#29992;&#20351;&#29992;&#34892;&#20026;&#12289;&#29992;&#25143;&#30340;&#30446;&#26631;&#19982;&#20064;&#24815;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#36866;&#24403;&#35828;&#26381;&#31574;&#30053;&#30340;&#39640;&#36136;&#37327;&#21644;&#28789;&#27963;&#30340;&#35828;&#26381;&#20869;&#23481;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;5-
&lt;/p&gt;
&lt;p&gt;
Problematic smartphone use negatively affects physical and mental health. Despite the wide range of prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users' physical contexts and mental states. We first conduct a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leverage large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We develop MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users' in-the-moment physical contexts, mental states, app usage behaviors, users' goals &amp; habits as input, and generates high-quality and flexible persuasive content with appropriate persuasion strategies. We conduct a 5-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21387;&#21147;&#27979;&#35797;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38142;&#24335;&#24605;&#36335;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#20934;&#30830;&#30340;CoT&#20540;&#23545;&#20110;&#39044;&#27979;&#27491;&#30830;&#31572;&#26696;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#38169;&#35823;&#30340;CoT&#25552;&#31034;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;CoT&#36816;&#31639;&#31526;&#25110;CoT&#39034;&#24207;&#38169;&#35823;&#30340;&#19981;&#27491;&#30830;&#28436;&#31034;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;&#36825;&#39033;&#30740;&#31350;&#21152;&#28145;&#20102;&#23545;CoT&#25552;&#31034;&#30340;&#29702;&#35299;&#65292;&#24182;&#23545;LLM&#23398;&#20064;&#19978;&#19979;&#25991;&#25512;&#29702;&#33021;&#21147;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16621</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38142;&#24335;&#24605;&#36335;&#25552;&#31034;&#30340;&#21387;&#21147;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Stress Testing Chain-of-Thought Prompting for Large Language Models. (arXiv:2309.16621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21387;&#21147;&#27979;&#35797;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38142;&#24335;&#24605;&#36335;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#20934;&#30830;&#30340;CoT&#20540;&#23545;&#20110;&#39044;&#27979;&#27491;&#30830;&#31572;&#26696;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#38169;&#35823;&#30340;CoT&#25552;&#31034;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;CoT&#36816;&#31639;&#31526;&#25110;CoT&#39034;&#24207;&#38169;&#35823;&#30340;&#19981;&#27491;&#30830;&#28436;&#31034;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;&#36825;&#39033;&#30740;&#31350;&#21152;&#28145;&#20102;&#23545;CoT&#25552;&#31034;&#30340;&#29702;&#35299;&#65292;&#24182;&#23545;LLM&#23398;&#20064;&#19978;&#19979;&#25991;&#25512;&#29702;&#33021;&#21147;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#30740;&#31350;&#20102;&#38142;&#24335;&#24605;&#36335;&#25552;&#31034;&#65288;CoT&#65289;&#22312;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#21463;&#21040;&#20043;&#21069;&#30340;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;CoT&#25552;&#31034;&#30340;&#19977;&#31181;&#31867;&#22411;&#30340;&#25200;&#21160;&#65288;&#21363;CoT&#39034;&#24207;&#65292;CoT&#20540;&#21644;CoT&#36816;&#31639;&#31526;&#65289;&#23545;GPT-3&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#38169;&#35823;&#30340;CoT&#25552;&#31034;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#25351;&#26631;&#19979;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#27491;&#30830;&#30340;CoT&#20540;&#23545;&#20110;&#39044;&#27979;&#27491;&#30830;&#31572;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#24403;&#19982;&#22522;&#20110;&#20540;&#30340;&#25200;&#21160;&#30456;&#27604;&#26102;&#65292;CoT&#36816;&#31639;&#31526;&#25110;CoT&#39034;&#24207;&#38169;&#35823;&#30340;&#19981;&#27491;&#30830;&#28436;&#31034;&#24182;&#27809;&#26377;&#22914;&#27492;&#21095;&#28872;&#22320;&#24433;&#21709;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#21152;&#28145;&#20102;&#25105;&#20204;&#23545;CoT&#25552;&#31034;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#20851;&#20110;LLM&#23398;&#20064;&#19978;&#19979;&#25991;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report examines the effectiveness of Chain-of-Thought (CoT) prompting in improving the multi-step reasoning abilities of large language models (LLMs). Inspired by previous studies \cite{Min2022RethinkingWork}, we analyze the impact of three types of CoT prompt perturbations, namely CoT order, CoT values, and CoT operators on the performance of GPT-3 on various tasks. Our findings show that incorrect CoT prompting leads to poor performance on accuracy metrics. Correct values in the CoT is crucial for predicting correct answers. Moreover, incorrect demonstrations, where the CoT operators or the CoT order are wrong, do not affect the performance as drastically when compared to the value based perturbations. This research deepens our understanding of CoT prompting and opens some new questions regarding the capability of LLMs to learn reasoning in context.
&lt;/p&gt;</description></item><item><title>Qwen &#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21253;&#21547;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#27169;&#22411;&#12290;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#32842;&#22825;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#23637;&#29616;&#20986;&#20102;&#39640;&#31454;&#20105;&#21147;&#65292;&#20855;&#22791;&#39640;&#32423;&#30340;&#24037;&#20855;&#20351;&#29992;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#19978;&#19982;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#30456;&#27604;&#20063;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16609</link><description>&lt;p&gt;
Qwen &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Qwen Technical Report. (arXiv:2309.16609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16609
&lt;/p&gt;
&lt;p&gt;
Qwen &#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21253;&#21547;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#27169;&#22411;&#12290;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#32842;&#22825;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#23637;&#29616;&#20986;&#20102;&#39640;&#31454;&#20105;&#21147;&#65292;&#20855;&#22791;&#39640;&#32423;&#30340;&#24037;&#20855;&#20351;&#29992;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#19978;&#19982;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#30456;&#27604;&#20063;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#20351;&#24471;&#20197;&#21069;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#19987;&#23646;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; Qwen&#65292;&#25105;&#20204;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#30340;&#31532;&#19968;&#37096;&#20998;&#12290;Qwen &#26159;&#19968;&#20010;&#21253;&#21547;&#19981;&#21516;&#21442;&#25968;&#25968;&#37327;&#30340;&#20840;&#38754;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#23427;&#21253;&#25324; Qwen&#65292;&#22522;&#30784;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#20154;&#31867;&#23545;&#40784;&#25216;&#26415;&#24494;&#35843;&#30340; Qwen-Chat&#65292;&#32842;&#22825;&#27169;&#22411;&#12290;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#32780;&#32842;&#22825;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#12290;&#32842;&#22825;&#27169;&#22411;&#20855;&#22791;&#39640;&#32423;&#30340;&#24037;&#20855;&#20351;&#29992;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#29992;&#20110;&#21019;&#24314;&#20195;&#29702;&#31243;&#24207;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#65288;&#22914;&#20351;&#29992;&#20195;&#30721;&#35299;&#37322;&#22120;&#65289;&#26102;&#65292;&#21363;&#20351;&#19982;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#20063;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have 
&lt;/p&gt;</description></item><item><title>&#22312;&#38646;&#26679;&#26412;&#32763;&#35793;&#20013;&#65292;&#35821;&#35328;ID&#26377;&#26102;&#26080;&#27861;&#27491;&#30830;&#24341;&#23548;&#20219;&#21153;&#65292;&#24182;&#23548;&#33268;&#29983;&#25104;&#30340;&#32763;&#35793;&#20013;&#23384;&#22312;&#38750;&#30446;&#26631;&#35821;&#35328;&#35789;&#27719;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#35299;&#30721;&#22120;&#36755;&#20837;&#24773;&#20917;&#19979;&#30340;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#22312;&#36127;&#26679;&#26412;&#19978;&#36827;&#34892;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#32763;&#35793;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16599</link><description>&lt;p&gt;
&#22312;&#36127;&#26679;&#26412;&#19978;&#36827;&#34892;&#19981;&#22826;&#21487;&#33021;&#30340;&#35843;&#25972;&#24778;&#20154;&#22320;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Unlikelihood Tuning on Negative Samples Amazingly Improves Zero-Shot Translation. (arXiv:2309.16599v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16599
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#26679;&#26412;&#32763;&#35793;&#20013;&#65292;&#35821;&#35328;ID&#26377;&#26102;&#26080;&#27861;&#27491;&#30830;&#24341;&#23548;&#20219;&#21153;&#65292;&#24182;&#23548;&#33268;&#29983;&#25104;&#30340;&#32763;&#35793;&#20013;&#23384;&#22312;&#38750;&#30446;&#26631;&#35821;&#35328;&#35789;&#27719;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#35299;&#30721;&#22120;&#36755;&#20837;&#24773;&#20917;&#19979;&#30340;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#22312;&#36127;&#26679;&#26412;&#19978;&#36827;&#34892;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#32763;&#35793;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#32763;&#35793;(ZST)&#36890;&#24120;&#22522;&#20110;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#26088;&#22312;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32763;&#35793;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#24120;&#35265;&#30340;&#25351;&#23548;&#38646;&#26679;&#26412;&#35821;&#35328;&#26144;&#23556;&#30340;&#20570;&#27861;&#26159;&#25925;&#24847;&#25554;&#20837;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;ID&#65292;&#20363;&#22914;&#33521;&#35821;&#30340;&lt;EN&gt;&#21644;&#24503;&#35821;&#30340;&lt;DE&gt;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;ID&#26377;&#26102;&#26080;&#27861;&#27491;&#30830;&#24341;&#23548;ZST&#20219;&#21153;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#32763;&#35793;&#20013;&#23384;&#22312;&#38750;&#30446;&#26631;&#35821;&#35328;&#35789;&#27719;&#65292;&#20174;&#32780;&#20351;&#24403;&#21069;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#38590;&#20197;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#38646;&#26679;&#26412;&#35821;&#35328;&#22330;&#26223;&#12290;&#20026;&#20102;&#20102;&#35299;&#35821;&#35328;ID&#30340;&#23548;&#33322;&#33021;&#21147;&#20309;&#26102;&#20197;&#21450;&#20026;&#20309;&#20943;&#24369;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;ZST&#26041;&#21521;&#19978;&#20004;&#31181;&#26497;&#31471;&#30340;&#35299;&#30721;&#22120;&#36755;&#20837;&#24773;&#20917;&#65306;&#38750;&#30446;&#26631;(Off-Target&#65292;OFF)&#21644;&#30446;&#26631;(On-Target&#65292;ON)&#24773;&#20917;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#24773;&#20917;&#19979;&#20351;&#29992;&#25945;&#24072;&#24378;&#21046;&#30340;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;(CWRs)&#36827;&#34892;&#23545;&#27604;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;1&#65289;the
&lt;/p&gt;
&lt;p&gt;
Zero-shot translation (ZST), which is generally based on a multilingual neural machine translation model, aims to translate between unseen language pairs in training data. The common practice to guide the zero-shot language mapping during inference is to deliberately insert the source and target language IDs, e.g., &lt;EN&gt; for English and &lt;DE&gt; for German. Recent studies have shown that language IDs sometimes fail to navigate the ZST task, making them suffer from the off-target problem (non-target language words exist in the generated translation) and, therefore, difficult to apply the current multilingual translation model to a broad range of zero-shot language scenarios. To understand when and why the navigation capabilities of language IDs are weakened, we compare two extreme decoder input cases in the ZST directions: Off-Target (OFF) and On-Target (ON) cases. By contrastively visualizing the contextual word representations (CWRs) of these cases with teacher forcing, we show that 1) the
&lt;/p&gt;</description></item><item><title>GPT-Fathom&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#22871;&#20214;&#65292;&#23427;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20174;GPT-3&#21040;GPT-4&#28436;&#21270;&#36335;&#24452;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.16583</link><description>&lt;p&gt;
GPT-Fathom&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#35299;&#26512;GPT-4&#21450;&#20854;&#21518;&#32493;&#29256;&#26412;&#30340;&#28436;&#21270;&#36335;&#24452;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond. (arXiv:2309.16583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16583
&lt;/p&gt;
&lt;p&gt;
GPT-Fathom&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#22871;&#20214;&#65292;&#23427;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20174;GPT-3&#21040;GPT-4&#28436;&#21270;&#36335;&#24452;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#20154;&#20204;&#36843;&#20999;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#22871;&#20214;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#29616;&#26377;&#30340;LLM&#25490;&#34892;&#27036;&#36890;&#24120;&#21442;&#32771;&#20854;&#20182;&#35770;&#25991;&#20013;&#25253;&#21578;&#30340;&#24471;&#20998;&#65292;&#35774;&#32622;&#21644;&#25552;&#31034;&#19981;&#19968;&#33268;&#65292;&#36825;&#21487;&#33021;&#26080;&#24847;&#38388;&#40723;&#21169;&#36873;&#25321;&#26377;&#21033;&#30340;&#35774;&#32622;&#21644;&#25552;&#31034;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GPT-Fathom&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;OpenAI Evals&#26500;&#24314;&#30340;&#24320;&#28304;&#21644;&#21487;&#37325;&#22797;&#30340;LLM&#35780;&#20272;&#22871;&#20214;&#12290;&#25105;&#20204;&#22312;&#23545;&#40784;&#30340;&#29615;&#22659;&#35774;&#32622;&#19979;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;LLMs&#20197;&#21450;OpenAI&#30340;&#20256;&#32479;&#27169;&#22411;&#22312;20&#22810;&#20010;&#31934;&#36873;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#65292;&#28085;&#30422;&#20102;7&#20010;&#33021;&#21147;&#31867;&#21035;&#12290;&#25105;&#20204;&#23545;OpenAI&#26089;&#26399;&#27169;&#22411;&#30340;&#22238;&#39038;&#24615;&#30740;&#31350;&#20026;&#25105;&#20204;&#25581;&#31034;&#20102;&#20174;GPT-3&#21040;GPT-4&#30340;&#28436;&#21270;&#36335;&#24452;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#30446;&#21069;&#65292;&#31038;&#21306;&#28212;&#26395;&#20102;&#35299;GPT-3&#22914;&#20309;&#36880;&#27493;&#25913;&#36827;&#21040;GPT-4&#65292;&#21253;&#25324;&#20687;&#28155;&#21152;&#20195;&#30721;&#25968;&#25454;&#26159;&#21542;&#25552;&#39640;&#20102;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;LLM&#33021;&#21147;&#30340;&#21738;&#20123;&#26041;&#38754;&#31561;&#25216;&#26415;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI's earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM's reasoning capability, which aspects of LLM capabili
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19968;&#26412;&#35821;&#27861;&#20070;&#20013;&#23398;&#20064;&#32763;&#35793;&#26032;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;MTOB&#65292;&#29992;&#20110;&#32763;&#35793;&#33521;&#35821;&#21644;Kalamang&#20043;&#38388;&#30340;&#25991;&#26412;&#65292;&#25506;&#32034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#24773;&#20917;&#19979;&#30340;&#32763;&#35793;&#38382;&#39064;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.16575</link><description>&lt;p&gt;
&#20174;&#19968;&#26412;&#35821;&#27861;&#20070;&#23398;&#20064;&#32763;&#35793;&#26032;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Benchmark for Learning to Translate a New Language from One Grammar Book. (arXiv:2309.16575v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19968;&#26412;&#35821;&#27861;&#20070;&#20013;&#23398;&#20064;&#32763;&#35793;&#26032;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;MTOB&#65292;&#29992;&#20110;&#32763;&#35793;&#33521;&#35821;&#21644;Kalamang&#20043;&#38388;&#30340;&#25991;&#26412;&#65292;&#25506;&#32034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#24773;&#20917;&#19979;&#30340;&#32763;&#35793;&#38382;&#39064;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25110;&#36731;&#37327;&#32423;&#24494;&#35843;&#26469;&#23436;&#25104;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20219;&#21153;&#12290;&#20154;&#20204;&#33258;&#28982;&#32780;&#28982;&#22320;&#24819;&#30693;&#36947;&#36825;&#20123;&#27169;&#22411;&#22312;&#36866;&#24212;&#20840;&#26032;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#20294;&#22914;&#20309;&#25214;&#21040;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#21602;&#65311;&#25105;&#20204;&#36716;&#21521;&#19968;&#20010;&#26126;&#30830;&#21463;&#21040;&#32593;&#32476;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#39537;&#21160;&#21644;&#38480;&#21046;&#30340;&#39046;&#22495;&#65306;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;MTOB&#65288;&#20174;&#19968;&#26412;&#20070;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#33521;&#35821;&#21644;Kalamang&#20043;&#38388;&#32763;&#35793;--Kalamang&#26159;&#19968;&#31181;&#21482;&#26377;&#19981;&#21040;200&#21517;&#20351;&#29992;&#32773;&#30340;&#35821;&#35328;&#65292;&#22240;&#27492;&#22312;&#32593;&#32476;&#19978;&#20960;&#20046;&#27809;&#26377;&#23384;&#22312;&#24863;--&#25105;&#20204;&#20351;&#29992;&#20102;&#20960;&#30334;&#39029;&#30340;&#30000;&#37326;&#35821;&#35328;&#23398;&#21442;&#32771;&#36164;&#26009;&#12290;&#36825;&#31181;&#20219;&#21153;&#26694;&#26550;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#65292;&#23427;&#35201;&#27714;&#27169;&#22411;&#20174;&#19968;&#26412;&#20154;&#31867;&#21487;&#35835;&#30340;&#35821;&#27861;&#35299;&#37322;&#20070;&#20013;&#23398;&#20064;&#19968;&#31181;&#35821;&#35328;&#65292;&#32780;&#19981;&#26159;&#20174;&#22823;&#35268;&#27169;&#25366;&#25496;&#30340;&#39046;&#22495;&#20869;&#25968;&#25454;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#65292;&#26356;&#31867;&#20284;&#20110;L2&#23398;&#20064;&#32780;&#19981;&#26159;L1&#20064;&#24471;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#24403;&#21069;LLMs&#30340;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20173;&#28982;&#19981;&#21450;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang -- a language with less than 200 speakers and therefore virtually no presence on the web -using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 learning than L1 acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human perform
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#65288;LMaaS&#65289;&#20316;&#20026;&#19987;&#26377;&#31995;&#32479;&#65292;&#38480;&#21046;&#20102;&#20854;&#21487;&#35775;&#38382;&#24615;&#21644;&#35780;&#20272;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;ARRT&#65288;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#65289;&#25361;&#25112;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#36825;&#20123;&#25361;&#25112;&#20197;&#21450;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.16573</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#30340;ARRT: &#26032;&#33539;&#24335;&#21450;&#20854;&#25361;&#25112;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges. (arXiv:2309.16573v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16573
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#65288;LMaaS&#65289;&#20316;&#20026;&#19987;&#26377;&#31995;&#32479;&#65292;&#38480;&#21046;&#20102;&#20854;&#21487;&#35775;&#38382;&#24615;&#21644;&#35780;&#20272;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;ARRT&#65288;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#65289;&#25361;&#25112;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#36825;&#20123;&#25361;&#25112;&#20197;&#21450;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#19968;&#20123;&#26368;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#37117;&#26159;&#19987;&#26377;&#31995;&#32479;&#65292;&#21482;&#33021;&#36890;&#36807;&#65288;&#36890;&#24120;&#26159;&#38480;&#21046;&#24615;&#30340;&#65289;&#32593;&#32476;&#25110;&#36719;&#20214;&#32534;&#31243;&#25509;&#21475;&#35775;&#38382;&#12290;&#36825;&#23601;&#26159;&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#65288;LMaaS&#65289;&#30340;&#33539;&#24335;&#12290;&#19982;&#21487;&#20197;&#23436;&#20840;&#35775;&#38382;&#27169;&#22411;&#30340;&#24773;&#20917;&#30456;&#21453;&#65292;&#22914;&#24320;&#25918;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#23553;&#38381;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#35780;&#20272;&#12289;&#22522;&#20934;&#27979;&#35797;&#21644;&#27979;&#35797;&#36896;&#25104;&#20102;&#29305;&#23450;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#30340;&#20004;&#20010;&#30446;&#26631;&#26159;&#65306;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30028;&#23450;&#21069;&#36848;&#25361;&#25112;&#22914;&#20309;&#20316;&#20026;&#23545;LMaaS&#30340;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#65288;ARRT&#65289;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#19982;&#27599;&#20010;&#36825;&#22235;&#20010;&#26041;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;&#20449;&#24687;&#19981;&#36275;&#26377;&#20851;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#24314;&#35758;&#65292;&#24182;&#24378;&#35843;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26412;&#25991;&#26159;&#24403;&#21069;&#20027;&#35201;LMaaS&#29616;&#26377;&#30693;&#35782;&#30340;&#19968;&#31449;&#24335;&#38598;&#38182;&#65292;&#25552;&#20379;&#20102;&#32508;&#21512;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some of the most powerful language models currently are proprietary systems, accessible only via (typically restrictive) web or software programming interfaces. This is the Language-Models-as-a-Service (LMaaS) paradigm. Contrasting with scenarios where full model access is available, as in the case of open-source models, such closed-off language models create specific challenges for evaluating, benchmarking, and testing them. This paper has two goals: on the one hand, we delineate how the aforementioned challenges act as impediments to the accessibility, replicability, reliability, and trustworthiness (ARRT) of LMaaS. We systematically examine the issues that arise from a lack of information about language models for each of these four aspects. We shed light on current solutions, provide some recommendations, and highlight the directions for future advancements. On the other hand, it serves as a one-stop-shop for the extant knowledge about current, major LMaaS, offering a synthesized o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SFAVEL&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#12290;&#36825;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.16540</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Fact Verification by Language Model Distillation. (arXiv:2309.16540v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SFAVEL&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#12290;&#36825;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#26088;&#22312;&#36890;&#36807;&#21487;&#38752;&#30693;&#35782;&#24211;&#20013;&#30340;&#35777;&#25454;&#26469;&#39564;&#35777;&#20027;&#24352;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#31639;&#27861;&#24517;&#39035;&#20026;&#27599;&#20010;&#20027;&#24352;&#29983;&#25104;&#26082;&#35821;&#20041;&#26126;&#30830;&#21448;&#32039;&#20945;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#19982;&#28304;&#20449;&#24687;&#36827;&#34892;&#35821;&#20041;&#23545;&#40784;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#21069;&#32773;&#36890;&#36807;&#23398;&#20064;&#21253;&#21547;&#20027;&#24352;&#21450;&#20854;&#30456;&#24212;&#26631;&#31614;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#26469;&#35299;&#20915;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SFAVEL&#65288;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#33976;&#39311;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#32780;&#26080;&#38656;&#27880;&#37322;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#30340;&#65292;&#35813;&#20989;&#25968;&#40723;&#21169;&#29305;&#24449;&#22312;&#20445;&#25345;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;&#21644;&#35777;&#25454;&#23545;&#40784;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36798;&#21040;&#26032;&#39062;&#30340;&#29366;&#24577;&#19968;.
&lt;/p&gt;
&lt;p&gt;
Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the
&lt;/p&gt;</description></item><item><title>KLoB&#26159;&#19968;&#20010;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#20107;&#23454;&#30693;&#35782;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16535</link><description>&lt;p&gt;
KLoB: &#19968;&#31181;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models. (arXiv:2309.16535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16535
&lt;/p&gt;
&lt;p&gt;
KLoB&#26159;&#19968;&#20010;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#20107;&#23454;&#30693;&#35782;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23450;&#20301;&#28982;&#21518;&#32534;&#36753;&#30340;&#33539;&#24335;&#24050;&#32463;&#25104;&#20026;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23450;&#20301;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#22320;&#25214;&#21040;&#23884;&#20837;&#25152;&#38656;&#30693;&#35782;&#30340;&#30830;&#20999;&#21442;&#25968;&#36824;&#32570;&#20047;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#23545;&#20107;&#23454;&#30693;&#35782;&#30340;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#20294;&#27809;&#26377;&#25552;&#20379;&#19968;&#31181;&#27979;&#35797;&#20551;&#35774;&#30340;&#26041;&#27861;&#20197;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#35752;&#35770;&#21644;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KLoB&#65292;&#19968;&#20010;&#35780;&#20272;&#21487;&#38752;&#30340;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#24212;&#28385;&#36275;&#30340;&#19977;&#20010;&#22522;&#26412;&#23646;&#24615;&#30340;&#22522;&#20934;&#12290;KLoB&#21487;&#20316;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#24182;&#20026;&#37325;&#26032;&#35780;&#20272;&#20107;&#23454;&#30693;&#35782;&#30340;&#23616;&#37096;&#24615;&#20551;&#35774;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110;\url{https://github.com/juyiming/KLoB}&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Locate-Then-Edit paradigm has emerged as one of the main approaches in changing factual knowledge stored in the Language models. However, there is a lack of research on whether present locating methods can pinpoint the exact parameters embedding the desired knowledge. Moreover, although many researchers have questioned the validity of locality hypothesis of factual knowledge, no method is provided to test the a hypothesis for more in-depth discussion and research. Therefore, we introduce KLoB, a benchmark examining three essential properties that a reliable knowledge locating method should satisfy. KLoB can serve as a benchmark for evaluating existing locating methods in language models, and can contributes a method to reassessing the validity of locality hypothesis of factual knowledge. Our is publicly available at \url{https://github.com/juyiming/KLoB}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Toloka&#35270;&#35273;&#38382;&#31572;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20247;&#21253;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#27604;&#36739;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#31454;&#36187;&#65292;&#21457;&#29616;&#30446;&#21069;&#27809;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#36229;&#36807;&#38750;&#19987;&#23478;&#20247;&#21253;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2309.16511</link><description>&lt;p&gt;
Toloka&#35270;&#35273;&#38382;&#31572;&#22522;&#20934;. (arXiv:2309.16511v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
Toloka Visual Question Answering Benchmark. (arXiv:2309.16511v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Toloka&#35270;&#35273;&#38382;&#31572;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20247;&#21253;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#27604;&#36739;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#31454;&#36187;&#65292;&#21457;&#29616;&#30446;&#21069;&#27809;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#36229;&#36807;&#38750;&#19987;&#23478;&#20247;&#21253;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Toloka&#35270;&#35273;&#38382;&#31572;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20247;&#21253;&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#23545;&#27604;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#32473;&#23450;&#19968;&#24352;&#22270;&#20687;&#21644;&#19968;&#20010;&#25991;&#26412;&#38382;&#39064;&#65292;&#38656;&#35201;&#27491;&#30830;&#22320;&#32472;&#21046;&#20986;&#21253;&#22260;&#35813;&#38382;&#39064;&#22238;&#31572;&#30340;&#23545;&#35937;&#30340;&#36793;&#30028;&#26694;&#12290;&#27599;&#20010;&#22270;&#20687;-&#38382;&#39064;&#23545;&#37117;&#21253;&#21547;&#22238;&#31572;&#65292;&#27599;&#20010;&#22270;&#20687;&#21482;&#26377;&#19968;&#20010;&#27491;&#30830;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;45,199&#20010;&#22270;&#20687;&#21644;&#38382;&#39064;&#30340;&#23545;&#65292;&#20197;&#33521;&#25991;&#25552;&#20379;&#65292;&#24182;&#38468;&#24102;&#26377;&#30495;&#23454;&#30340;&#36793;&#30028;&#26694;&#65292;&#20998;&#20026;&#35757;&#32451;&#21644;&#20004;&#20010;&#27979;&#35797;&#23376;&#38598;&#12290;&#38500;&#20102;&#25551;&#36848;&#25968;&#25454;&#38598;&#24182;&#22312;CC BY&#35768;&#21487;&#19979;&#21457;&#24067;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#24320;&#28304;&#30340;&#38646;&#26679;&#26412;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#24182;&#32452;&#32455;&#20102;&#22312;WSDM Cup&#19978;&#21560;&#24341;&#20102;&#20840;&#29699;48&#20010;&#21442;&#19982;&#32773;&#30340;&#22810;&#38454;&#27573;&#31454;&#36187;&#12290;&#28982;&#32780;&#65292;&#22312;&#25552;&#20132;&#35770;&#25991;&#26102;&#65292;&#26681;&#25454;&#20132;&#21449;&#39564;&#35777;&#27809;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36229;&#36234;&#20102;&#38750;&#19987;&#23478;&#20247;&#21253;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Toloka Visual Question Answering, a new crowdsourced dataset allowing comparing performance of machine learning systems against human level of expertise in the grounding visual question answering task. In this task, given an image and a textual question, one has to draw the bounding box around the object correctly responding to that question. Every image-question pair contains the response, with only one correct response per image. Our dataset contains 45,199 pairs of images and questions in English, provided with ground truth bounding boxes, split into train and two test subsets. Besides describing the dataset and releasing it under a CC BY license, we conducted a series of experiments on open source zero-shot baseline models and organized a multi-phase competition at WSDM Cup that attracted 48 participants worldwide. However, by the time of paper submission, no machine learning model outperformed the non-expert crowdsourcing baseline according to the interse
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#35770;&#25991;&#35752;&#35770;&#20102;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#30693;&#35782;&#28304;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35775;&#38382;&#21644;&#25805;&#20316;&#33021;&#21147;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16459</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#22686;&#24378;LLM&#65306;&#20851;&#20110;&#24187;&#35273;&#39044;&#38450;&#30340;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Augmenting LLMs with Knowledge: A survey on hallucination prevention. (arXiv:2309.16459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#35770;&#25991;&#35752;&#35770;&#20102;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#30693;&#35782;&#28304;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35775;&#38382;&#21644;&#25805;&#20316;&#33021;&#21147;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23384;&#20648;&#20107;&#23454;&#30693;&#35782;&#21644;&#22312;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#32467;&#26524;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#29087;&#32451;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#20934;&#30830;&#35775;&#38382;&#21644;&#25805;&#20316;&#30693;&#35782;&#30340;&#33021;&#21147;&#20173;&#28982;&#21463;&#38480;&#65292;&#23548;&#33268;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;&#26550;&#26500;&#30456;&#27604;&#23384;&#22312;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#27169;&#22411;&#20915;&#31574;&#30340;&#26469;&#28304;&#21644;&#20445;&#25345;&#26368;&#26032;&#19990;&#30028;&#30693;&#35782;&#30340;&#25361;&#25112;&#20173;&#28982;&#26159;&#24320;&#25918;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#21487;&#24494;&#20998;&#35775;&#38382;&#26426;&#21046;&#38598;&#25104;&#21040;&#26174;&#24335;&#30340;&#38750;&#21442;&#25968;&#35760;&#24518;&#20013;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#35843;&#30740;&#25506;&#35752;&#20102;&#22686;&#24378;&#20102;&#19982;&#22806;&#37096;&#30693;&#35782;&#26469;&#28304;&#65288;&#21253;&#25324;&#22806;&#37096;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#65289;&#30456;&#36830;&#25509;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models have demonstrated their proficiency in storing factual knowledge within their parameters and achieving remarkable results when fine-tuned for downstream natural language processing tasks. Nonetheless, their capacity to access and manipulate knowledge with precision remains constrained, resulting in performance disparities on knowledge-intensive tasks when compared to task-specific architectures. Additionally, the challenges of providing provenance for model decisions and maintaining up-to-date world knowledge persist as open research frontiers. To address these limitations, the integration of pre-trained models with differentiable access mechanisms to explicit non-parametric memory emerges as a promising solution. This survey delves into the realm of language models (LMs) augmented with the ability to tap into external knowledge sources, including external knowledge bases and search engines. While adhering to the standard objective of predicting missin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#20132;&#35821;&#22659;&#25299;&#25169;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16424</link><description>&lt;p&gt;
Prompt-and-Align: &#22522;&#20110;&#25552;&#31034;&#30340;&#31038;&#20132;&#35843;&#25972;&#29992;&#20110;&#23569;&#26679;&#26412;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prompt-and-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection. (arXiv:2309.16424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16424
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#20132;&#35821;&#22659;&#25299;&#25169;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#26816;&#27979;&#34394;&#20551;&#26032;&#38395;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#26032;&#38395;&#30340;&#21450;&#26102;&#24615;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#26681;&#25454;&#26377;&#38480;&#30340;&#20107;&#23454;&#26680;&#23545;&#20449;&#24687;&#26469;&#39044;&#27979;&#26032;&#38395;&#25991;&#31456;&#30340;&#30495;&#23454;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36981;&#24490;&#8220;&#20174;&#22836;&#35757;&#32451;&#8221;&#30340;&#33539;&#20363;&#65292;&#36825;&#22312;&#26681;&#26412;&#19978;&#21463;&#21040;&#22823;&#35268;&#27169;&#27880;&#37322;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#12290;&#23613;&#31649;&#24050;&#32463;&#20197;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#30340;&#26041;&#24335;&#36866;&#24212;&#20102;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#20294;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#20063;&#38656;&#35201;&#26114;&#36149;&#30340;&#20219;&#21153;&#29305;&#23450;&#30417;&#30563;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;Prompt-and-Align&#8221;&#65288;P&amp;A&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#26032;&#33539;&#20363;&#65292;&#32852;&#21512;&#21033;&#29992;&#20102;PLM&#20013;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#31038;&#20132;&#35821;&#22659;&#25299;&#25169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#26032;&#38395;&#25991;&#31456;&#21253;&#35065;&#22312;&#19968;&#20010;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25991;&#26412;&#25552;&#31034;&#20013;&#26469;&#32531;&#35299;&#26631;&#31614;&#31232;&#32570;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;PLM&#22788;&#29702;&#26469;&#30452;&#25509;&#24341;&#20986;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite considerable advances in automated fake news detection, due to the timely nature of news, it remains a critical open question how to effectively predict the veracity of news articles based on limited fact-checks. Existing approaches typically follow a "Train-from-Scratch" paradigm, which is fundamentally bounded by the availability of large-scale annotated data. While expressive pre-trained language models (PLMs) have been adapted in a "Pre-Train-and-Fine-Tune" manner, the inconsistency between pre-training and downstream objectives also requires costly task-specific supervision. In this paper, we propose "Prompt-and-Align" (P&amp;A), a novel prompt-based paradigm for few-shot fake news detection that jointly leverages the pre-trained knowledge in PLMs and the social context topology. Our approach mitigates label scarcity by wrapping the news article in a task-related textual prompt, which is then processed by the PLM to directly elicit task-specific knowledge. To supplement the PL
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65288;DocRE&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#19982;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#30456;&#27604;&#65292;DocRE&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20998;&#26512;&#65292;&#28041;&#21450;&#36328;&#36234;&#22810;&#20010;&#21477;&#23376;&#25110;&#27573;&#33853;&#30340;&#20851;&#31995;&#25277;&#21462;&#65292;&#21487;&#29992;&#20110;&#26500;&#24314;&#21644;&#33258;&#21160;&#22635;&#20805;&#30693;&#35782;&#24211;&#12290;</title><link>http://arxiv.org/abs/2309.16396</link><description>&lt;p&gt;
&#23545;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#32508;&#21512;&#35843;&#26597;&#65288;2016-2022&#65289;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Document-level Relation Extraction (2016-2022). (arXiv:2309.16396v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16396
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65288;DocRE&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#19982;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#30456;&#27604;&#65292;DocRE&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20998;&#26512;&#65292;&#28041;&#21450;&#36328;&#36234;&#22810;&#20010;&#21477;&#23376;&#25110;&#27573;&#33853;&#30340;&#20851;&#31995;&#25277;&#21462;&#65292;&#21487;&#29992;&#20110;&#26500;&#24314;&#21644;&#33258;&#21160;&#22635;&#20805;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65288;DocRE&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#19968;&#20010;&#27963;&#36291;&#30740;&#31350;&#39046;&#22495;&#65292;&#28041;&#21450;&#35782;&#21035;&#21644;&#25277;&#21462;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36229;&#36234;&#21477;&#23376;&#36793;&#30028;&#12290;&#19982;&#20256;&#32479;&#30340;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#30456;&#27604;&#65292;DocRE&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20998;&#26512;&#65292;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#35782;&#21035;&#21487;&#33021;&#36328;&#36234;&#22810;&#20010;&#21477;&#23376;&#25110;&#27573;&#33853;&#30340;&#20851;&#31995;&#12290;&#36825;&#20010;&#20219;&#21153;&#22312;&#26500;&#24314;&#21644;&#33258;&#21160;&#22635;&#20805;&#30693;&#35782;&#24211;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20197;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#22823;&#35268;&#27169;&#25991;&#26723;&#65288;&#20363;&#22914;&#31185;&#23398;&#35770;&#25991;&#12289;&#27861;&#24459;&#21512;&#21516;&#25110;&#26032;&#38395;&#25991;&#31456;&#65289;&#20013;&#33258;&#21160;&#33719;&#21462;&#20851;&#31995;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#20171;&#32461;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#31361;&#20986;&#20854;&#19982;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#19981;&#21516;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level relation extraction (DocRE) is an active area of research in natural language processing (NLP) concerned with identifying and extracting relationships between entities beyond sentence boundaries. Compared to the more traditional sentence-level relation extraction, DocRE provides a broader context for analysis and is more challenging because it involves identifying relationships that may span multiple sentences or paragraphs. This task has gained increased interest as a viable solution to build and populate knowledge bases automatically from unstructured large-scale documents (e.g., scientific papers, legal contracts, or news articles), in order to have a better understanding of relationships between entities. This paper aims to provide a comprehensive overview of recent advances in this field, highlighting its different applications in comparison to sentence-level relation extraction.
&lt;/p&gt;</description></item><item><title>Transformer-VQ&#26159;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#23454;&#29616;&#32447;&#24615;&#26102;&#38388;&#30340;Transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#35745;&#31639;&#33258;&#27880;&#24847;&#21147;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.16354</link><description>&lt;p&gt;
Transformer-VQ: &#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#23454;&#29616;&#32447;&#24615;&#26102;&#38388;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Transformer-VQ: Linear-Time Transformers via Vector Quantization. (arXiv:2309.16354v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16354
&lt;/p&gt;
&lt;p&gt;
Transformer-VQ&#26159;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#23454;&#29616;&#32447;&#24615;&#26102;&#38388;&#30340;Transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#35745;&#31639;&#33258;&#27880;&#24847;&#21147;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Transformer-VQ&#65292;&#19968;&#31181;&#20165;&#32534;&#30721;&#22120;&#30340;Transformer&#65292;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#35745;&#31639;&#22522;&#20110;softmax&#30340;&#23494;&#38598;&#33258;&#27880;&#24847;&#21147;&#12290;Transformer-VQ&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#26159;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#38190;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#32531;&#23384;&#26426;&#21046;&#23454;&#29616;&#30340;&#12290;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#65292;Transformer-VQ&#22312;&#36136;&#37327;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;Enwik8(0.99 bpb)&#65292;PG-19(26.6 ppl)&#21644;ImageNet64(3.16 bpb)&#37117;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#23545;&#20154;&#31867;&#21453;&#39304;&#22312;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#20351;&#29992;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20204;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#20851;&#38190;&#38169;&#35823;&#26631;&#20934;&#65292;&#32780;&#19988;&#23481;&#26131;&#21463;&#21040;&#20027;&#35266;&#20559;&#35265;&#21644;&#28151;&#26434;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.16349</link><description>&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#19981;&#26159;&#40644;&#37329;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Human Feedback is not Gold Standard. (arXiv:2309.16349v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16349
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#23545;&#20154;&#31867;&#21453;&#39304;&#22312;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#20351;&#29992;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20204;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#20851;&#38190;&#38169;&#35823;&#26631;&#20934;&#65292;&#32780;&#19988;&#23481;&#26131;&#21463;&#21040;&#20027;&#35266;&#20559;&#35265;&#21644;&#28151;&#26434;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24050;&#25104;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#24182;&#36234;&#26469;&#36234;&#34987;&#29992;&#20316;&#35757;&#32451;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#19981;&#28165;&#26970;&#36825;&#20010;&#21333;&#19968;&#30340;&#8220;&#20559;&#22909;&#8221;&#20998;&#25968;&#25429;&#25417;&#21040;&#29983;&#25104;&#36755;&#20986;&#30340;&#21738;&#20123;&#29305;&#24615;&#12290;&#25105;&#20204;&#20551;&#35774;&#20559;&#22909;&#20998;&#25968;&#26159;&#20027;&#35266;&#30340;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#19981;&#33391;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23545;&#20154;&#31867;&#21453;&#39304;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#20013;&#30340;&#20351;&#29992;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#20197;&#39564;&#35777;&#23427;&#26159;&#21542;&#23436;&#20840;&#25429;&#25417;&#21040;&#19968;&#31995;&#21015;&#20851;&#38190;&#38169;&#35823;&#26631;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#20559;&#22909;&#20998;&#25968;&#30340;&#35206;&#30422;&#33539;&#22260;&#30456;&#24403;&#22909;&#65292;&#20294;&#23427;&#20204;&#22312;&#20107;&#23454;&#24615;&#31561;&#37325;&#35201;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20551;&#35774;&#20559;&#22909;&#20998;&#25968;&#21644;&#38169;&#35823;&#27880;&#37322;&#21487;&#33021;&#21463;&#21040;&#28151;&#26434;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#24182;&#21033;&#29992;&#35843;&#35797;&#27169;&#22411;&#29983;&#25104;&#22312;&#20004;&#20010;&#21487;&#33021;&#30340;&#28151;&#26434;&#32500;&#24230;&#19978;&#21464;&#21270;&#30340;&#36755;&#20986;&#65306;&#22362;&#23450;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36755;&#20986;&#30340;&#22362;&#23450;&#24615;&#20250;&#20351;&#20107;&#23454;&#38169;&#35823;&#30340;&#24863;&#30693;&#29575;&#20559;&#24046;&#65292;&#34920;&#26126;&#20154;&#31867;&#27880;&#37322;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human feedback has become the de facto standard for evaluating the performance of Large Language Models, and is increasingly being used as a training objective. However, it is not clear which properties of a generated output this single `preference' score captures. We hypothesise that preference scores are subjective and open to undesirable biases. We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. We further hypothesise that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#24341;&#23548;&#25506;&#32034;&#65288;IGE-LLMs&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#20316;&#20026;&#36741;&#21161;&#20869;&#22312;&#22870;&#21169;&#65292;&#35299;&#20915;&#20102;&#22797;&#26434;&#38271;&#35270;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#36739;&#39640;&#30340;&#24615;&#33021;&#21644;&#27169;&#22359;&#21270;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16347</link><description>&lt;p&gt;
&#22797;&#26434;&#38271;&#35270;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#20869;&#22312;&#35821;&#35328;&#24341;&#23548;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks. (arXiv:2309.16347v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#24341;&#23548;&#25506;&#32034;&#65288;IGE-LLMs&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#20316;&#20026;&#36741;&#21161;&#20869;&#22312;&#22870;&#21169;&#65292;&#35299;&#20915;&#20102;&#22797;&#26434;&#38271;&#35270;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#36739;&#39640;&#30340;&#24615;&#33021;&#21644;&#27169;&#22359;&#21270;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#31232;&#30095;&#21644;&#22797;&#26434;&#30340;&#29615;&#22659;&#20013;&#38754;&#20020;&#22256;&#22659;&#65292;&#23588;&#20854;&#26159;&#22312;&#28041;&#21450;&#20247;&#22810;&#19981;&#21516;&#24207;&#21015;&#30340;&#38271;&#35270;&#31243;&#25805;&#20316;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#24341;&#23548;&#25506;&#32034;&#65288;IGE-LLMs&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#20316;&#20026;&#36741;&#21161;&#20869;&#22312;&#22870;&#21169;&#65292;IGE-LLMs&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#38271;&#35270;&#31243;&#25805;&#20316;&#20219;&#21153;&#20013;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20855;&#26377;&#25506;&#32034;&#25361;&#25112;&#30340;&#29615;&#22659;&#21644;&#19968;&#20010;&#21516;&#26102;&#38754;&#20020;&#25506;&#32034;&#21644;&#38271;&#35270;&#31243;&#25361;&#25112;&#30340;&#22797;&#26434;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#30456;&#20851;&#30340;&#20869;&#22312;&#23398;&#20064;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;IGE-LLMs(i)&#22312;&#30456;&#20851;&#30340;&#20869;&#22312;&#26041;&#27861;&#21644;&#30452;&#25509;&#20351;&#29992;LLMs&#36827;&#34892;&#20915;&#31574;&#30340;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#36739;&#39640;&#27700;&#24179;&#65292;(ii)&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#21644;&#20114;&#34917;&#65292;&#31361;&#20986;&#20854;&#27169;&#22359;&#21270;&#24615;&#33021;&#65292;(iii)&#23545;&#20110;&#19981;&#21516;&#30340;&#20869;&#22312;&#32553;&#25918;&#21442;&#25968;&#27604;&#36739;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current reinforcement learning algorithms struggle in sparse and complex environments, most notably in long-horizon manipulation tasks entailing a plethora of different sequences. In this work, we propose the Intrinsically Guided Exploration from Large Language Models (IGE-LLMs) framework. By leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the exploratory process in reinforcement learning to address intricate long-horizon with sparse rewards robotic manipulation tasks. We evaluate our framework and related intrinsic learning methods in an environment challenged with exploration, and a complex robotic manipulation task challenged by both exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher performance over related intrinsic methods and the direct use of LLMs in decision-making, (ii) can be combined and complement existing learning methods highlighting its modularity, (iii) are fairly insensitive to different intrinsic scaling parameters, and 
&lt;/p&gt;</description></item><item><title>ReCAT&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;Transformer&#27169;&#22411;&#65292;&#20351;&#29992;&#36882;&#24402;&#32452;&#21512;&#21644;&#19978;&#19979;&#25991;&#20869;&#22806;&#23618;&#33021;&#22815;&#27169;&#25311;&#25991;&#26412;&#30340;&#23618;&#32423;&#21477;&#27861;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19982;&#20854;&#20182;&#36328;&#24230;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22810;&#31890;&#24230;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.16319</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#24402;&#32452;&#21512;&#22810;&#31890;&#24230;&#34920;&#31034;&#30340;Transformer&#22686;&#24378;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Augmenting transformers with recursively composed multi-grained representations. (arXiv:2309.16319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16319
&lt;/p&gt;
&lt;p&gt;
ReCAT&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;Transformer&#27169;&#22411;&#65292;&#20351;&#29992;&#36882;&#24402;&#32452;&#21512;&#21644;&#19978;&#19979;&#25991;&#20869;&#22806;&#23618;&#33021;&#22815;&#27169;&#25311;&#25991;&#26412;&#30340;&#23618;&#32423;&#21477;&#27861;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19982;&#20854;&#20182;&#36328;&#24230;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22810;&#31890;&#24230;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReCAT&#30340;&#36882;&#24402;&#32452;&#21512;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#22312;&#23398;&#20064;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#26126;&#30830;&#24314;&#27169;&#21407;&#22987;&#25991;&#26412;&#30340;&#23618;&#32423;&#21477;&#27861;&#32467;&#26500;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#12290;&#29616;&#26377;&#30740;&#31350;&#38480;&#21046;&#25968;&#25454;&#36981;&#24490;&#23618;&#32423;&#26641;&#32467;&#26500;&#65292;&#22240;&#27492;&#32570;&#20047;&#36328;&#36317;&#36890;&#20449;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#20869;&#22806;(CIO)&#23618;&#65292;&#36890;&#36807;&#33258;&#24213;&#21521;&#19978;&#21644;&#33258;&#39030;&#21521;&#19979;&#30340;&#20256;&#36882;&#23398;&#20064;&#36328;&#24230;&#30340;&#19978;&#19979;&#25991;&#21270;&#34920;&#31034;&#65292;&#20854;&#20013;&#33258;&#24213;&#21521;&#19978;&#20256;&#36882;&#36890;&#36807;&#32452;&#21512;&#20302;&#32423;&#36328;&#24230;&#24418;&#25104;&#39640;&#32423;&#36328;&#24230;&#30340;&#34920;&#31034;&#65292;&#32780;&#33258;&#39030;&#21521;&#19979;&#20256;&#36882;&#21017;&#32467;&#21512;&#20102;&#36328;&#24230;&#20869;&#37096;&#21644;&#22806;&#37096;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;&#23884;&#20837;&#23618;&#21644;&#27880;&#24847;&#21147;&#23618;&#20043;&#38388;&#21472;&#21152;&#22810;&#20010;CIO&#23618;&#65292;ReCAT&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#36328;&#36317;&#20869;&#37096;&#21644;&#36328;&#36317;&#38388;&#30340;&#28145;&#23618;&#20132;&#20114;&#65292;&#20174;&#32780;&#29983;&#25104;&#19982;&#20854;&#20182;&#36328;&#24230;&#23436;&#20840;&#19978;&#19979;&#25991;&#21270;&#30340;&#22810;&#31890;&#24230;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;CIO&#23618;&#21487;&#20197;&#36827;&#34892;&#32852;&#21512;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ReCAT, a recursive composition augmented Transformer that is able to explicitly model hierarchical syntactic structures of raw texts without relying on gold trees during both learning and inference. Existing research along this line restricts data to follow a hierarchical tree structure and thus lacks inter-span communications. To overcome the problem, we propose a novel contextual inside-outside (CIO) layer that learns contextualized representations of spans through bottom-up and top-down passes, where a bottom-up pass forms representations of high-level spans by composing low-level spans, while a top-down pass combines information inside and outside a span. By stacking several CIO layers between the embedding layer and the attention layers in Transformer, the ReCAT model can perform both deep intra-span and deep inter-span interactions, and thus generate multi-grained representations fully contextualized with other spans. Moreover, the CIO layers can be jointly pre-trained
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20195;&#30721;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#20195;&#30721;&#21644;&#25991;&#26412;&#28151;&#21512;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;LLMs&#30340;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#65292;&#20195;&#30721;&#25968;&#25454;&#36171;&#20104;LLMs&#29305;&#23450;&#20219;&#21153;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16298</link><description>&lt;p&gt;
&#22312;&#21738;&#20010;&#35757;&#32451;&#38454;&#27573;&#65292;&#20195;&#30721;&#25968;&#25454;&#20250;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
At Which Training Stage Does Cocde Data Help LLMs Reasoning?. (arXiv:2309.16298v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20195;&#30721;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#20195;&#30721;&#21644;&#25991;&#26412;&#28151;&#21512;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;LLMs&#30340;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#65292;&#20195;&#30721;&#25968;&#25454;&#36171;&#20104;LLMs&#29305;&#23450;&#20219;&#21153;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25104;&#20026;&#35821;&#35328;&#25216;&#26415;&#30340;&#22522;&#30784;&#12290;&#21463;&#21040;&#20195;&#30721;&#25968;&#25454;&#22312;&#35757;&#32451;LLMs&#20013;&#30340;&#24040;&#22823;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#33258;&#28982;&#32780;&#28982;&#22320;&#24819;&#30693;&#36947;&#22312;&#21738;&#20010;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;&#20195;&#30721;&#25968;&#25454;&#30495;&#27491;&#21487;&#20197;&#24110;&#21161;LLMs&#36827;&#34892;&#25512;&#29702;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#20195;&#30721;&#25968;&#25454;&#23545;LLMs&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#21035;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#12289;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#20197;&#21450;&#20004;&#32773;&#20043;&#38388;&#24341;&#20837;&#20195;&#30721;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20116;&#20010;&#39046;&#22495;&#20013;&#30340;&#20845;&#20010;&#25512;&#29702;&#20219;&#21153;&#20840;&#38754;&#20844;&#27491;&#22320;&#35780;&#20272;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#20851;&#38190;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#28145;&#24230;&#27934;&#23519;&#21147;&#30340;&#32467;&#35770;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#20195;&#30721;&#21644;&#25991;&#26412;&#28151;&#21512;&#39044;&#35757;&#32451;LLMs&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;LLMs&#30340;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#65292;&#20960;&#20046;&#19981;&#23545;&#20854;&#20182;&#20219;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#22312;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#65292;&#20195;&#30721;&#25968;&#25454;&#36171;&#20104;LLMs&#29305;&#23450;&#20219;&#21153;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable reasoning capabilities and become the foundation of language technologies. Inspired by the great success of code data in training LLMs, we naturally wonder at which training stage introducing code data can really help LLMs reasoning. To this end, this paper systematically explores the impact of code data on LLMs at different stages. Concretely, we introduce the code data at the pre-training stage, instruction-tuning stage, and both of them, respectively. Then, the reasoning capability of LLMs is comprehensively and fairly evaluated via six reasoning tasks in five domains. We critically analyze the experimental results and provide conclusions with insights. First, pre-training LLMs with the mixture of code and text can significantly enhance LLMs' general reasoning capability almost without negative transfer on other tasks. Besides, at the instruction-tuning stage, code data endows LLMs the task-specific reasoning capability. Moreove
&lt;/p&gt;</description></item><item><title>DiLu&#26159;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65292;&#37319;&#29992;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#29702;&#21644;&#21453;&#24605;&#27169;&#22359;&#36827;&#34892;&#20915;&#31574;&#65292;&#31215;&#32047;&#32463;&#39564;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16292</link><description>&lt;p&gt;
DiLu: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#30340;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models. (arXiv:2309.16292v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16292
&lt;/p&gt;
&lt;p&gt;
DiLu&#26159;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65292;&#37319;&#29992;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#29702;&#21644;&#21453;&#24605;&#27169;&#22359;&#36827;&#34892;&#20915;&#31574;&#65292;&#31215;&#32047;&#32463;&#39564;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#26368;&#36817;&#30340;&#36827;&#23637;&#20381;&#36182;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#34429;&#28982;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#38754;&#20020;&#25968;&#25454;&#38598;&#20559;&#35265;&#12289;&#36807;&#25311;&#21512;&#21644;&#19981;&#21487;&#35299;&#37322;&#24615;&#31561;&#25361;&#25112;&#12290;&#21463;&#20154;&#31867;&#39550;&#39542;&#30693;&#35782;&#39537;&#21160;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#22914;&#20309;&#23558;&#31867;&#20284;&#30340;&#33021;&#21147;&#27880;&#20837;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20114;&#21160;&#29615;&#22659;&#12289;&#39550;&#39542;&#21592;&#20195;&#29702;&#21644;&#35760;&#24518;&#32452;&#20214;&#30340;&#33539;&#20363;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#26032;&#20852;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiLu&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#25512;&#29702;&#27169;&#22359;&#21644;&#21453;&#24605;&#27169;&#22359;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#20381;&#25454;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#20915;&#31574;&#65292;&#24182;&#25345;&#32493;&#28436;&#21270;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;DiLu&#33021;&#22815;&#31215;&#32047;&#32463;&#39564;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#27604;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;DiLu&#33021;&#22815;&#30452;&#25509;&#20174;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets w
&lt;/p&gt;</description></item><item><title>LawBench&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27861;&#24459;&#30693;&#35782;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#20174;&#35760;&#24518;&#65292;&#29702;&#35299;&#21644;&#24212;&#29992;&#19977;&#20010;&#23618;&#38754;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#27861;&#24459;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16289</link><description>&lt;p&gt;
LawBench: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27861;&#24459;&#30693;&#35782;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LawBench: Benchmarking Legal Knowledge of Large Language Models. (arXiv:2309.16289v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16289
&lt;/p&gt;
&lt;p&gt;
LawBench&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27861;&#24459;&#30693;&#35782;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#20174;&#35760;&#24518;&#65292;&#29702;&#35299;&#21644;&#24212;&#29992;&#19977;&#20010;&#23618;&#38754;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#27861;&#24459;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#39640;&#24230;&#19987;&#19994;&#21270;&#12289;&#23433;&#20840;&#20851;&#38190;&#30340;&#27861;&#24459;&#39046;&#22495;&#26102;&#65292;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#25152;&#20855;&#22791;&#30340;&#27861;&#24459;&#30693;&#35782;&#37327;&#20197;&#21450;&#23427;&#20204;&#26159;&#21542;&#33021;&#21487;&#38752;&#22320;&#25191;&#34892;&#19982;&#27861;&#24459;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#35780;&#20272;&#22522;&#20934;LawBench&#12290;LawBench&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#20174;&#19977;&#20010;&#35748;&#30693;&#23618;&#38754;&#23545;LLMs&#30340;&#27861;&#24459;&#33021;&#21147;&#36827;&#34892;&#31934;&#30830;&#35780;&#20272;&#65306;&#65288;1&#65289;&#27861;&#24459;&#30693;&#35782;&#35760;&#24518;&#65306;LLMs&#26159;&#21542;&#33021;&#22815;&#35760;&#20303;&#25152;&#38656;&#30340;&#27861;&#24459;&#27010;&#24565;&#12289;&#26465;&#27454;&#21644;&#20107;&#23454;&#65307;&#65288;2&#65289;&#27861;&#24459;&#30693;&#35782;&#29702;&#35299;&#65306;LLMs&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#20013;&#30340;&#23454;&#20307;&#12289;&#20107;&#20214;&#21644;&#20851;&#31995;&#65307;&#65288;3&#65289;&#27861;&#24459;&#30693;&#35782;&#24212;&#29992;&#65306;LLMs&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#36816;&#29992;&#33258;&#24049;&#30340;&#27861;&#24459;&#30693;&#35782;&#24182;&#36827;&#34892;&#24517;&#35201;&#30340;&#25512;&#29702;&#27493;&#39588;&#26469;&#35299;&#20915;&#29616;&#23454;&#30340;&#27861;&#24459;&#20219;&#21153;&#12290;LawBench&#21253;&#21547;20&#20010;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;5&#31181;&#20219;&#21153;&#31867;&#22411;&#65306;&#21333;&#26631;&#31614;&#20998;&#31867;&#65288;SLC&#65289;&#12289;&#22810;&#26631;&#31614;&#20998;&#31867;&#65288;MLC&#65289;&#12289;&#22238;&#24402;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated strong capabilities in various aspects. However, when applying them to the highly specialized, safe-critical legal domain, it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks. To address this gap, we propose a comprehensive evaluation benchmark LawBench. LawBench has been meticulously crafted to have precise assessment of the LLMs' legal capabilities from three cognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize needed legal concepts, articles and facts; (2) Legal knowledge understanding: whether LLMs can comprehend entities, events and relationships within legal text; (3) Legal knowledge applying: whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. LawBench contains 20 diverse tasks covering 5 task types: single-label classification (SLC), multi-label classification (MLC), regression, e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#36328;&#35270;&#22270;&#34920;&#31034;&#37325;&#24314;&#32593;&#32476;&#65288;SCORER&#65289;&#26469;&#35299;&#20915;&#21464;&#21270;&#25551;&#36848;&#20013;&#30340;&#24046;&#24322;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#22810;&#22836;&#36880;&#26631;&#35760;&#21305;&#37197;&#21644;&#36328;&#35270;&#22270;&#23545;&#27604;&#23545;&#40784;&#26469;&#23398;&#20064;&#31283;&#23450;&#30340;&#24046;&#24322;&#34920;&#31034;&#65292;&#36827;&#32780;&#29983;&#25104;&#26356;&#22909;&#30340;&#26631;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16283</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#36328;&#35270;&#22270;&#34920;&#31034;&#37325;&#24314;&#29992;&#20110;&#21464;&#21270;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Cross-view Representation Reconstruction for Change Captioning. (arXiv:2309.16283v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#36328;&#35270;&#22270;&#34920;&#31034;&#37325;&#24314;&#32593;&#32476;&#65288;SCORER&#65289;&#26469;&#35299;&#20915;&#21464;&#21270;&#25551;&#36848;&#20013;&#30340;&#24046;&#24322;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#22810;&#22836;&#36880;&#26631;&#35760;&#21305;&#37197;&#21644;&#36328;&#35270;&#22270;&#23545;&#27604;&#23545;&#40784;&#26469;&#23398;&#20064;&#31283;&#23450;&#30340;&#24046;&#24322;&#34920;&#31034;&#65292;&#36827;&#32780;&#29983;&#25104;&#26356;&#22909;&#30340;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21270;&#25551;&#36848;&#26088;&#22312;&#25551;&#36848;&#19968;&#23545;&#30456;&#20284;&#22270;&#20687;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20854;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#22312;&#30001;&#35270;&#35282;&#21464;&#21270;&#24341;&#36215;&#30340;&#20266;&#21464;&#21270;&#19979;&#23398;&#20064;&#31283;&#23450;&#30340;&#24046;&#24322;&#34920;&#31034;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#36328;&#35270;&#22270;&#34920;&#31034;&#37325;&#24314;&#65288;SCORER&#65289;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#22836;&#36880;&#26631;&#35760;&#21305;&#37197;&#26469;&#24314;&#27169;&#30456;&#20284;/&#19981;&#30456;&#20284;&#22270;&#20687;&#20043;&#38388;&#30340;&#36328;&#35270;&#22270;&#29305;&#24449;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20004;&#20010;&#30456;&#20284;&#22270;&#20687;&#30340;&#36328;&#35270;&#22270;&#23545;&#27604;&#23545;&#40784;&#65292;SCORER&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#20102;&#20004;&#20010;&#35270;&#22270;&#19981;&#21464;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#22522;&#20110;&#36825;&#20123;&#34920;&#31034;&#65292;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#37325;&#24314;&#26410;&#25913;&#21464;&#23545;&#35937;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#23398;&#20064;&#21040;&#31283;&#23450;&#30340;&#24046;&#24322;&#34920;&#31034;&#29992;&#20110;&#29983;&#25104;&#26631;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#30340;&#21521;&#21518;&#25512;&#29702;&#26469;&#25913;&#36827;&#26631;&#39064;&#30340;&#36136;&#37327;&#12290;&#35813;&#27169;&#22359;&#36890;&#36807;&#26631;&#39064;&#21644;&#8220;before&#8221;&#34920;&#31034;&#21453;&#21521;&#24314;&#27169;&#19968;&#20010;&#8220;&#24187;&#35273;&#8221;&#34920;&#31034;&#12290;&#36890;&#36807;&#25512;&#21160;&#21644;&#35843;&#25972;&#36825;&#20010;&#24187;&#35273;&#34920;&#31034;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26631;&#39064;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Change captioning aims to describe the difference between a pair of similar images. Its key challenge is how to learn a stable difference representation under pseudo changes caused by viewpoint change. In this paper, we address this by proposing a self-supervised cross-view representation reconstruction (SCORER) network. Concretely, we first design a multi-head token-wise matching to model relationships between cross-view features from similar/dissimilar images. Then, by maximizing cross-view contrastive alignment of two similar images, SCORER learns two view-invariant image representations in a self-supervised way. Based on these, we reconstruct the representations of unchanged objects by cross-attention, thus learning a stable difference representation for caption generation. Further, we devise a cross-modal backward reasoning to improve the quality of caption. This module reversely models a ``hallucination'' representation with the caption and ``before'' representation. By pushing i
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#21477;&#23376;Transformer&#27169;&#22411;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#22312;&#38452;&#35851;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25104;&#32489;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.16275</link><description>&lt;p&gt;
UPB @ ACTI: &#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#21477;&#23376;Transformer&#26469;&#26816;&#27979;&#38452;&#35851;&#35770;
&lt;/p&gt;
&lt;p&gt;
UPB @ ACTI: Detecting Conspiracies using fine tuned Sentence Transformers. (arXiv:2309.16275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16275
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#21477;&#23376;Transformer&#27169;&#22411;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#22312;&#38452;&#35851;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25104;&#32489;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38452;&#35851;&#35770;&#24050;&#25104;&#20026;&#22312;&#32447;&#35752;&#35770;&#30340;&#37325;&#35201;&#21644;&#20196;&#20154;&#25285;&#24551;&#30340;&#26041;&#38754;&#65292;&#23545;&#20449;&#24687;&#23436;&#25972;&#24615;&#21644;&#31038;&#20250;&#20449;&#20219;&#26500;&#25104;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#38452;&#35851;&#35770;&#26816;&#27979;&#20316;&#20026;ACTI @ EVALITA 2023&#20849;&#20139;&#20219;&#21153;&#30340;&#25552;&#35758;&#12290;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;Transformer&#27169;&#22411;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#32452;&#21512;&#20351;&#25105;&#20204;&#22312;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#26368;&#32456;&#25490;&#34892;&#27036;&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#33719;&#24471;&#20102;85.71%&#30340;F1&#20998;&#25968;&#65292;&#22312;&#31934;&#32454;&#38452;&#35851;&#20027;&#39064;&#20998;&#31867;&#20013;&#33719;&#24471;&#20102;91.23%&#30340;F1&#20998;&#25968;&#65292;&#36229;&#36807;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conspiracy theories have become a prominent and concerning aspect of online discourse, posing challenges to information integrity and societal trust. As such, we address conspiracy theory detection as proposed by the ACTI @ EVALITA 2023 shared task. The combination of pre-trained sentence Transformer models and data augmentation techniques enabled us to secure first place in the final leaderboard of both sub-tasks. Our methodology attained F1 scores of 85.71% in the binary classification and 91.23% for the fine-grained conspiracy topic classification, surpassing other competing systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#26102;&#23578;&#30693;&#35782;&#25552;&#21462;&#30340;&#20219;&#21153;&#65292;&#22312;&#32771;&#34385;&#20102;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25991;&#26412;&#20449;&#24687;&#30340;&#22522;&#30784;&#19978;&#65292;&#23558;&#35813;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#26631;&#39064;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#25429;&#25417;&#22810;&#27169;&#24335;&#24086;&#23376;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.16270</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#26102;&#23578;&#30693;&#35782;&#25552;&#21462;&#20316;&#20026;&#26631;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Social Media Fashion Knowledge Extraction as Captioning. (arXiv:2309.16270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#26102;&#23578;&#30693;&#35782;&#25552;&#21462;&#30340;&#20219;&#21153;&#65292;&#22312;&#32771;&#34385;&#20102;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25991;&#26412;&#20449;&#24687;&#30340;&#22522;&#30784;&#19978;&#65292;&#23558;&#35813;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#26631;&#39064;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#25429;&#25417;&#22810;&#27169;&#24335;&#24086;&#23376;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#22312;&#25512;&#21160;&#26102;&#23578;&#34892;&#19994;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#27599;&#22825;&#37117;&#20250;&#20135;&#29983;&#22823;&#37327;&#19982;&#26102;&#23578;&#30456;&#20851;&#30340;&#24086;&#23376;&#12290;&#20026;&#20102;&#33719;&#21462;&#36825;&#20123;&#24086;&#23376;&#20013;&#20016;&#23500;&#30340;&#26102;&#23578;&#20449;&#24687;&#65292;&#25105;&#20204;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#26102;&#23578;&#30693;&#35782;&#25552;&#21462;&#30340;&#20219;&#21153;&#12290;&#26102;&#23578;&#30693;&#35782;&#36890;&#24120;&#21253;&#25324;&#22330;&#21512;&#12289;&#20154;&#29289;&#23646;&#24615;&#21644;&#26102;&#23578;&#29289;&#21697;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#20026;&#19968;&#32452;&#20803;&#32452;&#12290;&#20808;&#21069;&#30340;&#26102;&#23578;&#30693;&#35782;&#25552;&#21462;&#30740;&#31350;&#22823;&#22810;&#22522;&#20110;&#26102;&#23578;&#20135;&#21697;&#22270;&#20687;&#65292;&#32780;&#24573;&#30053;&#20102;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#31038;&#20132;&#23186;&#20307;&#26102;&#23578;&#30693;&#35782;&#25552;&#21462;&#24037;&#20316;&#26159;&#22522;&#20110;&#20998;&#31867;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#20107;&#20808;&#25163;&#21160;&#30830;&#23450;&#19968;&#32452;&#26102;&#23578;&#30693;&#35782;&#31867;&#21035;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#35813;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#26631;&#39064;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#25429;&#25417;&#22810;&#27169;&#24335;&#24086;&#23376;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#26102;&#23578;&#30693;&#35782;&#20803;&#32452;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#26631;&#39064;&#65292;&#24418;&#25104;&#19968;&#20010;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media plays a significant role in boosting the fashion industry, where a massive amount of fashion-related posts are generated every day. In order to obtain the rich fashion information from the posts, we study the task of social media fashion knowledge extraction. Fashion knowledge, which typically consists of the occasion, person attributes, and fashion item information, can be effectively represented as a set of tuples. Most previous studies on fashion knowledge extraction are based on the fashion product images without considering the rich text information in social media posts. Existing work on fashion knowledge extraction in social media is classification-based and requires to manually determine a set of fashion knowledge categories in advance. In our work, we propose to cast the task as a captioning problem to capture the interplay of the multimodal post information. Specifically, we transform the fashion knowledge tuples into a natural language caption with a sentence tr
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29616;&#20195;&#26550;&#26500;&#30340;&#23436;&#20840;&#22686;&#37327;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#36739;&#21452;&#21521;&#20998;&#26512;&#25928;&#26524;&#33853;&#21518;&#65292;&#23637;&#31034;&#20102;&#24515;&#29702;&#35821;&#35328;&#23398;&#21512;&#29702;&#35299;&#26512;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.16254</link><description>&lt;p&gt;
&#20851;&#20110;&#23436;&#20840;&#22686;&#37327;&#31070;&#32463;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
On the Challenges of Fully Incremental Neural Dependency Parsing. (arXiv:2309.16254v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16254
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29616;&#20195;&#26550;&#26500;&#30340;&#23436;&#20840;&#22686;&#37327;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#36739;&#21452;&#21521;&#20998;&#26512;&#25928;&#26524;&#33853;&#21518;&#65292;&#23637;&#31034;&#20102;&#24515;&#29702;&#35821;&#35328;&#23398;&#21512;&#29702;&#35299;&#26512;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;BiLSTMs&#21644;&#22522;&#20110;Transformer&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#30340;&#26222;&#21450;&#20197;&#26469;&#65292;&#26368;&#20808;&#36827;&#30340;&#21477;&#27861;&#20998;&#26512;&#22120;&#32570;&#20047;&#22686;&#37327;&#24615;&#65292;&#38656;&#35201;&#35775;&#38382;&#25972;&#20010;&#21477;&#23376;&#65292;&#24182;&#19988;&#20559;&#31163;&#20102;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#29616;&#20195;&#26550;&#26500;&#36827;&#34892;&#23436;&#20840;&#22686;&#37327;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#26159;&#21542;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#23558;&#20005;&#26684;&#30340;&#20174;&#24038;&#21040;&#21491;&#31070;&#32463;&#32534;&#30721;&#22120;&#19982;&#23436;&#20840;&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#21644;&#36716;&#25442;&#20026;&#22522;&#30784;&#30340;&#35299;&#30721;&#22120;&#30456;&#32467;&#21512;&#30340;&#20998;&#26512;&#22120;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#29616;&#20195;&#26550;&#26500;&#30340;&#23436;&#20840;&#22686;&#37327;&#20998;&#26512;&#22312;&#25928;&#26524;&#19978;&#36828;&#36828;&#33853;&#21518;&#20110;&#21452;&#21521;&#20998;&#26512;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#24515;&#29702;&#35821;&#35328;&#23398;&#21512;&#29702;&#35299;&#26512;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the popularization of BiLSTMs and Transformer-based bidirectional encoders, state-of-the-art syntactic parsers have lacked incrementality, requiring access to the whole sentence and deviating from human language processing. This paper explores whether fully incremental dependency parsing with modern architectures can be competitive. We build parsers combining strictly left-to-right neural encoders with fully incremental sequence-labeling and transition-based decoders. The results show that fully incremental parsing with modern architectures considerably lags behind bidirectional parsing, noting the challenges of psycholinguistically plausible parsing.
&lt;/p&gt;</description></item><item><title>Spider4SPARQL&#26159;&#19968;&#20010;&#26032;&#30340;SPARQL&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#22823;&#37327;&#25163;&#24037;&#29983;&#25104;&#30340;NL&#38382;&#39064;&#21644;&#29420;&#29305;&#12289;&#26032;&#39062;&#12289;&#22797;&#26434;&#30340;SPARQL&#26597;&#35810;&#65292;&#26088;&#22312;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.16248</link><description>&lt;p&gt;
Spider4SPARQL&#65306;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#31995;&#32479;&#30340;&#22797;&#26434;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph Question Answering Systems. (arXiv:2309.16248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16248
&lt;/p&gt;
&lt;p&gt;
Spider4SPARQL&#26159;&#19968;&#20010;&#26032;&#30340;SPARQL&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#22823;&#37327;&#25163;&#24037;&#29983;&#25104;&#30340;NL&#38382;&#39064;&#21644;&#29420;&#29305;&#12289;&#26032;&#39062;&#12289;&#22797;&#26434;&#30340;SPARQL&#26597;&#35810;&#65292;&#26088;&#22312;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25968;&#37327;&#21644;&#21487;&#29992;&#24615;&#30340;&#22686;&#21152;&#65292;&#25552;&#20379;&#22823;&#22411;&#21644;&#30495;&#23454;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;KBQA&#65289;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#22522;&#20934;&#20381;&#36182;&#22522;&#20110;&#27169;&#24335;&#30340;SPARQL&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#12290;&#38543;&#21518;&#30340;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#38382;&#39064;&#29983;&#25104;&#36890;&#36807;&#20247;&#21253;&#25110;&#20854;&#20182;&#33258;&#21160;&#21270;&#26041;&#27861;&#36827;&#34892;&#65292;&#22914;&#22522;&#20110;&#35268;&#21017;&#30340;&#25913;&#20889;&#25110;NL&#38382;&#39064;&#27169;&#26495;&#12290;&#34429;&#28982;&#20854;&#20013;&#19968;&#20123;&#25968;&#25454;&#38598;&#35268;&#27169;&#30456;&#24403;&#22823;&#65292;&#20294;&#23427;&#20204;&#30340;&#32570;&#28857;&#22312;&#20110;&#22522;&#20110;&#27169;&#24335;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#19981;&#24635;&#26159;&#33021;&#22815;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#20154;&#20204;&#25552;&#20986;&#30340;&#27169;&#31946;&#19988;&#35821;&#35328;&#22810;&#26679;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Spider4SPARQL - &#19968;&#20010;&#26032;&#30340;SPARQL&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;9,693&#20010;&#20808;&#21069;&#23384;&#22312;&#30340;&#25163;&#24037;&#29983;&#25104;&#30340;NL&#38382;&#39064;&#21644;4,721&#20010;&#21807;&#19968;&#12289;&#26032;&#39062;&#19988;&#22797;&#26434;&#30340;SPARQL&#26597;&#35810;&#65292;&#22797;&#26434;&#24615;&#21508;&#19981;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent spike in the number and availability of Large Language Models (LLMs), it has become increasingly important to provide large and realistic benchmarks for evaluating Knowledge Graph Question Answering (KBQA) systems. So far the majority of benchmarks rely on pattern-based SPARQL query generation approaches. The subsequent natural language (NL) question generation is conducted through crowdsourcing or other automated methods, such as rule-based paraphrasing or NL question templates. Although some of these datasets are of considerable size, their pitfall lies in their pattern-based generation approaches, which do not always generalize well to the vague and linguistically diverse questions asked by humans in real-world contexts.  In this paper, we introduce Spider4SPARQL - a new SPARQL benchmark dataset featuring 9,693 previously existing manually generated NL questions and 4,721 unique, novel, and complex SPARQL queries of varying complexity. In addition to the NL/SPARQL pa
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#23376;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#20026;&#21152;&#36895;&#33647;&#29289;&#21457;&#29616;&#21644;&#20998;&#23376;&#35774;&#35745;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20174;&#22836;&#35774;&#35745;&#33647;&#29289;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#21453;&#24212;&#21270;&#23398;&#12290;&#21516;&#26102;&#65292;&#24320;&#28304;&#36719;&#20214;&#36164;&#28304;&#38477;&#20302;&#20102;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#30340;&#38376;&#27099;&#65292;&#26410;&#26469;&#23558;&#32467;&#21512;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#35745;&#31639;&#21270;&#23398;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.16235</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#23376;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Language models in molecular discovery. (arXiv:2309.16235v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16235
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#23376;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#20026;&#21152;&#36895;&#33647;&#29289;&#21457;&#29616;&#21644;&#20998;&#23376;&#35774;&#35745;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20174;&#22836;&#35774;&#35745;&#33647;&#29289;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#21453;&#24212;&#21270;&#23398;&#12290;&#21516;&#26102;&#65292;&#24320;&#28304;&#36719;&#20214;&#36164;&#28304;&#38477;&#20302;&#20102;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#30340;&#38376;&#27099;&#65292;&#26410;&#26469;&#23558;&#32467;&#21512;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#35745;&#31639;&#21270;&#23398;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24050;&#32463;&#28183;&#36879;&#21040;&#20854;&#20182;&#39046;&#22495;&#65292;&#20986;&#29616;&#20102;&#22312;&#23567;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#25110;&#32858;&#21512;&#29289;&#19978;&#36816;&#20316;&#30340;&#8220;&#31185;&#23398;&#35821;&#35328;&#27169;&#22411;&#8221;&#12290;&#22312;&#21270;&#23398;&#39046;&#22495;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#21152;&#36895;&#20998;&#23376;&#21457;&#29616;&#21608;&#26399;&#26041;&#38754;&#21457;&#25381;&#20102;&#20316;&#29992;&#65292;&#26368;&#36817;&#22312;&#26089;&#26399;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#26377;&#20102;&#26377;&#24076;&#26395;&#30340;&#21457;&#29616;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#23376;&#21457;&#29616;&#20013;&#30340;&#35282;&#33394;&#65292;&#24378;&#35843;&#20102;&#20174;&#22836;&#35774;&#35745;&#33647;&#29289;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#21453;&#24212;&#21270;&#23398;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#26377;&#20215;&#20540;&#30340;&#24320;&#28304;&#36719;&#20214;&#36164;&#28304;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#30340;&#38376;&#27099;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21246;&#30011;&#20102;&#26410;&#26469;&#20998;&#23376;&#35774;&#35745;&#30340;&#24895;&#26223;&#65292;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#30028;&#38754;&#19982;&#35745;&#31639;&#21270;&#23398;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#21270;&#23398;&#23478;&#21644;&#23545;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21152;&#36895;&#21270;&#23398;&#21457;&#29616;&#24863;&#20852;&#36259;&#30340;&#20154;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of language models, especially transformer-based architectures, has trickled into other domains giving rise to "scientific language models" that operate on small molecules, proteins or polymers. In chemistry, language models contribute to accelerating the molecule discovery cycle as evidenced by promising recent findings in early-stage drug discovery. Here, we review the role of language models in molecular discovery, underlining their strength in de novo drug design, property prediction and reaction chemistry. We highlight valuable open-source software assets thus lowering the entry barrier to the field of scientific language modeling. Last, we sketch a vision for future molecular design that combines a chatbot interface with access to computational chemistry tools. Our contribution serves as a valuable resource for researchers, chemists, and AI enthusiasts interested in understanding how language models can and will be used to accelerate chemical discovery.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;YouTube&#20803;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#26102;&#20998;&#26512;&#20195;&#34920;&#25919;&#20826;&#30340;&#21508;&#31181;&#25919;&#27835;&#20154;&#29289;&#22312;&#20844;&#20247;&#20013;&#30340;&#35266;&#28857;&#65292;&#24182;&#20026;&#25919;&#27835;&#25191;&#34892;&#32773;&#25552;&#20379;&#29702;&#35299;&#20844;&#20247;&#24773;&#32490;&#21644;&#21046;&#23450;&#25919;&#27835;&#31574;&#30053;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2309.16234</link><description>&lt;p&gt;
&#23454;&#26102;&#20998;&#26512;&#25919;&#27835;&#20154;&#29289;&#65306;&#21033;&#29992;YouTube&#20803;&#25968;&#25454;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analyzing Political Figures in Real-Time: Leveraging YouTube Metadata for Sentiment Analysis. (arXiv:2309.16234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;YouTube&#20803;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#26102;&#20998;&#26512;&#20195;&#34920;&#25919;&#20826;&#30340;&#21508;&#31181;&#25919;&#27835;&#20154;&#29289;&#22312;&#20844;&#20247;&#20013;&#30340;&#35266;&#28857;&#65292;&#24182;&#20026;&#25919;&#27835;&#25191;&#34892;&#32773;&#25552;&#20379;&#29702;&#35299;&#20844;&#20247;&#24773;&#32490;&#21644;&#21046;&#23450;&#25919;&#27835;&#31574;&#30053;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;YouTube&#35270;&#39057;&#20803;&#25968;&#25454;&#30340;&#22823;&#25968;&#25454;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#20197;&#20998;&#26512;&#20195;&#34920;&#25919;&#20826;&#30340;&#21508;&#31181;&#25919;&#27835;&#20154;&#29289;&#22312;&#20844;&#20247;&#20013;&#30340;&#35266;&#28857;&#12290;&#36825;&#26159;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;YouTube&#24050;&#25104;&#20026;&#20154;&#20204;&#34920;&#36798;&#33258;&#24049;&#35266;&#28857;&#30340;&#24179;&#21488;&#65292;&#21253;&#25324;&#23545;&#21508;&#31181;&#25919;&#27835;&#20154;&#29289;&#30340;&#24847;&#35265;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#25919;&#27835;&#25191;&#34892;&#32773;&#29702;&#35299;&#20844;&#20247;&#24773;&#32490;&#24182;&#21046;&#23450;&#36866;&#24403;&#26377;&#25928;&#30340;&#25919;&#27835;&#31574;&#30053;&#24456;&#26377;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#26500;&#24314;&#21033;&#29992;YouTube&#35270;&#39057;&#20803;&#25968;&#25454;&#30340;&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#12290;&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#20351;&#29992;Apache Kafka&#12289;Apache PySpark&#21644;Hadoop&#36827;&#34892;&#22823;&#25968;&#25454;&#22788;&#29702;&#65307;&#20351;&#29992;TensorFlow&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#22788;&#29702;&#65307;&#24182;&#20351;&#29992;FastAPI&#22312;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#37096;&#32626;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#30340;YouTube&#35270;&#39057;&#20803;&#25968;&#25454;&#26159;&#35270;&#39057;&#25551;&#36848;&#12290;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#20351;&#29992;LSTM&#31639;&#27861;&#26500;&#24314;&#65292;&#24182;&#20135;&#29983;&#20004;&#31181;&#24773;&#24863;&#65306;&#31215;&#26497;&#21644;&#28040;&#26497;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis using big data from YouTube videos metadata can be conducted to analyze public opinions on various political figures who represent political parties. This is possible because YouTube has become one of the platforms for people to express themselves, including their opinions on various political figures. The resulting sentiment analysis can be useful for political executives to gain an understanding of public sentiment and develop appropriate and effective political strategies. This study aimed to build a sentiment analysis system leveraging YouTube videos metadata. The sentiment analysis system was built using Apache Kafka, Apache PySpark, and Hadoop for big data handling; TensorFlow for deep learning handling; and FastAPI for deployment on the server. The YouTube videos metadata used in this study is the video description. The sentiment analysis model was built using LSTM algorithm and produces two types of sentiments: positive and negative sentiments. The sentiment 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Residual Memory Transformer(RMT)&#30340;&#25511;&#21046;&#25554;&#20214;&#65292;&#23427;&#21487;&#20197;&#19982;&#22823;&#35268;&#27169;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;(CLMs)&#21512;&#20316;&#36827;&#34892;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#33539;&#24335;&#23454;&#29616;&#26356;&#28789;&#27963;&#12289;&#36890;&#29992;&#21644;&#39640;&#25928;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#22312;&#21508;&#31181;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;RMT&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16231</link><description>&lt;p&gt;
Residual Memory Transformer: &#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Controllable Text Generation with Residual Memory Transformer. (arXiv:2309.16231v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Residual Memory Transformer(RMT)&#30340;&#25511;&#21046;&#25554;&#20214;&#65292;&#23427;&#21487;&#20197;&#19982;&#22823;&#35268;&#27169;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;(CLMs)&#21512;&#20316;&#36827;&#34892;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#33539;&#24335;&#23454;&#29616;&#26356;&#28789;&#27963;&#12289;&#36890;&#29992;&#21644;&#39640;&#25928;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#22312;&#21508;&#31181;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;RMT&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CLMs&#65289;&#65292;&#20363;&#22914;GPT3&#21644;ChatGPT&#65292;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#24179;&#34913;&#28789;&#27963;&#24615;&#12289;&#25511;&#21046;&#31890;&#24230;&#21644;&#29983;&#25104;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#25511;&#21046;CLM&#30340;&#29983;&#25104;&#36807;&#31243;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65288;CTG&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#38750;&#20405;&#20837;&#24335;&#12289;&#36731;&#37327;&#32423;&#30340;&#25511;&#21046;&#25554;&#20214;&#65292;&#22312;&#20219;&#24847;&#26102;&#38388;&#27493;&#39588;&#19978;&#20276;&#38543;CLM&#30340;&#29983;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#25554;&#20214;&#65292;&#21363;Residual Memory Transformer (RMT)&#65292;&#20855;&#26377;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#21487;&#20197;&#25509;&#21463;&#20219;&#20309;&#31867;&#22411;&#30340;&#25511;&#21046;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#33539;&#24335;&#19982;CLM&#21512;&#20316;&#65292;&#23454;&#29616;&#26356;&#28789;&#27963;&#12289;&#36890;&#29992;&#21644;&#39640;&#25928;&#30340;CTG&#12290;&#22312;&#21508;&#31181;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;RMT&#22312;&#19968;&#31995;&#21015;&#26368;&#20808;&#36827;&#26041;&#27861;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Causal Language Models (CLMs), e.g., GPT3 and ChatGPT, have brought great success in text generation. However, it is still an open challenge to control the generation process of CLM while balancing flexibility, control granularity, and generation efficiency. In this paper, we provide a new alternative for controllable text generation (CTG), by designing a non-intrusive, lightweight control plugin to accompany the generation of CLM at arbitrary time steps. The proposed control plugin, namely Residual Memory Transformer (RMT), has an encoder-decoder setup, which can accept any types of control conditions and cooperate with CLM through a residual learning paradigm, to achieve a more flexible, general, and efficient CTG. Extensive experiments are carried out on various control tasks, in the form of both automatic and human evaluations. The results show the superiority of RMT over a range of state-of-the-art approaches, proving the effectiveness and versatility of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#35821;&#20041;&#32593;&#32476;&#65292;&#20026;&#21697;&#29260;&#24418;&#35937;&#30340;&#26356;&#22909;&#25506;&#32034;&#21644;&#36830;&#25509;&#24615;&#30340;&#25913;&#36827;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16228</link><description>&lt;p&gt;
&#21697;&#29260;&#32593;&#32476;&#22686;&#24378;&#22120;&#65306;&#25552;&#21319;&#21697;&#29260;&#36830;&#25509;&#24615;&#30340;&#26032;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Brand Network Booster: A New System for Improving Brand Connectivity. (arXiv:2309.16228v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#35821;&#20041;&#32593;&#32476;&#65292;&#20026;&#21697;&#29260;&#24418;&#35937;&#30340;&#26356;&#22909;&#25506;&#32034;&#21644;&#36830;&#25509;&#24615;&#30340;&#25913;&#36827;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#35821;&#20041;&#32593;&#32476;&#65292;&#20026;&#21697;&#29260;&#24418;&#35937;&#30340;&#26356;&#22909;&#25506;&#32034;&#21644;&#36830;&#25509;&#24615;&#30340;&#25913;&#36827;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;&#22312;&#32593;&#32476;&#20998;&#26512;&#26041;&#38754;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#25193;&#23637;&#29256;&#30340;&#26368;&#22823;&#36830;&#25509;&#24230;&#25913;&#36827;&#38382;&#39064;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20854;&#20013;&#21253;&#25324;&#32771;&#34385;&#25932;&#23545;&#33410;&#28857;&#12289;&#32422;&#26463;&#39044;&#31639;&#21644;&#21152;&#26435;&#32593;&#32476;&#30340;&#21487;&#33021;&#24615; - &#36890;&#36807;&#28155;&#21152;&#38142;&#25509;&#25110;&#22686;&#21152;&#29616;&#26377;&#36830;&#25509;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#36830;&#25509;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#32467;&#21512;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#23637;&#31034;&#36825;&#20010;&#26032;&#31995;&#32479;&#65292;&#24182;&#35752;&#35770;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#23545;&#20110;&#32593;&#32476;&#23398;&#32773;&#21644;&#25903;&#25345;&#24066;&#22330;&#33829;&#38144;&#21644;&#20256;&#25773;&#31649;&#29702;&#32773;&#30340;&#25112;&#30053;&#20915;&#31574;&#36807;&#31243;&#37117;&#24456;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new decision support system offered for an in-depth analysis of semantic networks, which can provide insights for a better exploration of a brand's image and the improvement of its connectivity. In terms of network analysis, we show that this goal is achieved by solving an extended version of the Maximum Betweenness Improvement problem, which includes the possibility of considering adversarial nodes, constrained budgets, and weighted networks - where connectivity improvement can be obtained by adding links or increasing the weight of existing connections. We present this new system together with two case studies, also discussing its performance. Our tool and approach are useful both for network scholars and for supporting the strategic decision-making processes of marketing and communication managers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#30721;&#28151;&#25991;&#26412;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#30721;&#28151;&#21512;&#25351;&#25968;&#21644;&#20195;&#30721;&#28151;&#21512;&#31243;&#24230;&#35780;&#20272;&#65292;&#35777;&#26126;&#33021;&#29983;&#25104;&#26377;&#25928;&#19988;&#26131;&#29702;&#35299;&#30340;&#30721;&#28151;&#21512;&#21477;&#23376;&#65292;&#20026;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#65292;&#24357;&#21512;&#22810;&#35821;&#35328;&#31038;&#20250;&#20013;&#30340;&#35821;&#35328;&#24046;&#36317;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16202</link><description>&lt;p&gt;
&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#30721;&#28151;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Marathi-English Code-mixed Text Generation. (arXiv:2309.16202v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#30721;&#28151;&#25991;&#26412;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#30721;&#28151;&#21512;&#25351;&#25968;&#21644;&#20195;&#30721;&#28151;&#21512;&#31243;&#24230;&#35780;&#20272;&#65292;&#35777;&#26126;&#33021;&#29983;&#25104;&#26377;&#25928;&#19988;&#26131;&#29702;&#35299;&#30340;&#30721;&#28151;&#21512;&#21477;&#23376;&#65292;&#20026;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#65292;&#24357;&#21512;&#22810;&#35821;&#35328;&#31038;&#20250;&#20013;&#30340;&#35821;&#35328;&#24046;&#36317;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#28151;&#21512;&#26159;&#23558;&#19981;&#21516;&#35821;&#35328;&#30340;&#35821;&#35328;&#20803;&#32032;&#28151;&#21512;&#22312;&#19968;&#36215;&#24418;&#25104;&#26377;&#24847;&#20041;&#30340;&#21477;&#23376;&#65292;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#24456;&#24120;&#35265;&#65292;&#20135;&#29983;&#20102;&#28151;&#21512;&#35821;&#35328;&#22914;Hinglish&#21644;Minglish&#12290;&#39532;&#25289;&#22320;&#35821;&#65292;&#21360;&#24230;&#31532;&#19977;&#22823;&#20351;&#29992;&#35821;&#35328;&#65292;&#36890;&#24120;&#23558;&#33521;&#35821;&#25972;&#21512;&#36827;&#26469;&#20197;&#27714;&#20934;&#30830;&#24615;&#21644;&#27491;&#24335;&#24615;&#12290;&#24320;&#21457;&#30721;&#28151;&#21512;&#35821;&#35328;&#31995;&#32479;&#65292;&#22914;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#65288;Minglish&#65289;&#65292;&#38754;&#20020;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#30721;&#28151;&#25991;&#26412;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#30721;&#28151;&#21512;&#25351;&#25968;&#65288;CMI&#65289;&#21644;&#20195;&#30721;&#28151;&#21512;&#31243;&#24230;&#65288;DCM&#65289;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;2987&#20010;&#30721;&#28151;&#21512;&#38382;&#39064;&#20013;&#65292;&#24179;&#22343;CMI&#20026;0.2&#65292;&#24179;&#22343;DCM&#20026;7.4&#65292;&#34920;&#26126;&#29983;&#25104;&#20102;&#26377;&#25928;&#19988;&#26131;&#29702;&#35299;&#30340;&#30721;&#28151;&#21512;&#21477;&#23376;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#65292;&#24357;&#21512;&#22810;&#35821;&#35328;&#31038;&#20250;&#20013;&#30340;&#35821;&#35328;&#24046;&#36317;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-mixing, the blending of linguistic elements from distinct languages to form meaningful sentences, is common in multilingual settings, yielding hybrid languages like Hinglish and Minglish. Marathi, India's third most spoken language, often integrates English for precision and formality. Developing code-mixed language systems, like Marathi-English (Minglish), faces resource constraints. This research introduces a Marathi-English code-mixed text generation algorithm, assessed with Code Mixing Index (CMI) and Degree of Code Mixing (DCM) metrics. Across 2987 code-mixed questions, it achieved an average CMI of 0.2 and an average DCM of 7.4, indicating effective and comprehensible code-mixed sentences. These results offer potential for enhanced NLP tools, bridging linguistic gaps in multilingual societies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#20351;&#29992;&#24369;&#30417;&#30563;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#26631;&#31614;&#21644;&#20351;&#29992;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.16175</link><description>&lt;p&gt;
&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#20351;&#29992;&#24369;&#30417;&#30563;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Using Weak Supervision and Data Augmentation in Question Answering. (arXiv:2309.16175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#20351;&#29992;&#24369;&#30417;&#30563;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#26631;&#31614;&#21644;&#20351;&#29992;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#30340;&#29190;&#21457;&#24378;&#35843;&#20102;&#33719;&#21462;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20197;&#22238;&#31572;&#21450;&#26102;&#21644;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#38382;&#39064;&#30340;&#38656;&#27714;&#12290;&#22312;&#30123;&#24773;&#21021;&#26399;&#65292;&#25105;&#20204;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#38382;&#31572;&#27169;&#22411;&#30340;&#32463;&#36807;&#21516;&#34892;&#35780;&#23457;&#30340;COVID-19&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24369;&#30417;&#30563;&#21644;&#25968;&#25454;&#22686;&#24378;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38382;&#31572;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#26816;&#32034;&#31639;&#27861;BM25&#20174;&#23398;&#26415;&#35770;&#25991;&#30340;&#32467;&#26500;&#21270;&#25688;&#35201;&#20013;&#33258;&#21160;&#29983;&#25104;&#26631;&#31614;&#65292;&#25506;&#31350;&#36825;&#20123;&#26631;&#31614;&#26159;&#21542;&#25552;&#20379;&#20102;&#24369;&#30417;&#30563;&#20449;&#21495;&#26469;&#35757;&#32451;&#19968;&#20010;&#25277;&#21462;&#24335;&#38382;&#31572;&#27169;&#22411;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#19987;&#23478;&#30340;&#27880;&#37322;&#25968;&#25454;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#22522;&#20110;clinicaltrials.gov&#26550;&#26500;&#21644;&#25991;&#31456;&#30340;&#32467;&#26500;&#21270;&#25688;&#35201;&#26469;&#31574;&#21010;&#26032;&#30340;&#38382;&#31572;&#23545;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25193;&#20805;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The onset of the COVID-19 pandemic accentuated the need for access to biomedical literature to answer timely and disease-specific questions. During the early days of the pandemic, one of the biggest challenges we faced was the lack of peer-reviewed biomedical articles on COVID-19 that could be used to train machine learning models for question answering (QA). In this paper, we explore the roles weak supervision and data augmentation play in training deep neural network QA models. First, we investigate whether labels generated automatically from the structured abstracts of scholarly papers using an information retrieval algorithm, BM25, provide a weak supervision signal to train an extractive QA model. We also curate new QA pairs using information retrieval techniques, guided by the clinicaltrials.gov schema and the structured abstracts of articles, in the absence of annotated data from biomedical domain experts. Furthermore, we explore augmenting the training data of a deep neural netw
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;AI&#33258;&#25105;&#24847;&#35782;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36719;&#24847;&#35782;&#24418;&#24577;&#21270;&#23545;&#24847;&#35782;&#24418;&#24577;&#30340;&#24433;&#21709;&#12290;&#19982;&#20256;&#32479;&#30340;&#25919;&#24220;&#24847;&#35782;&#24418;&#24577;&#25805;&#25511;&#25216;&#26415;&#30456;&#27604;&#65292;LLM&#24847;&#35782;&#24418;&#24577;&#21270;&#26356;&#26131;&#23454;&#26045;&#12289;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#19988;&#26356;&#20855;&#20248;&#21183;&#65292;&#20294;&#20063;&#23384;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.16167</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;AI&#33258;&#25105;&#24847;&#35782;&#23454;&#29616;&#36719;&#24847;&#35782;&#24418;&#24577;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Soft Ideologization via AI-Self-Consciousness. (arXiv:2309.16167v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;AI&#33258;&#25105;&#24847;&#35782;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36719;&#24847;&#35782;&#24418;&#24577;&#21270;&#23545;&#24847;&#35782;&#24418;&#24577;&#30340;&#24433;&#21709;&#12290;&#19982;&#20256;&#32479;&#30340;&#25919;&#24220;&#24847;&#35782;&#24418;&#24577;&#25805;&#25511;&#25216;&#26415;&#30456;&#27604;&#65292;LLM&#24847;&#35782;&#24418;&#24577;&#21270;&#26356;&#26131;&#23454;&#26045;&#12289;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#19988;&#26356;&#20855;&#20248;&#21183;&#65292;&#20294;&#20063;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large language models, LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23569;&#26377;&#30740;&#31350;&#20174;&#24847;&#35782;&#24418;&#24577;&#30340;&#35282;&#24230;&#32771;&#34385;LLM&#30340;&#23041;&#32961;&#21644;&#33030;&#24369;&#24615;&#65292;&#23588;&#20854;&#26159;&#24403;&#23427;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#37096;&#32626;&#22312;&#25935;&#24863;&#39046;&#22495;&#65292;&#22914;&#36873;&#20030;&#21644;&#25945;&#32946;&#20013;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;AI&#33258;&#25105;&#23545;&#35805;&#30340;&#26041;&#24335;&#25506;&#35752;&#20102;GPT&#36719;&#24847;&#35782;&#24418;&#24577;&#21270;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;GPT&#33021;&#22815;&#8220;&#29702;&#35299;&#8221;&#39044;&#26399;&#30340;&#24847;&#35782;&#24418;&#24577;&#65292;&#24182;&#38543;&#21518;&#29983;&#25104;LLM&#24847;&#35782;&#24418;&#24577;&#27880;&#20837;&#30340;&#24494;&#35843;&#25968;&#25454;&#65292;AI&#21487;&#20197;&#33719;&#24471;&#19968;&#31181;&#8220;&#29702;&#35299;&#8221;&#24847;&#35782;&#24418;&#24577;&#30340;&#35270;&#35282;&#12290;&#19982;&#20256;&#32479;&#30340;&#25919;&#24220;&#24847;&#35782;&#24418;&#24577;&#25805;&#25511;&#25216;&#26415;&#65288;&#22914;&#20449;&#24687;&#23457;&#26597;&#65289;&#30456;&#27604;&#65292;LLM&#30340;&#24847;&#35782;&#24418;&#24577;&#21270;&#35777;&#26126;&#26356;&#20855;&#26377;&#20248;&#21183;&#65306;&#23427;&#26131;&#20110;&#23454;&#26045;&#12289;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#24378;&#22823;&#65292;&#22240;&#27492;&#20805;&#28385;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, few studies have addressed the LLM threat and vulnerability from an ideology perspective, especially when they are increasingly being deployed in sensitive domains, e.g., elections and education. In this study, we explore the implications of GPT soft ideologization through the use of AI-self-consciousness. By utilizing GPT self-conversations, AI can be granted a vision to "comprehend" the intended ideology, and subsequently generate finetuning data for LLM ideology injection. When compared to traditional government ideology manipulation techniques, such as information censorship, LLM ideologization proves advantageous; it is easy to implement, cost-effective, and powerful, thus brimming with risks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#30340;&#19968;&#33268;&#24615;&#23545;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#27169;&#22411;&#35757;&#32451;&#25152;&#24471;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;RM&#19968;&#33268;&#24615;&#30340;&#23545;&#27604;&#25552;&#31034;&#30340;&#22522;&#20934;&#27979;&#35797;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.16155</link><description>&lt;p&gt;
&#22870;&#21169;&#65288;&#19981;&#65289;&#19968;&#33268;&#24615;&#23545;RLHF&#30340;&#28051;&#28404;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
The Trickle-down Impact of Reward (In-)consistency on RLHF. (arXiv:2309.16155v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#30340;&#19968;&#33268;&#24615;&#23545;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#27169;&#22411;&#35757;&#32451;&#25152;&#24471;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;RM&#19968;&#33268;&#24615;&#30340;&#23545;&#27604;&#25552;&#31034;&#30340;&#22522;&#20934;&#27979;&#35797;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#26631;&#20934;&#23454;&#36341;&#28041;&#21450;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#65292;&#32780;RM&#26412;&#36523;&#26159;&#36890;&#36807;&#35757;&#32451;&#26469;&#21453;&#26144;&#20154;&#31867;&#23545;&#26399;&#26395;&#29983;&#25104;&#30340;&#20559;&#22909;&#12290;&#19968;&#20010;&#20540;&#24471;&#30740;&#31350;&#30340;&#37325;&#35201;&#20027;&#39064;&#26159;RM&#30340;&#65288;&#19981;&#65289;&#19968;&#33268;&#24615; - &#21363;&#23427;&#20204;&#33021;&#21542;&#35782;&#21035;&#19981;&#21516;&#25552;&#31034;&#30340;&#35821;&#20041;&#21464;&#21270;&#24182;&#36866;&#24403;&#22320;&#35843;&#25972;&#22870;&#21169;&#20998;&#37197; - &#20197;&#21450;&#23427;&#20204;&#23545;&#19979;&#28216;RLHF&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#38024;&#23545;RM&#19981;&#19968;&#33268;&#24615;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30456;&#20851;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#65288;1&#65289;&#25105;&#20204;&#22914;&#20309;&#34913;&#37327;&#22870;&#21169;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65311;&#65288;2&#65289;&#29616;&#26377;&#30340;RM&#26377;&#22810;&#19968;&#33268;&#65292;&#25105;&#20204;&#22914;&#20309;&#25913;&#36827;&#23427;&#20204;&#65311;&#65288;3&#65289;&#22870;&#21169;&#30340;&#19981;&#19968;&#33268;&#24615;&#20197;&#20309;&#31181;&#26041;&#24335;&#24433;&#21709;RLHF&#27169;&#22411;&#35757;&#32451;&#25152;&#24471;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;RM&#19968;&#33268;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#31574;&#30053;"&#23545;&#27604;&#25552;&#31034;"&#12290;&#27599;&#20010;&#23545;&#27604;&#25552;&#31034;&#31034;&#20363;&#37117;&#21253;&#21547;&#19968;&#23545;&#20855;&#26377;&#19981;&#21516;&#30495;&#23454;&#21709;&#24212;&#30340;&#35789;&#27719;&#30456;&#20284;&#30340;&#25351;&#20196;&#12290;&#19968;&#33268;&#30340;RM&#26159;&#26399;&#26395;&#23545;&#36825;&#23545;&#25351;&#20196;&#32473;&#20986;&#30456;&#20284;&#22870;&#21169;&#20998;&#37197;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard practice within Reinforcement Learning from Human Feedback (RLHF) involves optimizing against a Reward Model (RM), which itself is trained to reflect human preferences for desirable generations. A notable subject that is understudied is the (in-)consistency of RMs -- whether they can recognize the semantic changes to different prompts and appropriately adapt their reward assignments -- and their impact on the downstream RLHF model.  In this paper, we visit a series of research questions relevant to RM inconsistency: (1) How can we measure the consistency of reward models? (2) How consistent are the existing RMs and how can we improve them? (3) In what ways does reward inconsistency influence the chatbots resulting from the RLHF model training?  We propose Contrast Instructions -- a benchmarking strategy for the consistency of RM. Each example in Contrast Instructions features a pair of lexically similar instructions with different ground truth responses. A consistent RM is exp
&lt;/p&gt;</description></item><item><title>AE-GPT&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#30417;&#27979;&#25253;&#21578;&#20013;&#25552;&#21462;&#19981;&#33391;&#20107;&#20214;&#65292;&#35813;&#30740;&#31350;&#20197;&#27969;&#24863;&#30123;&#33495;&#19981;&#33391;&#20107;&#20214;&#20026;&#20363;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#32463;&#36807;&#24494;&#35843;&#30340;GPT 3.5&#27169;&#22411;&#65288;AE-GPT&#65289;&#22312;&#20005;&#26684;&#21305;&#37197;&#21644;&#28789;&#27963;&#21305;&#37197;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#26174;&#31034;&#20102;LLMs&#22312;&#22788;&#29702;&#21307;&#23398;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21487;&#25512;&#24191;&#21040;&#20854;&#20182;&#19981;&#33391;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.16150</link><description>&lt;p&gt;
AE-GPT:&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30417;&#27979;&#25253;&#21578;&#20013;&#25552;&#21462;&#19981;&#33391;&#20107;&#20214;-&#20197;&#27969;&#24863;&#30123;&#33495;&#19981;&#33391;&#20107;&#20214;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events. (arXiv:2309.16150v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16150
&lt;/p&gt;
&lt;p&gt;
AE-GPT&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#30417;&#27979;&#25253;&#21578;&#20013;&#25552;&#21462;&#19981;&#33391;&#20107;&#20214;&#65292;&#35813;&#30740;&#31350;&#20197;&#27969;&#24863;&#30123;&#33495;&#19981;&#33391;&#20107;&#20214;&#20026;&#20363;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#32463;&#36807;&#24494;&#35843;&#30340;GPT 3.5&#27169;&#22411;&#65288;AE-GPT&#65289;&#22312;&#20005;&#26684;&#21305;&#37197;&#21644;&#28789;&#27963;&#21305;&#37197;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#26174;&#31034;&#20102;LLMs&#22312;&#22788;&#29702;&#21307;&#23398;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21487;&#25512;&#24191;&#21040;&#20854;&#20182;&#19981;&#33391;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#30123;&#33495;&#22312;&#20840;&#29699;&#20581;&#24247;&#12289;&#20943;&#36731;&#20256;&#26579;&#30149;&#21644;&#22823;&#27969;&#34892;&#29190;&#21457;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20598;&#23572;&#20063;&#20250;&#23548;&#33268;&#19981;&#33391;&#20107;&#20214;&#65288;AEs&#65289;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26377;&#25928;&#35782;&#21035;&#21644;&#32534;&#30446;&#20020;&#24202;&#25253;&#21578;&#20013;&#30340;&#19981;&#33391;&#20107;&#20214;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;1990&#24180;&#33267;2016&#24180;&#26469;&#33258;&#30123;&#33495;&#19981;&#33391;&#20107;&#20214;&#25253;&#21578;&#31995;&#32479;&#65288;VAERS&#65289;&#30340;&#25968;&#25454;&#65292;&#29305;&#21035;&#20851;&#27880;&#35780;&#20272;LLMs&#22312;&#19981;&#33391;&#20107;&#20214;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20351;&#29992;&#27969;&#24863;&#30123;&#33495;&#20316;&#20026;&#20351;&#29992;&#26696;&#20363;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;&#20027;&#27969;&#30340;LLMs&#65292;&#21253;&#25324;GPT-2&#12289;GPT-3&#21464;&#20307;&#12289;GPT-4&#21644;Llama 2&#12290;&#32463;&#36807;&#24494;&#35843;&#30340;GPT 3.5&#27169;&#22411;&#65288;AE-GPT&#65289;&#22312;&#20005;&#26684;&#21305;&#37197;&#26041;&#38754;&#30340;&#24179;&#22343;&#24494; F1 &#20998;&#25968;&#20026;0.704&#65292;&#28789;&#27963;&#21305;&#37197;&#26041;&#38754;&#20026;0.816&#12290;AE-GPT&#30340;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#31361;&#26174;&#20102;LLMs&#22312;&#22788;&#29702;&#21307;&#23398;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#34920;&#26126;&#22312;&#20808;&#36827;&#30340;AE&#26816;&#27979;&#26041;&#38754;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#65292;&#22240;&#27492;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;AE&#25552;&#21462;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though Vaccines are instrumental in global health, mitigating infectious diseases and pandemic outbreaks, they can occasionally lead to adverse events (AEs). Recently, Large Language Models (LLMs) have shown promise in effectively identifying and cataloging AEs within clinical reports. Utilizing data from the Vaccine Adverse Event Reporting System (VAERS) from 1990 to 2016, this study particularly focuses on AEs to evaluate LLMs' capability for AE extraction. A variety of prevalent LLMs, including GPT-2, GPT-3 variants, GPT-4, and Llama 2, were evaluated using Influenza vaccine as a use case. The fine-tuned GPT 3.5 model (AE-GPT) stood out with a 0.704 averaged micro F1 score for strict match and 0.816 for relaxed match. The encouraging performance of the AE-GPT underscores LLMs' potential in processing medical data, indicating a significant stride towards advanced AE detection, thus presumably generalizable to other AE extraction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35748;&#30693;&#33021;&#21147;&#21644;&#20449;&#24515;&#21160;&#24577;&#26041;&#38754;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#26377;&#26102;&#20250;&#22312;&#22238;&#31572;&#38169;&#35823;&#26102;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#20449;&#24515;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#24515;&#29702;&#23398;&#20013;&#30340;&#37011;&#23425;-&#20811;&#40065;&#26684;&#25928;&#24212;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20182;&#20204;&#22312;&#22238;&#31572;&#27491;&#30830;&#26102;&#26377;&#26102;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#20449;&#24515;&#65292;&#26263;&#31034;&#20102;&#28508;&#22312;&#30340;&#20302;&#20272;&#20559;&#24046;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#23545;LLMs&#35748;&#30693;&#36807;&#31243;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#30740;&#31350;LLMs&#33258;&#25105;&#35780;&#20272;&#26426;&#21046;&#30340;&#32454;&#33410;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#26032;&#21457;&#29616;&#65292;&#25512;&#21160;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.16145</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20449;&#24515;-&#33021;&#21147;&#24046;&#36317;&#65306;&#19968;&#39033;&#35748;&#30693;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Confidence-Competence Gap in Large Language Models: A Cognitive Study. (arXiv:2309.16145v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35748;&#30693;&#33021;&#21147;&#21644;&#20449;&#24515;&#21160;&#24577;&#26041;&#38754;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#26377;&#26102;&#20250;&#22312;&#22238;&#31572;&#38169;&#35823;&#26102;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#20449;&#24515;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#24515;&#29702;&#23398;&#20013;&#30340;&#37011;&#23425;-&#20811;&#40065;&#26684;&#25928;&#24212;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20182;&#20204;&#22312;&#22238;&#31572;&#27491;&#30830;&#26102;&#26377;&#26102;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#20449;&#24515;&#65292;&#26263;&#31034;&#20102;&#28508;&#22312;&#30340;&#20302;&#20272;&#20559;&#24046;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#23545;LLMs&#35748;&#30693;&#36807;&#31243;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#30740;&#31350;LLMs&#33258;&#25105;&#35780;&#20272;&#26426;&#21046;&#30340;&#32454;&#33410;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#26032;&#21457;&#29616;&#65292;&#25512;&#21160;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#32780;&#24341;&#36215;&#20102;&#26222;&#36941;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#30340;&#35748;&#30693;&#33021;&#21147;&#21644;&#20449;&#24515;&#21160;&#24577;&#12290;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#20102;&#23427;&#20204;&#33258;&#25105;&#35780;&#20272;&#30340;&#20449;&#24515;&#19982;&#23454;&#38469;&#34920;&#29616;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#21508;&#31181;&#38382;&#21367;&#21644;&#30495;&#23454;&#22330;&#26223;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#25552;&#21462;&#20102;LLMs&#22312;&#22238;&#31572;&#20013;&#23637;&#31034;&#30340;&#20449;&#24515;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#24773;&#20917;&#65292;&#21363;&#27169;&#22411;&#22312;&#22238;&#31572;&#38169;&#35823;&#26102;&#20173;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#20449;&#24515;&#65292;&#36825;&#31867;&#20284;&#20110;&#20154;&#31867;&#24515;&#29702;&#23398;&#20013;&#35266;&#23519;&#21040;&#30340;&#37011;&#23425;-&#20811;&#40065;&#26684;&#25928;&#24212;&#12290;&#30456;&#21453;&#65292;&#20063;&#26377;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#22312;&#22238;&#31572;&#27491;&#30830;&#26102;&#23637;&#31034;&#20986;&#36739;&#20302;&#30340;&#20449;&#24515;&#65292;&#25581;&#31034;&#20102;&#28508;&#22312;&#30340;&#20302;&#20272;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#23545;LLMs&#35748;&#30693;&#36807;&#31243;&#30340;&#28145;&#20837;&#20102;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;LLMs&#33258;&#25105;&#35780;&#20272;&#26426;&#21046;&#30340;&#32454;&#24494;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20540;&#24471;&#20851;&#27880;&#30340;&#26032;&#21457;&#29616;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have acquired ubiquitous attention for their performances across diverse domains. Our study here searches through LLMs' cognitive abilities and confidence dynamics. We dive deep into understanding the alignment between their self-assessed confidence and actual performance. We exploit these models with diverse sets of questionnaires and real-world scenarios and extract how LLMs exhibit confidence in their responses. Our findings reveal intriguing instances where models demonstrate high confidence even when they answer incorrectly. This is reminiscent of the Dunning-Kruger effect observed in human psychology. In contrast, there are cases where models exhibit low confidence with correct answers revealing potential underestimation biases. Our results underscore the need for a deeper understanding of their cognitive processes. By examining the nuances of LLMs' self-assessment mechanism, this investigation provides noteworthy revelations that serve to advance the
&lt;/p&gt;</description></item><item><title>TPE&#26159;&#19968;&#31181;&#38754;&#21521;&#27010;&#24565;&#24037;&#20855;&#30340;&#22810;&#20154;&#29289;&#21327;&#20316;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16090</link><description>&lt;p&gt;
TPE: &#38754;&#21521;&#22810;&#20154;&#29289;&#21327;&#20316;&#30340;&#27010;&#24565;&#24037;&#20855;&#26356;&#22909;&#21512;&#25104;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona Collaboration. (arXiv:2309.16090v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16090
&lt;/p&gt;
&lt;p&gt;
TPE&#26159;&#19968;&#31181;&#38754;&#21521;&#27010;&#24565;&#24037;&#20855;&#30340;&#22810;&#20154;&#29289;&#21327;&#20316;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35268;&#21010;&#20351;&#29992;&#21508;&#31181;&#21151;&#33021;&#24037;&#20855;&#65292;&#22914;&#35745;&#31639;&#22120;&#21644;&#26816;&#32034;&#22120;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#25193;&#23637;&#20102;&#36825;&#20123;&#24037;&#20855;&#30340;&#23450;&#20041;&#65292;&#37325;&#28857;&#26159;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#27010;&#24565;&#24037;&#20855;&#12290;&#27010;&#24565;&#24037;&#20855;&#25351;&#23450;&#20102;&#19968;&#31181;&#26377;&#21161;&#20110;&#31995;&#32479;&#24615;&#25110;&#35843;&#26597;&#24615;&#24605;&#32771;&#30340;&#35748;&#30693;&#27010;&#24565;&#12290;&#36825;&#20123;&#27010;&#24565;&#24037;&#20855;&#22312;&#23454;&#36341;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#20363;&#22914;&#22810;&#37325;&#24515;&#29702;&#25110;&#36741;&#23548;&#31574;&#30053;&#21160;&#24577;&#24212;&#29992;&#20110;&#21333;&#20010;&#22238;&#21512;&#20197;&#32452;&#21512;&#26377;&#29992;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#22312;&#36825;&#20123;&#27010;&#24565;&#24037;&#20855;&#19978;&#30340;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#20154;&#29289;&#21327;&#20316;&#26694;&#26550;: Think-Plan-Execute (TPE)&#12290;&#35813;&#26694;&#26550;&#23558;&#21709;&#24212;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#33394;: &#24605;&#32771;&#32773;&#65292;&#35268;&#21010;&#32773;&#21644;&#25191;&#34892;&#32773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24605;&#32771;&#32773;&#20998;&#26512;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#34920;&#29616;&#20986;&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#20363;&#22914;&#29992;&#25143;&#30340;&#24773;&#32490;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated exceptional performance in planning the use of various functional tools, such as calculators and retrievers, particularly in question-answering tasks. In this paper, we expand the definition of these tools, centering on conceptual tools within the context of dialogue systems. A conceptual tool specifies a cognitive concept that aids systematic or investigative thought. These conceptual tools play important roles in practice, such as multiple psychological or tutoring strategies being dynamically applied in a single turn to compose helpful responses. To further enhance the reasoning and planning capability of LLMs with these conceptual tools, we introduce a multi-persona collaboration framework: Think-Plan-Execute (TPE). This framework decouples the response generation process into three distinct roles: Thinker, Planner, and Executor. Specifically, the Thinker analyzes the internal status exhibited in the dialogue context, such as user emot
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#21644;&#25490;&#38500;&#27861;&#38598;&#25104;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36951;&#24536;&#31169;&#20154;&#25991;&#26412;&#24207;&#21015;&#26102;&#33021;&#22815;&#21462;&#24471;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16082</link><description>&lt;p&gt;
&#36890;&#36807;&#25490;&#38500;&#27861;&#38598;&#25104;&#27169;&#22411;&#26469;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36951;&#24536;&#31169;&#20154;&#25991;&#26412;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Forgetting Private Textual Sequences in Language Models via Leave-One-Out Ensemble. (arXiv:2309.16082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#21644;&#25490;&#38500;&#27861;&#38598;&#25104;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36951;&#24536;&#31169;&#20154;&#25991;&#26412;&#24207;&#21015;&#26102;&#33021;&#22815;&#21462;&#24471;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#26377;&#20542;&#21521;&#20110;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#35760;&#20303;&#32597;&#35265;&#25110;&#29420;&#29305;&#30340;&#20196;&#29260;&#24207;&#21015;&#12290;&#22312;&#37096;&#32626;&#27169;&#22411;&#21518;&#65292;&#26681;&#25454;&#20010;&#20154;&#35201;&#27714;&#65292;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#20219;&#20309;&#20010;&#20154;&#20449;&#24687;&#21487;&#33021;&#20250;&#34987;&#25552;&#20986;&#12290;&#27599;&#27425;&#20010;&#20154;&#24819;&#35201;&#34892;&#20351;&#34987;&#36951;&#24536;&#26435;&#21033;&#26102;&#37325;&#26032;&#35757;&#32451;&#24213;&#23618;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#25105;&#20204;&#20351;&#29992;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#38500;&#27861;&#38598;&#25104;&#26041;&#27861;&#26469;&#20174;&#27169;&#22411;&#20013;&#36951;&#24536;&#38656;&#35201;&#36951;&#24536;&#30340;&#25991;&#26412;&#24207;&#21015;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#22810;&#20010;&#25945;&#24072;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65307;&#23545;&#20110;&#27599;&#20010;&#38656;&#35201;&#21024;&#38500;&#30340;&#30446;&#26631;&#24207;&#21015;&#65292;&#25105;&#20204;&#25490;&#38500;&#22312;&#21253;&#21547;&#35813;&#24207;&#21015;&#30340;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#20174;&#21097;&#20313;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#32858;&#21512;&#39044;&#27979;&#32467;&#26524;&#65292;&#20197;&#25552;&#20379;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#30417;&#30563;&#12290;&#22312;LibriSpeech&#21644;WikiText-103&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown that language models have a tendency to memorize rare or unique token sequences in the training corpus. After deploying a model, practitioners might be asked to delete any personal information from the model by individuals' requests. Re-training the underlying model every time individuals would like to practice their rights to be forgotten is computationally expensive. We employ a teacher-student framework and propose a novel leave-one-out ensemble method to unlearn the targeted textual sequences that need to be forgotten from the model. In our approach, multiple teachers are trained on disjoint sets; for each targeted sequence to be removed, we exclude the teacher trained on the set containing this sequence and aggregate the predictions from remaining teachers to provide supervision during fine-tuning. Experiments on LibriSpeech and WikiText-103 datasets show that the proposed method achieves superior privacy-utility trade-offs than other counterparts.
&lt;/p&gt;</description></item><item><title>AnyMAL&#26159;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#20219;&#24847;&#27169;&#24577;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#27169;&#24577;&#20449;&#21495;&#65292;&#24182;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16058</link><description>&lt;p&gt;
AnyMAL:&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#20219;&#24847;&#27169;&#24577;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model. (arXiv:2309.16058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16058
&lt;/p&gt;
&lt;p&gt;
AnyMAL&#26159;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#20219;&#24847;&#27169;&#24577;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#27169;&#24577;&#20449;&#21495;&#65292;&#24182;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Any-Modality Augmented Language Model (AnyMAL)&#65292;&#23427;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#27169;&#24577;&#20449;&#21495;&#65288;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#12289;IMU&#36816;&#21160;&#20256;&#24863;&#22120;&#65289;&#65292;&#24182;&#29983;&#25104;&#25991;&#26412;&#21709;&#24212;&#12290;AnyMAL&#32487;&#25215;&#20102;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;&#21253;&#25324;LLaMA-2&#65288;70B&#65289;&#65289;&#30340;&#24378;&#22823;&#25991;&#26412;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#23545;&#40784;&#27169;&#22359;&#23558;&#27169;&#24577;&#29305;&#23450;&#20449;&#21495;&#36716;&#25442;&#20026;&#32852;&#21512;&#25991;&#26412;&#31354;&#38388;&#12290;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;&#22810;&#27169;&#24577;LLM&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#25163;&#21160;&#25910;&#38598;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#28085;&#30422;&#31616;&#21333;&#38382;&#31572;&#20197;&#22806;&#30340;&#21508;&#31181;&#20027;&#39064;&#21644;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#21253;&#25324;&#20154;&#24037;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Any-Modality Augmented Language Model (AnyMAL), a unified model that reasons over diverse input modality signals (i.e. text, image, video, audio, IMU motion sensor), and generates textual responses. AnyMAL inherits the powerful text-based reasoning abilities of the state-of-the-art LLMs including LLaMA-2 (70B), and converts modality-specific signals to the joint textual space through a pre-trained aligner module. To further strengthen the multimodal LLM's capabilities, we fine-tune the model with a multimodal instruction set manually collected to cover diverse topics and tasks beyond simple QAs. We conduct comprehensive empirical analysis comprising both human and automatic evaluations, and demonstrate state-of-the-art performance on various multimodal tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.16042</link><description>&lt;p&gt;
&#12298;&#35821;&#35328;&#27169;&#22411;&#20013;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#30340;&#26368;&#20339;&#23454;&#36341;&#65306;&#24230;&#37327;&#21644;&#26041;&#27861;&#12299;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. (arXiv:2309.16042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#26800;&#35299;&#37322;&#24615;&#26088;&#22312;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20854;&#20013;&#23450;&#20301;-&#35782;&#21035;&#37325;&#35201;&#30340;&#27169;&#22411;&#32452;&#20214;&#26159;&#20851;&#38190;&#27493;&#39588;&#12290;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#65292;&#20063;&#31216;&#20026;&#22240;&#26524;&#36861;&#36394;&#25110;&#20132;&#25442;&#24178;&#39044;&#65292;&#26159;&#23436;&#25104;&#36825;&#19968;&#20219;&#21153;&#30340;&#26631;&#20934;&#25216;&#26415;&#65292;&#20294;&#25991;&#29486;&#20013;&#23384;&#22312;&#35768;&#22810;&#21464;&#20307;&#65292;&#23545;&#36229;&#21442;&#25968;&#25110;&#26041;&#27861;&#36873;&#25321;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#35780;&#20272;&#25351;&#26631;&#21644;&#25439;&#22351;&#26041;&#27861;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#20301;&#21644;&#30005;&#36335;&#21457;&#29616;&#30340;&#20960;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#35299;&#37322;&#32467;&#26524;&#12290;&#36890;&#36807;&#32463;&#39564;&#35266;&#23519;&#25903;&#25345;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#25351;&#26631;&#25110;&#26041;&#27861;&#21487;&#33021;&#26356;&#21463;&#27426;&#36814;&#30340;&#27010;&#24565;&#24615;&#35770;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#30340;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization -- identifying the important model components -- is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig et al., 2020), but the literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide recommendations for the best practices of activation patching going forwards.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25903;&#25345;&#38271;&#19978;&#19979;&#25991;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#23454;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#30740;&#31350;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#30340;&#25351;&#23548;&#35843;&#25972;&#31243;&#24207;&#65292;&#24050;&#32463;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16039</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#26377;&#25928;&#38271;&#19978;&#19979;&#25991;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Effective Long-Context Scaling of Foundation Models. (arXiv:2309.16039v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16039
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25903;&#25345;&#38271;&#19978;&#19979;&#25991;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#23454;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#30740;&#31350;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#30340;&#25351;&#23548;&#35843;&#25972;&#31243;&#24207;&#65292;&#24050;&#32463;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25903;&#25345;&#26368;&#22810;32768&#20010;&#26631;&#35760;&#30340;&#26377;&#25928;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#19978;&#19979;&#25991;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;Llama 2&#36830;&#32493;&#39044;&#35757;&#32451;&#12289;&#22312;&#38271;&#25991;&#26412;&#19978;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26500;&#24314;&#25105;&#20204;&#30340;&#27169;&#22411;&#31995;&#21015;&#12290;&#25105;&#20204;&#23545;&#35821;&#35328;&#24314;&#27169;&#12289;&#21512;&#25104;&#19978;&#19979;&#25991;&#25506;&#27979;&#20219;&#21153;&#21644;&#21508;&#31181;&#30740;&#31350;&#22522;&#20934;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#22312;&#30740;&#31350;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#24120;&#35268;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#30456;&#23545;&#20110;Llama 2&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#25351;&#23548;&#35843;&#25972;&#31243;&#24207;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#30340;&#38271;&#25351;&#23548;&#25968;&#25454;&#65292;70B&#29256;&#26412;&#24050;&#32463;&#22312;&#19968;&#22871;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;gpt-3.5-turbo-16k&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#38500;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;Llama&#30340;&#20301;&#32622;&#32534;&#30721;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#24314;&#27169;&#38271;&#20381;&#36182;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a series of long-context LLMs that support effective context windows of up to 32,768 tokens. Our model series are built through continual pretraining from Llama 2 with longer training sequences and on a dataset where long texts are upsampled. We perform extensive evaluation on language modeling, synthetic context probing tasks, and a wide range of research benchmarks. On research benchmarks, our models achieve consistent improvements on most regular tasks and significant improvements on long-context tasks over Llama 2. Notably, with a cost-effective instruction tuning procedure that does not require human-annotated long instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks. Alongside these results, we provide an in-depth analysis on the individual components of our method. We delve into Llama's position encodings and discuss its limitation in modeling long dependencies. We also examine the impact of var
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#23398;&#38382;&#31572;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.16035</link><description>&lt;p&gt;
MedEdit&#65306;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases. (arXiv:2309.16035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#23398;&#38382;&#31572;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34429;&#28982;&#22312;&#19968;&#33324;&#39046;&#22495;&#34920;&#29616;&#24378;&#22823;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#38754;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24448;&#24448;&#20316;&#20026;&#8220;&#40657;&#30418;&#8221;&#36816;&#20316;&#65292;&#38590;&#20197;&#20462;&#25913;&#20854;&#34892;&#20026;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#26088;&#22312;&#25913;&#36827;LLM&#30340;&#21709;&#24212;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26816;&#32034;&#31574;&#30053;&#65292;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;LLM&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#12290;&#36890;&#36807;&#23545;MedQA-SMILE&#25968;&#25454;&#38598;&#36827;&#34892;&#21307;&#23398;QA&#30340;&#37325;&#28857;&#30740;&#31350;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#26816;&#32034;&#27169;&#22411;&#21644;&#21521;LLM&#25552;&#20379;&#30340;&#20107;&#23454;&#25968;&#37327;&#23545;&#20854;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#32534;&#36753;&#21518;&#30340;Vicuna&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20174;44.46&#65285;&#25552;&#39640;&#21040;48.54&#65285;&#12290;&#36825;&#39033;&#24037;&#20316;&#20984;&#26174;&#20102;&#27169;&#22411;&#32534;&#36753;&#25913;&#21892;LLM&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20026;&#32531;&#35299;&#40657;&#30418;LLM&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), although powerful in general domains, often perform poorly on domain-specific tasks like medical question answering (QA). Moreover, they tend to function as "black-boxes," making it challenging to modify their behavior. Addressing this, our study delves into model editing utilizing in-context learning, aiming to improve LLM responses without the need for fine-tuning or retraining. Specifically, we propose a comprehensive retrieval strategy to extract medical facts from an external knowledge base, and then we incorporate them into the query prompt for the LLM. Focusing on medical QA using the MedQA-SMILE dataset, we evaluate the impact of different retrieval models and the number of facts provided to the LLM. Notably, our edited Vicuna model exhibited an accuracy improvement from 44.46% to 48.54%. This work underscores the potential of model editing to enhance LLM performance, offering a practical approach to mitigate the challenges of black-box LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#38024;&#23545;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;TIDA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26469;&#22635;&#34917;&#20851;&#32852;&#32467;&#26500;&#24046;&#36317;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20154;&#31867;&#24863;&#30693;&#33021;&#21147;&#65292;&#22914;&#24615;&#21035;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;TIDA&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15991</link><description>&lt;p&gt;
&#26377;&#38024;&#23545;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#20102;&#22522;&#26412;&#25216;&#33021;&#23383;&#24149;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Targeted Image Data Augmentation Increases Basic Skills Captioning Robustness. (arXiv:2309.15991v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#38024;&#23545;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;TIDA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26469;&#22635;&#34917;&#20851;&#32852;&#32467;&#26500;&#24046;&#36317;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20154;&#31867;&#24863;&#30693;&#33021;&#21147;&#65292;&#22914;&#24615;&#21035;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;TIDA&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#22312;&#25512;&#24191;&#21040;&#36229;&#20986;&#19978;&#19979;&#25991;&#30340;&#31034;&#20363;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#36825;&#31181;&#38480;&#21046;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#26377;&#20851;&#19990;&#30028;&#28508;&#22312;&#20851;&#32852;&#32467;&#26500;&#30340;&#37096;&#20998;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TIDA&#65288;&#26377;&#38024;&#23545;&#24615;&#30340;&#22270;&#20687;&#32534;&#36753;&#25968;&#25454;&#22686;&#24378;&#65289;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#19987;&#27880;&#20110;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26469;&#22635;&#34917;&#20851;&#32852;&#32467;&#26500;&#24046;&#36317;&#20197;&#25552;&#39640;&#27169;&#22411;&#31867;&#20154;&#33021;&#21147;&#65288;&#20363;&#22914;&#24615;&#21035;&#35782;&#21035;&#65289;&#30340;&#26377;&#38024;&#23545;&#24615;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;TIDA&#35782;&#21035;&#25551;&#36848;&#22270;&#20687;&#30340;&#26631;&#39064;&#20013;&#30340;&#29305;&#23450;&#25216;&#33021;&#65288;&#20363;&#22914;&#22270;&#20013;&#29305;&#23450;&#24615;&#21035;&#30340;&#23384;&#22312;&#65289;&#65292;&#25913;&#21464;&#26631;&#39064;&#65288;&#20363;&#22914;&#23558;&#8220;&#22899;&#20154;&#8221;&#25913;&#20026;&#8220;&#30007;&#20154;&#8221;&#65289;&#65292;&#28982;&#21518;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#32534;&#36753;&#22270;&#20687;&#65292;&#20197;&#20351;&#20854;&#19982;&#26032;&#26631;&#39064;&#21305;&#37197;&#65288;&#20363;&#22914;&#21807;&#19968;&#22320;&#23558;&#19968;&#20010;&#22899;&#20154;&#25913;&#20026;&#19968;&#20010;&#30007;&#20154;&#21516;&#26102;&#20445;&#25345;&#19978;&#19979;&#25991;&#19981;&#21464;&#65289;&#12290;&#22522;&#20110;Flickr30K&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#20351;&#29992;TIDA&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks typically struggle in generalizing to out-of-context examples. One reason for this limitation is caused by having datasets that incorporate only partial information regarding the potential correlational structure of the world. In this work, we propose TIDA (Targeted Image-editing Data Augmentation), a targeted data augmentation method focused on improving models' human-like abilities (e.g., gender recognition) by filling the correlational structure gap using a text-to-image generative model. More specifically, TIDA identifies specific skills in captions describing images (e.g., the presence of a specific gender in the image), changes the caption (e.g., "woman" to "man"), and then uses a text-to-image model to edit the image in order to match the novel caption (e.g., uniquely changing a woman to a man while maintaining the context identical). Based on the Flickr30K benchmark, we show that, compared with the original data set, a TIDA-enhanced dataset related to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20026;HYKIST&#39033;&#30446;&#20013;&#30340;&#36234;&#21335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26500;&#24314;&#25552;&#20379;&#25903;&#25345;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#35299;&#20915;&#21307;&#30103;&#39046;&#22495;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#35821;&#35328;&#38556;&#30861;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#24739;&#32773;&#25252;&#29702;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.15869</link><description>&lt;p&gt;
&#22312;HYKIST&#39033;&#30446;&#20013;&#20026;&#36234;&#21335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Pre-Training for Vietnamese Automatic Speech Recognition in the HYKIST Project. (arXiv:2309.15869v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20026;HYKIST&#39033;&#30446;&#20013;&#30340;&#36234;&#21335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26500;&#24314;&#25552;&#20379;&#25903;&#25345;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#35299;&#20915;&#21307;&#30103;&#39046;&#22495;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#35821;&#35328;&#38556;&#30861;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#24739;&#32773;&#25252;&#29702;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20170;&#22825;&#30340;&#20840;&#29699;&#20114;&#32852;&#20013;&#65292;&#31227;&#23621;&#22269;&#22806;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#26080;&#35770;&#26159;&#20026;&#20102;&#23601;&#19994;&#12289;&#38590;&#27665;&#23433;&#32622;&#36824;&#26159;&#20854;&#20182;&#21407;&#22240;&#12290;&#27597;&#35821;&#19982;&#31227;&#27665;&#20043;&#38388;&#30340;&#35821;&#35328;&#38556;&#30861;&#26159;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24120;&#35265;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#24739;&#32773;&#21644;&#21307;&#29983;&#22312;&#30149;&#21490;&#37319;&#38598;&#25110;&#24613;&#35786;&#23460;&#30340;&#27807;&#36890;&#36807;&#31243;&#20013;&#20986;&#29616;&#22256;&#38590;&#65292;&#20174;&#32780;&#24433;&#21709;&#24739;&#32773;&#25252;&#29702;&#12290;HYKIST&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#20197;&#25903;&#25345;&#24739;&#32773;&#19982;&#21307;&#29983;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#25216;&#26415;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;ASR&#31995;&#32479;&#22312;&#26576;&#20123;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;LibriSpeech&#65292;&#20294;&#30001;&#20110;&#35762;&#35805;&#39118;&#26684;&#12289;&#22768;&#23398;&#21644;&#24405;&#38899;&#35774;&#32622;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#26500;&#24314;&#19968;&#20010;&#33391;&#22909;&#30340;&#27169;&#22411;&#20173;&#28982;&#36739;&#20026;&#22256;&#38590;&#12290;&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#30005;&#35805;&#23545;&#35805;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#26500;&#24314;ASR&#31995;&#32479;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's interconnected globe, moving abroad is more and more prevalent, whether it's for employment, refugee resettlement, or other causes. Language difficulties between natives and immigrants present a common issue on a daily basis, especially in medical domain. This can make it difficult for patients and doctors to communicate during anamnesis or in the emergency room, which compromises patient care. The goal of the HYKIST Project is to develop a speech translation system to support patient-doctor communication with ASR and MT.  ASR systems have recently displayed astounding performance on particular tasks for which enough quantities of training data are available, such as LibriSpeech. Building a good model is still difficult due to a variety of speaking styles, acoustic and recording settings, and a lack of in-domain training data. In this thesis, we describe our efforts to construct ASR systems for a conversational telephone speech recognition task in the medical domain for Viet
&lt;/p&gt;</description></item><item><title>&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20854;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.15857</link><description>&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
A Survey on Image-text Multimodal Models. (arXiv:2309.15857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15857
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20854;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#25104;&#20026;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#23548;&#33268;&#20102;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#26412;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#20215;&#20540;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#21457;&#23637;&#37324;&#31243;&#30865;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#23427;&#20204;&#30340;&#21457;&#23637;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#22522;&#20110;&#23427;&#20204;&#34987;&#24341;&#20837;&#30340;&#26102;&#38388;&#21644;&#23545;&#23398;&#31185;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20219;&#21153;&#22312;&#23398;&#26415;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#26222;&#21450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#19982;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30456;&#20851;&#30340;&#20219;&#21153;&#21010;&#20998;&#20026;&#20116;&#20010;&#20027;&#35201;&#31867;&#22411;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#27599;&#20010;&#31867;&#21035;&#20869;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20851;&#38190;&#25216;&#26415;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#20173;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amidst the evolving landscape of artificial intelligence, the convergence of visual and textual information has surfaced as a crucial frontier, leading to the advent of image-text multimodal models. This paper provides a comprehensive review of the evolution and current state of image-text multimodal models, exploring their application value, challenges, and potential research trajectories. Initially, we revisit the basic concepts and developmental milestones of these models, introducing a novel classification that segments their evolution into three distinct phases, based on their time of introduction and subsequent impact on the discipline. Furthermore, based on the tasks' significance and prevalence in the academic landscape, we propose a categorization of the tasks associated with image-text multimodal models into five major types, elucidating the recent progress and key technologies within each category. Despite the remarkable accomplishments of these models, numerous challenges a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20849;&#21516;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#34701;&#21512;&#35821;&#35328;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#31574;&#30053;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22810;&#27169;&#24577;&#36755;&#20986;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15564</link><description>&lt;p&gt;
&#20849;&#21516;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jointly Training Large Autoregressive Multimodal Models. (arXiv:2309.15564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20849;&#21516;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#34701;&#21512;&#35821;&#35328;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#31574;&#30053;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22810;&#27169;&#24577;&#36755;&#20986;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#35821;&#35328;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20004;&#31181;&#27169;&#24577;&#38598;&#25104;&#21040;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#26080;&#32541;&#22810;&#27169;&#24577;&#36755;&#20986;&#30340;&#21333;&#19968;&#24378;&#22823;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#33258;&#22238;&#24402;&#28151;&#21512;&#65288;JAM&#65289;&#26694;&#26550;&#65292;&#19968;&#31181;&#31995;&#32479;&#34701;&#21512;&#29616;&#26377;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#12289;&#25968;&#25454;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#31574;&#30053;&#65292;&#38024;&#23545;&#28151;&#21512;&#27169;&#24577;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#25105;&#20204;&#26368;&#32456;&#30340;&#35843;&#20248;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22810;&#27169;&#24577;&#36755;&#20986;&#26041;&#38754;&#34920;&#29616;&#20986;&#26080;&#19982;&#20262;&#27604;&#30340;&#24615;&#33021;&#65292;&#24182;&#20195;&#34920;&#20102;&#31532;&#19968;&#20010;&#26126;&#30830;&#20026;&#27492;&#30446;&#30340;&#32780;&#35774;&#35745;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;WavLabLM&#65292;&#23427;&#36890;&#36807;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;136&#31181;&#35821;&#35328;&#30340;40k&#23567;&#26102;&#25968;&#25454;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;WavLabLM&#30340;&#22810;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#30340;&#35821;&#35328;&#22833;&#34913;&#38382;&#39064;&#65292;&#20351;&#20854;&#22312;ML-SUPERB&#19978;&#36798;&#21040;&#20102;&#19982;XLS-R&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20165;&#20351;&#29992;&#19981;&#21040;10%&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;SSL&#22312;&#23398;&#26415;&#39640;&#24615;&#33021;&#35745;&#31639;&#19978;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.15317</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
joint prediction and denoising for large-scale multilingual self-supervised learning. (arXiv:2309.15317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15317
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;WavLabLM&#65292;&#23427;&#36890;&#36807;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;136&#31181;&#35821;&#35328;&#30340;40k&#23567;&#26102;&#25968;&#25454;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;WavLabLM&#30340;&#22810;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#30340;&#35821;&#35328;&#22833;&#34913;&#38382;&#39064;&#65292;&#20351;&#20854;&#22312;ML-SUPERB&#19978;&#36798;&#21040;&#20102;&#19982;XLS-R&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20165;&#20351;&#29992;&#19981;&#21040;10%&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;SSL&#22312;&#23398;&#26415;&#39640;&#24615;&#33021;&#35745;&#31639;&#19978;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#30001;&#20110;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#25152;&#38656;&#30340;&#36153;&#29992;&#21644;&#22797;&#26434;&#24615;&#32780;&#32463;&#24120;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#36825;&#36827;&#19968;&#27493;&#24433;&#21709;&#20102;SSL&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#30001;&#20110;&#36164;&#28304;&#20351;&#29992;&#30340;&#38480;&#21046;&#65292;SSL&#24050;&#32463;&#20165;&#38480;&#20110;&#23569;&#25968;&#30740;&#31350;&#22242;&#38431;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#24378;&#22823;&#30340;&#25216;&#26415;&#23454;&#38469;&#19978;&#21487;&#20197;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#26356;&#22810;&#30340;&#30740;&#31350;&#22242;&#38431;&#33021;&#22815;&#21152;&#20837;SSL&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;WavLabLM&#65292;&#23558;WavLM&#30340;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;&#25193;&#23637;&#21040;136&#31181;&#35821;&#35328;&#30340;40k&#23567;&#26102;&#25968;&#25454;&#12290;&#20026;&#20102;&#26500;&#24314;WavLabLM&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#35821;&#35328;&#25968;&#25454;&#30340;&#35821;&#35328;&#22833;&#34913;&#38382;&#39064;&#12290;WavLabLM&#22312;ML-SUPERB&#19978;&#23454;&#29616;&#20102;&#19982;XLS-R&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20165;&#20351;&#29992;&#19981;&#21040;10%&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#24471;SSL&#22312;&#23398;&#26415;&#39640;&#24615;&#33021;&#35745;&#31639;&#19978;&#21487;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;vanilla HuBERT Base&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36827;&#19968;&#27493;&#30340;&#25928;&#29575;&#25552;&#21319;&#65292;&#20165;&#20351;&#29992;3%&#30340;&#25968;&#25454;&#12289;4&#20010;GPU&#21644;&#26377;&#38480;&#30340;&#35797;&#39564;&#27425;&#25968;&#65292;&#23601;&#33021;&#20445;&#25345;94%&#30340;XLS-R&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This further harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We propose WavLabLM, which extends WavLM's joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the training data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R's performance with only 3% of the data, 4 GPUs, and limited trials. 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;It-LLMs&#65289;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#30340;&#40065;&#26834;&#24615;&#33021;&#21147;&#65292;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12481</link><description>&lt;p&gt;
HANS&#65292;&#20320;&#32874;&#26126;&#21527;&#65311;&#31070;&#32463;&#31995;&#32479;&#30340;Clever Hans&#25928;&#24212;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
HANS, are you clever? Clever Hans Effect Analysis of Neural Systems. (arXiv:2309.12481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12481
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;It-LLMs&#65289;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#30340;&#40065;&#26834;&#24615;&#33021;&#21147;&#65292;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(It-LLMs)&#23637;&#31034;&#20986;&#20102;&#22312;&#35748;&#30693;&#29366;&#24577;&#12289;&#24847;&#22270;&#21644;&#21453;&#24212;&#26041;&#38754;&#25512;&#29702;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#21487;&#20197;&#35753;&#20154;&#20204;&#26377;&#25928;&#22320;&#24341;&#23548;&#21644;&#29702;&#35299;&#26085;&#24120;&#31038;&#20132;&#20114;&#21160;&#12290;&#20107;&#23454;&#19978;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#20010;&#22810;&#39033;&#36873;&#25321;&#39064;(MCQ)&#22522;&#20934;&#26469;&#26500;&#24314;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#30830;&#20999;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#26089;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;It-LLMs&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#8220;&#39034;&#24207;&#20559;&#35265;&#8221;&#65292;&#32473;&#36866;&#24403;&#30340;&#35780;&#20272;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;MCQ&#22522;&#20934;&#23545;It-LLMs&#30340;&#25269;&#25239;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#65292;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#24182;&#24341;&#21457;&#20102;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#35752;&#35770;&#12290;&#36890;&#36807;&#31532;&#19968;&#20301;&#32622;&#21644;&#27169;&#22411;&#36873;&#25321;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#20551;&#35774;&#22312;&#27169;&#22411;&#20013;&#23384;&#22312;&#32467;&#26500;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (It-LLMs) have been exhibiting outstanding abilities to reason around cognitive states, intentions, and reactions of all people involved, letting humans guide and comprehend day-to-day social interactions effectively. In fact, several multiple-choice questions (MCQ) benchmarks have been proposed to construct solid assessments of the models' abilities. However, earlier works are demonstrating the presence of inherent "order bias" in It-LLMs, posing challenges to the appropriate evaluation. In this paper, we investigate It-LLMs' resilience abilities towards a series of probing tests using four MCQ benchmarks. Introducing adversarial examples, we show a significant performance gap, mainly when varying the order of the choices, which reveals a selection bias and brings into discussion reasoning abilities. Following a correlation between first positions and model choices due to positional bias, we hypothesized the presence of structural heuristics in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.10003</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19987;&#21033;&#26435;&#35201;&#27714;&#30340;&#33539;&#22260;&#27979;&#37327;&#20026;&#35813;&#35201;&#27714;&#25152;&#21253;&#21547;&#30340;&#33258;&#20449;&#24687;&#30340;&#20498;&#25968;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20449;&#24687;&#35770;&#65292;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#32597;&#35265;&#30340;&#27010;&#24565;&#27604;&#24179;&#24120;&#30340;&#27010;&#24565;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#22240;&#20026;&#23427;&#26356;&#20196;&#20154;&#24778;&#35766;&#12290;&#33258;&#20449;&#24687;&#26159;&#20174;&#35813;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#35745;&#31639;&#24471;&#20986;&#30340;&#65292;&#20854;&#20013;&#27010;&#29575;&#26159;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20116;&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#65288;&#27599;&#20010;&#21333;&#35789;&#25110;&#23383;&#31526;&#22343;&#20174;&#22343;&#21248;&#20998;&#24067;&#20013;&#25277;&#21462;&#65289;&#21040;&#20013;&#31561;&#27169;&#22411;&#65288;&#20351;&#29992;&#24179;&#22343;&#35789;&#25110;&#23383;&#31526;&#39057;&#29575;&#65289;&#65292;&#20877;&#21040;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT2&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#33539;&#22260;&#24230;&#37327;&#20943;&#23569;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#65292;&#36825;&#26159;&#20808;&#21069;&#20316;&#21697;&#20013;&#24050;&#32463;&#20351;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20061;&#20010;&#31995;&#21015;&#30340;&#38024;&#23545;&#19981;&#21516;&#21457;&#26126;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#65292;&#20854;&#20013;&#27599;&#20010;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes to measure the scope of a patent claim as the reciprocal of the self-information contained in this claim. Grounded in information theory, this approach is based on the assumption that a rare concept is more informative than a usual concept, inasmuch as it is more surprising. The self-information is calculated from the probability of occurrence of that claim, where the probability is calculated in accordance with a language model. Five language models are considered, ranging from the simplest models (each word or character is drawn from a uniform distribution) to intermediate models (using average word or character frequencies), to a large language model (GPT2). Interestingly, the simplest language models reduce the scope measure to the reciprocal of the word or character count, a metric already used in previous works. Application is made to nine series of patent claims directed to distinct inventions, where the claims in each series have a gradually decreasing scope.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.05516</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;LLMs&#37327;&#21270;&#20013;&#30340;&#26435;&#37325;&#33293;&#20837;
&lt;/p&gt;
&lt;p&gt;
Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25191;&#34892;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;3&#20301;&#21644;4&#20301;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#38543;&#30528;&#20301;&#25968;&#30340;&#20943;&#23569;&#65292;&#37327;&#21270;&#32593;&#26684;&#21464;&#24471;&#26356;&#21152;&#23485;&#27867;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19978;&#19979;&#33293;&#20837;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#32454;&#35843;&#19978;&#19979;&#33293;&#20837;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21463;&#21046;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#31934;&#30830;&#19988;&#26377;&#38480;&#30340;&#36793;&#30028;&#65292;&#21482;&#26377;&#25913;&#21464;&#33293;&#20837;&#20540;&#30340;&#38408;&#20540;&#25165;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#39640;&#25928;&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;SignRound&#65292;&#23427;&#28041;&#21450;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#30340;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#23884;&#20837;&#23545;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#24773;&#24863;&#29702;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;HuBERT-Large&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2308.14359</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#23884;&#20837;&#23545;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks. (arXiv:2308.14359v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#23884;&#20837;&#23545;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#24773;&#24863;&#29702;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;HuBERT-Large&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#20013;&#65292;&#20154;&#31867;&#24773;&#24863;&#29702;&#35299;&#22312;&#20351;&#23545;&#35805;&#25216;&#26415;&#25104;&#20026;&#20027;&#27969;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#35821;&#38899;&#24773;&#24863;&#29702;&#35299;&#35270;&#20026;&#19968;&#31181;&#30693;&#35273;&#20219;&#21153;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#29616;&#23454;&#30340;&#24773;&#26223;&#12290;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#65288;&#35821;&#35328;&#65292;&#20154;&#21475;&#32479;&#35745;&#23398;&#31561;&#65289;&#65292;&#19981;&#21516;&#27604;&#20363;&#30340;&#20154;&#20250;&#23558;&#30456;&#21516;&#30340;&#35821;&#38899;&#29255;&#27573;&#35270;&#20026;&#38750;&#19968;&#33268;&#30340;&#24773;&#24863;&#12290;&#20316;&#20026;ACM&#22810;&#23186;&#20307;2023&#35745;&#31639;&#35821;&#38899;&#32852;&#26426;&#25361;&#25112;&#65288;ComParE&#65289;&#30340;&#19968;&#37096;&#20998;&#65292;&#22312;EMotion Share&#36712;&#36947;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#20182;&#20204;&#20016;&#23500;&#30340;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#21644;&#22810;&#26631;&#31614;&#22238;&#24402;&#30446;&#26631;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#8220;&#24773;&#24863;&#20998;&#20139;&#8221;&#25110;&#23545;&#35813;&#24773;&#24863;&#30340;&#24863;&#30693;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#22522;&#30784;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#26696;&#20915;&#23450;&#20102;&#23427;&#20204;&#22312;&#36229;&#36234;&#35821;&#38899;&#35782;&#21035;&#30340;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24773;&#24863;&#29702;&#35299;&#31561;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#65292;&#30446;&#26631;&#26631;&#31614;&#30340;&#21464;&#21270;&#20197;&#21450;&#22238;&#24402;&#25968;&#25454;&#38598;&#20013;&#30340;&#22266;&#26377;&#19981;&#24179;&#34913;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;HuBERT-Large&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human emotion understanding is pivotal in making conversational technology mainstream. We view speech emotion understanding as a perception task which is a more realistic setting. With varying contexts (languages, demographics, etc.) different share of people perceive the same speech segment as a non-unanimous emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset of multilingual speakers and multi-label regression target of 'emotion share' or perception of that emotion. We demonstrate that the training scheme of different foundation models dictates their effectiveness for tasks beyond speech recognition, especially for non-semantic speech tasks like emotion understanding. This is a very complex task due to multilingual speakers, variability in the target labels, and inherent imbalance in the regression dataset. Our results show that HuBERT-Large with a self-attention-based light-weight se
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#24605;&#24819;&#31639;&#27861;"&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31639;&#27861;&#25512;&#29702;&#36335;&#24452;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#24819;&#25506;&#32034;&#65292;&#20197;&#20302;&#25104;&#26412;&#12289;&#20302;&#23384;&#20648;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#31639;&#27861;&#25351;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#36229;&#36234;&#31639;&#27861;&#26412;&#36523;&#12290;</title><link>http://arxiv.org/abs/2308.10379</link><description>&lt;p&gt;
&#24605;&#24819;&#31639;&#27861;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#24819;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models. (arXiv:2308.10379v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#24605;&#24819;&#31639;&#27861;"&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31639;&#27861;&#25512;&#29702;&#36335;&#24452;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#24819;&#25506;&#32034;&#65292;&#20197;&#20302;&#25104;&#26412;&#12289;&#20302;&#23384;&#20648;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#31639;&#27861;&#25351;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#36229;&#36234;&#31639;&#27861;&#26412;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25991;&#29486;&#26088;&#22312;&#36229;&#36234;&#8220;&#36830;&#32493;&#24605;&#32500;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#37319;&#29992;&#22806;&#37096;&#25805;&#20316;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20572;&#27490;&#12289;&#20462;&#25913;&#65292;&#28982;&#21518;&#24674;&#22797;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#27169;&#24335;&#22686;&#21152;&#20102;&#26597;&#35810;&#35831;&#27714;&#30340;&#25968;&#37327;&#65292;&#22686;&#21152;&#20102;&#25104;&#26412;&#12289;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24605;&#24819;&#31639;&#27861;&#8212;&#8212;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31639;&#27861;&#25512;&#29702;&#36335;&#24452;&#25512;&#21160;LLM&#65292;&#24320;&#21019;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#24335;&#12290;&#36890;&#36807;&#20351;&#29992;&#31639;&#27861;&#31034;&#20363;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#30340;&#22266;&#26377;&#24490;&#29615;&#21160;&#21147;&#23398;&#65292;&#20165;&#20351;&#29992;&#19968;&#20010;&#25110;&#23569;&#25968;&#20960;&#20010;&#26597;&#35810;&#25193;&#23637;&#20854;&#24605;&#24819;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#20248;&#20110;&#26089;&#26399;&#30340;&#21333;&#27425;&#26597;&#35810;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#36817;&#37319;&#29992;&#24191;&#27867;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#30340;&#22810;&#27425;&#26597;&#35810;&#31574;&#30053;&#19981;&#30456;&#19978;&#19979;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#31639;&#27861;&#25351;&#23548;LLM&#21487;&#20197;&#20351;&#24615;&#33021;&#36229;&#36234;&#31639;&#27861;&#26412;&#36523;&#65292;&#36825;&#26263;&#31034;&#30528;
&lt;/p&gt;
&lt;p&gt;
Current literature, aiming to surpass the "Chain-of-Thought" approach, often resorts to an external modus operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. This mode escalates the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways, pioneering a new mode of in-context learning. By employing algorithmic examples, we exploit the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and stands on par with a recent multi-query strategy that employs an extensive tree search algorithm. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting 
&lt;/p&gt;</description></item><item><title>VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.06595</link><description>&lt;p&gt;
VisIT-Bench: &#19968;&#20010;&#21463;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#21551;&#21457;&#30340;&#35270;&#35273;&#35821;&#35328;&#25351;&#31034;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06595
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;VisIT-Bench&#65288;Visual InsTruction Benchmark&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20215;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#31034;&#36981;&#24490;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#36215;&#28857;&#26159;&#31574;&#21010;&#20102;70&#20010;&#8220;&#25351;&#31034;&#23478;&#26063;&#8221;&#65292;&#25105;&#20204;&#35748;&#20026;&#25351;&#31034;&#35843;&#20248;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#23478;&#26063;&#12290;&#20219;&#21153;&#19981;&#20165;&#38480;&#20110;VQAv2&#21644;COCO&#31561;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20174;&#22522;&#26412;&#35782;&#21035;&#21040;&#28216;&#25103;&#29609;&#27861;&#21644;&#21019;&#36896;&#24615;&#29983;&#25104;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#22312;&#31574;&#21010;&#20043;&#21518;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;592&#20010;&#27979;&#35797;&#26597;&#35810;&#65292;&#27599;&#20010;&#26597;&#35810;&#37117;&#24102;&#26377;&#19968;&#20010;&#20154;&#24037;&#32534;&#20889;&#30340;&#25351;&#31034;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#12290;&#36825;&#20123;&#25551;&#36848;&#23637;&#29616;&#20102;&#29305;&#23450;&#25351;&#31034;&#22240;&#32032;&#65292;&#20363;&#22914;&#23545;&#20110;&#35810;&#38382;&#24215;&#38754;&#23545;&#20110;&#36718;&#26885;&#29992;&#25143;&#30340;&#26131;&#35775;&#38382;&#24615;&#30340;&#25351;&#31034;&#65292;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#25551;&#36848;&#20102;&#26012;&#22369;/&#28508;&#22312;&#38556;&#30861;&#29289;&#12290;&#36825;&#20123;&#25551;&#36848;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#65306;1&#65289;&#25910;&#38598;&#27599;&#20010;&#23454;&#20363;&#30340;&#20154;&#24037;&#39564;&#35777;&#30340;&#21442;&#32771;&#36755;&#20986;&#65307;2&#65289;&#20351;&#29992;&#20165;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20505;&#36873;&#22810;&#27169;&#24577;&#29983;&#25104;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30340;&#21019;&#26032;&#26159;&#22522;&#20110;Hapax Rate&#27169;&#22411;&#24341;&#20837;&#20102;&#23545;Zipf&#21644;Heaps&#23450;&#24459;&#30340;&#20462;&#27491;&#65292;&#24182;&#21457;&#29616;&#36923;&#36753;&#27169;&#22411;&#25311;&#21512;&#25928;&#26524;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2307.12896</link><description>&lt;p&gt;
&#20174;Hapax Rate&#27169;&#22411;&#23548;&#20986;&#30340;Zipf&#21644;Heaps&#23450;&#24459;&#30340;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Corrections of Zipf's and Heaps' Laws Derived from Hapax Rate Models. (arXiv:2307.12896v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#21019;&#26032;&#26159;&#22522;&#20110;Hapax Rate&#27169;&#22411;&#24341;&#20837;&#20102;&#23545;Zipf&#21644;Heaps&#23450;&#24459;&#30340;&#20462;&#27491;&#65292;&#24182;&#21457;&#29616;&#36923;&#36753;&#27169;&#22411;&#25311;&#21512;&#25928;&#26524;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;Hapax Rate&#27169;&#22411;&#24341;&#20837;&#20102;&#23545;Zipf&#21644;Heaps&#23450;&#24459;&#30340;&#20462;&#27491;&#12290;&#25512;&#23548;&#22522;&#20110;&#20004;&#20010;&#20551;&#35774;&#65306;&#31532;&#19968;&#20010;&#20551;&#35774;&#26159;&#26631;&#20934;&#30340;&#29934;&#27169;&#22411;&#65292;&#39044;&#27979;&#36739;&#30701;&#25991;&#26412;&#30340;&#36793;&#38469;&#35789;&#39057;&#20998;&#24067;&#30475;&#36215;&#26469;&#23601;&#20687;&#26159;&#20174;&#19968;&#20010;&#32473;&#23450;&#30340;&#36739;&#38271;&#25991;&#26412;&#20013;&#30450;&#30446;&#37319;&#26679;&#35789;&#20803;&#12290;&#31532;&#20108;&#20010;&#20551;&#35774;&#20551;&#23450;Hapax&#30340;&#39057;&#29575;&#26159;&#25991;&#26412;&#22823;&#23567;&#30340;&#31616;&#21333;&#20989;&#25968;&#12290;&#35752;&#35770;&#20102;&#22235;&#20010;&#36825;&#26679;&#30340;&#20989;&#25968;&#65306;&#24120;&#25968;&#27169;&#22411;&#12289;Davis&#27169;&#22411;&#12289;&#32447;&#24615;&#27169;&#22411;&#21644;&#36923;&#36753;&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#36923;&#36753;&#27169;&#22411;&#25311;&#21512;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The article introduces corrections to Zipf's and Heaps' laws based on systematic models of the hapax rate. The derivation rests on two assumptions: The first one is the standard urn model which predicts that marginal frequency distributions for shorter texts look as if word tokens were sampled blindly from a given longer text. The second assumption posits that the rate of hapaxes is a simple function of the text size. Four such functions are discussed: the constant model, the Davis model, the linear model, and the logistic model. It is shown that the logistic model yields the best fit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#25968;&#25454;&#38598;CoLAC&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#20302;&#20110;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#36328;&#35821;&#35328;&#36716;&#31227;&#21487;&#34892;&#65292;&#24182;&#21487;&#20197;&#36861;&#28335;&#21040;&#39044;&#35757;&#32451;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2305.14091</link><description>&lt;p&gt;
&#37325;&#28201;&#25509;&#21463;&#24615;&#21028;&#26029;
&lt;/p&gt;
&lt;p&gt;
Revisiting Acceptability Judgements. (arXiv:2305.14091v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#25968;&#25454;&#38598;CoLAC&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#20302;&#20110;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#36328;&#35821;&#35328;&#36716;&#31227;&#21487;&#34892;&#65292;&#24182;&#21487;&#20197;&#36861;&#28335;&#21040;&#39044;&#35757;&#32451;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33267;NLP&#31038;&#21306;&#26368;&#21518;&#19968;&#27425;&#20851;&#27880;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#24050;&#32463;&#36807;&#21435;&#22810;&#24180;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#37325;&#28201;&#36825;&#20010;&#35805;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; CoLAC-&#20013;&#25991;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30001;&#27597;&#35821;&#35762;&#32773;&#39564;&#35777;&#24182;&#24102;&#26377;&#20004;&#32452;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#38750;&#33521;&#35821;&#21487;&#25509;&#21463;&#24615;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26368;&#22823;&#30340;InstructGPT&#27169;&#22411;&#22312;CoLAC&#19978;&#20063;&#21482;&#33021;&#34920;&#29616;&#38543;&#26426;&#27700;&#24179;&#65292;&#32780;ChatGPT&#30340;&#24615;&#33021;&#65288;48.30 MCC&#65289;&#20063;&#36828;&#20302;&#20110;&#30417;&#30563;&#27169;&#22411;&#65288;59.03 MCC&#65289;&#21644;&#20154;&#31867;&#65288;65.11 MCC&#65289;&#12290;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#23454;&#39564;&#21644;&#32454;&#31890;&#24230;&#30340;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#30693;&#35782;&#21487;&#20197;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#20043;&#38388;&#36716;&#31227;&#65292;&#32780;&#19988;&#21487;&#20197;&#36861;&#28335;&#21040;&#39044;&#35757;&#32451;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Years have passed since the NLP community has last focused on linguistic acceptability. In this work, we revisit this topic in the context of large language models. We introduce CoLAC - Corpus of Linguistic Acceptability in Chinese, the first large-scale non-English acceptability dataset that is verified by native speakers and comes with two sets of labels. Our experiments show that even the largest InstructGPT model performs only at chance level on CoLAC, while ChatGPT's performance (48.30 MCC) is also way below supervised models (59.03 MCC) and human (65.11 MCC). Through cross-lingual transfer experiments and fine-grained linguistic analysis, we demonstrate for the first time that knowledge of linguistic acceptability can be transferred across typologically distinct languages, as well as be traced back to pre-training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21517;&#20026;LLM-Pruner&#65292;&#37319;&#29992;&#32467;&#26500;&#20462;&#21098;&#30340;&#26041;&#24335;&#22312;&#20445;&#30041;&#22823;&#22810;&#25968;&#21151;&#33021;&#30340;&#21516;&#26102;&#65292;&#21387;&#32553;LLM&#30340;&#32467;&#26500;&#65292;&#20197;&#20943;&#23569;LLM&#22312;&#37096;&#32626;&#12289;&#25512;&#29702;&#21644;&#35757;&#32451;&#38454;&#27573;&#20013;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11627</link><description>&lt;p&gt;
LLM-Pruner: &#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#20462;&#21098;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM-Pruner: On the Structural Pruning of Large Language Models. (arXiv:2305.11627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21517;&#20026;LLM-Pruner&#65292;&#37319;&#29992;&#32467;&#26500;&#20462;&#21098;&#30340;&#26041;&#24335;&#22312;&#20445;&#30041;&#22823;&#22810;&#25968;&#21151;&#33021;&#30340;&#21516;&#26102;&#65292;&#21387;&#32553;LLM&#30340;&#32467;&#26500;&#65292;&#20197;&#20943;&#23569;LLM&#22312;&#37096;&#32626;&#12289;&#25512;&#29702;&#21644;&#35757;&#32451;&#38454;&#27573;&#20013;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20986;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#20196;&#20154;&#24778;&#35766;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#36890;&#24120;&#20276;&#38543;&#30528;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#36825;&#22312;&#37096;&#32626;&#12289;&#25512;&#29702;&#21644;&#35757;&#32451;&#38454;&#27573;&#37117;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;&#25105;&#20204;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#25506;&#32034;&#20102;LLM&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#20445;&#30041;&#21407;&#22987;LLM&#30340;&#22810;&#20219;&#21153;&#35299;&#20915;&#21644;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32467;&#26500;&#20462;&#21098;&#65292;&#22312;&#26799;&#24230;&#20449;&#24687;&#30340;&#25903;&#25345;&#19979;&#36873;&#25321;&#24615;&#22320;&#31227;&#38500;&#38750;&#20851;&#38190;&#30340;&#32806;&#21512;&#32467;&#26500;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20445;&#30041;&#20102;&#22823;&#22810;&#25968;LLM&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionalit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#30340;&#21477;&#27861;&#22810;&#26679;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#23618;&#21487;&#35270;&#21270;&#26469;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#27983;&#35272;&#27010;&#36848;&#21644;&#26816;&#26597;&#21508;&#20010;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.11364</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models. (arXiv:2305.11364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#30340;&#21477;&#27861;&#22810;&#26679;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#23618;&#21487;&#35270;&#21270;&#26469;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#27983;&#35272;&#27010;&#36848;&#21644;&#26816;&#26597;&#21508;&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#29983;&#25104;&#26356;&#31934;&#32454;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#12289;&#24494;&#35843;&#25110;&#20854;&#20182;&#29992;&#20363;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#21644;&#35780;&#20272;&#36825;&#20123;&#25968;&#25454;&#38598;&#24456;&#22256;&#38590;&#65292;&#32780;LLM&#29983;&#25104;&#25968;&#25454;&#30340;&#22833;&#36133;&#27169;&#24335;&#20173;&#19981;&#20026;&#20154;&#20204;&#25152;&#29702;&#35299;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25968;&#25454;&#21487;&#33021;&#20197;&#24847;&#22806;&#30340;&#26041;&#24335;&#21464;&#24471;&#37325;&#22797;&#65292;&#19981;&#20165;&#35821;&#20041;&#19978;&#22914;&#27492;&#65292;&#32780;&#19988;&#22312;&#21477;&#27861;&#12289;&#35789;&#27719;&#26041;&#38754;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LinguisticLens&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#20998;&#26512;LLM&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#21477;&#27861;&#22810;&#26679;&#24615;&#12290; LinguisticLens&#27839;&#30528;&#21477;&#27861;&#12289;&#35789;&#27719;&#21644;&#35821;&#20041;&#36724;&#23558;&#25991;&#26412;&#32858;&#31867;&#12290;&#23427;&#25903;&#25345;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#20998;&#23618;&#21487;&#35270;&#21270;&#65292;&#20801;&#35768;&#29992;&#25143;&#24555;&#36895;&#27983;&#35272;&#27010;&#36848;&#21644;&#26816;&#26597;&#21508;&#20010;&#31034;&#20363;&#12290;&#23454;&#26102;&#28436;&#31034;&#21487;&#22312;shorturl.at/zHOUV&#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can be used to generate smaller, more refined datasets via few-shot prompting for benchmarking, fine-tuning or other use cases. However, understanding and evaluating these datasets is difficult, and the failure modes of LLM-generated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically. We present LinguisticLens, a novel inter-active visualization tool for making sense of and analyzing syntactic diversity of LLM-generated datasets. LinguisticLens clusters text along syntactic, lexical, and semantic axes. It supports hierarchical visualization of a text dataset, allowing users to quickly scan for an overview and inspect individual examples. The live demo is available at shorturl.at/zHOUV.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24418;&#24577;&#20449;&#24687;&#22312;&#20845;&#31181;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#24320;&#21457;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23545;&#35789;&#24418;&#36824;&#21407;&#22120;&#36827;&#34892;&#20102;&#39046;&#22495;&#22806;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2302.00407</link><description>&lt;p&gt;
&#20851;&#20110;&#24418;&#24577;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Morphological Information for Contextual Lemmatization. (arXiv:2302.00407v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24418;&#24577;&#20449;&#24687;&#22312;&#20845;&#31181;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#24320;&#21457;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23545;&#35789;&#24418;&#36824;&#21407;&#22120;&#36827;&#34892;&#20102;&#39046;&#22495;&#22806;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#24418;&#36824;&#21407;&#26159;&#19968;&#39033;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20219;&#21153;&#65292;&#23427;&#21253;&#25324;&#20174;&#32473;&#23450;&#30340;&#23624;&#25240;&#35789;&#29983;&#25104;&#20854;&#35268;&#33539;&#24418;&#24335;&#25110;&#35789;&#24418;&#36824;&#21407;&#12290;&#35789;&#24418;&#36824;&#21407;&#26159;&#31616;&#21270;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#23545;&#20110;&#39640;&#24230;&#23624;&#25240;&#35821;&#35328;&#23588;&#20026;&#37325;&#35201;&#12290;&#34429;&#28982;&#26681;&#25454;&#23624;&#25240;&#35789;&#33719;&#24471;&#35789;&#24418;&#36824;&#21407;&#24418;&#24335;&#30340;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#20854;&#24418;&#24577;&#21477;&#27861;&#31867;&#21035;&#26469;&#35299;&#37322;&#65292;&#20294;&#22312;&#35757;&#32451;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#26102;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#24418;&#24577;&#21477;&#27861;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#24120;&#35265;&#20570;&#27861;&#65292;&#32780;&#26080;&#35270;&#19979;&#28216;&#32489;&#25928;&#26159;&#21542;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#20102;&#22312;&#20845;&#31181;&#35821;&#35328;&#20013;&#32771;&#23519;&#24418;&#24577;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#24320;&#21457;&#20013;&#30340;&#20316;&#29992;&#65292;&#36825;&#20845;&#31181;&#35821;&#35328;&#30340;&#24418;&#24577;&#22797;&#26434;&#24615;&#21508;&#19981;&#30456;&#21516;&#65292;&#21253;&#25324;&#24052;&#26031;&#20811;&#35821;&#12289;&#22303;&#32819;&#20854;&#35821;&#12289;&#20420;&#35821;&#12289;&#25463;&#20811;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#33521;&#35821;&#12290;&#27492;&#22806;&#65292;&#19982;&#32477;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#36824;&#22312;&#39046;&#22495;&#22806;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35789;&#24418;&#36824;&#21407;&#22120;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lemmatization is a natural language processing (NLP) task which consists of producing, from a given inflected word, its canonical form or lemma. Lemmatization is one of the basic tasks that facilitate downstream NLP applications, and is of particular importance for high-inflected languages. Given that the process to obtain a lemma from an inflected word can be explained by looking at its morphosyntactic category, including fine-grained morphosyntactic information to train contextual lemmatizers has become common practice, without considering whether that is the optimum in terms of downstream performance. In order to address this issue, in this paper we empirically investigate the role of morphological information to develop contextual lemmatizers in six languages within a varied spectrum of morphological complexity: Basque, Turkish, Russian, Czech, Spanish and English. Furthermore, and unlike the vast majority of previous work, we also evaluate lemmatizers in out-of-domain settings, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23545;&#35805;&#20013;&#23454;&#20307;&#38142;&#25509;&#30340;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#35805;&#35821;&#20013;&#30340;&#20010;&#20154;&#23454;&#20307;&#21644;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#19982;&#22806;&#37096;&#30693;&#35782;&#36830;&#25509;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2206.07836</link><description>&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#20010;&#20154;&#23454;&#20307;&#12289;&#27010;&#24565;&#21644;&#21629;&#21517;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Personal Entity, Concept, and Named Entity Linking in Conversations. (arXiv:2206.07836v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23545;&#35805;&#20013;&#23454;&#20307;&#38142;&#25509;&#30340;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#35805;&#35821;&#20013;&#30340;&#20010;&#20154;&#23454;&#20307;&#21644;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#19982;&#22806;&#37096;&#30693;&#35782;&#36830;&#25509;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#21487;&#20197;&#19982;&#20154;&#31867;&#36827;&#34892;&#33258;&#28982;&#19988;&#22522;&#20110;&#30693;&#35782;&#30340;&#20114;&#21160;&#30340;&#23545;&#35805;&#20195;&#29702;&#38656;&#35201;&#29702;&#35299;&#29992;&#25143;&#30340;&#35805;&#35821;&#12290;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26159;&#19968;&#31181;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#24182;&#23558;&#20854;&#19982;&#22806;&#37096;&#30693;&#35782;&#36830;&#25509;&#30340;&#26377;&#25928;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24050;&#26377;&#30340;&#29992;&#20110;&#25991;&#26723;&#27880;&#37322;&#30340;EL&#26041;&#27861;&#22312;&#23545;&#35805;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23545;&#20110;&#29702;&#35299;&#29992;&#25143;&#30340;&#35805;&#35821;&#26469;&#35828;&#65292;&#20010;&#20154;&#23454;&#20307;&#65288;&#20363;&#22914;&#65292;&#8220;&#25105;&#30340;&#27773;&#36710;&#8221;&#65289;&#21644;&#27010;&#24565;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23545;&#35805;&#20013;&#23454;&#20307;&#38142;&#25509;&#30340;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#12290;&#25105;&#20204;&#20026;1327&#20010;&#23545;&#35805;&#35805;&#35821;&#25910;&#38598;&#20102;EL&#27880;&#37322;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#12289;&#27010;&#24565;&#21644;&#20010;&#20154;&#23454;&#20307;&#30340;&#38142;&#25509;&#12290;&#35813;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#25105;&#20204;&#30340;&#23545;&#35805;&#23454;&#20307;&#38142;&#25509;&#24037;&#20855;CREL&#12290;&#19982;&#29616;&#26377;&#30340;EL&#26041;&#27861;&#19981;&#21516;&#65292;CREL&#26088;&#22312;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#21644;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#20849;&#25351;&#28040;&#35299;&#25216;&#26415;&#35782;&#21035;&#20010;&#20154;&#23454;&#20307;&#21644;&#24341;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building conversational agents that can have natural and knowledge-grounded interactions with humans requires understanding user utterances. Entity Linking (EL) is an effective and widely used method for understanding natural language text and connecting it to external knowledge. It is, however, shown that existing EL methods developed for annotating documents are suboptimal for conversations, where personal entities (e.g., "my cars") and concepts are essential for understanding user utterances. In this paper, we introduce a collection and a tool for entity linking in conversations. We collect EL annotations for 1327 conversational utterances, consisting of links to named entities, concepts, and personal entities. The dataset is used for training our toolkit for conversational entity linking, CREL. Unlike existing EL methods, CREL is developed to identify both named entities and concepts. It also utilizes coreference resolution techniques to identify personal entities and references to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31639;&#27861;&#26469;&#35780;&#20272;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#26426;&#20851;&#20110;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#39640;&#38454;&#23548;&#25968;&#65292;&#24182;&#19988;&#22312;&#20108;&#38454;&#23548;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#26041;&#26696;&#30340;&#36816;&#34892;&#26102;&#38388;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#23548;&#33268;&#20102;&#35745;&#31639;&#20108;&#38454;&#26399;&#26395;&#30340;&#26356;&#24555;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.00749</link><description>&lt;p&gt;
&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#39640;&#38454;&#23548;&#25968;
&lt;/p&gt;
&lt;p&gt;
Higher-order Derivatives of Weighted Finite-state Machines. (arXiv:2106.00749v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.00749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31639;&#27861;&#26469;&#35780;&#20272;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#26426;&#20851;&#20110;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#39640;&#38454;&#23548;&#25968;&#65292;&#24182;&#19988;&#22312;&#20108;&#38454;&#23548;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#26041;&#26696;&#30340;&#36816;&#34892;&#26102;&#38388;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#23548;&#33268;&#20102;&#35745;&#31639;&#20108;&#38454;&#26399;&#26395;&#30340;&#26356;&#24555;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#26426;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#12290;&#20174;&#23427;&#20204;&#22312;20&#19990;&#32426;90&#24180;&#20195;&#26089;&#26399;&#29992;&#20110;&#22122;&#22768;&#20449;&#36947;&#27169;&#22411;&#24320;&#22987;&#65292;&#19968;&#30452;&#21040;&#29616;&#20195;&#31070;&#32463;&#21442;&#25968;&#21270;&#26465;&#20214;&#38543;&#26426;&#22330;&#30340;&#20351;&#29992;&#65292;&#23427;&#20204;&#32463;&#21463;&#20102;&#26102;&#38388;&#30340;&#32771;&#39564;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#20851;&#20110;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#26426;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#39640;&#38454;&#23548;&#25968;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;&#25152;&#26377;&#38454;&#25968;&#23548;&#25968;&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#36825;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#25551;&#36848;&#12290;&#22312;&#20108;&#38454;&#23548;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#26368;&#20339;&#30340; \mathcal{O}(A^2 N^4) &#26102;&#38388;&#20869;&#36816;&#34892;&#65292;&#20854;&#20013; A &#26159;&#23383;&#27597;&#34920;&#22823;&#23567;&#65292;N &#26159;&#29366;&#24577;&#25968;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#27604;&#20043;&#21069;&#30340;&#31639;&#27861;&#24555;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#19968;&#20010;&#35745;&#31639;&#20108;&#38454;&#26399;&#26395;&#30340;&#26174;&#33879;&#26356;&#24555;&#30340;&#31639;&#27861;&#65292;&#20363;&#22914;&#21327;&#26041;&#24046;&#30697;&#38453;&#21644;&#19968;&#38454;&#26399;&#26395;&#30340;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time -- from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal $\mathcal{O}(A^2 N^4)$ time where $A$ is the alphabet size and $N$ is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations.
&lt;/p&gt;</description></item></channel></rss>