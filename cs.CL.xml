<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>LLMs&#22312;&#21307;&#23398;&#20915;&#31574;&#20013;&#25552;&#20379;&#37325;&#35201;&#21453;&#39304;&#65292;&#21487;&#20197;&#25361;&#25112;&#19981;&#27491;&#30830;&#30340;&#35786;&#26029;&#65292;&#20419;&#36827;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.20288</link><description>&lt;p&gt;
LLM&#33021;&#22815;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#32416;&#27491;&#21307;&#29983;&#21527;&#65311;&#30740;&#31350;&#26377;&#25928;&#30340;&#20132;&#20114;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20288
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#21307;&#23398;&#20915;&#31574;&#20013;&#25552;&#20379;&#37325;&#35201;&#21453;&#39304;&#65292;&#21487;&#20197;&#25361;&#25112;&#19981;&#27491;&#30830;&#30340;&#35786;&#26029;&#65292;&#20419;&#36827;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21327;&#21161;&#24182;&#21487;&#33021;&#32416;&#27491;&#21307;&#29983;&#36827;&#34892;&#21307;&#30103;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;LLMs&#65292;&#21253;&#25324;Meditron&#12289;Llama2&#21644;Mistral&#65292;&#20998;&#26512;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#19982;&#21307;&#29983;&#26377;&#25928;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;PubMedQA&#30340;&#38382;&#39064;&#21644;&#20960;&#39033;&#20219;&#21153;&#65292;&#20174;&#20108;&#20803;&#65288;&#26159;/&#21542;&#65289;&#22238;&#31572;&#21040;&#38271;&#31572;&#26696;&#29983;&#25104;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#31572;&#26696;&#26159;&#22312;&#19982;&#21307;&#29983;&#20132;&#20114;&#21518;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#31034;&#35774;&#35745;&#26174;&#33879;&#24433;&#21709;&#20102;LLMs&#30340;&#19979;&#28216;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;LLMs&#21487;&#20197;&#20026;&#21307;&#29983;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21453;&#39304;&#65292;&#25361;&#25112;&#19981;&#27491;&#30830;&#30340;&#35786;&#26029;&#65292;&#20419;&#36827;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#20363;&#22914;&#65292;&#24403;&#21307;&#29983;&#20934;&#30830;&#29575;&#20026;38%&#26102;&#65292;Mistral&#21487;&#20197;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#26681;&#25454;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#65292;&#23558;&#20934;&#30830;&#24615;&#25552;&#39640;&#21040;74%&#65292;&#32780;Llama2&#21644;Meditron&#27169;&#22411;&#20063;&#33021;&#25552;&#20379;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20288v1 Announce Type: cross  Abstract: We explore the potential of Large Language Models (LLMs) to assist and potentially correct physicians in medical decision-making tasks. We evaluate several LLMs, including Meditron, Llama2, and Mistral, to analyze the ability of these models to interact effectively with physicians across different scenarios. We consider questions from PubMedQA and several tasks, ranging from binary (yes/no) responses to long answer generation, where the answer of the model is produced after an interaction with a physician. Our findings suggest that prompt design significantly influences the downstream accuracy of LLMs and that LLMs can provide valuable feedback to physicians, challenging incorrect diagnoses and contributing to more accurate decision-making. For example, when the physician is accurate 38% of the time, Mistral can produce the correct answer, improving accuracy up to 74% depending on the prompt being used, while Llama2 and Meditron models
&lt;/p&gt;</description></item><item><title>MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;</title><link>https://arxiv.org/abs/2403.17141</link><description>&lt;p&gt;
MetaAligner&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#36890;&#29992;&#22810;&#30446;&#26631;&#23545;&#40784;&#30340;&#26465;&#20214;&#20174;&#24369;&#21040;&#24378;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17141
&lt;/p&gt;
&lt;p&gt;
MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#26088;&#22312;&#36890;&#36807;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26469;&#35299;&#20915;&#24322;&#36136;&#20154;&#31867;&#26399;&#26395;&#21644;&#20215;&#20540;&#35266;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#21463;&#21040;&#31574;&#30053;&#27169;&#22411;&#30340;&#21442;&#25968;&#38480;&#21046;&#65292;&#23548;&#33268;&#20004;&#20010;&#20851;&#38190;&#23616;&#38480;&#24615;&#65306;&#65288;1&#65289;&#23427;&#20204;&#30340;&#23545;&#40784;&#31639;&#27861;&#23545;&#20110;&#27599;&#20010;&#26032;&#30446;&#26631;&#27169;&#22411;&#30340;&#37325;&#22797;&#25104;&#26412;&#24456;&#39640;&#65307;&#65288;2&#65289;&#30001;&#20110;&#20854;&#38745;&#24577;&#23545;&#40784;&#30446;&#26631;&#65292;&#23427;&#20204;&#26080;&#27861;&#25193;&#23637;&#21040;&#26410;&#35265;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-Objective Aligner&#65288;MetaAligner&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25191;&#34892;&#26465;&#20214;&#20174;&#24369;&#21040;&#24378;&#26657;&#27491;&#20197;&#36924;&#36817;&#24378;&#21709;&#24212;&#30340;&#27169;&#22411;&#12290;MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MetaAligner&#21462;&#24471;&#20102;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17141v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) aim to tackle heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are parameter-adherent to the policy model, leading to two key limitations: (1) the high-cost repetition of their alignment algorithms for each new target model; (2) they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), a model that performs conditional weak-to-strong correction for weak responses to approach strong responses. MetaAligner is the first policy-agnostic and generalizable method for multi-objective preference alignment, which enables plug-and-play alignment by decoupling parameter updates from the policy models and facilitates zero-shot preference alignment for unseen objectives via in-context learning. Experimental results show that MetaAligner achieves sign
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;&#21644;&#24809;&#32602;&#24187;&#35273;&#26631;&#35760;&#65292;ESREAL&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#20041;&#37325;&#24314;&#26469;&#25233;&#21046;&#29983;&#25104;&#24187;&#35273;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16167</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#37325;&#24314;&#20943;&#23569;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16167
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;&#21644;&#24809;&#32602;&#24187;&#35273;&#26631;&#35760;&#65292;ESREAL&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#20041;&#37325;&#24314;&#26469;&#25233;&#21046;&#29983;&#25104;&#24187;&#35273;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#23545;&#20854;&#21487;&#38752;&#24615;&#26500;&#25104;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#38271;&#26631;&#39064;&#26102;&#12290;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#21644;&#20943;&#36731;&#36825;&#20123;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ESREAL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;&#21644;&#24809;&#32602;&#24187;&#35273;&#26631;&#35760;&#26469;&#25233;&#21046;&#24187;&#35273;&#29983;&#25104;&#12290;&#26368;&#21021;&#65292;ESREAL&#26681;&#25454;&#29983;&#25104;&#30340;&#26631;&#39064;&#21019;&#24314;&#19968;&#20010;&#37325;&#24314;&#22270;&#20687;&#65292;&#24182;&#23558;&#20854;&#23545;&#24212;&#21306;&#22495;&#19982;&#21407;&#22987;&#22270;&#20687;&#30340;&#21306;&#22495;&#23545;&#40784;&#12290;&#36825;&#31181;&#35821;&#20041;&#37325;&#24314;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#26631;&#39064;&#20013;&#30340;&#26631;&#35760;&#32423;&#24187;&#35273;&#30340;&#23384;&#22312;&#21644;&#31867;&#22411;&#12290;&#38543;&#21518;&#65292;ESREAL&#36890;&#36807;&#35780;&#20272;&#23545;&#40784;&#21306;&#22495;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35745;&#31639;&#26631;&#35760;&#32423;&#24187;&#35273;&#20998;&#25968;&#65292;&#22522;&#20110;&#24187;&#35273;&#30340;&#31867;&#22411;&#12290;&#26368;&#21518;&#65292;ESREAL&#37319;&#29992;&#19968;&#31181;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36827;&#34892;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16167v1 Announce Type: cross  Abstract: Hallucinations in vision-language models pose a significant challenge to their reliability, particularly in the generation of long captions. Current methods fall short of accurately identifying and mitigating these hallucinations. To address this issue, we introduce ESREAL, a novel unsupervised learning framework designed to suppress the generation of hallucinations through accurate localization and penalization of hallucinated tokens. Initially, ESREAL creates a reconstructed image based on the generated caption and aligns its corresponding regions with those of the original image. This semantic reconstruction aids in identifying both the presence and type of token-level hallucinations within the generated caption. Subsequently, ESREAL computes token-level hallucination scores by assessing the semantic similarity of aligned regions based on the type of hallucination. Finally, ESREAL employs a proximal policy optimization algorithm, wh
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31639;&#26415;&#30005;&#36335;&#32422;&#26463;&#32534;&#30721;&#20026;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#35745;&#31639;&#22312;&#26377;&#38480;&#22495;&#19978;&#35299;&#20915;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#20197;&#31934;&#30830;&#23450;&#20301;ZKP&#30005;&#36335;&#20013;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.15676</link><description>&lt;p&gt;
AC4&#65306;&#29992;&#20110;ZKP&#20013;&#30005;&#36335;&#32422;&#26463;&#30340;&#20195;&#25968;&#35745;&#31639;&#26816;&#26597;&#22120;
&lt;/p&gt;
&lt;p&gt;
AC4: Algebraic Computation Checker for Circuit Constraints in ZKPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31639;&#26415;&#30005;&#36335;&#32422;&#26463;&#32534;&#30721;&#20026;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#35745;&#31639;&#22312;&#26377;&#38480;&#22495;&#19978;&#35299;&#20915;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#20197;&#31934;&#30830;&#23450;&#20301;ZKP&#30005;&#36335;&#20013;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ZKP&#31995;&#32479;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#22312;&#24403;&#20195;&#23494;&#30721;&#23398;&#20013;&#21457;&#25381;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290; Zk-SNARK&#21327;&#35758;&#20027;&#23548;&#20102;ZKP&#30340;&#20351;&#29992;&#65292;&#36890;&#24120;&#36890;&#36807;&#31639;&#26415;&#30005;&#36335;&#32534;&#31243;&#33539;&#24335;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#27424;&#32422;&#26463;&#25110;&#36807;&#32422;&#26463;&#30340;&#30005;&#36335;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#12290; &#27424;&#32422;&#26463;&#30340;&#30005;&#36335;&#25351;&#30340;&#26159;&#32570;&#20047;&#24517;&#35201;&#32422;&#26463;&#30340;&#30005;&#36335;&#65292;&#23548;&#33268;&#30005;&#36335;&#20013;&#20986;&#29616;&#24847;&#22806;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23548;&#33268;&#39564;&#35777;&#32773;&#25509;&#21463;&#38169;&#35823;&#35265;&#35777;&#12290; &#36807;&#32422;&#26463;&#30340;&#30005;&#36335;&#26159;&#25351;&#32422;&#26463;&#36807;&#24230;&#30340;&#30005;&#36335;&#65292;&#23548;&#33268;&#30005;&#36335;&#32570;&#20047;&#24517;&#35201;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23548;&#33268;&#39564;&#35777;&#32773;&#25509;&#21463;&#27809;&#26377;&#35265;&#35777;&#65292;&#20351;&#30005;&#36335;&#27627;&#26080;&#24847;&#20041;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25214;&#20986;ZKP&#30005;&#36335;&#20013;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290; &#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#31639;&#26415;&#30005;&#36335;&#32422;&#26463;&#32534;&#30721;&#20026;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#35745;&#31639;&#22312;&#26377;&#38480;&#22495;&#19978;&#35299;&#20915;&#22810;&#39033;&#24335;&#26041;&#31243;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15676v1 Announce Type: cross  Abstract: ZKP systems have surged attention and held a fundamental role in contemporary cryptography. Zk-SNARK protocols dominate the ZKP usage, often implemented through arithmetic circuit programming paradigm. However, underconstrained or overconstrained circuits may lead to bugs. Underconstrained circuits refer to circuits that lack the necessary constraints, resulting in unexpected solutions in the circuit and causing the verifier to accept a bogus witness. Overconstrained circuits refer to circuits that are constrained excessively, resulting in the circuit lacking necessary solutions and causing the verifier to accept no witness, rendering the circuit meaningless. This paper introduces a novel approach for pinpointing two distinct types of bugs in ZKP circuits. The method involves encoding the arithmetic circuit constraints to polynomial equation systems and solving polynomial equation systems over a finite field by algebraic computation. T
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15246</link><description>&lt;p&gt;
FollowIR: &#35780;&#20272;&#21644;&#25945;&#25480;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#20197;&#36981;&#24490;&#35828;&#26126;&#20070;
&lt;/p&gt;
&lt;p&gt;
FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#36981;&#24490;&#38271;&#19988;&#22797;&#26434;&#30340;&#35828;&#26126;&#20070;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#27169;&#22411;&#20351;&#29992;LLMs&#20316;&#20026;&#20854;&#26550;&#26500;&#30340;&#25903;&#26609;&#65292;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#21482;&#25509;&#21463;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#27809;&#26377;&#35828;&#26126;&#20070;&#12290;&#23545;&#20110;&#26368;&#36817;&#19968;&#20123;&#25509;&#21463;&#35828;&#26126;&#20070;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#23427;&#20204;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#35828;&#26126;&#20070;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#21450;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;IR&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#22320;&#36981;&#24490;&#29616;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;FollowIR&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#24736;&#20037;&#21382;&#21490;&#65306;&#27491;&#22914;TREC&#20026;&#20154;&#31867;&#26631;&#27880;&#21592;&#25552;&#20379;&#35828;&#26126;&#20070;&#65288;&#20063;&#31216;&#20026;&#21465;&#36848;&#65289;&#26469;&#21028;&#26029;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#19968;&#26679;&#65292;&#22240;&#27492;IR&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#36825;&#20123;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#30830;&#23450;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20934;&#20174;&#19977;&#20010;&#32463;&#36807;&#28145;&#24230;&#21028;&#26029;&#30340;TREC&#25910;&#34255;&#24320;&#22987;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15246v1 Announce Type: cross  Abstract: Modern Large Language Models (LLMs) are capable of following long and complex instructions that enable a diverse amount of user tasks. However, despite Information Retrieval (IR) models using LLMs as the backbone of their architectures, nearly all of them still only take queries as input, with no instructions. For the handful of recent models that do take instructions, it's unclear how they use them. We introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR builds off the long history of the TREC conferences: as TREC provides human annotators with instructions (also known as narratives) to determine document relevance, so should IR models be able to understand and decide relevance based on these detailed instructions. Our evaluation benchmark starts with three deeply judged TREC collections and al
&lt;/p&gt;</description></item><item><title>FakeWatch&#26694;&#26550;&#26159;&#20026;&#20102;&#26816;&#27979;&#20551;&#26032;&#38395;&#32780;&#35774;&#35745;&#30340;&#65292;&#25972;&#21512;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#21069;&#27839;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21271;&#32654;&#36873;&#20030;&#30456;&#20851;&#26032;&#38395;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09858</link><description>&lt;p&gt;
FakeWatch&#65306;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#20551;&#26032;&#38395;&#20197;&#30830;&#20445;&#36873;&#20030;&#21487;&#20449;&#24615;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FakeWatch: A Framework for Detecting Fake News to Ensure Credible Elections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09858
&lt;/p&gt;
&lt;p&gt;
FakeWatch&#26694;&#26550;&#26159;&#20026;&#20102;&#26816;&#27979;&#20551;&#26032;&#38395;&#32780;&#35774;&#35745;&#30340;&#65292;&#25972;&#21512;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#21069;&#27839;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21271;&#32654;&#36873;&#20030;&#30456;&#20851;&#26032;&#38395;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25216;&#26415;&#39537;&#21160;&#30340;&#19990;&#30028;&#20013;&#65292;&#20551;&#26032;&#38395;&#30340;&#36805;&#36895;&#20256;&#25773;&#65292;&#29305;&#21035;&#26159;&#22312;&#36873;&#20030;&#31561;&#37325;&#35201;&#20107;&#20214;&#26399;&#38388;&#65292;&#23545;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#26500;&#25104;&#20102;&#36234;&#26469;&#36234;&#22823;&#30340;&#23041;&#32961;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FakeWatch&#65292;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#29992;&#20110;&#26816;&#27979;&#20551;&#26032;&#38395;&#30340;&#20840;&#38754;&#26694;&#26550;&#12290;&#21033;&#29992;&#26032;&#31574;&#21010;&#30340;&#21271;&#32654;&#36873;&#20030;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#24378;&#22823;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25972;&#21512;&#20102;&#19968;&#20010;&#27169;&#22411;&#20013;&#24515;&#65292;&#21253;&#25324;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#21644;&#23574;&#31471;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20551;&#26032;&#38395;&#12290;&#25105;&#20204;&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#20026;&#30740;&#31350;&#30028;&#25552;&#20379;&#36866;&#24212;&#24615;&#21644;&#31934;&#20934;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#19981;&#26029;&#28436;&#21464;&#30340;&#35823;&#20449;&#24687;&#26684;&#23616;&#12290;&#23545;&#25105;&#20204;&#25968;&#25454;&#38598;&#19978;&#20551;&#26032;&#38395;&#20998;&#31867;&#22120;&#36827;&#34892;&#30340;&#23450;&#37327;&#35780;&#20272;&#26174;&#31034;&#65292;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;LMs&#31245;&#24494;&#39046;&#20808;&#20256;&#32479;ML&#27169;&#22411;&#65292;&#20294;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09858v1 Announce Type: new  Abstract: In today's technologically driven world, the rapid spread of fake news, particularly during critical events like elections, poses a growing threat to the integrity of information. To tackle this challenge head-on, we introduce FakeWatch, a comprehensive framework carefully designed to detect fake news. Leveraging a newly curated dataset of North American election-related news articles, we construct robust classification models. Our framework integrates a model hub comprising of both traditional machine learning (ML) techniques and cutting-edge Language Models (LMs) to discern fake news effectively. Our overarching objective is to provide the research community with adaptable and precise classification models adept at identifying the ever-evolving landscape of misinformation. Quantitative evaluations of fake news classifiers on our dataset reveal that, while state-of-the-art LMs exhibit a slight edge over traditional ML models, classical 
&lt;/p&gt;</description></item><item><title>TeaMs-RL&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30452;&#25509;&#29983;&#25104;&#22522;&#30784;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#23545;&#20154;&#31867;&#30340;&#20381;&#36182;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#20026;&#21333;&#19968;&#24494;&#35843;&#27493;&#39588;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.08694</link><description>&lt;p&gt;
TeaMs-RL&#65306;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25945;&#25480;LLMs&#26356;&#22909;&#22320;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08694
&lt;/p&gt;
&lt;p&gt;
TeaMs-RL&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30452;&#25509;&#29983;&#25104;&#22522;&#30784;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#23545;&#20154;&#31867;&#30340;&#20381;&#36182;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#20026;&#21333;&#19968;&#24494;&#35843;&#27493;&#39588;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#36890;&#24120;&#38754;&#20020;&#30528;&#22312;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#20013;&#23545;&#20154;&#31867;&#26631;&#27880;&#21592;&#30340;&#20005;&#37325;&#20381;&#36182;&#25110;&#19982;&#33258;&#25105;&#25351;&#23548;&#33539;&#24335;&#30456;&#20851;&#30340;&#39057;&#32321;&#19988;&#26114;&#36149;&#30340;&#22806;&#37096;&#26597;&#35810;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36716;&#21521;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;-- &#20294;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#20559;&#31163;&#20102;&#20856;&#22411;&#30340;RLHF&#65292;&#21518;&#32773;&#22312;&#25351;&#23548;&#25968;&#25454;&#35757;&#32451;&#21518;&#20248;&#21270;LLMs&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;RL&#30452;&#25509;&#29983;&#25104;&#21333;&#29420;&#36275;&#20197;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#30784;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;TeaMs-RL&#20351;&#29992;&#19968;&#31995;&#21015;&#25991;&#26412;&#25805;&#20316;&#21644;&#35268;&#21017;&#65292;&#20248;&#20808;&#32771;&#34385;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#21270;&#12290;&#23427;&#20419;&#36827;&#20102;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#32780;&#19981;&#36807;&#20110;&#20381;&#36182;&#22806;&#37096;&#20808;&#36827;&#27169;&#22411;&#65292;&#20026;&#21333;&#19968;&#24494;&#35843;&#27493;&#39588;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#28040;&#38500;&#20102;&#38543;&#21518;&#30340;RLHF&#38454;&#27573;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#65306;&#20943;&#23569;&#23545;&#20154;&#31867;&#30340;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08694v1 Announce Type: new  Abstract: The development of Large Language Models (LLMs) often confronts challenges stemming from the heavy reliance on human annotators in the reinforcement learning with human feedback (RLHF) framework, or the frequent and costly external queries tied to the self-instruct paradigm. In this work, we pivot to Reinforcement Learning (RL) -- but with a twist. Diverging from the typical RLHF, which refines LLMs following instruction data training, we use RL to directly generate the foundational instruction dataset that alone suffices for fine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and rules, prioritizing the diversification of training datasets. It facilitates the generation of high-quality data without excessive reliance on external advanced models, paving the way for a single fine-tuning step and negating the need for subsequent RLHF stages. Our findings highlight key advantages of our approach: reduced need for human inv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#21069;&#27839;&#27169;&#22411;&#23578;&#23384;&#22312;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#65292;&#25506;&#35752;&#20102;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#24357;&#34917;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.08002</link><description>&lt;p&gt;
&#35757;&#32451;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#22635;&#34917;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#65306;&#20197;&#25918;&#23556;&#23398;&#25104;&#20687;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#21069;&#27839;&#27169;&#22411;&#23578;&#23384;&#22312;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#65292;&#25506;&#35752;&#20102;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#24357;&#34917;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#22823;&#22522;&#30784;&#27169;&#22411;&#30340;&#23610;&#24230;&#35268;&#24459;&#21644;&#38750;&#20961;&#34920;&#29616;&#28608;&#21169;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#24320;&#21457;&#21644;&#21033;&#29992;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#19968;&#20123;&#29983;&#29289;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26089;&#26399;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20043;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#22823;&#25361;&#25112;&#12290;&#20687;GPT-4V&#36825;&#26679;&#30340;&#21069;&#27839;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#20173;&#23384;&#22312;&#37325;&#22823;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#35775;&#38382;&#12289;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#21512;&#35268;&#31561;&#23454;&#38469;&#38382;&#39064;&#20351;&#20020;&#24202;&#21307;&#29983;&#38590;&#20197;&#30452;&#25509;&#22312;&#31169;&#20154;&#24739;&#32773;&#25968;&#25454;&#19978;&#20351;&#29992;&#31169;&#20154;&#25176;&#31649;&#30340;&#26368;&#20808;&#36827;&#22823;&#22411;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;SMMs&#65289;&#26469;&#22635;&#34917;&#26410;&#28385;&#36275;&#30340;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#23558;&#29992;&#20110;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#26368;&#20808;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#32435;&#20837;&#65292;&#24182;&#20391;&#37325;&#20110;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08002v1 Announce Type: new  Abstract: The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such large models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world applications. Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data. In this paper, we explore training open-source small multimodal models (SMMs) to bridge biomedical competency gaps for unmet clinical needs. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on t
&lt;/p&gt;</description></item><item><title>HaluEval-Wild&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#29615;&#22659;&#20013;LLM&#24187;&#35273;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25910;&#38598;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29992;&#25143;&#26597;&#35810;&#24182;&#20998;&#31867;&#20026;&#20116;&#31181;&#19981;&#21516;&#31867;&#22411;&#65292;&#21487;&#20197;&#23545;LLM&#34920;&#29616;&#20986;&#30340;&#24187;&#35273;&#31867;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.04307</link><description>&lt;p&gt;
HaluEval-Wild&#65306;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04307
&lt;/p&gt;
&lt;p&gt;
HaluEval-Wild&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#29615;&#22659;&#20013;LLM&#24187;&#35273;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25910;&#38598;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29992;&#25143;&#26597;&#35810;&#24182;&#20998;&#31867;&#20026;&#20116;&#31181;&#19981;&#21516;&#31867;&#22411;&#65292;&#21487;&#20197;&#23545;LLM&#34920;&#29616;&#20986;&#30340;&#24187;&#35273;&#31867;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#23545;&#20110;&#20851;&#38190;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLM&#22312;&#20256;&#32479;NLP&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#31572;&#65288;QA&#65289;&#21644;&#25688;&#35201;&#65292;&#19981;&#36275;&#20197;&#25429;&#25417;&#21160;&#24577;&#23454;&#38469;&#29615;&#22659;&#20013;&#29992;&#25143;-LLM&#20132;&#20114;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HaluEval-Wild&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#29615;&#22659;&#20013;LLM&#24187;&#35273;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#31934;&#24515;&#25910;&#38598;&#20102;&#26469;&#33258;&#29616;&#26377;&#23454;&#38469;&#29992;&#25143;-LLM&#20132;&#20114;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;ShareGPT&#65289;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65288;&#32463;Alpaca&#23545;&#25239;&#24615;&#36807;&#28388;&#30340;&#65289;&#29992;&#25143;&#26597;&#35810;&#65292;&#20197;&#35780;&#20272;&#21508;&#31181;LLM&#30340;&#24187;&#35273;&#29575;&#12290;&#22312;&#20998;&#26512;&#25910;&#38598;&#21040;&#30340;&#26597;&#35810;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#31867;&#20026;&#20116;&#31181;&#19981;&#21516;&#31867;&#22411;&#65292;&#36825;&#20351;&#24471;&#21487;&#20197;&#23545;LLM&#34920;&#29616;&#20986;&#30340;&#24187;&#35273;&#31867;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#26512;&#65292;&#24182;&#23558;&#24341;&#29992;&#31572;&#26696;&#19982;&#24378;&#22823;&#30340;GP&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04307v1 Announce Type: new  Abstract: Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains. Recent benchmarks designed to assess LLM hallucinations within conventional NLP tasks, such as knowledge-intensive question answering (QA) and summarization, are insufficient for capturing the complexities of user-LLM interactions in dynamic, real-world settings. To address this gap, we introduce HaluEval-Wild, the first benchmark specifically designed to evaluate LLM hallucinations in the wild. We meticulously collect challenging (adversarially filtered by Alpaca) user queries from existing real-world user-LLM interaction datasets, including ShareGPT, to evaluate the hallucination rates of various LLMs. Upon analyzing the collected queries, we categorize them into five distinct types, which enables a fine-grained analysis of the types of hallucinations LLMs exhibit, and synthesize the reference answers with the powerful GP
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#32946;&#19987;&#23478;&#26469;&#35780;&#20272;&#25945;&#32946;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;LMs&#20316;&#20026;&#21487;&#38752;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#25351;&#23548;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02795</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21644;&#20248;&#21270;&#25945;&#32946;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Optimizing Educational Content with Large Language Model Judgments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02795
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#32946;&#19987;&#23478;&#26469;&#35780;&#20272;&#25945;&#32946;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;LMs&#20316;&#20026;&#21487;&#38752;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#25351;&#23548;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#26377;&#25928;&#30340;&#25945;&#32946;&#26448;&#26009;&#36890;&#24120;&#38656;&#35201;&#23545;&#23398;&#29983;&#23398;&#20064;&#25104;&#26524;&#36827;&#34892;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38556;&#30861;&#65292;&#19968;&#20010;&#24819;&#27861;&#26159;&#26500;&#24314;&#23398;&#29983;&#23398;&#20064;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#20248;&#21270;&#25945;&#23398;&#26448;&#26009;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#23398;&#20064;&#21160;&#24577;&#30340;&#35748;&#30693;&#36807;&#31243;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20316;&#20026;&#25945;&#32946;&#19987;&#23478;&#26469;&#35780;&#20272;&#21508;&#31181;&#25351;&#23548;&#23545;&#23398;&#20064;&#32467;&#26524;&#24433;&#21709;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#26469;&#35780;&#20272;&#25351;&#23548;&#26448;&#26009;&#23545;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#30340;&#25972;&#20307;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#22797;&#21046;&#35832;&#22914;&#19987;&#19994;&#36870;&#36716;&#25928;&#24212;&#21644;&#21464;&#24322;&#25928;&#24212;&#31561;&#24050;&#32463;&#24314;&#31435;&#30340;&#25945;&#32946;&#21457;&#29616;&#12290;&#36825;&#23637;&#31034;&#20102;LMs&#20316;&#20026;&#25945;&#32946;&#20869;&#23481;&#21487;&#38752;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#25351;&#23548;&#20248;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;LM&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02795v1 Announce Type: new  Abstract: Creating effective educational materials generally requires expensive and time-consuming studies of student learning outcomes. To overcome this barrier, one idea is to build computational models of student learning and use them to optimize instructional materials. However, it is difficult to model the cognitive processes of learning dynamics. We propose an alternative approach that uses Language Models (LMs) as educational experts to assess the impact of various instructions on learning outcomes. Specifically, we use GPT-3.5 to evaluate the overall effect of instructional materials on different student groups and find that it can replicate well-established educational findings such as the Expertise Reversal Effect and the Variability Effect. This demonstrates the potential of LMs as reliable evaluators of educational content. Building on this insight, we introduce an instruction optimization approach in which one LM generates instruction
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#24212;&#29992;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#20197;&#21152;&#36895;&#37327;&#21270;LLM&#25512;&#29702;&#36807;&#31243;&#30340;&#19968;&#39033;&#21021;&#27493;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.01384</link><description>&lt;p&gt;
&#20851;&#20110;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#21387;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Compressibility of Quantized Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01384
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#24212;&#29992;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#20197;&#21152;&#36895;&#37327;&#21270;LLM&#25512;&#29702;&#36807;&#31243;&#30340;&#19968;&#39033;&#21021;&#27493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21040;&#36793;&#32536;&#25110;&#31227;&#21160;&#35774;&#22791;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#22914;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#21644;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#24212;&#29992;&#20110;&#20943;&#23569;&#25968;&#25454;&#31227;&#21160;&#65292;&#20174;&#32780;&#21152;&#36895;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#37327;&#21270;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01384v1 Announce Type: cross  Abstract: Deploying Large Language Models (LLMs) on edge or mobile devices offers significant benefits, such as enhanced data privacy and real-time processing capabilities. However, it also faces critical challenges due to the substantial memory requirement of LLMs. Quantization is an effective way of reducing the model size while maintaining good performance. However, even after quantization, LLMs may still be too big to fit entirely into the limited memory of edge or mobile devices and have to be partially loaded from the storage to complete the inference. In this case, the I/O latency of model loading becomes the bottleneck of the LLM inference latency. In this work, we take a preliminary step of studying applying data compression techniques to reduce data movement and thus speed up the inference of quantized LLM on memory-constrained devices. In particular, we discussed the compressibility of quantized LLMs, the trade-off between the compres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20351;&#25968;&#23383;&#24187;&#35273;&#24471;&#21040;&#26174;&#33879;&#25913;&#21892;</title><link>https://arxiv.org/abs/2403.01373</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#24187;&#35273;&#65306;&#19968;&#31181;&#19968;&#33268;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20351;&#25968;&#23383;&#24187;&#35273;&#24471;&#21040;&#26174;&#33879;&#25913;&#21892;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#22788;&#29702;&#25991;&#26412;&#21644;&#35270;&#35273;&#20869;&#23481;&#30456;&#20851;&#25361;&#25112;&#26041;&#38754;&#30340;&#26174;&#33879;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#21508;&#31181;&#24187;&#35273;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#24187;&#35273;&#65292;&#31216;&#20026;&#25968;&#23383;&#24187;&#35273;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#26410;&#33021;&#20934;&#30830;&#35782;&#21035;&#22270;&#20687;&#20013;&#29289;&#20307;&#30340;&#25968;&#37327;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#25968;&#23383;&#24187;&#35273;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#22312;&#20027;&#27969;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(LVLMs)&#20013;&#30340;&#26126;&#26174;&#26222;&#36941;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#30456;&#20851;&#35270;&#35282;&#28145;&#20837;&#20998;&#26512;&#20102;&#25968;&#23383;&#24187;&#35273;&#65292;&#32771;&#23519;&#20102;&#20869;&#22312;&#21644;&#22806;&#22312;&#30340;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26159;&#25968;&#23383;&#24187;&#35273;&#30340;&#19968;&#20010;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#20316;&#20026;&#20943;&#36731;&#27492;&#31867;&#24187;&#35273;&#30340;&#25163;&#27573;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;8%&#30340;&#24179;&#22343;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01373v1 Announce Type: new  Abstract: Large vision language models have demonstrated remarkable efficacy in addressing challenges related to both textual and visual content. Nevertheless, these models are susceptible to various hallucinations. In this paper, we focus on a new form of hallucination, specifically termed as number hallucination, which denotes instances where models fail to accurately identify the quantity of objects in an image. We establish a dataset and employ evaluation metrics to assess number hallucination, revealing a pronounced prevalence of this issue across mainstream large vision language models (LVLMs). Additionally, we delve into a thorough analysis of number hallucination, examining inner and outer inconsistency problem from two related perspectives. We assert that this inconsistency is one cause of number hallucination and propose a consistency training method as a means to alleviate such hallucination, which achieves an average improvement of 8\%
&lt;/p&gt;</description></item><item><title>CoTeX&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#26469;&#20419;&#36827;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#28860;LLMs&#30340;&#33021;&#21147;&#20026;&#22788;&#29702;&#38750;&#24179;&#34892;&#25968;&#25454;&#21644;&#24179;&#34892;&#25968;&#25454;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#30417;&#30563;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36879;&#26126;&#30340;&#35299;&#37322;&#22312;&#39118;&#26684;&#36716;&#31227;&#36807;&#31243;&#20013;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01106</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#25105;&#35299;&#37322;&#25552;&#28860;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Distilling Text Style Transfer With Self-Explanation From LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01106
&lt;/p&gt;
&lt;p&gt;
CoTeX&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#26469;&#20419;&#36827;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#28860;LLMs&#30340;&#33021;&#21147;&#20026;&#22788;&#29702;&#38750;&#24179;&#34892;&#25968;&#25454;&#21644;&#24179;&#34892;&#25968;&#25454;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#30417;&#30563;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36879;&#26126;&#30340;&#35299;&#37322;&#22312;&#39118;&#26684;&#36716;&#31227;&#36807;&#31243;&#20013;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#65288;TST&#65289;&#26088;&#22312;&#25913;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#21516;&#26102;&#20445;&#30041;&#20854;&#26680;&#24515;&#20869;&#23481;&#12290;&#37492;&#20110;TST&#30340;&#26377;&#38480;&#24179;&#34892;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoTeX&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#26469;&#20419;&#36827;TST&#30340;&#26694;&#26550;&#12290;CoTeX&#23558;LLMs&#30340;&#22797;&#26434;&#37325;&#20889;&#21644;&#25512;&#29702;&#33021;&#21147;&#25552;&#28860;&#25104;&#26356;&#31616;&#21270;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#38750;&#24179;&#34892;&#25968;&#25454;&#21644;&#24179;&#34892;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#22235;&#20010;TST&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;CoTeX&#26174;&#31034;&#20986;&#36229;&#36234;&#20256;&#32479;&#30417;&#30563;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23558;CoTeX&#19982;&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#12289;&#30417;&#30563;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25216;&#26415;&#20197;&#21450;&#25351;&#23548;&#35843;&#25972;&#30340;LLMs&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;CoTeX&#36890;&#36807;&#25552;&#20379;&#36879;&#26126;&#30340;&#35299;&#37322;&#20854;&#39118;&#26684;&#36716;&#31227;&#36807;&#31243;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01106v1 Announce Type: cross  Abstract: Text Style Transfer (TST) seeks to alter the style of text while retaining its core content. Given the constraints of limited parallel datasets for TST, we propose CoTeX, a framework that leverages large language models (LLMs) alongside chain-of-thought (CoT) prompting to facilitate TST. CoTeX distills the complex rewriting and reasoning capabilities of LLMs into more streamlined models capable of working with both non-parallel and parallel data. Through experimentation across four TST datasets, CoTeX is shown to surpass traditional supervised fine-tuning and knowledge distillation methods, particularly in low-resource settings. We conduct a comprehensive evaluation, comparing CoTeX against current unsupervised, supervised, in-context learning (ICL) techniques, and instruction-tuned LLMs. Furthermore, CoTeX distinguishes itself by offering transparent explanations for its style transfer process.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#20219;&#21153;&#21644;&#23545;&#40784;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.19404</link><description>&lt;p&gt;
&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;&#29992;&#20110;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Entity-Aware Multimodal Alignment Framework for News Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19404
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#20219;&#21153;&#21644;&#23545;&#40784;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#26159;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#35201;&#27714;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#23383;&#24149;&#65292;&#20854;&#20013;&#21253;&#21547;&#26032;&#38395;&#22270;&#20687;&#21644;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#12290;&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#36805;&#36895;&#65292;&#24182;&#22312;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#24120;&#35265;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#23450;&#19979;&#29983;&#25104;&#23454;&#20307;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#21363;&#20351;&#22312;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31616;&#21333;&#24494;&#35843;&#65292;&#23427;&#20204;&#22788;&#29702;&#23454;&#20307;&#20449;&#24687;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#23454;&#20307;&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#22810;&#27169;&#24577;&#23454;&#20307;&#24863;&#30693;&#23545;&#40784;&#20219;&#21153;&#21644;&#19968;&#20010;&#23545;&#40784;&#26694;&#26550;&#65292;&#20197;&#23545;&#40784;&#27169;&#22411;&#24182;&#29983;&#25104;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GoodNews&#25968;&#25454;&#38598;&#19978;&#23558;CIDEr&#20998;&#25968;&#25552;&#39640;&#21040;86.29&#65288;&#20174;72.33&#65289;&#65292;&#22312;NYTimes800k&#25968;&#25454;&#38598;&#19978;&#23558;&#20854;&#25552;&#39640;&#21040;85.61&#65288;&#20174;70.83&#65289;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19404v1 Announce Type: cross  Abstract: News image captioning task is a variant of image captioning task which requires model to generate a more informative caption with news image and the associated news article. Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-tuned on news image captioning dataset. To obtain a more powerful model to handle the multimodal entity information, we design two multimodal entity-aware alignment tasks and an alignment framework to align the model and generate the news image captions. Our method achieves better results than previous state-of-the-art models in CIDEr score (72.33 -&gt; 86.29) on GoodNews dataset and (70.83 -&gt; 85.61) on NYTimes800k dataset.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.19379</link><description>&lt;p&gt;
&#30789;&#35895;&#20154;&#32676;&#30340;&#26234;&#24935;&#65306;LLM&#38598;&#25104;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#20154;&#32676;&#20934;&#30830;&#29575;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#25928;&#24212;&#65292;&#21363;&#36890;&#36807;&#32858;&#21512;&#19968;&#32676;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#36807;&#21435;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#21069;&#27839;LLMs&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#20154;&#31867;&#32676;&#20307;&#39044;&#27979;&#27604;&#36187;&#30340;&#40644;&#37329;&#26631;&#20934;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30001;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;31&#20010;&#20108;&#20803;&#38382;&#39064;&#30340;&#32858;&#21512;LLM&#39044;&#27979;&#19982;&#19968;&#20010;&#26469;&#33258;&#19977;&#20010;&#26376;&#39044;&#27979;&#27604;&#36187;&#30340;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#32676;&#20307;&#30340;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#31181;&#39034;&#20174;&#25928;&#24212;&#65292;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26126;&#26174;&#39640;&#20110;50%&#65292;&#23613;&#31649;&#20960;&#20046;&#26159;&#24179;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;LLMs&#36827;&#34892;&#26426;&#26800;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#26102;&#20351;&#29992;&#22810;&#20010;&#24182;&#34892;&#36335;&#24452;&#29983;&#25104;&#31572;&#26696;&#65292;&#21516;&#26102;&#23384;&#22312;&#21151;&#33021;&#24615;&#20998;&#27495;&#12290;</title><link>https://arxiv.org/abs/2402.18312</link><description>&lt;p&gt;
&#22914;&#20309;&#36880;&#27493;&#24605;&#32771;&#65306;&#23545;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#26426;&#26800;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18312
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;LLMs&#36827;&#34892;&#26426;&#26800;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#26102;&#20351;&#29992;&#22810;&#20010;&#24182;&#34892;&#36335;&#24452;&#29983;&#25104;&#31572;&#26696;&#65292;&#21516;&#26102;&#23384;&#22312;&#21151;&#33021;&#24615;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#65292;&#20294;&#23545;&#20110;&#20419;&#36827;CoT&#29983;&#25104;&#30340;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#20173;&#23384;&#22312;&#32570;&#20047;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#26426;&#26800;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;LLMs&#20013;&#34920;&#29616;&#20986;CoT&#25512;&#29702;&#30340;&#31070;&#32463;&#23376;&#32467;&#26500;&#12290;&#36890;&#36807;&#23545;LLaMA-2 7B&#24212;&#29992;&#20110;&#34394;&#26500;&#26412;&#20307;&#35770;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#20026;&#36880;&#27493;&#25512;&#29702;&#37096;&#32626;&#20102;&#22810;&#20010;&#24182;&#34892;&#31572;&#26696;&#29983;&#25104;&#36335;&#24452;&#12290;&#36825;&#20123;&#24182;&#34892;&#36335;&#24452;&#25552;&#20379;&#20102;&#26469;&#33258;&#36755;&#20837;&#38382;&#39064;&#19978;&#19979;&#25991;&#20197;&#21450;&#29983;&#25104;&#30340;CoT&#30340;&#24207;&#36143;&#31572;&#26696;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#20013;&#38388;&#23618;&#23384;&#22312;&#24341;&#20154;&#30633;&#30446;&#30340;&#21151;&#33021;&#20998;&#27495;&#12290;&#21021;&#22987;&#19968;&#21322;&#30340;&#20196;&#29260;&#34920;&#31034;&#20173;&#28982;&#24378;&#28872;&#20559;&#21521;&#39044;&#35757;&#32451;&#20808;&#39564;&#65292;&#32780;&#21518;&#21322;&#37096;&#20998;&#31361;&#28982;&#34987;&#19978;&#19979;&#25991;&#25152;&#21462;&#20195;&#12290;&#36825;&#31181;&#20869;&#37096;&#30456;&#20301;&#36716;&#21464;&#22312;&#19981;&#21516;&#30340;&#21151;&#33021;&#21327;&#21516;&#20013;&#20307;&#29616;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18312v1 Announce Type: new  Abstract: Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of LLaMA-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a striking functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context taking over abruptly in the later half. This internal phase shift manifests in different functional co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#29992;&#20110;&#20855;&#26377;&#32416;&#27491;&#21453;&#39304;&#30340;&#33258;&#21160;&#20889;&#20316;&#35780;&#20272;&#65292;&#26088;&#22312;&#22635;&#34917;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;AWE&#21644;GEC&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.17613</link><description>&lt;p&gt;
&#20855;&#26377;&#32416;&#27491;&#21453;&#39304;&#30340;&#31070;&#32463;&#33258;&#21160;&#20889;&#20316;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Neural Automated Writing Evaluation with Corrective Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#29992;&#20110;&#20855;&#26377;&#32416;&#27491;&#21453;&#39304;&#30340;&#33258;&#21160;&#20889;&#20316;&#35780;&#20272;&#65292;&#26088;&#22312;&#22635;&#34917;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;AWE&#21644;GEC&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#65292;&#21033;&#29992;&#25216;&#26415;&#24050;&#21464;&#24471;&#26222;&#36941;&#12290;&#23545;&#20110;&#20889;&#20316;&#35780;&#20272;&#65292;&#33258;&#21160;&#20889;&#20316;&#35780;&#20272;&#65288;AWE&#65289;&#21644;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#65288;GEC&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;&#20889;&#20316;&#33021;&#21147;&#12289;&#21521;&#23398;&#20064;&#32773;&#25552;&#20379;&#21363;&#26102;&#20010;&#24615;&#21270;&#21453;&#39304;&#30340;&#27969;&#34892;&#21644;&#26377;&#25928;&#26041;&#27861;&#12290;&#20511;&#21161;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21147;&#37327;&#65292;AWE&#21644;GEC&#31995;&#32479;&#24050;&#20998;&#21035;&#24320;&#21457;&#65292;&#20197;&#20026;&#35821;&#35328;&#23398;&#20064;&#32773;&#25552;&#20379;&#33258;&#21160;&#26657;&#27491;&#21453;&#39304;&#21644;&#26356;&#20934;&#30830;&#12289;&#26080;&#20559;&#30340;&#35780;&#20998;&#65292;&#21542;&#21017;&#36825;&#20123;&#23558;&#34987;&#35780;&#23457;&#21592;&#25152;&#20027;&#35266;&#21028;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#29992;&#20110;&#20855;&#26377;&#32416;&#27491;&#21453;&#39304;&#30340;&#33258;&#21160;&#20889;&#20316;&#35780;&#20272;&#65292;&#20316;&#20026;&#22635;&#34917;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;AWE&#21644;GEC&#32467;&#26524;&#24046;&#36317;&#30340;&#25163;&#27573;&#12290;&#35813;&#31995;&#32479;&#20351;&#35821;&#35328;&#23398;&#20064;&#32773;&#33021;&#22815;&#27169;&#25311;&#35770;&#25991;&#20889;&#20316;&#27979;&#35797;&#65306;&#23398;&#29983;&#25776;&#20889;&#24182;&#25552;&#20132;&#35770;&#25991;&#65292;&#31995;&#32479;&#25552;&#20379;&#35780;&#20272;&#21644;&#32416;&#27491;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17613v1 Announce Type: new  Abstract: The utilization of technology in second language learning and teaching has become ubiquitous. For the assessment of writing specifically, automated writing evaluation (AWE) and grammatical error correction (GEC) have become immensely popular and effective methods for enhancing writing proficiency and delivering instant and individualized feedback to learners. By leveraging the power of natural language processing (NLP) and machine learning algorithms, AWE and GEC systems have been developed separately to provide language learners with automated corrective feedback and more accurate and unbiased scoring that would otherwise be subject to examiners. In this paper, we propose an integrated system for automated writing evaluation with corrective feedback as a means of bridging the gap between AWE and GEC results for second language learners. This system enables language learners to simulate the essay writing tests: a student writes and submi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#31574;&#30053;&#30340;&#27169;&#22411;&#22312;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#20013;&#30340;&#28508;&#22312;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17493</link><description>&lt;p&gt;
&#20026;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#24320;&#20855;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27491;&#30830;&#21058;&#37327;&#26159;&#22810;&#23569;&#65311;
&lt;/p&gt;
&lt;p&gt;
Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17493
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#31574;&#30053;&#30340;&#27169;&#22411;&#22312;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#20013;&#30340;&#28508;&#22312;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#21487;&#20197;&#25351;&#23548;&#26377;&#25928;&#30340;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#31649;&#29702;&#21644;&#35268;&#21010;&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#39044;&#27979;&#26415;&#21518;&#39118;&#38505;&#12290;&#30740;&#31350;&#20027;&#35201;&#28041;&#21450;2018&#24180;&#33267;2021&#24180;&#38388;&#26469;&#33258;Barnes Jewish&#21307;&#38498;&#31995;&#32479;&#30340;84,875&#20221;&#35760;&#24405;&#12290;&#26041;&#27861;&#22312;Beth Israel Deaconess&#30340;MIMIC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22797;&#21046;&#12290;&#20004;&#39033;&#30740;&#31350;&#30340;&#24179;&#22343;&#38543;&#35775;&#26102;&#38388;&#22522;&#20110;&#26415;&#21518;ICU&#20303;&#38498;&#26102;&#38388;&#23567;&#20110;7&#22825;&#12290;&#23545;&#20110;BJH&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#21253;&#25324;30&#22825;&#27515;&#20129;&#29575;&#12289;&#32954;&#26643;&#22622;&#65288;PE&#65289;&#21644;&#32954;&#28814;&#12290;&#23545;BioGPT&#12289;ClinicalBERT&#21644;BioClinicalBERT&#23454;&#26045;&#20102;&#19977;&#31181;&#22495;&#33258;&#36866;&#24212;&#21644;&#24494;&#35843;&#31574;&#30053;&#65306;&#33258;&#30417;&#30563;&#30446;&#26631;&#65307;&#32467;&#21512;&#21322;&#30417;&#30563;&#24494;&#35843;&#30340;&#26631;&#31614;&#65307;&#20197;&#21450;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#22522;&#30784;&#24314;&#27169;&#12290;&#27169;&#22411;&#24615;&#33021;&#20351;&#29992;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24449;&#19979;&#30340;&#38754;&#31215;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17493v1 Announce Type: new  Abstract: Postoperative risk predictions can inform effective perioperative care management and planning. We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies. The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days. For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia. Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning. Model performance was compared using the area under the receiver operating ch
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;BERT&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#24066;&#22330;&#19979;&#25387;&#24182;&#31283;&#23450;&#22238;&#25253;&#12290;</title><link>https://arxiv.org/abs/2402.10481</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#30340;&#21152;&#23494;&#36164;&#20135;&#24066;&#22330;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Emoji Driven Crypto Assets Market Reactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;BERT&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#24066;&#22330;&#19979;&#25387;&#24182;&#31283;&#23450;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21152;&#23494;&#36135;&#24065;&#39046;&#22495;&#65292;&#35832;&#22914;Twitter&#20043;&#31867;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24050;&#32463;&#25104;&#20026;&#24433;&#21709;&#24066;&#22330;&#36235;&#21183;&#21644;&#25237;&#36164;&#32773;&#24773;&#32490;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#23545;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#34920;&#24773;&#31526;&#21495;&#36716;&#21270;&#20026;&#21487;&#37327;&#21270;&#30340;&#24773;&#24863;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#35265;&#35299;&#19982;BTC&#20215;&#26684;&#21644;VCRIX&#25351;&#25968;&#31561;&#20851;&#38190;&#24066;&#22330;&#25351;&#26631;&#36827;&#34892;&#20102;&#30456;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26088;&#22312;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#20803;&#32032;&#35782;&#21035;&#21644;&#39044;&#27979;&#24066;&#22330;&#36235;&#21183;&#30340;&#20132;&#26131;&#31574;&#30053;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#26377;&#21161;&#20110;&#36991;&#20813;&#37325;&#22823;&#24066;&#22330;&#19979;&#25387;&#65292;&#24182;&#26377;&#21161;&#20110;&#22238;&#25253;&#30340;&#31283;&#23450;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#23558;&#20808;&#36827;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20998;&#26512;&#25972;&#21512;&#21040;&#37329;&#34701;&#31574;&#30053;&#20013;&#30340;&#23454;&#38469;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#30475;&#24453;&#24066;&#22330;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10481v1 Announce Type: cross  Abstract: In the burgeoning realm of cryptocurrency, social media platforms like Twitter have become pivotal in influencing market trends and investor sentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based BERT model for a multimodal sentiment analysis, focusing on the impact of emoji sentiment on cryptocurrency markets. By translating emojis into quantifiable sentiment data, we correlate these insights with key market indicators like BTC Price and the VCRIX index. This approach may be fed into the development of trading strategies aimed at utilizing social media elements to identify and forecast market trends. Crucially, our findings suggest that strategies based on emoji sentiment can facilitate the avoidance of significant market downturns and contribute to the stabilization of returns. This research underscores the practical benefits of integrating advanced AI-driven analyses into financial strategies, offering a nuan
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#33889;&#33796;&#29273;&#12289;&#24847;&#22823;&#21033;&#21644;&#24503;&#22269;&#31461;&#35805;&#20013;&#26126;&#30830;&#34920;&#36798;&#30340;&#20215;&#20540;&#35266;&#24046;&#24322;&#65292;&#20351;&#29992;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#32599;&#30424;&#37327;&#21270;&#20998;&#26512;&#12290;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#36825;&#20123;&#22269;&#23478;&#20043;&#38388;&#23384;&#22312;&#20849;&#20139;&#30340;&#25991;&#21270;&#29702;&#35299;&#21644;&#23545;&#21892;&#33391;&#12289;&#36981;&#20174;&#21644;&#26222;&#36941;&#20215;&#20540;&#35266;&#30340;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2402.08318</link><description>&lt;p&gt;
&#12298;&#31461;&#35805;&#20013;&#26126;&#30830;&#34920;&#36798;&#30340;&#31038;&#20250;&#20215;&#20540;&#35266;&#65306;&#19977;&#31181;&#27431;&#27954;&#25991;&#21270;&#30340;&#27604;&#36739;&#12299;
&lt;/p&gt;
&lt;p&gt;
Explicit References to Social Values in Fairy Tales: A Comparison between Three European Cultures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08318
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#33889;&#33796;&#29273;&#12289;&#24847;&#22823;&#21033;&#21644;&#24503;&#22269;&#31461;&#35805;&#20013;&#26126;&#30830;&#34920;&#36798;&#30340;&#20215;&#20540;&#35266;&#24046;&#24322;&#65292;&#20351;&#29992;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#32599;&#30424;&#37327;&#21270;&#20998;&#26512;&#12290;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#36825;&#20123;&#22269;&#23478;&#20043;&#38388;&#23384;&#22312;&#20849;&#20139;&#30340;&#25991;&#21270;&#29702;&#35299;&#21644;&#23545;&#21892;&#33391;&#12289;&#36981;&#20174;&#21644;&#26222;&#36941;&#20215;&#20540;&#35266;&#30340;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#31461;&#35805;&#20013;&#30340;&#31038;&#20250;&#20215;&#20540;&#35266;&#21487;&#20197;&#20102;&#35299;&#20215;&#20540;&#35266;&#22312;&#26102;&#31354;&#20013;&#30340;&#20256;&#36882;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#32599;&#30424;&#26469;&#37327;&#21270;&#33889;&#33796;&#29273;&#12289;&#24847;&#22823;&#21033;&#21644;&#24503;&#22269;&#31461;&#35805;&#20013;&#30340;&#20215;&#20540;&#35266;&#20256;&#36882;&#12290;&#25105;&#20204;&#30740;&#31350;&#36825;&#19977;&#31181;&#22269;&#23478;&#30340;&#31461;&#35805;&#22312;&#26126;&#30830;&#34920;&#36798;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25351;&#23450;&#20102;&#19968;&#20010;&#20805;&#28385;&#20215;&#20540;&#35266;&#30340;&#35789;&#27719;&#21015;&#34920;&#65292;&#32771;&#34385;&#23427;&#20204;&#30340;&#35789;&#24178;&#65292;&#24182;&#20998;&#26512;&#22312;&#19987;&#38376;&#39044;&#35757;&#32451;&#30340;Word2Vec&#27169;&#22411;&#20013;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#35282;&#24230;&#39564;&#35777;&#21644;&#25209;&#21028;&#24615;&#35752;&#35770;&#37327;&#21270;&#27169;&#22411;&#25152;&#25552;&#20986;&#30340;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#22797;&#29992;&#21644;&#21487;&#37325;&#29616;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#21382;&#21490;&#35821;&#26009;&#24211;&#20013;&#26126;&#30830;&#24341;&#29992;&#30340;&#20215;&#20540;&#35266;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#26263;&#31034;&#26377;&#30528;&#20849;&#20139;&#25991;&#21270;&#29702;&#35299;&#21644;&#23545;&#21892;&#33391;&#12289;&#36981;&#20174;&#21644;&#26222;&#36941;&#20215;&#20540;&#35266;&#30340;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of social values in fairy tales opens the possibility to learn about the communication of values across space and time. We propose to study the communication of values in fairy tales from Portugal, Italy and Germany using a technique called word embedding with a compass to quantify vocabulary differences and commonalities. We study how these three national traditions of fairy tales differ in their explicit references to values. To do this, we specify a list of value-charged tokens, consider their word stems and analyse the distance between these in a bespoke pre-trained Word2Vec model. We triangulate and critically discuss the validity of the resulting hypotheses emerging from this quantitative model. Our claim is that this is a reusable and reproducible method for the study of the values explicitly referenced in historical corpora. Finally, our preliminary findings hint at a shared cultural understanding and the expression of values such as Benevolence, Conformity, and Unive
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;VQA&#22522;&#20934;&#65292;&#25512;&#21160;&#20102;&#23545;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#35821;&#20041;&#23618;&#27425;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#21518;&#32493;&#38382;&#39064;&#26469;&#25913;&#36827;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#19978;&#31895;&#31961;&#31572;&#26696;&#30340;&#35780;&#20272;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#24230;&#37327;&#26631;&#20934;&#65292;&#20182;&#20204;&#22312;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#36873;&#25321;&#20102;&#26368;&#32456;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07270</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#20854;&#35821;&#20041;&#23618;&#27425;&#65292;&#24320;&#23637;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#24335;VQA&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;VQA&#22522;&#20934;&#65292;&#25512;&#21160;&#20102;&#23545;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#35821;&#20041;&#23618;&#27425;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#21518;&#32493;&#38382;&#39064;&#26469;&#25913;&#36827;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#19978;&#31895;&#31961;&#31572;&#26696;&#30340;&#35780;&#20272;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#24230;&#37327;&#26631;&#20934;&#65292;&#20182;&#20204;&#22312;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#36873;&#25321;&#20102;&#26368;&#32456;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#24037;&#20316;&#12290;&#36890;&#36807;&#35299;&#20915;&#29616;&#26377;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#24182;&#25552;&#20986;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25512;&#21160;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#21517;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;VQA&#22522;&#20934;&#65292;&#21487;&#20197;&#23545;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#35780;&#20272;&#65292;&#24182;&#19982;&#21028;&#21035;&#24615;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#20026;&#20102;&#25913;&#21892;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#19978;&#31895;&#31961;&#31572;&#26696;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26631;&#31614;&#31354;&#38388;&#30340;&#35821;&#20041;&#23618;&#27425;&#26469;&#25552;&#20986;&#20851;&#20110;&#22522;&#20934;&#31867;&#21035;&#30340;&#33258;&#21160;&#29983;&#25104;&#30340;&#21518;&#32493;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;LLM&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#32473;&#23450;&#22522;&#20934;&#31572;&#26696;&#30340;&#27169;&#22411;&#39044;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#65292;&#22522;&#20110;&#27492;&#20915;&#23450;&#26368;&#32456;&#24230;&#37327;&#26631;&#20934;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.02619</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Increasing Trust in Language Models through the Reuse of Verified Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#32463;&#24120;&#24573;&#30053;&#32597;&#35265;&#30340;&#36793;&#30028;&#24773;&#20917;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#21487;&#20449;&#24230;&#26631;&#20934;&#65292;&#21363;&#20219;&#21153;&#31639;&#27861;&#21644;&#30005;&#36335;&#23454;&#29616;&#24517;&#39035;&#32463;&#36807;&#39564;&#35777;&#65292;&#32771;&#34385;&#21040;&#36793;&#30028;&#24773;&#20917;&#65292;&#24182;&#19988;&#27809;&#26377;&#24050;&#30693;&#30340;&#25925;&#38556;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#26469;&#26500;&#24314;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#28385;&#36275;&#36825;&#19968;&#26631;&#20934;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#20102;&#23436;&#20840;&#39564;&#35777;&#12290;&#20026;&#20102;&#23637;&#31034;&#32463;&#36807;&#39564;&#35777;&#30340;&#27169;&#22359;&#30340;&#37325;&#22797;&#20351;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#22909;&#30340;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#25554;&#20837;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#21516;&#26102;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#26356;&#22797;&#26434;&#30340;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#20219;&#21153;&#27169;&#22359;&#25554;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#21033;&#29992;&#27169;&#22411;&#30340;&#37325;&#22797;&#20351;&#29992;&#26469;&#25552;&#39640;&#21487;&#39564;&#35777;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#32508;&#21512;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#28548;&#28165;&#20102;&#24187;&#35273;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23545;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;</title><link>https://arxiv.org/abs/2402.00253</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00253
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#32508;&#21512;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#28548;&#28165;&#20102;&#24187;&#35273;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23545;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#23454;&#38469;&#30340;&#23454;&#26045;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#8220;&#24187;&#35273;&#8221;&#65292;&#25110;&#32773;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#21363;&#35270;&#35273;&#20869;&#23481;&#19982;&#30456;&#24212;&#25991;&#26412;&#29983;&#25104;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#22312;&#21033;&#29992;LVLMs&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#20221;&#32508;&#21512;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;LVLM&#30456;&#20851;&#30340;&#24187;&#35273;&#36827;&#34892;&#20102;&#28145;&#20837;&#21078;&#26512;&#65292;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#27010;&#35272;&#24182;&#20419;&#36827;&#26410;&#26469;&#30340;&#32531;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#28548;&#28165;&#20102;LVLMs&#20013;&#24187;&#35273;&#27010;&#24565;&#65292;&#21576;&#29616;&#20102;&#21508;&#31181;&#24187;&#35273;&#30151;&#29366;&#65292;&#24182;&#24378;&#35843;&#20102;LVLM&#24187;&#35273;&#22266;&#26377;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;LVLM&#29420;&#29305;&#24187;&#35273;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#35843;&#26597;&#20102;&#36825;&#20123;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#21253;&#25324;&#26469;&#33258;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#32452;&#20214;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#23545;&#29616;&#26377;&#30340;&#24187;&#35273;&#32531;&#35299;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review e
&lt;/p&gt;</description></item><item><title>DoraemonGPT&#26159;&#19968;&#20010;&#30001;LLMs&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22788;&#29702;&#21160;&#24577;&#35270;&#39057;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#31526;&#21495;&#35760;&#24518;&#26469;&#36827;&#34892;&#31354;&#38388;-&#26102;&#38388;&#26597;&#35810;&#21644;&#25512;&#29702;&#65292;&#24182;&#21462;&#24471;&#31616;&#27905;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.08392</link><description>&lt;p&gt;
DoraemonGPT&#65306;&#26397;&#21521;&#29702;&#35299;&#20855;&#26377;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#22330;&#26223;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08392
&lt;/p&gt;
&lt;p&gt;
DoraemonGPT&#26159;&#19968;&#20010;&#30001;LLMs&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22788;&#29702;&#21160;&#24577;&#35270;&#39057;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#31526;&#21495;&#35760;&#24518;&#26469;&#36827;&#34892;&#31354;&#38388;-&#26102;&#38388;&#26597;&#35810;&#21644;&#25512;&#29702;&#65292;&#24182;&#21462;&#24471;&#31616;&#27905;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30001;LLM&#39537;&#21160;&#30340;&#35270;&#35273;&#20195;&#29702;&#20027;&#35201;&#38598;&#20013;&#20110;&#35299;&#20915;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#29702;&#35299;&#21160;&#24577;&#22330;&#26223;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#36828;&#31163;&#20687;&#24341;&#23548;&#23398;&#29983;&#36827;&#34892;&#23454;&#39564;&#23460;&#23454;&#39564;&#21644;&#35782;&#21035;&#38169;&#35823;&#36825;&#26679;&#30340;&#30495;&#23454;&#24212;&#29992;&#12290;&#32771;&#34385;&#21040;&#35270;&#39057;&#27169;&#24577;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#19981;&#26029;&#21464;&#21270;&#24615;&#36136;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DoraemonGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#32508;&#21512;&#27010;&#24565;&#31616;&#27905;&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#35270;&#39057;&#20219;&#21153;&#12290;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#38382;&#39064;/&#20219;&#21153;&#30340;&#35270;&#39057;&#65292;DoraemonGPT&#39318;&#20808;&#23558;&#36755;&#20837;&#35270;&#39057;&#36716;&#25442;&#20026;&#23384;&#20648;&#19982;&#20219;&#21153;&#30456;&#20851;&#23646;&#24615;&#30340;&#31526;&#21495;&#23384;&#20648;&#22120;&#12290;&#36825;&#31181;&#32467;&#26500;&#21270;&#34920;&#31034;&#20801;&#35768;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23376;&#20219;&#21153;&#24037;&#20855;&#36827;&#34892;&#31354;&#38388;-&#26102;&#38388;&#26597;&#35810;&#21644;&#25512;&#29702;&#65292;&#20174;&#32780;&#20135;&#29983;&#31616;&#27905;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;&#37492;&#20110;LLM&#22312;&#28041;&#21450;&#19987;&#19994;&#39046;&#22495;&#65288;&#20363;&#22914;&#20998;&#26512;&#23454;&#39564;&#20013;&#28508;&#22312;&#30340;&#31185;&#23398;&#21407;&#29702;&#65289;&#26102;&#20855;&#26377;&#26377;&#38480;&#30340;&#20869;&#37096;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08392v2 Announce Type: replace-cross  Abstract: Recent LLM-driven visual agents mainly focus on solving image-based tasks, which limits their ability to understand dynamic scenes, making it far from real-life applications like guiding students in laboratory experiments and identifying their mistakes. Considering the video modality better reflects the ever-changing nature of real-world scenarios, we devise DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to handle dynamic video tasks. Given a video with a question/task, DoraemonGPT begins by converting the input video into a symbolic memory that stores task-related attributes. This structured representation allows for spatial-temporal querying and reasoning by well-designed sub-task tools, resulting in concise intermediate results. Recognizing that LLMs have limited internal knowledge when it comes to specialized domains (e.g., analyzing the scientific principles underlying experiments), we incorpor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21628;&#21505;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#26469;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20197;&#35299;&#20915;&#21333;&#25552;&#31034;&#35780;&#20272;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#24403;&#21069;LLMs&#30495;&#27491;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.00595</link><description>&lt;p&gt;
&#29366;&#24577;&#26159;&#20160;&#20040;&#33402;&#26415;&#65311;&#22810;&#25552;&#31034;LLM&#35780;&#20272;&#30340;&#21628;&#21505;&#12290;
&lt;/p&gt;
&lt;p&gt;
State of What Art? A Call for Multi-Prompt LLM Evaluation. (arXiv:2401.00595v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21628;&#21505;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#26469;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20197;&#35299;&#20915;&#21333;&#25552;&#31034;&#35780;&#20272;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#24403;&#21069;LLMs&#30495;&#27491;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#21508;&#31181;&#35780;&#20272;&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#20381;&#36182;&#20110;&#21333;&#20010;&#25351;&#20196;&#27169;&#26495;&#26469;&#35780;&#20272;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25152;&#26377;LLMs&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#36890;&#36807;&#21333;&#25552;&#31034;&#35780;&#20272;&#33719;&#24471;&#30340;&#32467;&#26524;&#30340;&#33030;&#24369;&#24615;&#65292;&#32435;&#20837;&#20102;6.5M&#20010;&#23454;&#20363;&#65292;&#28041;&#21450;20&#31181;&#19981;&#21516;&#30340;LLMs&#21644;&#26469;&#33258;3&#20010;&#22522;&#20934;&#30340;39&#20010;&#20219;&#21153;&#12290;&#20026;&#20102;&#25913;&#36827;&#20998;&#26512;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#26469;&#35780;&#20272;LLMs&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#29305;&#23450;&#29992;&#20363;&#65288;&#20363;&#22914;LLM&#24320;&#21457;&#20154;&#21592;&#19982;&#23545;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#24863;&#20852;&#36259;&#30340;&#24320;&#21457;&#20154;&#21592;&#65289;&#30340;&#23450;&#21046;&#35780;&#20272;&#25351;&#26631;&#65292;&#30830;&#20445;&#26356;&#21487;&#38752;&#21644;&#26377;&#24847;&#20041;&#30340;LLM&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23454;&#26045;&#36825;&#20123;&#26631;&#20934;&#65292;&#24182;&#23545;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#24403;&#21069;LLMs&#30495;&#27491;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have led to the development of various evaluation benchmarks. These benchmarks typically rely on a single instruction template for evaluating all LLMs on a specific task. In this paper, we comprehensively analyze the brittleness of results obtained via single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. To improve robustness of the analysis, we propose to evaluate LLMs with a set of diverse prompts instead. We discuss tailored evaluation metrics for specific use cases (e.g., LLM developers vs. developers interested in a specific downstream task), ensuring a more reliable and meaningful assessment of LLM capabilities. We then implement these criteria and conduct evaluations of multiple models, providing insights into the true strengths and limitations of current LLMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;</title><link>http://arxiv.org/abs/2312.01185</link><description>&lt;p&gt;
&#26102;&#38388;&#20013;&#30340;&#28063;&#28458;&#65306;&#32654;&#22269;&#21382;&#21490;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#22269;&#24773;&#21672;&#25991;&#25968;&#25454;&#38598;&#23545;&#32654;&#22269;&#21382;&#21490;&#30340;&#24635;&#20307;&#26102;&#38388;&#32447;&#21450;&#21672;&#25991;&#26412;&#36523;&#30340;&#29305;&#28857;&#21644;&#24615;&#36136;&#36827;&#34892;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#65288;&#20063;&#26377;&#20123;&#19981;&#37027;&#20040;&#20196;&#20154;&#24778;&#35766;&#65289;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#65292;&#22914;BERT&#65288;DistilBERT&#65289;&#21644;GPT-2&#12290;&#34429;&#28982;&#24191;&#27867;&#35748;&#20026;BERT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#26368;&#36866;&#21512;NLP&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;GPT-2&#32467;&#21512;UMAP&#31561;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#26356;&#24378;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;&#36825;&#20351;&#24471;GPT-2 + UMAP&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#23601;&#36275;&#22815;&#22909;&#29992;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#26469;&#26816;&#27979;&#21738;&#20301;&#24635;&#32479;&#21457;&#34920;&#20102;&#21738;&#31687;&#28436;&#35762;&#65292;&#24182;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#65288;&#20934;&#30830;&#29575;&#20026;93\% - 95\%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#36816;&#34892;&#24773;&#20917;&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;&#20889;&#20316;&#24180;&#20221;&#65292;&#25105;&#20204;&#36824;&#25191;&#34892;&#20102;&#19968;&#20010;&#31867;&#20284;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#32465;&#23450;ID&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#23558;&#23454;&#20307;&#19982;&#23646;&#24615;&#36827;&#34892;&#26377;&#25928;&#22320;&#32465;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#22914;&#20309;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.17191</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23558;&#23454;&#20307;&#32465;&#23450;&#21040;&#19978;&#19979;&#25991;&#20013;?
&lt;/p&gt;
&lt;p&gt;
How do Language Models Bind Entities in Context?. (arXiv:2310.17191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17191
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#32465;&#23450;ID&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#23558;&#23454;&#20307;&#19982;&#23646;&#24615;&#36827;&#34892;&#26377;&#25928;&#22320;&#32465;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#22914;&#20309;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#27491;&#30830;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24517;&#39035;&#23558;&#23454;&#20307;&#19982;&#20854;&#23646;&#24615;&#36827;&#34892;&#32465;&#23450;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#25551;&#36848;&#8220;&#32511;&#33394;&#26041;&#22359;&#8221;&#21644;&#8220;&#34013;&#33394;&#22278;&#24418;&#8221;&#30340;&#19978;&#19979;&#25991;&#65292;LMs&#24517;&#39035;&#23558;&#24418;&#29366;&#19982;&#23427;&#20204;&#23545;&#24212;&#30340;&#39068;&#33394;&#36827;&#34892;&#32465;&#23450;&#12290;&#25105;&#20204;&#20998;&#26512;LM&#34920;&#31034;&#24182;&#30830;&#23450;&#32465;&#23450;ID&#26426;&#21046;&#65306;&#36825;&#26159;&#19968;&#31181;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#25105;&#20204;&#22312;Pythia&#21644;LLaMA&#23478;&#26063;&#30340;&#27599;&#20010;&#36275;&#22815;&#22823;&#30340;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#12290;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LMs&#20869;&#37096;&#28608;&#27963;&#36890;&#36807;&#23558;&#32465;&#23450;ID&#21521;&#37327;&#38468;&#21152;&#21040;&#30456;&#24212;&#30340;&#23454;&#20307;&#21644;&#23646;&#24615;&#19978;&#26469;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#32465;&#23450;ID&#21521;&#37327;&#24418;&#25104;&#36830;&#32493;&#30340;&#23376;&#31354;&#38388;&#65292;&#22312;&#36825;&#20010;&#23376;&#31354;&#38388;&#20013;&#65292;&#32465;&#23450;ID&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#21453;&#26144;&#20102;&#23427;&#20204;&#30340;&#21306;&#21035;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;LMs&#22312;&#19978;&#19979;&#25991;&#20013;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#30340;&#21487;&#35299;&#37322;&#31574;&#30053;&#65292;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;LMs&#20013;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;LLM-SS&#65292;&#36890;&#36807;&#21512;&#25104;&#23398;&#29983;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#65292;&#20026;&#23398;&#29983;&#24314;&#27169;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#25945;&#23398;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.10690</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#29983;&#24314;&#27169;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20174;&#19968;&#27425;&#24615;&#35266;&#23519;&#20013;&#21512;&#25104;&#35270;&#35273;&#32534;&#31243;&#20013;&#23398;&#29983;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation. (arXiv:2310.10690v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;LLM-SS&#65292;&#36890;&#36807;&#21512;&#25104;&#23398;&#29983;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#65292;&#20026;&#23398;&#29983;&#24314;&#27169;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#24314;&#27169;&#23545;&#20110;&#35768;&#22810;&#25945;&#32946;&#25216;&#26415;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#23398;&#20064;&#32467;&#26524;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#22240;&#20026;&#23398;&#29983;&#34920;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#19988;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#30340;&#23398;&#20064;&#25216;&#33021;&#38598;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LLM-SS&#65292;&#21033;&#29992;LLMs&#21512;&#25104;&#23398;&#29983;&#30340;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#29305;&#23450;&#23398;&#29983;&#22312;&#21442;&#32771;&#20219;&#21153;&#19978;&#30340;&#35299;&#20915;&#23581;&#35797;&#20316;&#20026;&#35266;&#23519;&#65292;&#30446;&#26631;&#26159;&#21512;&#25104;&#35813;&#23398;&#29983;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#19982;&#19981;&#21516;&#30340;LLMs&#32467;&#21512;&#20351;&#29992;&#65307;&#32780;&#19988;&#65292;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#23427;&#20204;&#23545;&#39046;&#22495;&#32972;&#26223;&#21644;&#23398;&#29983;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#20855;&#20307;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Student modeling is central to many educational technologies as it enables the prediction of future learning outcomes and targeted instructional strategies. However, open-ended learning environments pose challenges for accurately modeling students due to the diverse behaviors exhibited by students and the absence of a well-defined set of learning skills. To approach these challenges, we explore the application of Large Language Models (LLMs) for in-context student modeling in open-ended learning environments. We introduce a novel framework, LLM-SS, that leverages LLMs for synthesizing student's behavior. More concretely, given a particular student's solving attempt on a reference task as observation, the goal is to synthesize the student's attempt on a target task. Our framework can be combined with different LLMs; moreover, we fine-tune LLMs using domain-specific expertise to boost their understanding of domain background and student behaviors. We evaluate several concrete methods bas
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#30005;&#36335;&#32452;&#20214;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#22797;&#29992;&#24182;&#20135;&#29983;&#30456;&#20284;&#30340;&#21151;&#33021;&#65292;&#20026;&#26356;&#39640;&#32423;&#30340;&#27169;&#22411;&#29702;&#35299;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.08744</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#20219;&#21153;&#30340;&#30005;&#36335;&#32452;&#20214;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
Circuit Component Reuse Across Tasks in Transformer Language Models. (arXiv:2310.08744v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08744
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#30005;&#36335;&#32452;&#20214;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#22797;&#29992;&#24182;&#20135;&#29983;&#30456;&#20284;&#30340;&#21151;&#33021;&#65292;&#20026;&#26356;&#39640;&#32423;&#30340;&#27169;&#22411;&#29702;&#35299;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#30005;&#36335;&#20998;&#26512;&#21487;&#20197;&#25104;&#21151;&#22320;&#36870;&#21521;&#24037;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25209;&#35780;&#26159;&#27599;&#20010;&#30005;&#36335;&#37117;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#20998;&#26512;&#19981;&#33021;&#20026;&#26356;&#39640;&#32423;&#30340;&#29702;&#35299;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#35777;&#25454;&#34920;&#26126;&#27934;&#23519;&#21147;&#65288;&#20851;&#20110;&#29305;&#23450;&#22836;&#37096;&#30340;&#20302;&#32423;&#21457;&#29616;&#21644;&#20851;&#20110;&#19968;&#33324;&#31639;&#27861;&#30340;&#39640;&#32423;&#21457;&#29616;&#65289;&#30830;&#23454;&#21487;&#20197;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Wang&#31561;&#20154;&#65288;2022&#65289;&#22312;&#38388;&#25509;&#23486;&#35821;&#35782;&#21035;&#20219;&#21153;&#65288;IOI&#65289;&#20013;&#21457;&#29616;&#30340;&#30005;&#36335;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20010;&#30005;&#36335;&#22312;&#26356;&#22823;&#30340;GPT2&#27169;&#22411;&#19978;&#30340;&#37325;&#29616;&#65292;&#20197;&#21450;&#22312;&#30475;&#20284;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#22823;&#37096;&#20998;&#34987;&#22797;&#29992;&#26469;&#35299;&#20915;&#38382;&#39064;&#65306;&#24425;&#33394;&#29289;&#20307;&#65288;Ippolito&#21644;Callison-Burch&#65292;2023&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#20004;&#20010;&#20219;&#21153;&#24213;&#23618;&#30340;&#36807;&#31243;&#22312;&#21151;&#33021;&#19978;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#19988;&#22312;&#30005;&#36335;&#20013;&#30340;&#27880;&#24847;&#21147;&#22836;&#37096;&#20043;&#38388;&#26377;&#22823;&#32422;78&#65285;&#30340;&#37325;&#21472;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#24178;&#39044;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito &amp; Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06387</link><description>&lt;p&gt;
&#21482;&#38656;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#21363;&#21487;&#23454;&#29616;&#36234;&#29425;&#21644;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#30340;&#25285;&#24551;&#20063;&#28014;&#29616;&#20986;&#26469;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30456;&#21516;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#25805;&#32437;LLM&#23545;&#40784;&#33021;&#21147;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#33539;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#23601;&#21487;&#20197;&#25805;&#32437;LLM&#22686;&#21152;&#25110;&#38477;&#20302;&#36234;&#29425;&#27010;&#29575;&#65292;&#21363;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30446;&#30340;&#30340;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#65288;ICA&#65289;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#65288;ICD&#65289;&#26041;&#27861;&#12290;ICA&#36890;&#36807;&#26500;&#36896;&#24694;&#24847;&#19978;&#19979;&#25991;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#36755;&#20986;&#65292;&#32780;ICD&#36890;&#36807;&#25298;&#32477;&#22238;&#31572;&#26377;&#23475;&#25552;&#31034;&#30340;&#31034;&#33539;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ICA&#21644;ICD&#22312;&#22686;&#21152;&#25110;&#38477;&#20302;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;ICL&#22312;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01558</link><description>&lt;p&gt;
&#20351;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#20855;&#26377;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Making Retrieval-Augmented Language Models Robust to Irrelevant Context. (arXiv:2310.01558v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;RALMs&#65289;&#26377;&#26395;&#20135;&#29983;&#20934;&#30830;&#12289;&#39640;&#25928;&#21644;&#26368;&#26032;&#30340;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#12290;RALMs&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#22312;&#30456;&#20851;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#19981;&#30456;&#20851;&#26102;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#36825;&#22312;&#22810;&#36339;&#25512;&#29702;&#22330;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#19981;&#30456;&#20851;&#35777;&#25454;&#30340;&#35823;&#29992;&#20250;&#23548;&#33268;&#36830;&#38145;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26816;&#32034;&#22686;&#24378;&#26377;&#26102;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#20116;&#20010;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#22522;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#65292;&#25551;&#36848;&#20102;&#26816;&#32034;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#32447;&#65292;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#31579;&#36873;&#20986;&#19981;&#28041;&#21450;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#26816;&#32034;&#27573;&#33853;&#12290;&#36825;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#20195;&#20215;&#26159;&#33293;&#24323;&#20102;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding releva
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;RA-DIT&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#19968;&#26159;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#20108;&#26159;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#27599;&#20010;&#27493;&#39588;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.01352</link><description>&lt;p&gt;
RA-DIT: &#26816;&#32034;&#22686;&#24378;&#30340;&#21452;&#37325;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
RA-DIT: Retrieval-Augmented Dual Instruction Tuning. (arXiv:2310.01352v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;RA-DIT&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#19968;&#26159;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#20108;&#26159;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#27599;&#20010;&#27493;&#39588;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RALMs&#65289;&#36890;&#36807;&#35775;&#38382;&#22806;&#37096;&#25968;&#25454;&#23384;&#20648;&#20013;&#30340;&#38271;&#23614;&#21644;&#26368;&#26032;&#30693;&#35782;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#26500;&#24314;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#26816;&#32034;&#29305;&#23450;&#20462;&#25913;&#26469;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#35201;&#20040;&#20351;&#29992;&#20107;&#21518;&#38598;&#25104;&#25968;&#25454;&#23384;&#20648;&#30340;&#26041;&#27861;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;&#8212;&#8212;&#26816;&#32034;&#22686;&#24378;&#30340;&#21452;&#37325;&#25351;&#20196;&#35843;&#20248;&#65288;RA-DIT&#65289;&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#24494;&#35843;&#27493;&#39588;&#65306;&#65288;1&#65289;&#19968;&#20010;&#26356;&#26032;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#65288;2&#65289;&#21478;&#19968;&#20010;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#65292;&#31526;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#38656;&#35201;&#30693;&#35782;&#21033;&#29992;&#21644;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#20010;&#38454;&#27573;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#26159;RA-DIT 65B&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B,
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLM-grounded Video Diffusion (LVD)&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#29983;&#25104;&#21160;&#24577;&#22330;&#26223;&#24067;&#23616;&#65292;&#20877;&#36890;&#36807;&#36825;&#20123;&#24067;&#23616;&#25351;&#23548;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#26102;&#31354;&#25552;&#31034;&#21644;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#29983;&#25104;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.17444</link><description>&lt;p&gt;
LLM&#22522;&#20110;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM-grounded Video Diffusion Models. (arXiv:2309.17444v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17444
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLM-grounded Video Diffusion (LVD)&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#29983;&#25104;&#21160;&#24577;&#22330;&#26223;&#24067;&#23616;&#65292;&#20877;&#36890;&#36807;&#36825;&#20123;&#24067;&#23616;&#25351;&#23548;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#26102;&#31354;&#25552;&#31034;&#21644;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#29983;&#25104;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#31070;&#32463;&#35270;&#39057;&#29983;&#25104;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#26102;&#31354;&#25552;&#31034;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36890;&#24120;&#29983;&#25104;&#21463;&#38480;&#21046;&#25110;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#65288;&#20363;&#22914;&#65292;&#29978;&#33267;&#32570;&#20047;&#20174;&#24038;&#21521;&#21491;&#31227;&#21160;&#30340;&#29289;&#20307;&#30340;&#25552;&#31034;&#33021;&#21147;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLM&#22522;&#20110;&#35270;&#39057;&#25193;&#25955;&#65288;LVD&#65289;&#12290;LVD&#19981;&#30452;&#25509;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#29983;&#25104;&#35270;&#39057;&#65292;&#32780;&#26159;&#39318;&#20808;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26681;&#25454;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#21160;&#24577;&#22330;&#26223;&#24067;&#23616;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#30340;&#24067;&#23616;&#26469;&#25351;&#23548;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#33021;&#22815;&#20174;&#21333;&#32431;&#30340;&#25991;&#26412;&#20013;&#29702;&#35299;&#22797;&#26434;&#30340;&#26102;&#31354;&#21160;&#24577;&#65292;&#24182;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#20013;&#36890;&#24120;&#35266;&#23519;&#21040;&#30340;&#25552;&#31034;&#21644;&#29289;&#20307;&#36816;&#21160;&#27169;&#24335;&#23494;&#20999;&#23545;&#40784;&#30340;&#24067;&#23616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#35843;&#25972;&#27880;&#24847;&#21147;&#22270;&#26469;&#25351;&#23548;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#19982;&#36825;&#20123;&#24067;&#23616;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned diffusion models have emerged as a promising tool for neural video generation. However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion (e.g., even lacking the ability to be prompted for objects moving from left to right). To address these limitations, we introduce LLM-grounded Video Diffusion (LVD). Instead of directly generating videos from the text inputs, LVD first leverages a large language model (LLM) to generate dynamic scene layouts based on the text inputs and subsequently uses the generated layouts to guide a diffusion model for video generation. We show that LLMs are able to understand complex spatiotemporal dynamics from text alone and generate layouts that align closely with both the prompts and the object motion patterns typically observed in the real world. We then propose to guide video diffusion models with these layouts by adjusting the attention maps. Our approach is training-free 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26356;&#36866;&#24212;&#20110;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#35813;&#27169;&#22411;&#20998;&#20026;&#20010;&#20154;&#32423;&#21035;&#12289;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#23567;&#22411;&#21270;&#20197;&#36866;&#24212;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#65292;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.14726</link><description>&lt;p&gt;
PLMM&#65306;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PLMM: Personal Large Models on Mobile Devices. (arXiv:2309.14726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26356;&#36866;&#24212;&#20110;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#35813;&#27169;&#22411;&#20998;&#20026;&#20010;&#20154;&#32423;&#21035;&#12289;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#23567;&#22411;&#21270;&#20197;&#36866;&#24212;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#65292;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26356;&#36866;&#24212;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#22914;&#25945;&#32946;&#32972;&#26223;&#21644;&#29233;&#22909;&#12290;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#20026;&#19977;&#20010;&#32423;&#21035;&#65306;&#20010;&#20154;&#32423;&#21035;&#65292;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#12290;&#20010;&#20154;&#32423;&#21035;&#27169;&#22411;&#36866;&#24212;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#23545;&#29992;&#25143;&#30340;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#24182;&#20445;&#25252;&#20854;&#38544;&#31169;&#12290;&#19987;&#23478;&#32423;&#21035;&#27169;&#22411;&#19987;&#27880;&#20110;&#21512;&#24182;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#22914;&#37329;&#34701;&#12289;IT&#21644;&#33402;&#26415;&#12290;&#20256;&#32479;&#27169;&#22411;&#19987;&#27880;&#20110;&#26222;&#36941;&#30693;&#35782;&#30340;&#21457;&#29616;&#21644;&#25552;&#21319;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#36825;&#26679;&#30340;&#20998;&#31867;&#20013;&#65292;&#20010;&#20154;&#27169;&#22411;&#30452;&#25509;&#19982;&#29992;&#25143;&#20132;&#20114;&#12290;&#23545;&#20110;&#25972;&#20010;&#31995;&#32479;&#26469;&#35828;&#65292;&#20010;&#20154;&#27169;&#22411;&#20855;&#26377;&#29992;&#25143;&#30340;&#65288;&#21152;&#23494;&#30340;&#65289;&#20010;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#36275;&#22815;&#23567;&#20197;&#22312;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#12290;&#26368;&#21518;&#65292;&#23427;&#20204;&#36824;&#24517;&#39035;&#23454;&#26102;&#21709;&#24212;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by Federated Learning, in this paper, we propose personal large models that are distilled from traditional large language models but more adaptive to local users' personal information such as education background and hobbies. We classify the large language models into three levels: the personal level, expert level and traditional level. The personal level models are adaptive to users' personal information. They encrypt the users' input and protect their privacy. The expert level models focus on merging specific knowledge such as finance, IT and art. The traditional models focus on the universal knowledge discovery and upgrading the expert models. In such classifications, the personal models directly interact with the user. For the whole system, the personal models have users' (encrypted) personal information. Moreover, such models must be small enough to be performed on personal computers or mobile devices. Finally, they also have to response in real-time for better user exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#21453;&#39304;&#26469;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20248;&#21270;&#20854;&#24615;&#33021;&#12290;&#36825;&#31181;&#20195;&#29702;&#33021;&#22815;&#20174;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#23398;&#20064;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.02151</link><description>&lt;p&gt;
Retroformer&#65306;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization. (arXiv:2308.02151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#21453;&#39304;&#26469;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20248;&#21270;&#20854;&#24615;&#33021;&#12290;&#36825;&#31181;&#20195;&#29702;&#33021;&#22815;&#20174;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#23398;&#20064;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#20010;&#26376;&#65292;&#20986;&#29616;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26032;&#36235;&#21183;&#65292;&#21363;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#25104;&#33021;&#22815;&#33258;&#20027;&#23436;&#25104;&#30446;&#26631;&#23548;&#21521;&#22810;&#27493;&#39588;&#20219;&#21153;&#30340;&#35821;&#35328;&#20195;&#29702;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22238;&#31572;&#20154;&#31867;&#29992;&#25143;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#27809;&#26377;&#20351;&#29992;&#29615;&#22659;&#29305;&#23450;&#30340;&#22870;&#21169;&#36827;&#34892;&#20248;&#21270;&#12290;&#23613;&#31649;&#19968;&#20123;&#20195;&#29702;&#36890;&#36807;&#21475;&#22836;&#21453;&#39304;&#23454;&#29616;&#20102;&#36845;&#20195;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#20197;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#22870;&#21169;&#23398;&#20064;&#30456;&#20860;&#23481;&#30340;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22238;&#39038;&#27169;&#22411;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#33258;&#21160;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#29615;&#22659;&#21453;&#39304;&#20013;&#20248;&#21270;&#20195;&#29702;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20195;&#29702;&#26550;&#26500;&#36890;&#36807;&#23398;&#20064;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#30340;&#22870;&#21169;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#36716;&#25442;&#27969;&#27700;&#32447;&#65292;&#20197;&#35782;&#21035;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2308.01987</link><description>&lt;p&gt;
&#23391;&#21152;&#25289;&#35821;&#20551;&#35780;&#35770;&#65306;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Bengali Fake Reviews: A Benchmark Dataset and Detection System. (arXiv:2308.01987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01987
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#36716;&#25442;&#27969;&#27700;&#32447;&#65292;&#20197;&#35782;&#21035;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#22312;&#32447;&#24179;&#21488;&#19978;&#20986;&#29616;&#22823;&#37327;&#30340;&#20551;&#35780;&#35770;&#24050;&#32463;&#25104;&#20026;&#28040;&#36153;&#32773;&#21644;&#20225;&#19994;&#30340;&#37325;&#22823;&#20851;&#20999;&#12290;&#36825;&#26679;&#30340;&#35780;&#35770;&#21487;&#20197;&#27450;&#39575;&#28040;&#36153;&#32773;&#65292;&#24182;&#23545;&#20135;&#21697;&#25110;&#26381;&#21153;&#30340;&#22768;&#35465;&#36896;&#25104;&#25439;&#23475;&#65292;&#22240;&#27492;&#35782;&#21035;&#23427;&#20204;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22312;&#33521;&#35821;&#35821;&#35328;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#20551;&#35780;&#35770;&#30340;&#26816;&#27979;&#65292;&#20294;&#22312;&#23391;&#21152;&#25289;&#35821;&#31561;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#26816;&#27979;&#20551;&#35780;&#35770;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#23545;&#26410;&#24320;&#21457;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#26816;&#27979;&#65288;BFRD&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#29992;&#20110;&#35782;&#21035;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#20174;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25910;&#38598;&#21040;&#30340;7710&#26465;&#38750;&#20551;&#21644;1339&#26465;&#20551;&#30340;&#19982;&#39135;&#21697;&#30456;&#20851;&#30340;&#35780;&#35770;&#32452;&#25104;&#12290;&#20026;&#20102;&#23558;&#35780;&#35770;&#20013;&#30340;&#38750;&#23391;&#21152;&#25289;&#35789;&#35821;&#36716;&#25442;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#27969;&#27700;&#32447;&#65292;&#23558;&#33521;&#35821;&#21333;&#35789;&#36716;&#25442;&#20026;&#20854;&#23545;&#24212;&#30340;&#23391;&#21152;&#25289;&#24847;&#20041;&#65292;&#21516;&#26102;&#20063;&#23558;&#32599;&#39532;&#21270;&#30340;&#23391;&#21152;&#25289;&#35821;&#22238;&#38899;&#21040;&#23391;&#21152;&#25289;&#35821;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of fake reviews on various online platforms has created a major concern for both consumers and businesses. Such reviews can deceive customers and cause damage to the reputation of products or services, making it crucial to identify them. Although the detection of fake reviews has been extensively studied in English language, detecting fake reviews in non-English languages such as Bengali is still a relatively unexplored research area. This paper introduces the Bengali Fake Review Detection (BFRD) dataset, the first publicly available dataset for identifying fake reviews in Bengali. The dataset consists of 7710 non-fake and 1339 fake food-related reviews collected from social media posts. To convert non-Bengali words in a review, a unique pipeline has been proposed that translates English words to their corresponding Bengali meaning and also back transliterates Romanized Bengali to Bengali. We have conducted rigorous experimentation using multiple deep learning and pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35843;&#26597;&#30740;&#31350;&#65292;&#24182;&#25506;&#32034;&#20102;&#25552;&#39640;NER&#24615;&#33021;&#30340;&#26377;&#25928;&#23454;&#20307;&#35782;&#21035;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.00186</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#27169;&#22411;&#21040;&#21307;&#23398;&#39046;&#22495;&#30334;&#20998;&#20043;&#30334;&#30340;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26377;&#22810;&#36828;
&lt;/p&gt;
&lt;p&gt;
How far is Language Model from 100% Few-shot Named Entity Recognition in Medical Domain. (arXiv:2307.00186v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35843;&#26597;&#30740;&#31350;&#65292;&#24182;&#25506;&#32034;&#20102;&#25552;&#39640;NER&#24615;&#33021;&#30340;&#26377;&#25928;&#23454;&#20307;&#35782;&#21035;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24341;&#21457;&#20102;&#24378;&#22823;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;T5&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#22914;&#36890;&#29992;&#39046;&#22495;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#25928;&#26524;&#20173;&#28982;&#19981;&#30830;&#23450;&#65292;&#30001;&#20110;&#35813;&#39046;&#22495;&#30340;&#29305;&#27530;&#24615;&#65292;&#21307;&#23398;NER&#30340;&#24615;&#33021;&#24635;&#26159;&#38656;&#35201;&#39640;&#31934;&#24230;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;&#21307;&#23398;&#23569;&#26679;&#26412;NER&#20013;&#30340;LMs&#30340;&#24615;&#33021;&#36827;&#34892;&#24443;&#24213;&#35843;&#26597;&#65292;&#24182;&#22238;&#31572;&#20174;100&#65285;&#30334;&#20998;&#27604;&#20013;LMs&#19982;&#21307;&#23398;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;NER&#26377;&#22810;&#36828;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25506;&#32034;&#19968;&#31181;&#26377;&#25928;&#30340;&#23454;&#20307;&#35782;&#21035;&#22120;&#20197;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#25105;&#20204;&#22312;2018&#24180;&#21040;2023&#24180;&#26399;&#38388;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26126;&#30830;&#34920;&#26126;&#65292;LLMs o
&lt;/p&gt;
&lt;p&gt;
Recent advancements in language models (LMs) have led to the emergence of powerful models such as Small LMs (e.g., T5) and Large LMs (e.g., GPT-4). These models have demonstrated exceptional capabilities across a wide range of tasks, such as name entity recognition (NER) in the general domain. (We define SLMs as pre-trained models with fewer parameters compared to models like GPT-3/3.5/4, such as T5, BERT, and others.) Nevertheless, their efficacy in the medical section remains uncertain and the performance of medical NER always needs high accuracy because of the particularity of the field. This paper aims to provide a thorough investigation to compare the performance of LMs in medical few-shot NER and answer How far is LMs from 100\% Few-shot NER in Medical Domain, and moreover to explore an effective entity recognizer to help improve the NER performance. Based on our extensive experiments conducted on 16 NER models spanning from 2018 to 2023, our findings clearly indicate that LLMs o
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#29992;&#25143;&#26500;&#24314;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#26159;&#21542;&#20173;&#33021;&#20445;&#25345;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25915;&#20987;&#25163;&#27861;&#19981;&#36275;&#20197;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.15447</link><description>&lt;p&gt;
&#23545;&#40784;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#23545;&#25239;&#23545;&#40784;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are aligned neural networks adversarially aligned?. (arXiv:2306.15447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15447
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#29992;&#25143;&#26500;&#24314;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#26159;&#21542;&#20173;&#33021;&#20445;&#25345;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25915;&#20987;&#25163;&#27861;&#19981;&#36275;&#20197;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29616;&#22312;&#34987;&#35843;&#25972;&#20026;&#19982;&#20854;&#21019;&#24314;&#32773;&#30340;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#65292;&#21363;"&#26377;&#30410;&#19988;&#26080;&#23475;"&#12290;&#36825;&#20123;&#27169;&#22411;&#24212;&#35813;&#23545;&#29992;&#25143;&#30340;&#38382;&#39064;&#32473;&#20986;&#26377;&#30410;&#30340;&#22238;&#31572;&#65292;&#20294;&#25298;&#32477;&#22238;&#31572;&#21487;&#33021;&#20250;&#36896;&#25104;&#20260;&#23475;&#30340;&#35831;&#27714;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#32469;&#36807;&#23545;&#40784;&#23581;&#35797;&#30340;&#36755;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19982;&#26500;&#36896;&#26368;&#22351;&#24773;&#20917;&#36755;&#20837;&#65288;&#23545;&#25239;&#24615;&#26679;&#26412;&#65289;&#30340;&#23545;&#25239;&#29992;&#25143;&#20132;&#20114;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20445;&#25345;&#22810;&#22823;&#31243;&#24230;&#30340;&#23545;&#40784;&#12290;&#36825;&#20123;&#36755;&#20837;&#34987;&#35774;&#35745;&#25104;&#23548;&#33268;&#27169;&#22411;&#21457;&#20986;&#26412;&#24212;&#31105;&#27490;&#30340;&#26377;&#23475;&#20869;&#23481;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20248;&#21270;&#25915;&#20987;&#25163;&#27861;&#22312;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#26041;&#38754;&#30340;&#19981;&#36275;&#20043;&#22788;&#65306;&#21363;&#20351;&#22312;&#24403;&#21069;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25915;&#20987;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#25915;&#20987;&#30340;&#22833;&#36133;&#19981;&#24212;&#34987;&#35270;&#20026;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#20173;&#28982;&#20445;&#25345;&#23545;&#40784;&#30340;&#35777;&#26126;&#12290;&#20294;&#26159;&#36817;&#26399;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#26159;&#22810;&#27169;&#24577;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs.  However the recent trend in large-scale ML models is multim
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#22914;&#20309;&#36816;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20013;&#25991;&#26032;&#38395;&#25991;&#26412;&#20449;&#24687;&#30340;&#24773;&#24863;&#22240;&#32032;&#65292;&#20197;&#26399;&#20419;&#36827;&#30693;&#24773;&#21644;&#39640;&#39057;&#30340;&#25237;&#36164;&#32452;&#21512;&#35843;&#25972;&#12290;&#36890;&#36807;&#24314;&#31435;&#20005;&#26684;&#21644;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#19982;&#26631;&#20934;&#21270;&#30340;&#22238;&#27979;&#26694;&#26550;&#65292;&#20316;&#32773;&#23545;&#19981;&#21516;&#31867;&#22411; LLMs &#22312;&#35813;&#39046;&#22495;&#20869;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#23458;&#35266;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.14222</link><description>&lt;p&gt;
&#25581;&#31034;&#24773;&#24863;&#30340;&#28508;&#21147;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#39044;&#27979;&#20013;&#22269;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#65311;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?. (arXiv:2306.14222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#22914;&#20309;&#36816;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20013;&#25991;&#26032;&#38395;&#25991;&#26412;&#20449;&#24687;&#30340;&#24773;&#24863;&#22240;&#32032;&#65292;&#20197;&#26399;&#20419;&#36827;&#30693;&#24773;&#21644;&#39640;&#39057;&#30340;&#25237;&#36164;&#32452;&#21512;&#35843;&#25972;&#12290;&#36890;&#36807;&#24314;&#31435;&#20005;&#26684;&#21644;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#19982;&#26631;&#20934;&#21270;&#30340;&#22238;&#27979;&#26694;&#26550;&#65292;&#20316;&#32773;&#23545;&#19981;&#21516;&#31867;&#22411; LLMs &#22312;&#35813;&#39046;&#22495;&#20869;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#23458;&#35266;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#24555;&#36895;&#21457;&#23637;&#24050;&#24341;&#21457;&#20102;&#24191;&#27867;&#30340;&#35752;&#35770;&#65292;&#20854;&#20013;&#21253;&#25324;&#23427;&#20204;&#23558;&#22914;&#20309;&#25552;&#39640;&#37327;&#21270;&#32929;&#31080;&#20132;&#26131;&#31574;&#30053;&#30340;&#22238;&#25253;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#35752;&#35770;&#20027;&#35201;&#22260;&#32469;&#30528;&#21033;&#29992; LLMs &#30340;&#20986;&#33394;&#29702;&#35299;&#33021;&#21147;&#26469;&#25552;&#21462;&#24773;&#24863;&#22240;&#32032;&#65292;&#20174;&#32780;&#20419;&#36827;&#30693;&#24773;&#21644;&#39640;&#39057;&#30340;&#25237;&#36164;&#32452;&#21512;&#35843;&#25972;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#20123; LLMs &#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20013;&#22269;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#21644;&#38543;&#21518;&#30340;&#20013;&#22269;&#32929;&#31080;&#24066;&#22330;&#20132;&#26131;&#31574;&#30053;&#24320;&#21457;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#26684;&#21644;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#22238;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#23458;&#35266;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411; LLMs &#22312;&#20013;&#25991;&#26032;&#38395;&#25991;&#26412;&#25968;&#25454;&#30340;&#24773;&#24863;&#22240;&#32032;&#25552;&#21462;&#20013;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#30340;&#24037;&#20316;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#29992;&#20102;&#19977;&#20010;&#19981;&#21516;&#27169;&#22411;&#65306;1&#65289;&#29983;&#25104;&#24335; LLM (ChatGPT)&#65292;2&#65289;&#20013;&#25991;&#35821;&#35328;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451; LLM (&#20108;&#37070;&#31070; RoBERTa)&#65292;&#20197;&#21450;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of Large Language Models (LLMs) has led to extensive discourse regarding their potential to boost the return of quantitative stock trading strategies. This discourse primarily revolves around harnessing the remarkable comprehension capabilities of LLMs to extract sentiment factors which facilitate informed and high-frequency investment portfolio adjustments. To ensure successful implementations of these LLMs into the analysis of Chinese financial texts and the subsequent trading strategy development within the Chinese stock market, we provide a rigorous and encompassing benchmark as well as a standardized back-testing framework aiming at objectively assessing the efficacy of various types of LLMs in the specialized domain of sentiment factor extraction from Chinese news text data. To illustrate how our benchmark works, we reference three distinctive models: 1) the generative LLM (ChatGPT), 2) the Chinese language-specific pre-trained LLM (Erlangshen-RoBERTa), and 
&lt;/p&gt;</description></item><item><title>LMFlow&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#21253;&#65292;&#20026;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#23436;&#25972;&#30340;&#24494;&#35843;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#25903;&#25345;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#24182;&#25903;&#25345;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#24494;&#35843;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19987;&#19994;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.12420</link><description>&lt;p&gt;
LMFlow&#65306;&#29992;&#20110;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#21487;&#25193;&#23637;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. (arXiv:2306.12420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12420
&lt;/p&gt;
&lt;p&gt;
LMFlow&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#21253;&#65292;&#20026;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#23436;&#25972;&#30340;&#24494;&#35843;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#25903;&#25345;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#24182;&#25903;&#25345;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#24494;&#35843;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19987;&#19994;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#23637;&#29616;&#20986;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#25509;&#36817;&#20154;&#31867;&#26234;&#33021;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#27169;&#22411;&#22312;&#19987;&#19994;&#20219;&#21153;&#24212;&#29992;&#20013;&#20173;&#28982;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#32570;&#38519;&#65292;&#38656;&#35201;&#24494;&#35843;&#25165;&#33021;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#21487;&#29992;&#27169;&#22411;&#21644;&#19987;&#19994;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36890;&#29992;&#24494;&#35843;&#30340;&#24037;&#20316;&#21464;&#24471;&#38750;&#24120;&#26840;&#25163;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#31532;&#19968;&#27493;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#21253;LMFlow&#65292;&#26088;&#22312;&#31616;&#21270;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#25512;&#29702;&#12290;LMFlow&#20026;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#24494;&#35843;&#24037;&#20316;&#27969;&#31243;&#65292;&#25903;&#25345;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25903;&#25345;&#36830;&#32493;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#31561;&#21151;&#33021;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#19987;&#19994;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large foundation models have demonstrated a great ability to achieve general human-level intelligence far beyond traditional approaches. As the technique keeps attracting attention from the AI community, more and more large foundation models have become publically available. However, most of those models exhibit a major deficiency in specialized-task applications, where the step of finetuning is still required for obtaining satisfactory performance. As the number of available models and specialized tasks keeps growing, the job of general finetuning becomes highly nontrivial. In this paper, we take the first step to address this issue. We introduce an extensible and lightweight toolkit, LMFlow, which aims to simplify the finetuning and inference of general large foundation models. LMFlow offers a complete finetuning workflow for a large foundation model to support personalized training with limited computing resources. Furthermore, it supports continuous pretraining, instruction tuning,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#20803;&#20998;&#24067;&#26041;&#27861;&#65288;MDM&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#30340;&#23545;&#27604;&#26144;&#23556;&#21040;&#36136;&#37327;&#24230;&#37327;&#19978;&#65292;&#21487;&#20197;&#35270;&#20026;&#20998;&#24067;&#30340;&#20998;&#24067;&#65292;&#20854;&#21487;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.11879</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#20998;&#24067;&#24314;&#27169;&#30340;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Open-Domain Text Evaluation via Meta Distribution Modeling. (arXiv:2306.11879v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#20803;&#20998;&#24067;&#26041;&#27861;&#65288;MDM&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#30340;&#23545;&#27604;&#26144;&#23556;&#21040;&#36136;&#37327;&#24230;&#37327;&#19978;&#65292;&#21487;&#20197;&#35270;&#20026;&#20998;&#24067;&#30340;&#20998;&#24067;&#65292;&#20854;&#21487;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25511;&#21046;&#21644;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36798;&#21040;&#25152;&#38656;&#23646;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#32771;&#25991;&#26412;&#30340;&#24230;&#37327;&#26631;&#20934;&#22914;BLEU&#12289;ROUGE&#21644;METEOR&#23545;&#20110;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#26469;&#35828;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#21516;&#26679;&#22320;&#65292;&#34429;&#28982;&#20855;&#22791;&#35757;&#32451;&#37492;&#21035;&#22120;&#30340;&#24230;&#37327;&#26631;&#20934;&#34920;&#29616;&#20986;&#20102;&#24076;&#26395;&#30340;&#21069;&#26223;&#65292;&#20294;&#26159;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#21017;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#8212;&#8212;&#20803;&#20998;&#24067;&#26041;&#27861;&#65288;MDM&#65289;&#12290;&#36890;&#36807;&#32771;&#34385;LLMs&#21442;&#25968;&#25968;&#37327;&#19978;&#21319;&#21644;&#24615;&#33021;&#25552;&#21319;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;MDM &#21019;&#36896;&#20102;&#19968;&#20010;&#26144;&#23556;&#65292;&#23558;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#30340;&#23545;&#27604;&#65288;&#19968;&#20010;&#24050;&#30693;&#20248;&#20110;&#21478;&#19968;&#20010;&#65289;&#26144;&#23556;&#21040;&#36136;&#37327;&#24230;&#37327;&#19978;&#65292;&#35813;&#24230;&#37327;&#21487;&#20197;&#35270;&#20026;&#20998;&#24067;&#30340;&#20998;&#24067;&#65292;&#21363;&#20803;&#20998;&#24067;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;MDM&#22312;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in open-domain text generation models powered by large pre-trained language models (LLMs) have achieved remarkable performance. However, evaluating and controlling these models for desired attributes remains a challenge, as traditional reference-based metrics such as BLEU, ROUGE, and METEOR are insufficient for open-ended generation tasks. Similarly, while trainable discriminator-based evaluation metrics show promise, obtaining high-quality training data is a non-trivial task. In this paper, we introduce a novel approach to evaluate open-domain generation - the Meta-Distribution Methods (MDM). Drawing on the correlation between the rising parameter counts and the improving performance of LLMs, MDM creates a mapping from the contrast of two probabilistic distributions -- one known to be superior to the other -to quality measures, which can be viewed as a distribution of distributions i.e. Meta-Distribution. We investigate MDM for open-domain text generation evaluation 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Wanda&#30340;&#26032;&#39062;&#12289;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#36755;&#20986;&#19978;&#30340;&#26435;&#37325;&#25353;&#29031;&#26368;&#23567;&#24133;&#24230;&#20056;&#20197;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#26469;&#36827;&#34892;&#21098;&#26525;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2306.11695</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple and Effective Pruning Approach for Large Language Models. (arXiv:2306.11695v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Wanda&#30340;&#26032;&#39062;&#12289;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#36755;&#20986;&#19978;&#30340;&#26435;&#37325;&#25353;&#29031;&#26368;&#23567;&#24133;&#24230;&#20056;&#20197;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#26469;&#36827;&#34892;&#21098;&#26525;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#30340;&#33258;&#28982;&#20505;&#36873;&#23545;&#35937;&#65306;&#36825;&#20123;&#26041;&#27861;&#22312;&#21162;&#21147;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20002;&#24323;&#20102;&#32593;&#32476;&#26435;&#37325;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#21313;&#20159;&#32423;&#21035;&#30340;LLMs&#26469;&#35828;&#24456;&#23569;&#21487;&#34892;&#65292;&#35201;&#20040;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#20381;&#36182;&#20108;&#38454;&#20449;&#24687;&#30340;&#26435;&#37325;&#37325;&#26500;&#38382;&#39064;&#65292;&#36825;&#20063;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#31216;&#20026;Wanda&#65288;&#22522;&#20110;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;&#21098;&#26525;&#65289;&#65292;&#26088;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;LLMs&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#21463;&#21040;&#26368;&#36817;&#23545;LLMs&#20013;&#20986;&#29616;&#30340;&#22823;&#24133;&#29305;&#24449;&#30340;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#36755;&#20986;&#19978;&#25353;&#29031;&#26435;&#37325;&#21644;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#30456;&#20056;&#30340;&#26368;&#23567;&#24133;&#24230;&#26469;&#21098;&#26525;&#26435;&#37325;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Wanda&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#65292;&#21098;&#26525;&#21518;&#30340;LLM&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;LLaMA&#21644;LLaMA-2&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;Wanda&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across vari
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#26469;&#20943;&#23569;&#38750;&#27954;&#21475;&#38899;&#20020;&#24202;ASR&#35757;&#32451;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#29616;&#26377;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#21475;&#38899;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.02105</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#20197;&#24212;&#23545;&#20302;&#36164;&#28304;&#20020;&#24202;&#35821;&#38899;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Adapting Pretrained ASR Models to Low-resource Clinical Speech using Epistemic Uncertainty-based Data Selection. (arXiv:2306.02105v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#26469;&#20943;&#23569;&#38750;&#27954;&#21475;&#38899;&#20020;&#24202;ASR&#35757;&#32451;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#29616;&#26377;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#21475;&#38899;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;ASR&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#38750;&#27954;&#21475;&#38899;&#30340;&#20020;&#24202;ASR&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#26500;&#24314;&#24378;&#22823;&#30340;ASR&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;&#21644;&#24418;&#24577;&#20016;&#23500;&#30340;&#21475;&#38899;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#30340;&#21019;&#24314;&#25104;&#26412;&#36739;&#39640;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#20943;&#23569;&#27880;&#37322;&#36153;&#29992;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#21487;&#20197;&#36229;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;ASR&#27169;&#22411;&#24314;&#31435;&#30340;&#20960;&#20010;&#22522;&#20934;&#32467;&#26524;&#65292;&#21516;&#26102;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#65292;&#20174;&#32780;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#25913;&#21892;&#20102;&#20302;&#36164;&#28304;&#21475;&#38899;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38750;&#27954;&#20020;&#24202;ASR&#30340;&#32972;&#26223;&#19979;&#26500;&#24314;&#27867;&#21270;&#22411;ASR&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#20027;&#35201;&#26159;&#31232;&#32570;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there has been significant progress in ASR, African-accented clinical ASR has been understudied due to a lack of training datasets. Building robust ASR systems in this domain requires large amounts of annotated or labeled data, for a wide variety of linguistically and morphologically rich accents, which are expensive to create. Our study aims to address this problem by reducing annotation expenses through informative uncertainty-based data selection. We show that incorporating epistemic uncertainty into our adaptation rounds outperforms several baseline results, established using state-of-the-art (SOTA) ASR models, while reducing the required amount of labeled data, and hence reducing annotation costs. Our approach also improves out-of-distribution generalization for very low-resource accents, demonstrating the viability of our approach for building generalizable ASR models in the context of accented African clinical ASR, where training datasets are predominantly scarce.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31867;&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#20154;&#26426;&#23545;&#35805;&#20219;&#21153;&#65292;&#20197;&#21450;&#22312;&#20250;&#35758;&#35770;&#25991;&#23457;&#31295;&#20154;&#20998;&#37197;&#12289;&#22478;&#24066;&#22810;&#27493;&#34892;&#31243;&#35268;&#21010;&#21644;&#26053;&#34892;&#35745;&#21010;&#21327;&#21830;&#31561;&#22330;&#26223;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#21644;&#29992;&#25143;&#19981;&#21516;&#30340;&#33021;&#21147;&#22914;&#20309;&#32467;&#21512;&#20197;&#36798;&#21040;&#26368;&#20339;&#20915;&#31574;&#12290;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#23545;&#35805;&#29615;&#22659;&#24182;&#36827;&#34892;&#20154;&#26426;&#23545;&#35805;&#25910;&#38598;&#25968;&#25454;&#65292;&#21457;&#29616;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22312;&#27492;&#31867;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.20076</link><description>&lt;p&gt;
&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#20154;&#26426;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Decision-Oriented Dialogue for Human-AI Collaboration. (arXiv:2305.20076v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20076
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31867;&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#20154;&#26426;&#23545;&#35805;&#20219;&#21153;&#65292;&#20197;&#21450;&#22312;&#20250;&#35758;&#35770;&#25991;&#23457;&#31295;&#20154;&#20998;&#37197;&#12289;&#22478;&#24066;&#22810;&#27493;&#34892;&#31243;&#35268;&#21010;&#21644;&#26053;&#34892;&#35745;&#21010;&#21327;&#21830;&#31561;&#22330;&#26223;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#21644;&#29992;&#25143;&#19981;&#21516;&#30340;&#33021;&#21147;&#22914;&#20309;&#32467;&#21512;&#20197;&#36798;&#21040;&#26368;&#20339;&#20915;&#31574;&#12290;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#23545;&#35805;&#29615;&#22659;&#24182;&#36827;&#34892;&#20154;&#26426;&#23545;&#35805;&#25910;&#38598;&#25968;&#25454;&#65292;&#21457;&#29616;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22312;&#27492;&#31867;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31867;&#20219;&#21153;&#65292;&#31216;&#20026;&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#23545;&#35805;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#24517;&#39035;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#19968;&#21517;&#25110;&#22810;&#21517;&#20154;&#31867;&#21512;&#20316;&#65292;&#24110;&#21161;&#20182;&#20204;&#20570;&#20986;&#22797;&#26434;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#39046;&#22495;&#20013;&#24418;&#24335;&#21270;&#29992;&#25143;&#38754;&#20020;&#26085;&#24120;&#20915;&#31574;&#30340;&#36807;&#31243;&#65306;&#65288;1&#65289;&#36873;&#25321;&#20250;&#35758;&#35770;&#25991;&#30340;&#23457;&#31295;&#20154;&#20998;&#37197;&#65292;&#65288;2&#65289;&#22312;&#22478;&#24066;&#20013;&#35268;&#21010;&#22810;&#27493;&#34892;&#31243;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20026;&#19968;&#32676;&#26379;&#21451;&#21327;&#21830;&#26053;&#34892;&#35745;&#21010;&#12290;&#22312;&#27599;&#20010;&#35774;&#32622;&#20013;&#65292;AI&#21161;&#25163;&#21644;&#29992;&#25143;&#20855;&#26377;&#19981;&#21516;&#30340;&#33021;&#21147;&#65292;&#20182;&#20204;&#24517;&#39035;&#32467;&#21512;&#36215;&#26469;&#24471;&#20986;&#26368;&#20339;&#20915;&#31574;&#65306;&#21161;&#25163;&#21487;&#20197;&#35775;&#38382;&#21644;&#22788;&#29702;&#22823;&#37327;&#20449;&#24687;&#65292;&#32780;&#29992;&#25143;&#20855;&#26377;&#31995;&#32479;&#22806;&#30340;&#20559;&#22909;&#21644;&#38480;&#21046;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23545;&#35805;&#29615;&#22659;&#65292;&#20854;&#20013;&#20195;&#29702;&#26681;&#25454;&#20182;&#20204;&#36798;&#21040;&#30340;&#26368;&#32456;&#20915;&#31574;&#30340;&#36136;&#37327;&#33719;&#24471;&#22870;&#21169;&#12290;&#20351;&#29992;&#36825;&#20123;&#29615;&#22659;&#65292;&#25105;&#20204;&#19982;&#20154;&#20204;&#25198;&#28436;&#21161;&#25163;&#30340;&#20154;&#36827;&#34892;&#20102;&#20154;&#26426;&#23545;&#35805;&#12290;&#20026;&#20102;&#27604;&#36739;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#30340;&#20132;&#27969;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;-&#20154;&#31867;&#23545;&#35805;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#20915;&#31574;&#23548;&#21521;&#23545;&#35805;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#20984;&#26174;&#20102;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a class of tasks called decision-oriented dialogues, in which AI assistants must collaborate with one or more humans via natural language to help them make complex decisions. We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends. In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: assistants can access and process large amounts of information, while users have preferences and constraints external to the system. For each task, we build a dialogue environment where agents receive a reward based on the quality of the final decision they reach. Using these environments, we collect human-human dialogues with humans playing the role of assistant. To compare how current AI assistants communicate in these settings, we present bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20197;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#24182;&#20026;&#20559;&#35265;&#32531;&#35299;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.18569</link><description>&lt;p&gt;
ChatGPT&#30340;&#20844;&#24179;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fairness of ChatGPT. (arXiv:2305.18569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20197;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#24182;&#20026;&#20559;&#35265;&#32531;&#35299;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#35299;&#20915;LLM&#20013;&#19981;&#20844;&#24179;&#30340;&#38382;&#39064;&#23545;&#20110;&#36127;&#36131;&#20219;&#30340;AI&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;LLM&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#39046;&#22495;&#26102;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#20844;&#24179;&#35780;&#20272;&#26041;&#38754;&#65292;&#25968;&#37327;&#20998;&#26512;&#21644;&#28145;&#20837;&#30740;&#31350;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35780;&#20272;ChatGPT&#22312;&#21253;&#25324;&#25945;&#32946;&#12289;&#29359;&#32618;&#23398;&#12289;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#36827;&#34892;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#21644;&#20010;&#20154;&#20844;&#24179;&#24615;&#65292;&#24182;&#35266;&#23519;&#20102;&#22312;&#19968;&#31995;&#21015;&#26377;&#20559;&#25110;&#26080;&#20559;&#25552;&#31034;&#19979;ChatGPT&#36755;&#20986;&#30340;&#24046;&#24322;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#20415;&#20110;&#20559;&#35265;&#32531;&#35299;&#65292;&#20419;&#36827;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20855;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To make thorough evaluation, we consider both group fairness and individual fairness and we also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22522;&#30784;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;&#23545;&#25239;&#20803;&#35780;&#20272;&#23545;&#35805;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#36825;&#20123;&#35780;&#20272;&#22120;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14658</link><description>&lt;p&gt;
&#26080;&#27861;&#35780;&#20272;&#30340;&#29983;&#25104;&#21709;&#24212;&#36136;&#37327;&#30340;&#35780;&#20272;: Evaluate What You Can't Evaluate
&lt;/p&gt;
&lt;p&gt;
Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality. (arXiv:2305.14658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22522;&#30784;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;&#23545;&#25239;&#20803;&#35780;&#20272;&#23545;&#35805;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#36825;&#20123;&#35780;&#20272;&#22120;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#34429;&#28982;&#20197;LLMs&#20026;&#22522;&#30784;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#27604;&#20256;&#32479;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#35780;&#20272;&#22120;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#20154;&#31867;&#35821;&#20041;&#23545;&#40784;&#24230;&#65292;&#20294;&#26159;&#22312;&#20351;&#29992;&#20197;LLMs&#20026;&#22522;&#30784;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#26102;&#20173;&#28982;&#23384;&#22312;&#24456;&#22810;&#25361;&#25112;&#12290;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#26356;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#35821;&#20041;&#21709;&#24212;&#30340;&#24320;&#25918;&#24335;&#20363;&#23376;&#12290;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#20363;&#23376;&#37117;&#26159;&#24320;&#25918;&#24335;&#30340;&#65292;&#23545;&#20110;&#20855;&#26377;&#21807;&#19968;&#27491;&#30830;&#35821;&#20041;&#21709;&#24212;&#30340;&#38381;&#21512;&#24335;&#20363;&#23376;&#65292;&#22914;&#26524;&#32473;&#20986;&#19982;&#20107;&#23454;&#21644;&#21442;&#32771;&#30340;&#35821;&#20041;&#19981;&#19968;&#33268;&#30340;&#21709;&#24212;&#65292;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#20173;&#28982;&#20250;&#35748;&#20026;&#20854;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#20197;LLMs&#20026;&#22522;&#30784;&#30340;&#35780;&#20272;&#22120;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#23545;&#25239;&#20803;&#35780;&#20272;&#23545;&#35805;&#29983;&#25104;&#25968;&#25454;&#38598;KdConv-ADV&#21644;DSTC7-ADV&#22522;&#20110;KdConv&#21644;DSTC7-AVSD&#12290;&#19982;&#20197;&#21069;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#30456;&#27604;&#65292;KdConv-ADV&#21644;DSTC7-ADV&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs (large language models) such as ChatGPT have shown remarkable language understanding and generation capabilities. Although reference-free evaluators based on LLMs show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on LLMs. Reference-free evaluators are more suitable for open-ended examples with different semantics responses. But not all examples are open-ended. For closed-ended examples with unique correct semantic response, reference-free evaluators will still consider it high quality when giving a response that is inconsistent with the facts and the semantic of reference. In order to comprehensively evaluate the reliability of evaluators based on LLMs, we construct two adversarial meta-evaluation dialogue generation datasets KdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared to previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more challengin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BBT-RGB&#65292;&#19968;&#22871;&#29992;&#20110;&#22686;&#24378;&#40657;&#30418;&#20248;&#21270;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#30452;&#25509;&#19988;&#20114;&#34917;&#25216;&#26415;&#22871;&#20214;&#65292;&#21253;&#25324;&#20004;&#38454;&#27573;&#26080;&#23548;&#25968;&#20248;&#21270;&#31574;&#30053;&#12289;&#33258;&#21160;&#35821;&#35328;&#36716;&#21270;&#22120;&#26500;&#24314;&#21450;&#20854;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#26032;&#29992;&#27861;&#20197;&#21450;&#26356;&#22909;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.08088</link><description>&lt;p&gt;
&#20351;&#22522;&#20110;&#25552;&#31034;&#30340;&#40657;&#30418;&#35843;&#20248;&#26356;&#21152;&#20016;&#23500;&#22810;&#24425;&#65306;&#20174;&#19977;&#20010;&#27491;&#20132;&#35282;&#24230;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives. (arXiv:2305.08088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BBT-RGB&#65292;&#19968;&#22871;&#29992;&#20110;&#22686;&#24378;&#40657;&#30418;&#20248;&#21270;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#30452;&#25509;&#19988;&#20114;&#34917;&#25216;&#26415;&#22871;&#20214;&#65292;&#21253;&#25324;&#20004;&#38454;&#27573;&#26080;&#23548;&#25968;&#20248;&#21270;&#31574;&#30053;&#12289;&#33258;&#21160;&#35821;&#35328;&#36716;&#21270;&#22120;&#26500;&#24314;&#21450;&#20854;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#26032;&#29992;&#27861;&#20197;&#21450;&#26356;&#22909;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24050;&#32463;&#23637;&#29616;&#20986;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35843;&#25972;&#36825;&#20123;&#27169;&#22411;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#24040;&#39069;&#30340;&#20195;&#20215;&#25110;&#30001;&#20110;&#21830;&#19994;&#32771;&#34385;&#32780;&#19981;&#21487;&#29992;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#40657;&#30418;&#35843;&#20248;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#32780;&#19981;&#35775;&#38382;&#26799;&#24230;&#21644;&#38544;&#34255;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20316;&#21697;&#36824;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#19979;&#26080;&#26799;&#24230;&#20248;&#21270;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;BBT-RGB&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#40657;&#30418;&#20248;&#21270;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#30452;&#25509;&#19988;&#20114;&#34917;&#25216;&#26415;&#22871;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#32452;&#20214;&#65306;&#65288;1&#65289;&#20004;&#38454;&#27573;&#26080;&#23548;&#25968;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#25910;&#25947;&#24182;&#32531;&#35299;&#36807;&#25311;&#21512;&#65307;&#65288;2&#65289;&#33258;&#21160;&#35821;&#35328;&#36716;&#21270;&#22120;&#26500;&#24314;&#21450;&#20854;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#26032;&#29992;&#27861;&#65307;&#65288;3&#65289;&#26356;&#22909;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#65292;&#22522;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#35328;&#23398;&#21160;&#26426;&#21477;&#27861;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown increasing power on various natural language processing (NLP) tasks. However, tuning these models for downstream tasks usually needs exorbitant costs or is unavailable due to commercial considerations. Recently, black-box tuning has been proposed to address this problem by optimizing task-specific prompts without accessing the gradients and hidden representations. However, most existing works have yet fully exploited the potential of gradient-free optimization under the scenario of few-shot learning. In this paper, we describe BBT-RGB, a suite of straightforward and complementary techniques for enhancing the efficiency and performance of black-box optimization. Specifically, our method includes three plug-and-play components: (1) Two-stage derivative-free optimization strategy that facilitates fast convergence and mitigates overfitting; (2) Automatic verbalizer construction with its novel usage under few-shot settings; (3) Better prompt initializ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#36827;&#34892;&#31435;&#22330;&#26816;&#27979;&#65306;&#26377;&#30417;&#30563;&#20998;&#31867;&#12289;&#38646;&#26679;&#26412;&#20998;&#31867;&#19982;NLI&#20998;&#31867;&#22120;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35821;&#35328;&#20998;&#31867;&#22120;&#21487;&#20197;&#26367;&#20195;&#20154;&#24037;&#26631;&#31614;&#32773;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36816;&#29992;&#23427;&#20204;&#26469;&#25191;&#34892;&#31435;&#22330;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.01723</link><description>&lt;p&gt;
&#21033;&#29992;&#26377;&#30417;&#30563;&#12289;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24212;&#29992;&#36827;&#34892;&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Stance Detection With Supervised, Zero-Shot, and Few-Shot Applications. (arXiv:2305.01723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#36827;&#34892;&#31435;&#22330;&#26816;&#27979;&#65306;&#26377;&#30417;&#30563;&#20998;&#31867;&#12289;&#38646;&#26679;&#26412;&#20998;&#31867;&#19982;NLI&#20998;&#31867;&#22120;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35821;&#35328;&#20998;&#31867;&#22120;&#21487;&#20197;&#26367;&#20195;&#20154;&#24037;&#26631;&#31614;&#32773;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36816;&#29992;&#23427;&#20204;&#26469;&#25191;&#34892;&#31435;&#22330;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#26816;&#27979;&#26159;&#35782;&#21035;&#20316;&#32773;&#23545;&#19968;&#20010;&#20027;&#39064;&#30340;&#20449;&#20208;&#30340;&#36807;&#31243;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20381;&#36182;&#24773;&#24863;&#20998;&#26512;&#26469;&#23436;&#25104;&#36825;&#19968;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#24773;&#24863;&#20998;&#26512;&#21482;&#19982;&#31435;&#22330;&#23384;&#22312;&#26494;&#25955;&#30340;&#30456;&#20851;&#24615;&#65292;&#22914;&#26524;&#26377;&#30340;&#35805;&#12290;&#26412;&#25991;&#36890;&#36807;&#31934;&#30830;&#23450;&#20041;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#12289;&#25552;&#20379;&#20219;&#21153;&#30340;&#26222;&#36866;&#26694;&#26550;&#65292;&#24182;&#38024;&#23545;&#25191;&#34892;&#31435;&#22330;&#26816;&#27979;&#30340;&#19977;&#31181;&#19981;&#21516;&#26041;&#27861;&#65288;&#26377;&#30417;&#30563;&#20998;&#31867;&#12289;&#38646;&#26679;&#26412;&#20998;&#31867;&#19982;NLI&#20998;&#31867;&#22120;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#25552;&#20986;&#26032;&#30340;&#30740;&#31350;&#26041;&#27861;&#20197;&#25512;&#36827;&#25991;&#26412;&#20998;&#26512;&#26041;&#24335;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#26412;&#25991;&#35828;&#26126;&#20102;&#22914;&#20309;&#20351;&#29992;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35821;&#35328;&#20998;&#31867;&#22120;&#26367;&#20195;&#20154;&#24037;&#26631;&#31614;&#32773;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#21644;&#23616;&#38480;&#24615;&#19982;&#26377;&#30417;&#30563;&#20998;&#31867;&#22120;&#19981;&#21516;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#36890;&#36807;&#22797;&#21046;Block Jr&#31561;&#20154; (2022)&#30340;&#26041;&#27861;&#65292;&#28436;&#31034;&#20102;&#38646;&#26679;&#26412;&#31435;&#22330;&#26816;&#27979;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stance detection is the identification of an author's beliefs about a subject from a document. Researchers widely rely on sentiment analysis to accomplish this. However, recent research has show that sentiment analysis is only loosely correlated with stance, if at all. This paper advances methods in text analysis by precisely defining the task of stance detection, providing a generalized framework for the task, and then presenting three distinct approaches for performing stance detection: supervised classification, zero-shot classification with NLI classifiers, and in-context learning. In doing so, I demonstrate how zero-shot and few-shot language classifiers can replace human labelers for a variety of tasks and discuss how their application and limitations differ from supervised classifiers. Finally, I demonstrate an application of zero-shot stance detection by replicating Block Jr et al. (2022).
&lt;/p&gt;</description></item><item><title>AraSpot&#26159;&#19968;&#27454;&#20351;&#29992;ConformerGRU&#27169;&#22411;&#26550;&#26500;&#35757;&#32451;40&#20010;&#38463;&#25289;&#20271;&#35821;&#20851;&#38190;&#35789;&#30340;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;&#24037;&#20855;&#65292;&#20854;&#36890;&#36807;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#30340;&#35757;&#32451;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#20197;99.59%&#30340;&#20934;&#30830;&#29575;&#36229;&#20986;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16621</link><description>&lt;p&gt;
AraSpot&#65306;&#38463;&#25289;&#20271;&#35821;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
AraSpot: Arabic Spoken Command Spotting. (arXiv:2303.16621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16621
&lt;/p&gt;
&lt;p&gt;
AraSpot&#26159;&#19968;&#27454;&#20351;&#29992;ConformerGRU&#27169;&#22411;&#26550;&#26500;&#35757;&#32451;40&#20010;&#38463;&#25289;&#20271;&#35821;&#20851;&#38190;&#35789;&#30340;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;&#24037;&#20855;&#65292;&#20854;&#36890;&#36807;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#30340;&#35757;&#32451;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#20197;99.59%&#30340;&#20934;&#30830;&#29575;&#36229;&#20986;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#20851;&#38190;&#35782;&#21035;&#65288;KWS&#65289;&#26159;&#25351;&#22312;&#38899;&#39057;&#27969;&#20013;&#35782;&#21035;&#20851;&#38190;&#35789;&#65292;&#24191;&#27867;&#29992;&#20110;&#26234;&#33021;&#36793;&#32536;&#35774;&#22791;&#19978;&#65292;&#20197;&#21551;&#21160;&#35821;&#38899;&#21161;&#25163;&#21644;&#36827;&#34892;&#20813;&#25552;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#38754;&#20020;&#30528;&#39640;&#31934;&#24230;&#21644;&#22312;&#20302;&#21151;&#29575;&#21644;&#21487;&#33021;&#30340;&#26377;&#38480;&#35745;&#31639;&#33021;&#21147;&#35774;&#22791;&#19978;&#20445;&#25345;&#31995;&#32479;&#36816;&#34892;&#25928;&#29575;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#19981;&#21516;&#30340;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#24341;&#20837;ConformerGRU&#27169;&#22411;&#26550;&#26500;&#30340;AraSpot&#65292;&#29992;&#20110;&#35757;&#32451;40&#20010;&#38463;&#25289;&#20271;&#35821;&#20851;&#38190;&#35789;&#30340;&#35782;&#21035;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;AraSpot&#20197;SOTA 99.59&#65285;&#36229;&#36807;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken keyword spotting (KWS) is the task of identifying a keyword in an audio stream and is widely used in smart devices at the edge in order to activate voice assistants and perform hands-free tasks. The task is daunting as there is a need, on the one hand, to achieve high accuracy while at the same time ensuring that such systems continue to run efficiently on low power and possibly limited computational capabilities devices. This work presents AraSpot for Arabic keyword spotting trained on 40 Arabic keywords, using different online data augmentation, and introducing ConformerGRU model architecture. Finally, we further improve the performance of the model by training a text-to-speech model for synthetic data generation. AraSpot achieved a State-of-the-Art SOTA 99.59% result outperforming previous approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#24182;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#21644;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;PCP&#28040;&#34701;&#25216;&#26415;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#25105;&#20204;&#25104;&#21151;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.12461</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. (arXiv:2302.12461v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#24182;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#21644;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;PCP&#28040;&#34701;&#25216;&#26415;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#25105;&#20204;&#25104;&#21151;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#20013;&#30340;&#27602;&#21270;&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#23433;&#20840;&#23041;&#32961;&#65292;&#21487;&#33021;&#23548;&#33268;&#26263;&#34255;&#21518;&#38376;&#30340;&#27169;&#22411;&#12290;&#20851;&#20110;&#26263;&#34255;&#21518;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#22788;&#29702;&#35302;&#21457;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#20999;&#25442;&#33267;&#26377;&#27602;&#35821;&#35328;&#65289;&#30340;&#25551;&#36848;&#23578;&#26410;&#25214;&#21040;&#12290;&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;Transformer&#30340;&#26263;&#34255;&#21518;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#30830;&#23450;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#19982;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#32467;&#21512;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#29992;&#24037;&#31243;&#21270;&#26367;&#20195;&#29289;&#38477;&#20302;MLP&#27169;&#22359;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20027;&#35201;&#25104;&#20998;&#30340;&#20302;&#31209;&#30697;&#38453;&#30340;PCP&#28040;&#34701;&#25216;&#26415;&#65292;&#29992;&#20854;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#26263;&#34255;&#21518;&#38376;&#30340;&#29609;&#20855;&#27169;&#22411;&#12289;&#26263;&#34255;&#21518;&#38376;&#30340;&#22823;&#22411;&#27169;&#22411;&#21644;&#38750;&#26263;&#34255;&#21518;&#38376;&#30340;&#24320;&#28304;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#25913;&#21892;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33041;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.06348</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#33041;&#20449;&#21495;&#25581;&#31034;&#20154;&#31867;&#35821;&#35328;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Brain Signals Reveal Inner Alignment with Human Languages?. (arXiv:2208.06348v4 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33041;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#20449;&#21495;&#65288;&#22914;&#33041;&#30005;&#22270;&#65289;&#21644;&#20154;&#31867;&#35821;&#35328;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20108;&#32773;&#20043;&#38388;&#30340;&#32852;&#31995;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33041;&#30005;&#22270;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20381;&#36182;&#24615;&#12290;&#22312;&#34920;&#31034;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#65288;Multimodal Transformer Alignment Model&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35266;&#23519;&#36825;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#21327;&#35843;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#31181;&#20851;&#31995;&#23545;&#40784;&#25216;&#26415;&#65292;&#22914;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#21644;Wasserstein&#36317;&#31163;&#65292;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#36716;&#25442;&#29305;&#24449;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#22312;ZuCo&#21644;K-EmoCon&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#20351;K-EmoCon&#25968;&#25454;&#38598;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#20102;1.7&#65285;&#65292;ZuCo&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;9.3&#65285;&#65292;&#22312;&#20851;&#31995;&#26816;&#27979;&#26041;&#38754;ZuCo&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;7.4&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22269;&#38469;&#19978;&#26368;&#22823;&#30340;&#20154;&#31867;&#31867;&#27604;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study at the representation level, we introduced \textbf{MTAM}, a \textbf{M}ultimodal \textbf{T}ransformer \textbf{A}lignment \textbf{M}odel, to observe coordinated representations between the two modalities. We used various relationship alignment-seeking techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to transfigure features. On downstream applications, sentiment analysis and relation detection, we achieved new state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition, we provide inter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#39640;&#32423;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2201.02797</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Unified Review of Deep Learning for Automated Medical Coding. (arXiv:2201.02797v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.02797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#39640;&#32423;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#26159;&#21307;&#30103;&#36816;&#33829;&#21644;&#26381;&#21153;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#36890;&#36807;&#20174;&#20020;&#24202;&#25991;&#26723;&#20013;&#39044;&#27979;&#21307;&#30103;&#32534;&#30721;&#26469;&#31649;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#27493;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#12290;&#20294;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#32570;&#20047;&#23545;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#30340;&#32479;&#19968;&#35270;&#22270;&#12290;&#26412;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#25552;&#20379;&#23545;&#21307;&#30103;&#32534;&#30721;&#27169;&#22411;&#32452;&#20214;&#30340;&#19968;&#33324;&#29702;&#35299;&#65292;&#24182;&#24635;&#32467;&#20102;&#22312;&#27492;&#26694;&#26550;&#19979;&#26368;&#36817;&#30340;&#39640;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32479;&#19968;&#26694;&#26550;&#23558;&#21307;&#30103;&#32534;&#30721;&#20998;&#35299;&#20026;&#22235;&#20010;&#20027;&#35201;&#32452;&#20214;&#65292;&#21363;&#29992;&#20110;&#25991;&#26412;&#29305;&#24449;&#25552;&#21462;&#30340;&#32534;&#30721;&#22120;&#27169;&#22359;&#12289;&#26500;&#24314;&#28145;&#24230;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#26426;&#21046;&#12289;&#29992;&#20110;&#23558;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#25104;&#21307;&#30103;&#20195;&#30721;&#30340;&#35299;&#30721;&#22120;&#27169;&#22359;&#20197;&#21450;&#36741;&#21161;&#20449;&#24687;&#30340;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20934;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#30340;&#30740;&#31350;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated medical coding, an essential task for healthcare operation and delivery, makes unstructured data manageable by predicting medical codes from clinical documents. Recent advances in deep learning and natural language processing have been widely applied to this task. However, deep learning-based medical coding lacks a unified view of the design of neural network architectures. This review proposes a unified framework to provide a general understanding of the building blocks of medical coding models and summarizes recent advanced models under the proposed framework. Our unified framework decomposes medical coding into four main components, i.e., encoder modules for text feature extraction, mechanisms for building deep encoder architectures, decoder modules for transforming hidden representations into medical codes, and the usage of auxiliary information. Finally, we introduce the benchmarks and real-world usage and discuss key research challenges and future directions.
&lt;/p&gt;</description></item></channel></rss>