<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#30340;&#26041;&#24335;&#23545;&#24515;&#20869;&#30005;&#22270;&#36827;&#34892;&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#12290;&#30456;&#27604;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25151;&#39076;&#20998;&#31867;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01115</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#35299;&#35835;&#24515;&#20869;&#30005;&#22270;
&lt;/p&gt;
&lt;p&gt;
Interpretation of Intracardiac Electrograms Through Textual Representations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#30340;&#26041;&#24335;&#23545;&#24515;&#20869;&#30005;&#22270;&#36827;&#34892;&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#12290;&#30456;&#27604;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25151;&#39076;&#20998;&#31867;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#25151;&#39076;(AFib)&#30340;&#19981;&#35268;&#21017;&#30005;&#27963;&#21160;&#19968;&#30452;&#26159;&#24515;&#30005;&#22270;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#23545;&#20110;&#20005;&#37325;&#30340;&#25151;&#39076;&#30149;&#20363;&#65292;&#36827;&#34892;&#23548;&#31649;&#28040;&#34701;&#20197;&#33719;&#21462;&#24515;&#20869;&#30005;&#22270;(EGMs)&#12290;EGMs&#25552;&#20379;&#20102;&#24515;&#33039;&#30005;&#27963;&#21160;&#30340;&#22797;&#26434;&#32454;&#33410;&#21644;&#23616;&#37096;&#21270;&#20449;&#24687;&#65292;&#26159;&#21487;&#35299;&#37322;&#30340;&#24515;&#33039;&#30740;&#31350;&#30340;&#29702;&#24819;&#27169;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#36827;&#23637;&#20351;&#24471;&#19968;&#20123;&#30740;&#31350;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#37322;&#25151;&#39076;&#20013;&#30340;EGMs&#12290;&#27492;&#22806;&#65292;&#35821;&#35328;&#27169;&#22411;(LMs)&#22312;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LMs&#26469;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#23545;EGM&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23558;EGM&#24418;&#24335;&#21270;&#20026;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#19982;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#25151;&#39076;&#20998;&#31867;&#26041;&#38754;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the irregular electrical activity of atrial fibrillation (AFib) has been a key challenge in electrocardiography. For serious cases of AFib, catheter ablations are performed to collect intracardiac electrograms (EGMs). EGMs offer intricately detailed and localized electrical activity of the heart and are an ideal modality for interpretable cardiac studies. Recent advancements in artificial intelligence (AI) has allowed some works to utilize deep learning frameworks to interpret EGMs during AFib. Additionally, language models (LMs) have shown exceptional performance in being able to generalize to unseen domains, especially in healthcare. In this study, we are the first to leverage pretrained LMs for finetuning of EGM interpolation and AFib classification via masked language modeling. We formulate the EGM as a textual sequence and present competitive performances on AFib classification compared against other representations. Lastly, we provide a comprehensive interpretabilit
&lt;/p&gt;</description></item><item><title>KoCoNovel&#26159;&#19968;&#20010;&#20174;&#38889;&#22269;&#25991;&#23398;&#25991;&#26412;&#20013;&#34893;&#29983;&#20986;&#30340;&#23567;&#35828;&#20154;&#29289;&#20849;&#25351;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#27880;&#37322;&#25351;&#21335;&#65292;&#26159;&#38889;&#35821;&#20844;&#20849;&#20849;&#25351;&#35299;&#20915;&#35821;&#26009;&#24211;&#20013;&#31532;&#20108;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25991;&#23398;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;</title><link>https://arxiv.org/abs/2404.01140</link><description>&lt;p&gt;
KoCoNovel&#65306;&#38889;&#22269;&#23567;&#35828;&#20013;&#20154;&#29289;&#20849;&#25351;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01140
&lt;/p&gt;
&lt;p&gt;
KoCoNovel&#26159;&#19968;&#20010;&#20174;&#38889;&#22269;&#25991;&#23398;&#25991;&#26412;&#20013;&#34893;&#29983;&#20986;&#30340;&#23567;&#35828;&#20154;&#29289;&#20849;&#25351;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#27880;&#37322;&#25351;&#21335;&#65292;&#26159;&#38889;&#35821;&#20844;&#20849;&#20849;&#25351;&#35299;&#20915;&#35821;&#26009;&#24211;&#20013;&#31532;&#20108;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25991;&#23398;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;KoCoNovel&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;&#38889;&#22269;&#25991;&#23398;&#25991;&#26412;&#20013;&#34893;&#29983;&#20986;&#30340;&#23567;&#35828;&#20154;&#29289;&#20849;&#25351;&#25968;&#25454;&#38598;&#65292;&#37197;&#22791;&#20102;&#35814;&#32454;&#30340;&#27880;&#37322;&#25351;&#21335;&#12290;KoCoNovel&#30001;50&#37096;&#29616;&#20195;&#21644;&#24403;&#20195;&#38889;&#22269;&#23567;&#35828;&#20013;&#30340;178,000&#20010;&#26631;&#35760;&#32452;&#25104;&#65292;&#26159;&#32487;NIKL&#35821;&#26009;&#24211;&#20043;&#21518;&#38889;&#35821;&#20844;&#20849;&#20849;&#25351;&#35299;&#20915;&#35821;&#26009;&#24211;&#20013;&#31532;&#20108;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25991;&#23398;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25193;&#22823;&#20854;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22235;&#20010;&#19981;&#21516;&#29256;&#26412;&#30340;KoCoNovel&#65292;&#20026;&#20840;&#30693;&#20316;&#32773;&#21644;&#35835;&#32773;&#30340;&#35266;&#28857;&#20197;&#21450;&#22788;&#29702;&#22810;&#23454;&#20307;&#30340;&#20998;&#31163;&#25110;&#37325;&#21472;&#25552;&#20379;&#36873;&#25321;&#12290;&#36825;&#31181;&#26041;&#27861;&#25972;&#21512;&#20102;&#22260;&#32469;&#25991;&#23398;&#25991;&#26412;&#20849;&#25351;&#35299;&#26512;&#30340;&#29616;&#26377;&#35805;&#35821;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#12290;KoCoNovel&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;24%&#30340;&#20154;&#29289;&#25552;&#21450;&#26159;&#21333;&#19968;&#26222;&#36890;&#21517;&#35789;&#65292;&#32570;&#23569;&#25152;&#26377;&#26684;&#26631;&#35760;&#25110;&#20896;&#35789;&#12290;&#36825;&#19968;&#29305;&#24449;&#29305;&#21035;&#21463;&#21040;&#38889;&#35821;&#31216;&#35859;&#30340;&#32454;&#24494;&#24046;&#21035;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01140v1 Announce Type: new  Abstract: We present KoCoNovel, an novel character coreference dataset derived from Korean literary texts, complete with detailed annotation guidelines. Comprising 178K tokens from 50 modern and contemporary Korean novels, KoCoNovel stands as the second-largest public coreference resolution corpus in Korean, after the NIKL corpus, and the first to be based on literary texts. To broaden its utility, we provide four distinct versions of KoCoNovel, offering options for the perspectives of the omniscient author and readers, and for handling multiple entities as either separate or overlapping. This approach integrates existing discourse surrounding coreference resolution in literary texts, providing a comprehensive dataset for exploration. One of KoCoNovel's distinctive features is that 24% of all character mentions are single common nouns, lacking possessive markers or articles. This feature is particularly influenced by the nuances of Korean address 
&lt;/p&gt;</description></item><item><title>&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;&#65292;&#36827;&#32780;&#22686;&#24378;&#20102;&#20854;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01019</link><description>&lt;p&gt;
&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Source-Aware Training Enables Knowledge Attribution in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01019
&lt;/p&gt;
&lt;p&gt;
&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;&#65292;&#36827;&#32780;&#22686;&#24378;&#20102;&#20854;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#21040;&#20102;&#22823;&#37327;&#30693;&#35782;&#65292;&#20294;&#24448;&#24448;&#23545;&#27492;&#31867;&#30693;&#35782;&#30340;&#26469;&#28304;&#27627;&#19981;&#22312;&#24847;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#22312;&#28304;&#24341;&#29992;&#38382;&#39064;&#65292;&#35201;&#27714;LLMs&#24341;&#29992;&#25903;&#25345;&#29983;&#25104;&#21709;&#24212;&#30340;&#39044;&#35757;&#32451;&#26469;&#28304;&#12290;&#20869;&#22312;&#28304;&#24341;&#29992;&#21487;&#20197;&#22686;&#24378;LLMs&#30340;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;&#20026;&#36171;&#20104;LLMs&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28304;&#24863;&#30693;&#35757;&#32451;&#8212;&#8212;&#19968;&#20010;&#21518;&#39044;&#35757;&#32451;&#37197;&#26041;&#65292;&#21253;&#25324;&#65288;i&#65289;&#35757;&#32451;LLMs&#23558;&#21807;&#19968;&#28304;&#25991;&#26723;&#26631;&#35782;&#31526;&#19982;&#27599;&#20010;&#25991;&#26723;&#20013;&#30340;&#30693;&#35782;&#20851;&#32852;&#36215;&#26469;&#65292;&#28982;&#21518;&#65288;ii&#65289;&#36827;&#34892;&#25351;&#31034;&#35843;&#25972;&#65292;&#25945;&#23548;LLMs&#22312;&#34987;&#25552;&#31034;&#26102;&#24341;&#29992;&#25903;&#25345;&#30340;&#39044;&#35757;&#32451;&#26469;&#28304;&#12290;&#28304;&#24863;&#30693;&#35757;&#32451;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#30340;&#39044;&#35757;&#32451;LLMs&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;/&#24494;&#35843;&#26694;&#26550;&#30340;&#24046;&#24322;&#26368;&#23567;&#12290;&#36890;&#36807;&#23545;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#37197;&#26041;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01019v1 Announce Type: cross  Abstract: Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, we explore source-aware training -- a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, we demonstrate that our training recipe can en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#21644;&#22810;&#27169;&#24577;&#24773;&#32490;&#21407;&#22240;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#38376;&#30340;&#24773;&#32490;&#32534;&#30721;&#22120;&#21644;&#27169;&#24577;&#29305;&#23450;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#25552;&#21319;&#24773;&#32490;&#29702;&#35299;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#31454;&#20105;&#24615;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00511</link><description>&lt;p&gt;
MIPS&#22312;SemEval-2024&#20219;&#21153;3&#20013;&#30340;&#34920;&#29616;&#65306;&#20351;&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#20013;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#32490;-&#21407;&#22240;&#23545;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#21644;&#22810;&#27169;&#24577;&#24773;&#32490;&#21407;&#22240;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#38376;&#30340;&#24773;&#32490;&#32534;&#30721;&#22120;&#21644;&#27169;&#24577;&#29305;&#23450;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#25552;&#21319;&#24773;&#32490;&#29702;&#35299;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#31454;&#20105;&#24615;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;SemEval 2024&#20219;&#21153;3&#30340;&#23376;&#20219;&#21153;2&#20013;&#20851;&#20110;&#23545;&#35805;&#20013;&#22810;&#27169;&#24577;&#24773;&#32490;&#21407;&#22240;&#20998;&#26512;&#30340;&#33719;&#22870;&#25552;&#20132;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#21644;&#22810;&#27169;&#24577;&#24773;&#32490;&#21407;&#22240;&#25552;&#21462;&#65288;MER-MCE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#19987;&#38376;&#30340;&#24773;&#32490;&#32534;&#30721;&#22120;&#25972;&#21512;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#19977;&#31181;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#27169;&#24577;&#29305;&#23450;&#29305;&#24449;&#25552;&#21319;&#24773;&#32490;&#29702;&#35299;&#21644;&#22240;&#26524;&#25512;&#29702;&#65292;&#20351;&#33258;&#24049;&#33073;&#39062;&#32780;&#20986;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#20102;&#25105;&#20204;&#22810;&#27169;&#24577;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#25552;&#20132;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#21152;&#26435;F1&#20998;&#25968;&#20026;0.3435&#65292;&#22312;0.0339&#20043;&#21518;&#25490;&#21517;&#31532;&#19968;&#30340;&#22242;&#38431;&#65292;&#20165;&#22312;0.0025&#20043;&#21518;&#25490;&#21517;&#31532;&#20108;&#12290;&#39033;&#30446;&#38142;&#25509;&#65306;https://github.com/MIPS-COLT/MER-MCE.git
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00511v1 Announce Type: new  Abstract: This paper presents our winning submission to Subtask 2 of SemEval 2024 Task 3 on multimodal emotion cause analysis in conversations. We propose a novel Multimodal Emotion Recognition and Multimodal Emotion Cause Extraction (MER-MCE) framework that integrates text, audio, and visual modalities using specialized emotion encoders. Our approach sets itself apart from top-performing teams by leveraging modality-specific features for enhanced emotion understanding and causality inference. Experimental evaluation demonstrates the advantages of our multimodal approach, with our submission achieving a competitive weighted F1 score of 0.3435, ranking third with a margin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team. Project: https://github.com/MIPS-COLT/MER-MCE.git
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.19837</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#27010;&#24565;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Concept-based Analysis of Neural Networks via Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24418;&#24335;&#21270;&#20998;&#26512;&#38750;&#24120;&#21487;&#21462;&#65292;&#20294;&#30001;&#20110;&#38590;&#20197;&#34920;&#36798;&#35270;&#35273;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#35268;&#33539;&#20197;&#21450;&#32570;&#20047;&#39640;&#25928;&#30340;&#39564;&#35777;&#31243;&#24207;&#65292;&#36825;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#12289;&#35270;&#35273;&#35821;&#35328;&#12289;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#36890;&#36807;&#20854;&#21487;&#20197;&#25512;&#29702;&#35270;&#35273;&#27169;&#22411;&#30340;&#36879;&#38236;&#12290;VLMs&#24050;&#32463;&#22312;&#22823;&#37327;&#22270;&#20687;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#22240;&#27492;&#38544;&#24335;&#22320;&#20102;&#35299;&#25551;&#36848;&#36825;&#20123;&#22270;&#20687;&#30340;&#39640;&#23618;&#27425;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{Con}_{\texttt{spec}}$&#30340;&#36923;&#36753;&#35268;&#33539;&#35821;&#35328;&#65292;&#26088;&#22312;&#20415;&#20110;&#25353;&#29031;&#36825;&#20123;&#27010;&#24565;&#32534;&#20889;&#35268;&#33539;&#12290;&#20026;&#20102;&#23450;&#20041;&#21644;&#24418;&#24335;&#21270;&#26816;&#26597;$\texttt{Con}_{\texttt{spec}}$&#35268;&#33539;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;VLM&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#32534;&#30721;&#21644;&#39640;&#25928;&#26816;&#26597;&#35270;&#35273;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#23646;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;te
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19837v1 Announce Type: cross  Abstract: Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our te
&lt;/p&gt;</description></item><item><title>DANCER&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Description Augmented Named entity CorrEctoR&#65288;DANCER&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#25551;&#36848;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;NEC&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#65292;&#24110;&#21161;&#32531;&#35299;NE&#21015;&#34920;&#20013;&#30340;&#38899;&#32032;&#28151;&#28102;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17645</link><description>&lt;p&gt;
DANCER&#65306;&#38024;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#23454;&#20307;&#25551;&#36848;&#22686;&#24378;&#21629;&#21517;&#23454;&#20307;&#26657;&#27491;&#22120;
&lt;/p&gt;
&lt;p&gt;
DANCER: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17645
&lt;/p&gt;
&lt;p&gt;
DANCER&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Description Augmented Named entity CorrEctoR&#65288;DANCER&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#25551;&#36848;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;NEC&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#65292;&#24110;&#21161;&#32531;&#35299;NE&#21015;&#34920;&#20013;&#30340;&#38899;&#32032;&#28151;&#28102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;ASR&#30340;&#24555;&#36895;&#36731;&#37327;&#32423;&#21629;&#21517;&#23454;&#20307;&#26657;&#27491;&#65288;NEC&#65289;&#27169;&#22411;&#65292;&#36890;&#24120;&#26500;&#24314;&#22312;&#38899;&#32032;&#32423;&#32534;&#36753;&#36317;&#31163;&#31639;&#27861;&#22522;&#30784;&#19978;&#65292;&#24182;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;NEC&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#21629;&#21517;&#23454;&#20307;&#65288;NE&#65289;&#21015;&#34920;&#30340;&#22686;&#21152;&#65292;NE&#21015;&#34920;&#20013;&#30340;&#38899;&#32032;&#28151;&#28102;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65307;&#20363;&#22914;&#65292;&#21516;&#38899;&#24322;&#20041;&#35789;&#30340;&#38382;&#39064;&#22823;&#22823;&#22686;&#21152;&#12290;&#37492;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25551;&#36848;&#22686;&#24378;&#22411;&#21629;&#21517;&#23454;&#20307;&#26657;&#27491;&#22120;&#65288;&#31216;&#20026;DANCER&#65289;&#65292;&#21033;&#29992;&#23454;&#20307;&#25551;&#36848;&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#65292;&#20197;&#20415;&#22312;ASR&#36716;&#24405;&#20013;&#20026;NEC&#25552;&#20379;&#36741;&#21161;&#20943;&#36731;&#38899;&#32032;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17645v1 Announce Type: new  Abstract: End-to-end automatic speech recognition (E2E ASR) systems often suffer from mistranscription of domain-specific phrases, such as named entities, sometimes leading to catastrophic failures in downstream tasks. A family of fast and lightweight named entity correction (NEC) models for ASR have recently been proposed, which normally build on phonetic-level edit distance algorithms and have shown impressive NEC performance. However, as the named entity (NE) list grows, the problems of phonetic confusion in the NE list are exacerbated; for example, homophone ambiguities increase substantially. In view of this, we proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER), which leverages entity descriptions to provide additional information to facilitate mitigation of phonetic confusion for NEC on ASR transcription. To this end, an efficient entity description augmented masked language model (EDA-MLM) comprised of a dense re
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24314;&#31435;&#19968;&#20221;&#21253;&#25324; 100 &#31687;&#25991;&#26723;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598; OBSINFOX&#65292;&#30740;&#31350;&#20102;&#20154;&#31867;&#19982;&#26426;&#22120;&#22312;&#35748;&#23450;&#20551;&#26032;&#38395;&#29305;&#24449;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#35821;&#26009;&#24211;&#20013;&#35773;&#21050;&#25991;&#26412;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2403.16099</link><description>&lt;p&gt;
&#19968;&#20221;&#27861;&#22269;&#20551;&#26032;&#38395;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65306;&#20154;&#31867;&#19982;&#26426;&#22120;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Multi-Label Dataset of French Fake News: Human and Machine Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16099
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;&#19968;&#20221;&#21253;&#25324; 100 &#31687;&#25991;&#26723;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598; OBSINFOX&#65292;&#30740;&#31350;&#20102;&#20154;&#31867;&#19982;&#26426;&#22120;&#22312;&#35748;&#23450;&#20551;&#26032;&#38395;&#29305;&#24449;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#35821;&#26009;&#24211;&#20013;&#35773;&#21050;&#25991;&#26412;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001; 8 &#20301;&#27880;&#37322;&#32773;&#20351;&#29992; 11 &#20010;&#26631;&#31614;&#27880;&#37322;&#30340;&#26469;&#33258; 17 &#20010;&#27861;&#22269;&#34987;&#19987;&#23478;&#26426;&#26500;&#35748;&#20026;&#19981;&#21487;&#38752;&#30340;&#26032;&#38395;&#26469;&#28304;&#36873;&#21462;&#30340; 100 &#31687;&#25991;&#26723;&#30340;&#35821;&#26009;&#24211; OBSINFOX&#12290;&#36890;&#36807;&#25910;&#38598;&#27604;&#36890;&#24120;&#26356;&#22810;&#30340;&#26631;&#31614;&#21644;&#27880;&#37322;&#32773;&#65292;&#25105;&#20204;&#21487;&#20197;&#35782;&#21035;&#20154;&#31867;&#35748;&#20026;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#20551;&#26032;&#38395;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#19982;&#33258;&#21160;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992; Gate Cloud &#36827;&#34892;&#20027;&#39064;&#21644;&#20307;&#35009;&#20998;&#26512;&#65292;&#36825;&#34920;&#26126;&#35821;&#26009;&#24211;&#20013;&#31867;&#20284;&#35773;&#21050;&#30340;&#25991;&#26412;&#26222;&#36941;&#23384;&#22312;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992; VAGO &#20027;&#35266;&#24615;&#20998;&#26512;&#22120;&#21450;&#20854;&#31070;&#32463;&#29256;&#26412;&#65292;&#20197;&#28548;&#28165;&#26631;&#31614;&#8220;&#20027;&#35266;&#8221;&#19982;&#26631;&#31614;&#8220;&#20551;&#26032;&#38395;&#8221;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#24102;&#26377;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21487;&#36890;&#36807;&#20197;&#19979;&#32593;&#22336;&#22312;&#32447;&#33719;&#21462;&#65306;https://github.com/obs-info/obsinfox
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16099v1 Announce Type: new  Abstract: We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of French press considered unreliable by expert agencies, annotated using 11 labels by 8 annotators. By collecting more labels than usual, by more annotators than is typically done, we can identify features that humans consider as characteristic of fake news, and compare them to the predictions of automated classifiers. We present a topic and genre analysis using Gate Cloud, indicative of the prevalence of satire-like text in the corpus. We then use the subjectivity analyzer VAGO, and a neural version of it, to clarify the link between ascriptions of the label Subjective and ascriptions of the label Fake News. The annotated dataset is available online at the following url: https://github.com/obs-info/obsinfox   Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion, Exaggeration, French Press
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.11322</link><description>&lt;p&gt;
&#20351;&#29992;StateFlow&#22686;&#24378;LLM&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#36890;&#36807;&#29366;&#24577;&#39537;&#21160;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#36235;&#21183;&#26085;&#30410;&#26126;&#26174;&#65292;&#20363;&#22914;&#38656;&#35201;&#19968;&#31995;&#21015;&#25805;&#20316;&#21644;&#19982;&#24037;&#20855;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;StateFlow&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#27714;&#35299;&#33539;&#24335;&#65292;&#23558;&#30001;LLM&#25903;&#25345;&#30340;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#12290;&#36890;&#36807;&#27491;&#30830;&#26500;&#24314;&#29366;&#24577;&#21644;&#23450;&#20041;&#29366;&#24577;&#36716;&#25442;&#65292;StateFlow&#30830;&#23450;&#20102;&#20219;&#21153;&#27714;&#35299;&#30340;&#36827;&#23637;&#65292;&#30830;&#20445;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;LLM&#22312;&#25972;&#20010;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#30340;&#21709;&#24212;&#12290;&#22312;&#27599;&#20010;&#29366;&#24577;&#20013;&#65292;StateFlow&#20801;&#35768;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65292;&#19981;&#20165;&#21253;&#25324;&#26681;&#25454;&#29305;&#23450;&#25552;&#31034;&#25351;&#23548;&#29983;&#25104;LLM&#21709;&#24212;&#65292;&#36824;&#21253;&#25324;&#26681;&#25454;&#38656;&#35201;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#12290;&#29366;&#24577;&#36716;&#25442;&#30001;LLM&#20570;&#20986;&#30340;&#29305;&#23450;&#35268;&#21017;&#25110;&#20915;&#31574;&#25511;&#21046;&#65292;&#20801;&#35768;&#36890;&#36807;&#20219;&#21153;&#30340;&#39044;&#23450;&#20041;StateFlow&#27169;&#22411;&#21160;&#24577;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
&lt;/p&gt;</description></item><item><title>Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.08295</link><description>&lt;p&gt;
Gemma&#65306;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#30340;&#24320;&#25918;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemma: Open Models Based on Gemini Research and Technology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08295
&lt;/p&gt;
&lt;p&gt;
Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Gemma&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Gemini&#27169;&#22411;&#30740;&#31350;&#21644;&#25216;&#26415;&#26500;&#24314;&#30340;&#36731;&#37327;&#32423;&#12289;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#12290;Gemma&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#23398;&#26415;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;20&#20159;&#21644;70&#20159;&#21442;&#25968;&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26816;&#26597;&#28857;&#12290;Gemma&#22312;18&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#20219;&#21153;&#20013;&#65292;&#26377;11&#20010;&#20219;&#21153;&#20248;&#20110;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#23545;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#36131;&#20219;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21516;&#26102;&#35814;&#32454;&#25551;&#36848;&#20102;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;&#25105;&#20204;&#30456;&#20449;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23454;&#29616;&#19979;&#19968;&#27874;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#26032;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08295v1 Announce Type: cross  Abstract: This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.04814</link><description>&lt;p&gt;
&#22312;&#21477;&#27861;&#24863;&#30693;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Syntax-Aware Fill-In-the-Middle&#65288;SAFIM&#65289;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#22635;&#31354;&#65288;FIM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#22522;&#20934;&#20391;&#37325;&#20110;&#31243;&#24207;&#32467;&#26500;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#65292;&#22914;&#20195;&#30721;&#22359;&#21644;&#26465;&#20214;&#34920;&#36798;&#24335;&#65292;&#24182;&#21253;&#25324;&#26469;&#33258;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;17,720&#20010;&#31034;&#20363;&#65292;&#26469;&#28304;&#20110;2022&#24180;4&#26376;&#20043;&#21518;&#30340;&#26368;&#26032;&#20195;&#30721;&#25552;&#20132;&#65292;&#20197;&#26368;&#23567;&#21270;&#25968;&#25454;&#27745;&#26579;&#12290; SAFIM&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#21644;&#26032;&#39062;&#30340;&#21477;&#27861;&#24863;&#30693;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#22312;LLMs&#20043;&#38388;&#36827;&#34892;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;15&#20010;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#21319;&#20102;FIM&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#36824;&#25913;&#36827;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#65288;L2R&#65289;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#65292;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#22823;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;SAFIM&#20026;&#26410;&#26469;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04814v1 Announce Type: cross  Abstract: We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future 
&lt;/p&gt;</description></item><item><title>Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;</title><link>https://arxiv.org/abs/2402.12749</link><description>&lt;p&gt;
Me LLaMA: &#20026;&#21307;&#30103;&#24212;&#29992;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Me LLaMA: Foundation Large Language Models for Medical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12749
&lt;/p&gt;
&lt;p&gt;
Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#19981;&#22815;&#29702;&#24819;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22312;&#22823;&#22411;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Me LLaMA&#65292;&#19968;&#20010;&#21307;&#23398;LLM&#31995;&#21015;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;- Me LLaMA 13/70B&#21450;&#20854; chat-enhanced &#29256;&#26412;- Me LLaMA 13/70B-chat&#65292;&#36890;&#36807;&#25345;&#32493;&#23545;LLaMA2&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25968;&#25454;&#24320;&#21457;&#32780;&#25104;&#12290;&#25105;&#20204;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#22871;&#20214;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;129B tokens&#30340;&#22823;&#35268;&#27169;&#25345;&#32493;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#21253;&#21547;214k&#20010;&#26679;&#26412;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#36328;&#36234;14&#20010;&#25968;&#25454;&#38598;&#30340;&#20845;&#39033;&#20219;&#21153;&#30340;&#21307;&#23398;&#35780;&#20272;&#22522;&#20934;(MIBE)&#12290;&#25105;&#20204;&#20351;&#29992;MIBE&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#65292;Me LLaMA&#27169;&#22411;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24320;&#28304;&#21307;&#23398;LLMs&#65292;&#24182;&#19988;&#22312;&#21830;&#19994;&#24040;&#22836;&#22914;ChatGPT&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12749v1 Announce Type: cross  Abstract: Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LLaDA&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#20351;&#24471;&#39550;&#39542;&#21592;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33021;&#22815;&#22312;&#21508;&#22320;&#39550;&#39542;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21644;&#36816;&#21160;&#35745;&#21010;&#35843;&#25972;&#21040;&#26032;&#20301;&#32622;&#30340;&#20132;&#36890;&#35268;&#21017;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;LLaDA&#25351;&#23548;&#22312;&#35299;&#20915;&#24847;&#22806;&#24773;&#20917;&#21644;&#36866;&#24212;AV&#36816;&#21160;&#35745;&#21010;&#31574;&#30053;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05932</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25919;&#31574;&#36866;&#24212;&#22312;&#21508;&#22788;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Driving Everywhere with Large Language Model Policy Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LLaDA&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#20351;&#24471;&#39550;&#39542;&#21592;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33021;&#22815;&#22312;&#21508;&#22320;&#39550;&#39542;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21644;&#36816;&#21160;&#35745;&#21010;&#35843;&#25972;&#21040;&#26032;&#20301;&#32622;&#30340;&#20132;&#36890;&#35268;&#21017;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;LLaDA&#25351;&#23548;&#22312;&#35299;&#20915;&#24847;&#22806;&#24773;&#20917;&#21644;&#36866;&#24212;AV&#36816;&#21160;&#35745;&#21010;&#31574;&#30053;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#23558;&#39550;&#39542;&#34892;&#20026;&#36866;&#24212;&#26032;&#29615;&#22659;&#12289;&#20064;&#20439;&#21644;&#27861;&#24459;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;(AVs)&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LLaDA&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20351;&#20154;&#31867;&#39550;&#39542;&#21592;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#37117;&#33021;&#22312;&#21508;&#22788;&#34892;&#39542;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21644;&#36816;&#21160;&#35745;&#21010;&#35843;&#25972;&#21040;&#26032;&#20301;&#32622;&#30340;&#20132;&#36890;&#35268;&#21017;&#12290;LLaDA&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#37322;&#26412;&#22320;&#39550;&#39542;&#25163;&#20876;&#20013;&#30340;&#20132;&#36890;&#35268;&#21017;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLaDA&#30340;&#25351;&#23548;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24847;&#22806;&#24773;&#20917;&#26102;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLaDA&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#35843;&#25972;AV&#36816;&#21160;&#35745;&#21010;&#31574;&#30053;&#30340;&#33021;&#21147;&#65307;LLaDA&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#20248;&#20110;&#22522;&#32447;&#35268;&#21010;&#26041;&#27861;&#12290;&#35831;&#35775;&#38382;&#25105;&#20204;&#30340;&#32593;&#31449;&#20102;&#35299;&#26356;&#22810;&#35814;&#32454;&#20449;&#24687;&#65306;https://boyiliee.github.io/llada.
&lt;/p&gt;
&lt;p&gt;
Adapting driving behavior to new environments, customs, and laws is a long-standing problem in autonomous driving, precluding the widespread deployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a simple yet powerful tool that enables human drivers and autonomous vehicles alike to drive everywhere by adapting their tasks and motion plans to traffic rules in new locations. LLaDA achieves this by leveraging the impressive zero-shot generalizability of large language models (LLMs) in interpreting the traffic rules in the local driver handbook. Through an extensive user study, we show that LLaDA's instructions are useful in disambiguating in-the-wild unexpected situations. We also demonstrate LLaDA's ability to adapt AV motion planning policies in real-world datasets; LLaDA outperforms baseline planning approaches on all our metrics. Please check our website for more details: https://boyiliee.github.io/llada.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;RLAIF&#35299;&#37322;&#20026;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#36807;&#31934;&#28860;&#30340;&#33258;&#25105;&#25209;&#35780;&#23545;LLM&#30340;&#36755;&#20986;&#36827;&#34892;&#31934;&#28860;&#65292;&#20026;&#33719;&#24471;&#24494;&#35843;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#19988;&#24265;&#20215;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.01957</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#32463;&#36807;&#31934;&#28860;&#30340;LLMs&#33258;&#25105;&#25209;&#35780;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;RLAIF&#35299;&#37322;&#20026;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#36807;&#31934;&#28860;&#30340;&#33258;&#25105;&#25209;&#35780;&#23545;LLM&#30340;&#36755;&#20986;&#36827;&#34892;&#31934;&#28860;&#65292;&#20026;&#33719;&#24471;&#24494;&#35843;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#19988;&#24265;&#20215;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;RLAIF&#35299;&#37322;&#20026;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32463;&#36807;&#31934;&#28860;&#30340;&#33258;&#25105;&#25209;&#35780;(dSC)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;Gibbs&#37319;&#26679;&#22120;&#23545;LLM&#30340;&#36755;&#20986;&#36827;&#34892;&#31934;&#28860;&#65292;&#28982;&#21518;&#23558;&#20854;&#33976;&#39311;&#25104;&#19968;&#20010;&#24494;&#35843;&#27169;&#22411;&#12290;&#21482;&#38656;&#35201;&#21512;&#25104;&#25968;&#25454;&#65292;dSC&#22312;&#28041;&#21450;&#23433;&#20840;&#24615;&#12289;&#24773;&#24863;&#21644;&#38544;&#31169;&#25511;&#21046;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#34920;&#26126;&#23427;&#21487;&#20197;&#20316;&#20026;&#23545;&#40784;LLMs&#30340;&#19968;&#31181;&#21487;&#34892;&#19988;&#24265;&#20215;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20195;&#30721;&#22312;\url{https://github.com/vicgalle/distilled-self-critique}&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01957v2 Announce Type: replace  Abstract: This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model. Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs. Code released at \url{https://github.com/vicgalle/distilled-self-critique}.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#21512;&#36923;&#36753;&#65292;&#29978;&#33267;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#65292;&#20294;&#21363;&#20351;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#19982;&#20154;&#31867;&#25512;&#29702;&#31867;&#20284;&#30340;&#38169;&#35823;&#65292;&#24635;&#20307;&#19978;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#20154;&#31867;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2311.00445</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19977;&#27573;&#35770;&#25512;&#29702;&#30340;&#31995;&#32479;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. (arXiv:2311.00445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00445
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#21512;&#36923;&#36753;&#65292;&#29978;&#33267;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#65292;&#20294;&#21363;&#20351;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#19982;&#20154;&#31867;&#25512;&#29702;&#31867;&#20284;&#30340;&#38169;&#35823;&#65292;&#24635;&#20307;&#19978;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#20154;&#31867;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24615;&#34892;&#20026;&#30340;&#19968;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26159;&#36923;&#36753;&#25512;&#29702;&#65306;&#30830;&#23450;&#21738;&#20123;&#32467;&#35770;&#21487;&#20197;&#20174;&#19968;&#32452;&#21069;&#25552;&#20013;&#24471;&#20986;&#12290;&#24515;&#29702;&#23398;&#23478;&#24050;&#32463;&#35760;&#24405;&#19979;&#20154;&#31867;&#25512;&#29702;&#19982;&#36923;&#36753;&#35268;&#21017;&#19981;&#31526;&#30340;&#20960;&#31181;&#26041;&#24335;&#12290;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22797;&#21046;&#36825;&#20123;&#20559;&#24046;&#65292;&#25110;&#32773;&#23427;&#20204;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#20559;&#24046;&#65311;&#25105;&#20204;&#20851;&#27880;&#19977;&#27573;&#35770;&#30340;&#24773;&#20917; - &#20174;&#20004;&#20010;&#31616;&#21333;&#21069;&#25552;&#20013;&#25512;&#23548;&#20986;&#30340;&#25512;&#29702;&#65292;&#36825;&#22312;&#24515;&#29702;&#23398;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350; - &#25105;&#20204;&#21457;&#29616;&#36739;&#22823;&#30340;&#27169;&#22411;&#27604;&#36739;&#21512;&#36923;&#36753;&#65292;&#32780;&#19988;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21363;&#20351;&#26159;&#26368;&#22823;&#30340;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#31995;&#32479;&#24615;&#38169;&#35823;&#65292;&#20854;&#20013;&#19968;&#20123;&#38169;&#35823;&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;&#20559;&#35265;&#30456;&#20284;&#65292;&#20363;&#22914;&#25490;&#24207;&#25928;&#24212;&#21644;&#36923;&#36753;&#35884;&#35823;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#27169;&#20223;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#20154;&#31867;&#20559;&#35265;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans' inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate these biases, or are they able to overcome them? Focusing on the case of syllogisms -- inferences from two simple premises, which have been studied extensively in psychology -- we show that larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases such as ordering effects and logical fallacies. Overall, we find that language models mimic the human biases included in their training data, but are able to overcome them in some cases.
&lt;/p&gt;</description></item><item><title>&#28145;&#23618;&#30340;transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#24335;&#27867;&#21270;&#33021;&#21147;&#19978;&#27604;&#27973;&#23618;&#27169;&#22411;&#26356;&#22909;&#65292;&#20294;&#39069;&#22806;&#23618;&#25968;&#30340;&#30456;&#23545;&#25910;&#30410;&#20250;&#36805;&#36895;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.19956</link><description>&lt;p&gt;
&#28145;&#24230;&#21644;&#23485;&#24230;&#23545;Transformer&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Depth and Width on Transformer Language Model Generalization. (arXiv:2310.19956v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19956
&lt;/p&gt;
&lt;p&gt;
&#28145;&#23618;&#30340;transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#24335;&#27867;&#21270;&#33021;&#21147;&#19978;&#27604;&#27973;&#23618;&#27169;&#22411;&#26356;&#22909;&#65292;&#20294;&#39069;&#22806;&#23618;&#25968;&#30340;&#30456;&#23545;&#25910;&#30410;&#20250;&#36805;&#36895;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;&#26032;&#30340;&#21477;&#23376;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24517;&#39035;&#20197;&#32452;&#21512;&#30340;&#26041;&#24335;&#36827;&#34892;&#27867;&#21270; - &#23558;&#29087;&#24713;&#30340;&#20803;&#32032;&#20197;&#26032;&#30340;&#26041;&#24335;&#32467;&#21512;&#36215;&#26469;&#12290;&#27169;&#22411;&#32467;&#26500;&#30340;&#21738;&#20123;&#26041;&#38754;&#20419;&#36827;&#20102;&#32452;&#21512;&#24335;&#27867;&#21270;&#65311;&#38024;&#23545;transformers&#65292;&#25105;&#20204;&#27979;&#35797;&#20551;&#35774;&#65292;&#21363;&#24403;transformers&#26356;&#28145;&#65288;&#20855;&#26377;&#26356;&#22810;&#23618;&#27425;&#65289;&#26102;&#65292;&#23427;&#20204;&#26356;&#23481;&#26131;&#36827;&#34892;&#32452;&#21512;&#24335;&#27867;&#21270;&#65292;&#36825;&#20010;&#20551;&#35774;&#22522;&#20110;&#26368;&#36817;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#12290;&#30001;&#20110;&#31616;&#21333;&#22320;&#22686;&#21152;&#23618;&#25968;&#20250;&#22686;&#21152;&#24635;&#21442;&#25968;&#25968;&#37327;&#65292;&#28151;&#28102;&#20102;&#28145;&#24230;&#21644;&#22823;&#23567;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19977;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#20197;&#20445;&#25345;&#24635;&#21442;&#25968;&#25968;&#37327;&#24658;&#23450;&#30340;&#26041;&#24335;&#26469;&#26435;&#34913;&#28145;&#24230;&#21644;&#23485;&#24230;&#65288;&#20998;&#21035;&#20026;41M&#12289;134M&#21644;374M&#20010;&#21442;&#25968;&#65289;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#27169;&#22411;&#39044;&#35757;&#32451;&#20026;LMS&#65292;&#28982;&#21518;&#22312;&#27979;&#35797;&#32452;&#21512;&#24335;&#27867;&#21270;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#32467;&#35770;&#65306;&#65288;1&#65289;&#22312;&#24494;&#35843;&#21518;&#65292;&#28145;&#23618;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#24773;&#20917;&#19979;&#27604;&#27973;&#23618;&#27169;&#22411;&#26356;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#65292;&#20294;&#39069;&#22806;&#23618;&#27425;&#30340;&#30456;&#23545;&#25910;&#30410;&#36805;&#36895;&#20943;&#23567;&#65307;&#65288;2&#65289;&#22312;&#27599;&#20010;&#27169;&#22411;&#32452;&#20013;&#65292;&#28145;&#23618;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#32452;&#21512;&#24335;&#27867;&#21270;&#33021;&#21147;...&#65288;&#25991;&#26412;&#24050;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
To process novel sentences, language models (LMs) must generalize compositionally -- combine familiar elements in new ways. What aspects of a model's structure promote compositional generalization? Focusing on transformers, we test the hypothesis, motivated by recent theoretical and empirical work, that transformers generalize more compositionally when they are deeper (have more layers). Because simply adding layers increases the total number of parameters, confounding depth and size, we construct three classes of models which trade off depth for width such that the total number of parameters is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs and fine-tune them on tasks that test for compositional generalization. We report three main conclusions: (1) after fine-tuning, deeper models generalize better out-of-distribution than shallower models do, but the relative benefit of additional layers diminishes rapidly; (2) within each family, deeper models show bett
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#26426;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22914;&#20309;&#25903;&#25345;&#33258;&#20027;&#24341;&#23548;&#24335;&#24515;&#29702;&#20581;&#24247;&#24178;&#39044;&#65292;&#36890;&#36807;&#20197;&#35748;&#30693;&#37325;&#24314;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#23545;&#21442;&#19982;&#32773;&#24773;&#32490;&#24378;&#24230;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#24182;&#24110;&#21161;&#20182;&#20204;&#20811;&#26381;&#20102;&#36127;&#38754;&#24605;&#32500;&#12290;</title><link>http://arxiv.org/abs/2310.15461</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#26426;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#20419;&#36827;&#33258;&#20027;&#24341;&#23548;&#24335;&#24515;&#29702;&#20581;&#24247;&#24178;&#39044;&#65306;&#35748;&#30693;&#37325;&#24314;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring. (arXiv:2310.15461v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#26426;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22914;&#20309;&#25903;&#25345;&#33258;&#20027;&#24341;&#23548;&#24335;&#24515;&#29702;&#20581;&#24247;&#24178;&#39044;&#65292;&#36890;&#36807;&#20197;&#35748;&#30693;&#37325;&#24314;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#23545;&#21442;&#19982;&#32773;&#24773;&#32490;&#24378;&#24230;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#24182;&#24110;&#21161;&#20182;&#20204;&#20811;&#26381;&#20102;&#36127;&#38754;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#24341;&#23548;&#24335;&#24515;&#29702;&#20581;&#24247;&#24178;&#39044;&#65292;&#22914;&#23398;&#20064;&#21644;&#23454;&#36341;&#24212;&#23545;&#31574;&#30053;&#30340;&#8220;&#33258;&#21161;&#24037;&#20855;&#8221;&#65292;&#22312;&#25913;&#21892;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#30340;&#21487;&#21450;&#24615;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24178;&#39044;&#24120;&#24120;&#38656;&#35201;&#35748;&#30693;&#36127;&#25285;&#21644;&#24773;&#32490;&#35302;&#21457;&#65292;&#20174;&#32780;&#36896;&#25104;&#38480;&#21046;&#20854;&#22823;&#35268;&#27169;&#23454;&#26045;&#21644;&#26222;&#21450;&#30340;&#21487;&#21450;&#24615;&#38556;&#30861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20154;&#26426;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22914;&#20309;&#25903;&#25345;&#33258;&#20027;&#24341;&#23548;&#24335;&#24515;&#29702;&#20581;&#24247;&#24178;&#39044;&#12290;&#25105;&#20204;&#20197;&#35748;&#30693;&#37325;&#24314;&#20316;&#20026;&#19968;&#20010;&#20197;&#35777;&#25454;&#20026;&#22522;&#30784;&#30340;&#27835;&#30103;&#25216;&#26415;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#22312;&#19968;&#39033;&#32463;&#36807;IRB&#25209;&#20934;&#30340;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#32593;&#31449;&#19978;&#36827;&#34892;&#30340;&#38543;&#26426;&#29616;&#22330;&#30740;&#31350;&#20013;&#65292;&#28041;&#21450;&#20102;15,531&#21517;&#21442;&#19982;&#32773;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25903;&#25345;&#20154;&#20204;&#22312;&#35748;&#30693;&#37325;&#24314;&#30340;&#21508;&#20010;&#27493;&#39588;&#20013;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#23545;67%&#30340;&#21442;&#19982;&#32773;&#30340;&#24773;&#32490;&#24378;&#24230;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#24182;&#24110;&#21161;65%&#30340;&#20154;&#20811;&#26381;&#28040;&#26497;&#24605;&#32500;&#12290;&#23613;&#31649;&#38738;&#23569;&#24180;&#25253;&#36947;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-guided mental health interventions, such as "do-it-yourself" tools to learn and practice coping strategies, show great promise to improve access to mental health care. However, these interventions are often cognitively demanding and emotionally triggering, creating accessibility barriers that limit their wide-scale implementation and adoption. In this paper, we study how human-language model interaction can support self-guided mental health interventions. We take cognitive restructuring, an evidence-based therapeutic technique to overcome negative thinking, as a case study. In an IRB-approved randomized field study on a large mental health website with 15,531 participants, we design and evaluate a system that uses language models to support people through various steps of cognitive restructuring. Our findings reveal that our system positively impacts emotional intensity for 67% of participants and helps 65% overcome negative thoughts. Although adolescents report relatively worse o
&lt;/p&gt;</description></item><item><title>CrisisTransformers&#26159;&#19968;&#32452;&#38024;&#23545;&#21361;&#26426;&#30456;&#20851;&#25991;&#26412;&#20248;&#21270;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#21477;&#23376;&#32534;&#30721;&#22120;&#65292;&#26088;&#22312;&#26377;&#25928;&#22788;&#29702;&#21361;&#26426;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#65292;&#24182;&#20026;&#32039;&#24613;&#21709;&#24212;&#32773;&#25552;&#20379;&#32508;&#21512;&#35270;&#22270;&#12290;</title><link>http://arxiv.org/abs/2309.05494</link><description>&lt;p&gt;
CrisisTransformers&#65306;&#29992;&#20110;&#21361;&#26426;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#21477;&#23376;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts. (arXiv:2309.05494v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05494
&lt;/p&gt;
&lt;p&gt;
CrisisTransformers&#26159;&#19968;&#32452;&#38024;&#23545;&#21361;&#26426;&#30456;&#20851;&#25991;&#26412;&#20248;&#21270;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#21477;&#23376;&#32534;&#30721;&#22120;&#65292;&#26088;&#22312;&#26377;&#25928;&#22788;&#29702;&#21361;&#26426;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#65292;&#24182;&#20026;&#32039;&#24613;&#21709;&#24212;&#32773;&#25552;&#20379;&#32508;&#21512;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#21361;&#26426;&#27807;&#36890;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#30001;&#20110;&#20854;&#38750;&#27491;&#24335;&#24615;&#36136;&#65292;&#20998;&#26512;&#21361;&#26426;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;BERT&#21644;RoBERTa&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#38024;&#23545;&#21361;&#26426;&#30456;&#20851;&#25991;&#26412;&#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#36890;&#29992;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#29992;&#20110;&#29983;&#25104;&#21477;&#23376;&#23884;&#20837;&#65292;&#32780;&#19981;&#32771;&#34385;&#21361;&#26426;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#25991;&#26412;&#22797;&#26434;&#24615;&#12290;&#25991;&#26412;&#20998;&#31867;&#12289;&#35821;&#20041;&#25628;&#32034;&#21644;&#32858;&#31867;&#31561;&#24212;&#29992;&#30340;&#36827;&#23637;&#26377;&#21161;&#20110;&#26377;&#25928;&#22788;&#29702;&#21361;&#26426;&#30456;&#20851;&#25991;&#26412;&#65292;&#36825;&#23545;&#20110;&#32039;&#24613;&#21709;&#24212;&#32773;&#33719;&#24471;&#21361;&#26426;&#20107;&#20214;&#30340;&#32508;&#21512;&#35270;&#22270;&#33267;&#20851;&#37325;&#35201;&#65292;&#26080;&#35770;&#35813;&#20107;&#20214;&#26159;&#21382;&#21490;&#20107;&#20214;&#36824;&#26159;&#23454;&#26102;&#20107;&#20214;&#12290;&#20026;&#22635;&#34917;&#21361;&#26426;&#20449;&#24687;&#23398;&#25991;&#29486;&#20013;&#30340;&#36825;&#20123;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;CrisisTransformers&#65292;&#36825;&#26159;&#19968;&#32452;&#22312;&#36229;&#36807;150&#20159;&#20010;&#35789;&#20803;&#30340;&#25512;&#25991;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#21477;&#23376;&#32534;&#30721;&#22120;&#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms play an essential role in crisis communication, but analyzing crisis-related social media texts is challenging due to their informal nature. Transformer-based pre-trained models like BERT and RoBERTa have shown success in various NLP tasks, but they are not tailored for crisis-related texts. Furthermore, general-purpose sentence encoders are used to generate sentence embeddings, regardless of the textual complexities in crisis-related texts. Advances in applications like text classification, semantic search, and clustering contribute to effective processing of crisis-related texts, which is essential for emergency responders to gain a comprehensive view of a crisis event, whether historical or real-time. To address these gaps in crisis informatics literature, this study introduces CrisisTransformers, an ensemble of pre-trained language models and sentence encoders trained on an extensive corpus of over 15 billion word tokens from tweets associated with more than 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20854;&#22312;&#31616;&#21333;&#35821;&#35328;&#25512;&#26029;&#20219;&#21153;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#20854;&#23545;&#22522;&#26412;&#35821;&#35328;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.14785</link><description>&lt;p&gt;
ChatGPT&#21644;&#31616;&#21333;&#30340;&#35821;&#35328;&#25512;&#26029;&#65306;&#30450;&#28857;&#21644;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds. (arXiv:2305.14785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20854;&#22312;&#31616;&#21333;&#35821;&#35328;&#25512;&#26029;&#20219;&#21153;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#20854;&#23545;&#22522;&#26412;&#35821;&#35328;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#29702;&#35299;&#33021;&#21147;&#38480;&#21046;&#65292;&#38024;&#23545;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#36890;&#24120;&#31616;&#21333;&#30340;&#25512;&#26029;&#20219;&#21153;&#65292;&#20294;&#36825;&#20123;&#20284;&#20046;&#23545;&#35813;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#38024;&#23545;(i)&#35821;&#27861;&#35268;&#23450;&#30340;&#34164;&#21547;&#65292;(ii)&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#35777;&#25454;&#21103;&#35789;&#30340;&#21069;&#25552;&#65292;&#20197;&#21450;(iii)&#21333;&#35843;&#34164;&#21547;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#20026;&#36825;&#20123;&#25512;&#29702;&#31867;&#22411;&#25552;&#20379;&#20102;&#19987;&#23478;&#35774;&#35745;&#30340;&#35780;&#20272;&#38598;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#36825;&#20123;&#25512;&#29702;&#31867;&#22411;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23637;&#31034;&#20013;&#31561;&#21040;&#20302;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;ChatGPT&#22312;&#30452;&#25509;&#25552;&#31034;&#19979;&#34920;&#29616;&#20986;&#23545;&#24213;&#23618;&#35821;&#35328;&#27010;&#24565;&#30340;&#20102;&#35299;&#65292;&#20294;&#23427;&#32463;&#24120;&#19981;&#33021;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#20316;&#20986;&#27491;&#30830;&#30340;&#25512;&#26029;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#21069;&#25552;&#23884;&#20837;&#21069;&#25552;&#26465;&#20214;&#35302;&#21457;&#25110;&#38750;&#23454;&#38469;&#24615;&#21160;&#35789;&#20250;&#23548;&#33268;&#27169;&#22411;&#26356;&#39057;&#32321;&#22320;&#39044;&#27979;&#34164;&#21547;&#65292;&#32780;&#19981;&#32771;&#34385;&#27491;&#30830;&#30340;&#35821;&#20041;&#26631;&#31614;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#32487;&#32493;&#25913;&#21892;&#23427;&#20204;&#23545;&#22522;&#26412;&#35821;&#35328;&#27010;&#24565;&#30340;&#29702;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper sheds light on the limitations of ChatGPT's understanding capabilities, focusing on simple inference tasks that are typically easy for humans but appear to be challenging for the model. Specifically, we target (i) grammatically-specified entailments, (ii) premises with evidential adverbs of uncertainty, and (iii) monotonicity entailments. We present expert-designed evaluation sets for these inference types and conduct experiments in a zero-shot setup. Our results show that the model struggles with these types of inferences, exhibiting moderate to low accuracy. Moreover, while ChatGPT demonstrates knowledge of the underlying linguistic concepts when prompted directly, it often fails to incorporate this knowledge to make correct inferences. Even more strikingly, further experiments show that embedding the premise under presupposition triggers or non-factive verbs causes the model to predict entailment more frequently {regardless} of the correct semantic label. Overall these re
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;</title><link>http://arxiv.org/abs/2304.13567</link><description>&lt;p&gt;
&#20301;&#32622;&#20559;&#24046;&#23545;token&#20998;&#31867;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13567
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#25110;&#35789;&#24615;&#26631;&#27880;&#31561;&#19979;&#28216;&#20219;&#21153;&#24050;&#30693;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#36127;&#31034;&#20363;&#30340;&#27604;&#20363;&#21644;&#31867;&#19981;&#24179;&#34913;&#26041;&#38754;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#65292;&#21363;token&#20998;&#31867;&#20219;&#21153;&#20013;&#27491;&#31034;&#20363;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;Token&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20301;&#32622;&#20559;&#24046;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;CoNLL03&#21644;OntoNote5.0&#29992;&#20110;NER&#65292;English Tree Bank UD_en&#21644;TweeBank&#29992;&#20110;POS&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30740;&#31350;Transformer&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#20687;BERT&#12289;ERNIE&#12289;ELECTRA&#36825;&#26679;&#30340;&#32534;&#30721;&#22120;&#21644;&#20687;GPT2 &#21644;BLOOM&#36825;&#26679;&#30340;&#35299;&#30721;&#22120;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have shown state-of-the-art performance in Natural Language Processing (NLP) tasks. Downstream tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data imbalance issues, specifically in terms of the ratio of positive to negative examples, and class imbalance. In this paper, we investigate an additional specific issue for language models, namely the position bias of positive examples in token classification tasks. Therefore, we conduct an in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We propose an evaluation approach to investigate position bias in Transformer models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in their performan
&lt;/p&gt;</description></item></channel></rss>