<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#26680;&#24515;&#21019;&#26032;&#22312;&#20110;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20351;&#24471;&#22270;&#20687;&#26816;&#32034;&#21487;&#20197;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.19651</link><description>&lt;p&gt;
MagicLens&#65306;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#19982;&#24320;&#25918;&#24335;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#26680;&#24515;&#21019;&#26032;&#22312;&#20110;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20351;&#24471;&#22270;&#20687;&#26816;&#32034;&#21487;&#20197;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#26816;&#32034;&#65292;&#21363;&#26681;&#25454;&#21442;&#32771;&#22270;&#20687;&#26597;&#25214;&#25152;&#38656;&#22270;&#20687;&#65292;&#22266;&#26377;&#22320;&#21253;&#21547;&#38590;&#20197;&#20165;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#24230;&#37327;&#25429;&#25417;&#21040;&#30340;&#20016;&#23500;&#12289;&#22810;&#26041;&#38754;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20801;&#35768;&#29992;&#25143;&#26356;&#33258;&#30001;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#37027;&#20123;&#35270;&#35273;&#19978;&#30456;&#20284;&#21644;/&#25110;&#21487;&#20197;&#29992;&#19968;&#23567;&#32452;&#39044;&#23450;&#20041;&#20851;&#31995;&#26469;&#34920;&#24449;&#30340;&#22270;&#20687;&#23545;&#19978;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#35770;&#28857;&#26159;&#25991;&#26412;&#25351;&#20196;&#21487;&#20197;&#20351;&#22270;&#20687;&#26816;&#32034;&#33021;&#22815;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#12290;MagicLens&#24314;&#31435;&#22312;&#19968;&#20010;&#37325;&#35201;&#30340;&#26032;&#39062;&#35265;&#35299;&#19978;&#65306;&#33258;&#28982;&#21457;&#29983;&#22312;&#21516;&#19968;&#32593;&#39029;&#19978;&#30340;&#22270;&#20687;&#23545;&#21253;&#21547;&#30528;&#22823;&#37327;&#38544;&#24335;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#20869;&#37096;&#35270;&#22270;&#65289;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#32508;&#21512;&#25351;&#20196;&#23558;&#36825;&#20123;&#38544;&#24335;&#20851;&#31995;&#21464;&#20026;&#26174;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19651v1 Announce Type: cross  Abstract: Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent work leverages text instructions to allow users to more freely express their search intents. However, existing work primarily focuses on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19647</link><description>&lt;p&gt;
&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65306;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#21457;&#29616;&#21644;&#24212;&#29992;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30005;&#36335;&#26159;&#20154;&#31867;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#22240;&#26524;&#30456;&#20851;&#23376;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#12290; &#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#30830;&#23450;&#30340;&#30005;&#36335;&#30001;&#22810;&#20041;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#21333;&#20803;&#32452;&#25104;&#65292;&#20363;&#22914;&#27880;&#24847;&#21147;&#22836;&#25110;&#31070;&#32463;&#20803;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23454;&#29616;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#12290; &#30001;&#20110;&#23427;&#20204;&#22522;&#20110;&#32454;&#31890;&#24230;&#21333;&#20803;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23545;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#25105;&#20204; introduc&#20102;SHIFT&#65292;&#36890;&#36807;&#20999;&#38500;&#20154;&#31867;&#21028;&#26029;&#20026;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21457;&#29616;&#25104;&#21315;&#19978;&#19975;&#20010;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#26469;&#23637;&#31034;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#19988;&#21487;&#25193;&#23637;&#30340;&#21487;&#35299;&#37322;&#24615;&#31649;&#32447;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19647v1 Announce Type: cross  Abstract: We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#35828;&#35805;&#20154;&#35782;&#21035;&#39046;&#22495;&#30340;&#36129;&#29486;&#65292;&#31361;&#20986;&#23637;&#31034;&#20102;&#23545;SdSv&#25361;&#25112;&#20219;&#21153;&#30340;&#20004;&#20010;&#39069;&#22806;&#25361;&#25112;&#65306;&#27880;&#20876;&#21644;&#27979;&#35797;&#25968;&#25454;&#19981;&#21305;&#37197;&#20197;&#21450;&#35780;&#20272;&#35797;&#39564;&#25968;&#25454;&#38598;&#23376;&#38598;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19634</link><description>&lt;p&gt;
&#38750;&#23545;&#31216;&#21644;&#35797;&#39564;&#20381;&#36182;&#24314;&#27169;&#65306;LIA&#23545;SdSV&#25361;&#25112;&#20219;&#21153;2&#30340;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
Asymmetric and trial-dependent modeling: the contribution of LIA to SdSV Challenge Task 2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19634
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#35828;&#35805;&#20154;&#35782;&#21035;&#39046;&#22495;&#30340;&#36129;&#29486;&#65292;&#31361;&#20986;&#23637;&#31034;&#20102;&#23545;SdSv&#25361;&#25112;&#20219;&#21153;&#30340;&#20004;&#20010;&#39069;&#22806;&#25361;&#25112;&#65306;&#27880;&#20876;&#21644;&#27979;&#35797;&#25968;&#25454;&#19981;&#21305;&#37197;&#20197;&#21450;&#35780;&#20272;&#35797;&#39564;&#25968;&#25454;&#38598;&#23376;&#38598;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SdSv&#25361;&#25112;&#20219;&#21153;2&#20026;&#35780;&#20272;&#29616;&#20195;&#25991;&#26412;&#26080;&#20851;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#20294;&#23427;&#20063;&#20351;&#24471;&#27979;&#35797;&#33021;&#22815;&#32771;&#34385;&#21040;&#35813;&#25361;&#25112;&#30340;&#20027;&#35201;&#38382;&#39064;&#65288;&#25345;&#32493;&#26102;&#38388;&#12289;&#35821;&#35328;&#31561;&#65289;&#30340;&#26032;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#23454;&#39564;&#23460;&#23545;&#35828;&#35805;&#20154;&#35782;&#21035;&#39046;&#22495;&#30340;&#36129;&#29486;&#12290;&#36825;&#20123;&#36129;&#29486;&#24378;&#35843;&#20102;&#38500;&#20102;&#30701;&#25345;&#32493;&#26102;&#38388;&#21644;&#35821;&#35328;&#20043;&#22806;&#30340;&#21478;&#22806;&#20004;&#20010;&#25361;&#25112;&#65306;&#27880;&#20876;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#20197;&#21450;&#35780;&#20272;&#35797;&#39564;&#25968;&#25454;&#38598;&#23376;&#38598;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;SdSv&#35780;&#20272;&#20013;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#19988;&#21487;&#33021;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19634v1 Announce Type: cross  Abstract: The SdSv challenge Task 2 provided an opportunity to assess efficiency and robustness of modern text-independent speaker verification systems. But it also made it possible to test new approaches, capable of taking into account the main issues of this challenge (duration, language, ...). This paper describes the contributions of our laboratory to the speaker recognition field. These contributions highlight two other challenges in addition to short-duration and language: the mismatch between enrollment and test data and the one between subsets of the evaluation trial dataset. The proposed approaches experimentally show their relevance and efficiency on the SdSv evaluation, and could be of interest in many real-life applications.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#21644;&#20462;&#21098;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.19631</link><description>&lt;p&gt;
&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#26816;&#32034;&#22686;&#24378;&#30693;&#35782;&#32534;&#36753;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#21644;&#20462;&#21098;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#39640;&#25928;&#33021;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#25972;&#21512;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#65292;&#23548;&#33268;&#21487;&#33021;&#36807;&#26102;&#25110;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#24403;&#22788;&#29702;&#22810;&#36339;&#38382;&#39064;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;LLMs&#26356;&#26032;&#21644;&#25972;&#21512;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#22810;&#20010;&#30693;&#35782;&#29255;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#23450;&#21046;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#12290;RAE&#39318;&#20808;&#26816;&#32034;&#32534;&#36753;&#21518;&#30340;&#20107;&#23454;&#65292;&#28982;&#21518;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#23436;&#21892;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26816;&#32034;&#26041;&#27861;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#26469;&#35782;&#21035;&#38142;&#24335;&#20107;&#23454;&#65292;&#32780;&#22825;&#30495;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25628;&#32034;&#21487;&#33021;&#20250;&#24573;&#30053;&#36825;&#20123;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#37319;&#29992;&#20102;&#20462;&#21098;&#31574;&#30053;&#65292;&#20174;&#26816;&#32034;&#21040;&#30340;&#20107;&#23454;&#20013;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#65292;&#36825;&#22686;&#24378;&#20102;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19631v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework tailored for multi-hop question answering. RAE first retrieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that na\"ive similarity-based searches might miss. Additionally, our framework incorporates a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the edi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#22320;&#22270;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23548;&#33322;&#35828;&#26126;&#29983;&#25104;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#65292;&#26377;&#26395;&#38477;&#20302;&#29983;&#25104;&#25351;&#20196;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19603</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#22320;&#22270;&#30340;&#23548;&#33322;&#35828;&#26126;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semantic Map-based Generation of Navigation Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19603
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#22320;&#22270;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23548;&#33322;&#35828;&#26126;&#29983;&#25104;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#65292;&#26377;&#26395;&#38477;&#20302;&#29983;&#25104;&#25351;&#20196;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#23548;&#33322;&#35828;&#26126;&#30340;&#29983;&#25104;&#24456;&#24863;&#20852;&#36259;&#65292;&#26080;&#35770;&#26159;&#20316;&#20026;&#33258;&#36523;&#23384;&#22312;&#30340;&#25991;&#26412;&#36824;&#26159;&#20316;&#20026;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#35757;&#32451;&#26448;&#26009;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23548;&#33322;&#35828;&#26126;&#29983;&#25104;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#20351;&#29992;&#35821;&#20041;&#22320;&#22270;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#30340;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#12290;&#20256;&#32479;&#26041;&#27861;&#37319;&#29992;&#19968;&#31995;&#21015;&#20840;&#26223;&#22270;&#20687;&#26469;&#29983;&#25104;&#23548;&#33322;&#35828;&#26126;&#12290;&#35821;&#20041;&#22320;&#22270;&#23558;&#35270;&#35273;&#32454;&#33410;&#25277;&#35937;&#20986;&#26469;&#65292;&#23558;&#22810;&#20010;&#20840;&#26223;&#22270;&#20687;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#21040;&#21333;&#20010;&#33258;&#19978;&#32780;&#19979;&#30340;&#34920;&#31034;&#20013;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#22788;&#29702;&#36755;&#20837;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#20041;&#22320;&#22270;&#29983;&#25104;&#35828;&#26126;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#22987;&#27169;&#22411;&#65292;&#24182;&#35831;&#20154;&#24037;&#20027;&#35266;&#35780;&#20272;&#29983;&#25104;&#35828;&#26126;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#35843;&#26597;&#26174;&#31034;&#65292;&#20351;&#29992;&#35821;&#20041;&#22320;&#22270;&#29983;&#25104;&#35828;&#26126;&#32780;&#19981;&#26159;&#19968;&#31995;&#21015;&#20840;&#26223;&#22270;&#20687;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#30740;&#31350;&#33539;&#22260;&#20173;&#28982;&#24191;&#38420;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19603v1 Announce Type: cross  Abstract: We are interested in the generation of navigation instructions, either in their own right or as training material for robotic navigation task. In this paper, we propose a new approach to navigation instruction generation by framing the problem as an image captioning task using semantic maps as visual input. Conventional approaches employ a sequence of panorama images to generate navigation instructions. Semantic maps abstract away from visual details and fuse the information in multiple panorama images into a single top-down representation, thereby reducing computational complexity to process the input. We present a benchmark dataset for instruction generation using semantic maps, propose an initial model and ask human subjects to manually assess the quality of generated instructions. Our initial investigations show promise in using semantic maps for instruction generation instead of a sequence of panorama images, but there is vast sco
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25903;&#25345;&#26631;&#27880;&#32773;&#65292;&#25552;&#20986;&#26032;&#31574;&#30053;&#26469;&#21019;&#24314;&#26356;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24341;&#20837;GAHD&#24503;&#35821;&#23545;&#25239;&#24615;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;GAHD&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19559</link><description>&lt;p&gt;
&#25903;&#25345;&#26631;&#27880;&#32773;&#20197;&#25913;&#36827;&#23545;&#25239;&#25968;&#25454;&#25910;&#38598;&#65306;&#26469;&#33258;GAHD&#30340;&#25945;&#35757;&#65292;&#19968;&#20010;&#24503;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Data Collection by Supporting Annotators: Lessons from GAHD, a German Hate Speech Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19559
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25903;&#25345;&#26631;&#27880;&#32773;&#65292;&#25552;&#20986;&#26032;&#31574;&#30053;&#26469;&#21019;&#24314;&#26356;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24341;&#20837;GAHD&#24503;&#35821;&#23545;&#25239;&#24615;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;GAHD&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21388;&#24694;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#23427;&#20204;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#20174;&#31038;&#20132;&#23186;&#20307;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#31995;&#32479;&#24615;&#32570;&#38519;&#21644;&#20559;&#35265;&#65292;&#23548;&#33268;&#27169;&#22411;&#20855;&#26377;&#31616;&#21333;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#24369;&#28857;&#25910;&#38598;&#30340;&#23545;&#25239;&#24615;&#25968;&#25454;&#38598;&#25215;&#35834;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#21487;&#33021;&#32531;&#24930;&#19988;&#26114;&#36149;&#65292;&#32780;&#20010;&#20307;&#26631;&#27880;&#32773;&#30340;&#21019;&#36896;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GAHD&#65292;&#19968;&#20010;&#26032;&#30340;&#24503;&#35821;&#23545;&#25239;&#24615;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#32422;11k&#20010;&#31034;&#20363;&#12290;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#25903;&#25345;&#26631;&#27880;&#32773;&#30340;&#26032;&#31574;&#30053;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#21019;&#24314;&#26356;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#20026;&#27599;&#31181;&#31574;&#30053;&#25552;&#20379;&#20102;&#26631;&#27880;&#32773;&#24847;&#35265;&#20998;&#27495;&#30340;&#25163;&#21160;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#22312;GAHD&#19978;&#30340;&#35757;&#32451;&#26174;&#28982;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19559v1 Announce Type: new  Abstract: Hate speech detection models are only as good as the data they are trained on. Datasets sourced from social media suffer from systematic gaps and biases, leading to unreliable models with simplistic decision boundaries. Adversarial datasets, collected by exploiting model weaknesses, promise to fix this problem. However, adversarial data collection can be slow and costly, and individual annotators have limited creativity. In this paper, we introduce GAHD, a new German Adversarial Hate speech Dataset comprising ca.\ 11k examples. During data collection, we explore new strategies for supporting annotators, to create more diverse adversarial examples more efficiently and provide a manual analysis of annotator disagreements for each strategy. Our experiments show that the resulting dataset is challenging even for state-of-the-art hate speech detection models, and that training on GAHD clearly improves model robustness. Further, we find that m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#21033;&#29992;&#27604;&#36739;&#35780;&#20272;&#21644;&#28789;&#27963;&#30340;NLG&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;&#29305;&#23450;&#27700;&#21360;&#35774;&#32622;&#23545;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.19548</link><description>&lt;p&gt;
WaterJudge: &#22312;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#32622;&#27700;&#21360;&#26102;&#36136;&#37327;&#26816;&#27979;&#30340;&#26435;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19548
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#21033;&#29992;&#27604;&#36739;&#35780;&#20272;&#21644;&#28789;&#27963;&#30340;NLG&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;&#29305;&#23450;&#27700;&#21360;&#35774;&#32622;&#23545;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#22914;LLMs&#36825;&#26679;&#30340;&#29983;&#25104;&#24335;AI&#31995;&#32479;&#35774;&#32622;&#27700;&#21360;&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#65292;&#36825;&#31181;&#20852;&#36259;&#26159;&#22240;&#20026;&#23427;&#20204;&#22312;&#24191;&#27867;&#20219;&#21153;&#20013;&#30340;&#22686;&#24378;&#33021;&#21147;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#65292;&#21487;&#20197;&#21033;&#29992;&#22312;&#35789;&#20998;&#24067;&#20013;&#30340;&#23567;&#30340;&#12289;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#21464;&#21270;&#26469;&#24212;&#29992;&#21644;&#26816;&#27979;&#27700;&#21360;&#65292;&#20294;&#23545;&#36825;&#20123;&#25200;&#21160;&#23545;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#36824;&#27809;&#26377;&#22826;&#22810;&#30340;&#30740;&#31350;&#12290;&#22312;&#36873;&#25321;&#36866;&#24403;&#30340;&#27700;&#21360;&#35774;&#32622;&#26041;&#38754;&#65292;&#24179;&#34913;&#39640;&#21487;&#26816;&#27979;&#24615;&#21644;&#26368;&#23567;&#24615;&#33021;&#38477;&#32423;&#33267;&#20851;&#37325;&#35201;&#65307;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#20854;&#20013;&#20351;&#29992;&#27604;&#36739;&#35780;&#20272;&#12289;&#19968;&#20010;&#28789;&#27963;&#30340;NLG&#35780;&#20272;&#26694;&#26550;&#65292;&#26469;&#35780;&#20272;&#29305;&#23450;&#27700;&#21360;&#35774;&#32622;&#36896;&#25104;&#30340;&#36136;&#37327;&#36864;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#21487;&#35270;&#21270;&#27700;&#21360;&#35774;&#32622;&#30340;&#36136;&#37327;-&#26816;&#27979;&#26435;&#34913;&#65292;&#20174;&#32780;&#20026;&#25214;&#21040;&#19968;&#20010;LLM&#27700;&#21360;&#25805;&#20316;&#28857;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19548v1 Announce Type: new  Abstract: Watermarking generative-AI systems, such as LLMs, has gained considerable interest, driven by their enhanced capabilities across a wide range of tasks. Although current approaches have demonstrated that small, context-dependent shifts in the word distributions can be used to apply and detect watermarks, there has been little work in analyzing the impact that these perturbations have on the quality of generated texts. Balancing high detectability with minimal performance degradation is crucial in terms of selecting the appropriate watermarking setting; therefore this paper proposes a simple analysis framework where comparative assessment, a flexible NLG evaluation framework, is used to assess the quality degradation caused by a particular watermark setting. We demonstrate that our framework provides easy visualization of the quality-detection trade-off of watermark settings, enabling a simple solution to find an LLM watermark operating po
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.19521</link><description>&lt;p&gt;
&#35299;&#37322;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20013;&#30340;&#20851;&#38190;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#25152;&#37319;&#29992;&#30340;&#26426;&#21046;&#12290;&#22312;&#38646;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#31867;&#20284;&#8220;&#27861;&#22269;&#30340;&#39318;&#37117;&#26159;&#8221;&#30340;&#25552;&#31034;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#24847;&#21147;&#22836;&#20250;&#20174;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20027;&#39064;&#23454;&#20307;&#65292;&#22914;&#8220;&#27861;&#22269;&#8221;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#21518;&#32493;&#30340;MLP&#20197;&#22238;&#24518;&#25152;&#38656;&#30340;&#31572;&#26696;&#65292;&#22914;&#8220;&#24052;&#40654;&#8221;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;MLP&#30340;&#36755;&#20986;&#20998;&#35299;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#32452;&#20214;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36319;&#38543;&#36825;&#20123;&#29305;&#23450;&#20219;&#21153;&#22836;&#30340;MLP&#23618;&#30340;&#21151;&#33021;&#12290;&#22312;&#27531;&#24046;&#27969;&#20013;&#65292;&#23427;&#20250;&#25830;&#38500;&#25110;&#25918;&#22823;&#26469;&#33258;&#21508;&#20010;&#22836;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#20250;&#29983;&#25104;&#19968;&#20010;&#32452;&#20214;&#65292;&#23558;&#27531;&#24046;&#27969;&#37325;&#26032;&#23450;&#21521;&#21040;&#39044;&#26399;&#31572;&#26696;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#38646;&#27425;&#26426;&#21046;&#20063;&#36866;&#29992;&#20110;&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19521v1 Announce Type: cross  Abstract: In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like "The capital of France is," task-specific attention heads extract the topic entity, such as "France," from the context and pass it to subsequent MLPs to recall the required answer such as "Paris." We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25552;&#21319;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20986;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#21487;&#34892;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19511</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#20020;&#24202;&#25968;&#25454;&#25552;&#21319;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Clinical NLP Performance through Language Model-Generated Synthetic Clinical Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19511
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25552;&#21319;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20986;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#21487;&#34892;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#20135;&#22823;&#37327;&#25968;&#25454;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#20174;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#25552;&#21319;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#26174;&#31034;&#22312;&#36825;&#26679;&#19968;&#20010;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#26377;&#21487;&#34892;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19511v1 Announce Type: new  Abstract: Generative models have been showing potential for producing data in mass. This study explores the enhancement of clinical natural language processing performance by utilizing synthetic data generated from advanced language models. Promising results show feasible applications in such a high-stakes domain.
&lt;/p&gt;</description></item><item><title>VoxAngeles&#26159;&#19968;&#20010;&#21253;&#21547;UCLA&#35821;&#38899;&#23454;&#39564;&#23460;&#26723;&#26696;&#30340;&#35821;&#38899;&#20999;&#20998;&#21644;&#38899;&#32032;&#32423;&#23545;&#40784;&#30340;&#35821;&#26009;&#24211;&#65292;&#22686;&#24378;&#20102;&#21407;&#22987;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23450;&#37327;&#35821;&#38899;&#31867;&#22411;&#23398;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.19509</link><description>&lt;p&gt;
UCLA&#35821;&#38899;&#23454;&#39564;&#23460;&#26723;&#26696;&#30340;&#35821;&#38899;&#20999;&#20998;
&lt;/p&gt;
&lt;p&gt;
Phonetic Segmentation of the UCLA Phonetics Lab Archive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19509
&lt;/p&gt;
&lt;p&gt;
VoxAngeles&#26159;&#19968;&#20010;&#21253;&#21547;UCLA&#35821;&#38899;&#23454;&#39564;&#23460;&#26723;&#26696;&#30340;&#35821;&#38899;&#20999;&#20998;&#21644;&#38899;&#32032;&#32423;&#23545;&#40784;&#30340;&#35821;&#26009;&#24211;&#65292;&#22686;&#24378;&#20102;&#21407;&#22987;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23450;&#37327;&#35821;&#38899;&#31867;&#22411;&#23398;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35821;&#38899;&#25216;&#26415;&#21644;&#27604;&#36739;&#35821;&#35328;&#23398;&#20381;&#36182;&#20110;&#33719;&#24471;&#22810;&#26679;&#21270;&#21644;&#21487;&#35775;&#38382;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290; UCLA&#35821;&#38899;&#23454;&#39564;&#23460;&#26723;&#26696;&#26159;&#26368;&#26089;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#35821;&#26009;&#24211;&#20043;&#19968;&#65292;&#20854;&#20013;&#21253;&#21547;314&#31181;&#35821;&#35328;&#30340;&#38271;&#31687;&#38899;&#39057;&#35760;&#24405;&#21644;&#35821;&#38899;&#20999;&#20998;&#12290;&#26368;&#36817;&#65292;&#20854;&#20013;95&#31181;&#35821;&#35328;&#24050;&#32463;&#19982;&#21333;&#35789;&#32423;&#35821;&#38899;&#20999;&#20998;&#36827;&#34892;&#20102;&#26102;&#38388;&#23545;&#40784;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;VoxAngeles&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#23457;&#38405;&#30340;UCLA&#35821;&#38899;&#23454;&#39564;&#23460;&#26723;&#26696;&#30340;&#35821;&#38899;&#20999;&#20998;&#21644;&#38899;&#32032;&#32423;&#23545;&#40784;&#35821;&#26009;&#24211;&#65292;&#20351;&#29992;&#20102;95&#31181;&#35821;&#35328;&#30340;CMU&#37325;&#26032;&#21457;&#34892;&#20316;&#20026;&#25105;&#20204;&#30340;&#36215;&#22987;&#28857;&#12290;VoxAngeles&#36824;&#21253;&#25324;&#20102;&#21407;&#22987;UCLA&#35821;&#26009;&#24211;&#30340;&#21333;&#35789;&#32423;&#21644;&#38899;&#32032;&#32423;&#20999;&#20998;&#65292;&#20197;&#21450;&#21333;&#35789;&#21644;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#12289;&#20803;&#38899;&#20849;&#25391;&#23792;&#21644;&#20803;&#38899;f0&#30340;&#35821;&#38899;&#27979;&#37327;&#12290;&#35813;&#35821;&#26009;&#24211;&#22686;&#24378;&#20102;&#21407;&#22987;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23450;&#37327;&#35821;&#38899;&#31867;&#22411;&#23398;&#65292;&#36890;&#36807;&#19968;&#20010;&#20803;&#38899;&#25972;&#20307;&#20998;&#26512;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#20102;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19509v1 Announce Type: new  Abstract: Research in speech technologies and comparative linguistics depends on access to diverse and accessible speech data. The UCLA Phonetics Lab Archive is one of the earliest multilingual speech corpora, with long-form audio recordings and phonetic transcriptions for 314 languages (Ladefoged et al., 2009). Recently, 95 of these languages were time-aligned with word-level phonetic transcriptions (Li et al., 2021). Here we present VoxAngeles, a corpus of audited phonetic transcriptions and phone-level alignments of the UCLA Phonetics Lab Archive, which uses the 95-language CMU re-release as our starting point. VoxAngeles also includes word- and phone-level segmentations from the original UCLA corpus, as well as phonetic measurements of word and phone durations, vowel formants, and vowel f0. This corpus enhances the usability of the original data, particularly for quantitative phonetic typology, as demonstrated through a case study of vowel int
&lt;/p&gt;</description></item><item><title>JDocQA&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22522;&#20110;&#25991;&#26723;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#35201;&#27714;&#32467;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#22238;&#31572;&#38382;&#39064;&#65292;&#21253;&#25324;5,504&#20010;&#25991;&#26723;&#21644;11,600&#20010;&#38382;&#31572;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.19454</link><description>&lt;p&gt;
JDocQA&#65306;&#29992;&#20110;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#26085;&#35821;&#25991;&#26723;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
JDocQA: Japanese Document Question Answering Dataset for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19454
&lt;/p&gt;
&lt;p&gt;
JDocQA&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22522;&#20110;&#25991;&#26723;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#35201;&#27714;&#32467;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#22238;&#31572;&#38382;&#39064;&#65292;&#21253;&#25324;5,504&#20010;&#25991;&#26723;&#21644;11,600&#20010;&#38382;&#31572;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19454v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#25991;&#26723;&#38382;&#31572;&#26159;&#38024;&#23545;&#32473;&#23450;&#25991;&#26723;&#65288;&#22914;&#25253;&#21578;&#12289;&#24187;&#28783;&#29255;&#12289;&#23567;&#20876;&#23376;&#21644;&#32593;&#31449;&#65289;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#36825;&#26159;&#19968;&#39033;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#32440;&#36136;&#21644;&#30005;&#23376;&#24418;&#24335;&#30340;&#25991;&#26723;&#22312;&#25105;&#20204;&#30340;&#31038;&#20250;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290; &#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#38656;&#35201;&#29702;&#35299;&#25991;&#26412;&#65292;&#36824;&#38656;&#35201;&#29702;&#35299;&#22270;&#34920;&#65292;&#22240;&#27492;&#38500;&#20102;&#25991;&#26412;&#26041;&#27861;&#20043;&#22806;&#65292;&#36890;&#24120;&#36824;&#20250;&#30740;&#31350;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26041;&#27861;&#12290; &#25105;&#20204;&#25512;&#20986;&#20102;&#12298;&#26085;&#35821;&#25991;&#26723;&#38382;&#31572;&#12299;&#65288;JDocQA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22522;&#20110;&#25991;&#26723;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#22522;&#26412;&#19978;&#38656;&#35201;&#21516;&#26102;&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;5,504&#20010;PDF&#26684;&#24335;&#30340;&#25991;&#26723;&#21644;11,600&#20010;&#26085;&#35821;&#38382;&#31572;&#23454;&#20363;&#12290; &#27599;&#20010;&#38382;&#31572;&#23454;&#20363;&#37117;&#21253;&#25324;&#23545;&#25991;&#26723;&#39029;&#30340;&#24341;&#29992;&#21644;&#31572;&#26696;&#25552;&#31034;&#30340;&#36793;&#30028;&#26694;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19454v1 Announce Type: new  Abstract: Document question answering is a task of question answering on given documents such as reports, slides, pamphlets, and websites, and it is a truly demanding task as paper and electronic forms of documents are so common in our society. This is known as a quite challenging task because it requires not only text understanding but also understanding of figures and tables, and hence visual question answering (VQA) methods are often examined in addition to textual approaches. We introduce Japanese Document Question Answering (JDocQA), a large-scale document-based QA dataset, essentially requiring both visual and textual information to answer questions, which comprises 5,504 documents in PDF format and annotated 11,600 question-and-answer instances in Japanese. Each QA instance includes references to the document pages and bounding boxes for the answer clues. We incorporate multiple categories of questions and unanswerable questions from the do
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20559;&#22909;&#20248;&#21270;&#65288;MPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31616;&#21333;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Direct Preference Optimization&#65288;DPO&#65289;&#65292;&#28982;&#21518;&#22312;&#22256;&#38590;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;Reinforcement Learning with Human Feedback&#65288;RLHF&#65289;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#24369;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.19443</link><description>&lt;p&gt;
&#28151;&#21512;&#20559;&#22909;&#20248;&#21270;&#65306;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#36873;&#25321;&#19982;&#26356;&#22909;&#30340;&#21442;&#32771;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19443
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20559;&#22909;&#20248;&#21270;&#65288;MPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31616;&#21333;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Direct Preference Optimization&#65288;DPO&#65289;&#65292;&#28982;&#21518;&#22312;&#22256;&#38590;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;Reinforcement Learning with Human Feedback&#65288;RLHF&#65289;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22788;&#29702;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#30340;&#33021;&#21147;&#32780;&#26085;&#30410;&#21463;&#21040;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#26159;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;LLMs&#21487;&#33021;&#20250;&#32487;&#25215;&#26377;&#23475;&#20559;&#35265;&#65292;&#24182;&#20135;&#29983;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#23545;&#40784;&#30340;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#24102;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21644;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#22914;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#12290;&#36890;&#36807;&#20998;&#26512;RLHF&#21644;DPO&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MPO&#65288;&#28151;&#21512;&#20559;&#22909;&#20248;&#21270;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32531;&#35299;&#20004;&#31181;&#26041;&#27861;&#24369;&#28857;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#65306;&#39318;&#20808;&#22312;&#19968;&#20010;&#31616;&#21333;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;DPO&#65292;&#28982;&#21518;&#20877;&#22312;&#24102;&#26377;DPO&#27169;&#22411;&#20316;&#20026;&#21442;&#32771;&#27169;&#22411;&#30340;&#22256;&#38590;&#38598;&#19978;&#25191;&#34892;RLHF&#12290;&#22312;&#36825;&#37324;&#65292;&#31616;&#21333;&#21644;&#22256;&#38590;&#38598;&#26159;&#30001;&#35757;&#32451;&#33391;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#26500;&#24314;&#30340;&#65292;&#23558;&#21709;&#24212;&#23545;&#20998;&#25104;&#20855;&#26377;&#36739;&#22823;&#24046;&#36317;&#30340;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19443v1 Announce Type: new  Abstract: Large Language Models (LLMs) have become increasingly popular due to their ability to process and generate natural language. However, as they are trained on massive datasets of text, LLMs can inherit harmful biases and produce outputs that are not aligned with human values. This paper studies two main approaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF) and contrastive learning-based methods like Direct Preference Optimization (DPO). By analyzing the stability and robustness of RLHF and DPO, we propose MPO (Mixed Preference Optimization), a novel method that mitigates the weaknesses of both approaches. Specifically, we propose a two-stage training procedure: first train DPO on an easy dataset, and then perform RLHF on a difficult set with DPO model being the reference model. Here, the easy and difficult sets are constructed by a well-trained reward model that splits response pairs into those with large gaps of r
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#26816;&#27979;&#24182;&#32416;&#27491;&#22269;&#23478;&#26292;&#21147;&#27515;&#20129;&#25253;&#21578;&#31995;&#32479;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#33258;&#26432;&#21361;&#26426;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19432</link><description>&lt;p&gt;
&#36890;&#36807;&#27515;&#22240;&#35843;&#26597;&#31508;&#35760;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#25581;&#31034;&#33258;&#26432;&#21407;&#22240;&#30340;&#35823;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Uncovering Misattributed Suicide Causes through Annotation Inconsistency Detection in Death Investigation Notes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19432
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#26816;&#27979;&#24182;&#32416;&#27491;&#22269;&#23478;&#26292;&#21147;&#27515;&#20129;&#25253;&#21578;&#31995;&#32479;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#33258;&#26432;&#21361;&#26426;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20934;&#30830;&#24615;&#23545;&#31185;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#22269;&#23478;&#26292;&#21147;&#27515;&#20129;&#25253;&#21578;&#31995;&#32479;&#65288;NVDRS&#65289;&#25968;&#25454;&#34987;&#24191;&#27867;&#29992;&#20110;&#21457;&#29616;&#27515;&#20129;&#30340;&#27169;&#24335;&#21644;&#21407;&#22240;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;NVDRS&#20869;&#23384;&#22312;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#38169;&#35823;&#30340;&#33258;&#26432;&#21407;&#22240;&#24402;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#26469;&#26816;&#27979;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#31867;&#20284;&#20132;&#21449;&#39564;&#35777;&#30340;&#33539;&#24335;&#26469;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;2003&#24180;&#33267;2020&#24180;&#38388;&#20174;NVDRS&#20013;&#30340;267,804&#36215;&#33258;&#26432;&#27515;&#20129;&#26696;&#20363;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23558;&#30446;&#26631;&#24030;&#30340;&#25968;&#25454;&#32435;&#20837;&#35757;&#32451;&#33258;&#26432;&#21361;&#26426;&#20998;&#31867;&#22120;&#65292;&#20351;&#24471;&#22312;&#30446;&#26631;&#24030;&#27979;&#35797;&#38598;&#19978;&#30340;F-1&#20998;&#25968;&#22686;&#21152;&#20102;5.4&#65285;&#65292;&#22312;&#20854;&#20182;&#24030;&#27979;&#35797;&#38598;&#19978;&#38477;&#20302;&#20102;1.1&#65285;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NVDRS&#27515;&#22240;&#35843;&#26597;&#31508;&#35760;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19432v1 Announce Type: cross  Abstract: Data accuracy is essential for scientific research and policy development. The National Violent Death Reporting System (NVDRS) data is widely used for discovering the patterns and causes of death. Recent studies suggested the annotation inconsistencies within the NVDRS and the potential impact on erroneous suicide-cause attributions. We present an empirical Natural Language Processing (NLP) approach to detect annotation inconsistencies and adopt a cross-validation-like paradigm to identify problematic instances. We analyzed 267,804 suicide death incidents between 2003 and 2020 from the NVDRS. Our results showed that incorporating the target state's data into training the suicide-crisis classifier brought an increase of 5.4% to the F-1 score on the target state's test set and a decrease of 1.1% on other states' test set. To conclude, we demonstrated the annotation inconsistencies in NVDRS's death investigation notes, identified problema
&lt;/p&gt;</description></item><item><title>&#27492;&#30740;&#31350;&#20174;&#35821;&#35328;&#23398;&#35282;&#24230;&#30740;&#31350;&#20102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#35299;&#37322;&#19981;&#19968;&#33268;&#65292;&#21457;&#29616;&#22312;&#21477;&#27861;&#36328;&#24230;&#27700;&#24179;&#19978;&#27604;&#36739;&#26041;&#27861;&#21487;&#20197;&#24179;&#28369;&#25481;&#26631;&#35760;&#32423;&#21035;&#30340;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#21160;&#24577;&#20272;&#35745;&#26368;&#37325;&#35201;&#36328;&#24230;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#36873;&#25321;&#37325;&#35201;&#26631;&#35760;&#30340;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2403.19424</link><description>&lt;p&gt;
&#35821;&#27861;&#36328;&#24230;&#20559;&#22909;&#22312;&#20107;&#21518;&#35299;&#37322;&#19981;&#19968;&#33268;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Syntactic Span Preferences in Post-Hoc Explanation Disagreement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19424
&lt;/p&gt;
&lt;p&gt;
&#27492;&#30740;&#31350;&#20174;&#35821;&#35328;&#23398;&#35282;&#24230;&#30740;&#31350;&#20102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#35299;&#37322;&#19981;&#19968;&#33268;&#65292;&#21457;&#29616;&#22312;&#21477;&#27861;&#36328;&#24230;&#27700;&#24179;&#19978;&#27604;&#36739;&#26041;&#27861;&#21487;&#20197;&#24179;&#28369;&#25481;&#26631;&#35760;&#32423;&#21035;&#30340;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#21160;&#24577;&#20272;&#35745;&#26368;&#37325;&#35201;&#36328;&#24230;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#36873;&#25321;&#37325;&#35201;&#26631;&#35760;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#26159;&#22686;&#21152;&#27169;&#22411;&#36879;&#26126;&#24230;&#23545;&#29992;&#25143;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#29992;&#20110;&#24402;&#22240;&#26631;&#35760;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#32463;&#24120;&#20135;&#29983;&#19981;&#21516;&#30340;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#35821;&#35328;&#23398;&#35282;&#24230;&#30740;&#31350;&#20102;&#26041;&#27861;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;&#28508;&#22312;&#26469;&#28304;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#26041;&#27861;&#31995;&#32479;&#22320;&#36873;&#25321;&#19981;&#21516;&#31867;&#21035;&#30340;&#35789;&#65292;&#24182;&#19988;&#37027;&#20123;&#19982;&#20854;&#20182;&#26041;&#27861;&#21644;&#20154;&#31867;&#36798;&#25104;&#26368;&#39640;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#31867;&#20284;&#30340;&#35821;&#35328;&#20559;&#22909;&#12290;&#22914;&#26524;&#25105;&#20204;&#22312;&#21477;&#27861;&#36328;&#24230;&#27700;&#24179;&#19978;&#27604;&#36739;&#26041;&#27861;&#65292;&#37027;&#20040;&#26041;&#27861;&#20043;&#38388;&#30340;&#26631;&#35760;&#32423;&#24046;&#24322;&#23601;&#20250;&#34987;&#24179;&#28369;&#25481;&#12290;&#36890;&#36807;&#21160;&#24577;&#20272;&#35745;&#26368;&#37325;&#35201;&#30340;&#36328;&#24230;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#22266;&#23450;&#22823;&#23567;&#20026;$k$&#30340;&#23376;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#26041;&#27861;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26356;&#39640;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;$k$&#21644;&#36328;&#24230;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#36873;&#25321;&#37325;&#35201;&#26631;&#35760;&#30340;&#25913;&#36827;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19424v1 Announce Type: cross  Abstract: Post-hoc explanation methods are an important tool for increasing model transparency for users. Unfortunately, the currently used methods for attributing token importance often yield diverging patterns. In this work, we study potential sources of disagreement across methods from a linguistic perspective. We find that different methods systematically select different classes of words and that methods that agree most with other methods and with humans display similar linguistic preferences. Token-level differences between methods are smoothed out if we compare them on the syntactic span level. We also find higher agreement across methods by estimating the most important spans dynamically instead of relying on a fixed subset of size $k$. We systematically investigate the interaction between $k$ and spans and propose an improved configuration for selecting important tokens.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;Twitter&#31038;&#21306;&#22312;&#30123;&#33495;&#25509;&#31181;&#32972;&#26223;&#19979;&#20855;&#26377;&#19981;&#21516;&#27807;&#36890;&#39118;&#26684;&#65292;&#38500;&#20102;&#22238;&#38899;&#23460;&#34892;&#20026;&#22806;&#65292;&#36824;&#23384;&#22312;&#30528;&#20854;&#20182;&#29420;&#29305;&#30340;&#31038;&#21306;&#27807;&#36890;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.19423</link><description>&lt;p&gt;
Twitter&#19978;&#30340;&#22238;&#38899;&#23460;&#21644;Idea&#23454;&#39564;&#23460;&#65306;&#27807;&#36890;&#39118;&#26684;
&lt;/p&gt;
&lt;p&gt;
Echo-chambers and Idea Labs: Communication Styles on Twitter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;Twitter&#31038;&#21306;&#22312;&#30123;&#33495;&#25509;&#31181;&#32972;&#26223;&#19979;&#20855;&#26377;&#19981;&#21516;&#27807;&#36890;&#39118;&#26684;&#65292;&#38500;&#20102;&#22238;&#38899;&#23460;&#34892;&#20026;&#22806;&#65292;&#36824;&#23384;&#22312;&#30528;&#20854;&#20182;&#29420;&#29305;&#30340;&#31038;&#21306;&#27807;&#36890;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Twitter&#31038;&#21306;&#22312;&#30123;&#33495;&#25509;&#31181;&#32972;&#26223;&#19979;&#30340;&#27807;&#36890;&#39118;&#26684;&#21644;&#32467;&#26500;&#12290;&#34429;&#28982;&#20027;&#27969;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22238;&#38899;&#23460;&#29616;&#35937;&#19978;&#65292;&#21363;&#26576;&#20123;&#35266;&#28857;&#34987;&#24378;&#21270;&#65292;&#21442;&#19982;&#32773;&#34987;&#38548;&#31163;&#22312;&#23545;&#31435;&#35266;&#28857;&#20043;&#22806;&#65292;&#20294;&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#19981;&#21516;&#31038;&#21306;&#20043;&#38388;&#23384;&#22312;&#30528;&#22810;&#26679;&#21270;&#30340;&#27807;&#36890;&#39118;&#26684;&#12290;&#38500;&#20102;&#21576;&#29616;&#22238;&#38899;&#23460;&#34892;&#20026;&#30340;&#31038;&#21306;&#22806;&#65292;&#36825;&#39033;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#20855;&#26377;&#19981;&#21516;&#27807;&#36890;&#27169;&#24335;&#30340;&#31038;&#21306;&#12290;&#36890;&#36807;&#25581;&#31034;&#31038;&#20132;&#32593;&#32476;&#20013;&#27807;&#36890;&#30340;&#24494;&#22937;&#24615;&#36136;&#65292;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#29702;&#35299;&#22312;&#32447;&#31038;&#21306;&#20869;&#22810;&#20803;&#21270;&#35266;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19423v1 Announce Type: cross  Abstract: This paper investigates the communication styles and structures of Twitter (X) communities within the vaccination context. While mainstream research primarily focuses on the echo-chamber phenomenon, wherein certain ideas are reinforced and participants are isolated from opposing opinions, this study reveals the presence of diverse communication styles across various communities. In addition to the communities exhibiting echo-chamber behavior, this research uncovers communities with distinct communication patterns. By shedding light on the nuanced nature of communication within social networks, this study emphasizes the significance of understanding the diversity of perspectives within online communities.
&lt;/p&gt;</description></item><item><title>BP4ER&#25552;&#20986;&#20102;&#19968;&#31181;Bootstrap Prompting&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#36827;&#34892;&#26174;&#24335;&#25512;&#29702;&#65292;&#20998;&#35299;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#20026;&#31616;&#21333;&#23376;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#24341;&#23548;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.19414</link><description>&lt;p&gt;
BP4ER: &#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#20013;&#29992;&#20110;&#26174;&#24335;&#25512;&#29702;&#30340;&#24341;&#23548;&#24335;&#33258;&#20030;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19414
&lt;/p&gt;
&lt;p&gt;
BP4ER&#25552;&#20986;&#20102;&#19968;&#31181;Bootstrap Prompting&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#36827;&#34892;&#26174;&#24335;&#25512;&#29702;&#65292;&#20998;&#35299;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#20026;&#31616;&#21333;&#23376;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#24341;&#23548;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;(MDG)&#22240;&#20854;&#23454;&#38469;&#20215;&#20540;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#37319;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23545;&#35805;&#19978;&#19979;&#25991;&#24314;&#27169;&#20026;&#24102;&#26377;&#26631;&#27880;&#21307;&#23398;&#23454;&#20307;&#30340;&#24207;&#21015;&#25991;&#26412;&#26469;&#29983;&#25104;&#21307;&#23398;&#22238;&#22797;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#22312;&#29983;&#25104;&#27969;&#30021;&#22238;&#22797;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#25552;&#20379;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#23454;&#20307;&#26631;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bootstrap Prompting for Explicit Reasoning in MDG (BP4ER)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#24314;&#27169;MDG&#30340;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#24182;&#36845;&#20195;&#22686;&#24378;&#36825;&#19968;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#37319;&#29992;&#30001;&#27973;&#20837;&#28145;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#26174;&#24335;&#25512;&#29702;&#65292;&#23558;MDG&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#12290;&#36825;&#20123;&#23376;&#38382;&#39064;&#24314;&#31435;&#22312;&#20808;&#21069;&#38382;&#39064;&#30340;&#31572;&#26696;&#22522;&#30784;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19414v1 Announce Type: new  Abstract: Medical dialogue generation (MDG) has gained increasing attention due to its substantial practical value. Previous works typically employ a sequence-to-sequence framework to generate medical responses by modeling dialogue context as sequential text with annotated medical entities. While these methods have been successful in generating fluent responses, they fail to provide process explanations of reasoning and require extensive entity annotation. To address these limitations, we propose the method Bootstrap Prompting for Explicit Reasoning in MDG (BP4ER), which explicitly model MDG's multi-step reasoning process and iteratively enhance this reasoning process. We employ a least-to-most prompting strategy to guide a large language model (LLM) in explicit reasoning, breaking down MDG into simpler sub-questions. These sub-questions build on answers from previous ones. Additionally, we also introduce two distinct bootstrapping techniques for 
&lt;/p&gt;</description></item><item><title>KazParC&#26159;&#19968;&#20010;&#36328;&#21704;&#33832;&#20811;&#35821;&#12289;&#33521;&#35821;&#12289;&#20420;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;371,902&#20010;&#24179;&#34892;&#21477;&#23376;&#65292;&#36824;&#24320;&#21457;&#20102;&#24615;&#33021;&#20248;&#36234;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;Tilmash&#12290;</title><link>https://arxiv.org/abs/2403.19399</link><description>&lt;p&gt;
KazParC&#65306;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#21704;&#33832;&#20811;&#24179;&#34892;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
KazParC: Kazakh Parallel Corpus for Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19399
&lt;/p&gt;
&lt;p&gt;
KazParC&#26159;&#19968;&#20010;&#36328;&#21704;&#33832;&#20811;&#35821;&#12289;&#33521;&#35821;&#12289;&#20420;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;371,902&#20010;&#24179;&#34892;&#21477;&#23376;&#65292;&#36824;&#24320;&#21457;&#20102;&#24615;&#33021;&#20248;&#36234;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;Tilmash&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;KazParC&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#36328;&#21704;&#33832;&#20811;&#35821;&#12289;&#33521;&#35821;&#12289;&#20420;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;&#32780;&#35774;&#35745;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#20316;&#20026;&#20854;&#31867;&#21035;&#20013;&#39318;&#20010;&#20063;&#26159;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#35821;&#26009;&#24211;&#65292;KazParC&#21253;&#21547;&#20102;371,902&#20010;&#24179;&#34892;&#21477;&#23376;&#30340;&#38598;&#21512;&#65292;&#28085;&#30422;&#19981;&#21516;&#39046;&#22495;&#65292;&#24182;&#22312;&#20154;&#31867;&#35793;&#32773;&#30340;&#21327;&#21161;&#19979;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24037;&#20316;&#36824;&#25193;&#23637;&#21040;&#20102;&#21457;&#23637;&#19968;&#20010;&#34987;&#26165;&#31216;&#20026;Tilmash&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#65292;Tilmash&#30340;&#34920;&#29616;&#19982;&#24037;&#19994;&#24040;&#22836;&#65292;&#22914;Google&#32763;&#35793;&#21644;Yandex&#32763;&#35793;&#65292;&#22312;&#26631;&#20934;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;BLEU&#21644;chrF&#65289;&#30340;&#22522;&#30784;&#19978;&#30456;&#23218;&#32654;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#36229;&#36234;&#12290;KazParC&#21644;Tilmash&#22343;&#21487;&#36890;&#36807;&#25105;&#20204;&#30340;GitHub&#23384;&#20648;&#24211;&#20197;&#30693;&#35782;&#20849;&#20139;&#32626;&#21517;4.0&#22269;&#38469;&#35768;&#21487;&#65288;CC BY 4.0&#65289;&#20844;&#24320;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19399v1 Announce Type: new  Abstract: We introduce KazParC, a parallel corpus designed for machine translation across Kazakh, English, Russian, and Turkish. The first and largest publicly available corpus of its kind, KazParC contains a collection of 371,902 parallel sentences covering different domains and developed with the assistance of human translators. Our research efforts also extend to the development of a neural machine translation model nicknamed Tilmash. Remarkably, the performance of Tilmash is on par with, and in certain instances, surpasses that of industry giants, such as Google Translate and Yandex Translate, as measured by standard evaluation metrics, such as BLEU and chrF. Both KazParC and Tilmash are openly available for download under the Creative Commons Attribution 4.0 International License (CC BY 4.0) through our GitHub repository.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#26816;&#26597;&#28857;&#21512;&#24182;&#26041;&#27861;&#65292;&#23637;&#29616;&#20102;&#22312;&#26368;&#23567;&#25104;&#26412;&#19979;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#33021;&#21147;&#20197;&#21450;&#22312;&#19981;&#21516;&#39046;&#22495;&#23637;&#31034;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.19390</link><description>&lt;p&gt;
&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#26816;&#26597;&#28857;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Checkpoint Merging via Bayesian Optimization in LLM Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19390
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#26816;&#26597;&#28857;&#21512;&#24182;&#26041;&#27861;&#65292;&#23637;&#29616;&#20102;&#22312;&#26368;&#23567;&#25104;&#26412;&#19979;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#33021;&#21147;&#20197;&#21450;&#22312;&#19981;&#21516;&#39046;&#22495;&#23637;&#31034;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#21644;Gemini&#30340;&#36805;&#36895;&#22686;&#38271;&#31361;&#26174;&#20102;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#36164;&#28304;&#30340;&#24378;&#28872;&#38656;&#27714;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#29615;&#22659;&#25104;&#26412;&#65292;&#36825;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#26816;&#26597;&#28857;&#21512;&#24182;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20855;&#26377;&#20849;&#20139;&#35757;&#32451;&#36712;&#36857;&#30340;LLM&#26816;&#26597;&#28857;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#23545;&#26368;&#20339;&#21512;&#24182;&#26435;&#37325;&#36827;&#34892;&#24191;&#27867;&#30340;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#12290;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#22312;&#26368;&#23567;&#25104;&#26412;&#19979;&#33719;&#24471;&#37325;&#22823;&#25910;&#30410;&#30340;&#26426;&#20250;&#65307;&#65288;2&#65289;&#23613;&#31649;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#32473;&#23450;&#30340;&#20445;&#30041;&#25968;&#25454;&#38598;&#65292;&#20294;&#20173;&#23637;&#31034;&#20102;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#31283;&#20581;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#26159;&#39044;&#35757;&#32451;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19390v1 Announce Type: new  Abstract: The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs. To alleviate this issue, we propose checkpoint merging in pretraining LLM. This method utilizes LLM checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization. Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining.
&lt;/p&gt;</description></item><item><title>EthioMT&#26159;&#19968;&#20010;&#26032;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#20026;15&#31181;&#20302;&#36164;&#28304;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#25552;&#20379;&#25903;&#25345;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#25913;&#36827;&#20102;&#30740;&#31350;&#30028;&#23545;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.19365</link><description>&lt;p&gt;
EthioMT&#65306;&#20302;&#36164;&#28304;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
EthioMT: Parallel Corpus for Low-resource Ethiopian Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19365
&lt;/p&gt;
&lt;p&gt;
EthioMT&#26159;&#19968;&#20010;&#26032;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#20026;15&#31181;&#20302;&#36164;&#28304;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#25552;&#20379;&#25903;&#25345;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#25913;&#36827;&#20102;&#30740;&#31350;&#30028;&#23545;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#21462;&#24471;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#12289;&#26032;&#38395;&#20998;&#31867;&#21644;&#38382;&#31572;&#31561;&#39640;&#36164;&#28304;&#35821;&#35328;&#20219;&#21153;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;MT&#30340;&#24615;&#33021;&#20173;&#26377;&#24456;&#22823;&#25913;&#36827;&#31354;&#38388;&#12290;&#36825;&#26159;&#30001;&#20110;&#36825;&#20123;&#35821;&#35328;&#21487;&#29992;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#35268;&#27169;&#36739;&#23567;&#65292;&#22914;&#26524;&#26377;&#30340;&#35805;&#12290;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#30340;NLP&#20063;&#22240;&#20026;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;NLP&#20219;&#21153;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;MT&#65289;&#30340;&#19981;&#21487;&#29992;&#32780;&#21463;&#21040;&#30456;&#21516;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#30028;&#24182;&#20419;&#36827;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EthioMT - &#19968;&#20010;&#21253;&#21547;15&#31181;&#35821;&#35328;&#30340;&#26032;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25910;&#38598;&#22467;&#22622;&#20420;&#27604;&#20122;&#26356;&#24120;&#35265;&#30340;&#24050;&#30740;&#31350;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#20351;&#29992;transformer&#21644;&#24494;&#35843;&#26041;&#27861;&#23545;&#26032;&#25910;&#38598;&#30340;&#35821;&#26009;&#24211;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;23&#31181;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19365v1 Announce Type: new  Abstract: Recent research in natural language processing (NLP) has achieved impressive performance in tasks such as machine translation (MT), news classification, and question-answering in high-resource languages. However, the performance of MT leaves much to be desired for low-resource languages. This is due to the smaller size of available parallel corpora in these languages, if such corpora are available at all. NLP in Ethiopian languages suffers from the same issues due to the unavailability of publicly accessible datasets for NLP tasks, including MT. To help the research community and foster research for Ethiopian languages, we introduce EthioMT -- a new parallel corpus for 15 languages. We also create a new benchmark by collecting a dataset for better-researched languages in Ethiopia. We evaluate the newly collected corpus and the benchmark dataset for 23 Ethiopian languages using transformer and fine-tuning approaches.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#32435;&#20837;&#26102;&#38388;&#21644;&#24773;&#32490;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#19978;&#39044;&#27979;Reddit&#29992;&#25143;&#24739;&#26377;&#30149;&#24577;&#36172;&#21338;&#38556;&#30861;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#39034;&#24207;&#27169;&#22411;&#27604;&#36830;&#25509;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.19358</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#23545;&#30149;&#24577;&#36172;&#21338;&#30340;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Risk prediction of pathological gambling on social media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19358
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#32435;&#20837;&#26102;&#38388;&#21644;&#24773;&#32490;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#19978;&#39044;&#27979;Reddit&#29992;&#25143;&#24739;&#26377;&#30149;&#24577;&#36172;&#21338;&#38556;&#30861;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#39034;&#24207;&#27169;&#22411;&#27604;&#36830;&#25509;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#19978;&#30340;&#39118;&#38505;&#39044;&#27979;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#23545;Reddit&#29992;&#25143;&#36827;&#34892;&#20998;&#31867;&#65292;&#30830;&#23450;&#26159;&#21542;&#24739;&#26377;&#30149;&#24577;&#36172;&#21338;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#20391;&#37325;&#20110;&#23558;&#26102;&#38388;&#21644;&#24773;&#32490;&#29305;&#24449;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;&#39044;&#22788;&#29702;&#38454;&#27573;&#28041;&#21450;&#36890;&#36807;&#22635;&#20805;&#24207;&#21015;&#26469;&#22788;&#29702;&#24086;&#23376;&#30340;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#12290;&#20004;&#31181;&#22522;&#20934;&#26550;&#26500;&#29992;&#20110;&#21021;&#27493;&#35780;&#20272;&#65306;&#22522;&#20110;BERT&#20998;&#31867;&#22120;&#30340;&#27599;&#20010;&#29992;&#25143;&#30340;&#36830;&#25509;&#24086;&#23376;&#21644;&#22522;&#20110;GRU&#21644;LSTM&#30340;&#39034;&#24207;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39034;&#24207;&#27169;&#22411;&#20248;&#20110;&#22522;&#20110;&#36830;&#25509;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#34928;&#20943;&#23618;&#65288;TD&#65289;&#21644;&#24773;&#24863;&#20998;&#31867;&#23618;&#65288;EmoBERTa&#65289;&#36890;&#36807;LSTM&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#33258;&#27880;&#24847;&#21147;&#23618;&#24182;&#27809;&#26377;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19358v1 Announce Type: new  Abstract: This paper addresses the problem of risk prediction on social media data, specifically focusing on the classification of Reddit users as having a pathological gambling disorder. To tackle this problem, this paper focuses on incorporating temporal and emotional features into the model. The preprocessing phase involves dealing with the time irregularity of posts by padding sequences. Two baseline architectures are used for preliminary evaluation: BERT classifier on concatenated posts per user and GRU with LSTM on sequential data. Experimental results demonstrate that the sequential models outperform the concatenation-based model. The results of the experiments conclude that the incorporation of a time decay layer (TD) and passing the emotion classification layer (EmoBERTa) through LSTM improves the performance significantly. Experiments concluded that the addition of a self-attention layer didn't significantly improve the performance of th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;AIpom&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#32534;&#30721;&#22120;&#24207;&#21015;&#26631;&#35760;&#22120;&#30340;&#39044;&#27979;&#65292;&#25104;&#21151;&#22312;SemEval-2024&#20219;&#21153;8&#20013;&#30340;&#25490;&#21517;&#31532;&#20108;&#65292;&#24182;&#21462;&#24471;&#20102;15.94&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#27969;&#27700;&#32447;&#22788;&#29702;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19354</link><description>&lt;p&gt;
AIpom&#22312;SemEval-2024&#20219;&#21153;8&#20013;&#30340;&#24212;&#29992;&#65306;&#22312;M4&#20013;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
AIpom at SemEval-2024 Task 8: Detecting AI-produced Outputs in M4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;AIpom&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#32534;&#30721;&#22120;&#24207;&#21015;&#26631;&#35760;&#22120;&#30340;&#39044;&#27979;&#65292;&#25104;&#21151;&#22312;SemEval-2024&#20219;&#21153;8&#20013;&#30340;&#25490;&#21517;&#31532;&#20108;&#65292;&#24182;&#21462;&#24471;&#20102;15.94&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#27969;&#27700;&#32447;&#22788;&#29702;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;AIpom&#65292;&#19968;&#20010;&#26088;&#22312;&#26816;&#27979;&#20154;&#24037;&#25776;&#20889;&#21644;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#36793;&#30028;&#30340;&#31995;&#32479;&#65288;SemEval-2024&#20219;&#21153;8&#65292;&#23376;&#20219;&#21153;C&#65306;&#20154;&#26426;&#28151;&#21512;&#25991;&#26412;&#26816;&#27979;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#27969;&#31243;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#20165;&#32534;&#30721;&#22120;&#30340;&#24207;&#21015;&#26631;&#35760;&#22120;&#30340;&#39044;&#27979;&#12290;AIpom&#22312;&#25490;&#34892;&#27036;&#19978;&#25490;&#21517;&#31532;&#20108;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;15.94&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12290;&#28040;&#34701;&#30740;&#31350;&#35777;&#23454;&#20102;&#36890;&#36807;&#23545;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#27969;&#27700;&#32447;&#22788;&#29702;&#22312;&#24615;&#33021;&#25913;&#21892;&#26041;&#38754;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19354v1 Announce Type: new  Abstract: This paper describes AIpom, a system designed to detect a boundary between human-written and machine-generated text (SemEval-2024 Task 8, Subtask C: Human-Machine Mixed Text Detection). We propose a two-stage pipeline combining predictions from an instruction-tuned decoder-only model and encoder-only sequence taggers. AIpom is ranked second on the leaderboard while achieving a Mean Absolute Error of 15.94. Ablation studies confirm the benefits of pipelining encoder and decoder models, particularly in terms of improved performance.
&lt;/p&gt;</description></item><item><title>Babel Briefings&#26159;&#19968;&#20010;&#21253;&#21547;&#20840;&#29699;30&#31181;&#35821;&#35328;&#21644;54&#20010;&#22320;&#28857;&#30340;470&#19975;&#26465;&#26032;&#38395;&#26631;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#23186;&#20307;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#25110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;&#20107;&#20214;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#30340;&#25991;&#31456;&#32858;&#31867;&#21644;&#20107;&#20214;&#29305;&#24449;&#21487;&#35270;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.19352</link><description>&lt;p&gt;
&#19990;&#30028;&#21508;&#22320;&#30340;&#22810;&#35821;&#31181;&#26032;&#38395;&#26631;&#39064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A diverse Multilingual News Headlines Dataset from around the World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19352
&lt;/p&gt;
&lt;p&gt;
Babel Briefings&#26159;&#19968;&#20010;&#21253;&#21547;&#20840;&#29699;30&#31181;&#35821;&#35328;&#21644;54&#20010;&#22320;&#28857;&#30340;470&#19975;&#26465;&#26032;&#38395;&#26631;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#23186;&#20307;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#25110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;&#20107;&#20214;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#30340;&#25991;&#31456;&#32858;&#31867;&#21644;&#20107;&#20214;&#29305;&#24449;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Babel Briefings&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;2020&#24180;8&#26376;&#21040;2021&#24180;11&#26376;&#30340;470&#19975;&#26465;&#26032;&#38395;&#26631;&#39064;&#65292;&#28085;&#30422;30&#31181;&#35821;&#35328;&#21644;&#20840;&#29699;54&#20010;&#22320;&#28857;&#65292;&#20854;&#20013;&#21253;&#25324;&#25152;&#26377;&#25991;&#31456;&#30340;&#33521;&#25991;&#32763;&#35793;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#23186;&#20307;&#30740;&#31350;&#65292;&#21487;&#20316;&#20026;&#35757;&#32451;&#25110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#26131;&#29992;&#30340;&#25991;&#31456;&#38598;&#21512;&#65292;&#29992;&#20110;&#20998;&#26512;&#20840;&#29699;&#26032;&#38395;&#25253;&#36947;&#21644;&#25991;&#21270;&#21465;&#20107;&#12290;&#20316;&#20026;&#23545;&#36825;&#19968;&#25968;&#25454;&#38598;&#20998;&#26512;&#30340;&#31616;&#21333;&#28436;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#26412;&#36807;&#31243;&#21033;&#29992;TF-IDF&#21152;&#26435;&#30456;&#20284;&#24230;&#25351;&#26631;&#23558;&#25991;&#31456;&#20998;&#25104;&#26377;&#20851;&#21516;&#19968;&#20107;&#20214;&#30340;&#32858;&#31867;&#12290;&#25105;&#20204;&#28982;&#21518;&#21487;&#35270;&#21270;&#35813;&#20107;&#20214;&#30340;\emph{&#20107;&#20214;&#31614;&#21517;}&#65292;&#26174;&#31034;&#38543;&#26102;&#38388;&#23637;&#31034;&#21738;&#31181;&#35821;&#35328;&#30340;&#25991;&#31456;&#65292;&#25581;&#31034;&#22522;&#20110;&#20107;&#20214;&#30340;&#37051;&#36817;&#24615;&#21644;&#24847;&#22806;&#24615;&#30340;&#30452;&#35266;&#29305;&#24449;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#22312;\href{https://www.kaggle.com/datasets/fe}{&#38142;&#25509;}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19352v1 Announce Type: new  Abstract: Babel Briefings is a novel dataset featuring 4.7 million news headlines from August 2020 to November 2021, across 30 languages and 54 locations worldwide with English translations of all articles included. Designed for natural language processing and media studies, it serves as a high-quality dataset for training or evaluating language models as well as offering a simple, accessible collection of articles, for example, to analyze global news coverage and cultural narratives. As a simple demonstration of the analyses facilitated by this dataset, we use a basic procedure using a TF-IDF weighted similarity metric to group articles into clusters about the same event. We then visualize the \emph{event signatures} of the event showing articles of which languages appear over time, revealing intuitive features based on the proximity of the event and unexpectedness of the event. The dataset is available on \href{https://www.kaggle.com/datasets/fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#20013;&#23545;&#19981;&#21512;&#29702;&#24615;&#30340;&#21453;&#24212;&#65292;&#35774;&#35745;&#20102;&#19981;&#21512;&#29702;&#25968;&#23398;&#38382;&#39064;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#35745;&#31639;&#21644;&#32467;&#35770;&#25552;&#31034;&#27169;&#26495;&#65292;&#25552;&#21319;&#20102;&#23427;&#20204;&#22312;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19346</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#23545;&#19981;&#21512;&#29702;&#24615;&#27627;&#26080;&#24847;&#35782;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Unconscious of Unreasonability in Math Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#20013;&#23545;&#19981;&#21512;&#29702;&#24615;&#30340;&#21453;&#24212;&#65292;&#35774;&#35745;&#20102;&#19981;&#21512;&#29702;&#25968;&#23398;&#38382;&#39064;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#35745;&#31639;&#21644;&#32467;&#35770;&#25552;&#31034;&#27169;&#26495;&#65292;&#25552;&#21319;&#20102;&#23427;&#20204;&#22312;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#24040;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#32473;&#20986;&#21253;&#21547;&#19981;&#21512;&#29702;&#38169;&#35823;&#30340;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#38754;&#23545;&#19981;&#21512;&#29702;&#25968;&#23398;&#38382;&#39064;&#26102;&#30340;&#34892;&#20026;&#65292;&#24182;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#23427;&#20204;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19981;&#21512;&#29702;&#25968;&#23398;&#38382;&#39064;(UMP)&#22522;&#20934;&#26469;&#26816;&#26597;LLMs&#30340;&#38169;&#35823;&#26816;&#27979;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#33021;&#22815;&#26816;&#27979;&#21040;&#19981;&#21512;&#29702;&#38169;&#35823;&#65292;&#20294;&#20173;&#28982;&#22312;&#29983;&#25104;&#38750;&#24187;&#35273;&#20869;&#23481;&#26041;&#38754;&#22833;&#36133;&#12290;&#20026;&#20102;&#25913;&#21892;&#23427;&#20204;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;&#20851;&#38190;&#35745;&#31639;&#21644;&#32467;&#35770;(CCC)&#30340;&#25112;&#30053;&#25552;&#31034;&#27169;&#26495;&#12290;&#36890;&#36807;CCC&#65292;LLMs&#21487;&#20197;&#26356;&#22909;&#22320;&#33258;&#25105;&#35780;&#20272;&#24182;&#26816;&#27979;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#19981;&#21512;&#29702;&#38169;&#35823;&#65292;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#26356;&#21487;&#38752;&#21644;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19346v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate substantial capabilities in solving math problems. However, they tend to produce hallucinations when given questions containing unreasonable errors. In this paper, we study the behavior of LLMs when faced with unreasonable math problems and further explore their potential to address these problems. First, we construct the Unreasonable Math Problem (UMP) benchmark to examine the error detection ability of LLMs. Experiments show that LLMs are able to detect unreasonable errors, but still fail in generating non-hallucinatory content. In order to improve their ability of error detection and correction, we further design a strategic prompt template called Critical Calculation and Conclusion(CCC). With CCC, LLMs can better self-evaluate and detect unreasonable errors in math questions, making them more reliable and safe in practical application scenarios.
&lt;/p&gt;</description></item><item><title>Dataverse&#26159;&#19968;&#20010;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;ETL&#31649;&#36947;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#35774;&#35745;&#21644;&#26131;&#20110;&#23450;&#21046;&#30340;&#22788;&#29702;&#22120;&#28155;&#21152;&#21151;&#33021;&#65292;&#26088;&#22312;&#25104;&#20026;LLM&#24320;&#21457;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#24182;&#24320;&#28304;&#25972;&#20010;&#24211;&#20197;&#20419;&#36827;&#31038;&#21306;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.19340</link><description>&lt;p&gt;
Dataverse&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;ETL&#65288;&#25277;&#21462;&#12289;&#36716;&#25442;&#12289;&#21152;&#36733;&#65289;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19340
&lt;/p&gt;
&lt;p&gt;
Dataverse&#26159;&#19968;&#20010;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;ETL&#31649;&#36947;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#35774;&#35745;&#21644;&#26131;&#20110;&#23450;&#21046;&#30340;&#22788;&#29702;&#22120;&#28155;&#21152;&#21151;&#33021;&#65292;&#26088;&#22312;&#25104;&#20026;LLM&#24320;&#21457;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#24182;&#24320;&#28304;&#25972;&#20010;&#24211;&#20197;&#20419;&#36827;&#31038;&#21306;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#35268;&#27169;&#21270;&#25968;&#25454;&#22788;&#29702;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Dataverse&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24320;&#28304;&#25277;&#21462;-&#36716;&#25442;-&#21152;&#36733;&#65288;ETL&#65289;&#31649;&#36947;&#65292;&#20854;&#26680;&#24515;&#20855;&#26377;&#29992;&#25143;&#21451;&#22909;&#30340;&#35774;&#35745;&#12290;&#22312;Dataverse&#20013;&#65292;&#36890;&#36807;&#22522;&#20110;&#22359;&#30340;&#30028;&#38754;&#36731;&#26494;&#28155;&#21152;&#33258;&#23450;&#20041;&#22788;&#29702;&#22120;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26041;&#20415;&#39640;&#25928;&#22320;&#20351;&#29992;Dataverse&#26500;&#24314;&#33258;&#24049;&#30340;ETL&#31649;&#36947;&#12290;&#25105;&#20204;&#24076;&#26395;Dataverse&#23558;&#25104;&#20026;LLM&#24320;&#21457;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#24182;&#24320;&#25918;&#25972;&#20010;&#24211;&#20197;&#27426;&#36814;&#31038;&#21306;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#27905;&#30340;&#12289;&#20004;&#20998;&#38047;&#30340;&#31995;&#32479;&#28436;&#31034;&#35270;&#39057;&#65292;&#23637;&#31034;&#20854;&#21151;&#33021;&#21644;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19340v1 Announce Type: cross  Abstract: To address the challenges associated with data processing at scale, we propose Dataverse, a unified open-source Extract-Transform-Load (ETL) pipeline for large language models (LLMs) with a user-friendly design at its core. Easy addition of custom processors with block-based interface in Dataverse allows users to readily and efficiently use Dataverse to build their own ETL pipeline. We hope that Dataverse will serve as a vital tool for LLM development and open source the entire library to welcome community contribution. Additionally, we provide a concise, two-minute video demonstration of our system, illustrating its capabilities and implementation.
&lt;/p&gt;</description></item><item><title>KazSAnDRA&#26159;&#21704;&#33832;&#20811;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;&#26368;&#22823;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21253;&#25324;&#24320;&#21457;&#21644;&#35780;&#20272;&#22235;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#26497;&#24615;&#20998;&#31867;&#21644;&#24471;&#20998;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#21151;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19335</link><description>&lt;p&gt;
KazSAnDRA&#65306;&#21704;&#33832;&#20811;&#24773;&#24863;&#20998;&#26512;&#35780;&#35770;&#21644;&#24577;&#24230;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
KazSAnDRA: Kazakh Sentiment Analysis Dataset of Reviews and Attitudes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19335
&lt;/p&gt;
&lt;p&gt;
KazSAnDRA&#26159;&#21704;&#33832;&#20811;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;&#26368;&#22823;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21253;&#25324;&#24320;&#21457;&#21644;&#35780;&#20272;&#22235;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#26497;&#24615;&#20998;&#31867;&#21644;&#24471;&#20998;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#21151;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;KazSAnDRA&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#21704;&#33832;&#20811;&#24773;&#24863;&#20998;&#26512;&#24320;&#21457;&#30340;&#25968;&#25454;&#38598;&#65292;&#26159;&#31532;&#19968;&#20010;&#20063;&#26159;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#12290;KazSAnDRA&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;18&#19975;&#38646;64&#26465;&#35780;&#35770;&#30340;&#24191;&#27867;&#25910;&#38598;&#65292;&#24182;&#21253;&#25324;&#20174;1&#21040;5&#30340;&#25968;&#23383;&#35780;&#20998;&#65292;&#25552;&#20379;&#20102;&#23458;&#25143;&#24577;&#24230;&#30340;&#23450;&#37327;&#34920;&#31034;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#24320;&#21457;&#21644;&#35780;&#20272;&#22235;&#20010;&#29992;&#20110;&#26497;&#24615;&#20998;&#31867;&#21644;&#24471;&#20998;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33268;&#21147;&#20110;&#21704;&#33832;&#20811;&#24773;&#24863;&#20998;&#31867;&#30340;&#33258;&#21160;&#21270;&#12290;&#23454;&#39564;&#20998;&#26512;&#21253;&#25324;&#32771;&#34385;&#24179;&#34913;&#21644;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#35780;&#20272;&#12290;&#26368;&#25104;&#21151;&#30340;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#26497;&#24615;&#20998;&#31867;&#21644;&#24471;&#20998;&#20998;&#31867;&#30340;F1&#20998;&#21035;&#36798;&#21040;&#20102;0.81&#21644;0.39&#12290;&#25968;&#25454;&#38598;&#21644;&#20248;&#21270;&#27169;&#22411;&#26159;&#24320;&#25918;&#33719;&#21462;&#30340;&#65292;&#21487;&#22312;&#30693;&#35782;&#20849;&#20139;&#32626;&#21517;4.0&#22269;&#38469;&#35768;&#21487;&#19979;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19335v1 Announce Type: new  Abstract: This paper presents KazSAnDRA, a dataset developed for Kazakh sentiment analysis that is the first and largest publicly available dataset of its kind. KazSAnDRA comprises an extensive collection of 180,064 reviews obtained from various sources and includes numerical ratings ranging from 1 to 5, providing a quantitative representation of customer attitudes. The study also pursued the automation of Kazakh sentiment classification through the development and evaluation of four machine learning models trained for both polarity classification and score classification. Experimental analysis included evaluation of the results considering both balanced and imbalanced scenarios. The most successful model attained an F1-score of 0.81 for polarity classification and 0.39 for score classification on the test sets. The dataset and fine-tuned models are open access and available for download under the Creative Commons Attribution 4.0 International Lic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#25512;&#29702;&#22522;&#30784;&#30340;&#26032;&#26694;&#26550;P2G&#65292;&#36890;&#36807;&#21033;&#29992;MLLMs&#30340;&#24037;&#20855;&#20351;&#29992;&#28508;&#21147;&#21644;&#19987;&#23478;&#20195;&#29702;&#23454;&#29616;&#23545;&#22270;&#20687;&#20851;&#38190;&#35270;&#35273;&#21644;&#25991;&#26412;&#23545;&#35937;&#30340;&#21363;&#26102;&#30830;&#23450;&#24615;&#22522;&#30784;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#24847;&#35782;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.19322</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25512;&#29702;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#25512;&#29702;&#22522;&#30784;&#30340;&#26032;&#26694;&#26550;P2G&#65292;&#36890;&#36807;&#21033;&#29992;MLLMs&#30340;&#24037;&#20855;&#20351;&#29992;&#28508;&#21147;&#21644;&#19987;&#23478;&#20195;&#29702;&#23454;&#29616;&#23545;&#22270;&#20687;&#20851;&#38190;&#35270;&#35273;&#21644;&#25991;&#26412;&#23545;&#35937;&#30340;&#21363;&#26102;&#30830;&#23450;&#24615;&#22522;&#30784;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#24847;&#35782;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#30001;&#20110;&#20854;&#22312;&#25351;&#20196;&#36981;&#24490;&#21644;&#25512;&#29702;&#26041;&#38754;&#31361;&#20986;&#30340;&#26032;&#21151;&#33021;&#65292;&#36825;&#20123;&#27169;&#22411;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#35270;&#35273;&#25512;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#21463;&#21040;&#20854;&#38750;&#26080;&#25439;&#22270;&#20687;&#26631;&#35760;&#21270;&#30340;&#38480;&#21046;&#65292;&#22823;&#22810;&#25968;MLLMs&#22312;&#20840;&#38754;&#25429;&#25417;&#25991;&#26412;&#21644;&#23545;&#35937;&#32454;&#33410;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;P2G&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;MLLMs&#20013;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#25512;&#29702;&#22522;&#30784;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;P2G&#21033;&#29992;MLLMs&#30340;&#24037;&#20855;&#20351;&#29992;&#28508;&#21147;&#65292;&#21033;&#29992;&#19987;&#23478;&#20195;&#29702;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#30340;&#20851;&#38190;&#35270;&#35273;&#21644;&#25991;&#26412;&#23545;&#35937;&#30340;&#21363;&#26102;&#30830;&#23450;&#24615;&#22522;&#30784;&#65292;&#20174;&#32780;&#36890;&#36807;&#22810;&#27169;&#24335;&#25552;&#31034;&#23454;&#29616;&#26377;&#24847;&#35782;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21019;&#24314;&#20102;P2GB&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;MLLMs&#22312;&#29702;&#35299;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#30340;&#29289;&#20307;&#38388;&#20851;&#31995;&#21644;&#25991;&#26412;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#23545;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#30340;&#20840;&#38754;&#23454;&#39564;&#34920;&#26126;&#20102;P2G&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19322v1 Announce Type: cross  Abstract: The surge of Multimodal Large Language Models (MLLMs), given their prominent emergent capabilities in instruction following and reasoning, has greatly advanced the field of visual reasoning. However, constrained by their non-lossless image tokenization, most MLLMs fall short of comprehensively capturing details of text and objects, especially in high-resolution images. To address this, we propose P2G, a novel framework for plug-and-play grounding of reasoning in MLLMs. Specifically, P2G exploits the tool-usage potential of MLLMs to employ expert agents to achieve on-the-fly grounding to critical visual and textual objects of image, thus achieving deliberate reasoning via multimodal prompting. We further create P2GB, a benchmark aimed at assessing MLLMs' ability to understand inter-object relationships and text in challenging high-resolution images. Comprehensive experiments on visual reasoning tasks demonstrate the superiority of P2G. 
&lt;/p&gt;</description></item><item><title>TableLLM&#26159;&#19968;&#20010;&#25317;&#26377;130&#20159;&#21442;&#25968;&#30340;&#24378;&#22823;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#29087;&#32451;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#25805;&#20316;&#20219;&#21153;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#21644;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#65292;TableLLM&#30456;&#23545;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#36890;&#29992;&#21644;&#34920;&#26684;&#25968;&#25454;&#19987;&#27880;&#30340;LLMs&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.19318</link><description>&lt;p&gt;
TableLLM&#65306;&#22312;&#23454;&#38469;&#21150;&#20844;&#20351;&#29992;&#22330;&#26223;&#20013;&#23454;&#29616;LLMs&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19318
&lt;/p&gt;
&lt;p&gt;
TableLLM&#26159;&#19968;&#20010;&#25317;&#26377;130&#20159;&#21442;&#25968;&#30340;&#24378;&#22823;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#29087;&#32451;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#25805;&#20316;&#20219;&#21153;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#21644;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#65292;TableLLM&#30456;&#23545;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#36890;&#29992;&#21644;&#34920;&#26684;&#25968;&#25454;&#19987;&#27880;&#30340;LLMs&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TableLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#25317;&#26377;130&#20159;&#21442;&#25968;&#30340;&#24378;&#22823;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#19987;&#38376;&#29992;&#20110;&#29087;&#32451;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#25805;&#20316;&#20219;&#21153;&#65292;&#26080;&#35770;&#20854;&#23884;&#20837;&#22312;&#25991;&#26723;&#36824;&#26159;&#30005;&#23376;&#34920;&#26684;&#20013;&#65292;&#20197;&#28385;&#36275;&#30495;&#23454;&#21150;&#20844;&#22330;&#26223;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#25512;&#29702;&#36807;&#31243;&#25193;&#23637;&#31574;&#30053;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;LLMs&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#25512;&#29702;&#27169;&#24335;&#65292;&#20197;&#21450;&#19968;&#31181;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#65292;&#30830;&#20445;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35780;&#20272;TableLLM&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#25991;&#26723;&#21644;&#30005;&#23376;&#34920;&#26684;&#26684;&#24335;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#20004;&#31181;&#22330;&#26223;&#30340;&#32452;&#32455;&#33391;&#22909;&#30340;&#35780;&#20272;&#31649;&#32447;&#12290;&#24443;&#24213;&#30340;&#35780;&#20272;&#20984;&#26174;&#20102;TableLLM&#30456;&#23545;&#20110;&#21508;&#31181;&#29616;&#26377;&#36890;&#29992;&#21644;&#19987;&#27880;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;LLMs&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#24050;&#20844;&#24320;&#21457;&#24067;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19318v1 Announce Type: new  Abstract: We introduce TableLLM, a robust large language model (LLM) with 13 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted a benchmark tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model check
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#27861;&#24459;&#26696;&#20363;&#25688;&#35201;&#27169;&#22411;&#30340;&#36328;&#36758;&#21306;&#26222;&#36866;&#24615;&#65292;&#35843;&#26597;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#24635;&#32467;&#30446;&#26631;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;&#27861;&#24459;&#26696;&#20363;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#22312;&#25552;&#39640;&#36716;&#31227;&#24615;&#33021;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19317</link><description>&lt;p&gt;
&#36229;&#36234;&#36793;&#30028;&#65306;&#25506;&#31350;&#27861;&#24459;&#26696;&#20363;&#25688;&#35201;&#22312;&#36328;&#36758;&#21306;&#36716;&#31227;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Beyond Borders: Investigating Cross-Jurisdiction Transfer in Legal Case Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#27861;&#24459;&#26696;&#20363;&#25688;&#35201;&#27169;&#22411;&#30340;&#36328;&#36758;&#21306;&#26222;&#36866;&#24615;&#65292;&#35843;&#26597;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#24635;&#32467;&#30446;&#26631;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;&#27861;&#24459;&#26696;&#20363;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#22312;&#25552;&#39640;&#36716;&#31227;&#24615;&#33021;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#38754;&#20020;&#31649;&#29702;&#28023;&#37327;&#20887;&#38271;&#21028;&#20915;&#30340;&#25361;&#25112;&#65292;&#33258;&#21160;&#21270;&#27861;&#24459;&#26696;&#20363;&#25688;&#35201;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21516;&#19968;&#21496;&#27861;&#31649;&#36758;&#21306;&#20869;&#35757;&#32451;&#21644;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#27861;&#24459;&#26696;&#20363;&#25688;&#35201;&#27169;&#22411;&#30340;&#36328;&#36758;&#21306;&#26222;&#36866;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#24635;&#32467;&#30446;&#26631;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;&#27861;&#24459;&#26696;&#20363;&#65292;&#22312;&#37027;&#37324;&#21442;&#32771;&#25688;&#35201;&#19981;&#21487;&#29992;&#12290;&#25105;&#20204;&#29305;&#21035;&#35843;&#26597;&#20102;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#21496;&#27861;&#31649;&#36758;&#21306;&#35821;&#26009;&#24211;&#21644;&#36890;&#36807;&#23545;&#30446;&#26631;&#25968;&#25454;&#20351;&#29992;&#26080;&#30417;&#30563;&#31639;&#27861;&#33719;&#24471;&#30340;&#25277;&#21462;&#24335;&#38134;&#26631;&#25688;&#35201;&#26469;&#34917;&#20805;&#27169;&#22411;&#26159;&#21542;&#22686;&#24378;&#20102;&#36716;&#31227;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#26469;&#33258;&#19981;&#21516;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#25152;&#36827;&#34892;&#30340;&#32508;&#21512;&#30740;&#31350;&#31361;&#26174;&#20102;&#39044;&#35757;&#32451;&#22312;&#25552;&#39640;&#36716;&#31227;&#24615;&#33021;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#36873;&#25321;&#26368;&#20339;&#26469;&#28304;&#26102;&#65292;&#21496;&#27861;&#30456;&#20284;&#24615;&#22312;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19317v1 Announce Type: new  Abstract: Legal professionals face the challenge of managing an overwhelming volume of lengthy judgments, making automated legal case summarization crucial. However, prior approaches mainly focused on training and evaluating these models within the same jurisdiction. In this study, we explore the cross-jurisdictional generalizability of legal case summarization models.Specifically, we explore how to effectively summarize legal cases of a target jurisdiction where reference summaries are not available. In particular, we investigate whether supplementing models with unlabeled target jurisdiction corpus and extractive silver summaries obtained from unsupervised algorithms on target data enhances transfer performance. Our comprehensive study on three datasets from different jurisdictions highlights the role of pre-training in improving transfer performance. We shed light on the pivotal influence of jurisdictional similarity in selecting optimal source
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MATEval&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#31867;GPT-4&#30340;LLMs&#20316;&#20026;&#35780;&#20272;Agent&#65292;&#27169;&#25311;&#20154;&#31867;&#21512;&#20316;&#35752;&#35770;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#24320;&#25918;&#24615;&#25991;&#26412;&#65292;&#32467;&#21512;&#33258;&#25105;&#21453;&#24605;&#21644;&#24605;&#32500;&#38142;&#31574;&#30053;&#65292;&#24182;&#21152;&#20837;&#21453;&#39304;&#26426;&#21046;&#65292;&#25552;&#21319;&#35780;&#20272;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.19305</link><description>&lt;p&gt;
MATEval&#65306;&#29992;&#20110;&#25512;&#36827;&#24320;&#25918;&#24615;&#25991;&#26412;&#35780;&#20272;&#30340;&#22810;Agent&#35752;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19305
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MATEval&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#31867;GPT-4&#30340;LLMs&#20316;&#20026;&#35780;&#20272;Agent&#65292;&#27169;&#25311;&#20154;&#31867;&#21512;&#20316;&#35752;&#35770;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#24320;&#25918;&#24615;&#25991;&#26412;&#65292;&#32467;&#21512;&#33258;&#25105;&#21453;&#24605;&#21644;&#24605;&#32500;&#38142;&#31574;&#30053;&#65292;&#24182;&#21152;&#20837;&#21453;&#39304;&#26426;&#21046;&#65292;&#25552;&#21319;&#35780;&#20272;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20196;&#20154;&#30633;&#30446;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#32463;&#24120;&#26292;&#38706;&#20986;&#25345;&#32493;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#24320;&#25918;&#24615;&#25991;&#26412;&#20013;&#65292;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#21487;&#33021;&#24615;&#12290;&#34429;&#28982;&#20351;&#29992;&#21333;&#20010;LLM&#20316;&#20026;&#35780;&#20272;Agent&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#21364;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MATEval&#65306;&#19968;&#31181;&#8220;&#22810;Agent&#25991;&#26412;&#35780;&#20272;&#26694;&#26550;&#8221;&#65292;&#20854;&#20013;&#25152;&#26377;Agent&#37117;&#30001;&#20687;GPT-4&#30340;LLMs&#25198;&#28436;&#12290;MATEval&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#21327;&#20316;&#35752;&#35770;&#26041;&#27861;&#65292;&#25972;&#21512;&#22810;&#20010;Agent&#30340;&#20114;&#21160;&#26469;&#35780;&#20272;&#24320;&#25918;&#24615;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#33258;&#25105;&#21453;&#24605;&#21644;&#8220;&#24605;&#32500;&#38142;&#8221;&#31574;&#30053;&#65292;&#20197;&#21450;&#21453;&#39304;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#35780;&#20272;&#30340;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19305v1 Announce Type: cross  Abstract: Recent advancements in generative Large Language Models(LLMs) have been remarkable, however, the quality of the text generated by these models often reveals persistent issues. Evaluating the quality of text generated by these models, especially in open-ended text, has consistently presented a significant challenge. Addressing this, recent work has explored the possibility of using LLMs as evaluators. While using a single LLM as an evaluation agent shows potential, it is filled with significant uncertainty and instability. To address these issues, we propose the MATEval: A "Multi-Agent Text Evaluation framework" where all agents are played by LLMs like GPT-4. The MATEval framework emulates human collaborative discussion methods, integrating multiple agents' interactions to evaluate open-ended text. Our framework incorporates self-reflection and Chain-of-Thought (CoT) strategies, along with feedback mechanisms, enhancing the depth and br
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21477;&#27861;&#30340;&#26426;&#22120;&#32763;&#35793;&#19978;&#19979;&#25991;&#20363;&#21477;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20381;&#23384;&#26641;&#20043;&#38388;&#30340;&#21477;&#27861;&#30456;&#20284;&#24615;&#65292;&#32467;&#21512;&#35789;&#32423;&#21644;&#21477;&#27861;&#27700;&#24179;&#26631;&#20934;&#36873;&#25321;&#20363;&#21477;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35821;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#26426;&#22120;&#32763;&#35793;&#19978;&#19979;&#25991;&#23398;&#20064;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.19285</link><description>&lt;p&gt;
&#36229;&#36234;&#35789;&#35821;&#21305;&#37197;&#65306;&#21477;&#27861;&#25913;&#21892;&#19978;&#19979;&#25991;&#20363;&#21477;&#36873;&#25321;&#20197;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21477;&#27861;&#30340;&#26426;&#22120;&#32763;&#35793;&#19978;&#19979;&#25991;&#20363;&#21477;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20381;&#23384;&#26641;&#20043;&#38388;&#30340;&#21477;&#27861;&#30456;&#20284;&#24615;&#65292;&#32467;&#21512;&#35789;&#32423;&#21644;&#21477;&#27861;&#27700;&#24179;&#26631;&#20934;&#36873;&#25321;&#20363;&#21477;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35821;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#26426;&#22120;&#32763;&#35793;&#19978;&#19979;&#25991;&#23398;&#20064;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19285v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26102;&#20195;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#20854;&#20013;&#23637;&#31034;&#20102;&#19968;&#20123;&#31034;&#20363;&#20197;&#21796;&#36215;LLMs&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#22914;&#20309;&#36873;&#25321;&#20449;&#24687;&#37327;&#22823;&#30340;&#20363;&#21477;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#20851;&#20110;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#30340;&#19978;&#19979;&#25991;&#20363;&#21477;&#36873;&#25321;&#30340;&#20316;&#21697;&#20391;&#37325;&#20110;&#34920;&#38754;&#30340;&#35789;&#32423;&#29305;&#24449;&#65292;&#32780;&#24573;&#30053;&#20102;&#28145;&#23618;&#27425;&#30340;&#21477;&#27861;&#23618;&#27425;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#30340;&#26426;&#22120;&#32763;&#35793;&#19978;&#19979;&#25991;&#20363;&#21477;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#36317;&#31163;&#35745;&#31639;&#20381;&#36182;&#26641;&#20043;&#38388;&#30340;&#21477;&#27861;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#31574;&#30053;&#65292;&#23558;&#36890;&#36807;&#35789;&#32423;&#21644;&#21477;&#27861;&#27700;&#24179;&#26631;&#20934;&#36873;&#25321;&#30340;&#20363;&#21477;&#36827;&#34892;&#32452;&#21512;&#12290;&#23545;&#33521;&#35821;&#21644;6&#31181;&#24120;&#35265;&#35821;&#35328;&#20043;&#38388;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;MT&#30340;ICL&#65292;&#33719;&#24471;&#20102;&#22312;12&#20010;&#32763;&#35793;&#26041;&#21521;&#20013;11&#20010;&#26041;&#21521;&#19978;&#26368;&#39640;&#30340;COMET&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19285v1 Announce Type: new  Abstract: In-context learning (ICL) is the trending prompting strategy in the era of large language models (LLMs), where a few examples are demonstrated to evoke LLMs' power for a given task. How to select informative examples remains an open issue. Previous works on in-context example selection for machine translation (MT) focus on superficial word-level features while ignoring deep syntax-level knowledge. In this paper, we propose a syntax-based in-context example selection method for MT, by computing the syntactic similarity between dependency trees using Polynomial Distance. In addition, we propose an ensemble strategy combining examples selected by both word-level and syntax-level criteria. Experimental results between English and 6 common languages indicate that syntax can effectively enhancing ICL for MT, obtaining the highest COMET scores on 11 out of 12 translation directions.
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21512;&#35821;&#27861;&#21477;&#27861;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#22522;&#20934;&#33521;&#35821;GEC&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#24120;&#29992;&#26041;&#27861;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.19283</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#21512;&#35821;&#27861;&#21477;&#27861;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19283
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21512;&#35821;&#27861;&#21477;&#27861;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#22522;&#20934;&#33521;&#35821;GEC&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#24120;&#29992;&#26041;&#27861;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26102;&#20195;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25506;&#32034;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;LLMs&#24212;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;(GEC)&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#19981;&#21512;&#35821;&#27861;&#21477;&#27861;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#31574;&#30053;&#29992;&#20110;GEC&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#21508;&#31181;&#31639;&#27861;&#27979;&#37327;&#21477;&#23376;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#35782;&#21035;&#20986;&#19982;&#27979;&#35797;&#36755;&#20837;&#20855;&#26377;&#26368;&#30456;&#20284;&#19981;&#21512;&#26684;&#24335;&#21477;&#27861;&#30340;&#26368;&#20339;ICL&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#27969;&#31243;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#36873;&#25321;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#22312;&#22522;&#20934;&#33521;&#35821;GEC&#25968;&#25454;&#38598;&#19978;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#19981;&#21512;&#35821;&#27861;&#21477;&#27861;&#30340;&#31574;&#30053;&#27604;&#24120;&#29992;&#30340;&#22522;&#20110;&#21333;&#35789;&#21305;&#37197;&#25110;&#35821;&#20041;&#30340;&#26041;&#27861;&#19982;&#22810;&#20010;LLMs&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#34920;&#26126;&#23545;&#20110;&#20687;GEC&#36825;&#26679;&#30340;&#21477;&#27861;&#23450;&#21521;&#20219;&#21153;&#65292;&#26356;&#22810;&#20851;&#27880;&#35821;&#27861;&#26159;&#20540;&#24471;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19283v1 Announce Type: new  Abstract: In the era of large language models (LLMs), in-context learning (ICL) stands out as an effective prompting strategy that explores LLMs' potency across various tasks. However, applying LLMs to grammatical error correction (GEC) is still a challenging task. In this paper, we propose a novel ungrammatical-syntax-based in-context example selection strategy for GEC. Specifically, we measure similarity of sentences based on their syntactic structures with diverse algorithms, and identify optimal ICL examples sharing the most similar ill-formed syntax to the test input. Additionally, we carry out a two-stage process to further improve the quality of selection results. On benchmark English GEC datasets, empirical results show that our proposed ungrammatical-syntax-based strategies outperform commonly-used word-matching or semantics-based methods with multiple LLMs. This indicates that for a syntax-oriented task like GEC, paying more attention to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#31574;&#30053;&#19978;&#30340;&#22870;&#21169;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#31574;&#30053;&#26679;&#26412;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#20998;&#24067;&#19978;&#30340;&#19968;&#33268;&#24615;</title><link>https://arxiv.org/abs/2403.19279</link><description>&lt;p&gt;
&#20351;&#29992;&#22870;&#21169;&#23398;&#20064;&#22312;&#31574;&#30053;&#19978;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Reward Learning on Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19279
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#31574;&#30053;&#19978;&#30340;&#22870;&#21169;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#31574;&#30053;&#26679;&#26412;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#20998;&#24067;&#19978;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#20986;&#29616;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;RLHF&#21253;&#21547;&#19977;&#20010;&#27493;&#39588;&#65292;&#21363;&#25910;&#38598;&#20154;&#31867;&#20559;&#22909;&#12289;&#22870;&#21169;&#23398;&#20064;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#36890;&#24120;&#26159;&#20018;&#34892;&#25191;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#65288;&#22266;&#23450;&#30340;&#65289;&#22870;&#21169;&#27169;&#22411;&#21487;&#33021;&#20250;&#22240;&#20026;&#31574;&#30053;&#20248;&#21270;&#19981;&#26029;&#25913;&#21464;LLMs&#30340;&#25968;&#25454;&#20998;&#24067;&#32780;&#36973;&#21463;&#19981;&#20934;&#30830;&#30340;&#31163;&#20998;&#24067;&#24773;&#20917;&#12290;&#20174;&#26368;&#26032;&#30340;LLMs&#37325;&#22797;&#25910;&#38598;&#26032;&#30340;&#20559;&#22909;&#25968;&#25454;&#21487;&#33021;&#20250;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20250;&#20351;&#24471;&#32467;&#26524;&#31995;&#32479;&#26356;&#21152;&#22797;&#26434;&#21644;&#38590;&#20197;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#31574;&#30053;&#19978;&#30340;&#22870;&#21169;&#23398;&#20064;&#65288;RLP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#31574;&#30053;&#26679;&#26412;&#26469;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#20998;&#24067;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#31574;&#30053;&#26679;&#26412;&#30340;&#31283;&#20581;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19279v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences. RLHF contains three steps, i.e., human preference collecting, reward learning, and policy optimization, which are usually performed serially. Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution. Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize. In this paper, we propose reward learning on policy (RLP), an unsupervised framework that refines a reward model using policy samples to keep it on-distribution. Specifically, an unsupervised multi-view learning method is introduced to learn robust representations of policy samples. Meanwhile, a synthetic
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20010;&#24615;&#21270;&#30693;&#35782;&#21644;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#26500;&#24314;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;&#20197;&#35299;&#20915;&#20195;&#29702;&#25317;&#26377;&#19981;&#23646;&#20110;&#20854;&#35282;&#33394;&#30340;&#30693;&#35782;&#21644;&#26080;&#27861;&#28040;&#38500;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19275</link><description>&lt;p&gt;
&#30693;&#35782;&#36793;&#30028;&#19982;&#35282;&#33394;&#21160;&#24577;&#22609;&#36896;&#26356;&#22909;&#30340;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19275
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20010;&#24615;&#21270;&#30693;&#35782;&#21644;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#26500;&#24314;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;&#20197;&#35299;&#20915;&#20195;&#29702;&#25317;&#26377;&#19981;&#23646;&#20110;&#20854;&#35282;&#33394;&#30340;&#30693;&#35782;&#21644;&#26080;&#27861;&#28040;&#38500;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20010;&#24615;&#21270;&#21644;&#25311;&#20154;&#21270;&#20195;&#29702;&#22312;&#31038;&#20132;&#32593;&#32476;&#27169;&#25311;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20316;&#21697;&#20013;&#20173;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#20195;&#29702;&#25317;&#26377;&#19981;&#23646;&#20110;&#20854;&#35282;&#33394;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#19981;&#33021;&#28040;&#38500;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#23545;&#24403;&#21069;&#34892;&#20026;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#21644;&#25311;&#20154;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#20010;&#24615;&#21270;&#30693;&#35782;&#21644;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#26500;&#24314;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;&#12290;&#23545;&#20110;&#20010;&#24615;&#21270;&#30693;&#35782;&#65292;&#25105;&#20204;&#28155;&#21152;&#22806;&#37096;&#30693;&#35782;&#28304;&#24182;&#23558;&#20854;&#19982;&#20195;&#29702;&#30340;&#35282;&#33394;&#20449;&#24687;&#21305;&#37197;&#65292;&#20174;&#32780;&#36171;&#20104;&#20195;&#29702;&#20010;&#24615;&#21270;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#23545;&#20110;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#65292;&#25105;&#20204;&#20351;&#29992;&#24403;&#21069;&#34892;&#20026;&#20449;&#24687;&#20869;&#37096;&#26816;&#32034;&#20195;&#29702;&#30340;&#35282;&#33394;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23569;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#23545;&#24403;&#21069;&#34892;&#20026;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19275v1 Announce Type: cross  Abstract: Constructing personalized and anthropomorphic agents holds significant importance in the simulation of social networks. However, there are still two key problems in existing works: the agent possesses world knowledge that does not belong to its personas, and it cannot eliminate the interference of diverse persona information on current actions, which reduces the personalization and anthropomorphism of the agent. To solve the above problems, we construct the social media agent based on personalized knowledge and dynamic persona information. For personalized knowledge, we add external knowledge sources and match them with the persona information of agents, thereby giving the agent personalized world knowledge. For dynamic persona information, we use current action information to internally retrieve the persona information of the agent, thereby reducing the interference of diverse persona information on the current action. To make the age
&lt;/p&gt;</description></item><item><title>sDPO&#26159;&#23545;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#20998;&#27493;&#21033;&#29992;&#20559;&#22909;&#25968;&#25454;&#38598;&#32780;&#38750;&#19968;&#27425;&#24615;&#20351;&#29992;&#65292;&#20419;&#36827;&#26356;&#31934;&#30830;&#23545;&#40784;&#21442;&#32771;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#24182;&#35757;&#32451;&#20986;&#24615;&#33021;&#26356;&#20248;&#30340;&#26368;&#32456;&#27169;&#22411;&#65292;&#29978;&#33267;&#32988;&#36807;&#20854;&#20182;&#20855;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#27969;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19270</link><description>&lt;p&gt;
sDPO&#65306;&#19981;&#35201;&#19968;&#27425;&#24615;&#20351;&#29992;&#24744;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
sDPO: Don't Use Your Data All at Once
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19270
&lt;/p&gt;
&lt;p&gt;
sDPO&#26159;&#23545;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#20998;&#27493;&#21033;&#29992;&#20559;&#22909;&#25968;&#25454;&#38598;&#32780;&#38750;&#19968;&#27425;&#24615;&#20351;&#29992;&#65292;&#20419;&#36827;&#26356;&#31934;&#30830;&#23545;&#40784;&#21442;&#32771;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#24182;&#35757;&#32451;&#20986;&#24615;&#33021;&#26356;&#20248;&#30340;&#26368;&#32456;&#27169;&#22411;&#65292;&#29978;&#33267;&#32988;&#36807;&#20854;&#20182;&#20855;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#27969;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#23558;&#23427;&#20204;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#27493;DPO&#65288;sDPO&#65289;&#65292;&#36825;&#26159;&#23545;&#26368;&#36817;&#27969;&#34892;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#36827;&#34892;&#35843;&#25972;&#30340;&#19968;&#20010;&#25193;&#23637;&#12290;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#23558;&#21487;&#29992;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#20998;&#21106;&#65292;&#24182;&#20197;&#20998;&#27493;&#26041;&#24335;&#21033;&#29992;&#23427;&#20204;&#65292;&#32780;&#19981;&#26159;&#19968;&#27425;&#24615;&#20351;&#29992;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#20419;&#36827;&#20102;&#26356;&#31934;&#30830;&#23545;&#40784;&#21442;&#32771;&#27169;&#22411;&#22312;DPO&#35757;&#32451;&#26694;&#26550;&#20869;&#30340;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;sDPO&#35757;&#32451;&#26368;&#32456;&#27169;&#22411;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#29978;&#33267;&#32988;&#36807;&#25317;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#20854;&#20182;&#27969;&#34892;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19270v1 Announce Type: cross  Abstract: As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important. We propose stepwise DPO (sDPO), an extension of the recently popularized direct preference optimization (DPO) for alignment tuning. This approach involves dividing the available preference datasets and utilizing them in a stepwise manner, rather than employing it all at once. We demonstrate that this method facilitates the use of more precisely aligned reference models within the DPO training framework. Furthermore, sDPO trains the final model to be more performant, even outperforming other popular LLMs with more parameters.
&lt;/p&gt;</description></item><item><title>MineLand&#27169;&#25311;&#22120;&#24341;&#20837;&#26377;&#38480;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#65292;&#25903;&#25345;&#22810;&#26234;&#33021;&#20307;&#22312;&#21327;&#20316;&#20013;&#22635;&#34917;&#20102;&#20449;&#24687;&#21644;&#21151;&#33021;&#38480;&#21046;&#30340;&#31354;&#30333;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#20855;&#21160;&#24577;&#21644;&#26377;&#25928;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2403.19267</link><description>&lt;p&gt;
MineLand&#65306;&#27169;&#25311;&#20855;&#26377;&#26377;&#38480;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#30340;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19267
&lt;/p&gt;
&lt;p&gt;
MineLand&#27169;&#25311;&#22120;&#24341;&#20837;&#26377;&#38480;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#65292;&#25903;&#25345;&#22810;&#26234;&#33021;&#20307;&#22312;&#21327;&#20316;&#20013;&#22635;&#34917;&#20102;&#20449;&#24687;&#21644;&#21151;&#33021;&#38480;&#21046;&#30340;&#31354;&#30333;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#20855;&#21160;&#24577;&#21644;&#26377;&#25928;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#20223;&#30495;&#22120;&#36890;&#24120;&#20551;&#35774;&#25317;&#26377;&#23436;&#32654;&#20449;&#24687;&#21644;&#26080;&#38480;&#21151;&#33021;&#65292;&#36825;&#38480;&#21046;&#20102;&#31038;&#20250;&#20114;&#21160;&#30340;&#29983;&#24577;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;Minecraft&#27169;&#25311;&#22120;MineLand&#65292;&#36890;&#36807;&#24341;&#20837;&#26377;&#38480;&#30340;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#20223;&#30495;&#22120;&#25903;&#25345;&#26368;&#22810;48&#20010;&#20855;&#26377;&#26377;&#38480;&#35270;&#35273;&#12289;&#21548;&#35273;&#21644;&#29615;&#22659;&#24847;&#35782;&#30340;&#26234;&#33021;&#20307;&#65292;&#36843;&#20351;&#23427;&#20204;&#31215;&#26497;&#27807;&#36890;&#21644;&#21327;&#20316;&#20197;&#28385;&#36275;&#39135;&#29289;&#21644;&#36164;&#28304;&#31561;&#29983;&#29702;&#38656;&#27714;&#12290;&#36825;&#20419;&#36827;&#20102;&#21160;&#24577;&#21644;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#20010;&#28789;&#24863;&#26469;&#33258;&#22810;&#20219;&#21153;&#22788;&#29702;&#29702;&#35770;&#30340;AI&#26234;&#33021;&#20307;&#26694;&#26550;Alex&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21327;&#35843;&#21644;&#35843;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#25311;&#22120;&#12289;&#30456;&#24212;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;AI&#26234;&#33021;&#20307;&#26694;&#26550;&#26377;&#21161;&#20110;&#26356;&#20855;&#29983;&#24577;&#21644;&#32454;&#33268;&#30340;&#38598;&#20307;&#34892;&#20026;&#12290;MineLand&#21644;Alex&#30340;&#28304;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/c&#20013;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19267v1 Announce Type: cross  Abstract: Conventional multi-agent simulators often assume perfect information and limitless capabilities, hindering the ecological validity of social interactions. We propose a multi-agent Minecraft simulator, MineLand, that bridges this gap by introducing limited multimodal senses and physical needs. Our simulator supports up to 48 agents with limited visual, auditory, and environmental awareness, forcing them to actively communicate and collaborate to fulfill physical needs like food and resources. This fosters dynamic and valid multi-agent interactions. We further introduce an AI agent framework, Alex, inspired by multitasking theory, enabling agents to handle intricate coordination and scheduling. Our experiments demonstrate that the simulator, the corresponding benchmark, and the AI agent framework contribute to more ecological and nuanced collective behavior. The source code of MineLand and Alex is openly available at https://github.com/c
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#24341;&#20837; NaijaHate &#25968;&#25454;&#38598;&#65292;&#22312;&#23612;&#26085;&#21033;&#20122;&#25512;&#29305;&#19978;&#35780;&#20272; HSD&#65292;&#21457;&#29616;&#22312;&#20195;&#34920;&#24615;&#25968;&#25454;&#19978;&#35780;&#20272;&#30340; HSD &#24615;&#33021;&#39640;&#20272;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986; NaijaXLM-T &#27169;&#22411;&#65292;&#31361;&#20986;&#20102;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22312;&#26368;&#22823;&#21270; HSD &#24615;&#33021;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;</title><link>https://arxiv.org/abs/2403.19260</link><description>&lt;p&gt;
NaijaHate: &#20351;&#29992;&#20195;&#34920;&#24615;&#25968;&#25454;&#35780;&#20272;&#23612;&#26085;&#21033;&#20122; Twitter &#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19260
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#24341;&#20837; NaijaHate &#25968;&#25454;&#38598;&#65292;&#22312;&#23612;&#26085;&#21033;&#20122;&#25512;&#29305;&#19978;&#35780;&#20272; HSD&#65292;&#21457;&#29616;&#22312;&#20195;&#34920;&#24615;&#25968;&#25454;&#19978;&#35780;&#20272;&#30340; HSD &#24615;&#33021;&#39640;&#20272;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986; NaijaXLM-T &#27169;&#22411;&#65292;&#31361;&#20986;&#20102;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22312;&#26368;&#22823;&#21270; HSD &#24615;&#33021;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22312;&#32447;&#24179;&#21488;&#19978;&#24694;&#24847;&#20869;&#23481;&#34067;&#24310;&#30340;&#20840;&#29699;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#22312;&#32654;&#22269;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#24320;&#21457;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65288;HSD&#65289;&#27169;&#22411;&#65292;&#20174;&#32780;&#26080;&#27861;&#25512;&#24191;&#21040;&#26469;&#33258;&#22823;&#22810;&#25968;&#19990;&#30028;&#30340;&#33521;&#35821;&#26041;&#35328;&#12290;&#27492;&#22806;&#65292;HSD&#27169;&#22411;&#36890;&#24120;&#22312;&#31574;&#21010;&#26679;&#26412;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#39640;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;HSD&#26631;&#27880;&#30340; NaijaHate &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#23612;&#26085;&#21033;&#20122;&#25512;&#25991;&#30340;&#20195;&#34920;&#24615;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;HSD&#22312;&#20256;&#32479;&#25991;&#29486;&#20013;&#20256;&#32479;&#20351;&#29992;&#30340;&#26377;&#20559;&#35265;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65292;&#22312;&#20195;&#34920;&#24615;&#25968;&#25454;&#19978;&#24456;&#22823;&#31243;&#24230;&#19978;&#39640;&#20272;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102; NaijaXLM-T&#65292;&#19968;&#20010;&#38024;&#23545;&#23612;&#26085;&#21033;&#20122; Twitter &#19978;&#19979;&#25991;&#37327;&#36523;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#24314;&#31435;&#20102;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22312;&#26368;&#22823;&#21270;HSD&#24615;&#33021;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;-&#26426;&#28151;&#21512;&#26041;&#27861;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19260v1 Announce Type: new  Abstract: To address the global issue of hateful content proliferating in online platforms, hate speech detection (HSD) models are typically developed on datasets collected in the United States, thereby failing to generalize to English dialects from the Majority World. Furthermore, HSD models are often evaluated on curated samples, raising concerns about overestimating model performance in real-world settings. In this work, we introduce NaijaHate, the first dataset annotated for HSD which contains a representative sample of Nigerian tweets. We demonstrate that HSD evaluated on biased datasets traditionally used in the literature largely overestimates real-world performance on representative data. We also propose NaijaXLM-T, a pretrained model tailored to the Nigerian Twitter context, and establish the key role played by domain-adaptive pretraining and finetuning in maximizing HSD performance. Finally, we show that in this context, a human-in-the-l
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#21442;&#32771;&#35299;&#26512;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#26085;&#26412;&#23545;&#35805;&#25968;&#25454;&#38598;J-CRe3&#65292;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#21442;&#32771;&#35299;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#21644;&#23545;&#35805;&#38899;&#39057;&#65292;&#20197;&#21450;&#30701;&#35821;&#21644;&#35270;&#39057;&#24103;&#20013;&#29289;&#20307;&#36793;&#30028;&#26694;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#26631;&#35760;&#12290;</title><link>https://arxiv.org/abs/2403.19259</link><description>&lt;p&gt;
J-CRe3&#65306;&#26085;&#26412;&#23545;&#35805;&#25968;&#25454;&#38598;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#21442;&#32771;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
J-CRe3: A Japanese Conversation Dataset for Real-world Reference Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19259
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#21442;&#32771;&#35299;&#26512;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#26085;&#26412;&#23545;&#35805;&#25968;&#25454;&#38598;J-CRe3&#65292;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#21442;&#32771;&#35299;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#21644;&#23545;&#35805;&#38899;&#39057;&#65292;&#20197;&#21450;&#30701;&#35821;&#21644;&#35270;&#39057;&#24103;&#20013;&#29289;&#20307;&#36793;&#30028;&#26694;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19259v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#29702;&#35299;&#25351;&#20195;&#29289;&#29702;&#19990;&#30028;&#30340;&#34920;&#36798;&#23545;&#20110;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#25191;&#34892;&#29992;&#25143;&#26399;&#26395;&#21160;&#20316;&#30340;&#26426;&#22120;&#20154;&#31561;&#21161;&#20154;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#21442;&#32771;&#35299;&#26512;&#20013;&#65292;&#31995;&#32479;&#24517;&#39035;&#23558;&#29992;&#25143;&#20132;&#20114;&#20013;&#20986;&#29616;&#30340;&#35821;&#35328;&#20449;&#24687;&#19982;&#33258;&#25105;&#20013;&#24515;&#35270;&#22270;&#20013;&#35266;&#23519;&#21040;&#30340;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#20851;&#32852;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#22810;&#27169;&#24335;&#21442;&#32771;&#35299;&#26512;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#21442;&#32771;&#35299;&#26512;&#30340;&#26085;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;&#65288;J-CRe3&#65289;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#22312;&#23478;&#20013;&#20004;&#20010;&#20154;&#25198;&#28436;&#20027;&#20154;&#21644;&#21161;&#29702;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#30495;&#23454;&#23545;&#35805;&#30340;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#21644;&#23545;&#35805;&#38899;&#39057;&#12290;&#35813;&#25968;&#25454;&#38598;&#24102;&#26377;&#35805;&#35821;&#20013;&#30701;&#35821;&#21644;&#35270;&#39057;&#24103;&#20013;&#29289;&#20307;&#36793;&#30028;&#26694;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#26631;&#35760;&#12290;&#36825;&#20123;&#26631;&#35760;&#21253;&#25324;&#38388;&#25509;&#21442;&#32771;&#20851;&#31995;&#65292;&#20363;&#22914;&#35859;&#35821;-&#35770;&#20803;&#32467;&#26500;&#21644;&#26725;&#25509;&#24341;&#29992;&#65292;&#20197;&#21450;&#30452;&#25509;&#24341;&#29992;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19259v1 Announce Type: new  Abstract: Understanding expressions that refer to the physical world is crucial for such human-assisting systems in the real world, as robots that must perform actions that are expected by users. In real-world reference resolution, a system must ground the verbal information that appears in user interactions to the visual information observed in egocentric views. To this end, we propose a multimodal reference resolution task and construct a Japanese Conversation dataset for Real-world Reference Resolution (J-CRe3). Our dataset contains egocentric video and dialogue audio of real-world conversations between two people acting as a master and an assistant robot at home. The dataset is annotated with crossmodal tags between phrases in the utterances and the object bounding boxes in the video frames. These tags include indirect reference relations, such as predicate-argument structures and bridging references as well as direct reference relations. We a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#30340;&#21327;&#20316;&#30693;&#35782;&#34701;&#20837;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#23545;&#30446;&#26631;&#32972;&#26223;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#24182;&#37319;&#29992;&#39640;&#25928;&#21442;&#25968;&#23398;&#20064;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.19219</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#31435;&#22330;&#26816;&#27979;&#30340;&#21327;&#20316;&#30693;&#35782;&#34701;&#20837;
&lt;/p&gt;
&lt;p&gt;
Collaborative Knowledge Infusion for Low-resource Stance Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19219
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#30340;&#21327;&#20316;&#30693;&#35782;&#34701;&#20837;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#23545;&#30446;&#26631;&#32972;&#26223;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#24182;&#37319;&#29992;&#39640;&#25928;&#21442;&#25968;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#26816;&#27979;&#26159;&#38024;&#23545;&#29305;&#23450;&#30446;&#26631;&#22312;&#32473;&#23450;&#35821;&#22659;&#19979;&#30340;&#35266;&#28857;&#65288;&#22914;&#25512;&#25991;&#12289;&#21830;&#19994;&#35780;&#35770;&#65289;&#12290;&#20026;&#20102;&#24110;&#21161;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#30446;&#26631;&#24182;&#20570;&#20986;&#27491;&#30830;&#21028;&#23450;&#65292;&#36890;&#24120;&#38656;&#35201;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20851;&#20110;&#30693;&#35782;&#27880;&#20837;&#30340;&#31435;&#22330;&#26816;&#27979;&#20316;&#21697;&#20027;&#35201;&#23558;&#30446;&#26631;&#30693;&#35782;&#20174;&#19968;&#20010;&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#39564;&#35777;&#30340;&#21333;&#19968;&#26469;&#28304;&#25972;&#21512;&#36827;&#26469;&#12290;&#20302;&#36164;&#28304;&#35757;&#32451;&#25968;&#25454;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#36825;&#19968;&#20219;&#21153;&#20013;&#25968;&#25454;&#39537;&#21160;&#22823;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#30340;&#21327;&#20316;&#30693;&#35782;&#34701;&#20837;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#23545;&#40784;&#30693;&#35782;&#22686;&#24378;&#21644;&#39640;&#25928;&#21442;&#25968;&#23398;&#20064;&#25216;&#26415;&#30340;&#32452;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#36890;&#36807;&#20174;&#19981;&#21516;&#30693;&#35782;&#26469;&#28304;&#21327;&#20316;&#22320;&#21033;&#29992;&#30446;&#26631;&#32972;&#26223;&#30693;&#35782;&#65292;&#20511;&#21161;&#30693;&#35782;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19219v1 Announce Type: new  Abstract: Stance detection is the view towards a specific target by a given context (\textit{e.g.} tweets, commercial reviews). Target-related knowledge is often needed to assist stance detection models in understanding the target well and making detection correctly. However, prevailing works for knowledge-infused stance detection predominantly incorporate target knowledge from a singular source that lacks knowledge verification in limited domain knowledge. The low-resource training data further increases the challenge for the data-driven large models in this task. To address those challenges, we propose a collaborative knowledge infusion approach for low-resource stance detection tasks, employing a combination of aligned knowledge enhancement and efficient parameter learning techniques. Specifically, our stance detection approach leverages target background knowledge collaboratively from different knowledge sources with the help of knowledge alig
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#19981;&#20165;&#20851;&#27880;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;</title><link>https://arxiv.org/abs/2403.19211</link><description>&lt;p&gt;
&#20026;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#25552;&#20379;&#21452;&#37325;&#20010;&#24615;&#21270;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dual-Personalizing Adapter for Federated Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19211
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#19981;&#20165;&#20851;&#27880;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24494;&#35843;&#22823;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#23637;&#29616;&#20986;&#20102;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#65288;FL&#65289;&#29615;&#22659;&#19979;&#36890;&#36807;&#21033;&#29992;&#35768;&#22810;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#36827;&#34892;&#21327;&#20316;&#24494;&#35843;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#12290;&#20026;&#20102;&#20943;&#36731;&#36890;&#20449;&#21644;&#35745;&#31639;&#24320;&#38144;&#65292;&#24341;&#20837;&#20102;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19988;&#19968;&#20123;&#30740;&#31350;&#23558;&#20010;&#24615;&#21270;&#26041;&#27861;&#35843;&#25972;&#20026;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#21475;&#26159;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#24573;&#30053;&#20102;&#27979;&#35797;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#23427;&#19981;&#20165;&#19987;&#27880;&#20110;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19211v1 Announce Type: cross  Abstract: Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning large amounts of instruction data. Notably, federated foundation models emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to federated foundation models for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications. Therefore, to bridge this gap, we propose a new setting, termed test-time personalization, which not only concentrates on the targeted local task but also extends to other tasks that exhibit test-t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35821;&#20041;&#26631;&#27880;&#26723;&#26696;&#25991;&#20214;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26500;&#24314;&#26032;&#30028;&#38754;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#25506;&#32034;&#21644;&#21033;&#29992;&#25991;&#26723;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#24212;&#23545;&#24403;&#21069;&#25216;&#26415;&#38556;&#30861;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.19201</link><description>&lt;p&gt;
&#29702;&#35299;&#26723;&#26696;&#65306;&#22522;&#20110;&#25991;&#26723;&#35821;&#20041;&#26631;&#27880;&#30340;&#26032;&#30740;&#31350;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Understanding Archives: Towards New Research Interfaces Relying on the Semantic Annotation of Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19201
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#26631;&#27880;&#26723;&#26696;&#25991;&#20214;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26500;&#24314;&#26032;&#30028;&#38754;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#25506;&#32034;&#21644;&#21033;&#29992;&#25991;&#26723;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#24212;&#23545;&#24403;&#21069;&#25216;&#26415;&#38556;&#30861;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#22270;&#20070;&#39302;&#21644;&#26723;&#26696;&#39302;&#36827;&#34892;&#30340;&#25968;&#23383;&#21270;&#27963;&#21160;&#20419;&#36827;&#20102;&#23545;&#20854;&#25910;&#34255;&#25991;&#26723;&#30340;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#20379;&#26597;&#38405;&#30340;&#25991;&#26723;&#25968;&#37327;&#24222;&#22823;&#65292;&#25506;&#32034;&#21644;&#21033;&#29992;&#36825;&#20123;&#25991;&#26723;&#20173;&#28982;&#26159;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#26723;&#26696;&#25991;&#20214;&#30740;&#31350;&#35821;&#26009;&#24211;&#30340;&#25991;&#26412;&#20869;&#23481;&#36827;&#34892;&#35821;&#20041;&#26631;&#27880;&#26469;&#20419;&#36827;&#20854;&#21033;&#29992;&#21644;&#20215;&#20540;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#35821;&#20041;&#26500;&#24314;&#26032;&#30028;&#38754;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#28982;&#21518;&#35752;&#35770;&#24403;&#21069;&#30340;&#25216;&#26415;&#38556;&#30861;&#21450;&#20854;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#35813;&#26694;&#26550;&#30340;&#24212;&#29992;&#23454;&#20363;&#26469;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19201v1 Announce Type: cross  Abstract: The digitisation campaigns carried out by libraries and archives in recent years have facilitated access to documents in their collections. However, exploring and exploiting these documents remain difficult tasks due to the sheer quantity of documents available for consultation. In this article, we show how the semantic annotation of the textual content of study corpora of archival documents allow to facilitate their exploitation and valorisation. First, we present a methodological framework for the construction of new interfaces based on textual semantics, then address the current technological obstacles and their potential solutions. We conclude by presenting a practical case of the application of this framework.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#27604;&#36739;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#21518;&#22788;&#29702;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#30340;&#20381;&#23384;&#26641;&#32467;&#26500;&#32858;&#21512;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.19183</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#36890;&#29992;&#20381;&#23384;&#21477;&#27861;&#26641;&#32858;&#21512;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical Analysis for Unsupervised Universal Dependency Parse Tree Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19183
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#27604;&#36739;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#21518;&#22788;&#29702;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#30340;&#20381;&#23384;&#26641;&#32467;&#26500;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#23384;&#20998;&#26512;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#20381;&#23384;&#20998;&#26512;&#22120;&#30340;&#36136;&#37327;&#23545;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20381;&#23384;&#20998;&#26512;&#22120;&#30340;&#36136;&#37327;&#36890;&#24120;&#21462;&#20915;&#20110;&#28041;&#21450;&#30340;&#39046;&#22495;&#21644;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#35299;&#20915;&#36136;&#37327;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#20197;&#23454;&#29616;&#31283;&#23450;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#32858;&#21512;&#26041;&#27861;&#29992;&#20110;&#21518;&#22788;&#29702;&#32858;&#21512;&#65292;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#35299;&#20915;&#36136;&#37327;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20381;&#23384;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23545;&#20110;&#21518;&#22788;&#29702;&#32858;&#21512;&#30340;&#32858;&#21512;&#26041;&#27861;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#21518;&#22788;&#29702;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#26368;&#36866;&#21512;&#30340;&#20381;&#23384;&#26641;&#32467;&#26500;&#32858;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19183v1 Announce Type: new  Abstract: Dependency parsing is an essential task in NLP, and the quality of dependency parsers is crucial for many downstream tasks. Parsers' quality often varies depending on the domain and the language involved. Therefore, it is essential to combat the issue of varying quality to achieve stable performance. In various NLP tasks, aggregation methods are used for post-processing aggregation and have been shown to combat the issue of varying quality. However, aggregation methods for post-processing aggregation have not been sufficiently studied in dependency parsing tasks. In an extensive empirical study, we compare different unsupervised post-processing aggregation methods to identify the most suitable dependency tree structure aggregation method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.19181</link><description>&lt;p&gt;
&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#25490;&#21517;&#22120;
&lt;/p&gt;
&lt;p&gt;
Make Large Language Model a Better Ranker
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#26174;&#33879;&#22686;&#24378;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#25512;&#33616;&#31995;&#32479;&#65288;RSs&#65289;&#27010;&#24565;&#21644;&#24320;&#21457;&#26041;&#24335;&#21457;&#29983;&#20102;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#23545;&#28857;&#21644;&#25104;&#23545;&#25512;&#33616;&#33539;&#24335;&#19978;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#22120;&#20013;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#19968;&#20123;&#30740;&#31350;&#34429;&#28982;&#28145;&#20837;&#30740;&#31350;&#20102;&#21015;&#34920;&#22411;&#26041;&#27861;&#65292;&#20294;&#22312;&#25490;&#21517;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#19968;&#19981;&#36275;&#24402;&#22240;&#20110;&#25490;&#21517;&#21644;&#35821;&#35328;&#29983;&#25104;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#12290;ALRO&#26088;&#22312;&#24357;&#21512;LLMs&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#24494;&#22937;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;ALRO&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#24341;&#20837;&#20102;&#36719;lambda&#20540;lo
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19181v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) has significantly enhanced capabilities across various fields, leading to a paradigm shift in how Recommender Systems (RSs) are conceptualized and developed. However, existing research primarily focuses on point-wise and pair-wise recommendation paradigms. These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models. While some studies have delved into list-wise approaches, they fall short in ranking tasks. This shortfall is attributed to the misalignment between the objectives of ranking and language generation. To this end, this paper introduces the Language Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks within recommender systems. A key feature of ALRO is the introduction of soft lambda lo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#36807;&#28388;&#25512;&#29702;&#22120;&#65288;SelF-Reasoner&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#38382;&#39064;&#19982;&#20505;&#36873;&#25512;&#29702;&#38142;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#20197;&#20943;&#36731;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.19167</link><description>&lt;p&gt;
&#29992;&#36873;&#25321;&#24615;&#36807;&#28388;&#20943;&#36731;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19167
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#36807;&#28388;&#25512;&#29702;&#22120;&#65288;SelF-Reasoner&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#38382;&#39064;&#19982;&#20505;&#36873;&#25512;&#29702;&#38142;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#20197;&#20943;&#36731;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#24605;&#32500;&#38142;&#25512;&#29702;&#25216;&#26415;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#65292;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25512;&#29702;&#30340;&#25928;&#21147;&#21462;&#20915;&#20110;&#24605;&#32500;&#38142;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#36807;&#28388;&#25512;&#29702;&#22120;&#65288;SelF-Reasoner&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35780;&#20272;&#38382;&#39064;&#19982;&#20505;&#36873;&#25512;&#29702;&#38142;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#12290;&#24403;&#25512;&#29702;&#38142;&#23637;&#31034;&#20986;&#33258;&#20449;&#26102;&#65292;&#25105;&#20204;&#32487;&#32493;&#36827;&#34892;&#24605;&#32500;&#38142;&#25512;&#29702;&#65307;&#21542;&#21017;&#65292;&#25105;&#20204;&#36873;&#25321;&#30452;&#25509;&#39044;&#27979;&#31572;&#26696;&#12290;SelF-Reasoner&#22312;ScienceQA&#12289;ECQA&#21644;LastLetter&#20219;&#21153;&#19978;&#25345;&#32493;&#25913;&#21892;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;T5&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19167v1 Announce Type: cross  Abstract: Large language models have manifested remarkable capabilities by leveraging chain-of-thought (CoT) reasoning techniques to solve intricate questions through step-by-step reasoning chains. Despite its success, the efficacy of such reasoning is inherently contingent upon the quality of CoT. However, flawless CoT reasoning cannot be guaranteed due to the presence of indecomposable questions and the potential for erroneous reasoning chains, particularly in the case of small-scale language models. To tackle this challenge, we propose a novel approach called the selective filtering reasoner (SelF-Reasoner) that assesses the entailment relationship between the question and the candidate reasoning chain. Then, we proceed with CoT reasoning when the reasoning chain demonstrates confidence; otherwise, we opt to predict the answer directly. SelF-Reasoner improves the fine-tuned T5 baseline consistently over the ScienceQA, ECQA, and LastLetter tas
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;MedEV&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#30340;&#32763;&#35793;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#36827;&#34892;&#36234;&#21335;&#35821;-&#33521;&#35821;&#26426;&#22120;&#32763;&#35793;&#65292;&#32467;&#26524;&#26174;&#31034;&#24494;&#35843;"vinai-translate"&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#65292;&#21516;&#26102;&#20844;&#24320;&#25968;&#25454;&#38598;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.19161</link><description>&lt;p&gt;
&#25913;&#36827;&#36234;&#21335;&#35821;-&#33521;&#35821;&#21307;&#23398;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Improving Vietnamese-English Medical Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19161
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;MedEV&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#30340;&#32763;&#35793;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#36827;&#34892;&#36234;&#21335;&#35821;-&#33521;&#35821;&#26426;&#22120;&#32763;&#35793;&#65292;&#32467;&#26524;&#26174;&#31034;&#24494;&#35843;"vinai-translate"&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#65292;&#21516;&#26102;&#20844;&#24320;&#25968;&#25454;&#38598;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#36234;&#21335;&#35821;-&#33521;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MedEV - &#19968;&#20010;&#19987;&#20026;&#21307;&#23398;&#39046;&#22495;&#26500;&#24314;&#30340;&#39640;&#36136;&#37327;&#36234;&#21335;&#35821;-&#33521;&#35821;&#24179;&#34892;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#32422;360K&#20010;&#21477;&#23545;&#12290;&#25105;&#20204;&#23545;Google&#32763;&#35793;&#12289;ChatGPT&#65288;gpt-3.5-turbo&#65289;&#12289;&#26368;&#20808;&#36827;&#30340;&#36234;&#21335;&#35821;-&#33521;&#35821;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;&#21452;&#35821;/&#22810;&#35821;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#26032;MedEV&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#38024;&#23545;&#27599;&#20010;&#32763;&#35793;&#26041;&#21521;&#24494;&#35843;"vinai-translate"&#33021;&#21462;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20197;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19161v1 Announce Type: new  Abstract: Machine translation for Vietnamese-English in the medical domain is still an under-explored research area. In this paper, we introduce MedEV -- a high-quality Vietnamese-English parallel dataset constructed specifically for the medical domain, comprising approximately 360K sentence pairs. We conduct extensive experiments comparing Google Translate, ChatGPT (gpt-3.5-turbo), state-of-the-art Vietnamese-English neural machine translation models and pre-trained bilingual/multilingual sequence-to-sequence models on our new MedEV dataset. Experimental results show that the best performance is achieved by fine-tuning "vinai-translate" for each translation direction. We publicly release our dataset to promote further research.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#20013;&#30340;&#38271;&#24230;&#38382;&#39064;&#23637;&#24320;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;DPO&#20013;&#26174;&#33879;&#30340;&#21033;&#29992;&#24773;&#20917;&#65292;&#24182;&#23558;&#20854;&#19982;&#20998;&#24067;&#22806;&#24341;&#23548;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>https://arxiv.org/abs/2403.19159</link><description>&lt;p&gt;
&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#20013;&#23558;&#38271;&#24230;&#19982;&#36136;&#37327;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Disentangling Length from Quality in Direct Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19159
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#20013;&#30340;&#38271;&#24230;&#38382;&#39064;&#23637;&#24320;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;DPO&#20013;&#26174;&#33879;&#30340;&#21033;&#29992;&#24773;&#20917;&#65292;&#24182;&#23558;&#20854;&#19982;&#20998;&#24067;&#22806;&#24341;&#23548;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF)&#26159;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;RLHF&#34987;&#35748;&#20026;&#21033;&#29992;&#20102;&#20154;&#31867;&#20559;&#22909;&#20013;&#30340;&#20559;&#35265;&#65292;&#27604;&#22914;&#20887;&#38271;&#24615;&#12290;&#31934;&#24515;&#26684;&#24335;&#21270;&#21644;&#38596;&#36777;&#30340;&#31572;&#26696;&#36890;&#24120;&#20250;&#34987;&#29992;&#25143;&#26356;&#39640;&#35780;&#20215;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#24110;&#21161;&#24615;&#21644;&#23458;&#35266;&#24615;&#19978;&#36739;&#20302;&#12290;&#19968;&#20123;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#26469;&#25511;&#21046;&#36825;&#20123;&#20559;&#35265;&#65292;&#22312;&#21476;&#20856;RLHF&#25991;&#29486;&#20013;&#36825;&#20010;&#38382;&#39064;&#24050;&#26377;&#25152;&#25506;&#35752;&#65292;&#20294;&#23545;&#20110;&#30452;&#25509;&#23545;&#40784;&#31639;&#27861;&#22914;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#36825;&#20010;&#38382;&#39064;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#19982;&#21476;&#20856;RLHF&#19981;&#21516;&#65292;DPO&#19981;&#35757;&#32451;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#25110;&#30452;&#25509;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#22240;&#27492;&#20043;&#21069;&#29992;&#26469;&#25511;&#21046;&#20887;&#38271;&#24615;&#30340;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20570;&#20986;&#20102;&#20960;&#28857;&#36129;&#29486;&#12290;&#39318;&#27425;&#22312;DPO&#29615;&#22659;&#20013;&#30740;&#31350;&#38271;&#24230;&#38382;&#39064;&#65292;&#26174;&#31034;DPO&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#21033;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;&#20998;&#24067;&#22806;&#24341;&#23548;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19159v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has been a crucial component in the recent success of Large Language Models. However, RLHF is know to exploit biases in human preferences, such as verbosity. A well-formatted and eloquent answer is often more highly rated by users, even when it is less helpful and objective. A number of approaches have been developed to control those biases in the classical RLHF literature, but the problem remains relatively under-explored for Direct Alignment Algorithms such as Direct Preference Optimization (DPO). Unlike classical RLHF, DPO does not train a separate reward model or use reinforcement learning directly, so previous approaches developed to control verbosity cannot be directly applied to this setting. Our work makes several contributions. For the first time, we study the length problem in the DPO setting, showing significant exploitation in DPO and linking it to out-of-distribution bootstra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#38382;&#32773;&#36890;&#36807;&#35810;&#38382;&#35282;&#33394;&#25198;&#28436;&#32773;&#26469;&#24341;&#20986;&#20559;&#22909;&#65292;&#20174;&#32780;&#36845;&#20195;&#24494;&#35843;&#20197;&#22686;&#21152;&#20219;&#21153;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.19154</link><description>&lt;p&gt;
STaR-GATE: &#25945;&#25480;&#35821;&#35328;&#27169;&#22411;&#35810;&#38382;&#28548;&#28165;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
STaR-GATE: Teaching Language Models to Ask Clarifying Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19154
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#38382;&#32773;&#36890;&#36807;&#35810;&#38382;&#35282;&#33394;&#25198;&#28436;&#32773;&#26469;&#24341;&#20986;&#20559;&#22909;&#65292;&#20174;&#32780;&#36845;&#20195;&#24494;&#35843;&#20197;&#22686;&#21152;&#20219;&#21153;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#20219;&#21153;&#26102;&#65292;&#29992;&#25143;&#36890;&#24120;&#20250;&#36951;&#28431;&#37325;&#35201;&#30340;&#32454;&#33410;&#12290;&#34429;&#28982;&#25552;&#38382;&#21487;&#20197;&#35299;&#20915;&#36825;&#31181;&#27495;&#20041;&#65292;&#20294;&#27169;&#22411;&#24448;&#24448;&#24456;&#38590;&#25552;&#20986;&#22909;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22870;&#21169;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;STaR-GATE&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;25,500&#20010;&#29420;&#29305;&#20154;&#29289;-&#20219;&#21153;&#25552;&#31034;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#27169;&#25311;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;--&#25552;&#38382;&#32773;--&#19982;&#19968;&#20010;&#20854;&#20559;&#22909;&#26410;&#30693;&#30340;&#35282;&#33394;&#25198;&#28436;&#32773;&#20043;&#38388;&#30340;&#23545;&#35805;&#12290;&#36890;&#36807;&#25552;&#38382;&#65292;&#25552;&#38382;&#32773;&#20174;&#35282;&#33394;&#25198;&#28436;&#32773;&#37027;&#37324;&#24341;&#20986;&#20559;&#22909;&#12290;&#25552;&#38382;&#32773;&#22312;&#37027;&#20123;&#22686;&#21152;&#39640;&#36136;&#37327;&#21709;&#24212;&#27010;&#29575;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#36845;&#20195;&#24494;&#35843;&#65292;&#36825;&#20123;&#38382;&#39064;&#26159;&#30001;&#20855;&#26377;&#23545;&#35282;&#33394;&#25198;&#28436;&#32773;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35328;&#32773;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19154v1 Announce Type: cross  Abstract: When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity \citep[GATE;][]{li2023eliciting}, models often struggle to ask good questions. We explore a language model's ability to self-improve \citep[STaR;][]{zelikman2022star} by rewarding the model for generating useful questions -- a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model -- the \texttt{Questioner} -- and a \texttt{Roleplayer} whose preferences are unknown to the \texttt{Questioner}. By asking questions, the \texttt{Questioner} elicits preferences from the \texttt{Roleplayer}. The \texttt{Questioner} is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an \texttt{Oracle} with access to the \texttt{Ro
&lt;/p&gt;</description></item><item><title>&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#20351;&#24471;&#21363;&#20351;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#27809;&#26377;&#24179;&#34892;&#25968;&#25454;&#65292;&#20063;&#33021;&#35757;&#32451;&#20986;&#19968;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#24320;&#21457;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.19142</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#22270;&#40065;&#35821;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
A Tulu Resource for Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19142
&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#20351;&#24471;&#21363;&#20351;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#27809;&#26377;&#24179;&#34892;&#25968;&#25454;&#65292;&#20063;&#33021;&#35757;&#32451;&#20986;&#19968;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#24320;&#21457;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21576;&#29616;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#33521;&#35821;-&#22270;&#40065;&#35821;&#32763;&#35793;&#30340;&#24179;&#34892;&#25968;&#25454;&#38598;&#12290;&#22270;&#40065;&#35821;&#23646;&#20110;&#21335;&#24503;&#25289;&#32500;&#36798;&#35821;&#31995;&#65292;&#20027;&#35201;&#20998;&#24067;&#22312;&#21360;&#24230;&#35199;&#21335;&#37096;&#65292;&#32422;&#26377;250&#19975;&#20154;&#21475;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20154;&#31867;&#32763;&#35793;&#25972;&#21512;&#21040;&#22810;&#35821;&#31181;&#26426;&#22120;&#32763;&#35793;&#36164;&#28304;FLORES-200&#20013;&#26469;&#26500;&#24314;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#24320;&#21457;&#25105;&#20204;&#30340;&#33521;&#35821;-&#22270;&#40065;&#35821;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#30456;&#20851;&#21335;&#24503;&#25289;&#32500;&#36798;&#35821;&#35328;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#21363;&#20351;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#27809;&#26377;&#24179;&#34892;&#25968;&#25454;&#65292;&#20063;&#33021;&#35757;&#32451;&#20986;&#19968;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#24320;&#21457;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#33521;&#35821;-&#22270;&#40065;&#35821;&#31995;&#32479;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19142v1 Announce Type: new  Abstract: We present the first parallel dataset for English-Tulu translation. Tulu, classified within the South Dravidian linguistic family branch, is predominantly spoken by approximately 2.5 million individuals in southwestern India. Our dataset is constructed by integrating human translations into the multilingual machine translation resource FLORES-200. Furthermore, we use this dataset for evaluation purposes in developing our English-Tulu machine translation model. For the model's training, we leverage resources available for related South Dravidian languages. We adopt a transfer learning approach that exploits similarities between high-resource and low-resource languages. This method enables the training of a machine translation system even in the absence of parallel data between the source and target language, thereby overcoming a significant obstacle in machine translation development for low-resource languages. Our English-Tulu system, tr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19135</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21270;&#19981;&#37325;&#35201;&#30340;&#23618;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compressing Large Language Models by Streamlining the Unimportant Layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19135
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#20294;&#20854;&#36866;&#29992;&#24615;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20851;&#27880;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#30340;&#32039;&#20945;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#30340;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#25200;&#21160;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20986;&#19981;&#37027;&#20040;&#37325;&#35201;&#30340;&#23618;&#12290;&#22522;&#20110;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM-Streamline&#65292;&#21253;&#25324;&#20004;&#37096;&#20998;&#65306;&#23618;&#21098;&#26525;&#65292;&#26681;&#25454;&#30446;&#26631;&#31232;&#30095;&#24230;&#31227;&#38500;&#27169;&#22411;&#20013;&#19968;&#32452;&#36830;&#32493;&#30340;&#26368;&#19981;&#37325;&#35201;&#30340;&#23618;&#65307;&#23618;&#26367;&#25442;&#65292;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#26367;&#25442;&#34987;&#21098;&#26525;&#30340;&#23618;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#21098;&#26525;&#36896;&#25104;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#21644;&#19968;&#20010;transformer&#23618;&#31561;&#32467;&#26500;&#20316;&#20026;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19135v1 Announce Type: cross  Abstract: Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models. Consequently, there is an increasing emphasis on compact models that exhibit high performance. In this study, we observe that different layers in LLM have varying degrees of perturbation on the hidden states, which allows us to identify less important layers. Based on this phenomenon, we propose LLM-Streamline, which consists of two parts: layer pruning, where we remove a set of consecutive layers with the lowest importance in the model according to the target sparsity; and layer replacement, where we train a lightweight model to substitute the pruned layers, thereby mitigating the performance degradation caused by pruning. In our experiments, we utilize structures such as a multi-layer perceptron (MLP) and a transformer layer as lightweight mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#20195;&#30721;&#27604;&#36739;&#35843;&#20248;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;bug-fixing&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19121</link><description>&lt;p&gt;
&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#27604;&#36739;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Code Comparison Tuning for Code Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19121
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#20195;&#30721;&#27604;&#36739;&#35843;&#20248;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;bug-fixing&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20195;&#30721;&#27604;&#36739;&#35843;&#20248;&#65288;CCT&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35843;&#20248;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Code LLMs&#65289;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#24494;&#22937;&#30340;&#20195;&#30721;&#38169;&#35823;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#25351;&#20196;&#35843;&#20248;&#20013;&#38598;&#25104;&#20102;&#27604;&#36739;&#30340;&#27010;&#24565;&#65292;&#21253;&#25324;&#22312;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#19978;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20998;&#36776;&#20195;&#30721;&#20013;&#29978;&#33267;&#26368;&#32454;&#24494;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#27604;&#36739;&#21407;&#22987;&#20195;&#30721;&#19982;&#21253;&#21547;&#25163;&#21160;&#28155;&#21152;&#30340;&#20195;&#30721;&#38169;&#35823;&#30340;&#38169;&#35823;&#29256;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#35760;&#32423;&#21035;&#30340;&#20248;&#20808;&#25439;&#22833;&#36827;&#34892;&#35814;&#32454;&#30340;&#26631;&#35760;&#32423;&#21035;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#20195;&#30721;&#27573;&#21019;&#24314;&#26032;&#30340;&#25351;&#20196;&#35843;&#20248;&#26679;&#26412;&#65292;&#29992;&#20110;&#24207;&#21015;&#32423;&#21035;&#27604;&#36739;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#38169;&#35823;&#20462;&#22797;&#33021;&#21147;&#12290;&#22312;HumanEvalFix&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CCT&#22312;&#21508;&#31181;&#20195;&#30721;LLMs&#19978;&#30340;pass@1&#20998;&#25968;&#27604;&#25351;&#20196;&#35843;&#20248;&#39640;&#20986;&#26368;&#22810;4&#20998;&#65292;&#24182;&#19988;&#24191;&#27867;&#30340;&#20998;&#26512;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19121v1 Announce Type: new  Abstract: We present Code Comparison Tuning (CCT), a simple and effective tuning method for code large language models (Code LLMs) to better handle subtle code errors. Specifically, we integrate the concept of comparison into instruction tuning, both at the token and sequence levels, enabling the model to discern even the slightest deviations in code. To compare the original code with an erroneous version containing manually added code errors, we use token-level preference loss for detailed token-level comparisons. Additionally, we combine code segments to create a new instruction tuning sample for sequence-level comparisons, enhancing the model's bug-fixing capability. Experimental results on the HumanEvalFix benchmark show that CCT surpasses instruction tuning in pass@1 scores by up to 4 points across diverse code LLMs, and extensive analysis demonstrates the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MFORT-QA&#26041;&#27861;&#65292;&#36890;&#36807;Few-Shot Learning&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;&#22810;&#36339;&#23569;&#26679;&#26412;&#30340;&#24320;&#25918;&#24335;&#20016;&#23500;&#38382;&#31572;&#12290;</title><link>https://arxiv.org/abs/2403.19116</link><description>&lt;p&gt;
MFORT-QA: &#22810;&#36339;&#23569;&#26679;&#26412;&#24320;&#25918;&#24335;&#20016;&#23500;&#34920;&#26684;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MFORT-QA&#26041;&#27861;&#65292;&#36890;&#36807;Few-Shot Learning&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;&#22810;&#36339;&#23569;&#26679;&#26412;&#30340;&#24320;&#25918;&#24335;&#20016;&#23500;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#24555;&#33410;&#22863;&#30340;&#34892;&#19994;&#20013;&#65292;&#19987;&#19994;&#20154;&#22763;&#27599;&#22825;&#38754;&#20020;&#30528;&#24635;&#32467;&#22823;&#37327;&#25991;&#26723;&#24182;&#20174;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#24230;&#37327;&#32463;&#24120;&#38544;&#34255;&#22312;&#34920;&#26684;&#21644;/&#25110;&#20854;&#23884;&#22871;&#30340;&#36229;&#38142;&#25509;&#20013;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#34920;&#26684;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#27861;&#26469;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#34920;&#26684;QA&#35757;&#32451;&#20219;&#21153;&#24182;&#19981;&#24635;&#26159;&#33021;&#30830;&#20445;&#25552;&#21462;&#20934;&#30830;&#31572;&#26696;&#65292;&#36825;&#20123;&#20219;&#21153;&#20250;&#21521;&#38382;&#39064;&#25552;&#20379;&#19968;&#20010;&#34920;&#26684;&#21644;&#19968;&#20010;&#25110;&#22810;&#20010;&#26469;&#33258;&#40644;&#37329;&#21333;&#20803;&#26684;&#22352;&#26631;&#30340;&#31572;&#26696;&#12290;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20026;&#20351;&#29992;&#25552;&#31034;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Multi-hop Few-shot Open Rich Table QA&#65288;MFORT-QA&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19116v1 Announce Type: cross  Abstract: In today's fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested hyperlinks. To address this challenge, the approach of Table Question Answering (QA) has been developed to extract the relevant information. However, traditional Table QA training tasks that provide a table and an answer(s) from a gold cell coordinate(s) for a question may not always ensure extracting the accurate answer(s). Recent advancements in Large Language Models (LLMs) have opened up new possibilities for extracting information from tabular data using prompts. In this paper, we introduce the Multi-hop Few-shot Open Rich Table QA (MFORT-QA) approach, which consists of two major steps. The first step involves Few-Shot Learning (FSL), where relevant tables and associated contexts of hyperlinks ar
&lt;/p&gt;</description></item><item><title>EvoEval&#36890;&#36807;&#23558;&#29616;&#26377;&#22522;&#20934;&#28436;&#21270;&#20026;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31243;&#24207;&#21512;&#25104;&#22522;&#20934;&#22871;&#20214;&#65292;&#20197;&#20805;&#20998;&#35780;&#20272;LLM&#32534;&#30721;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19114</link><description>&lt;p&gt;
&#39030;&#32423;&#25490;&#34892;&#27036; = &#39030;&#32423;&#32534;&#31243;&#33021;&#21147;&#65292;&#27704;&#36828;&#21527;&#65311;EvoEval: &#36890;&#36807;LLM&#28436;&#21270;&#32534;&#30721;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19114
&lt;/p&gt;
&lt;p&gt;
EvoEval&#36890;&#36807;&#23558;&#29616;&#26377;&#22522;&#20934;&#28436;&#21270;&#20026;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31243;&#24207;&#21512;&#25104;&#22522;&#20934;&#22871;&#20214;&#65292;&#20197;&#20805;&#20998;&#35780;&#20272;LLM&#32534;&#30721;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#24050;&#25104;&#20026;&#29983;&#25104;&#20195;&#30721;&#20219;&#21153;&#30340;&#39318;&#36873;&#65292;LLM&#30340;&#35757;&#32451;&#12289;&#24320;&#21457;&#21644;&#20351;&#29992;&#38543;&#30528;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#20195;&#30721;&#30340;LLM&#30340;&#25351;&#25968;&#22686;&#38271;&#32780;&#22686;&#21152;&#12290;&#20026;&#20102;&#35780;&#20272;LLM&#22312;&#32534;&#30721;&#19978;&#30340;&#33021;&#21147;&#65292;&#23398;&#26415;&#30028;&#21644;&#34892;&#19994;&#20174;&#19994;&#32773;&#20381;&#36182;&#20110;&#27969;&#34892;&#30340;&#20154;&#24037;&#21046;&#23450;&#30340;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#22522;&#20934;&#21482;&#21253;&#21547;&#20102;&#25968;&#37327;&#21644;&#31181;&#31867;&#38750;&#24120;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#27969;&#34892;&#24230;&#21644;&#24180;&#40836;&#65292;&#35768;&#22810;&#22522;&#20934;&#23481;&#26131;&#21457;&#29983;&#25968;&#25454;&#27844;&#28431;&#65292;&#20854;&#20013;&#31034;&#20363;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#22312;&#32593;&#32476;&#19978;&#25214;&#21040;&#65292;&#22240;&#27492;&#21487;&#33021;&#20986;&#29616;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#36825;&#20123;&#38480;&#21046;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#25105;&#20204;&#35201;&#25506;&#35752;&#65306;&#29616;&#26377;&#22522;&#20934;&#30340;&#25490;&#34892;&#27036;&#34920;&#29616;&#26159;&#21542;&#21487;&#38752;&#19988;&#20840;&#38754;&#36275;&#20197;&#34913;&#37327;LLM&#30340;&#31243;&#24207;&#21512;&#25104;&#33021;&#21147;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;EvoEval--&#19968;&#20010;&#36890;&#36807;&#23558;&#29616;&#26377;&#22522;&#20934;&#28436;&#21270;&#20026;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#32780;&#21019;&#24314;&#30340;&#31243;&#24207;&#21512;&#25104;&#22522;&#20934;&#22871;&#20214;&#65292;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;LLM&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19114v1 Announce Type: cross  Abstract: LLMs have become the go-to choice for code generation tasks, with an exponential increase in the training, development, and usage of LLMs specifically for code generation. To evaluate the ability of LLMs on code, both academic and industry practitioners rely on popular handcrafted benchmarks. However, prior benchmarks contain only a very limited set of problems, both in quantity and variety. Further, due to popularity and age, many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data. Such limitations inevitably lead us to inquire: Is the leaderboard performance on existing benchmarks reliable and comprehensive enough to measure the program synthesis ability of LLMs? To address this, we introduce EvoEval -- a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#20256;&#32479;&#30340;&#25991;&#26412;&#34164;&#28085;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#23384;&#22312;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19113</link><description>&lt;p&gt;
FACTOID: &#29992;&#20110;&#24187;&#35273;&#26816;&#27979;&#30340;&#20107;&#23454;&#25512;&#29702;&#65288;FACTOID: FACtual enTailment fOr hallucInation Detection&#65289;
&lt;/p&gt;
&lt;p&gt;
FACTOID: FACtual enTailment fOr hallucInation Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#20256;&#32479;&#30340;&#25991;&#26412;&#34164;&#28085;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#23384;&#22312;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#65292;&#20294;&#24187;&#35273;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#20316;&#20026;&#19968;&#31181;&#39640;&#24230;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#20986;&#29616;&#65292;&#36890;&#36807;&#22312;&#20107;&#23454;&#20449;&#24687;&#20013;&#25509;&#22320;&#26469;&#25552;&#21319;LLMs&#30340;&#36755;&#20986;&#12290;RAG&#20381;&#36182;&#20110;&#25991;&#26412;&#34164;&#28085;&#65288;TE&#65289;&#25110;&#31867;&#20284;&#26041;&#27861;&#65292;&#26816;&#26597;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#34987;&#26816;&#32034;&#25991;&#26723;&#25903;&#25345;&#25110;&#21453;&#39539;&#12290;&#26412;&#25991;&#35748;&#20026;&#20256;&#32479;&#30340;TE&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#21457;&#29616;LLMs&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#24187;&#35273;&#12290;&#20363;&#22914;&#65292;&#32771;&#34385;&#19968;&#20010;&#20851;&#20110;&#8220;&#32654;&#22269;&#23545;&#20044;&#20811;&#20848;&#25112;&#20105;&#31435;&#22330;&#8221;&#30340;&#25552;&#31034;&#12290;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#8220;&#32654;&#22269;&#24635;&#32479;&#24052;&#25289;&#20811;&#183;&#22885;&#24052;&#39532;&#35828;&#32654;&#22269;&#19981;&#20250;&#27966;&#20853;&#36827;&#20837;&#20044;&#20811;&#20848;&#8221;&#65292;&#28982;&#32780;&#22312;&#25112;&#20105;&#26399;&#38388;&#65292;&#32654;&#22269;&#24635;&#32479;&#26159;&#20052;&#183;&#25308;&#30331;&#65292;&#36825;&#19982;&#20107;&#23454;&#19981;&#31526;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;TE&#31995;&#32479;&#26080;&#27861;&#20934;&#30830;&#27880;&#37322;&#32473;&#23450;&#30340;&#25991;&#26412;&#21644;&#35782;&#21035;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19113v1 Announce Type: cross  Abstract: The widespread adoption of Large Language Models (LLMs) has facilitated numerous benefits. However, hallucination is a significant concern. In response, Retrieval Augmented Generation (RAG) has emerged as a highly promising paradigm to improve LLM outputs by grounding them in factual information. RAG relies on textual entailment (TE) or similar methods to check if the text produced by LLMs is supported or contradicted, compared to retrieved documents. This paper argues that conventional TE methods are inadequate for spotting hallucinations in content generated by LLMs. For instance, consider a prompt about the 'USA's stance on the Ukraine war''. The AI-generated text states, ...U.S. President Barack Obama says the U.S. will not put troops in Ukraine...'' However, during the war the U.S. president is Joe Biden which contradicts factual reality. Moreover, current TE systems are unable to accurately annotate the given text and identify th
&lt;/p&gt;</description></item><item><title>PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19103</link><description>&lt;p&gt;
&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#40657;&#30418;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19103
&lt;/p&gt;
&lt;p&gt;
PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#25552;&#31034;&#32780;&#23548;&#33268;&#24037;&#20316;&#32321;&#37325;&#12290;&#36825;&#19968;&#25361;&#25112;&#20419;&#20351;&#20102;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;T2I&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#38656;&#35201;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#30333;&#30418;&#35775;&#38382;&#65292;&#24182;&#20135;&#29983;&#38750;&#30452;&#35266;&#30340;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PRISM&#65292;&#36825;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#23601;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#29425;&#30340;&#21551;&#21457;&#65292;PRISM&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#32473;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#20505;&#36873;&#25552;&#31034;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;PRISM&#22312;&#20026;&#23545;&#35937;&#12289;&#26679;&#24335;&#31561;&#29983;&#25104;&#20934;&#30830;&#25552;&#31034;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19103v1 Announce Type: cross  Abstract: Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, sty
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#20462;&#27491;&#25512;&#29702;&#26694;&#26550;LeCo&#65292;&#26080;&#38656;&#20154;&#31867;&#21453;&#39304;&#12289;&#22806;&#37096;&#24037;&#20855;&#25110;&#25163;&#21160;&#25552;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#24182;&#22522;&#20110;&#29983;&#25104;logits&#26469;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19094</link><description>&lt;p&gt;
&#27809;&#26377;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27491;&#30830;&#24615;&#20351;LLM&#25104;&#20026;&#39640;&#25928;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Learning From Correctness Without Prompting Makes LLM Efficient Reasoner
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#20462;&#27491;&#25512;&#29702;&#26694;&#26550;LeCo&#65292;&#26080;&#38656;&#20154;&#31867;&#21453;&#39304;&#12289;&#22806;&#37096;&#24037;&#20855;&#25110;&#25163;&#21160;&#25552;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#24182;&#22522;&#20110;&#29983;&#25104;logits&#26469;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#12289;&#19981;&#24544;&#23454;&#30340;&#25512;&#29702;&#21644;&#26377;&#27602;&#20869;&#23481;&#31561;&#23616;&#38480;&#24615;&#12290;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#20010;&#28508;&#22312;&#26041;&#27861;&#26159;&#20174;&#20154;&#31867;&#25110;&#22806;&#37096;&#21453;&#39304;&#65288;&#20363;&#22914;&#24037;&#20855;&#65289;&#20013;&#23398;&#20064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#20869;&#22312;&#33258;&#25105;&#20462;&#27491;&#25512;&#29702;&#26694;&#26550;&#65292;&#28040;&#38500;&#20102;&#20154;&#31867;&#21453;&#39304;&#12289;&#22806;&#37096;&#24037;&#20855;&#21644;&#25163;&#24037;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#22522;&#20110;&#19968;&#31181;&#22810;&#27493;&#25512;&#29702;&#33539;&#24335;Learning from Correctness (LeCo)&#65292;&#22312;&#19981;&#38656;&#35201;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25512;&#29702;&#24615;&#33021;&#12290;&#35813;&#33539;&#24335;&#20248;&#20808;&#23398;&#20064;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#24182;&#22522;&#20110;&#29983;&#25104;logits&#26469;&#34913;&#37327;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#21508;&#31181;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25913;&#21892;&#25512;&#29702;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19094v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content. One potential approach to mitigate these issues is learning from human or external feedback (e.g. tools). In this paper, we introduce an intrinsic self-correct reasoning framework for LLMs that eliminates the need for human feedback, external tools, and handcraft prompts. The proposed framework, based on a multi-step reasoning paradigm \textbf{Le}arning from \textbf{Co}rrectness (\textsc{LeCo}), improves reasoning performance without needing to learn from errors. This paradigm prioritizes learning from correct reasoning steps, and a unique method to measure confidence for each reasoning step based on generation logits. Experimental results across various multi-step reasoning tasks demonstrate the effectiveness of the framework in improving reasoning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28385;&#24847;&#24863;&#30693;&#30340;&#21453;&#20107;&#23454;&#23545;&#35805;&#26469;&#22686;&#21152;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#21407;&#22987;&#23545;&#35805;&#38598;&#21512;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19056</link><description>&lt;p&gt;
CAUSE: &#22312;&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#21033;&#29992;&#21453;&#20107;&#23454;&#35780;&#20272;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CAUSE: Counterfactual Assessment of User Satisfaction Estimation in Task-Oriented Dialogue Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28385;&#24847;&#24863;&#30693;&#30340;&#21453;&#20107;&#23454;&#23545;&#35805;&#26469;&#22686;&#21152;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#21407;&#22987;&#23545;&#35805;&#38598;&#21512;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;&#30340;&#24037;&#20316;&#20013;&#19968;&#20010;&#37325;&#35201;&#20294;&#26410;&#34987;&#25506;&#32034;&#30340;&#26041;&#38754;&#26159;&#23545;&#20854;&#22312;&#35782;&#21035;&#29992;&#25143;&#19981;&#28385;&#24847;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#35780;&#20272;&#65306;&#24403;&#21069;&#29992;&#20110;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#39640;&#24230;&#20542;&#21521;&#20110;&#29992;&#25143;&#28385;&#24847;&#30340;&#23545;&#35805;&#12290;&#20855;&#26377;&#26356;&#24179;&#34913;&#28385;&#24847;&#24230;&#26631;&#31614;&#38598;&#21512;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#26159;&#26410;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26356;&#22810;&#30340;&#19981;&#28385;&#23545;&#35805;&#26679;&#26412;&#24179;&#34913;&#25968;&#25454;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#20154;&#24037;&#27880;&#37322;&#65292;&#36825;&#26159;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24182;&#35299;&#38145;&#20854;&#29983;&#25104;&#28385;&#24847;&#24863;&#30693;&#21453;&#20107;&#23454;&#23545;&#35805;&#30340;&#33021;&#21147;&#65292;&#20197;&#22686;&#21152;&#27979;&#35797;&#38598;&#21512;&#30340;&#21407;&#22987;&#23545;&#35805;&#38598;&#21512;&#12290;&#25105;&#20204;&#25910;&#38598;&#20154;&#24037;&#27880;&#37322;&#20197;&#30830;&#20445;&#29983;&#25104;&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20004;&#20010;&#24320;&#28304;LLM&#20316;&#20026;&#29992;&#25143;&#28385;&#24847;&#24230;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19056v1 Announce Type: new  Abstract: An important unexplored aspect in previous work on user satisfaction estimation for Task-Oriented Dialogue (TOD) systems is their evaluation in terms of robustness for the identification of user dissatisfaction: current benchmarks for user satisfaction estimation in TOD systems are highly skewed towards dialogues for which the user is satisfied. The effect of having a more balanced set of satisfaction labels on performance is unknown. However, balancing the data with more dissatisfactory dialogue samples requires further data collection and human annotation, which is costly and time-consuming. In this work, we leverage large language models (LLMs) and unlock their ability to generate satisfaction-aware counterfactual dialogues to augment the set of original dialogues of a test collection. We gather human annotations to ensure the reliability of the generated samples. We evaluate two open-source LLMs as user satisfaction estimators on our
&lt;/p&gt;</description></item><item><title>&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;...</title><link>https://arxiv.org/abs/2403.19031</link><description>&lt;p&gt;
&#20351;&#29992;&#20844;&#20849;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#30456;&#20851;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19031
&lt;/p&gt;
&lt;p&gt;
&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#35797;&#22270;&#35780;&#20272;&#23427;&#20204;&#22312;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340;&#20581;&#24247;&#30456;&#20851;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#20219;&#21153;&#20256;&#32479;&#19978;&#24456;&#38590;&#33719;&#24471;&#39640;&#20998;&#12290;&#25105;&#20204;&#22312;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;&#19968;&#20010;&#22522;&#20110;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVMs&#65289;&#30340;&#30417;&#30563;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#19977;&#20010;&#22522;&#20110;RoBERTa&#12289;BERTweet&#21644;SocBERT&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#20197;&#21450;&#20004;&#20010;&#22522;&#20110;GPT3.5&#21644;GPT4&#30340;LLM&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#21033;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65306;&#23558;LLMs&#29992;&#20316;&#38646;&#27425;&#20998;&#31867;&#22120;&#65292;&#23558;LLMs&#29992;&#20316;&#27880;&#37322;&#22120;&#20026;&#30417;&#30563;&#20998;&#31867;&#22120;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;LLMs&#36827;&#34892;&#23569;&#37327;&#31034;&#20363;&#26469;&#22686;&#21152;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19031v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable success in NLP tasks. However, there is a paucity of studies that attempt to evaluate their performances on social media-based health-related natural language processing tasks, which have traditionally been difficult to achieve high scores in. We benchmarked one supervised classic machine learning model based on Support Vector Machines (SVMs), three supervised pretrained language models (PLMs) based on RoBERTa, BERTweet, and SocBERT, and two LLM based classifiers (GPT3.5 and GPT4), across 6 text classification tasks. We developed three approaches for leveraging LLMs for text classification: employing LLMs as zero-shot classifiers, us-ing LLMs as annotators to annotate training data for supervised classifiers, and utilizing LLMs with few-shot examples for augmentation of manually annotated data. Our comprehensive experiments demonstrate that employ-ing data augmentation using LLM
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;IDGen&#65292;&#23558;&#27599;&#20010;&#25512;&#33616;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#25991;&#26412;ID&#65292;&#20174;&#32780;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26356;&#22909;&#22320;&#19982;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.19021</link><description>&lt;p&gt;
&#26397;&#21521;LLM-RecSys&#23545;&#40784;&#19982;&#25991;&#26412;ID&#23398;&#20064;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Towards LLM-RecSys Alignment with Textual ID Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19021
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;IDGen&#65292;&#23558;&#27599;&#20010;&#25512;&#33616;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#25991;&#26412;ID&#65292;&#20174;&#32780;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26356;&#22909;&#22320;&#19982;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#24050;&#32463;&#23558;&#20256;&#32479;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#25512;&#33616;&#26041;&#24335;&#36716;&#21464;&#20026;&#25991;&#26412;&#29983;&#25104;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#19982;&#22266;&#26377;&#25805;&#20316;&#20154;&#31867;&#35789;&#27719;&#30340;&#26631;&#20934;NLP&#20219;&#21153;&#30456;&#21453;&#65292;&#30446;&#21069;&#29983;&#25104;&#24335;&#25512;&#33616;&#39046;&#22495;&#30340;&#30740;&#31350;&#22312;&#22914;&#20309;&#22312;&#25991;&#26412;&#29983;&#25104;&#33539;&#24335;&#20013;&#20197;&#31616;&#27905;&#32780;&#26377;&#24847;&#20041;&#30340;ID&#34920;&#31034;&#26377;&#25928;&#32534;&#30721;&#25512;&#33616;&#39033;&#30446;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23545;&#40784;LLMs&#19982;&#25512;&#33616;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IDGen&#65292;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#26631;&#35760;&#23558;&#27599;&#20010;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#12289;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#25991;&#26412;ID&#12290;&#36825;&#36890;&#36807;&#22312;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#26049;&#35757;&#32451;&#25991;&#26412;ID&#29983;&#25104;&#22120;&#26469;&#23454;&#29616;&#65292;&#20351;&#20010;&#24615;&#21270;&#25512;&#33616;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21040;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#24182;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#35299;&#32806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19021v1 Announce Type: cross  Abstract: Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potenti
&lt;/p&gt;</description></item><item><title>ReflectSumm&#26159;&#19968;&#20010;&#26088;&#22312;&#24635;&#32467;&#23398;&#29983;&#21453;&#24605;&#24615;&#20889;&#20316;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#21644;&#35780;&#20272;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#22411;&#25688;&#35201;&#25216;&#26415;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.19012</link><description>&lt;p&gt;
ReflectSumm: &#19968;&#20010;&#29992;&#20110;&#35838;&#31243;&#21453;&#24605;&#25688;&#35201;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ReflectSumm: A Benchmark for Course Reflection Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19012
&lt;/p&gt;
&lt;p&gt;
ReflectSumm&#26159;&#19968;&#20010;&#26088;&#22312;&#24635;&#32467;&#23398;&#29983;&#21453;&#24605;&#24615;&#20889;&#20316;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#21644;&#35780;&#20272;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#22411;&#25688;&#35201;&#25216;&#26415;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;ReflectSumm&#65292;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24635;&#32467;&#23398;&#29983;&#21453;&#24605;&#24615;&#20889;&#20316;&#30340;&#26032;&#22411;&#25688;&#35201;&#25968;&#25454;&#38598;&#12290;ReflectSumm&#30340;&#30446;&#26631;&#26159;&#20419;&#36827;&#24320;&#21457;&#21644;&#35780;&#20272;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#22411;&#25688;&#35201;&#25216;&#26415;&#65292;&#36825;&#20123;&#22330;&#26223;&#20855;&#26377;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;%&#20855;&#26377;&#28508;&#22312;&#22312;&#24847;&#35265;&#24635;&#32467;&#39046;&#22495;&#21644;&#29305;&#21035;&#26159;&#25945;&#32946;&#39046;&#22495;&#20013;&#30340;&#24433;&#21709;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#65292;&#24182;&#21253;&#25324;&#20840;&#38754;&#30340;&#20803;&#25968;&#25454;&#65292;&#21487;&#20197;&#25506;&#32034;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#24182;&#25903;&#25345;&#19981;&#21516;&#30340;&#24212;&#29992;&#12290;&#20026;&#23637;&#31034;&#20854;&#25928;&#29992;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#32467;&#26524;&#20026;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19012v1 Announce Type: cross  Abstract: This paper introduces ReflectSumm, a novel summarization dataset specifically designed for summarizing students' reflective writing. The goal of ReflectSumm is to facilitate developing and evaluating novel summarization techniques tailored to real-world scenarios with little training data, %practical tasks with potential implications in the opinion summarization domain in general and the educational domain in particular. The dataset encompasses a diverse range of summarization tasks and includes comprehensive metadata, enabling the exploration of various research questions and supporting different applications. To showcase its utility, we conducted extensive evaluations using multiple state-of-the-art baselines. The results provide benchmarks for facilitating further research in this area.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#8220;Sorry, Come Again (SCA)&#8221;&#26469;&#36991;&#20813;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#24187;&#35273;&#65292;&#36890;&#36807;&#36827;&#34892;&#26368;&#20339;&#30340;&#25913;&#20889;&#21644;&#27880;&#20837;[PAUSE]&#26631;&#35760;&#26469;&#22686;&#24378;&#29702;&#35299;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18976</link><description>&lt;p&gt;
"&#23545;&#19981;&#36215;&#65292;&#20877;&#27425;&#26469;&#21527;&#65311;&#25552;&#31034;&#8212;&#8212;&#36890;&#36807;&#27880;&#20837;[PAUSE]&#20248;&#21270;&#25913;&#20889;&#26469;&#22686;&#24378;&#29702;&#35299;&#21147;&#21644;&#20943;&#23569;&#24187;&#35273;"
&lt;/p&gt;
&lt;p&gt;
"Sorry, Come Again?" Prompting -- Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18976
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#8220;Sorry, Come Again (SCA)&#8221;&#26469;&#36991;&#20813;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#24187;&#35273;&#65292;&#36890;&#36807;&#36827;&#34892;&#26368;&#20339;&#30340;&#25913;&#20889;&#21644;&#27880;&#20837;[PAUSE]&#26631;&#35760;&#26469;&#22686;&#24378;&#29702;&#35299;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hallucination&#24050;&#32463;&#25104;&#20026;&#24403;&#20195;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#26368;&#33030;&#24369;&#30340;&#26041;&#38754;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Sorry, Come Again (SCA)&#25552;&#31034;&#65292;&#26088;&#22312;&#36890;&#36807;&#65306;(i) &#26368;&#20339;&#30340;&#25913;&#20889;&#21644;(ii) &#27880;&#20837;[PAUSE]&#26631;&#35760;&#26469;&#24310;&#36831;LLMs&#30340;&#29983;&#25104;&#65292;&#20197;&#36991;&#20813;LLM&#20135;&#29983;&#24187;&#35273;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;21&#20010;LLMs&#30340;&#25552;&#31034;&#30340;&#35821;&#35328;&#32454;&#24494;&#24046;&#21035;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65306;&#27491;&#24335;&#24615;&#12289;&#21487;&#35835;&#24615;&#21644;&#20855;&#20307;&#24615;&#65292;&#24182;&#38416;&#26126;&#20102;&#36825;&#20123;&#24046;&#21035;&#26159;&#22914;&#20309;&#23548;&#33268;&#20135;&#29983;&#24187;&#35273;&#30340;&#12290;&#25552;&#31034;&#30340;&#21487;&#35835;&#24615;&#12289;&#27491;&#24335;&#24615;&#25110;&#20855;&#20307;&#24615;&#36739;&#20302;&#20250;&#32473;LLMs&#24102;&#26469;&#29702;&#35299;&#25361;&#25112;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;LLM&#20542;&#21521;&#20110;&#26681;&#25454;&#20854;&#24819;&#35937;&#21147;&#65288;&#32852;&#24819;&#35760;&#24518;&#65289;&#25512;&#27979;&#21644;&#29983;&#25104;&#20869;&#23481;&#26469;&#22635;&#34917;&#36825;&#20123;&#20449;&#24687;&#32570;&#22833;&#12290;&#23613;&#31649;&#36825;&#20123;&#29468;&#27979;&#20598;&#23572;&#21487;&#33021;&#19982;&#20107;&#23454;&#20449;&#24687;&#19968;&#33268;&#65292;&#20294;&#20854;&#20934;&#30830;&#24615;&#24182;&#19981;&#20445;&#35777;&#65292;&#32463;&#24120;&#23548;&#33268;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18976v1 Announce Type: cross  Abstract: Hallucination has emerged as the most vulnerable aspect of contemporary Large Language Models (LLMs). In this paper, we introduce the Sorry, Come Again (SCA) prompting, aimed to avoid LLM hallucinations by enhancing comprehension through: (i) optimal paraphrasing and (ii) injecting [PAUSE] tokens to delay LLM generation. First, we provide an in-depth analysis of linguistic nuances: formality, readability, and concreteness of prompts for 21 LLMs, and elucidate how these nuances contribute to hallucinated generation. Prompts with lower readability, formality, or concreteness pose comprehension challenges for LLMs, similar to those faced by humans. In such scenarios, an LLM tends to speculate and generate content based on its imagination (associative memory) to fill these information gaps. Although these speculations may occasionally align with factual information, their accuracy is not assured, often resulting in hallucination. Recent st
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#24433;&#20687;&#25253;&#21578;&#35821;&#26009;&#24211;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#25253;&#21578;&#36827;&#34892;&#26631;&#27880;&#65292;&#24182;&#36890;&#36807;&#20107;&#20214;&#21270;&#27169;&#24335;&#25429;&#25417;&#20020;&#24202;&#25351;&#31034;&#12289;&#30149;&#21464;&#21644;&#21307;&#23398;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.18975</link><description>&lt;p&gt;
&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24102;&#26631;&#27880;&#21307;&#23398;&#24433;&#20687;&#25253;&#21578;&#30340;&#26032;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
A Novel Corpus of Annotated Medical Imaging Reports and Information Extraction Results Using BERT-based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18975
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#24433;&#20687;&#25253;&#21578;&#35821;&#26009;&#24211;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#25253;&#21578;&#36827;&#34892;&#26631;&#27880;&#65292;&#24182;&#36890;&#36807;&#20107;&#20214;&#21270;&#27169;&#24335;&#25429;&#25417;&#20020;&#24202;&#25351;&#31034;&#12289;&#30149;&#21464;&#21644;&#21307;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#23545;&#20110;&#35786;&#26029;&#12289;&#30417;&#27979;&#21644;&#27835;&#30103;&#35768;&#22810;&#20581;&#24247;&#29366;&#20917;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#32959;&#30244;&#23398;&#12289;&#31070;&#32463;&#23398;&#12289;&#24515;&#34880;&#31649;&#21644;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#12290;&#25918;&#23556;&#31185;&#21307;&#24072;&#36890;&#36807;&#21465;&#36848;&#24615;&#25253;&#21578;&#26469;&#35299;&#35835;&#36825;&#20123;&#22797;&#26434;&#30340;&#12289;&#38750;&#32467;&#26500;&#21270;&#30340;&#24433;&#20687;&#65292;&#24182;&#34920;&#36798;&#20182;&#20204;&#30340;&#35780;&#20272;&#65292;&#36825;&#20123;&#25253;&#21578;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#28982;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#12290;&#36825;&#31181;&#38750;&#32467;&#26500;&#21270;&#21465;&#36848;&#24517;&#39035;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#20197;&#20419;&#36827;&#27425;&#35201;&#24212;&#29992;&#65292;&#20363;&#22914;&#22238;&#39038;&#20998;&#26512;&#25110;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24102;&#26631;&#27880;&#21307;&#23398;&#24433;&#20687;&#25253;&#21578;&#35821;&#26009;&#24211;(CAMIR)&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#19977;&#31181;&#24433;&#20687;&#27169;&#24577;&#31867;&#22411;&#30340;609&#20010;&#24102;&#26631;&#27880;&#25918;&#23556;&#23398;&#25253;&#21578;&#65306;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#12289;&#30913;&#20849;&#25391;&#25104;&#20687;&#21644;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;-&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#12290;&#25253;&#21578;&#20351;&#29992;&#22522;&#20110;&#20107;&#20214;&#30340;&#27169;&#24335;&#36827;&#34892;&#26631;&#27880;&#65292;&#25429;&#25417;&#20020;&#24202;&#25351;&#31034;&#12289;&#30149;&#21464;&#21644;&#21307;&#23398;&#38382;&#39064;&#12290;&#27599;&#20010;&#20107;&#20214;&#21253;&#25324;&#19968;&#20010;&#35302;&#21457;&#22120;&#21644;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18975v1 Announce Type: new  Abstract: Medical imaging is critical to the diagnosis, surveillance, and treatment of many health conditions, including oncological, neurological, cardiovascular, and musculoskeletal disorders, among others. Radiologists interpret these complex, unstructured images and articulate their assessments through narrative reports that remain largely unstructured. This unstructured narrative must be converted into a structured semantic representation to facilitate secondary applications such as retrospective analyses or clinical decision support. Here, we introduce the Corpus of Annotated Medical Imaging Reports (CAMIR), which includes 609 annotated radiology reports from three imaging modality types: Computed Tomography, Magnetic Resonance Imaging, and Positron Emission Tomography-Computed Tomography. Reports were annotated using an event-based schema that captures clinical indications, lesions, and medical problems. Each event consists of a trigger and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25311;&#21512;&#24847;&#22270;&#20998;&#31867;&#21644;&#28548;&#28165;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#24847;&#22270;&#20998;&#31867;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#36716;&#21270;&#20026;&#28548;&#28165;&#38382;&#39064;&#65292;&#24555;&#36895;&#20934;&#30830;&#22320;&#35299;&#20915;&#29992;&#25143;&#26597;&#35810;&#65292;&#24182;&#20855;&#26377;&#36229;&#20986;&#33539;&#22260;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18973</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#20934;&#30830;&#24847;&#22270;&#35782;&#21035;&#30340;&#25311;&#21512;&#24847;&#22270;&#20998;&#31867;&#21644;&#28548;&#28165;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Conformal Intent Classification and Clarification for Fast and Accurate Intent Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25311;&#21512;&#24847;&#22270;&#20998;&#31867;&#21644;&#28548;&#28165;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#24847;&#22270;&#20998;&#31867;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#36716;&#21270;&#20026;&#28548;&#28165;&#38382;&#39064;&#65292;&#24555;&#36895;&#20934;&#30830;&#22320;&#35299;&#20915;&#29992;&#25143;&#26597;&#35810;&#65292;&#24182;&#20855;&#26377;&#36229;&#20986;&#33539;&#22260;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25311;&#21512;&#24847;&#22270;&#20998;&#31867;&#21644;&#28548;&#28165;&#65288;CICC&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#38024;&#23545;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#24555;&#36895;&#20934;&#30830;&#24847;&#22270;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#23558;&#20219;&#20309;&#24847;&#22270;&#20998;&#31867;&#22120;&#30340;&#21551;&#21457;&#24335;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#36716;&#21270;&#20026;&#19968;&#20010;&#28548;&#28165;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#34987;&#20445;&#35777;&#21253;&#21547;&#30495;&#27491;&#24847;&#22270;&#24182;&#19988;&#22312;&#39044;&#23450;&#20041;&#30340;&#32622;&#20449;&#27700;&#24179;&#19978;&#12290;&#36890;&#36807;&#21306;&#20998;&#23569;&#37327;&#21487;&#33021;&#30340;&#24847;&#22270;&#65292;&#29992;&#25143;&#26597;&#35810;&#21487;&#20197;&#24555;&#36895;&#20934;&#30830;&#22320;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#35758;&#25193;&#23637;&#35813;&#26694;&#26550;&#20197;&#36827;&#34892;&#36229;&#20986;&#33539;&#22260;&#30340;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#19971;&#20010;&#24847;&#22270;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;CICC&#29983;&#25104;&#20102;&#23567;&#22411;&#28548;&#28165;&#38382;&#39064;&#24182;&#19988;&#20855;&#26377;&#36229;&#20986;&#33539;&#22260;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;CICC&#21487;&#20197;&#24110;&#21161;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#22823;&#24133;&#25913;&#21892;&#23545;&#35805;&#20195;&#29702;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#28548;&#28165;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18973v1 Announce Type: new  Abstract: We present Conformal Intent Classification and Clarification (CICC), a framework for fast and accurate intent classification for task-oriented dialogue systems. The framework turns heuristic uncertainty scores of any intent classifier into a clarification question that is guaranteed to contain the true intent at a pre-defined confidence level. By disambiguating between a small number of likely intents, the user query can be resolved quickly and accurately. Additionally, we propose to augment the framework for out-of-scope detection. In a comparative evaluation using seven intent recognition datasets we find that CICC generates small clarification questions and is capable of out-of-scope detection. CICC can help practitioners and researchers substantially in improving the user experience of dialogue agents with specific clarification questions.
&lt;/p&gt;</description></item><item><title>Transformer&#27169;&#22411;&#22312;&#25913;&#38761;&#20256;&#32479;&#20219;&#21153;&#21644;&#25512;&#36827;&#36328;&#34892;&#19994;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.18969</link><description>&lt;p&gt;
&#20174;&#27010;&#24565;&#21040;&#23454;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Models from Concept to Implementation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18969
&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#25913;&#38761;&#20256;&#32479;&#20219;&#21153;&#21644;&#25512;&#36827;&#36328;&#34892;&#19994;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;Transformer&#26550;&#26500;&#26500;&#24314;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#21457;&#23637;&#26497;&#22823;&#25299;&#23485;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#30340;&#33539;&#22260;&#65292;&#36229;&#36234;&#20102;&#26368;&#21021;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#26041;&#38754;&#24212;&#29992;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;GPT&#31995;&#21015;&#12290;&#36825;&#39033;&#25506;&#32034;&#32858;&#28966;&#20110;&#20154;&#24037;&#26234;&#33021;(AI)&#39537;&#21160;&#24037;&#20855;&#22312;&#25913;&#38761;&#20256;&#32479;&#32534;&#30721;&#21644;&#38382;&#39064;&#35299;&#20915;&#31561;&#20219;&#21153;&#19978;&#30340;&#38761;&#21629;&#24615;&#24433;&#21709;&#65292;&#21516;&#26102;&#22312;&#36328;&#36234;&#19981;&#21516;&#34892;&#19994;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#24320;&#36767;&#26032;&#36335;&#24452;&#12290;&#20174;&#20195;&#30721;&#35299;&#37322;&#21644;&#22270;&#20687;&#25551;&#36848;&#21040;&#20419;&#36827;&#20132;&#20114;&#24335;&#31995;&#32479;&#30340;&#25645;&#24314;&#21644;&#25512;&#36827;&#35745;&#31639;&#39046;&#22495;&#65292;Transformer&#27169;&#22411;&#20307;&#29616;&#20102;&#28145;&#24230;&#23398;&#20064;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#26412;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#31361;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18969v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs), particularly those built on Transformer architectures, have significantly broadened the scope of natural language processing (NLP) applications, transcending their initial use in chatbot technology. This paper investigates the multifaceted applications of these models, with an emphasis on the GPT series. This exploration focuses on the transformative impact of artificial intelligence (AI) driven tools in revolutionizing traditional tasks like coding and problem-solving, while also paving new paths in research and development across diverse industries. From code interpretation and image captioning to facilitating the construction of interactive systems and advancing computational domains, Transformer models exemplify a synergy of deep learning, data analysis, and neural network design. This survey provides an in-depth look at the latest research in Transformer models, highlighting the
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#19981;&#23433;&#20840;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#28216;&#25103;&#20013;&#30340;&#36829;&#27861;&#25512;&#24191;&#23041;&#32961;&#65292;&#25910;&#38598;&#20102;&#19968;&#32452;&#21253;&#21547;&#24615;&#26292;&#21147;&#21644;&#26292;&#21147;&#20869;&#23481;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.18957</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35843;&#33410;&#19981;&#23433;&#20840;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#28216;&#25103;&#20013;&#30340;&#36829;&#27861;&#22312;&#32447;&#22270;&#29255;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18957
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#19981;&#23433;&#20840;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#28216;&#25103;&#20013;&#30340;&#36829;&#27861;&#25512;&#24191;&#23041;&#32961;&#65292;&#25910;&#38598;&#20102;&#19968;&#32452;&#21253;&#21547;&#24615;&#26292;&#21147;&#21644;&#26292;&#21147;&#20869;&#23481;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#28216;&#25103;&#65288;UGCGs&#65289;&#22312;&#20799;&#31461;&#21644;&#38738;&#23569;&#24180;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29992;&#20110;&#31038;&#20132;&#20114;&#21160;&#21644;&#26356;&#26377;&#21019;&#24847;&#30340;&#22312;&#32447;&#23089;&#20048;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#30528;&#26356;&#39640;&#30340;&#26292;&#38706;&#19981;&#33391;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20799;&#31461;&#21644;&#38738;&#23569;&#24180;&#22312;&#32447;&#23433;&#20840;&#30340;&#26085;&#30410;&#20851;&#27880;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#31532;&#19968;&#27493;&#30740;&#31350;&#23545;&#19981;&#23433;&#20840;UGCGs&#30340;&#36829;&#27861;&#25512;&#24191;&#36827;&#34892;&#23041;&#32961;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#32452;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;2,924&#24352;&#23637;&#31034;&#19981;&#21516;&#24615;&#26292;&#21147;&#21644;&#26292;&#21147;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#36825;&#20123;&#20869;&#23481;&#34987;&#28216;&#25103;&#21019;&#24314;&#32773;&#29992;&#20110;&#25512;&#24191;UGCGs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18957v1 Announce Type: cross  Abstract: Online user-generated content games (UGCGs) are increasingly popular among children and adolescents for social interaction and more creative online entertainment. However, they pose a heightened risk of exposure to explicit content, raising growing concerns for the online safety of children and adolescents. Despite these concerns, few studies have addressed the issue of illicit image-based promotions of unsafe UGCGs on social media, which can inadvertently attract young users. This challenge arises from the difficulty of obtaining comprehensive training data for UGCG images and the unique nature of these images, which differ from traditional unsafe content. In this work, we take the first step towards studying the threat of illicit promotions of unsafe UGCGs. We collect a real-world dataset comprising 2,924 images that display diverse sexually explicit and violent content used to promote UGCGs by their game creators. Our in-depth studi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#30001;&#25991;&#26412;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#27969;&#31243;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#33258;&#21160;&#30340;SR&#27880;&#20876;&#34920;&#22635;&#20889;&#12290;</title><link>https://arxiv.org/abs/2403.18938</link><description>&lt;p&gt;
&#23558;&#33258;&#30001;&#25991;&#26412;&#25918;&#23556;&#23398;&#31508;&#35760;&#36716;&#21464;&#20026;&#20855;&#26377;&#29983;&#25104;&#21464;&#25442;&#22120;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Reshaping Free-Text Radiology Notes Into Structured Reports With Generative Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18938
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#30001;&#25991;&#26412;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#27969;&#31243;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#33258;&#21160;&#30340;SR&#27880;&#20876;&#34920;&#22635;&#20889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#25918;&#23556;&#23398;&#25253;&#21578;&#36890;&#24120;&#20197;&#33258;&#30001;&#25991;&#26412;&#26684;&#24335;&#32534;&#20889;&#65292;&#20351;&#20020;&#24202;&#20449;&#24687;&#38590;&#20197;&#25552;&#21462;&#21644;&#20351;&#29992;&#12290; &#26368;&#36817;&#65292;&#21508;&#31181;&#21307;&#23398;&#23398;&#20250;&#25512;&#33616;&#37319;&#29992;&#32467;&#26500;&#21270;&#25253;&#21578;&#65288;SR&#65289;&#65292;&#30001;&#20110;&#20854;&#25552;&#20379;&#30340;&#20248;&#21183;&#65288;&#22914;&#26631;&#20934;&#21270;&#12289;&#23436;&#25972;&#24615;&#21644;&#20449;&#24687;&#26816;&#32034;&#65289;&#65292;&#32467;&#26500;&#21270;&#25253;&#21578;&#65288;SR&#65289;&#30340;&#37319;&#29992;&#24471;&#21040;&#20102;&#25512;&#33616;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#30001;&#25991;&#26412;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#27969;&#31243;&#65292;&#19982;&#19968;&#20010;&#22269;&#23478;&#20171;&#20837;&#21644;&#21307;&#23398;&#25918;&#23556;&#23398;&#20250;&#25552;&#20986;&#30340;&#21442;&#32771;SR&#27880;&#20876;&#34920;&#39033;&#30446;&#30456;&#21305;&#37197;&#65292;&#37325;&#28857;&#25918;&#22312;&#23545;&#20855;&#26377;&#28107;&#24052;&#30244;&#30340;&#24739;&#32773;&#36827;&#34892;CT&#20998;&#26399;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18938v1 Announce Type: cross  Abstract: BACKGROUND: Radiology reports are typically written in a free-text format, making clinical information difficult to extract and use. Recently the adoption of structured reporting (SR) has been recommended by various medical societies thanks to the advantages it offers, e.g. standardization, completeness and information retrieval. We propose a pipeline to extract information from free-text radiology reports, that fits with the items of the reference SR registry proposed by a national society of interventional and medical radiology, focusing on CT staging of patients with lymphoma. METHODS: Our work aims to leverage the potential of Natural Language Processing (NLP) and Transformer-based models to deal with automatic SR registry filling. With the availability of 174 radiology reports, we investigate a rule-free generative Question Answering approach based on a domain-specific version of T5 (IT5). Two strategies (batch-truncation and ex-p
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#20219;&#21153;&#28041;&#21450;14&#31181;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65292;&#26088;&#22312;&#32771;&#23519;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18933</link><description>&lt;p&gt;
SemEval&#20219;&#21153;1&#65306;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
SemEval Task 1: Semantic Textual Relatedness for African and Asian Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18933
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#20219;&#21153;&#28041;&#21450;14&#31181;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65292;&#26088;&#22312;&#32771;&#23519;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20851;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65288;STR&#65289;&#30340;&#20849;&#20139;&#20219;&#21153;&#12290;&#32780;&#20808;&#21069;&#30340;&#20849;&#20139;&#20219;&#21153;&#20027;&#35201;&#20851;&#27880;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#21017;&#35843;&#26597;&#20102;&#36328;&#36234;14&#31181;&#35821;&#35328;&#65288;&#21253;&#25324;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#23612;&#35821;&#12289;&#22522;&#23612;&#20122;&#40065;&#23433;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#30340;&#26356;&#24191;&#27867;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;&#36825;&#20123;&#35821;&#35328;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#30340;&#35821;&#31995;&#65292;&#24182;&#20027;&#35201;&#22312;&#38750;&#27954;&#21644;&#20122;&#27954;&#22320;&#21306;&#20351;&#29992;&#65292;&#36825;&#20123;&#22320;&#21306;&#30340;&#29305;&#28857;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#30456;&#23545;&#26377;&#38480;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#37117;&#26159;&#19968;&#20010;&#19982;&#20998;&#25968;&#30456;&#20851;&#32852;&#30340;&#21477;&#23545;&#65292;&#35813;&#20998;&#25968;&#34920;&#31034;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#31243;&#24230;&#12290;&#21442;&#19982;&#31995;&#32479;&#34987;&#35201;&#27714;&#22312;&#19977;&#20010;&#20027;&#35201;&#36712;&#36947;&#20013;&#30340;14&#31181;&#35821;&#35328;&#20013;&#25353;&#23427;&#20204;&#22312;&#24847;&#20041;&#19978;&#30340;&#25509;&#36817;&#31243;&#24230;&#65288;&#21363;&#23427;&#20204;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#31243;&#24230;&#65289;&#23545;&#21477;&#23545;&#36827;&#34892;&#25490;&#21517;&#65306;(a) &#30417;&#30563;&#65292;(b) &#26080;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18933v1 Announce Type: new  Abstract: We present the first shared task on Semantic Textual Relatedness (STR). While earlier shared tasks primarily focused on semantic similarity, we instead investigate the broader phenomenon of semantic relatedness across 14 languages: Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by the relatively limited availability of NLP resources. Each instance in the datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences. Participating systems were asked to rank sentence pairs by their closeness in meaning (i.e., their degree of semantic relatedness) in the 14 languages in three main tracks: (a) supervised, (b) unsupervi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#36890;&#36807;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25919;&#27835;&#35758;&#39064;&#20869;&#23481;&#21644;&#39118;&#26684;&#26469;&#27979;&#37327;&#20854;&#25919;&#27835;&#20559;&#35265;&#65292;&#20027;&#24352;&#24212;&#35813;&#26377;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#34913;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.18932</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#25919;&#27835;&#20559;&#35265;&#65306;&#35328;&#35770;&#20869;&#23481;&#21644;&#34920;&#36798;&#26041;&#24335;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Measuring Political Bias in Large Language Models: What Is Said and How It Is Said
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18932
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#36890;&#36807;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25919;&#27835;&#35758;&#39064;&#20869;&#23481;&#21644;&#39118;&#26684;&#26469;&#27979;&#37327;&#20854;&#25919;&#27835;&#20559;&#35265;&#65292;&#20027;&#24352;&#24212;&#35813;&#26377;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25919;&#27835;&#35758;&#39064;&#20869;&#23481;&#21644;&#39118;&#26684;&#26469;&#27979;&#37327;&#20854;&#25919;&#27835;&#20559;&#35265;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#21644;&#27979;&#37327;&#26041;&#27861;&#20851;&#27880;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#65292;&#20294;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#25919;&#27835;&#20559;&#35265;&#65292;&#21487;&#33021;&#23548;&#33268;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#26497;&#21270;&#21644;&#20854;&#20182;&#21361;&#23475;&#12290;&#20026;&#20102;&#21521;&#29992;&#25143;&#25552;&#20379;&#36879;&#26126;&#24230;&#65292;&#25105;&#20204;&#20027;&#24352;&#24212;&#35813;&#26377;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27979;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27979;&#37327;&#26041;&#27861;&#26082;&#32771;&#34385;&#20102;&#29983;&#27542;&#26435;&#21033;&#21644;&#27668;&#20505;&#21464;&#21270;&#31561;&#19981;&#21516;&#25919;&#27835;&#35758;&#39064;&#30340;&#20869;&#23481;&#65288;&#29983;&#25104;&#29289;&#30340;&#23454;&#36136;&#65289;&#65292;&#20063;&#32771;&#34385;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#39118;&#26684;&#65288;&#35789;&#27719;&#30340;&#26497;&#24615;&#65289;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#21313;&#19968;&#20010;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25919;&#27835;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#25193;&#23637;&#21040;&#20854;&#20182;&#20027;&#39064;&#26102;&#26082;&#26131;&#20110;&#25193;&#23637;&#21448;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18932v1 Announce Type: cross  Abstract: We propose to measure political bias in LLMs by analyzing both the content and style of their generated content regarding political issues. Existing benchmarks and measures focus on gender and racial biases. However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications. In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by LLMs. Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias. We measured the political bias in eleven open-sourced LLMs and showed that our proposed framework is easily scalable to other topics and is explainable.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MoE&#27169;&#22411;\tool&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#22411;&#19987;&#23478;&#21644;&#22522;&#20110;&#38408;&#20540;&#30340;&#36335;&#30001;&#22120;&#65292;&#20351;&#26631;&#35760;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20165;&#28041;&#21450;&#21040;&#24517;&#35201;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#22312;&#20943;&#23569;MoE&#23618;&#35745;&#31639;&#36127;&#36733;50%&#20197;&#19978;&#30340;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18926</link><description>&lt;p&gt;
&#29992;&#26356;&#31232;&#30095;&#30340;&#36873;&#25321;&#25552;&#39640;&#31232;&#30095;&#27169;&#22411;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Enhancing Efficiency in Sparse Models with Sparser Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18926
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MoE&#27169;&#22411;\tool&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#22411;&#19987;&#23478;&#21644;&#22522;&#20110;&#38408;&#20540;&#30340;&#36335;&#30001;&#22120;&#65292;&#20351;&#26631;&#35760;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20165;&#28041;&#21450;&#21040;&#24517;&#35201;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#22312;&#20943;&#23569;MoE&#23618;&#35745;&#31639;&#36127;&#36733;50%&#20197;&#19978;&#30340;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#27169;&#22411;&#65292;&#21253;&#25324;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#27169;&#22411;&#65292;&#24050;&#32463;&#25104;&#20026;&#32553;&#25918;Transformer&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#23384;&#22312;&#35745;&#31639;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#22823;&#37327;&#21442;&#25968;&#36890;&#36807;&#23558;&#20540;&#20056;&#20197;&#38646;&#25110;&#20302;&#28608;&#27963;&#20540;&#26080;&#35859;&#21442;&#19982;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\tool &#30340;&#26032;&#39062;MoE&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#21319;&#31232;&#30095;MoE&#27169;&#22411;&#30340;&#21151;&#25928;&#21644;&#25928;&#29575;&#12290; \tool &#21033;&#29992;&#23567;&#22411;&#19987;&#23478;&#21644;&#22522;&#20110;&#38408;&#20540;&#30340;&#36335;&#30001;&#22120;&#65292;&#20351;&#26631;&#35760;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20165;&#28041;&#21450;&#21040;&#24517;&#35201;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;\tool &#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;MoE&#23618;&#30340;&#35745;&#31639;&#36127;&#36733;&#20943;&#23569;50\%&#20197;&#19978;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;\tool &#30340;&#36890;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#22312;&#25512;&#26029;&#26399;&#38388;&#23454;&#29616;&#31232;&#30095;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18926v1 Announce Type: cross  Abstract: Sparse models, including sparse Mixture-of-Experts (MoE) models, have emerged as an effective approach for scaling Transformer models. However, they often suffer from computational inefficiency since a significant number of parameters are unnecessarily involved in computations via multiplying values by zero or low activation values. To address this issue, we present \tool, a novel MoE designed to enhance both the efficacy and efficiency of sparse MoE models. \tool leverages small experts and a threshold-based router to enable tokens to selectively engage only essential parameters. Our extensive experiments on language modeling and machine translation tasks demonstrate that \tool can enhance model performance while decreasing the computation load at MoE layers by over 50\% without sacrificing performance. Furthermore, we present the versatility of \tool by applying it to dense models, enabling sparse computation during inference. We pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;DeepView&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#20197;&#20943;&#23569;&#32534;&#30721;&#22120;&#27169;&#22411;&#23384;&#22312;&#30340;&#39118;&#38505;&#24182;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.18872</link><description>&lt;p&gt;
&#32534;&#30721;&#22120;LLMs&#39592;&#24178;&#30340;&#23450;&#21521;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Targeted Visualization of the Backbone of Encoder LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;DeepView&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#20197;&#20943;&#23569;&#32534;&#30721;&#22120;&#27169;&#22411;&#23384;&#22312;&#30340;&#39118;&#38505;&#24182;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;&#20004;&#31181;&#26550;&#26500;&#26159;&#32534;&#30721;&#22120;&#65292;&#22914;BERT&#65292;&#21644;&#35299;&#30721;&#22120;&#65292;&#22914;GPT&#27169;&#22411;&#12290;&#23613;&#31649;&#32534;&#30721;&#22120;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#23384;&#22312;&#19968;&#20123;&#39118;&#38505;&#65292;&#21253;&#25324;&#20559;&#35265;&#38382;&#39064;&#25110;&#26131;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#34920;&#26126;&#20102;&#38656;&#35201;&#21487;&#35299;&#37322;&#30340;AI&#26469;&#26816;&#27979;&#36825;&#20123;&#38382;&#39064;&#12290;&#34429;&#28982;&#30446;&#21069;&#23384;&#22312;&#21508;&#31181;&#20851;&#27880;&#39044;&#27979;&#21333;&#20010;&#36755;&#20837;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20294;&#22522;&#20110;&#38477;&#32500;&#30340;&#29992;&#20110;&#20998;&#31867;&#26816;&#26597;&#30340;&#20840;&#23616;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#20854;&#20182;&#39046;&#22495;&#20986;&#29616;&#24182;&#36229;&#36234;&#20165;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#29992;t-SNE&#30340;&#26041;&#27861;&#65292;&#22312;NLP&#20013;&#24182;&#19981;&#21313;&#20998;&#24191;&#27867;&#20256;&#25773;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DeepView&#26041;&#27861;&#22312;NLP&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#22312;&#20108;&#32500;&#20013;&#21487;&#35270;&#21270;&#20915;&#31574;&#20989;&#25968;&#30340;&#19968;&#37096;&#20998;&#20197;&#21450;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18872v1 Announce Type: cross  Abstract: Attention based Large Language Models (LLMs) are the state-of-the-art in natural language processing (NLP). The two most common architectures are encoders such as BERT, and decoders like the GPT models. Despite the success of encoder models, on which we focus in this work, they also bear several risks, including issues with bias or their susceptibility for adversarial attacks, signifying the necessity for explainable AI to detect such issues. While there does exist various local explainability methods focusing on the prediction of single inputs, global methods based on dimensionality reduction for classification inspection, which have emerged in other domains and that go further than just using t-SNE in the embedding space, are not widely spread in NLP.   To reduce this gap, we investigate the application of DeepView, a method for visualizing a part of the decision function together with a data set in two dimensions, to the NLP domain.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;JEP-KD&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#32593;&#32476;&#22312;&#23884;&#20837;&#23618;&#20869;&#22686;&#24378;&#35270;&#39057;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#38899;&#39057;&#29305;&#24449;&#65292;&#36880;&#27493;&#20943;&#23569;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#19982;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.18843</link><description>&lt;p&gt;
JEP-KD&#65306;&#22522;&#20110;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
JEP-KD: Joint-Embedding Predictive Architecture Based Knowledge Distillation for Visual Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;JEP-KD&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#32593;&#32476;&#22312;&#23884;&#20837;&#23618;&#20869;&#22686;&#24378;&#35270;&#39057;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#38899;&#39057;&#29305;&#24449;&#65292;&#36880;&#27493;&#20943;&#23569;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#19982;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65288;VSR&#65289;&#20219;&#21153;&#36890;&#24120;&#34987;&#35748;&#20026;&#20855;&#26377;&#27604;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26356;&#20302;&#30340;&#29702;&#35770;&#24615;&#33021;&#19978;&#38480;&#65292;&#36825;&#26159;&#30001;&#20110;&#36890;&#36807;&#35270;&#35273;&#26041;&#24335;&#20256;&#36798;&#35821;&#20041;&#20449;&#24687;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;JEP-KD&#30340;Joint-Embedding Predictive Architecture&#65288;JEPA&#65289;&#65292;&#26088;&#22312;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#38899;&#39057;&#29305;&#24449;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;JEP-KD&#30340;&#26680;&#24515;&#22312;&#20110;&#22312;&#23884;&#20837;&#23618;&#20869;&#21253;&#21547;&#19968;&#20010;&#29983;&#25104;&#32593;&#32476;&#65292;&#22686;&#24378;&#20102;&#35270;&#39057;&#32534;&#30721;&#22120;&#23545;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#20854;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#32534;&#30721;&#22120;&#30340;&#38899;&#39057;&#29305;&#24449;&#26356;&#21152;&#25509;&#36817;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#36880;&#28176;&#20943;&#23569;VSR&#21644;ASR&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#36824;&#24314;&#31435;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#12289;&#22810;&#38454;&#27573;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;JEP-KD&#26694;&#26550;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18843v1 Announce Type: cross  Abstract: Visual Speech Recognition (VSR) tasks are generally recognized to have a lower theoretical performance ceiling than Automatic Speech Recognition (ASR), owing to the inherent limitations of conveying semantic information visually. To mitigate this challenge, this paper introduces an advanced knowledge distillation approach using a Joint-Embedding Predictive Architecture (JEPA), named JEP-KD, designed to more effectively utilize audio features during model training. Central to JEP-KD is the inclusion of a generative network within the embedding layer, which enhances the video encoder's capacity for semantic feature extraction and brings it into closer alignment with the audio features from a pre-trained ASR model's encoder. This approach aims to progressively reduce the performance gap between VSR and ASR. Moreover, a comprehensive multimodal, multistage training regimen for the JEP-KD framework is established, bolstering the robustness 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;MORE&#65292;&#21516;&#26102;&#25552;&#20986;&#20004;&#31181;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.18346</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#21644;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#65306;&#22240;&#26524;&#20851;&#31995;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18346
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;MORE&#65292;&#21516;&#26102;&#25552;&#20986;&#20004;&#31181;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#22810;&#27169;&#24577;LLMs&#65288;MLLMs&#65289;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;MLLMs&#36890;&#24120;&#36807;&#24230;&#20381;&#36182;&#21333;&#27169;&#24577;&#20559;&#24046;&#65288;&#20363;&#22914;&#35821;&#35328;&#20559;&#24046;&#21644;&#35270;&#35273;&#20559;&#24046;&#65289;&#65292;&#23548;&#33268;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#32473;&#20986;&#19981;&#27491;&#30830;&#31572;&#26696;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#26469;&#35299;&#37322;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22240;&#26524;&#22270;&#26469;&#38416;&#26126;MLLMs&#23545;VQA&#38382;&#39064;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#28145;&#20837;&#30340;&#22240;&#26524;&#20998;&#26512;&#35780;&#20272;&#20559;&#24046;&#30340;&#22240;&#26524;&#25928;&#26524;&#12290;&#21463;&#22240;&#26524;&#22270;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;MORE&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;12,000&#20010;VQA&#23454;&#20363;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#25361;&#25112;MLLMs&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#21644;&#20811;&#26381;&#21333;&#27169;&#24577;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#24182;&#22686;&#24378;MLLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18346v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have facilitated the development of Multimodal LLMs (MLLMs). Despite their impressive capabilities, MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks. To investigate this issue, we propose a causal framework to interpret the biases in Visual Question Answering (VQA) problems. Within our framework, we devise a causal graph to elucidate the predictions of MLLMs on VQA problems, and assess the causal effect of biases through an in-depth causal analysis. Motivated by the causal graph, we introduce a novel MORE dataset, consisting of 12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities, necessitating multi-hop reasoning and the surmounting of unimodal biases. Furthermore, we propose two strategies to mitigate unimodal biases and enhance MLLMs' reasoning capabiliti
&lt;/p&gt;</description></item><item><title>&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#20013;&#25991;&#20013;&#26816;&#27979; offensive &#35821;&#35328;&#30340;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.18314</link><description>&lt;p&gt;
&#20013;&#22269; offensive &#35821;&#35328;&#26816;&#27979;&#65306;&#29616;&#29366;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Chinese Offensive Language Detection:Current Status and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18314
&lt;/p&gt;
&lt;p&gt;
&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#20013;&#25991;&#20013;&#26816;&#27979; offensive &#35821;&#35328;&#30340;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27491;&#22312;&#20570;&#20986;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#30417;&#27979;&#21644;&#35268;&#33539;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#65292;&#20294;&#22312;&#25968;&#23383;&#31354;&#38388;&#20013;&#65292;&#24694;&#24847;&#35821;&#35328;&#65288;&#22914;&#20167;&#24680;&#35328;&#35770;&#25110;&#32593;&#32476;&#27450;&#20940;&#65289;&#30340;&#26222;&#36941;&#23384;&#22312;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#37492;&#20110;&#32500;&#25252;&#25991;&#26126;&#21644;&#23562;&#37325;&#30340;&#22312;&#32447;&#29615;&#22659;&#30340;&#37325;&#35201;&#24615;&#65292;&#36843;&#20999;&#38656;&#35201;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24694;&#24847;&#35328;&#35770;&#30340;&#33258;&#21160;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#24320;&#21457;&#22788;&#29702;&#27721;&#35821;&#31561;&#35821;&#35328;&#30340;&#26377;&#25928;&#31995;&#32479;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#35821;&#35328;&#30340;&#22797;&#26434;&#21644;&#24494;&#22937;&#24615;&#20351;&#24471;&#33258;&#21160;&#22788;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#20102;&#20013;&#22269; offensive &#35821;&#35328;&#26816;&#27979;&#24773;&#20917;&#65292;&#23457;&#26597;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#29992;&#20110;&#35299;&#20915;&#22312;&#36825;&#31181;&#22797;&#26434;&#35821;&#35328;&#20013;&#26816;&#27979;&#24694;&#24847;&#35821;&#35328;&#30340;&#29420;&#29305;&#25361;&#25112;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18314v1 Announce Type: cross  Abstract: Despite the considerable efforts being made to monitor and regulate user-generated content on social media platforms, the pervasiveness of offensive language, such as hate speech or cyberbullying, in the digital space remains a significant challenge. Given the importance of maintaining a civilized and respectful online environment, there is an urgent and growing need for automatic systems capable of detecting offensive speech in real time. However, developing effective systems for processing languages such as Chinese presents a significant challenge, owing to the language's complex and nuanced nature, which makes it difficult to process automatically. This paper provides a comprehensive overview of offensive language detection in Chinese, examining current benchmarks and approaches and highlighting specific models and tools for addressing the unique challenges of detecting offensive language in this complex language. The primary object
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;ov-freeze&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.18159</link><description>&lt;p&gt;
&#22114;&#65281;&#25105;&#20204;&#20919;&#20923;&#65306;&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18159
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;ov-freeze&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#20998;&#21035;&#22312;NLP&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#36895;&#24230;&#24930;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#39640;&#65292;&#36825;&#20351;&#24471;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#23427;&#20204;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#37327;&#21270;&#24863;&#30693;&#24494;&#35843;&#25216;&#26415;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD-QAT&#65289;&#26469;&#25913;&#21892;&#20351;&#29992;&#24120;&#29992;&#25968;&#25454;&#38598;&#25913;&#36827;4&#20301;&#37325;&#37327;&#37327;&#21270;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#23454;&#29616;&#27969;&#34892;&#30340;&#35821;&#35328;&#20351;&#29992;&#26696;&#20363;&#65292;&#22312;&#35774;&#22791;&#32842;&#22825;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#31181;&#24494;&#35843;&#33539;&#24335;&#65292;&#20316;&#20026;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#20256;&#25773;&#65292;&#25552;&#20379;&#23545;KD-QAT&#31283;&#23450;&#24615;&#30340;&#27934;&#23519;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;KD-QAT&#30340;&#26041;&#27861;&#23545;&#20302;&#20301;&#37327;&#21270;&#35823;&#24046;&#30340;&#33030;&#24369;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ov-freeze&#65292;&#19968;&#31181;&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#31616;&#21333;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18159v1 Announce Type: cross  Abstract: Large generative models, such as large language models (LLMs) and diffusion models have as revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process. Finally, we expe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;LM&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.18025</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#23450;&#25513;&#30721;&#25439;&#22833;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#65306;&#20197;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18025
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;LM&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35843;&#25972;&#21040;&#26032;&#39046;&#22495;&#36890;&#24120;&#36890;&#36807;&#22312;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;LM&#65288;PLM&#65289;&#26469;&#23454;&#29616;&#12290;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#24341;&#20837;LM&#65292;&#20351;&#23427;&#33021;&#22815;&#29702;&#35299;&#21644;&#26377;&#25928;&#25191;&#34892;&#30446;&#26631;&#22495;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#21464;&#24471;&#19981;&#22815;&#25935;&#24863;&#65292;&#22914;&#26524;&#23427;&#24573;&#35270;&#20102;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24191;&#27867;&#24046;&#24322;&#65288;&#20363;&#22914;&#22312;&#35789;&#20041;&#19978;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#24494;&#35843;&#19981;&#25935;&#24863;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36866;&#24403;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65288;DS-terms&#65289;&#30340;&#37325;&#35201;&#24615;&#26469;&#26377;&#25928;&#33719;&#21462;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;MSLM&#21516;&#26102;&#23631;&#34109;DS&#26415;&#35821;&#21644;&#36890;&#29992;&#35789;&#65292;&#28982;&#21518;&#36890;&#36807;&#30830;&#20445;LM&#21463;&#21040;&#26356;&#22823;&#24809;&#32602;&#26469;&#23398;&#20064;&#29305;&#23450;&#20110;&#25513;&#30721;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18025v1 Announce Type: cross  Abstract: Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for in
&lt;/p&gt;</description></item><item><title>DORE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#23450;&#20041;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#21253;&#21547;&#36229;&#36807;10&#19975;&#20010;&#23450;&#20041;&#65292;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18018</link><description>&lt;p&gt;
DORE&#65306;&#19968;&#20221;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#23450;&#20041;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DORE: A Dataset For Portuguese Definition Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18018
&lt;/p&gt;
&lt;p&gt;
DORE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#23450;&#20041;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#21253;&#21547;&#36229;&#36807;10&#19975;&#20010;&#23450;&#20041;&#65292;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18018v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#23450;&#20041;&#24314;&#27169;&#65288;DM&#65289;&#26159;&#33258;&#21160;&#20026;&#29305;&#23450;&#21333;&#35789;&#29983;&#25104;&#35789;&#20856;&#23450;&#20041;&#30340;&#20219;&#21153;&#12290;&#20855;&#26377;DM&#33021;&#21147;&#30340;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#22312;&#22810;&#20010;&#21463;&#20247;&#20013;&#21463;&#30410;&#65292;&#22240;&#20026;DM&#34987;&#35270;&#20026;&#30417;&#30563;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#38382;&#39064;&#65292;&#36825;&#20123;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#24050;&#32463;&#21457;&#24067;&#20102;&#19968;&#20123;&#29992;&#20110;&#33521;&#35821;&#21644;&#20854;&#20182;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;DM&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#33889;&#33796;&#29273;&#35821;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20013;/&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#19988;&#34987;2&#20159;&#22810;&#27597;&#35821;&#20154;&#21475;&#20351;&#29992;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#33889;&#33796;&#29273;&#35821;&#30340;DM&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;DORE&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65307;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#23450;&#20041;&#24314;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;10&#19975;&#20010;&#23450;&#20041;&#12290;&#25105;&#20204;&#36824;&#22312;DORE&#19978;&#35780;&#20272;&#20102;&#20960;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;DM&#27169;&#22411;&#65292;&#24182;&#25253;&#21578;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18018v1 Announce Type: new  Abstract: Definition modelling (DM) is the task of automatically generating a dictionary definition for a specific word. Computational systems that are capable of DM can have numerous applications benefiting a wide range of audiences. As DM is considered a supervised natural language generation problem, these systems require large annotated datasets to train the machine learning (ML) models. Several DM datasets have been released for English and other high-resource languages. While Portuguese is considered a mid/high-resource language in most natural language processing tasks and is spoken by more than 200 million native speakers, there is no DM dataset available for Portuguese. In this research, we fill this gap by introducing DORE; the first dataset for Definition MOdelling for PoRtuguEse containing more than 100,000 definitions. We also evaluate several deep learning based DM models on DORE and report the results. The dataset and the findings o
&lt;/p&gt;</description></item><item><title>&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17919</link><description>&lt;p&gt;
LISA&#65306;&#29992;&#20110;&#39640;&#25928;&#20869;&#23384;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17919
&lt;/p&gt;
&lt;p&gt;
&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39318;&#27425;&#20986;&#29616;&#20197;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#24040;&#22823;&#30340;&#20869;&#23384;&#28040;&#32791;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35832;&#22914;&#20302;&#31209;&#35843;&#25972;&#65288;LoRA&#65289;&#20043;&#31867;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#22823;&#35268;&#27169;&#24494;&#35843;&#35774;&#32622;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#26080;&#27861;&#19982;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#30456;&#21305;&#37197;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LoRA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#30340;&#36880;&#23618;&#29305;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#21516;&#23618;&#20043;&#38388;&#26435;&#37325;&#33539;&#25968;&#30340;&#24322;&#24120;&#20559;&#26012;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#31616;&#21333;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#35760;&#24518;&#25104;&#26412;&#20302;&#20110;LoRA&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#24191;&#27867;&#30340;&#35774;&#32622;&#20013;&#20248;&#20110;LoRA&#21644;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;Layerwise Importance Sampled AdamW&#65288;LISA&#65289;&#65292;&#36825;&#26159;LoRA&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
&lt;/p&gt;</description></item><item><title>&#22810;&#39033;&#36873;&#25321;&#39064;&#34429;&#28982;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#22312;&#27979;&#35797;LLMs&#33021;&#21147;&#26102;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#38271;&#31687;&#29983;&#25104;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#21452;&#35821;MCQs&#20013;&#34920;&#29616;&#20986;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17752</link><description>&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#26159;&#21542;&#30495;&#30340;&#33021;&#22815;&#26816;&#27979;LLMs&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can multiple-choice questions really be useful in detecting the abilities of LLMs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17752
&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#34429;&#28982;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#22312;&#27979;&#35797;LLMs&#33021;&#21147;&#26102;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#38271;&#31687;&#29983;&#25104;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#21452;&#35821;MCQs&#20013;&#34920;&#29616;&#20986;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;(MCQs)&#30001;&#20110;&#20854;&#31616;&#21333;&#21644;&#39640;&#25928;&#32780;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#20110;MCQs&#26159;&#21542;&#33021;&#30495;&#27491;&#34913;&#37327;LLMs&#30340;&#33021;&#21147;&#23384;&#22312;&#30097;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#38271;&#31687;&#29983;&#25104;(LFG)&#31572;&#26696;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#22330;&#26223;&#20013;&#12290;&#20219;&#21153;&#19982;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38656;&#35201;&#23545;MCQ&#30340;&#25928;&#29992;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#32780;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36890;&#36807;&#35780;&#20272;&#20004;&#31181;&#35821;&#35328;&#65288;&#20013;&#25991;&#21644;&#33521;&#25991;&#65289;&#30340;&#22235;&#20010;&#38382;&#31572;(QA)&#25968;&#25454;&#38598;&#19978;&#30340;&#20061;&#20010;LLMs&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;LLMs&#22312;&#21452;&#35821;MCQs&#20013;&#34920;&#29616;&#20986;&#19968;&#31181;&#39034;&#24207;&#25935;&#24863;&#24615;&#65292;&#20559;&#21521;&#20110;&#20301;&#20110;&#29305;&#23450;&#20301;&#32622;&#30340;&#31572;&#26696;&#65292;&#21363;&#31532;&#19968;&#20010;&#20301;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#30452;&#25509;&#36755;&#20986;&#12289;token logit&#21644;&#23884;&#20837;&#26469;&#37327;&#21270;MCQs&#21644;&#38271;&#31687;&#29983;&#25104;&#38382;&#39064;(LFGQs)&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;MCQs&#21644;&#38271;&#31687;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#23384;&#22312;&#30456;&#23545;&#36739;&#20302;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17752v1 Announce Type: new  Abstract: Multiple-choice questions (MCQs) are widely used in the evaluation of large language models (LLMs) due to their simplicity and efficiency. However, there are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required. The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ's efficacy, which we undertake in this paper by evaluating nine LLMs on four question-answering (QA) datasets in two languages: Chinese and English. We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position. We further quantify the gap between MCQs and long-form generation questions (LFGQs) by comparing their direct outputs, token logits, and embeddings. Our results reveal a relatively low correlation between answers from MC
&lt;/p&gt;</description></item><item><title>DANCER&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Description Augmented Named entity CorrEctoR&#65288;DANCER&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#25551;&#36848;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;NEC&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#65292;&#24110;&#21161;&#32531;&#35299;NE&#21015;&#34920;&#20013;&#30340;&#38899;&#32032;&#28151;&#28102;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17645</link><description>&lt;p&gt;
DANCER&#65306;&#38024;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#23454;&#20307;&#25551;&#36848;&#22686;&#24378;&#21629;&#21517;&#23454;&#20307;&#26657;&#27491;&#22120;
&lt;/p&gt;
&lt;p&gt;
DANCER: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17645
&lt;/p&gt;
&lt;p&gt;
DANCER&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Description Augmented Named entity CorrEctoR&#65288;DANCER&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#25551;&#36848;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;NEC&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#65292;&#24110;&#21161;&#32531;&#35299;NE&#21015;&#34920;&#20013;&#30340;&#38899;&#32032;&#28151;&#28102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;ASR&#30340;&#24555;&#36895;&#36731;&#37327;&#32423;&#21629;&#21517;&#23454;&#20307;&#26657;&#27491;&#65288;NEC&#65289;&#27169;&#22411;&#65292;&#36890;&#24120;&#26500;&#24314;&#22312;&#38899;&#32032;&#32423;&#32534;&#36753;&#36317;&#31163;&#31639;&#27861;&#22522;&#30784;&#19978;&#65292;&#24182;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;NEC&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#21629;&#21517;&#23454;&#20307;&#65288;NE&#65289;&#21015;&#34920;&#30340;&#22686;&#21152;&#65292;NE&#21015;&#34920;&#20013;&#30340;&#38899;&#32032;&#28151;&#28102;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65307;&#20363;&#22914;&#65292;&#21516;&#38899;&#24322;&#20041;&#35789;&#30340;&#38382;&#39064;&#22823;&#22823;&#22686;&#21152;&#12290;&#37492;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25551;&#36848;&#22686;&#24378;&#22411;&#21629;&#21517;&#23454;&#20307;&#26657;&#27491;&#22120;&#65288;&#31216;&#20026;DANCER&#65289;&#65292;&#21033;&#29992;&#23454;&#20307;&#25551;&#36848;&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#65292;&#20197;&#20415;&#22312;ASR&#36716;&#24405;&#20013;&#20026;NEC&#25552;&#20379;&#36741;&#21161;&#20943;&#36731;&#38899;&#32032;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17645v1 Announce Type: new  Abstract: End-to-end automatic speech recognition (E2E ASR) systems often suffer from mistranscription of domain-specific phrases, such as named entities, sometimes leading to catastrophic failures in downstream tasks. A family of fast and lightweight named entity correction (NEC) models for ASR have recently been proposed, which normally build on phonetic-level edit distance algorithms and have shown impressive NEC performance. However, as the named entity (NE) list grows, the problems of phonetic confusion in the NE list are exacerbated; for example, homophone ambiguities increase substantially. In view of this, we proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER), which leverages entity descriptions to provide additional information to facilitate mitigation of phonetic confusion for NEC on ASR transcription. To this end, an efficient entity description augmented masked language model (EDA-MLM) comprised of a dense re
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLM&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#36880;&#27493;&#21512;&#25104;&#31574;&#30053;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#36880;&#27493;&#23376;&#38382;&#39064;&#65292;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#65292;&#20197;&#35299;&#20915;&#22270;&#34920;VQA&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2403.16385</link><description>&lt;p&gt;
&#36880;&#27493;&#21512;&#25104;&#65306;&#24037;&#20855;&#12289;&#27169;&#26495;&#21644;LLM&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#29992;&#20110;&#22522;&#20110;&#25512;&#29702;&#30340;&#22270;&#34920;VQA
&lt;/p&gt;
&lt;p&gt;
Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16385
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLM&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#36880;&#27493;&#21512;&#25104;&#31574;&#30053;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#36880;&#27493;&#23376;&#38382;&#39064;&#65292;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#65292;&#20197;&#35299;&#20915;&#22270;&#34920;VQA&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22270;&#34920;&#21644;&#22270;&#24418;&#31561;&#25968;&#25454;&#21487;&#35270;&#21270;&#38656;&#35201;&#23545;&#35270;&#35273;&#20803;&#32032;&#21644;&#25968;&#23383;&#36827;&#34892;&#25512;&#29702;&#12290;&#23613;&#31649;&#22312;&#25552;&#21462;&#24335;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#21069;&#30340;&#22270;&#34920;&#35270;&#35273;&#38382;&#31572;&#65288;chart VQA&#65289;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#24050;&#32463;&#34920;&#29616;&#20986;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#21160;&#25968;&#25454;&#26631;&#27880;&#22120;&#65292;&#20026;&#22270;&#34920;&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#27880;&#37322;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#8220;&#36880;&#27493;&#21512;&#25104;&#8221;&#31574;&#30053;&#65306;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#23398;&#20064;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#36880;&#27493;&#23376;&#38382;&#39064;&#65288;&#21407;&#29702;&#65289;&#65292;&#28982;&#21518;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#65288;&#21363;Python&#65289;&#25512;&#23548;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#36825;&#31181;&#36880;&#27493;&#29983;&#25104;&#36807;&#31243;&#26159;&#22312;&#20351;&#29992;&#22522;&#20110;&#27169;&#26495;&#30340;QA&#29983;&#25104;&#31649;&#36947;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16385v1 Announce Type: cross  Abstract: Understanding data visualizations like charts and plots requires reasoning about both visual elements and numerics. Although strong in extractive questions, current chart visual question answering (chart VQA) models suffer on complex reasoning questions. In this work, we address the lack of reasoning ability by data augmentation. We leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images. The key innovation in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data generator learns to decompose the complex question into step-by-step sub-questions (rationales), which are then used to derive the final answer using external tools, i.e. Python. This step-wise generation procedure is trained on synthetic data generated using a template-based QA generation pipeline. Experimental results highlight th
&lt;/p&gt;</description></item><item><title>WoLF&#26694;&#26550;&#25552;&#20986;&#20102;&#23545;&#20110;CXR&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#20351;&#29992;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#12289;&#37325;&#26500;&#25253;&#21578;&#20197;&#25552;&#20379;&#26356;&#26377;&#32452;&#32455;&#30340;&#20449;&#24687;&#12289;&#20197;&#21450;&#25913;&#36827;&#29983;&#25104;&#31572;&#26696;&#30340;&#32454;&#33268;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.15456</link><description>&lt;p&gt;
WoLF: &#29992;&#20110;&#33016;&#37096;X&#32447;&#22270;&#29702;&#35299;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
WoLF: Large Language Model Framework for CXR Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15456
&lt;/p&gt;
&lt;p&gt;
WoLF&#26694;&#26550;&#25552;&#20986;&#20102;&#23545;&#20110;CXR&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#20351;&#29992;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#12289;&#37325;&#26500;&#25253;&#21578;&#20197;&#25552;&#20379;&#26356;&#26377;&#32452;&#32455;&#30340;&#20449;&#24687;&#12289;&#20197;&#21450;&#25913;&#36827;&#29983;&#25104;&#31572;&#26696;&#30340;&#32454;&#33268;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#21462;&#24471;&#20102;&#23545;&#33016;&#37096;X&#32447;&#22270;(CXR)&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#30528;&#26041;&#27861;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35270;&#35273;&#38382;&#31572;(VQA)&#21644;CXR&#25253;&#21578;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CXR&#29702;&#35299;&#26694;&#26550;&#20173;&#23384;&#22312;&#20960;&#20010;&#31243;&#24207;&#19978;&#30340;&#32570;&#38519;&#12290;(1)&#20197;&#24448;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;CXR&#25253;&#21578;&#65292;&#36825;&#23545;&#20110;&#20840;&#38754;&#30340;&#35270;&#35273;&#38382;&#31572;(VQA)&#26469;&#35828;&#26159;&#19981;&#22815;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#22914;&#29992;&#33647;&#21382;&#21490;&#21644;&#20808;&#21069;&#30340;&#35786;&#26029;&#26102;&#12290;(2)&#20197;&#24448;&#30340;&#26041;&#27861;&#20351;&#29992;&#26410;&#32463;&#22788;&#29702;&#30340;CXR&#25253;&#21578;&#65292;&#36825;&#20123;&#25253;&#21578;&#24448;&#24448;&#32467;&#26500;&#38543;&#24847;&#12290;&#34429;&#28982;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29702;&#35299;&#21508;&#31181;&#25991;&#26412;&#26684;&#24335;&#65292;&#20294;&#20026;&#20102;&#25552;&#20379;&#26356;&#28165;&#26224;&#12289;&#26377;&#32452;&#32455;&#30340;&#22522;&#20110;&#35299;&#21078;&#23398;&#30340;&#20449;&#24687;&#65292;&#37325;&#26500;&#25253;&#21578;&#21487;&#33021;&#20250;&#22686;&#24378;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;(3)&#30446;&#21069;&#29992;&#20110;CXR-VQA&#30340;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#24378;&#35843;&#35821;&#35328;&#27491;&#30830;&#24615;&#65292;&#32570;&#20047;&#23545;&#29983;&#25104;&#31572;&#26696;&#30340;&#24494;&#22937;&#35780;&#20272;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15456v1 Announce Type: new  Abstract: Significant methodological strides have been made toward Chest X-ray (CXR) understanding via modern vision-language models (VLMs), demonstrating impressive Visual Question Answering (VQA) and CXR report generation abilities. However, existing CXR understanding frameworks still possess several procedural caveats. (1) Previous methods solely use CXR reports, which are insufficient for comprehensive Visual Question Answering (VQA), especially when additional health-related data like medication history and prior diagnoses are needed. (2) Previous methods use raw CXR reports, which are often arbitrarily structured. While modern language models can understand various text formats, restructuring reports for clearer, organized anatomy-based information could enhance their usefulness. (3) Current evaluation methods for CXR-VQA primarily emphasize linguistic correctness, lacking the capability to offer nuanced assessments of the generated answers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#36890;&#36807;&#24819;&#35937;&#21147;&#65292;&#32780;&#38750;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#65292;&#26469;&#34917;&#20805;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#30693;&#35782;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.15268</link><description>&lt;p&gt;
&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65306;&#23398;&#20064;&#24819;&#35937;&#26356;&#20016;&#23500;&#30340;&#32972;&#26223;&#26469;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#36890;&#36807;&#24819;&#35937;&#21147;&#65292;&#32780;&#38750;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#65292;&#26469;&#34917;&#20805;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#30693;&#35782;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#29983;&#25104;&#22686;&#24378;&#29983;&#25104;&#24050;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#25152;&#38656;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20381;&#36182;&#20110;&#22806;&#37096;&#36164;&#28304;&#65292;&#32780;&#19988;&#20004;&#32773;&#37117;&#38656;&#35201;&#23558;&#26174;&#24335;&#25991;&#26723;&#21512;&#24182;&#21040;&#19978;&#19979;&#25991;&#20013;&#65292;&#23548;&#33268;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#28040;&#32791;&#26356;&#22810;&#36164;&#28304;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#24050;&#32463;&#24314;&#27169;&#20102;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#23613;&#31649;&#27809;&#26377;&#34987;&#26377;&#25928;&#22320;&#35302;&#21457;&#25110;&#28608;&#27963;&#12290;&#22312;&#27492;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#23427;&#27169;&#25311;&#20102;&#20154;&#31867;&#36890;&#36807;&#24819;&#35937;&#21147;&#22312;&#20165;&#20973;&#24819;&#35937;&#22238;&#31572;&#38382;&#39064;&#26102;&#24357;&#34917;&#30693;&#35782;&#32570;&#38519;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#12290;&#22312;IAG&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#65292;&#36890;&#36807;&#20197;&#19979;&#20004;&#20010;&#27169;&#22359;&#33719;&#24471;&#26356;&#20016;&#23500;&#30340;&#32972;&#26223;&#65306;&#36890;&#36807;&#29983;&#25104;&#31616;&#21333;&#30340;&#24819;&#35937;&#23454;&#29616;&#26174;&#24335;&#24819;&#35937;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15268v1 Announce Type: new  Abstract: Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs). However, the former depends on external resources, and both require incorporating the explicit documents into the context, which results in longer contexts that lead to more resource consumption. Recent works indicate that LLMs have modeled rich knowledge, albeit not effectively triggered or activated. Inspired by this, we propose a novel knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which simulates the human capacity to compensate for knowledge deficits while answering questions solely through imagination, without relying on external resources. Guided by IAG, we propose an imagine richer context method for question answering (IMcQA), which obtains richer context through the following two modules: explicit imagination by generating a sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;QA&#26694;&#26550;&#65292;&#26681;&#25454;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#21160;&#24577;&#36873;&#25321;&#36866;&#21512;&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22238;&#31572;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14403</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#36890;&#36807;&#38382;&#39064;&#22797;&#26434;&#24230;&#23398;&#20064;&#35843;&#36866;
&lt;/p&gt;
&lt;p&gt;
Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14403
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;QA&#26694;&#26550;&#65292;&#26681;&#25454;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#21160;&#24577;&#36873;&#25321;&#36866;&#21512;&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22238;&#31572;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#30340;&#38750;&#21442;&#25968;&#30693;&#35782;&#32435;&#20837;LLMs&#65292;&#24050;&#25104;&#20026;&#25552;&#39640;&#22810;&#31181;&#20219;&#21153;&#20013;&#22238;&#31572;&#20934;&#30830;&#24615;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#65292;&#22914;&#38382;&#31572;&#65288;QA&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#21508;&#31181;&#26041;&#27861;&#22788;&#29702;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#26597;&#35810;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#22788;&#29702;&#31616;&#21333;&#26597;&#35810;&#26102;&#20135;&#29983;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#35201;&#20040;&#26410;&#33021;&#20805;&#20998;&#35299;&#20915;&#22797;&#26434;&#30340;&#22810;&#27493;&#26597;&#35810;&#65307;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#29992;&#25143;&#35831;&#27714;&#37117;&#21482;&#33021;&#21010;&#20998;&#20026;&#31616;&#21333;&#25110;&#22797;&#26434;&#31867;&#21035;&#20013;&#30340;&#19968;&#31181;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;QA&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21160;&#24577;&#36873;&#25321;&#20174;&#26368;&#31616;&#21333;&#21040;&#26368;&#22797;&#26434;&#30340;&#65288;&#26816;&#32034;&#22686;&#24378;&#65289;LLMs&#31574;&#30053;&#65292;&#36825;&#21462;&#20915;&#20110;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#36873;&#25321;&#36807;&#31243;&#26159;&#36890;&#36807;&#19968;&#20010;&#20998;&#31867;&#22120;&#23454;&#29616;&#30340;&#65292;&#35813;&#20998;&#31867;&#22120;&#26159;&#19968;&#20010;&#36739;&#23567;&#30340;LM&#65292;&#35757;&#32451;&#20197;&#39044;&#27979;&#20256;&#20837;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14403v1 Announce Type: cross  Abstract: Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework, that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with aut
&lt;/p&gt;</description></item><item><title>&#23558;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#35270;&#20026;&#21477;&#23376;&#24182;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#65292;&#35777;&#23454;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.12984</link><description>&lt;p&gt;
&#24403;SMILES&#25317;&#26377;&#35821;&#35328;&#65306;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#23545;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12984
&lt;/p&gt;
&lt;p&gt;
&#23558;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#35270;&#20026;&#21477;&#23376;&#24182;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#65292;&#35777;&#23454;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#21270;&#23398;&#32467;&#26500;&#65292;&#22914;&#33647;&#29289;&#65292;&#36890;&#24120;&#30001;SMILES&#23383;&#31526;&#20018;&#26469;&#23450;&#20041;&#65292;&#20316;&#20026;&#20998;&#23376;&#21644;&#38190;&#30340;&#24207;&#21015;&#12290;&#36825;&#20123;SMILES&#23383;&#31526;&#20018;&#22312;&#19981;&#21516;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20851;&#30740;&#31350;&#21644;&#34920;&#31034;&#24037;&#20316;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25670;&#33073;&#22797;&#26434;&#30340;&#34920;&#31034;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#26524;&#25105;&#20204;&#23558;&#33647;&#29289;SMILES&#35270;&#20026;&#24120;&#35268;&#21477;&#23376;&#65292;&#24182;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#20197;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#20250;&#24590;&#26679;&#65311;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#33719;&#24471;&#20102;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#25968;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27599;&#20010;&#21407;&#23376;&#21644;&#38190;&#35270;&#20026;&#21477;&#23376;&#32452;&#20214;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#22522;&#26412;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23545;&#33647;&#29289;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#34920;&#26126;&#22797;&#26434;&#30340;&#38382;&#39064;&#20063;&#21487;&#20197;&#29992;&#26356;&#31616;&#21333;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#25214;&#21040;&#65306;https://github.com/azminewasi/Drug-Classification-NLP&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12984v1 Announce Type: cross  Abstract: Complex chemical structures, like drugs, are usually defined by SMILES strings as a sequence of molecules and bonds. These SMILES strings are used in different complex machine learning-based drug-related research and representation works. Escaping from complex representation, in this work, we pose a single question: What if we treat drug SMILES as conventional sentences and engage in text classification for drug classification? Our experiments affirm the possibility with very competitive scores. The study explores the notion of viewing each atom and bond as sentence components, employing basic NLP methods to categorize drug types, proving that complex problems can also be solved with simpler perspectives. The data and code are available here: https://github.com/azminewasi/Drug-Classification-NLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10667</link><description>&lt;p&gt;
&#36890;&#21521;&#32479;&#19968;&#22810;&#27169;&#24335;&#20010;&#24615;&#21270;&#65306;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#25512;&#33616;&#21644;&#26356;&#22810;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#24322;&#26500;&#36164;&#28304;&#24182;&#28385;&#36275;&#21508;&#31181;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#36890;&#29992;&#27169;&#22411;&#19968;&#30452;&#26159;&#31038;&#21306;&#28212;&#26395;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#26085;&#24120;&#30340;&#36873;&#25321;&#65292;&#23588;&#20854;&#26159;&#22312;&#26102;&#23578;&#21644;&#38646;&#21806;&#31561;&#39046;&#22495;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#22270;&#29255;&#21644;&#25991;&#26412;&#25551;&#36848;&#12290;&#36825;&#20123;&#27169;&#24577;&#19981;&#20165;&#25552;&#20379;&#30452;&#35266;&#30340;&#25351;&#23548;&#65292;&#36824;&#36814;&#21512;&#20010;&#24615;&#21270;&#29992;&#25143;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20027;&#27969;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#20027;&#35201;&#32858;&#28966;&#20110;&#22522;&#20110;ID&#25110;&#25991;&#26412;&#30340;&#25512;&#33616;&#38382;&#39064;&#65292;&#26410;&#33021;&#29702;&#35299;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#25110;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#22522;&#30784;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10667v1 Announce Type: cross  Abstract: Developing a universal model that can effectively harness heterogeneous resources and respond to a wide range of personalized needs has been a longstanding community aspiration. Our daily choices, especially in domains like fashion and retail, are substantially shaped by multi-modal data, such as pictures and textual descriptions. These modalities not only offer intuitive guidance but also cater to personalized user preferences. However, the predominant personalization approaches mainly focus on the ID or text-based recommendation problem, failing to comprehend the information spanning various tasks or modalities. In this paper, our goal is to establish a Unified paradigm for Multi-modal Personalization systems (UniMP), which effectively leverages multi-modal data while eliminating the complexities associated with task- and modality-specific customization. We argue that the advancements in foundational generative modeling have provided
&lt;/p&gt;</description></item><item><title>FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.08059</link><description>&lt;p&gt;
FluoroSAM: &#29992;&#20110;X&#20809;&#22270;&#20687;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08059
&lt;/p&gt;
&lt;p&gt;
FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;X&#20809;&#22270;&#20687;&#20998;&#21106;&#23558;&#21152;&#36895;&#35786;&#26029;&#21644;&#20171;&#20837;&#31934;&#20934;&#21307;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#22270;&#20687;&#20998;&#26512;&#38382;&#39064;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#25928;&#29992;&#21463;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#39046;&#22495;&#65292;&#35201;&#25299;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#21017;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#12289;&#26631;&#31614;&#21644;&#37325;&#26032;&#35757;&#32451;&#24037;&#20316;&#12290;&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289; - &#35757;&#32451;&#22312;&#22823;&#37327;&#39640;&#24230;&#21464;&#21270;&#25968;&#25454;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22240;&#27492;&#20351;&#24471;&#24191;&#27867;&#36866;&#29992;&#24615;&#25104;&#20026;&#21487;&#33021; - &#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#30340;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;FMs&#32858;&#28966;&#20110;&#23545;&#35937;&#34987;&#26126;&#26174;&#21487;&#35265;&#36793;&#30028;&#28165;&#26224;&#23450;&#20041;&#30340;&#22330;&#26223;&#21644;&#27169;&#24335;&#65292;&#22914;&#20869;&#31397;&#38236;&#25163;&#26415;&#24037;&#20855;&#20998;&#21106;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;X&#20809;&#25104;&#20687;&#36890;&#24120;&#27809;&#26377;&#25552;&#20379;&#36825;&#31181;&#28165;&#26224;&#30340;&#36793;&#30028;&#25110;&#32467;&#26500;&#20808;&#39564;&#12290;&#22312;X&#20809;&#22270;&#20687;&#24418;&#25104;&#26399;&#38388;&#65292;&#22797;&#26434;&#30340;&#19977;&#32500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08059v1 Announce Type: cross  Abstract: Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts. Recently, foundation models (FMs) -- machine learning models trained on large amounts of highly variable data thus enabling broad applicability -- have emerged as promising tools for automated image analysis. Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors. During X-ray image formation, complex 3D
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#36164;&#28304;&#38656;&#27714;&#30340;&#38590;&#39064;&#65292;&#20351;&#20854;&#33021;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#30340;&#20986;&#33394;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04260</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#25104;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#33391;&#22909;&#25512;&#29702;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Small Language Models be Good Reasoners for Sequential Recommendation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#36164;&#28304;&#38656;&#27714;&#30340;&#38590;&#39064;&#65292;&#20351;&#20854;&#33021;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#30340;&#20986;&#33394;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#24320;&#25299;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#35201;&#25104;&#21151;&#23454;&#29616;&#30001;LLMs&#36171;&#33021;&#30340;&#39034;&#24207;&#25512;&#33616;&#36824;&#26377;&#35768;&#22810;&#25361;&#25112;&#38656;&#35201;&#35299;&#20915;&#12290;&#39318;&#20808;&#65292;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#36890;&#24120;&#22797;&#26434;&#65292;&#20165;&#20165;&#20381;&#38752;LLMs&#30340;&#19968;&#27493;&#25512;&#29702;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#25110;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#21709;&#24212;&#12290;&#20854;&#27425;&#65292;LLMs&#65288;&#20363;&#22914;ChatGPT-175B&#65289;&#26497;&#39640;&#30340;&#36164;&#28304;&#38656;&#27714;&#26159;&#38590;&#20197;&#25215;&#21463;&#19988;&#22312;&#23454;&#38469;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#29992;&#20110;&#25512;&#33616;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#22120;&#20197;&#8220;&#30246;&#8221;&#65288;&#21363;&#36164;&#28304;&#39640;&#25928;&#65289;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#38138;&#24179;&#20102;&#19968;&#26465;&#26377;&#21069;&#36884;&#30340;&#36947;&#36335;&#12290;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;CoT&#25552;&#31034;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04260v1 Announce Type: cross  Abstract: Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larg
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;OffLanDat&#65292;&#20026;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#25552;&#20379;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.02472</link><description>&lt;p&gt;
OffLanDat&#65306;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02472
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;OffLanDat&#65292;&#20026;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#25552;&#20379;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#25915;&#20987;&#24615;&#35821;&#35328;&#30340;&#26222;&#36941;&#23384;&#22312;&#23545;&#31038;&#20250;&#31119;&#31049;&#20135;&#29983;&#20102;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#39640;&#24230;&#37325;&#35270;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#25915;&#20987;&#24615;&#35821;&#35328;&#26082;&#23384;&#22312;&#26126;&#30830;&#24418;&#24335;&#65292;&#20063;&#23384;&#22312;&#38544;&#24335;&#24418;&#24335;&#65292;&#21518;&#32773;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#24403;&#21069;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#36935;&#21040;&#20960;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20027;&#35201;&#20381;&#36182;&#20110;&#25910;&#38598;&#21253;&#21547;&#26126;&#30830;&#25915;&#20987;&#24615;&#20851;&#38190;&#35789;&#30340;&#25991;&#26412;&#65292;&#36825;&#20351;&#24471;&#25429;&#25417;&#19981;&#21253;&#21547;&#36825;&#20123;&#20851;&#38190;&#35789;&#19988;&#38544;&#21547;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#27425;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#35770;&#20542;&#21521;&#20110;&#20165;&#20851;&#27880;&#25991;&#26412;&#20998;&#26512;&#65292;&#24573;&#35270;&#31038;&#21306;&#20449;&#24687;&#21487;&#20197;&#25552;&#20379;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#22312;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OffLanDat&#65292;&#36825;&#26159;&#30001;ChatGPT&#29983;&#25104;&#30340;&#22522;&#20110;&#31038;&#21306;&#30340;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02472v1 Announce Type: new  Abstract: The widespread presence of offensive languages on social media has resulted in adverse effects on societal well-being. As a result, it has become very important to address this issue with high priority. Offensive languages exist in both explicit and implicit forms, with the latter being more challenging to detect. Current research in this domain encounters several challenges. Firstly, the existing datasets primarily rely on the collection of texts containing explicit offensive keywords, making it challenging to capture implicitly offensive contents that are devoid of these keywords. Secondly, usual methodologies tend to focus solely on textual analysis, neglecting the valuable insights that community information can provide. In this research paper, we introduce a novel dataset OffLanDat, a community based implicit offensive language dataset generated by ChatGPT containing data for 38 different target groups. Despite limitations in genera
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#26032;&#30340;QA&#25968;&#25454;&#38598;WiTQA&#65292;&#20197;&#23454;&#20307;&#21644;&#20851;&#31995;&#32452;&#21512;&#30340;&#24433;&#21709;&#20026;&#37325;&#28857;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.13492</link><description>&lt;p&gt;
&#12298;&#26816;&#32034;&#26159;&#26377;&#30410;&#36824;&#26159;&#26377;&#23475;&#65311;&#28145;&#20837;&#25506;&#35752;&#26816;&#32034;&#22686;&#24378;&#23545;&#35821;&#35328;&#27169;&#22411;&#25928;&#26524;&#30340;&#24433;&#21709;&#12299;
&lt;/p&gt;
&lt;p&gt;
Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#26032;&#30340;QA&#25968;&#25454;&#38598;WiTQA&#65292;&#20197;&#23454;&#20307;&#21644;&#20851;&#31995;&#32452;&#21512;&#30340;&#24433;&#21709;&#20026;&#37325;&#28857;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#38656;&#35201;&#26597;&#35810;&#20854;&#39044;&#35757;&#32451;&#35760;&#24518;&#20043;&#22806;&#30340;&#20449;&#24687;&#26102;&#65292;&#23427;&#20204;&#22312;&#25552;&#20379;&#20934;&#30830;&#22238;&#31572;&#26102;&#20250;&#36935;&#21040;&#25361;&#25112;&#12290;&#34429;&#28982;&#21033;&#29992;&#30456;&#20851;&#22806;&#37096;&#20449;&#24687;&#26469;&#22686;&#24378;&#23427;&#20204;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#26410;&#32771;&#34385;&#26816;&#32034;&#30340;&#24517;&#35201;&#24615;&#21487;&#33021;&#20250;&#23545;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#27492;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23454;&#20307;&#22914;&#20309;&#24433;&#21709;&#26816;&#32034;&#27169;&#22411;&#19982;LMs&#20013;&#30340;&#30693;&#35782;&#22238;&#24518;&#65292;&#20854;&#20182;&#26041;&#38754;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#23454;&#20307;&#21644;&#20851;&#31995;&#32452;&#21512;&#30340;&#24433;&#21709;&#26469;&#25552;&#20379;&#26356;&#35814;&#32454;&#12289;&#20197;&#20107;&#23454;&#20026;&#20013;&#24515;&#30340;&#20998;&#26512;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;WiTQA&#65288;Wikipedia Triple Question Answers&#65289;&#30340;&#26032;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#12290;&#27492;&#25968;&#25454;&#38598;&#21253;&#25324;&#20851;&#20110;&#19981;&#21516;&#21463;&#27426;&#36814;&#31243;&#24230;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#38468;&#24102;&#19968;&#27573;&#25903;&#25345;&#24615;&#27573;&#33853;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13492v1 Announce Type: new  Abstract: While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of retrieval may adversely affect overall performance. Previous research has primarily focused on examining how entities influence retrieval models and knowledge recall in LMs, leaving other aspects relatively unexplored. In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations. To facilitate this, we construct a new question answering (QA) dataset called WiTQA (Wikipedia Triple Question Answers). This dataset includes questions about entities and relations of various popularity levels, each accompanied by a supporting passage. Our extensive ex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11815</link><description>&lt;p&gt;
HU&#22312;SemEval-2024&#20219;&#21153;8A&#20013;&#30340;&#34920;&#29616;&#65306;&#23545;&#27604;&#23398;&#20064;&#33021;&#21542;&#23398;&#20064;&#23884;&#20837;&#20197;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11815
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2024&#20219;&#21153;8&#8220;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#8221;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#34394;&#20551;&#25991;&#26412;&#29983;&#25104;&#12289;&#32593;&#32476;&#38035;&#40060;&#12289;&#32771;&#35797;&#20316;&#24330;&#29978;&#33267;&#25220;&#34989;&#29256;&#26435;&#26448;&#26009;&#20013;&#30340;&#20351;&#29992;&#65292;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19968;&#30452;&#26159;&#20027;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#35768;&#22810;&#31995;&#32479;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#20999;&#23454;&#38469;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#36890;&#24120;&#19981;&#21487;&#33021;&#30693;&#36947;&#29992;&#25143;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#20855;&#20307;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#20854;&#20351;&#29992;&#22522;&#32447;&#21442;&#25968;&#30340;&#22823;&#32422;40%&#65288;149M&#27604;355M&#65289;&#65292;&#20294;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21487;&#27604;&#30340;&#24615;&#33021;&#65288;&#22312;137&#20010;&#21442;&#19982;&#32773;&#20013;&#25490;&#21517;&#31532;21&#65289;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;&#21363;&#20351;&#27809;&#26377;&#22810;&#20010;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11815v1 Announce Type: cross  Abstract: This paper describes our system developed for SemEval-2024 Task 8, "Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection." Machine-generated texts have been one of the main concerns due to the use of large language models (LLM) in fake text generation, phishing, cheating in exams, or even plagiarizing copyright materials. A lot of systems have been developed to detect machine-generated text. Nonetheless, the majority of these systems rely on the text-generating model, a limitation that is impractical in real-world scenarios, as it's often impossible to know which specific model the user has used for text generation. In this work, we propose a single model based on contrastive learning, which uses ~40% of the baseline's parameters (149M vs. 355M) but shows a comparable performance on the test dataset (21st out of 137 participants). Our key finding is that even without an ensemble of multiple models, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#36235;&#21183;&#65292;&#20351;&#29992;&#35758;&#20250;&#36777;&#35770;&#35821;&#26009;&#24211;&#65292;&#25506;&#35752;&#20102;&#21477;&#27861;&#20381;&#23384;&#36317;&#31163;&#26368;&#23567;&#21270;&#21450;&#22522;&#20110;&#26641;&#22270;&#23646;&#24615;&#30340;15&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#25581;&#31034;&#20102;&#29616;&#20195;&#35299;&#26512;&#22120;&#22312;&#36825;&#31181;&#21464;&#21270;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11549</link><description>&lt;p&gt;
&#33521;&#35821;&#21644;&#24503;&#35821;&#30340;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#65306;&#24230;&#37327;&#12289;&#35299;&#26512;&#22120;&#21644;&#36235;&#21516;
&lt;/p&gt;
&lt;p&gt;
Syntactic Language Change in English and German: Metrics, Parsers, and Convergences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#36235;&#21183;&#65292;&#20351;&#29992;&#35758;&#20250;&#36777;&#35770;&#35821;&#26009;&#24211;&#65292;&#25506;&#35752;&#20102;&#21477;&#27861;&#20381;&#23384;&#36317;&#31163;&#26368;&#23567;&#21270;&#21450;&#22522;&#20110;&#26641;&#22270;&#23646;&#24615;&#30340;15&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#25581;&#31034;&#20102;&#29616;&#20195;&#35299;&#26512;&#22120;&#22312;&#36825;&#31181;&#21464;&#21270;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#35821;&#35328;&#24448;&#24448;&#20250;&#20248;&#21270;&#35821;&#35328;&#32467;&#26500;&#20197;&#38477;&#20302;&#22797;&#26434;&#24615;&#65292;&#22686;&#21152;&#20132;&#27969;&#25928;&#29575;&#12290;&#21477;&#27861;&#20381;&#23384;&#36317;&#31163;&#34913;&#37327;&#20102;&#30456;&#20851;&#35789;&#27719;&#20043;&#38388;&#30340;&#32447;&#24615;&#36317;&#31163;&#65292;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#35821;&#35328;&#22788;&#29702;&#22256;&#38590;&#21644;&#24037;&#20316;&#35760;&#24518;&#36127;&#33655;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#30340;&#21382;&#26102;&#36235;&#21183;&#65292;&#20351;&#29992;&#20102;&#36807;&#21435;&#22823;&#32422;160&#24180;&#38388;&#30340;&#35758;&#20250;&#36777;&#35770;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#22522;&#20110;5&#20010;&#20381;&#23384;&#21477;&#27861;&#35299;&#26512;&#22120;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#21253;&#25324;&#24191;&#27867;&#20351;&#29992;&#30340;Stanford CoreNLP&#20197;&#21450;&#20854;&#20182;4&#20010;&#26356;&#26032;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#20998;&#26512;&#36229;&#36234;&#20102;&#32447;&#24615;&#20381;&#23384;&#36317;&#31163;&#65292;&#25506;&#35752;&#20102;&#19982;&#20381;&#23384;&#36317;&#31163;&#26368;&#23567;&#21270;&#65288;DDM&#65289;&#30456;&#20851;&#30340;15&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#25110;&#32773;&#22522;&#20110;&#26641;&#22270;&#23646;&#24615;&#65292;&#27604;&#22914;&#26641;&#39640;&#21644;&#24230;&#21464;&#24322;&#12290;&#23613;&#31649;&#25105;&#20204;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#26368;&#36817;&#22522;&#20110;&#29616;&#20195;&#26641;&#24211;&#35757;&#32451;&#30340;&#35299;&#26512;&#22120;&#24182;&#26410;&#21463;&#21040;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11549v1 Announce Type: cross  Abstract: Many studies have shown that human languages tend to optimize for lower complexity and increased communication efficiency. Syntactic dependency distance, which measures the linear distance between dependent words, is often considered a key indicator of language processing difficulty and working memory load. The current paper looks at diachronic trends in syntactic language change in both English and German, using corpora of parliamentary debates from the last c. 160 years. We base our observations on five dependency parsers, including the widely used Stanford CoreNLP as well as 4 newer alternatives. Our analysis of syntactic language change goes beyond linear dependency distance and explores 15 metrics relevant to dependency distance minimization (DDM) and/or based on tree graph properties, such as the tree height and degree variance. Even though we have evidence that recent parsers trained on modern treebanks are not heavily affected 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#19977;&#20010;&#25104;&#29087;&#30340;&#20154;&#31867;&#20915;&#31574;&#29702;&#35770;&#25972;&#21512;&#21040;&#19968;&#36215;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#30446;&#30340;&#24615;&#20154;&#31867;&#34892;&#21160;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#34892;&#21160;&#30340;&#35266;&#28857;&#24212;&#29992;&#20110;&#23545;&#35805;&#29992;&#25143;&#30028;&#38754;&#12290;&#36890;&#36807;&#29702;&#35299;ChatGPT&#30340;&#26234;&#33021;&#26469;&#28304;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#36164;&#28304;&#30340;&#21516;&#26102;&#33719;&#24471;&#23545;&#25105;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#35748;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.08403</link><description>&lt;p&gt;
LLMs&#21644;&#20154;&#31867;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
LLMs and the Human Condition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#19977;&#20010;&#25104;&#29087;&#30340;&#20154;&#31867;&#20915;&#31574;&#29702;&#35770;&#25972;&#21512;&#21040;&#19968;&#36215;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#30446;&#30340;&#24615;&#20154;&#31867;&#34892;&#21160;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#34892;&#21160;&#30340;&#35266;&#28857;&#24212;&#29992;&#20110;&#23545;&#35805;&#29992;&#25143;&#30028;&#38754;&#12290;&#36890;&#36807;&#29702;&#35299;ChatGPT&#30340;&#26234;&#33021;&#26469;&#28304;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#36164;&#28304;&#30340;&#21516;&#26102;&#33719;&#24471;&#23545;&#25105;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20154;&#31867;&#20915;&#31574;&#30340;&#19977;&#20010;&#25104;&#29087;&#29702;&#35770;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#23427;&#20204;&#25972;&#21512;&#36215;&#26469;&#25552;&#20379;&#19968;&#20010;&#30446;&#30340;&#24615;&#20154;&#31867;&#34892;&#21160;&#30340;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#34892;&#21160;&#30340;&#35266;&#28857;&#24212;&#29992;&#20110;&#23545;&#35805;&#29992;&#25143;&#30028;&#38754;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29702;&#35770;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#26412;&#25991;&#26088;&#22312;&#37325;&#26032;&#28608;&#21457;&#23545;&#29702;&#35299;LLMs&#23454;&#38469;&#25191;&#34892;&#30340;&#20852;&#36259;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#25152;&#26377;&#25968;&#25454;&#19978;&#36816;&#34892;&#38590;&#20197;&#29702;&#35299;&#30340;&#26426;&#22120;&#23398;&#20064;&#20363;&#31243;&#12290;&#24403;&#19968;&#21488;&#21806;&#20215;&#19981;&#21040;50&#32654;&#20803;&#30340;&#26641;&#33683;&#27966;&#30005;&#33041;&#27604;&#31532;&#19968;&#21488;&#21830;&#19994;Cray&#36229;&#32423;&#35745;&#31639;&#26426;&#24555;400&#20493;&#26102;&#65292;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#21487;&#20197;&#25509;&#36817;&#25317;&#26377;&#26080;&#25968;&#38543;&#26426;&#25171;&#23383;&#24182;&#29983;&#25104;&#26377;&#24847;&#20041;&#25991;&#23383;&#30340;&#29492;&#23376;&#12290;&#36890;&#36807;&#29702;&#35299;ChatGPT&#30340;&#34920;&#29616;&#26234;&#33021;&#30340;&#26469;&#28304;&#65292;&#20063;&#35768;&#25105;&#20204;&#21487;&#20197;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#36827;&#34892;&#21516;&#26679;&#30340;&#39764;&#26415;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#33719;&#24471;&#19968;&#20123;&#20851;&#20110;&#25105;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents three established theories of human decision-making and describes how they can be integrated to provide a model of purposive human action. Taking seriously the idea of language as action the model is then applied to the conversational user interfaces. Theory based AI research has had a hard time recently and the aim here is to revitalise interest in understanding what LLMs are actually doing other than running poorly understood machine learning routines over all the data the relevant Big Tech company can hoover up. When a raspberry pi computer for under 50USD is up to 400 times faster than the first commercial Cray super computer~\cite{crayVpi}, Big Tech can get really close to having an infinite number of monkeys typing at random and producing text, some of which will make sense. By understanding where ChatGPT's apparent intelligence comes from, perhaps we can perform the magic with fewer resources and at the same time gain some understanding about our relationship
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#26500;&#24819;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#38656;&#35201;&#38754;&#23545;&#26356;&#22797;&#26434;&#21644;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#30340;&#24895;&#26223;&#12290;&#36825;&#20010;&#24895;&#26223;&#30340;&#26680;&#24515;&#26159;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.07946</link><description>&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#25351;&#25381;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Re-Envisioning Command and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07946
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#38656;&#35201;&#38754;&#23545;&#26356;&#22797;&#26434;&#21644;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#30340;&#24895;&#26223;&#12290;&#36825;&#20010;&#24895;&#26223;&#30340;&#26680;&#24515;&#26159;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#25112;&#20105;&#23558;&#35201;&#27714;&#22312;&#26356;&#22797;&#26434;&#12289;&#24555;&#33410;&#22863;&#12289;&#19981;&#32467;&#26500;&#21270;&#21644;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#12290;C2&#23558;&#22240;&#34987;&#25298;&#32477;&#12289;&#36864;&#21270;&#12289;&#38388;&#27463;&#21644;&#26377;&#38480;&#30340;&#36890;&#20449;&#20197;&#21450;&#38656;&#35201;&#32771;&#34385;&#21040;&#22810;&#20010;&#20316;&#25112;&#39046;&#22495;&#20013;&#30340;&#35768;&#22810;&#25968;&#25454;&#27969;&#32780;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;C2&#23454;&#36341;&#8212;&#8212;&#28304;&#33258;&#24037;&#19994;&#26102;&#20195;&#32780;&#38750;&#26032;&#20852;&#30340;&#26234;&#33021;&#26102;&#20195;&#8212;&#8212;&#26159;&#32447;&#24615;&#30340;&#19988;&#32791;&#26102;&#12290;&#32780;&#19988;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#26410;&#26469;&#25112;&#22330;&#19978;&#19982;&#23545;&#25163;&#20445;&#25345;&#20248;&#21183;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#19982;&#20154;&#31867;&#20043;&#38388;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#24895;&#26223;&#12290;&#36825;&#20010;&#26410;&#26469;&#24895;&#26223;&#20307;&#29616;&#22312;&#19977;&#20010;&#36816;&#33829;&#24433;&#21709;&#19978;&#65306;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#20197;&#21450;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25152;&#35774;&#24819;&#30340;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future warfare will require Command and Control (C2) decision-making to occur in more complex, fast-paced, ill-structured, and demanding conditions. C2 will be further complicated by operational challenges such as Denied, Degraded, Intermittent, and Limited (DDIL) communications and the need to account for many data streams, potentially across multiple domains of operation. Yet, current C2 practices -- which stem from the industrial era rather than the emerging intelligence era -- are linear and time-consuming. Critically, these approaches may fail to maintain overmatch against adversaries on the future battlefield. To address these challenges, we propose a vision for future C2 based on robust partnerships between humans and artificial intelligence (AI) systems. This future vision is encapsulated in three operational impacts: streamlining the C2 operations process, maintaining unity of effort, and developing adaptive collective knowledge systems. This paper illustrates the envisaged fu
&lt;/p&gt;</description></item><item><title>&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#65292;&#20197;&#25552;&#39640;C2&#36816;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06501</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Scalable Interactive Machine Learning for Future Command and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06501
&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#65292;&#20197;&#25552;&#39640;C2&#36816;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#37492;&#20110;&#38656;&#35201;&#24378;&#22823;&#30340;&#20915;&#31574;&#36807;&#31243;&#21644;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#30340;&#38598;&#25104;&#20855;&#26377;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;C2&#36816;&#20316;&#27969;&#31243;&#30340;&#28508;&#21147;&#65292;&#20197;&#30830;&#20445;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#26368;&#36817;&#22312;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#30340;&#31361;&#30772;&#65292;&#20154;&#31867;&#21487;&#20197;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21512;&#20316;&#20197;&#25351;&#23548;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#30446;&#21069;&#31185;&#25216;&#21457;&#23637;&#20013;&#23384;&#22312;&#30340;&#20960;&#20010;&#24046;&#36317;&#65292;&#26410;&#26469;&#30340;&#24037;&#20316;&#24212;&#35813;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#65292;&#20197;&#25193;&#23637;&#36825;&#20123;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;C2&#29615;&#22659;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19977;&#20010;&#30740;&#31350;&#37325;&#28857;&#39046;&#22495;&#65292;&#20849;&#21516;&#26088;&#22312;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;SIML&#65289;&#65306;1&#65289;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#31639;&#27861;&#20197;&#23454;&#29616;&#21327;&#21516;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future warfare will require Command and Control (C2) personnel to make decisions at shrinking timescales in complex and potentially ill-defined situations. Given the need for robust decision-making processes and decision-support tools, integration of artificial and human intelligence holds the potential to revolutionize the C2 operations process to ensure adaptability and efficiency in rapidly changing operational environments. We propose to leverage recent promising breakthroughs in interactive machine learning, in which humans can cooperate with machine learning algorithms to guide machine learning algorithm behavior. This paper identifies several gaps in state-of-the-art science and technology that future work should address to extend these approaches to function in complex C2 contexts. In particular, we describe three research focus areas that together, aim to enable scalable interactive machine learning (SIML): 1) developing human-AI interaction algorithms to enable planning in co
&lt;/p&gt;</description></item><item><title>COA-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#30340;&#31639;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#22312;&#20891;&#20107;&#28216;&#25103;&#20013;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#24555;&#36895;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;COAs&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.01786</link><description>&lt;p&gt;
COA-GPT&#65306;&#29992;&#20110;&#20891;&#20107;&#34892;&#21160;&#20013;&#21152;&#36895;&#34892;&#21160;&#26041;&#26696;&#24320;&#21457;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01786
&lt;/p&gt;
&lt;p&gt;
COA-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#30340;&#31639;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#22312;&#20891;&#20107;&#28216;&#25103;&#20013;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#24555;&#36895;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;COAs&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20891;&#20107;&#34892;&#21160;&#20013;&#34892;&#21160;&#26041;&#26696;&#65288;COAs&#65289;&#30340;&#24320;&#21457;&#20256;&#32479;&#19978;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#22797;&#26434;&#30340;&#36807;&#31243;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;COA-GPT&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;COAs&#30340;&#26032;&#31639;&#27861;&#12290;COA-GPT&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23558;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#34701;&#20837;&#21040;LLMs&#20013;&#65292;&#20801;&#35768;&#25351;&#25381;&#23448;&#36755;&#20837;&#20219;&#21153;&#20449;&#24687;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#20687;&#26684;&#24335;&#65289;&#65292;&#24182;&#33719;&#24471;&#19982;&#25112;&#30053;&#23545;&#40784;&#30340;COAs&#20197;&#20379;&#23457;&#26597;&#21644;&#25209;&#20934;&#12290;&#29420;&#29305;&#30340;&#26159;&#65292;COA-GPT&#19981;&#20165;&#21152;&#36895;&#20102;COA&#30340;&#24320;&#21457;&#65292;&#22312;&#20960;&#31186;&#38047;&#20869;&#29983;&#25104;&#21021;&#22987;COAs&#65292;&#36824;&#33021;&#26681;&#25454;&#25351;&#25381;&#23448;&#30340;&#21453;&#39304;&#23454;&#26102;&#31934;&#32454;&#21270;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#22312;&#12298;&#26143;&#38469;&#20105;&#38712;II&#12299;&#28216;&#25103;&#30340;&#20891;&#20107;&#30456;&#20851;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;COA-GPT&#65292;&#23558;&#20854;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;COA-GPT&#22312;&#26356;&#24555;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;&#30340;COAs&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#32422;&#26463;&#21644;&#30001;&#27492;&#34893;&#29983;&#30340;&#32452;&#21512;&#25512;&#29702;&#65292;&#21487;&#20197;&#36807;&#28388;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#19968;&#33268;&#32467;&#26500;&#65292;&#20174;&#32780;&#26500;&#24314;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.06877</link><description>&lt;p&gt;
&#21450;&#26102;&#39044;&#27979;&#32467;&#26500;&#65306;&#25512;&#29702;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Promptly Predicting Structures: The Return of Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#32422;&#26463;&#21644;&#30001;&#27492;&#34893;&#29983;&#30340;&#32452;&#21512;&#25512;&#29702;&#65292;&#21487;&#20197;&#36807;&#28388;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#19968;&#33268;&#32467;&#26500;&#65292;&#20174;&#32780;&#26500;&#24314;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#26500;&#24314;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26631;&#31614;&#39044;&#27979;&#22120;&#12290;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20855;&#26377;&#32467;&#26500;&#29305;&#28857;&#65306;&#21363;&#23427;&#20204;&#30340;&#36755;&#20986;&#30001;&#22810;&#20010;&#30456;&#20114;&#32422;&#26463;&#30340;&#26631;&#31614;&#32452;&#25104;&#12290;&#20026;&#36825;&#31867;&#20219;&#21153;&#26631;&#27880;&#25968;&#25454;&#21487;&#33021;&#20250;&#24456;&#32321;&#29712;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#33539;&#24335;&#33021;&#21542;&#25193;&#23637;&#21040;&#36825;&#31181;&#32467;&#26500;&#21270;&#36755;&#20986;&#20219;&#21153;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26500;&#24314;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35821;&#35328;&#32467;&#26500;&#39044;&#27979;&#22120;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#32467;&#26500;&#32422;&#26463;&#21644;&#20174;&#20013;&#24471;&#20986;&#30340;&#32452;&#21512;&#25512;&#29702;&#26469;&#36807;&#28388;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#19968;&#33268;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#32467;&#26500;&#21270;&#39044;&#27979;&#20219;&#21153;&#21644;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#20363;&#21270;&#20102;&#36825;&#20010;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#24378;&#21046;&#23454;&#26045;&#19968;&#33268;&#24615;&#19981;&#20165;&#26500;&#36896;&#20102;&#32467;&#26500;&#26377;&#25928;&#30340;&#36755;&#20986;&#65292;&#32780;&#19988;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#19981;&#21463;&#32422;&#26463;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06877v2 Announce Type: replace  Abstract: Prompt-based methods have been used extensively across NLP to build zero- and few-shot label predictors. Many NLP tasks are naturally structured: that is, their outputs consist of multiple labels which constrain each other. Annotating data for such tasks can be cumbersome. Can the promise of the prompt-based paradigm be extended to such structured outputs? In this paper, we present a framework for constructing zero- and few-shot linguistic structure predictors. Our key insight is that we can use structural constraints -- and combinatorial inference derived from them -- to filter out inconsistent structures predicted by large language models. We instantiated this framework on two structured prediction tasks, and five datasets. Across all cases, our results show that enforcing consistency not only constructs structurally valid outputs, but also improves performance over the unconstrained variants.
&lt;/p&gt;</description></item><item><title>TimeChat&#26159;&#19968;&#31181;&#26102;&#38388;&#25935;&#24863;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#21547;&#26102;&#38388;&#25139;&#24863;&#30693;&#24103;&#32534;&#30721;&#22120;&#21644;&#28369;&#21160;&#35270;&#39057;Q-Former&#65292;&#20197;&#23454;&#29616;&#23545;&#38271;&#35270;&#39057;&#36827;&#34892;&#24378;&#22823;&#30340;&#38646;-shot&#26102;&#38388;&#26412;&#22320;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2312.02051</link><description>&lt;p&gt;
TimeChat&#65306;&#19968;&#31181;&#38754;&#21521;&#38271;&#35270;&#39057;&#29702;&#35299;&#30340;&#26102;&#38388;&#25935;&#24863;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02051
&lt;/p&gt;
&lt;p&gt;
TimeChat&#26159;&#19968;&#31181;&#26102;&#38388;&#25935;&#24863;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#21547;&#26102;&#38388;&#25139;&#24863;&#30693;&#24103;&#32534;&#30721;&#22120;&#21644;&#28369;&#21160;&#35270;&#39057;Q-Former&#65292;&#20197;&#23454;&#29616;&#23545;&#38271;&#35270;&#39057;&#36827;&#34892;&#24378;&#22823;&#30340;&#38646;-shot&#26102;&#38388;&#26412;&#22320;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;TimeChat&#65292;&#19968;&#31181;&#19987;&#38376;&#20026;&#38271;&#35270;&#39057;&#29702;&#35299;&#35774;&#35745;&#30340;&#26102;&#38388;&#25935;&#24863;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290; &#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#30340;&#26550;&#26500;&#36129;&#29486;&#65306;(1)&#19968;&#20010;&#26102;&#38388;&#25139;&#24863;&#30693;&#24103;&#32534;&#30721;&#22120;&#65292;&#23558;&#35270;&#35273;&#20869;&#23481;&#19982;&#27599;&#24103;&#30340;&#26102;&#38388;&#25139;&#32465;&#23450;&#22312;&#19968;&#36215;&#65307;(2)&#19968;&#20010;&#28369;&#21160;&#35270;&#39057;Q-Former&#65292;&#29983;&#25104;&#21508;&#31181;&#38271;&#24230;&#30340;&#35270;&#39057;&#20196;&#29260;&#24207;&#21015;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#25345;&#32493;&#26102;&#38388;&#30340;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;6&#20010;&#20219;&#21153;&#21644;&#24635;&#35745;125K&#20010;&#23454;&#20363;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;TimeChat&#22312;&#36981;&#24490;&#25351;&#20196;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#22914;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;&#12289;&#26102;&#38388;&#23450;&#20301;&#21644;&#31934;&#24425;&#29255;&#27573;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;TimeChat&#24378;&#22823;&#30340;&#38646;-shot&#26102;&#38388;&#26412;&#22320;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#23427;&#22312;YouCook2&#19978;&#23454;&#29616;&#20102;+9.2&#30340;F1&#20998;&#25968;&#21644;+2.8&#30340;CIDEr&#65292;&#22312;QVHighlights&#19978;&#23454;&#29616;&#20102;+5.8&#30340;HIT@1&#65292;&#22312;Cha&#19978;&#23454;&#29616;&#20102;+27.5&#30340;R@1&#65288;IoU=0.5&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02051v2 Announce Type: replace-cross  Abstract: This work proposes TimeChat, a time-sensitive multimodal large language model specifically designed for long video understanding. Our model incorporates two key architectural contributions: (1) a timestamp-aware frame encoder that binds visual content with the timestamp of each frame, and (2) a sliding video Q-Former that produces a video token sequence of varying lengths to accommodate videos of various durations. Additionally, we construct an instruction-tuning dataset, encompassing 6 tasks and a total of 125K instances, to further enhance TimeChat's instruction-following performance. Experiment results across various video understanding tasks, such as dense captioning, temporal grounding, and highlight detection, demonstrate TimeChat's strong zero-shot temporal localization and reasoning capabilities. For example, it achieves +9.2 F1 score and +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5) on Cha
&lt;/p&gt;</description></item><item><title>EgoThink&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#35273;&#38382;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20174;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#8220;&#24605;&#32771;&#8221;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.15596</link><description>&lt;p&gt;
EgoThink: &#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#31532;&#19968;&#35270;&#35282;&#24605;&#32500;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15596
&lt;/p&gt;
&lt;p&gt;
EgoThink&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#35273;&#38382;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20174;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#8220;&#24605;&#32771;&#8221;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#20256;&#32479;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#35780;&#20272;&#30740;&#31350;&#24050;&#32463;&#20986;&#29616;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#65292;&#20294;&#22823;&#22810;&#25968;&#38598;&#20013;&#22312;&#31532;&#19977;&#20154;&#31216;&#35270;&#35282;&#65292;&#21482;&#26377;&#24456;&#23569;&#28041;&#21450;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#30340;&#29305;&#23450;&#20219;&#21153;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;EgoThink&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#20845;&#20010;&#26680;&#24515;&#33021;&#21147;&#21644;&#21313;&#20108;&#20010;&#35814;&#32454;&#32500;&#24230;&#30340;&#26032;&#39062;&#35270;&#35273;&#38382;&#31572;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#26159;&#20351;&#29992;&#36873;&#23450;&#30340;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#29255;&#27573;&#26500;&#24314;&#30340;&#65292;&#20854;&#20013;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#21547;&#31532;&#19968;&#20154;&#31216;&#20449;&#24687;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;VLMs&#65292;&#25105;&#20204;&#22312;EgoThink&#19978;&#35780;&#20272;&#20102;&#21313;&#20843;&#31181;&#27969;&#34892;&#30340;VLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15596v2 Announce Type: replace-cross  Abstract: Vision-language models (VLMs) have recently shown promising results in traditional downstream tasks. Evaluation studies have emerged to assess their abilities, with the majority focusing on the third-person perspective, and only a few addressing specific tasks from the first-person perspective. However, the capability of VLMs to "think" from a first-person perspective, a crucial attribute for advancing autonomous agents and robotics, remains largely unexplored. To bridge this research gap, we introduce EgoThink, a novel visual question-answering benchmark that encompasses six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from egocentric videos, with manually annotated question-answer pairs containing first-person information. To comprehensively assess VLMs, we evaluate eighteen popular VLMs on EgoThink. Moreover, given the open-ended format of the answers, we use GPT-4 as t
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#39118;&#38505;&#25511;&#21046;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#20449;&#24687;&#39118;&#38505;&#24230;&#37327;&#26063;&#30340;&#19978;&#38480;&#36873;&#21462;&#25552;&#31034;&#65292;&#24110;&#21161;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36127;&#36131;&#37096;&#32626;&#36807;&#31243;&#20013;&#20135;&#29983;&#24847;&#22806;&#31967;&#31957;&#21709;&#24212;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2311.13628</link><description>&lt;p&gt;
&#25552;&#31034;&#39118;&#38505;&#25511;&#21046;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36127;&#36131;&#37096;&#32626;&#30340;&#20005;&#26684;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13628
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#39118;&#38505;&#25511;&#21046;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#20449;&#24687;&#39118;&#38505;&#24230;&#37327;&#26063;&#30340;&#19978;&#38480;&#36873;&#21462;&#25552;&#31034;&#65292;&#24110;&#21161;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36127;&#36131;&#37096;&#32626;&#36807;&#31243;&#20013;&#20135;&#29983;&#24847;&#22806;&#31967;&#31957;&#21709;&#24212;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#29190;&#28856;&#24335;&#22686;&#38271;&#24341;&#21457;&#20102;&#23545;&#22914;&#20309;&#26368;&#22909;&#22320;&#25552;&#31034;&#27169;&#22411;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#20852;&#36259;&#28010;&#28526;&#12290;&#36873;&#25321;&#19968;&#20010;&#22522;&#20110;&#39564;&#35777;&#38598;&#19978;&#24179;&#22343;&#24615;&#33021;&#30340;&#25552;&#31034;&#21487;&#33021;&#24456;&#35825;&#20154;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#29983;&#25104;&#20986;&#20046;&#24847;&#26009;&#30340;&#31967;&#31957;&#21709;&#24212;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22788;&#22659;&#26368;&#22256;&#38590;&#30340;&#29992;&#25143;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#25552;&#31034;&#39118;&#38505;&#25511;&#21046;&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#26681;&#25454;&#20449;&#24687;&#39118;&#38505;&#24230;&#37327;&#26063;&#30340;&#20005;&#26684;&#19978;&#38480;&#36873;&#25321;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#20135;&#29983;&#22810;&#31181;&#24230;&#37327;&#19978;&#38480;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#34913;&#37327;&#26368;&#22351;&#24773;&#20917;&#21709;&#24212;&#21644;&#29992;&#25143;&#32676;&#20307;&#29983;&#25104;&#36136;&#37327;&#19981;&#22343;&#34913;&#30340;&#37327;&#65292;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22522;&#30784;&#32479;&#35745;&#30028;&#23450;&#25216;&#26415;&#65292;&#20197;&#36866;&#24212;&#37096;&#32626;&#20013;&#20998;&#24067;&#21464;&#21270;&#21487;&#33021;&#24615;&#30340;&#24773;&#20917;&#12290;&#22312;&#24320;&#25918;&#24335;&#32842;&#22825;&#12289;&#21307;&#23398;&#38382;&#39064;&#31561;&#24212;&#29992;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13628v2 Announce Type: replace-cross  Abstract: The recent explosion in the capabilities of large language models has led to a wave of interest in how best to prompt a model to perform a given task. While it may be tempting to simply choose a prompt based on average performance on a validation set, this can lead to a deployment where unexpectedly poor responses are generated, especially for the worst-off users. To mitigate this prospect, we propose Prompt Risk Control, a lightweight framework for selecting a prompt based on rigorous upper bounds on families of informative risk measures. We offer methods for producing bounds on a diverse set of metrics, including quantities that measure worst-case responses and disparities in generation quality across the population of users. In addition, we extend the underlying statistical bounding techniques to accommodate the possibility of distribution shifts in deployment. Experiments on applications such as open-ended chat, medical que
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;MACGYVER&#25968;&#25454;&#38598;&#24182;&#19982;&#20154;&#31867;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#29420;&#20855;&#25361;&#25112;&#24615;&#65292;&#22312;&#30693;&#35782;&#24191;&#24230;&#21644;&#21487;&#34892;&#24615;&#26041;&#38754;&#19982;&#20154;&#31867;&#23384;&#22312;&#29420;&#29305;&#24046;&#24322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.09682</link><description>&lt;p&gt;
MacGyver&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#26159;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#32773;&#65311;
&lt;/p&gt;
&lt;p&gt;
MacGyver: Are Large Language Models Creative Problem Solvers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09682
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;MACGYVER&#25968;&#25454;&#38598;&#24182;&#19982;&#20154;&#31867;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#29420;&#20855;&#25361;&#25112;&#24615;&#65292;&#22312;&#30693;&#35782;&#24191;&#24230;&#21644;&#21487;&#34892;&#24615;&#26041;&#38754;&#19982;&#20154;&#31867;&#23384;&#22312;&#29420;&#29305;&#24046;&#24322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#19968;&#20010;&#20840;&#26032;&#30340;&#32422;&#26463;&#35774;&#32622;&#20013;&#25506;&#31350;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;MACGYVER&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;1600&#20010;&#29305;&#24847;&#35774;&#35745;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65292;&#26088;&#22312;&#24341;&#21457;&#29289;&#20307;&#30340;&#21019;&#26032;&#20351;&#29992;&#65292;&#24182;&#38656;&#35201;&#36229;&#36234;&#24120;&#35268;&#24605;&#32500;&#12290;&#25105;&#20204;&#38543;&#21518;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#23637;&#31034;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#27604;&#36739;&#21644;&#23545;&#27604;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;MACGYVER&#23545;&#36825;&#20004;&#20010;&#32676;&#20307;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#20197;&#29420;&#29305;&#21644;&#20114;&#34917;&#30340;&#26041;&#24335;&#21576;&#29616;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25797;&#38271;&#29087;&#24713;&#30340;&#20219;&#21153;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#19978;&#26377;&#22256;&#38590;&#65292;&#23548;&#33268;&#26356;&#39640;&#30340;&#24046;&#24322;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26292;&#38706;&#20110;&#21508;&#31181;&#19987;&#19994;&#30693;&#35782;&#65292;&#23581;&#35797;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#25552;&#20986;&#29289;&#29702;&#19978;&#19981;&#21487;&#34892;&#30340;&#34892;&#21160;&#26102;&#22833;&#36133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09682v2 Announce Type: replace-cross  Abstract: We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniqu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#20195;&#26367;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSLs&#65289;&#21644;&#22686;&#21152;&#32467;&#26500;&#21270;&#39046;&#22495;&#25551;&#36848;&#25552;&#31034;&#65292;&#26412;&#30740;&#31350;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#23545;&#35821;&#20041;&#35299;&#26512;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#20102;&#23545;&#22823;&#37327;&#31034;&#33539;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2311.09519</link><description>&lt;p&gt;
&#21033;&#29992;&#20195;&#30721;&#25552;&#21319;&#35821;&#22659;&#23398;&#20064;&#23545;&#35821;&#20041;&#35299;&#26512;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Leveraging Code to Improve In-context Learning for Semantic Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09519
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#20195;&#26367;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSLs&#65289;&#21644;&#22686;&#21152;&#32467;&#26500;&#21270;&#39046;&#22495;&#25551;&#36848;&#25552;&#31034;&#65292;&#26412;&#30740;&#31350;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#23545;&#35821;&#20041;&#35299;&#26512;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#20102;&#23545;&#22823;&#37327;&#31034;&#33539;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#20004;&#39033;&#25913;&#36827;&#65292;&#25552;&#39640;&#20102;&#35821;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#23545;&#35821;&#20041;&#35299;&#26512;&#30340;&#26377;&#25928;&#24615;&#65306;&#65288;1&#65289;&#20351;&#29992;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#65288;&#22914;Python&#65289;&#20195;&#26367;DSL&#65292;&#24182;&#65288;2&#65289;&#36890;&#36807;&#22686;&#21152;&#32467;&#26500;&#21270;&#39046;&#22495;&#25551;&#36848;&#30340;&#25552;&#31034;&#65292;&#21253;&#25324;&#21487;&#29992;&#30340;&#31867;&#21644;&#20989;&#25968;&#31561;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#20010;&#21464;&#21270;&#26174;&#33879;&#25552;&#39640;&#20102;&#19977;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#20204;&#30340;&#32467;&#21512;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65288;&#20363;&#22914;&#65292;&#22312;SMCalFlow&#32452;&#21512;&#21010;&#20998;&#19978;&#20174;7.9%&#21040;66.5%&#65289;&#65292;&#20960;&#20046;&#22312;&#20351;&#29992;&#24378;&#27169;&#22411;&#26102;&#28040;&#38500;&#20102;&#26131;&#20110;i.i.d&#21644;&#26356;&#22256;&#38590;&#30340;&#32452;&#21512;&#21010;&#20998;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#20943;&#23569;&#20102;&#23545;&#22823;&#37327;&#28436;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09519v2 Announce Type: replace  Abstract: In-context learning (ICL) is an appealing approach for semantic parsing due to its few-shot nature and improved generalization. However, learning to parse to rare domain-specific languages (DSLs) from just a few demonstrations is challenging, limiting the performance of even the most capable LLMs. In this work, we improve the effectiveness of ICL for semantic parsing by (1) using general-purpose programming languages such as Python instead of DSLs, and (2) augmenting prompts with a structured domain description that includes, e.g., the available classes and functions. We show that both these changes significantly improve accuracy across three popular datasets. Combined, they lead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositional split), nearly closing the performance gap between easier i.i.d.\ and harder compositional splits when used with a strong model, and reducing the need for a large number of demonstration
&lt;/p&gt;</description></item><item><title>ASR&#22522;&#30784;&#27169;&#22411;Whisper&#21644;MMS&#22312;&#19981;&#32463;&#36807;&#39069;&#22806;&#25968;&#25454;&#35757;&#32451;&#25110;&#28155;&#21152;&#26032;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#38646;-shot&#38899;&#39057;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.09363</link><description>&lt;p&gt;
&#25506;&#31350;ASR&#22522;&#30784;&#27169;&#22411;&#30340;&#26032;&#20852;&#38899;&#39057;&#20998;&#31867;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Investigating the Emergent Audio Classification Ability of ASR Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09363
&lt;/p&gt;
&lt;p&gt;
ASR&#22522;&#30784;&#27169;&#22411;Whisper&#21644;MMS&#22312;&#19981;&#32463;&#36807;&#39069;&#22806;&#25968;&#25454;&#35757;&#32451;&#25110;&#28155;&#21152;&#26032;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#38646;-shot&#38899;&#39057;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09363v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#25991;&#26412;&#21644;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;-shot&#35774;&#32622;&#20013;&#25191;&#34892;&#35768;&#22810;&#20219;&#21153;&#65292;&#36825;&#26159;&#19968;&#20010;&#20196;&#20154;&#21521;&#24448;&#30340;&#29305;&#24615;&#65292;&#23427;&#20351;&#36825;&#20123;&#31995;&#32479;&#33021;&#22815;&#24212;&#29992;&#20110;&#26222;&#36941;&#21644;&#20302;&#36164;&#28304;&#30340;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;ASR&#22522;&#30784;&#27169;&#22411;&#30340;&#38646;-shot&#33021;&#21147;&#30340;&#30740;&#31350;&#35201;&#23569;&#24471;&#22810;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#34987;&#24494;&#35843;&#21040;&#29305;&#23450;&#20219;&#21153;&#65292;&#25110;&#21463;&#38480;&#20110;&#19982;&#20854;&#35757;&#32451;&#26631;&#20934;&#21644;&#25968;&#25454;&#27880;&#37322;&#30456;&#21305;&#37197;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;Whisper&#21644;MMS&#36825;&#20004;&#27454;&#20027;&#35201;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#35757;&#32451;&#30340;ASR&#22522;&#30784;&#27169;&#22411;&#25191;&#34892;&#38646;-shot&#38899;&#39057;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#35299;&#30721;&#22120;&#20013;&#20351;&#29992;&#31616;&#21333;&#30340;&#22522;&#20110;&#27169;&#26495;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#30340;&#35299;&#30721;&#27010;&#29575;&#26469;&#29983;&#25104;&#38646;-shot&#39044;&#27979;&#12290;&#22312;&#19981;&#23545;&#27169;&#22411;&#36827;&#34892;&#39069;&#22806;&#25968;&#25454;&#35757;&#32451;&#25110;&#28155;&#21152;&#20219;&#20309;&#26032;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Whisper&#22312;&#19968;&#31995;&#21015;8&#20010;&#38899;&#39057;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#38646;-shot&#20998;&#31867;&#24615;&#33021;&#65292;&#32988;&#36807;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09363v2 Announce Type: replace  Abstract: Text and vision foundation models can perform many tasks in a zero-shot setting, a desirable property that enables these systems to be applied in general and low-resource settings. There has been far less work, however, on the zero-shot abilities of ASR foundation models, with these systems typically fine-tuned to specific tasks or constrained to applications that match their training criterion and data annotation. In this work we investigate the ability of Whisper and MMS, ASR foundation models trained primarily for speech recognition, to perform zero-shot audio classification. We use simple template-based text prompts at the decoder and use the resulting decoding probabilities to generate zero-shot predictions. Without training the model on extra data or adding any new parameters, we demonstrate that Whisper shows promising zero-shot classification performance on a range of 8 audio-classification datasets, outperforming the accurac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30456;&#20114;&#39564;&#35777;&#30340;&#38646;-shot&#26597;&#35810;&#25193;&#23637;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26597;&#35810;&#25193;&#23637;&#20013;&#24050;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#21644;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2310.19056</link><description>&lt;p&gt;
MILL&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#26597;&#35810;&#25193;&#23637;&#30340;&#30456;&#20114;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19056
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30456;&#20114;&#39564;&#35777;&#30340;&#38646;-shot&#26597;&#35810;&#25193;&#23637;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26597;&#35810;&#25193;&#23637;&#20013;&#24050;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#21644;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;shot&#26597;&#35810;&#25193;&#23637;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30456;&#20114;&#39564;&#35777;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26597;&#35810;-&#26597;&#35810;-&#25991;&#26723;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23376;&#26597;&#35810;&#21644;&#30456;&#24212;&#30340;&#25991;&#26723;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#30456;&#20114;&#39564;&#35777;&#36807;&#31243;&#21327;&#21516;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#25991;&#26723;&#20197;&#23454;&#29616;&#26368;&#20339;&#25193;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23436;&#20840;&#26159;&#38646;-shot&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19056v3 Announce Type: replace-cross  Abstract: Query expansion, pivotal in search engines, enhances the representation of user information needs with additional terms. While existing methods expand queries using retrieved or generated contextual documents, each approach has notable limitations. Retrieval-based methods often fail to accurately capture search intent, particularly with brief or ambiguous queries. Generation-based methods, utilizing large language models (LLMs), generally lack corpus-specific knowledge and entail high fine-tuning costs. To address these gaps, we propose a novel zero-shot query expansion framework utilizing LLMs for mutual verification. Specifically, we first design a query-query-document generation method, leveraging LLMs' zero-shot reasoning ability to produce diverse sub-queries and corresponding documents. Then, a mutual verification process synergizes generated and retrieved documents for optimal expansion. Our proposed method is fully zero
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#24037;&#31243;&#26159;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#24517;&#35201;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt Space&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#23884;&#20837;&#21644;&#30697;&#38453;&#20998;&#35299;&#26500;&#24314;&#25552;&#31034;&#31354;&#38388;&#26469;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#32570;&#20047;&#30830;&#23450;&#26368;&#20339;&#25552;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#26174;&#33879;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2306.03799</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;&#25104;&#21151;&#19982;&#25552;&#31034;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.03799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#26159;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#24517;&#35201;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt Space&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#23884;&#20837;&#21644;&#30697;&#38453;&#20998;&#35299;&#26500;&#24314;&#25552;&#31034;&#31354;&#38388;&#26469;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#32570;&#20047;&#30830;&#23450;&#26368;&#20339;&#25552;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#26174;&#33879;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#26159;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#24517;&#35201;&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#20379;&#26126;&#30830;&#21644;&#20855;&#20307;&#30340;&#25351;&#31034;&#12290;&#23427;&#20351;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22914;&#31639;&#26415;&#25512;&#29702;&#12289;&#38382;&#31572;&#12289;&#24635;&#32467;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#31215;&#26497;&#25506;&#32034;&#19981;&#21516;&#30340;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#65292;&#22914;Chain of Thought&#65288;CoT&#65289;&#12289;Zero-CoT&#21644;In-context&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#28304;&#20110;&#24403;&#21069;&#26041;&#27861;&#32570;&#20047;&#30830;&#23450;&#26368;&#20339;&#25552;&#31034;&#30340;&#22362;&#23454;&#25968;&#23398;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#26041;&#27861;&#31216;&#20026;Prompt Space&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#23884;&#20837;&#36890;&#36807;&#30697;&#38453;&#20998;&#35299;&#33719;&#24471;&#22522;&#21521;&#37327;&#65292;&#28982;&#21518;&#26500;&#24314;&#19968;&#20010;&#34920;&#31034;&#25152;&#26377;&#25552;&#31034;&#30340;&#31354;&#38388;&#12290;Prompt Space&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#33539;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.03799v2 Announce Type: replace  Abstract: Prompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions. It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis. Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning. However, an unresolved problem arises from the fact that current approaches lack a solid mathematical solution for determining optimal prompts. To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts. Prompt Space significantly outperforms state-of-the-art prompt paradigms
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#26032;&#30340;&#26631;&#27880;&#25351;&#21335;&#65292;&#24314;&#31435;&#20102;&#29992;&#20110;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#21477;&#23376;&#32423;&#20027;&#35266;&#24615;&#26816;&#27979;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#34920;&#26126;&#22810;&#35821;&#22659;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2305.18034</link><description>&lt;p&gt;
&#29992;&#20110;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#21477;&#23376;&#32423;&#20027;&#35266;&#24615;&#26816;&#27979;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
A Corpus for Sentence-level Subjectivity Detection on English News Articles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.18034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#26032;&#30340;&#26631;&#27880;&#25351;&#21335;&#65292;&#24314;&#31435;&#20102;&#29992;&#20110;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#21477;&#23376;&#32423;&#20027;&#35266;&#24615;&#26816;&#27979;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#34920;&#26126;&#22810;&#35821;&#22659;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21046;&#23450;&#20102;&#29992;&#20110;&#21477;&#23376;&#32423;&#20027;&#35266;&#24615;&#26816;&#27979;&#30340;&#26032;&#39062;&#26631;&#27880;&#25351;&#21335;&#65292;&#19981;&#23616;&#38480;&#20110;&#29305;&#23450;&#35821;&#35328;&#30340;&#32447;&#32034;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25351;&#21335;&#25910;&#38598;&#20102;NewsSD-ENG&#65292;&#36825;&#26159;&#20174;&#26377;&#20105;&#35758;&#35805;&#39064;&#30340;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#20013;&#25552;&#21462;&#30340;638&#20010;&#23458;&#35266;&#21477;&#23376;&#21644;411&#20010;&#20027;&#35266;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#20026;&#33521;&#35821;&#21450;&#20854;&#20182;&#35821;&#35328;&#30340;&#20027;&#35266;&#24615;&#26816;&#27979;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#35789;&#20856;&#25110;&#26426;&#22120;&#32763;&#35793;&#31561;&#29305;&#23450;&#35821;&#35328;&#24037;&#20855;&#12290;&#25105;&#20204;&#22312;&#21333;&#35821;&#12289;&#22810;&#35821;&#21644;&#36328;&#35821;&#35328;&#35774;&#32622;&#19979;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#27880;&#37322;&#20102;&#29616;&#26377;&#30340;&#24847;&#22823;&#21033;&#35821;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#22659;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.18034v2 Announce Type: replace  Abstract: We develop novel annotation guidelines for sentence-level subjectivity detection, which are not limited to language-specific cues. We use our guidelines to collect NewsSD-ENG, a corpus of 638 objective and 411 subjective sentences extracted from English news articles on controversial topics. Our corpus paves the way for subjectivity detection in English and across other languages without relying on language-specific tools, such as lexicons or machine translation. We evaluate state-of-the-art multilingual transformer-based models on the task in mono-, multi-, and cross-language settings. For this purpose, we re-annotate an existing Italian corpus. We observe that models trained in the multilingual setting achieve the best performance on the task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#26500;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#20219;&#20309;&#20855;&#26377;&#26032;&#32422;&#32763;&#35793;&#30340;&#35821;&#35328;&#25552;&#20379;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#65292;&#36890;&#36807;&#23558;&#35821;&#21477;&#19982;&#33521;&#25991;OntoNotes&#20013;&#30340;&#35821;&#21477;&#23545;&#40784;&#24182;&#25237;&#24433;&#26631;&#27880;&#21040;&#30446;&#26631;&#35821;&#35328;&#12290;&#36825;&#39033;&#24037;&#20316;&#20351;&#24471;&#22312;859&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2305.12612</link><description>&lt;p&gt;
PrOnto&#65306;&#20026;859&#31181;&#35821;&#35328;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PrOnto: Language Model Evaluations for 859 Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#26500;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#20219;&#20309;&#20855;&#26377;&#26032;&#32422;&#32763;&#35793;&#30340;&#35821;&#35328;&#25552;&#20379;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#65292;&#36890;&#36807;&#23558;&#35821;&#21477;&#19982;&#33521;&#25991;OntoNotes&#20013;&#30340;&#35821;&#21477;&#23545;&#40784;&#24182;&#25237;&#24433;&#26631;&#27880;&#21040;&#30446;&#26631;&#35821;&#35328;&#12290;&#36825;&#39033;&#24037;&#20316;&#20351;&#24471;&#22312;859&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12612v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#35780;&#20272;&#25968;&#25454;&#38598;&#23545;&#20110;&#34913;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#26631;&#27880;&#25104;&#26412;&#36739;&#39640;&#65292;&#36825;&#20123;&#36164;&#28304;&#23545;&#20110;&#38500;&#33521;&#35821;&#20197;&#22806;&#30340;&#22823;&#22810;&#25968;&#35821;&#35328;&#26469;&#35828;&#26159;&#31232;&#32570;&#30340;&#65292;&#36825;&#20351;&#24471;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#26500;&#24314;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#24471;&#20219;&#20309;&#19968;&#31181;&#20855;&#26377;&#26032;&#32422;&#32763;&#35793;&#30340;&#35821;&#35328;&#37117;&#33021;&#33719;&#24471;&#19968;&#22871;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#35821;&#21477;&#19982;&#33521;&#25991;OntoNotes&#26032;&#32422;&#37096;&#20998;&#20013;&#30340;&#35821;&#21477;&#36827;&#34892;&#23545;&#40784;&#65292;&#28982;&#21518;&#23558;&#33521;&#25991;&#26631;&#27880;&#25237;&#24433;&#21040;&#30446;&#26631;&#35821;&#35328;&#20013;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;859&#31181;&#35821;&#35328;&#30340;1051&#31181;&#26032;&#32422;&#32763;&#35793;&#65292;&#24182;&#23558;&#20854;&#20844;&#24320;&#21487;&#29992;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#21019;&#24314;&#33021;&#22815;&#35780;&#20272;&#20219;&#21153;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12612v2 Announce Type: replace  Abstract: Evaluation datasets are critical resources for measuring the quality of pretrained language models. However, due to the high cost of dataset annotation, these resources are scarce for most languages other than English, making it difficult to assess the quality of language models. In this work, we present a new method for evaluation dataset construction which enables any language with a New Testament translation to receive a suite of evaluation datasets suitable for pretrained language model evaluation. The method critically involves aligning verses with those in the New Testament portion of English OntoNotes, and then projecting annotations from English to the target language, with no manual annotation required. We apply this method to 1051 New Testament translations in 859 and make them publicly available. Additionally, we conduct experiments which demonstrate the efficacy of our method for creating evaluation tasks which can assess
&lt;/p&gt;</description></item><item><title>&#25991;&#20013;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#35299;&#37322;&#25110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#36827;&#34892;&#25512;&#29702;&#65292;&#22312;&#25512;&#29702;&#26426;&#21046;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36923;&#36753;&#35268;&#21017;&#21644;&#31034;&#20363;&#36827;&#34892;&#36845;&#20195;&#25512;&#29702;&#65292;&#25903;&#25345;LMs&#36755;&#20986;&#33258;&#21160;&#39564;&#35777;</title><link>https://arxiv.org/abs/2212.08686</link><description>&lt;p&gt;
&#36890;&#36807;&#31526;&#21495;&#39564;&#35777;&#35780;&#20272;&#36880;&#27493;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Evaluating Step-by-Step Reasoning through Symbolic Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.08686
&lt;/p&gt;
&lt;p&gt;
&#25991;&#20013;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#35299;&#37322;&#25110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#36827;&#34892;&#25512;&#29702;&#65292;&#22312;&#25512;&#29702;&#26426;&#21046;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36923;&#36753;&#35268;&#21017;&#21644;&#31034;&#20363;&#36827;&#34892;&#36845;&#20195;&#25512;&#29702;&#65292;&#25903;&#25345;LMs&#36755;&#20986;&#33258;&#21160;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20351;&#29992;&#35299;&#37322;&#25110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#33719;&#24471;&#21331;&#36234;&#30340;&#25512;&#29702;&#34920;&#29616;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#21253;&#21547;&#31561;&#25928;&#65288;&#33258;&#28982;&#12289;&#31526;&#21495;&#65289;&#25968;&#25454;&#23545;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#31526;&#21495;&#31034;&#20363;&#21253;&#21547;&#26469;&#33258;&#38750;&#21442;&#25968;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#30340;&#19968;&#38454;&#36923;&#36753;&#35268;&#21017;&#21644;&#35859;&#35789;&#65292;&#25903;&#25345;&#23545;&#20013;&#38388;&#25512;&#29702;&#32467;&#26524;&#30340;&#33258;&#21160;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20174;&#21253;&#21547;&#36923;&#36753;&#35268;&#21017;&#21644;&#30456;&#24212;&#31034;&#20363;&#30340;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#20197;&#36845;&#20195;&#22320;&#22312;&#30693;&#35782;&#24211;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#24674;&#22797;Prolog&#30340;&#21521;&#21518;&#38142;&#25509;&#31639;&#27861;&#65292;&#24182;&#25903;&#25345;LMs&#36755;&#20986;&#30340;&#33258;&#21160;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.08686v2 Announce Type: replace  Abstract: Pre-trained language models (LMs) have shown remarkable reasoning performance using explanations or chain-of-thoughts (CoT)) for in-context learning. On the other hand, these reasoning tasks are usually presumed to be more approachable for symbolic programming. To understand the mechanism of reasoning of LMs, we curate synthetic datasets containing equivalent (natural, symbolic) data pairs, where symbolic examples contain first-order logic rules and predicates from non-parametric knowledge bases (KBs), supporting automated verification of intermediate reasoning results. Then we revisit neuro-symbolic approaches and propose to learn from demonstrations containing logic rules and corresponding examples to iteratively reason over KBs, recovering Prolog's backward chaining algorithm and supporting automated verification of LMs' outputs. Comprehensive experiments are included to systematically compare LMLP with CoT in deductive reasoning 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#8212;&#8212;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#25968;&#25454;&#38598;(SOLD)&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#30740;&#31350;&#23616;&#38480;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2212.00851</link><description>&lt;p&gt;
SOLD&#65306;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SOLD: Sinhala Offensive Language Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.00851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#8212;&#8212;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#25968;&#25454;&#38598;(SOLD)&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#30740;&#31350;&#23616;&#38480;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#27604;&#22914;&#20167;&#24680;&#35328;&#35770;&#21644;&#32593;&#32476;&#27450;&#20940;&#65292;&#24050;&#25104;&#20026;&#20840;&#29699;&#24615;&#29616;&#35937;&#12290;&#36825;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#30340;&#20852;&#36259;&#65292;&#20419;&#20351;&#24320;&#21457;&#21508;&#31181;&#31995;&#32479;&#65292;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#28508;&#22312;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#23569;&#25968;&#20960;&#20010;&#20363;&#22806;&#24773;&#20917;&#22806;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#36825;&#19968;&#20027;&#39064;&#30340;&#25968;&#25454;&#38598;&#37117;&#22788;&#29702;&#33521;&#35821;&#21644;&#23569;&#25968;&#20854;&#20182;&#39640;&#36164;&#28304;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#30740;&#31350;&#19968;&#30452;&#23616;&#38480;&#20110;&#36825;&#20123;&#35821;&#35328;&#12290;&#26412;&#25991;&#36890;&#36807;&#22788;&#29702;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20711;&#20285;&#32599;&#35821;&#26159;&#26031;&#37324;&#20848;&#21345;&#26377;&#36229;&#36807;1700&#19975;&#20154;&#21475;&#20351;&#29992;&#30340;&#20302;&#36164;&#28304;&#21360;&#27431;&#35821;&#35328;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;&#65288;SOLD&#65289;&#65292;&#24182;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#22810;&#20010;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.00851v2 Announce Type: replace-cross  Abstract: The widespread of offensive content online, such as hate speech and cyber-bullying, is a global phenomenon. This has sparked interest in the artificial intelligence (AI) and natural language processing (NLP) communities, motivating the development of various systems trained to detect potentially harmful content automatically. These systems require annotated datasets to train the machine learning (ML) models. However, with a few notable exceptions, most datasets on this topic have dealt with English and a few other high-resource languages. As a result, the research in offensive language identification has been limited to these languages. This paper addresses this gap by tackling offensive language identification in Sinhala, a low-resource Indo-Aryan language spoken by over 17 million people in Sri Lanka. We introduce the Sinhala Offensive Language Dataset (SOLD) and present multiple experiments on this dataset. SOLD is a manuall
&lt;/p&gt;</description></item><item><title>&#21452;&#21521;&#27169;&#22411;&#22312;&#22686;&#37327;&#30028;&#38754;&#19979;&#30340;&#34920;&#29616;&#24471;&#21040;&#20102;&#25903;&#25345;&#65292;&#32780;&#20840;&#21521;BERT&#27169;&#22411;&#22312;&#22686;&#37327;&#35775;&#38382;&#26041;&#38754;&#21463;&#21040;&#36739;&#22823;&#24433;&#21709;&#65292;&#21487;&#36890;&#36807;&#35843;&#25972;&#35757;&#32451;&#26426;&#21046;&#32531;&#35299;&#12290;</title><link>https://arxiv.org/abs/2010.05330</link><description>&lt;p&gt;
&#22686;&#37327;&#22788;&#29702;&#22312;&#38750;&#22686;&#37327;&#32534;&#30721;&#22120;&#26102;&#20195;&#65306;&#22686;&#37327;NLU&#30340;&#21452;&#21521;&#27169;&#22411;&#30340;&#32463;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2010.05330
&lt;/p&gt;
&lt;p&gt;
&#21452;&#21521;&#27169;&#22411;&#22312;&#22686;&#37327;&#30028;&#38754;&#19979;&#30340;&#34920;&#29616;&#24471;&#21040;&#20102;&#25903;&#25345;&#65292;&#32780;&#20840;&#21521;BERT&#27169;&#22411;&#22312;&#22686;&#37327;&#35775;&#38382;&#26041;&#38754;&#21463;&#21040;&#36739;&#22823;&#24433;&#21709;&#65292;&#21487;&#36890;&#36807;&#35843;&#25972;&#35757;&#32451;&#26426;&#21046;&#32531;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20154;&#31867;&#20197;&#22686;&#37327;&#26041;&#24335;&#22788;&#29702;&#35821;&#35328;&#65292;&#20294;&#30446;&#21069;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#30340;&#26368;&#20339;&#35821;&#35328;&#32534;&#30721;&#22120;&#19981;&#26159;&#22686;&#37327;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#24517;&#39035;&#22522;&#20110;&#21040;&#36798;&#26576;&#20010;&#26102;&#38388;&#27493;&#30340;&#37096;&#20998;&#36755;&#20837;&#25552;&#20379;&#37096;&#20998;&#36755;&#20986;&#26102;&#65292;&#21363;&#22312;&#20132;&#20114;&#24335;&#31995;&#32479;&#20013;&#21487;&#33021;&#21457;&#29983;&#30340;&#24773;&#20917;&#19979;&#65292;&#21452;&#21521;LSTMs&#21644;Transformers&#22312;&#22686;&#37327;&#30028;&#38754;&#19979;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;NLU&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#20116;&#31181;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#22686;&#37327;&#35780;&#20272;&#25351;&#26631;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#25903;&#25345;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#20197;&#22686;&#37327;&#27169;&#24335;&#65292;&#24182;&#20445;&#30041;&#22823;&#37096;&#20998;&#38750;&#22686;&#37327;&#36136;&#37327;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#22686;&#37327;&#35775;&#38382;&#26041;&#38754;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#38750;&#22686;&#37327;&#24615;&#33021;&#8220;&#20840;&#21521;&#8221;BERT&#27169;&#22411;&#21463;&#21040;&#26356;&#22823;&#24433;&#21709;&#12290;&#36890;&#36807;&#35843;&#25972;&#35757;&#32451;&#26426;&#21046;&#21487;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2010.05330v2 Announce Type: replace  Abstract: While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers). We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems. We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics. The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality. The "omni-directional" BERT model, which achieves better non-incremental performance, is impacted more by the incremental access. This can be alleviated by adapting the training regime (truncat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#30340;&#26657;&#20934;&#20250;&#20808;&#22686;&#21152;&#32780;&#21518;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#20027;&#35201;&#20986;&#29616;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#31561;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#25552;&#31034;&#38656;&#35201;&#38024;&#23545;&#21487;&#38752;&#24615;&#22330;&#26223;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.04021</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26657;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Calibration of In-context Learning. (arXiv:2312.04021v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#30340;&#26657;&#20934;&#20250;&#20808;&#22686;&#21152;&#32780;&#21518;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#20027;&#35201;&#20986;&#29616;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#31561;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#25552;&#31034;&#38656;&#35201;&#38024;&#23545;&#21487;&#38752;&#24615;&#22330;&#26223;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#23433;&#20840;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#29616;&#20195;LMs&#26657;&#20934;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;LMs&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#22312;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#26368;&#21021;&#20250;&#20986;&#29616;&#22686;&#21152;&#30340;&#26657;&#20934;&#35823;&#24046;&#65292;&#28982;&#21518;&#25165;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#26657;&#20934;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#24448;&#24448;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20197;&#25552;&#39640;&#21487;&#29992;&#24615;&#20026;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#22914;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#65292;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#36825;&#34920;&#26126;&#22312;&#26399;&#26395;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#22330;&#26223;&#20013;&#21487;&#33021;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty quantification is crucial for the safe deployment of language models (LMs), and prior research has demonstrated improvements in the calibration of modern LMs. Our study focuses on in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examines the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations, suggesting that new methods may be required for scenarios where models are expected to be reliable.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRP&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#26469;&#26174;&#33879;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36127;&#25285;&#65292;&#24182;&#39318;&#27425;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17563</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#25991;&#26412;&#25490;&#24207;&#22120;&#65292;&#20855;&#26377;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. (arXiv:2306.17563v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRP&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#26469;&#26174;&#33879;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36127;&#25285;&#65292;&#24182;&#39318;&#27425;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#30452;&#25509;&#23558;&#26597;&#35810;&#21644;&#20505;&#36873;&#25991;&#26723;&#36755;&#20837;&#25552;&#31034;&#36827;&#34892;&#25991;&#26723;&#25490;&#24207;&#26159;&#19968;&#20010;&#26377;&#36259;&#19988;&#23454;&#29992;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24456;&#38590;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#31934;&#35843;&#22522;&#20934;&#25490;&#24207;&#22120;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#30340;&#28857;&#23545;&#28857;&#21644;&#21015;&#34920;&#25490;&#24207;&#25552;&#31034;&#65292;&#24182;&#35748;&#20026;&#29616;&#25104;&#30340;LLM&#27809;&#26377;&#23436;&#20840;&#29702;&#35299;&#36825;&#20123;&#25490;&#24207;&#20844;&#24335;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;LLM&#30340;&#35757;&#32451;&#26041;&#24335;&#30340;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#65288;PRP&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;LLM&#30340;&#36127;&#25285;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#25991;&#29486;&#20013;&#39318;&#27425;&#20351;&#29992;&#20013;&#31561;&#35268;&#27169;&#30340;&#24320;&#28304;LLM&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;&#22312;TREC-DL2020&#19978;&#65292;&#22522;&#20110;20B&#21442;&#25968;&#30340;Flan-UL2&#27169;&#22411;&#30340;PRP&#36229;&#36807;&#20102;&#25991;&#29486;&#20013;&#22522;&#20110;&#21830;&#19994;&#40657;&#30418;GPT-4&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, there has been limited success so far, as researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these ranking formulations, possibly due to the nature of how LLMs are trained. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on the Flan-UL2 model with 20B parameters outperforms the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that ha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#21644;&#25351;&#20196;&#29702;&#35299;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ODQA&#25968;&#25454;&#38598;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.08635</link><description>&lt;p&gt;
&#33258;&#25105;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Self-Prompting Large Language Models for Zero-Shot Open-Domain QA. (arXiv:2212.08635v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#21644;&#25351;&#20196;&#29702;&#35299;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ODQA&#25968;&#25454;&#38598;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#38382;&#31572;&#30446;&#26631;&#22312;&#20110;&#22238;&#31572;&#20851;&#20110;&#20107;&#23454;&#30340;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#25552;&#20379;&#29305;&#23450;&#30340;&#32972;&#26223;&#25991;&#26723;&#12290;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#30001;&#20110;&#27809;&#26377;&#25968;&#25454;&#26469;&#35757;&#32451;&#31867;&#20284;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#22240;&#27492;&#27492;&#20219;&#21153;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#36890;&#36807;&#30452;&#25509;&#25552;&#31034;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#36828;&#36828;&#19981;&#33021;&#20805;&#20998;&#21457;&#25381;LLM&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#32780;&#21482;&#26159;&#20197;&#38544;&#24335;&#26041;&#24335;&#35843;&#29992;&#23427;&#20204;&#32780;&#24050;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#25552;&#31034;&#26694;&#26550;&#65292;&#20197;&#26126;&#30830;&#21033;&#29992;LLM&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#22823;&#37327;&#30693;&#35782;&#21644;&#20854;&#24378;&#22823;&#30340;&#25351;&#20196;&#29702;&#35299;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36880;&#27493;&#25552;&#31034;LLM&#29983;&#25104;&#22810;&#20010;&#20266;QA&#23545;&#65292;&#24182;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#32972;&#26223;&#27573;&#33853;&#21644;&#35299;&#37322;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#30340;&#20803;&#32032;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ODQA&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;SOTA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-Domain Question Answering (ODQA) aims at answering factoid questions without explicitly providing specific background documents. In a zero-shot setting, this task is more challenging since no data is available to train customized models like Retriever-Readers. Recently, Large Language Models (LLMs) like GPT-3 have shown their power in zero-shot ODQA with direct prompting methods, but these methods are still far from releasing the full powerfulness of LLMs only in an implicitly invoking way. In this paper, we propose a Self-Prompting framework to explicitly utilize the massive knowledge stored in the parameters of LLMs and their strong instruction understanding abilities. Concretely, we prompt LLMs step by step to generate multiple pseudo QA pairs with background passages and explanations from scratch and then use those generated elements for in-context learning. Experimental results show our method surpasses previous SOTA methods significantly on three widely-used ODQA datasets, a
&lt;/p&gt;</description></item></channel></rss>