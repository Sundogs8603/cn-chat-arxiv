<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#35770;&#25454;&#25688;&#35201;&#26041;&#27861;&#22312;&#25429;&#25417;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#22810;&#26679;&#24615;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#24847;&#35265;&#12289;&#27880;&#37322;&#32773;&#21644;&#26469;&#28304;&#12290;&#25152;&#30740;&#31350;&#30340;&#20851;&#38190;&#28857;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;LLMs&#21644;&#19987;&#38376;&#30340;KPA&#27169;&#22411;&#37117;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#23558;&#26377;&#21161;&#20110;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#24212;&#23545;&#35770;&#35777;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#38656;&#35201;&#37319;&#29992;&#22810;&#31181;&#31574;&#30053;&#26469;&#22788;&#29702;&#20027;&#35266;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01535</link><description>&lt;p&gt;
&#35770;&#35770;&#25991;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Empirical Analysis of Diversity in Argument Summarization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#35770;&#25454;&#25688;&#35201;&#26041;&#27861;&#22312;&#25429;&#25417;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#22810;&#26679;&#24615;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#24847;&#35265;&#12289;&#27880;&#37322;&#32773;&#21644;&#26469;&#28304;&#12290;&#25152;&#30740;&#31350;&#30340;&#20851;&#38190;&#28857;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;LLMs&#21644;&#19987;&#38376;&#30340;KPA&#27169;&#22411;&#37117;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#23558;&#26377;&#21161;&#20110;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#24212;&#23545;&#35770;&#35777;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#38656;&#35201;&#37319;&#29992;&#22810;&#31181;&#31574;&#30053;&#26469;&#22788;&#29702;&#20027;&#35266;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20250;&#35752;&#35770;&#20013;&#65292;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#35770;&#25454;&#26159;&#20419;&#36827;&#21442;&#19982;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#35770;&#25454;&#25688;&#35201;&#26041;&#27861;&#32570;&#22833;&#20102;&#36825;&#39033;&#20219;&#21153;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#25429;&#25417;&#22810;&#26679;&#24615;&#65292;&#36825;&#23545;&#20110;&#21253;&#23481;&#22810;&#20010;&#35266;&#28857;&#26159;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#65306;&#24847;&#35265;&#12289;&#27880;&#37322;&#32773;&#21644;&#26469;&#28304;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31181;&#21517;&#20026;&#20851;&#38190;&#28857;&#20998;&#26512;&#30340;&#27969;&#34892;&#35770;&#25454;&#25688;&#35201;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#26174;&#31034;&#36825;&#20123;&#26041;&#27861;&#22312;(1)&#20195;&#34920;&#23569;&#25968;&#20154;&#20849;&#20139;&#30340;&#35770;&#28857;&#19978;&#65292;(2)&#22788;&#29702;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#25968;&#25454;&#20197;&#21450;(3)&#19982;&#20154;&#24037;&#25552;&#20379;&#30340;&#20027;&#35266;&#27880;&#37322;&#30456;&#19968;&#33268;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#29992;&#30340;LLM&#21644;&#19987;&#38376;&#30340;KPA&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#20102;&#36825;&#31181;&#34892;&#20026;&#65292;&#20294;&#20855;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#21487;&#33021;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#24212;&#23545;&#35770;&#35777;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#38656;&#35201;&#37319;&#29992;&#19968;&#31995;&#21015;&#31574;&#30053;&#26469;&#22788;&#29702;&#20027;&#35266;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Presenting high-level arguments is a crucial task for fostering participation in online societal discussions. Current argument summarization approaches miss an important facet of this task -- capturing diversity -- which is important for accommodating multiple perspectives. We introduce three aspects of diversity: those of opinions, annotators, and sources. We evaluate approaches to a popular argument summarization task called Key Point Analysis, which shows how these approaches struggle to (1) represent arguments shared by few people, (2) deal with data from various sources, and (3) align with subjectivity in human-provided annotations. We find that both general-purpose LLMs and dedicated KPA models exhibit this behavior, but have complementary strengths. Further, we observe that diversification of training data may ameliorate generalization. Addressing diversity in argument summarization requires a mix of strategies to deal with subjectivity.
&lt;/p&gt;</description></item><item><title>AQA-Bench&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#19978;&#19979;&#25991;&#20013;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#65292;&#26174;&#33879;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09404</link><description>&lt;p&gt;
AQA-Bench&#65306;&#35780;&#20272;LLM&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#30340;&#20132;&#20114;&#24335;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09404
&lt;/p&gt;
&lt;p&gt;
AQA-Bench&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#19978;&#19979;&#25991;&#20013;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#65292;&#26174;&#33879;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;AQA-Bench&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31639;&#27861;&#19978;&#19979;&#25991;&#20013;&#65292;&#22914;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;DFS&#65289;&#31561;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#30340;&#20851;&#38190;&#29305;&#28857;&#22312;&#20110;&#20854;&#20132;&#20114;&#24335;&#35780;&#20272;&#21327;&#35758;-&#20363;&#22914;&#65292;&#22312;DFS&#20013;&#65292;&#27599;&#20010;&#33410;&#28857;&#30340;&#21487;&#29992;&#36830;&#25509;&#36793;&#21462;&#20915;&#20110;&#27169;&#22411;&#23545;&#35813;&#33410;&#28857;&#30340;&#36941;&#21382;&#65292;&#22240;&#27492;&#38656;&#35201;LLM&#26377;&#25928;&#22320;&#35760;&#20303;&#24050;&#35775;&#38382;&#33410;&#28857;&#24182;&#31574;&#21010;&#21518;&#32493;&#31227;&#21160;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#26500;&#24314;&#20102;AQA-Bench&#65292;&#20998;&#21035;&#26159;&#20108;&#20998;&#25628;&#32034;&#65292;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65292;&#24182;&#35780;&#20272;&#20102;12&#31181;&#19981;&#21516;&#30340;LLMs&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;&#65288;1&#65289;&#31867;&#20284;GPT-4&#21644;Gemini&#31561;&#38381;&#28304;&#27169;&#22411;&#36890;&#24120;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#65292;&#26126;&#26174;&#20248;&#20110;&#24320;&#28304;LLMs&#12290;&#65288;2&#65289;&#22825;&#30495;&#22320;&#25552;&#20379;&#20114;&#25805;&#20316;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09404v1 Announce Type: cross Abstract: This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol -- for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 12 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show strong sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing inter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#26597;&#35810;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#20195;&#20215;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09401</link><description>&lt;p&gt;
&#20351;&#29992;&#20027;&#21160;&#26597;&#35810;&#30340;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback with Active Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#26597;&#35810;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#20195;&#20215;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#65292;&#22312;&#26500;&#24314;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#65292;&#32780;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#36153;&#26102;&#36153;&#21147;&#12290;&#26412;&#25991;&#21463;&#21040;&#20027;&#21160;&#23398;&#20064;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#36890;&#36807;&#25552;&#20986;&#26597;&#35810;&#25928;&#29575;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#23545;&#40784;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#31454;&#20105;&#20108;&#33218;&#24378;&#30423;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#20027;&#21160;&#26597;&#35810;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;APPO&#65289;&#31639;&#27861;&#65292;&#20855;&#26377;$\tilde{O}(d^2/\Delta)$&#30340;&#36951;&#25022;&#30028;&#21644;$\tilde{O}(d^2/\Delta^2)$&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;$d$&#26159;&#29305;&#24449;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;$\Delta$&#26159;&#25152;&#26377;&#19978;&#19979;&#25991;&#20013;&#30340;&#27425;&#20248;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ADPO&#65292;&#36825;&#26159;&#25105;&#20204;&#31639;&#27861;&#30340;&#23454;&#38469;&#29256;&#26412;&#65292;&#22522;&#20110;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09401v1 Announce Type: cross Abstract: Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods. We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ regret bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\Delta$ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to 
&lt;/p&gt;</description></item><item><title>&#38271;&#25991;&#26412;&#35780;&#20272;&#27169;&#22411;&#32534;&#36753;&#65288;LEME&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#32534;&#36753;&#22312;&#38271;&#31687;&#29983;&#25104;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#24433;&#21709;&#12290;&#36825;&#20010;&#21327;&#35758;&#19982;&#20808;&#21069;&#30340;&#30701;&#25991;&#26412;&#25351;&#26631;&#20960;&#20046;&#27809;&#26377;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#32500;&#24230;&#26469;&#29702;&#35299;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09394</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#35780;&#20272;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Long-form evaluation of model editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09394
&lt;/p&gt;
&lt;p&gt;
&#38271;&#25991;&#26412;&#35780;&#20272;&#27169;&#22411;&#32534;&#36753;&#65288;LEME&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#32534;&#36753;&#22312;&#38271;&#31687;&#29983;&#25104;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#24433;&#21709;&#12290;&#36825;&#20010;&#21327;&#35758;&#19982;&#20808;&#21069;&#30340;&#30701;&#25991;&#26412;&#25351;&#26631;&#20960;&#20046;&#27809;&#26377;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#32500;&#24230;&#26469;&#29702;&#35299;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23545;&#20110;&#27169;&#22411;&#32534;&#36753;&#30340;&#35780;&#20272;&#21482;&#20351;&#29992;&#20102;&#25552;&#31034;&#21518;&#30340;&#8220;&#19979;&#20960;&#20010;&#26631;&#35760;&#8221;&#30340;&#23436;&#25104;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#26356;&#38271;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#24433;&#21709;&#22823;&#37096;&#20998;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#38271;&#25991;&#26412;&#35780;&#20272;&#27169;&#22411;&#32534;&#36753;&#65288;LEME&#65289;&#30340;&#26032;&#39062;&#35780;&#20272;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#22312;&#38271;&#31687;&#29983;&#25104;&#35774;&#32622;&#20013;&#34913;&#37327;&#27169;&#22411;&#32534;&#36753;&#30340;&#21151;&#25928;&#21644;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21327;&#35758;&#21253;&#25324;&#26426;&#22120;&#35780;&#23450;&#30340;&#35843;&#26597;&#21644;&#19982;&#20154;&#31867;&#35780;&#20998;&#30456;&#20851;&#24615;&#24456;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#35780;&#20272;&#21327;&#35758;&#19982;&#20808;&#21069;&#30340;&#30701;&#25991;&#26412;&#25351;&#26631;&#20960;&#20046;&#27809;&#26377;&#20851;&#31995;&#65288;&#23613;&#31649;&#35774;&#35745;&#20026;&#25193;&#23637;&#21151;&#25928;&#12289;&#27867;&#21270;&#24615;&#12289;&#23616;&#37096;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#21040;&#38271;&#25991;&#26412;&#29615;&#22659;&#20013;&#65289;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#32500;&#24230;&#26469;&#29702;&#35299;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;&#20351;&#29992;&#36825;&#20010;&#21327;&#35758;&#65292;&#25105;&#20204;&#23545;&#19968;&#20123;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#21457;&#29616;&#65292;&#21253;&#25324;&#19968;&#20123;&#26041;&#27861;&#65288;R
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09394v1 Announce Type: new Abstract: Evaluations of model editing currently only use the `next few token' completions after a prompt. As a result, the impact of these methods on longer natural language generation is largely unknown. We introduce long-form evaluation of model editing (\textbf{\textit{LEME}}) a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings. Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings. Importantly, we find that our protocol has very little relationship with previous short-form metrics (despite being designed to extend efficacy, generalization, locality, and portability into a long-form setting), indicating that our method introduces a novel set of dimensions for understanding model editing methods. Using this protocol, we benchmark a number of model editing techniques and present several findings including that, while some methods (R
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LlaSMol&#65292;&#23427;&#26159;&#19968;&#31181;&#25512;&#36827;&#21270;&#23398;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;LlaSMol&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4&#24182;&#25509;&#36817;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09391</link><description>&lt;p&gt;
LlaSMol:&#21033;&#29992;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#25512;&#36827;&#21270;&#23398;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LlaSMol&#65292;&#23427;&#26159;&#19968;&#31181;&#25512;&#36827;&#21270;&#23398;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;LlaSMol&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4&#24182;&#25509;&#36817;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#22312;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#29616;&#26377;&#24037;&#20316;&#34920;&#26126;&#23427;&#20204;&#22312;&#21270;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20196;&#20154;&#22833;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;LLM&#22312;&#19968;&#31995;&#21015;&#21270;&#23398;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GPT-4&#65292;&#24182;&#25509;&#36817;SoTA&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#25105;&#20204;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#26159;&#19968;&#20010;&#21517;&#20026;SMolInstruct&#30340;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;&#20102;14&#20010;&#32463;&#36807;&#31934;&#24515;&#25361;&#36873;&#30340;&#21270;&#23398;&#20219;&#21153;&#21644;&#36229;&#36807;&#19977;&#30334;&#19975;&#20010;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#21270;&#23398;LLM&#22880;&#23450;&#20102;&#22362;&#23454;&#22522;&#30784;&#12290;&#22522;&#20110;SMolInstruct&#65292;&#25105;&#20204;&#23545;&#19968;&#32452;&#24320;&#28304;LLM&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20854;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;Mistral ser&#26159;&#26368;&#20339;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09391v1 Announce Type: new Abstract: Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models. The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct. It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral ser
&lt;/p&gt;</description></item><item><title>HGOT&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#20998;&#23618;&#24605;&#32500;&#22270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#21644;&#24605;&#32500;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#26469;&#25552;&#39640;&#30456;&#20851;&#27573;&#33853;&#30340;&#26816;&#32034;&#21644;&#31572;&#26696;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.09390</link><description>&lt;p&gt;
HGOT: &#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#20998;&#23618;&#24605;&#32500;&#22270;
&lt;/p&gt;
&lt;p&gt;
HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09390
&lt;/p&gt;
&lt;p&gt;
HGOT&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#20998;&#23618;&#24605;&#32500;&#22270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#21644;&#24605;&#32500;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#26469;&#25552;&#39640;&#30456;&#20851;&#27573;&#33853;&#30340;&#26816;&#32034;&#21644;&#31572;&#26696;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20107;&#23454;&#24615;&#21644;&#24187;&#35273;&#30340;&#20542;&#21521;&#24341;&#21457;&#20102;&#37325;&#22823;&#20851;&#20999;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26816;&#32034;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#24605;&#32500;&#22270;&#65288;HGOT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#12289;&#22810;&#23618;&#27425;&#30340;&#22270;&#24418;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#20013;&#30456;&#20851;&#27573;&#33853;&#30340;&#26816;&#32034;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;LLMs&#30340;&#36880;&#28176;&#35268;&#21010;&#33021;&#21147;&#65292;&#37319;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#23558;&#22797;&#26434;&#26597;&#35810;&#20998;&#35299;&#20026;&#21487;&#22788;&#29702;&#30340;&#23376;&#26597;&#35810;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#26368;&#36817;&#25552;&#20986;&#30340;&#24341;&#25991;&#22238;&#24518;&#21644;&#31934;&#30830;&#24230;&#25351;&#26631;&#26469;&#35780;&#20272;&#24605;&#32500;&#36136;&#37327;&#65292;&#23558;&#31572;&#26696;&#30340;&#21487;&#20449;&#24230;&#19982;&#24605;&#32500;&#30340;&#36136;&#37327;&#20869;&#22312;&#22320;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#33258;&#27965;&#24615;&#22810;&#25968;&#25237;&#31080;&#30340;&#31572;&#26696;&#36873;&#25321;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#21152;&#26435;&#31995;&#32479;&#65292;&#22312;&#22810;&#25968;&#25237;&#31080;&#20013;&#20248;&#20808;&#32771;&#34385;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09390v1 Announce Type: new Abstract: With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns. To address this issue, particularly in retrieval-augmented in-context learning, we introduce the hierarchical graph of thoughts (HGOT), a structured, multi-layered graph approach designed to enhance the retrieval of pertinent passages during in-context learning. The framework utilizes the emergent planning capabilities of LLMs, employing the divide-and-conquer strategy to break down complex queries into manageable sub-queries. It refines self-consistency majority voting for answer selection, which incorporates the recently proposed citation recall and precision metrics to assess the quality of thoughts, linking an answer's credibility intrinsically to the thought's quality. This methodology introduces a weighted system in majority voting, prioritizing answers 
&lt;/p&gt;</description></item><item><title>Transformers&#22312;&#29305;&#23450;&#32452;&#21512;&#30340;&#25968;&#25454;&#26684;&#24335;&#21644;&#20301;&#32622;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#38271;&#24230;&#30340;&#27867;&#21270;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#33030;&#24369;&#24615;&#21644;&#22823;&#37327;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.09371</link><description>&lt;p&gt;
Transformers&#21487;&#20197;&#23454;&#29616;&#38271;&#24230;&#30340;&#27867;&#21270;&#65292;&#20294;&#24182;&#19981;&#31283;&#20581;
&lt;/p&gt;
&lt;p&gt;
Transformers Can Achieve Length Generalization But Not Robustly
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09371
&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#29305;&#23450;&#32452;&#21512;&#30340;&#25968;&#25454;&#26684;&#24335;&#21644;&#20301;&#32622;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#38271;&#24230;&#30340;&#27867;&#21270;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#33030;&#24369;&#24615;&#21644;&#22823;&#37327;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#30340;&#27867;&#21270;&#65292;&#21363;&#20174;&#36739;&#30701;&#30340;&#35757;&#32451;&#24207;&#21015;&#25512;&#24191;&#21040;&#36739;&#38271;&#30340;&#27979;&#35797;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#21363;&#20351;&#26159;&#22788;&#29702;&#30456;&#23545;&#31616;&#21333;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;Transformer&#20063;&#23384;&#22312;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25972;&#25968;&#30456;&#21152;&#30340;&#20219;&#21153;&#26469;&#27979;&#35797;Transformer&#30340;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38271;&#24230;&#27867;&#21270;&#30340;&#25104;&#21151;&#19982;&#25968;&#25454;&#26684;&#24335;&#21644;&#20301;&#32622;&#32534;&#30721;&#30340;&#31867;&#22411;&#23494;&#20999;&#30456;&#20851;&#12290;&#36890;&#36807;&#27491;&#30830;&#32452;&#21512;&#25968;&#25454;&#26684;&#24335;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20986;&#26631;&#20934;&#30340;Transformer&#21487;&#20197;&#25512;&#24191;&#21040;&#36755;&#20837;&#38271;&#24230;&#30340;2.5&#20493;&#30340;&#24207;&#21015;&#38271;&#24230;&#12290;&#28982;&#32780;&#65292;&#19982;&#20869;&#20998;&#24067;&#27867;&#21270;&#19981;&#21516;&#65292;&#38271;&#24230;&#27867;&#21270;&#20173;&#28982;&#24456;&#33030;&#24369;&#65292;&#21463;&#21040;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#25968;&#25454;&#39034;&#24207;&#31561;&#22240;&#32032;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#23548;&#33268;&#19981;&#21516;&#38543;&#26426;&#31181;&#23376;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09371v1 Announce Type: cross Abstract: Length generalization, defined as the ability to extrapolate from shorter training sequences to longer test ones, is a significant challenge for language models. This issue persists even with large-scale Transformers handling relatively straightforward tasks. In this paper, we test the Transformer's ability of length generalization using the task of addition of two integers. We show that the success of length generalization is intricately linked to the data format and the type of position encoding. Using the right combination of data format and position encodings, we show for the first time that standard Transformers can extrapolate to a sequence length that is 2.5x the input length. Nevertheless, unlike in-distribution generalization, length generalization remains fragile, significantly influenced by factors like random weight initialization and training data order, leading to large variances across different random seeds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#35268;&#27169;&#33719;&#21462;&#22810;&#20803;&#25991;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#25429;&#25417;&#19990;&#30028;&#21508;&#22320;&#22810;&#26679;&#32780;&#20016;&#23500;&#30340;&#25991;&#21270;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#26500;&#24314;&#30340;CultureAtlas&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21508;&#31181;&#23376;&#22269;&#23478;&#32423;&#22320;&#29702;&#21306;&#22495;&#21644;&#27665;&#26063;&#35821;&#35328;&#32676;&#65292;&#20026;&#36328;&#25991;&#21270;&#27807;&#36890;&#21644;&#20114;&#21160;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.09369</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#20803;&#25991;&#21270;&#30693;&#35782;&#33719;&#21462;&#19982;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Massively Multi-Cultural Knowledge Acquisition &amp; LM Benchmarking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#35268;&#27169;&#33719;&#21462;&#22810;&#20803;&#25991;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#25429;&#25417;&#19990;&#30028;&#21508;&#22320;&#22810;&#26679;&#32780;&#20016;&#23500;&#30340;&#25991;&#21270;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#26500;&#24314;&#30340;CultureAtlas&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21508;&#31181;&#23376;&#22269;&#23478;&#32423;&#22320;&#29702;&#21306;&#22495;&#21644;&#27665;&#26063;&#35821;&#35328;&#32676;&#65292;&#20026;&#36328;&#25991;&#21270;&#27807;&#36890;&#21644;&#20114;&#21160;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#20026;&#35768;&#22810;&#24212;&#29992;&#24102;&#26469;&#20102;&#38761;&#21629;&#65292;&#20294;&#20173;&#38754;&#20020;&#19982;&#25991;&#21270;&#20559;&#35265;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#22312;&#24341;&#23548;&#36328;&#25991;&#21270;&#27807;&#36890;&#21644;&#20114;&#21160;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25991;&#21270;&#24120;&#35782;&#30693;&#35782;&#30340;&#32570;&#20047;&#12290;&#37492;&#20110;&#29616;&#26377;&#26041;&#27861;&#22312;&#25429;&#25417;&#19990;&#30028;&#21508;&#22320;&#22810;&#26679;&#32780;&#20016;&#23500;&#30340;&#25991;&#21270;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#22810;&#20803;&#25991;&#21270;&#30693;&#35782;&#33719;&#21462;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#23494;&#38598;&#20449;&#24687;&#21270;&#30340;&#32500;&#22522;&#30334;&#31185;&#25991;&#26723;&#20013;&#25112;&#30053;&#23548;&#33322;&#21040;&#19968;&#20010;&#24191;&#27867;&#30340;&#38142;&#25509;&#39029;&#38754;&#32593;&#32476;&#12290;&#21033;&#29992;&#36825;&#20010;&#23453;&#36149;&#30340;&#25968;&#25454;&#25910;&#38598;&#26469;&#28304;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;CultureAtlas&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21508;&#31181;&#23376;&#22269;&#23478;&#32423;&#22320;&#29702;&#21306;&#22495;&#21644;&#27665;&#26063;&#35821;&#35328;&#32676;&#65292;&#36890;&#36807;&#25968;&#25454;&#28165;&#29702;&#21644;&#39044;&#22788;&#29702;&#30830;&#20445;&#25991;&#26412;&#26029;&#35328;&#21477;&#23376;&#30340;&#33258;&#25105;&#23436;&#25972;&#24615;&#65292;&#20197;&#21450;&#32454;&#31890;&#24230;&#30340;&#25991;&#21270;&#20010;&#20154;&#26723;&#26696;&#20449;&#24687;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19981;&#20165;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09369v1 Announce Type: new Abstract: Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions. Recognizing the shortcomings of existing methods in capturing the diverse and rich cultures across the world, this paper introduces a novel approach for massively multicultural knowledge acquisition. Specifically, our method strategically navigates from densely informative Wikipedia documents on cultural topics to an extensive network of linked pages. Leveraging this valuable source of data collection, we construct the CultureAtlas dataset, which covers a wide range of sub-country level geographical regions and ethnolinguistic groups, with data cleaning and preprocessing to ensure textual assertion sentence self-containment, as well as fine-grained cultural profile information extraction. Our dataset not only 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#20013;&#23545;&#29256;&#26435;&#20445;&#25252;&#20869;&#23481;&#30340;&#21512;&#29702;&#20351;&#29992;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#20351;&#29992;&#29256;&#26435;&#38519;&#38449;&#26469;&#35782;&#21035;&#19981;&#33258;&#28982;&#35760;&#24518;&#30340;&#27169;&#22411;&#20013;&#30340;&#29256;&#26435;&#26448;&#26009;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09363</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29256;&#26435;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Copyright Traps for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#20013;&#23545;&#29256;&#26435;&#20445;&#25252;&#20869;&#23481;&#30340;&#21512;&#29702;&#20351;&#29992;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#20351;&#29992;&#29256;&#26435;&#38519;&#38449;&#26469;&#35782;&#21035;&#19981;&#33258;&#28982;&#35760;&#24518;&#30340;&#27169;&#22411;&#20013;&#30340;&#29256;&#26435;&#26448;&#26009;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#23545;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20869;&#23481;&#30340;&#21512;&#29702;&#20351;&#29992;&#30340;&#38382;&#39064;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#36890;&#36807;&#23545;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#26469;&#25512;&#26029;&#19968;&#27573;&#20869;&#23481;&#26159;&#21542;&#22312;&#35757;&#32451;&#20013;&#20986;&#29616;&#36807;&#12290;&#30446;&#21069;&#30340;&#26368;&#20248;&#26041;&#27861;&#20381;&#36182;&#20110;&#65288;&#37096;&#20998;&#65289;&#20869;&#23481;&#30340;&#33258;&#28982;&#35760;&#24518;&#65292;&#23545;&#20110;&#22823;&#37327;&#35760;&#24518;&#30340;&#27169;&#22411;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#25105;&#20204;&#20551;&#35774;&#24182;&#21518;&#26469;&#35777;&#23454;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#19981;&#33258;&#28982;&#35760;&#24518;&#65292;&#20363;&#22914;&#20013;&#22411; 1B &#27169;&#22411;&#23558;&#19981;&#36215;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#29256;&#26435;&#38519;&#38449;&#26469;&#35782;&#21035;LLM&#20013;&#30340;&#29256;&#26435;&#26448;&#26009;&#20351;&#29992;&#65292;&#37325;&#28857;&#25918;&#22312;&#19981;&#33258;&#28982;&#35760;&#24518;&#30340;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#23558;&#38519;&#38449;&#38543;&#26426;&#25554;&#20837;&#21407;&#22987;&#20869;&#23481;&#65288;&#20070;&#31821;&#65289;&#20013;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;1.3B&#30340;LLM&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#39564;&#35777;&#20102;&#22312;LLM&#20013;&#20351;&#29992;&#20869;&#23481;&#26159;&#21542;&#20250;&#23548;&#33268;&#38519;&#38449;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09363v1 Announce Type: new Abstract: Questions of fair use of copyright-protected content to train Large Language Models (LLMs) are being very actively debated. Document-level inference has been proposed as a new task: inferring from black-box access to the trained model whether a piece of content has been seen during training. SOTA methods however rely on naturally occurring memorization of (part of) the content. While very effective against models that memorize a lot, we hypothesize--and later confirm--that they will not work against models that do not naturally memorize, e.g. medium-size 1B models. We here propose to use copyright traps, the inclusion of fictitious entries in original content, to detect the use of copyrighted materials in LLMs with a focus on models where memorization does not naturally occur. We carefully design an experimental setup, randomly inserting traps into original content (books) and train a 1.3B LLM. We first validate that the use of content in
&lt;/p&gt;</description></item><item><title>DoRA&#26159;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#26435;&#37325;&#20998;&#35299;&#20026;&#24133;&#24230;&#21644;&#26041;&#21521;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#30340;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26469;&#25552;&#39640;&#23545;LLaMA&#65292;LLaVA&#21644;VL-B&#30340;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09353</link><description>&lt;p&gt;
DoRA: &#20998;&#35299;&#26435;&#37325;&#30340;&#20302;&#31209;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
DoRA: Weight-Decomposed Low-Rank Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09353
&lt;/p&gt;
&lt;p&gt;
DoRA&#26159;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#26435;&#37325;&#20998;&#35299;&#20026;&#24133;&#24230;&#21644;&#26041;&#21521;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#30340;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26469;&#25552;&#39640;&#23545;LLaMA&#65292;LLaVA&#21644;VL-B&#30340;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PEFT&#65289;&#26041;&#27861;&#20013;&#65292;&#30001;&#20110;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;LoRA&#21450;&#20854;&#21464;&#31181;&#26041;&#27861;&#22240;&#27492;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#31934;&#24230;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26435;&#37325;&#20998;&#35299;&#20998;&#26512;&#26041;&#27861;&#26469;&#30740;&#31350;FT&#21644;LoRA&#20043;&#38388;&#30340;&#20869;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#27169;&#25311;FT&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DoRA&#30340;&#26435;&#37325;&#20998;&#35299;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#12290;DoRA&#23558;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#20998;&#35299;&#20026;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24133;&#24230;&#21644;&#26041;&#21521;&#65292;&#24182;&#19988;&#20855;&#20307;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#65292;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;DoRA&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;LoRA&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#20219;&#20309;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;&#24494;&#35843;LLaMA&#65292;LLaVA&#21644;VL-B&#19978;&#65292;DoRA&#22987;&#32456;&#20248;&#20110;LoRA&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09353v1 Announce Type: new Abstract: Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-B
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#25200;&#21160;&#30340;kNN-MT&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36229;&#26657;&#27491;&#38382;&#39064;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#32763;&#35793;&#20505;&#36873;&#12290;&#23454;&#39564;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20505;&#36873;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25200;&#21160;&#30340;&#24133;&#24230;&#25511;&#21046;&#22810;&#26679;&#24615;&#31243;&#24230;</title><link>https://arxiv.org/abs/2402.09344</link><description>&lt;p&gt;
&#20351;&#29992;&#25200;&#21160;kNN-MT&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Generating Diverse Translation with Perturbed kNN-MT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#25200;&#21160;&#30340;kNN-MT&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36229;&#26657;&#27491;&#38382;&#39064;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#32763;&#35793;&#20505;&#36873;&#12290;&#23454;&#39564;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20505;&#36873;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25200;&#21160;&#30340;&#24133;&#24230;&#25511;&#21046;&#22810;&#26679;&#24615;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22810;&#20010;&#32763;&#35793;&#20505;&#36873;&#33021;&#22815;&#35753;&#29992;&#25143;&#36873;&#25321;&#28385;&#36275;&#20182;&#20204;&#38656;&#27714;&#30340;&#32763;&#35793;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#20102;&#22810;&#26679;&#21270;&#29983;&#25104;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#20294;&#26159;&#20173;&#28982;&#23384;&#22312;&#25552;&#39640;&#22810;&#26679;&#24615;&#30340;&#31354;&#38388;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#20197;&#24448;&#30340;&#26041;&#27861;&#26410;&#35299;&#20915;&#36229;&#26657;&#27491;&#38382;&#39064;&#8212;&#8212;&#21363;&#27169;&#22411;&#20302;&#20272;&#19968;&#20010;&#19982;&#35757;&#32451;&#25968;&#25454;&#26126;&#26174;&#19981;&#21516;&#20294;&#21487;&#33021;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#25200;&#21160;&#30340;k&#26368;&#36817;&#37051;&#26426;&#22120;&#32763;&#35793;&#65288;kNN-MT&#65289;&#26469;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;kNN-MT&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#36229;&#26657;&#27491;&#38382;&#39064;&#23558;&#22810;&#26679;&#30340;&#35789;&#27719;&#32435;&#20837;&#20505;&#36873;&#38598;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20505;&#36873;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25200;&#21160;&#30340;&#24133;&#24230;&#25511;&#21046;&#22810;&#26679;&#24615;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09344v1 Announce Type: new Abstract: Generating multiple translation candidates would enable users to choose the one that satisfies their needs. Although there has been work on diversified generation, there exists room for improving the diversity mainly because the previous methods do not address the overcorrection problem -- the model underestimates a prediction that is largely different from the training data, even if that prediction is likely. This paper proposes methods that generate more diverse translations by introducing perturbed k-nearest neighbor machine translation (kNN-MT). Our methods expand the search space of kNN-MT and help incorporate diverse words into candidates by addressing the overcorrection problem. Our experiments show that the proposed methods drastically improve candidate diversity and control the degree of diversity by tuning the perturbation's magnitude.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ICDPO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#29992;&#20182;&#20154;&#30340;&#23545;&#40784;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#20248;&#21270;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09320</link><description>&lt;p&gt;
ICDPO: &#36890;&#36807;&#19978;&#19979;&#25991;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26377;&#25928;&#22320;&#20511;&#29992;&#20182;&#20154;&#30340;&#23545;&#40784;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ICDPO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#29992;&#20182;&#20154;&#30340;&#23545;&#40784;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#20248;&#21270;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20381;&#36182;&#20110;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65288;HPA&#65289;&#26469;&#30830;&#20445;&#29983;&#25104;&#23433;&#20840;&#20869;&#23481;&#12290;&#30001;&#20110;&#24494;&#35843;&#24102;&#26469;&#30340;&#24040;&#22823;&#25104;&#26412;&#65292;&#20986;&#29616;&#20102;&#20813;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#36890;&#36807;&#22806;&#37096;&#36741;&#21161;&#26041;&#27861;&#20462;&#25913;LLM&#35299;&#30721;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#20174;&#26412;&#36136;&#19978;&#22686;&#24378;LLM&#26412;&#36523;&#12290;&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;DPO&#30340;&#25512;&#23548;&#36807;&#31243;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;LLM&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#29366;&#24577;&#30340;&#21363;&#26102;&#25171;&#20998;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICDPO&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#20351;LLM&#33021;&#22815;&#36890;&#36807;ICL&#20174;&#20248;&#31168;&#30340;LLM&#20013;&#20511;&#29992;HPA&#33021;&#21147;&#65292;&#29983;&#25104;&#30001;&#19978;&#36848;&#21363;&#26102;&#25171;&#20998;&#22120;&#20272;&#35745;&#30340;&#33391;&#22909;&#23545;&#40784;&#30340;&#21709;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#26368;&#32456;&#24615;&#33021;&#12290;ICDPO&#21487;&#20197;&#36890;&#36807;&#20004;&#38454;&#27573;&#26816;&#32034;&#22120;&#21644;&#21319;&#32423;&#30340;&#25171;&#20998;&#22120;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#37117;&#20855;&#26377;&#30410;&#22788;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;ICDPO&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09320v1 Announce Type: cross Abstract: Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to ensure the generation of safe content. Due to the heavy cost associated with fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM decoding with external auxiliary methods. However, these methods do not essentially enhance the LLM itself. In this paper, we rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after In-context Learning (ICL). Accordingly, we propose a novel approach called In-Context Direct Preference Optimization (ICDPO). It enables LLMs to borrow the HPA capabilities from superior LLMs with ICL, generating well-aligned responses as estimated by the aforementioned instant scorer, thereby enhancing the final performance. ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer, both offering benefits. Extensive experiments sho
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.09283</link><description>&lt;p&gt;
&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#24212;&#29992;&#20013;&#24050;&#32463;&#24456;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#22238;&#22797;&#30340;&#39118;&#38505;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#31038;&#20250;&#20851;&#20999;&#65292;&#24182;&#28608;&#21457;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#27492;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#25688;&#35201;&#65292;&#22686;&#36827;&#23545;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#37325;&#35201;&#35838;&#39064;&#12290;&#20026;&#20102;&#26041;&#20415;&#21442;&#32771;&#65292;&#25105;&#20204;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#23545;&#25152;&#26377;&#22312;&#27492;&#35843;&#26597;&#20013;&#25552;&#21040;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/niconi19/LLM-conversation-safety&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#35757;&#32451;&#31574;&#30053;&#25552;&#39640;NLP&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#37319;&#29992;&#32454;&#24605;&#36830;&#24819;&#25552;&#31034;&#25216;&#26415;&#65292;&#23558;GPT-4&#20013;&#25552;&#28860;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;BERT&#27169;&#22411;&#65292;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#20026;&#36164;&#28304;&#26377;&#38480;&#25110;&#23553;&#38381;&#32593;&#32476;&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.09282</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#20248;&#21270;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;NLP&#20219;&#21153;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#35757;&#32451;&#31574;&#30053;&#25552;&#39640;NLP&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#37319;&#29992;&#32454;&#24605;&#36830;&#24819;&#25552;&#31034;&#25216;&#26415;&#65292;&#23558;GPT-4&#20013;&#25552;&#28860;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;BERT&#27169;&#22411;&#65292;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#20026;&#36164;&#28304;&#26377;&#38480;&#25110;&#23553;&#38381;&#32593;&#32476;&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#30340;&#25972;&#21512;&#21040;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#65292;&#20026;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#20943;&#23569;&#23545;&#22823;&#37327;&#20154;&#24037;&#27880;&#37322;&#30340;&#20381;&#36182;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32454;&#24605;&#36830;&#24819;&#65288;CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#20174;GPT-4&#20013;&#25552;&#28860;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25913;&#36827;&#36739;&#23567;&#27169;&#22411;BERT&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20219;&#21153;&#19978;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#65306;&#39318;&#20808;&#20351;&#29992;GPT-4&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#33976;&#39311;&#21644;&#21407;&#22987;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#30340;&#32452;&#21512;&#23545;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#28151;&#21512;&#35757;&#32451;&#31574;&#30053;&#26126;&#26174;&#20248;&#20110;&#20165;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;F1&#20998;&#25968;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#36164;&#28304;&#26377;&#38480;&#25110;&#23553;&#38381;&#32593;&#32476;&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09282v1 Announce Type: new Abstract: The integration of Large Language Models (LLMs) like GPT-4 into traditional Natural Language Processing (NLP) tasks has opened new avenues for enhancing model performance while reducing the reliance on extensive human annotations. This paper presents a novel approach that leverages the Chain of Thought (CoT) prompting technique to distill knowledge from GPT-4, subsequently applying it to improve the efficiency and effectiveness of a smaller model, BERT, on Named Entity Recognition (NER) tasks. Our method involves a two-phase training process: initially employing GPT-4 annotated data for pre-training and then refining the model with a combination of distilled and original human-annotated data. The results demonstrate that our mixed-training strategy significantly outperforms models trained solely on human annotations, achieving superior F1-scores and showcasing a cost-effective solution for resource-limited or closed-network settings. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#24494;&#35843;&#21644;&#38646;&#26679;&#26412;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#21457;&#29616;&#20010;&#24615;&#21270;&#24494;&#35843;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#24773;&#24863;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20063;&#33719;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.09269</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Personalized Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#24494;&#35843;&#21644;&#38646;&#26679;&#26412;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#21457;&#29616;&#20010;&#24615;&#21270;&#24494;&#35843;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#24773;&#24863;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20063;&#33719;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36890;&#29992;&#24615;&#22312;&#38656;&#35201;&#20010;&#24615;&#21270;&#22238;&#24212;&#30340;&#22330;&#26223;&#65288;&#22914;&#25512;&#33616;&#31995;&#32479;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#65289;&#20013;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;LLM&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#24494;&#35843;&#21644;&#38646;&#26679;&#26412;&#25512;&#29702;&#26041;&#27861;&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#38750;&#20010;&#24615;&#21270;&#27169;&#22411;&#30456;&#27604;&#65292;&#20010;&#24615;&#21270;&#24494;&#35843;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#24773;&#24863;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20010;&#24615;&#21270;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;LLM&#26550;&#26500;&#19978;&#33719;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20027;&#35266;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#20013;&#25552;&#21319;LLM&#33021;&#21147;&#30340;&#20010;&#24615;&#21270;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09269v1 Announce Type: cross Abstract: Large language models (LLMs) have significantly advanced Natural Language Processing (NLP) tasks in recent years. However, their universal nature poses limitations in scenarios requiring personalized responses, such as recommendation systems and chatbots. This paper investigates methods to personalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on subjective tasks. Results demonstrate that personalized fine-tuning improves model reasoning compared to non-personalized models. Experiments on datasets for emotion recognition and hate speech detection show consistent performance gains with personalized methods across different LLM architectures. These findings underscore the importance of personalization for enhancing LLM capabilities in subjective text perception tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#24341;&#23548;&#27169;&#22411;&#21521;&#23454;&#20107;&#24615;&#38752;&#36817;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#32622;&#20449;&#20272;&#35745;&#21644;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.09267</link><description>&lt;p&gt;
&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#65306;&#36890;&#36807;&#33258;&#35780;&#20943;&#32531;LLMs&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#24341;&#23548;&#27169;&#22411;&#21521;&#23454;&#20107;&#24615;&#38752;&#36817;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#32622;&#20449;&#20272;&#35745;&#21644;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26174;&#31034;&#20986;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#65288;&#21363;&#8220;&#24187;&#35273;&#8221;&#65289;&#24448;&#24448;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#20351;&#23427;&#20204;&#20855;&#26377;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24187;&#35273;&#38382;&#39064;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#20107;&#23454;&#24615;&#27880;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#65292;&#21363;&#21033;&#29992;LLM&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#25552;&#20379;&#35757;&#32451;&#20449;&#21495;&#65292;&#23558;&#27169;&#22411;&#24341;&#23548;&#21521;&#23454;&#20107;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#33258;&#25105;&#35780;&#20272;&#32452;&#20214;Self-Eval&#32435;&#20837;&#21040;LLM&#20013;&#65292;&#20197;&#20165;&#22522;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#39564;&#35777;&#20854;&#33258;&#24049;&#29983;&#25104;&#30340;&#22238;&#22797;&#30340;&#23454;&#20107;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#25105;&#30693;&#35782;&#35843;&#25972;&#65288;SK-Tuning&#65289;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#25913;&#21892;&#27169;&#22411;&#30340;&#32622;&#20449;&#20272;&#35745;&#21644;&#26657;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#33258;&#25105;&#27880;&#37322;&#30340;&#22238;&#22797;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#31639;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09267v1 Announce Type: cross Abstract: Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. "hallucinations", even when they hold relevant knowledge. To address these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's self-evaluation ability by improving the model's confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23616;&#37096;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;SyntaxShap&#65292;&#20854;&#36890;&#36807;&#32771;&#34385;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#27861;&#26469;&#25193;&#23637;Shapley&#20540;&#20197;&#35299;&#37322;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#12290;&#36890;&#36807;&#37319;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;SyntaxShap&#21482;&#32771;&#34385;&#30001;&#20381;&#36182;&#26641;&#32422;&#26463;&#30340;&#32852;&#30431;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09259</link><description>&lt;p&gt;
SyntaxShap&#65306;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#35821;&#27861;&#24863;&#30693;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SyntaxShap: Syntax-aware Explainability Method for Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23616;&#37096;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;SyntaxShap&#65292;&#20854;&#36890;&#36807;&#32771;&#34385;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#27861;&#26469;&#25193;&#23637;Shapley&#20540;&#20197;&#35299;&#37322;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#12290;&#36890;&#36807;&#37319;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;SyntaxShap&#21482;&#32771;&#34385;&#30001;&#20381;&#36182;&#26641;&#32422;&#26463;&#30340;&#32852;&#30431;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#38656;&#35201;&#30830;&#20445;&#20854;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21463;&#21040;&#20102;&#37325;&#35201;&#20851;&#27880;&#65292;&#20294;&#20173;&#26377;&#19968;&#20010;&#23578;&#26410;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#21363;&#20351;&#29992;&#38024;&#23545;&#25991;&#26412;&#25968;&#25454;&#37327;&#36523;&#23450;&#21046;&#30340;&#26041;&#27861;&#35299;&#37322;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SyntaxShap&#65292;&#19968;&#31181;&#23616;&#37096;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#32771;&#34385;&#20102;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;Shapley&#20540;&#25193;&#23637;&#21040;&#32771;&#34385;&#22522;&#20110;&#35299;&#26512;&#30340;&#21477;&#27861;&#20381;&#36182;&#20851;&#31995;&#12290;&#37319;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;SyntaxShap&#21482;&#32771;&#34385;&#30001;&#20381;&#36182;&#26641;&#32422;&#26463;&#30340;&#32852;&#30431;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#35780;&#20272;&#26469;&#27604;&#36739;SyntaxShap&#21450;&#20854;&#21152;&#26435;&#24418;&#24335;&#19982;&#38024;&#23545;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#26368;&#26032;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20351;&#29992;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#22797;&#26434;&#24230;&#12289;&#36830;&#36143;&#24615;&#21644;&#35299;&#37322;&#19982;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#22810;&#26679;&#21270;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09259v1 Announce Type: cross Abstract: To harness the power of large language models in safety-critical domains we need to ensure the explainability of their predictions. However, despite the significant attention to model interpretability, there remains an unexplored domain in explaining sequence-to-sequence tasks using methods tailored for textual data. This paper introduces SyntaxShap, a local, model-agnostic explainability method for text generation that takes into consideration the syntax in the text data. The presented work extends Shapley values to account for parsing-based syntactic dependencies. Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree. We adopt a model-based evaluation to compare SyntaxShap and its weighted form to state-of-the-art explainability methods adapted to text generation tasks, using diverse metrics including faithfulness, complexity, coherency, and semantic alignment of the explanations to the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23450;&#20041;&#39057;&#35889;&#28388;&#27874;&#22120;&#21644;&#27880;&#24847;&#21147;&#38519;&#38449;&#65292;&#25581;&#31034;&#20102;&#23558;&#20013;&#38388;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#34920;&#30340;&#35299;&#37322;&#24037;&#20855;&#23545;&#20110;transformer-based LLMs&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#29305;&#23450;&#39057;&#35889;&#21306;&#22495;&#30340;&#25439;&#22833;&#23545;&#20110;&#20445;&#25345;&#20302;&#25439;&#22833;&#26159;&#21487;&#34892;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.09221</link><description>&lt;p&gt;
&#39057;&#35889;&#28388;&#27874;&#22120;&#12289;&#26263;&#20449;&#21495;&#21644;&#27880;&#24847;&#21147;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Spectral Filters, Dark Signals, and Attention Sinks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09221
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23450;&#20041;&#39057;&#35889;&#28388;&#27874;&#22120;&#21644;&#27880;&#24847;&#21147;&#38519;&#38449;&#65292;&#25581;&#31034;&#20102;&#23558;&#20013;&#38388;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#34920;&#30340;&#35299;&#37322;&#24037;&#20855;&#23545;&#20110;transformer-based LLMs&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#29305;&#23450;&#39057;&#35889;&#21306;&#22495;&#30340;&#25439;&#22833;&#23545;&#20110;&#20445;&#25345;&#20302;&#25439;&#22833;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09221v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#23558;&#20013;&#38388;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#34920;&#19978;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#36716;&#25442;&#22120;&#22411;LLM&#30340;&#35299;&#37322;&#24037;&#20855;&#65292;&#20063;&#31216;&#20026;logit lens&#12290;&#25105;&#20204;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#23450;&#37327;&#25193;&#23637;&#65292;&#24182;&#22522;&#20110;&#23558;&#35789;&#27719;&#23884;&#20837;&#21644;&#35299;&#23884;&#30697;&#38453;&#30340;&#22855;&#24322;&#21521;&#37327;&#20998;&#25104;&#27874;&#27573;&#26469;&#23450;&#20041;&#20013;&#38388;&#34920;&#31034;&#30340;&#39057;&#35889;&#28388;&#27874;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39057;&#35889;&#30340;&#23614;&#37096;&#20132;&#25442;&#30340;&#20449;&#21495;&#36127;&#36131;&#27880;&#24847;&#21147;&#38519;&#38449;&#65288;Xiao et al. 2023&#65289;&#65292;&#25105;&#20204;&#23545;&#27492;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#20197;&#23618;&#20026;&#22522;&#30784;&#25233;&#21046;&#20102;&#23884;&#20837;&#39057;&#35889;&#30340;&#30456;&#24403;&#22823;&#37096;&#20998;&#65292;&#20294;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25439;&#22833;&#20173;&#28982;&#21487;&#20197;&#20445;&#25345;&#36739;&#20302;&#65292;&#21482;&#35201;&#20445;&#25345;&#27880;&#24847;&#21147;&#38519;&#38449;&#21363;&#21487;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21560;&#24341;&#22810;&#20010;&#20196;&#29260;&#27880;&#24847;&#21147;&#30340;&#20196;&#29260;&#34920;&#31034;&#22312;&#39057;&#35889;&#30340;&#23614;&#37096;&#20855;&#26377;&#36739;&#22823;&#30340;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09221v1 Announce Type: new Abstract: Projecting intermediate representations onto the vocabulary is an increasingly popular interpretation tool for transformer-based LLMs, also known as the logit lens. We propose a quantitative extension to this approach and define spectral filters on intermediate representations based on partitioning the singular vectors of the vocabulary embedding and unembedding matrices into bands. We find that the signals exchanged in the tail end of the spectrum are responsible for attention sinking (Xiao et al. 2023), of which we provide an explanation. We find that the loss of pretrained models can be kept low despite suppressing sizable parts of the embedding spectrum in a layer-dependent way, as long as attention sinking is preserved. Finally, we discover that the representation of tokens that draw attention from many tokens have large projections on the tail end of the spectrum.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25776;&#20889;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#20445;&#30041;&#20256;&#32479;&#36741;&#23548;&#31995;&#32479;&#32467;&#26500;&#21644;&#25945;&#23398;&#27861;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09216</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;AutoTutor&#30340;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Scaling the Authoring of AutoTutors with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25776;&#20889;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#20445;&#30041;&#20256;&#32479;&#36741;&#23548;&#31995;&#32479;&#32467;&#26500;&#21644;&#25945;&#23398;&#27861;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25945;&#32946;&#39046;&#22495;&#26377;&#22810;&#31181;&#29992;&#36884;&#65292;&#20174;&#33258;&#21160;&#39064;&#30446;&#29983;&#25104;&#21040;&#20316;&#25991;&#35780;&#20272;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25776;&#20889;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;LLMs&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#23427;&#20204;&#23481;&#26131;&#20559;&#31163;&#25152;&#26399;&#26395;&#30340;&#25945;&#23398;&#31574;&#30053;&#65292;&#20363;&#22914;&#27844;&#38706;&#31572;&#26696;&#32473;&#23398;&#29983;&#65292;&#24635;&#20307;&#19978;&#25552;&#20379;&#30340;&#20445;&#35777;&#24456;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34429;&#28982;&#24102;&#26377;&#26576;&#20123;&#38480;&#21046;&#30340;LLMs&#21487;&#20197;&#21462;&#20195;&#23398;&#31185;&#19987;&#23478;&#30340;&#20301;&#32622;&#65292;&#20294;&#25972;&#20307;&#30340;&#25945;&#23398;&#35774;&#35745;&#20173;&#38656;&#25163;&#24037;&#21046;&#20316;&#20197;&#21462;&#24471;&#26368;&#20339;&#23398;&#20064;&#25928;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#21407;&#21017;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#31034;&#20363;&#30340;&#31471;&#21040;&#31471;&#36741;&#23548;&#31995;&#32479;&#65292;&#21629;&#21517;&#20026;MWPTutor&#65292;&#23427;&#20351;&#29992;LLMs&#22635;&#20805;&#39044;&#23450;&#20041;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#20445;&#30041;&#20102;&#22810;&#24180;&#26469;&#30001;&#23398;&#20064;&#31185;&#23398;&#23478;&#24320;&#21457;&#30340;&#20256;&#32479;&#36741;&#23548;&#31995;&#32479;&#30340;&#32467;&#26500;&#21644;&#25945;&#23398;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09216v1 Announce Type: new Abstract: Large Language Models (LLMs) have found several use cases in education, ranging from automatic question generation to essay evaluation. In this paper, we explore the potential of using Large Language Models (LLMs) to author Intelligent Tutoring Systems. A common pitfall of LLMs is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees. We posit that while LLMs with certain guardrails can take the place of subject experts, the overall pedagogical design still needs to be handcrafted for the best learning results. Based on this principle, we create a sample end-to-end tutoring system named MWPTutor, which uses LLMs to fill in the state space of a pre-defined finite state transducer. This approach retains the structure and the pedagogy of traditional tutoring systems that has been developed over the years by learning scientists but brings in additional flexibility
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38544;&#24335;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;Intention-in-Interaction (IN3) &#22522;&#20934;&#21644;&#22312;&#20195;&#29702;&#35774;&#35745;&#20013;&#34701;&#20837;&#27169;&#22411;&#19987;&#23478;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#21319;&#23545;&#29992;&#25143;&#25351;&#20196;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.09205</link><description>&lt;p&gt;
&#21578;&#35785;&#25105;&#26356;&#22810;&#65281;&#38754;&#21521;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38544;&#24335;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38544;&#24335;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;Intention-in-Interaction (IN3) &#22522;&#20934;&#21644;&#22312;&#20195;&#29702;&#35774;&#35745;&#20013;&#34701;&#20837;&#27169;&#22411;&#19987;&#23478;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#21319;&#23545;&#29992;&#25143;&#25351;&#20196;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#20195;&#29702;&#24120;&#24120;&#32570;&#20047;&#26377;&#25928;&#30340;&#29992;&#25143;&#21442;&#19982;&#26426;&#21046;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#25351;&#20196;&#20013;&#24120;&#35265;&#30340;&#27169;&#31946;&#24615;&#65292;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#34429;&#28982;&#36825;&#20123;&#20195;&#29702;&#22312;&#21046;&#23450;&#31574;&#30053;&#21644;&#25191;&#34892;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#23547;&#27714;&#28548;&#28165;&#21644;&#25235;&#20303;&#31934;&#30830;&#30340;&#29992;&#25143;&#24847;&#22270;&#26041;&#38754;&#21364;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Intention-in-Interaction (IN3) &#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#26126;&#30830;&#30340;&#26597;&#35810;&#26816;&#26597;&#29992;&#25143;&#30340;&#38544;&#21547;&#24847;&#22270;&#30340;&#26032;&#39062;&#22522;&#20934;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#27169;&#22411;&#19987;&#23478;&#20316;&#20026;&#19978;&#28216;&#34701;&#20837;&#20195;&#29702;&#35774;&#35745;&#20013;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;-&#20195;&#29702;&#20132;&#20114;&#12290;&#21033;&#29992;IN3&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35757;&#32451;&#20102;Mistral-Interact&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20027;&#21160;&#35780;&#20272;&#20219;&#21153;&#30340;&#27169;&#31946;&#24615;&#65292;&#35810;&#38382;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#30446;&#26631;&#65292;&#28982;&#21518;&#24320;&#22987;&#19979;&#28216;&#20195;&#29702;&#20219;&#21153;&#25191;&#34892;&#12290;&#23558;&#20854;&#38598;&#25104;&#21040;XAgent&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#23545;&#22686;&#24378;&#30340;&#20195;&#29702;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#29992;&#25143;&#25351;&#20196;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09205v1 Announce Type: cross Abstract: Current language model-driven agents often lack mechanisms for effective user participation, which is crucial given the vagueness commonly found in user instructions. Although adept at devising strategies and performing tasks, these agents struggle with seeking clarification and grasping precise user intentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a novel benchmark designed to inspect users' implicit intentions through explicit queries. Next, we propose the incorporation of model experts as the upstream in agent designs to enhance user-agent interaction. Employing IN3, we empirically train Mistral-Interact, a powerful model that proactively assesses task vagueness, inquires user intentions, and refines them into actionable goals before starting downstream agent task execution. Integrating it into the XAgent framework, we comprehensively evaluate the enhanced agent system regarding user instruction understand
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20195;&#29702;&#24341;&#23548;&#30340;&#39640;&#25928;&#37325;&#37319;&#26679;&#26041;&#27861;&#26469;&#25913;&#36827;&#40657;&#30418;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#12290;&#36890;&#36807;&#20272;&#35745;&#21333;&#35789;&#29983;&#25104;&#27010;&#29575;&#20316;&#20026;&#20266;&#30333;&#30418;&#29305;&#24449;&#65292;&#36873;&#25321;&#23569;&#37327;&#20195;&#34920;&#24615;&#35789;&#27719;&#36827;&#34892;&#22810;&#27425;&#37325;&#37319;&#26679;&#65292;&#22312;&#21253;&#21547;&#20154;&#31867;&#25991;&#26412;&#21644;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09199</link><description>&lt;p&gt;
&#21313;&#20010;&#20851;&#38190;&#35789;&#20173;&#28982;&#26377;&#29992;&#65306;&#36890;&#36807;&#20195;&#29702;&#24341;&#23548;&#30340;&#39640;&#25928;&#37325;&#37319;&#26679;&#25913;&#36827;&#40657;&#30418;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20195;&#29702;&#24341;&#23548;&#30340;&#39640;&#25928;&#37325;&#37319;&#26679;&#26041;&#27861;&#26469;&#25913;&#36827;&#40657;&#30418;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#12290;&#36890;&#36807;&#20272;&#35745;&#21333;&#35789;&#29983;&#25104;&#27010;&#29575;&#20316;&#20026;&#20266;&#30333;&#30418;&#29305;&#24449;&#65292;&#36873;&#25321;&#23569;&#37327;&#20195;&#34920;&#24615;&#35789;&#27719;&#36827;&#34892;&#22810;&#27425;&#37325;&#37319;&#26679;&#65292;&#22312;&#21253;&#21547;&#20154;&#31867;&#25991;&#26412;&#21644;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#23427;&#20204;&#30340;&#28389;&#29992;&#24341;&#21457;&#20102;&#35768;&#22810;&#19981;&#21463;&#27426;&#36814;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#22914;&#20551;&#26032;&#38395;&#12289;&#23398;&#26415;&#19981;&#35802;&#23454;&#21644;&#20449;&#24687;&#27745;&#26579;&#12290;&#36825;&#20351;&#24471;AI&#29983;&#25104;&#25991;&#26412;&#65288;AIGT&#65289;&#30340;&#26816;&#27979;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#30333;&#30418;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#36890;&#24120;&#20248;&#20110;&#40657;&#30418;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#35775;&#38382;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#19981;&#36866;&#29992;&#20110;&#40657;&#30418;&#35774;&#32622;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27425;&#37325;&#37319;&#26679;&#26469;&#20272;&#35745;&#21333;&#35789;&#29983;&#25104;&#27010;&#29575;&#20316;&#20026;&#20266;&#30333;&#30418;&#29305;&#24449;&#20197;&#24110;&#21161;&#25913;&#36827;&#40657;&#30418;AIGT&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;POGER&#65292;&#19968;&#31181;&#20195;&#29702;&#24341;&#23548;&#30340;&#39640;&#25928;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#22312;&#40657;&#30418;AIGT&#26816;&#27979;&#20013;&#36873;&#25321;&#19968;&#20010;&#23567;&#30340;&#20195;&#34920;&#24615;&#35789;&#27719;&#23376;&#38598;&#65288;&#20363;&#22914;10&#20010;&#35789;&#65289;&#65292;&#36827;&#34892;&#22810;&#27425;&#37325;&#37319;&#26679;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21253;&#21547;&#20154;&#31867;&#25991;&#26412;&#21644;&#19971;&#20010;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09199v1 Announce Type: cross Abstract: With the rapidly increasing application of large language models (LLMs), their abuse has caused many undesirable societal problems such as fake news, academic dishonesty, and information pollution. This makes AI-generated text (AIGT) detection of great importance. Among existing methods, white-box methods are generally superior to black-box methods in terms of performance and generalizability, but they require access to LLMs' internal states and are not applicable to black-box settings. In this paper, we propose to estimate word generation probabilities as pseudo white-box features via multiple re-sampling to help improve AIGT detection under the black-box setting. Specifically, we design POGER, a proxy-guided efficient re-sampling method, which selects a small subset of representative words (e.g., 10 words) for performing multiple re-sampling in black-box AIGT detection. Experiments on datasets containing texts from humans and seven LL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19971;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#24515;&#29702;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#20154;&#31867;&#19968;&#26679;&#23384;&#22312;&#38750;&#29702;&#24615;&#65292;&#20294;&#23637;&#31034;&#30340;&#38750;&#29702;&#24615;&#26041;&#24335;&#19982;&#20154;&#31867;&#20559;&#35265;&#19981;&#21516;&#65292;&#21516;&#26102;&#36824;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09193</link><description>&lt;p&gt;
(&#19981;)&#29702;&#24615;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
(Ir)rationality and Cognitive Biases in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19971;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#24515;&#29702;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#20154;&#31867;&#19968;&#26679;&#23384;&#22312;&#38750;&#29702;&#24615;&#65292;&#20294;&#23637;&#31034;&#30340;&#38750;&#29702;&#24615;&#26041;&#24335;&#19982;&#20154;&#31867;&#20559;&#35265;&#19981;&#21516;&#65292;&#21516;&#26102;&#36824;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#23637;&#29616;&#20986;&#29702;&#24615;&#25512;&#29702;&#65311;&#30001;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#25152;&#21547;&#30340;&#20154;&#31867;&#20559;&#35265;&#65292;LLMs&#24050;&#34987;&#35777;&#23454;&#23384;&#22312;&#20154;&#31867;&#20559;&#35265;&#65307;&#28982;&#32780;&#65292;&#20854;&#26159;&#21542;&#21453;&#26144;&#20986;&#20102;&#29702;&#24615;&#25512;&#29702;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#19971;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#26469;&#33258;&#35748;&#30693;&#24515;&#29702;&#23398;&#25991;&#29486;&#30340;&#20219;&#21153;&#20013;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21644;&#20154;&#31867;&#19968;&#26679;&#65292;LLMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#29702;&#24615;&#12290;&#28982;&#32780;&#65292;LLMs&#23637;&#29616;&#20986;&#30340;&#36825;&#31181;&#38750;&#29702;&#24615;&#19982;&#20154;&#31867;&#30340;&#20559;&#35265;&#19981;&#21516;&#12290;&#24403;LLMs&#32473;&#20986;&#38169;&#35823;&#31572;&#26696;&#26102;&#65292;&#23427;&#20204;&#36890;&#24120;&#20250;&#20197;&#19982;&#20154;&#31867;&#20559;&#35265;&#19981;&#21516;&#30340;&#26041;&#24335;&#38169;&#35823;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;LLMs&#36824;&#23637;&#29616;&#20986;&#20102;&#21709;&#24212;&#30340;&#26174;&#33879;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#34920;&#26126;&#20102;&#39069;&#22806;&#30340;&#38750;&#29702;&#24615;&#23618;&#38754;&#12290;&#38500;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#26412;&#25991;&#36824;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#35780;&#20272;&#21644;&#27604;&#36739;&#36825;&#31867;&#27169;&#22411;&#30340;&#19981;&#21516;&#21151;&#33021;&#65292;&#23545;&#26041;&#27861;&#35770;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09193v1 Announce Type: cross Abstract: Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#38382;&#31572;&#26469;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09177</link><description>&lt;p&gt;
&#20511;&#21161;&#22810;&#36718;&#20132;&#20114;&#21033;&#29992;&#19978;&#19979;&#25991;&#36827;&#34892;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#38382;&#31572;&#26469;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36234;&#29425;&#25915;&#20987;&#36890;&#36807;&#24494;&#22937;&#22320;&#20462;&#25913;&#25915;&#20987;&#26597;&#35810;&#26469;&#25552;&#21462;&#26377;&#23475;&#20449;&#24687;&#12290;&#38543;&#30528;&#38450;&#24481;&#26426;&#21046;&#30340;&#36827;&#21270;&#65292;&#36234;&#29425;&#25915;&#20987;&#30452;&#25509;&#33719;&#21462;&#26377;&#23475;&#20449;&#24687;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#20154;&#31867;&#38388;&#25509;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#30340;&#23454;&#36341;&#21551;&#21457;&#65292;&#38024;&#23545;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;LLMs&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#33258;&#22238;&#24402;&#24615;&#36136;&#12290;&#25105;&#20204;&#35748;&#20026;&#20808;&#21069;&#30340;&#19978;&#19979;&#25991;&#8212;&#8212;&#25915;&#20987;&#26597;&#35810;&#20043;&#21069;&#30340;&#20449;&#24687;&#22312;&#23454;&#29616;&#24378;&#22823;&#30340;&#36234;&#29425;&#25915;&#20987;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21021;&#27493;&#38382;&#31572;&#23545;&#19982;LLMs&#20132;&#20114;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#30340;&#22238;&#31572;&#26397;&#30528;&#25581;&#31034;&#8220;&#26399;&#26395;&#30340;&#8221;&#26377;&#23475;&#20449;&#24687;&#30340;&#26041;&#21521;&#21457;&#23637;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;LLMs&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09177v1 Announce Type: cross Abstract: Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which aim to extract harmful information by subtly modifying the attack query. As defense mechanisms evolve, directly obtaining harmful information becomes increasingly challenging for Jailbreaking attacks. In this work, inspired by human practices of indirect context to elicit harmful information, we focus on a new attack form called Contextual Interaction Attack. The idea relies on the autoregressive nature of the generation process in LLMs. We contend that the prior context--the information preceding the attack query--plays a pivotal role in enabling potent Jailbreaking attacks. Specifically, we propose an approach that leverages preliminary question-answer pairs to interact with the LLM. By doing so, we guide the responses of the model toward revealing the 'desired' harmful information. We conduct experiments on four different LLMs and demonstrate the efficacy of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;Chinese MentalBERT&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#20998;&#26512;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#24515;&#29702;&#23398;&#35789;&#20856;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09151</link><description>&lt;p&gt;
Chinese MentalBERT: &#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#38024;&#23545;&#20013;&#22269;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#20998;&#26512;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;Chinese MentalBERT&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#20998;&#26512;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#24515;&#29702;&#23398;&#35789;&#20856;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#31038;&#20132;&#23186;&#20307;&#30340;&#24433;&#21709;&#65292;&#24515;&#29702;&#38382;&#39064;&#22312;&#24403;&#21069;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#31038;&#20132;&#23186;&#20307;&#25104;&#20026;&#20010;&#20154;&#20998;&#20139;&#24863;&#21463;&#30340;&#37325;&#35201;&#20986;&#21475;&#12290;&#36825;&#23548;&#33268;&#27599;&#22825;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#20854;&#20013;&#36127;&#38754;&#24773;&#32490;&#26377;&#28508;&#21147;&#24341;&#21457;&#21361;&#26426;&#12290;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#20986;&#33021;&#22815;&#39640;&#25928;&#20998;&#26512;&#36825;&#20123;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#26174;&#31034;&#20986;&#25928;&#26524;&#65292;&#20294;&#38024;&#23545;&#24515;&#29702;&#23398;&#31561;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23384;&#22312;&#26126;&#26174;&#32570;&#22833;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25910;&#38598;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#20016;&#23500;&#20102;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;336&#19975;&#26465;&#25991;&#26412;&#26465;&#30446;&#30340;&#32508;&#21512;&#25968;&#25454;&#24211;&#12290;&#20026;&#25552;&#39640;&#27169;&#22411;&#22312;&#24515;&#29702;&#25991;&#26412;&#20998;&#26512;&#19978;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#24515;&#29702;&#23398;&#35789;&#20856;&#34701;&#20837;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#26426;&#21046;&#12290;&#22312;&#29616;&#26377;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#19978;&#36827;&#34892;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09151v1 Announce Type: new Abstract: In the current environment, psychological issues are prevalent and widespread, with social media serving as a key outlet for individuals to share their feelings. This results in the generation of vast quantities of data daily, where negative emotions have the potential to precipitate crisis situations. There is a recognized need for models capable of efficient analysis. While pre-trained language models have demonstrated their effectiveness broadly, there's a noticeable gap in pre-trained models tailored for specialized domains like psychology. To address this, we have collected a huge dataset from Chinese social media platforms and enriched it with publicly available datasets to create a comprehensive database encompassing 3.36 million text entries. To enhance the model's applicability to psychological text analysis, we integrated psychological lexicons into the pre-training masking mechanism. Building on an existing Chinese language mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;NLP&#20219;&#21153;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#19982;&#25913;&#36827;&#30340;&#24490;&#29615;&#35838;&#31243;&#23398;&#20064;&#65288;MCCL&#65289;&#30456;&#32467;&#21512;&#26102;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;NLP&#27169;&#22411;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#37325;&#35201;&#31361;&#30772;&#12290;</title><link>https://arxiv.org/abs/2402.09141</link><description>&lt;p&gt;
&#36890;&#36807;&#25112;&#30053;&#25991;&#26412;&#22686;&#24378;&#25512;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#30740;&#31350;&#65306;&#22686;&#24378;&#26041;&#27861;&#21644;&#35838;&#31243;&#31574;&#30053;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Advancing NLP Models with Strategic Text Augmentation: A Comprehensive Study of Augmentation Methods and Curriculum Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;NLP&#20219;&#21153;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#19982;&#25913;&#36827;&#30340;&#24490;&#29615;&#35838;&#31243;&#23398;&#20064;&#65288;MCCL&#65289;&#30456;&#32467;&#21512;&#26102;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;NLP&#27169;&#22411;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#37325;&#35201;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#25991;&#26412;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#21487;&#38752;&#12289;&#26222;&#36941;&#35777;&#25454;&#30340;&#38382;&#39064;&#12290;&#23427;&#32771;&#23519;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#22686;&#24378;&#35757;&#32451;&#38598;&#20197;&#25552;&#39640;&#20027;&#39064;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#20882;&#29359;&#35821;&#35328;&#26816;&#27979;&#31561;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#19981;&#20165;&#24378;&#35843;&#20102;&#22686;&#24378;&#26041;&#27861;&#65292;&#36824;&#24378;&#35843;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#30495;&#23454;&#21644;&#22686;&#24378;&#23454;&#20363;&#30340;&#25112;&#30053;&#39034;&#24207;&#12290;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#20026;&#22686;&#24378;&#25968;&#25454;&#38598;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#25913;&#36827;&#30340;&#24490;&#29615;&#35838;&#31243;&#23398;&#20064;&#65288;MCCL&#65289;&#65292;&#36825;&#22312;&#35813;&#39046;&#22495;&#20013;&#23646;&#20110;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29305;&#23450;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#19982;MCCL&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#22312;NLP&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#23545;&#21487;&#38752;&#30340;&#22686;&#24378;&#26041;&#27861;&#21644;&#25112;&#30053;&#39034;&#24207;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09141v1 Announce Type: cross Abstract: This study conducts a thorough evaluation of text augmentation techniques across a variety of datasets and natural language processing (NLP) tasks to address the lack of reliable, generalized evidence for these methods. It examines the effectiveness of these techniques in augmenting training sets to improve performance in tasks such as topic classification, sentiment analysis, and offensive language detection. The research emphasizes not only the augmentation methods, but also the strategic order in which real and augmented instances are introduced during training. A major contribution is the development and evaluation of Modified Cyclical Curriculum Learning (MCCL) for augmented datasets, which represents a novel approach in the field. Results show that specific augmentation methods, especially when integrated with MCCL, significantly outperform traditional training approaches in NLP model performance. These results underscore the need
&lt;/p&gt;</description></item><item><title>DolphCoder&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#23548;&#21644;&#33258;&#25105;&#35780;&#20272;&#25552;&#39640;&#20102;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#22312;HumanEval&#21644;MBPP&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09136</link><description>&lt;p&gt;
DolphCoder: &#29992;&#22810;&#26679;&#21270;&#21644;&#22810;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#36827;&#34892;&#22238;&#22768;&#23450;&#20301;&#30340;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09136
&lt;/p&gt;
&lt;p&gt;
DolphCoder&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#23548;&#21644;&#33258;&#25105;&#35780;&#20272;&#25552;&#39640;&#20102;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#22312;HumanEval&#21644;MBPP&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Code Large Language Models (Code LLMs)&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;Code LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#25105;&#35780;&#20272;&#30340;&#22810;&#26679;&#21270;&#25351;&#20196;&#27169;&#22411;&#65288;DolphCoder&#65289;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;&#23427;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#30446;&#26631;&#65292;&#24182;&#32467;&#21512;&#20195;&#30721;&#35780;&#20272;&#30446;&#26631;&#26469;&#22686;&#24378;&#20854;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;HumanEval&#21644;MBPP&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#30340;&#20195;&#30721;&#25351;&#20196;&#35843;&#25972;&#24037;&#20316;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;&#65288;1&#65289;&#36890;&#36807;&#22686;&#21152;&#20855;&#26377;&#19981;&#21516;&#25512;&#29702;&#36335;&#24452;&#30340;&#22810;&#26679;&#21270;&#21709;&#24212;&#26469;&#22686;&#24378;LLMs&#30340;&#20195;&#30721;&#33021;&#21147;&#12290; &#65288;2&#65289;&#25552;&#39640;&#35780;&#20272;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#27491;&#30830;&#24615;&#30340;&#33021;&#21147;&#20063;&#20250;&#22686;&#24378;&#20854;&#21019;&#36896;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09136v1 Announce Type: cross Abstract: Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Several instruction tuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are: (1) Augmenting more diverse responses with distinct reasoning paths increases the code capability of LLMs. (2) Improving one's ability to evaluate the correctness of code solutions also enhances their ability to create it.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;MonoCoder&#36827;&#34892;MPI-based&#31243;&#24207;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09126</link><description>&lt;p&gt;
MPIrigen: &#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
MPIrigen: MPI Code Generation through Domain-Specific Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;MonoCoder&#36827;&#34892;MPI-based&#31243;&#24207;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#20013;&#65292;&#39640;&#25928;&#30340;&#24182;&#34892;&#35745;&#31639;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#28040;&#24687;&#20256;&#36882;&#25509;&#21475;&#65288;MPI&#65289;&#38598;&#25104;&#39046;&#22495;&#12290;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#31243;&#24207;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24182;&#34892;&#32534;&#31243;&#20219;&#21153;&#65292;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#25506;&#35752;&#20102;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#31243;&#24207;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#21457;&#29616;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#22914;GPT-3.5&#21644;PolyCoder&#65288;&#19987;&#38376;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#27169;&#22411;&#65289;&#65292;&#22312;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#27604;&#36890;&#29992;&#31243;&#24207;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;MPI&#30456;&#20851;&#32534;&#31243;&#35821;&#35328;C&#21644;C++&#39044;&#35757;&#32451;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;MonoCoder&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;HPCorpusMPI&#19978;&#23545;MonoCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;MPI-based&#31243;&#24207;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09126v1 Announce Type: cross Abstract: The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored. This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI. We call the r
&lt;/p&gt;</description></item><item><title>SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09025</link><description>&lt;p&gt;
SLEB: &#36890;&#36807;&#20887;&#20313;&#39564;&#35777;&#21644;&#28040;&#38500;Transformer&#22359;&#20248;&#21270;LLM&#30340;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09025
&lt;/p&gt;
&lt;p&gt;
SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#32473;&#23454;&#38469;&#37096;&#32626;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#31934;&#31616;&#65292;&#19968;&#31181;&#26088;&#22312;&#20943;&#23567;LLM&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;&#20887;&#20313;&#32452;&#20214;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#31934;&#31616;&#26377;&#24076;&#26395;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31471;&#21040;&#31471;LLM&#25512;&#29702;&#21152;&#36895;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SLEB&#65292;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;Transformer&#22359;&#20316;&#20026;&#31934;&#31616;&#30340;&#22522;&#26412;&#21333;&#20301;&#65292;&#22240;&#20026;LLM&#22312;&#30456;&#37051;&#22359;&#30340;&#36755;&#20986;&#20043;&#38388;&#20855;&#26377;&#22359;&#32423;&#21035;&#30340;&#20887;&#20313;&#21644;&#39640;&#30456;&#20284;&#24615;&#12290;&#36825;&#20010;&#36873;&#25321;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22686;&#24378;LLM&#30340;&#22788;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SLEB&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09025v1 Announce Type: new Abstract: Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;AgentEval&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#30340;&#20219;&#21153;&#25928;&#29992;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#20102;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#65292;&#24182;&#23545;&#24212;&#29992;&#30340;&#25928;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#37327;&#21270;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09015</link><description>&lt;p&gt;
&#26397;&#30528;&#26356;&#22909;&#30340;&#20154;&#26426;&#23545;&#40784;&#26041;&#21521;&#65306;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#20013;&#30340;&#20219;&#21153;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;AgentEval&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#30340;&#20219;&#21153;&#25928;&#29992;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#20102;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#65292;&#24182;&#23545;&#24212;&#29992;&#30340;&#25928;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#37327;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#24212;&#29992;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#24212;&#29992;&#36890;&#36807;&#21327;&#21161;&#22810;&#20010;&#20195;&#29702;&#20154;&#19982;&#20154;&#31867;&#21512;&#20316;&#65292;&#24110;&#21161;&#20154;&#20204;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#23384;&#22312;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#26159;&#21542;&#30495;&#27491;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#21644;&#20219;&#21153;&#25191;&#34892;&#25928;&#29575;&#12290;&#36825;&#20984;&#26174;&#20102;&#39564;&#35777;LLM&#39537;&#21160;&#24212;&#29992;&#25928;&#29992;&#30340;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#35201;&#30830;&#20445;&#24212;&#29992;&#31243;&#24207;&#30340;&#21151;&#33021;&#19982;&#26368;&#32456;&#29992;&#25143;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AgentEval&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#26045;&#25968;&#23398;&#38382;&#39064;&#30340;&#20272;&#27979;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#20219;&#20309;&#32473;&#23450;&#24212;&#29992;&#31243;&#24207;&#29420;&#29305;&#30446;&#26631;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#12290;&#36825;&#26679;&#21487;&#20197;&#23545;&#24212;&#29992;&#31243;&#24207;&#30340;&#25928;&#29992;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#37327;&#21270;&#20854;&#19982;&#24314;&#35758;&#26631;&#20934;&#30456;&#27604;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23545;&#35813;&#26694;&#26550;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09015v1 Announce Type: cross Abstract: The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval provides an implementation for the math problems}, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#22522;&#20110;&#22810;&#27969;&#20107;&#23454;&#26597;&#25214;&#30340;&#26041;&#24335;&#26469;&#25512;&#36827;&#20851;&#20110;&#28798;&#23475;&#20107;&#20214;&#30340;&#33258;&#21160;&#25688;&#35201;&#65292;&#25552;&#20986;&#20351;&#29992;&#26816;&#32034;&#12289;&#37325;&#26032;&#25490;&#24207;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#25688;&#35201;&#26041;&#27861;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#24182;&#25506;&#32034;&#20102;&#20197;&#38382;&#31572;&#20026;&#21160;&#26426;&#30340;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#21462;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;</title><link>https://arxiv.org/abs/2402.09008</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25351;&#20196;&#30340;&#25552;&#31034;&#36827;&#34892;&#22810;&#26597;&#35810;&#20851;&#27880;&#30340;&#28798;&#23475;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Multi-Query Focused Disaster Summarization via Instruction-Based Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#22522;&#20110;&#22810;&#27969;&#20107;&#23454;&#26597;&#25214;&#30340;&#26041;&#24335;&#26469;&#25512;&#36827;&#20851;&#20110;&#28798;&#23475;&#20107;&#20214;&#30340;&#33258;&#21160;&#25688;&#35201;&#65292;&#25552;&#20986;&#20351;&#29992;&#26816;&#32034;&#12289;&#37325;&#26032;&#25490;&#24207;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#25688;&#35201;&#26041;&#27861;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#24182;&#25506;&#32034;&#20102;&#20197;&#38382;&#31572;&#20026;&#21160;&#26426;&#30340;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#21462;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#28798;&#23475;&#20107;&#20214;&#25688;&#35201;&#22312;&#28798;&#23475;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;CrisisFACTS&#30340;&#31532;&#20108;&#29256;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#22810;&#27969;&#20107;&#23454;&#26597;&#25214;&#30340;&#26041;&#24335;&#26469;&#25512;&#36827;&#28798;&#23475;&#25688;&#35201;&#65292;&#37325;&#28857;&#20851;&#27880;Twitter&#12289;Reddit&#12289;Facebook&#21644;Webnews&#31561;&#32593;&#32476;&#20449;&#24687;&#26469;&#28304;&#12290;&#22312;&#36825;&#37324;&#65292;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#24320;&#21457;&#33021;&#22815;&#20174;&#22810;&#20010;&#19982;&#28798;&#23475;&#30456;&#20851;&#30340;&#20107;&#20214;&#20013;&#25552;&#21462;&#20851;&#38190;&#20107;&#23454;&#30340;&#31995;&#32479;&#65292;&#36825;&#20123;&#20107;&#23454;&#26368;&#32456;&#20316;&#20026;&#25688;&#35201;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36981;&#24490;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#25552;&#20986;&#20351;&#29992;&#26816;&#32034;&#12289;&#37325;&#26032;&#25490;&#24207;&#21644;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#36981;&#24490;&#25351;&#20196;&#30340;&#25688;&#35201;&#26041;&#27861;&#30340;&#32452;&#21512;&#12290;&#20004;&#38454;&#27573;&#30340;&#26816;&#32034;&#27969;&#31243;&#20381;&#36182;&#20110;BM25&#21644;MonoT5&#65292;&#32780;&#25688;&#35201;&#22120;&#27169;&#22359;&#22522;&#20110;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;LLaMA-13b&#12290;&#23545;&#20110;&#25688;&#35201;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#20197;&#38382;&#31572;&#20026;&#21160;&#26426;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#35777;&#25454;&#23545;&#20110;&#25552;&#21462;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20107;&#23454;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09008v1 Announce Type: new Abstract: Automatic summarization of mass-emergency events plays a critical role in disaster management. The second edition of CrisisFACTS aims to advance disaster summarization based on multi-stream fact-finding with a focus on web sources such as Twitter, Reddit, Facebook, and Webnews. Here, participants are asked to develop systems that can extract key facts from several disaster-related events, which ultimately serve as a summary. This paper describes our method to tackle this challenging task. We follow previous work and propose to use a combination of retrieval, reranking, and an embarrassingly simple instruction-following summarization. The two-stage retrieval pipeline relies on BM25 and MonoT5, while the summarizer module is based on the open-source Large Language Model (LLM) LLaMA-13b. For summarization, we explore a Question Answering (QA)-motivated prompting approach and find the evidence useful for extracting query-relevant facts. The a
&lt;/p&gt;</description></item><item><title>&#40065;&#26834;&#32467;&#26500;&#39044;&#27979;&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#25512;&#29702;&#26041;&#27861;&#30340;&#28151;&#21512;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#36755;&#20986;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#27809;&#26377;&#25968;&#25454;&#38598;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#26684;&#24335;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.08971</link><description>&lt;p&gt;
&#40065;&#26834;&#32467;&#26500;&#39044;&#27979;&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Structured Language Generation Model for Robust Structure Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08971
&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#32467;&#26500;&#39044;&#27979;&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#25512;&#29702;&#26041;&#27861;&#30340;&#28151;&#21512;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#36755;&#20986;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#27809;&#26377;&#25968;&#25454;&#38598;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#26684;&#24335;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;SLGM&#65289;&#65292;&#36890;&#36807;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#25512;&#29702;&#26041;&#27861;&#30340;&#28151;&#21512;&#26469;&#25913;&#21892;&#32467;&#26500;&#21270;&#36755;&#20986;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20197;&#24448;&#30340;&#32467;&#26500;&#39044;&#27979;&#30740;&#31350;&#65288;&#22914;NER&#65292;RE&#65289;&#21033;&#29992;&#20102;&#26174;&#24335;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#21487;&#33021;&#20250;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#40065;&#26834;&#27867;&#21270;&#24615;&#20135;&#29983;&#25361;&#25112;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#38388;&#25509;&#22320;&#25552;&#20379;&#20102;&#26377;&#20851;&#25968;&#25454;&#30340;&#36890;&#29992;&#26684;&#24335;&#20449;&#24687;&#12290;&#21033;&#29992;&#26684;&#24335;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25439;&#22833;&#26657;&#20934;&#21644;&#26684;&#24335;&#21270;&#35299;&#30721;&#23558;&#24207;&#21015;&#21040;&#24207;&#21015;&#38382;&#39064;&#31616;&#21270;&#20026;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SLGM&#22312;&#27809;&#26377;&#25968;&#25454;&#38598;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#20445;&#25345;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#36739;&#23569;&#30340;&#26684;&#24335;&#38169;&#35823;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20687;&#36866;&#37197;&#22120;&#19968;&#26679;&#22312;&#21508;&#20010;&#25968;&#25454;&#38598;&#19978;&#24037;&#20316;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08971v1 Announce Type: new Abstract: We propose Structured Language Generation Model (SLGM), a mixture of new loss function and inference method for better generalization of structured outputs. Previous studies on structure prediction (e.g. NER, RE) make use of explicit dataset information, which would boost performance, yet it might pose challenges to robust generalization in real-world situations. Instead, our model gives generalized format information about data indirectly. With format information, we could reduce sequence-to-sequence problem into classification problem via loss calibration and formatted decoding. Our experimental results showed SLGM successfully maintain performance without dataset information, and showed much less format errors. We also showed our model can work like adapters on individual dataset, with no additional training.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PLURAL&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20013;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08966</link><description>&lt;p&gt;
&#29992;&#20110;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20013;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08966
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PLURAL&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20013;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;(diff-VQA)&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#35201;&#27714;&#26681;&#25454;&#19968;&#23545;&#22270;&#20687;&#30340;&#24046;&#24322;&#22238;&#31572;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#22312;&#35835;&#21462;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#25918;&#23556;&#31185;&#21307;&#29983;&#36890;&#24120;&#20250;&#23545;&#21516;&#19968;&#24739;&#32773;&#22312;&#19981;&#21516;&#26102;&#38388;&#25293;&#25668;&#30340;&#22810;&#24133;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#36861;&#36394;&#30142;&#30149;&#30340;&#36827;&#23637;&#21644;&#20854;&#20020;&#24202;&#23454;&#36341;&#20013;&#20005;&#37325;&#31243;&#24230;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20026;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#35774;&#35745;&#29305;&#23450;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#38169;&#36807;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLM)&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PLURAL&#30340;&#26032;&#22411;VLM&#65292;&#23427;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#36880;&#27493;&#30340;&#26041;&#27861;&#24320;&#21457;&#65292;&#20174;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24320;&#22987;&#65292;&#28982;&#21518;&#20351;&#29992;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#32437;&#21521;&#25968;&#25454;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08966v1 Announce Type: cross Abstract: Difference visual question answering (diff-VQA) is a challenging task that requires answering complex questions based on differences between a pair of images. This task is particularly important in reading chest X-ray images because radiologists often compare multiple images of the same patient taken at different times to track disease progression and changes in its severity in their clinical practice. However, previous works focused on designing specific network architectures for the diff-VQA task, missing opportunities to enhance the model's performance using a pretrained vision-language model (VLM). Here, we introduce a novel VLM called PLURAL, which is pretrained on natural and longitudinal chest X-ray data for the diff-VQA task. The model is developed using a step-by-step approach, starting with being pretrained on natural images and texts, followed by being trained using longitudinal chest X-ray data. The longitudinal data consist
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.08957</link><description>&lt;p&gt;
MUSTARD&#65306;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#23398;&#25512;&#29702;&#21644;&#23450;&#29702;&#35777;&#26126;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#20005;&#26684;&#21644;&#24418;&#24335;&#21270;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#23427;&#20204;&#26159;&#25506;&#32034;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#21560;&#24341;&#39046;&#22495;&#65292;&#20294;&#20173;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22914;Chain-of-Thought&#65288;CoT&#65289;&#25581;&#31034;&#20102;&#20013;&#38388;&#27493;&#39588;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36880;&#27493;&#27880;&#37322;&#38656;&#35201;&#22823;&#37327;&#30340;&#21171;&#21160;&#21147;&#65292;&#23548;&#33268;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#30340;&#35757;&#32451;&#27493;&#39588;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#20027;&#23548;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;&#12290;MUSTARD&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#21512;&#25104;&#25968;&#25454;&#65306;&#65288;1&#65289;&#23427;&#38543;&#26426;&#36873;&#25321;&#20960;&#20010;&#25968;&#23398;&#27010;&#24565;&#20316;&#20026;&#38382;&#39064;&#30340;&#31867;&#21035;&#12290;&#65288;2&#65289;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#36873;&#23450;&#30340;&#27010;&#24565;&#25552;&#31034;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#38382;&#39064;&#21644;&#23427;&#20204;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08957v1 Announce Type: new Abstract: Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#32452;&#21453;&#20107;&#23454;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#30340;&#24191;&#27867;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#20154;&#31867;&#22312;&#25152;&#26377;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;GPT&#27169;&#22411;&#22312;&#21453;&#20107;&#23454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.08955</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#20219;&#21153;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31867;&#27604;&#25512;&#29702;&#30340;&#24191;&#27867;&#24615;
&lt;/p&gt;
&lt;p&gt;
Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#32452;&#21453;&#20107;&#23454;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#30340;&#24191;&#27867;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#20154;&#31867;&#22312;&#25152;&#26377;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;GPT&#27169;&#22411;&#22312;&#21453;&#20107;&#23454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#27979;&#35797;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#30495;&#27491;&#36827;&#34892;&#20154;&#31867;&#25277;&#35937;&#25512;&#29702;&#65292;&#36824;&#26159;&#20381;&#36182;&#20110;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#25152;&#35265;&#30456;&#20284;&#30340;&#36739;&#23569;&#36890;&#29992;&#36807;&#31243;&#30340;&#20105;&#35770;&#19968;&#30452;&#23384;&#22312;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20808;&#21069;&#22768;&#31216;LLMs&#20855;&#26377;&#31867;&#27604;&#33021;&#21147;&#30340;&#24191;&#27867;&#24615;&#65288;Webb, Holyoak, &amp; Lu, 2023&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#19968;&#32452;&#31867;&#27604;&#38382;&#39064;&#65292;&#21019;&#24314;&#20102;&#19968;&#32452;&#8220;&#21453;&#20107;&#23454;&#8221;&#21464;&#20307;&#65292;&#21363;&#27979;&#35797;&#30456;&#21516;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#24456;&#21487;&#33021;&#19982;&#20219;&#20309;&#39044;&#35757;&#32451;&#36164;&#26009;&#19981;&#21516;&#12290;&#25105;&#20204;&#23545;&#20154;&#31867;&#21644;&#19977;&#20010;GPT&#27169;&#22411;&#22312;&#21407;&#22987;&#38382;&#39064;&#21644;&#21453;&#20107;&#23454;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#65292;&#34429;&#28982;&#20154;&#31867;&#30340;&#34920;&#29616;&#23545;&#25152;&#26377;&#38382;&#39064;&#20445;&#25345;&#39640;&#27700;&#24179;&#65292;&#20294;GPT&#27169;&#22411;&#22312;&#21453;&#20107;&#23454;&#38598;&#19978;&#30340;&#34920;&#29616;&#24613;&#21095;&#19979;&#38477;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08955v1 Announce Type: new Abstract: Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, it has been debated whether they are actually performing humanlike abstract reasoning or instead employing less general processes that rely on similarity to what has been seen in their training data. Here we investigate the generality of analogy-making abilities previously claimed for LLMs (Webb, Holyoak, &amp; Lu, 2023). We take one set of analogy problems used to evaluate LLMs and create a set of "counterfactual" variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data. We test humans and three GPT models on both the original and counterfactual problems, and show that, while the performance of humans remains high for all the problems, the GPT models' performance declines sharply on the counterfactual set. This work provides evide
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#35770;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08939</link><description>&lt;p&gt;
&#35770;&#25454;&#39034;&#24207;&#22312;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#36215;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Premise Order Matters in Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08939
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#35770;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#33030;&#24369;&#24615;&#65306;&#23613;&#31649;&#36825;&#31181;&#39034;&#24207;&#19981;&#20250;&#25913;&#21464;&#22522;&#26412;&#20219;&#21153;&#65292;&#20294;LLMs&#23545;&#20110;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#33030;&#24369;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#35770;&#25454;&#39034;&#24207;&#19982;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#25152;&#38656;&#30340;&#19978;&#19979;&#25991;&#23545;&#40784;&#26102;&#65292;LLMs&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#35770;&#25454;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#65288;&#32780;&#19981;&#26159;&#38543;&#26426;&#39034;&#24207;&#65289;&#20250;&#26497;&#22823;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19981;&#21516;LLMs&#23545;&#28436;&#32462;&#25512;&#29702;&#20013;&#35770;&#25454;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35843;&#25972;&#35770;&#25454;&#39034;&#24207;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#36229;&#36807;30&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#22522;&#20110;GSM8K&#30340;&#22522;&#20934;&#27979;&#35797;R-GSM&#26469;&#30740;&#31350;&#39034;&#24207;&#25928;&#24212;&#23545;&#25968;&#23398;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08939v1 Announce Type: new Abstract: Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathema
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#24182;&#20351;&#29992;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#20154;&#31867;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.08925</link><description>&lt;p&gt;
MaxMin-RLHF:&#38754;&#21521;&#20855;&#26377;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20844;&#24179;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08925
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#24182;&#20351;&#29992;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;(RLHF)&#36890;&#36807;&#20351;&#29992;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#27966;&#29983;&#30340;&#21333;&#19968;&#22870;&#21169;&#27169;&#22411;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#20174;&#22810;&#20010;&#29992;&#25143;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#20016;&#23500;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#20351;&#29992;&#21333;&#19968;&#22870;&#21169;RLHF&#36827;&#34892;&#23545;&#40784;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#65292;&#20174;&#32780;&#20984;&#26174;&#20102;&#20854;&#26080;&#27861;&#34920;&#31034;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#20026;&#20102;&#25552;&#20379;&#19968;&#20010;&#20844;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20013;&#30340;&#24179;&#31561;&#21407;&#21017;&#21551;&#21457;&#30340;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#21644;&#36890;&#29992;&#25928;&#29992;RL&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#31361;&#26174;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08925v1 Announce Type: cross Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. To provide an equitable solution to the problem, we learn a mixture of preference distributions via an expectation-maximization algorithm and propose a MaxMin alignment objective for policy learning inspired by the Egalitarian principle in social choice theory to better represent diverse human preferences. We elucidate the connection of our proposed approach to distributionally robust optimization and general utility RL, thereby highlighting the generality and robustness of our proposed
&lt;/p&gt;</description></item><item><title>UniEnc-CASSNAT&#26159;&#19968;&#31181;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#38750;&#33258;&#22238;&#24402;&#35821;&#38899;SSL&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65292;&#36890;&#36807;&#32467;&#21512;CTC&#21644;CASS-NAT&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20805;&#24403;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#35282;&#33394;&#65292;&#33021;&#26377;&#25928;&#38598;&#25104;SFM&#24182;&#32531;&#35299;&#20381;&#36182;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08898</link><description>&lt;p&gt;
UniEnc-CASSNAT:&#19968;&#31181;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#38750;&#33258;&#22238;&#24402;&#35821;&#38899;SSL&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
UniEnc-CASSNAT: An Encoder-only Non-autoregressive ASR for Speech SSL Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08898
&lt;/p&gt;
&lt;p&gt;
UniEnc-CASSNAT&#26159;&#19968;&#31181;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#38750;&#33258;&#22238;&#24402;&#35821;&#38899;SSL&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65292;&#36890;&#36807;&#32467;&#21512;CTC&#21644;CASS-NAT&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20805;&#24403;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#35282;&#33394;&#65292;&#33021;&#26377;&#25928;&#38598;&#25104;SFM&#24182;&#32531;&#35299;&#20381;&#36182;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;NASR&#65289;&#27169;&#22411;&#20197;&#20854;&#24182;&#34892;&#24615;&#21644;&#24555;&#36895;&#25512;&#29702;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;NASR&#65288;&#20363;&#22914;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#65289;&#21487;&#20197;&#20174;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65288;SFM&#65289;&#21021;&#22987;&#21270;&#65292;&#20294;&#19981;&#32771;&#34385;&#20013;&#38388;&#26631;&#35760;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;NASR&#65288;&#22914;&#22522;&#20110;CTC&#23545;&#40784;&#30340;&#21333;&#27493;&#38750;&#33258;&#22238;&#24402;Transformer&#65288;CASS-NAT&#65289;&#65289;&#21487;&#20197;&#32531;&#35299;&#20381;&#36182;&#38382;&#39064;&#65292;&#20294;&#19981;&#33021;&#26377;&#25928;&#22320;&#38598;&#25104;SFM&#12290;&#21463;&#26368;&#36817;&#20351;&#29992;&#20849;&#20139;Transformer&#32534;&#30721;&#22120;&#36827;&#34892;&#35821;&#38899;-&#25991;&#26412;&#32852;&#21512;&#39044;&#35757;&#32451;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;NASR&#65292;UniEnc-CASSNAT&#65292;&#20197;&#32467;&#21512;CTC&#21644;CASS-NAT&#30340;&#20248;&#21183;&#12290;UniEnc-CASSNAT&#21482;&#30001;&#32534;&#30721;&#22120;&#20316;&#20026;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#26159;SFM&#12290;&#32534;&#30721;&#22120;&#36890;&#36807;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#21516;&#26102;&#20805;&#24403;CASS-NAT&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#35282;&#33394;&#12290;&#32534;&#30721;&#22120;&#30340;&#31532;&#19968;&#27425;&#20256;&#36882;&#25509;&#21463;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08898v1 Announce Type: cross Abstract: Non-autoregressive automatic speech recognition (NASR) models have gained attention due to their parallelism and fast inference. The encoder-based NASR, e.g. connectionist temporal classification (CTC), can be initialized from the speech foundation models (SFM) but does not account for any dependencies among intermediate tokens. The encoder-decoder-based NASR, like CTC alignment-based single-step non-autoregressive transformer (CASS-NAT), can mitigate the dependency problem but is not able to efficiently integrate SFM. Inspired by the success of recent work of speech-text joint pre-training with a shared transformer encoder, we propose a new encoder-based NASR, UniEnc-CASSNAT, to combine the advantages of CTC and CASS-NAT. UniEnc-CASSNAT consists of only an encoder as the major module, which can be the SFM. The encoder plays the role of both the CASS-NAT encoder and decoder by two forward passes. The first pass of the encoder accepts th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TEAROOM&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#22522;&#20110;&#26641;&#29366;&#30828;&#27880;&#24847;&#21147;&#21644;&#33258;&#25105;&#28608;&#21169;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20998;&#23618;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#26426;&#21046;&#20351;&#27169;&#22411;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#19982;&#29305;&#23450;&#20219;&#21153;&#30456;&#20851;&#30340;&#21494;&#23376;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.08874</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#29366;&#30828;&#27880;&#24847;&#21147;&#21644;&#33258;&#25105;&#28608;&#21169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tree-Based Hard Attention with Self-Motivation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08874
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TEAROOM&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#22522;&#20110;&#26641;&#29366;&#30828;&#27880;&#24847;&#21147;&#21644;&#33258;&#25105;&#28608;&#21169;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20998;&#23618;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#26426;&#21046;&#20351;&#27169;&#22411;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#19982;&#29305;&#23450;&#20219;&#21153;&#30456;&#20851;&#30340;&#21494;&#23376;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#32431;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#19987;&#38376;&#35774;&#35745;&#26469;&#22788;&#29702;&#20998;&#23618;&#25991;&#26412;&#32467;&#26500;&#12290;&#20174;&#23427;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#22238;&#22797;&#20013;&#25552;&#21462;&#20219;&#21153;&#25152;&#38656;&#30340;&#23646;&#24615;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#22788;&#29702;&#27493;&#39588;&#12290;&#20107;&#23454;&#19978;&#65292;&#36873;&#25321;&#24615;&#22320;&#29702;&#35299;&#22823;&#35268;&#27169;&#25991;&#26412;&#30340;&#23618;&#27425;&#32467;&#26500;&#23545;&#20110;&#29702;&#35299;&#20854;&#23454;&#36136;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#25552;&#31034;&#23558;LLM&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;&#20998;&#31867;&#25110;&#22238;&#24402;&#20540;&#26356;&#32039;&#23494;&#22320;&#23545;&#40784;&#20063;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Tree-Based Hard Attention with Self-Motivation for Large Language Models&#65288;TEAROOM&#65289;&#12290;TEAROOM&#23558;&#26641;&#29366;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;&#32435;&#20837;LLM&#20013;&#65292;&#20197;&#22788;&#29702;&#20998;&#23618;&#32467;&#26500;&#30340;&#25991;&#26412;&#36755;&#20837;&#12290;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#26426;&#21046;&#65292;&#23427;&#20351;&#20923;&#32467;&#30340;LLM&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#19982;&#26681;&#33410;&#28857;&#30456;&#20851;&#30340;&#21494;&#23376;&#33410;&#28857;&#65292;&#29983;&#25104;&#19968;&#20010;&#23450;&#21046;&#30340;&#31526;&#21495;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08874v1 Announce Type: new Abstract: While large language models (LLMs) excel at understanding and generating plain text, they are not specifically tailored to handle hierarchical text structures. Extracting the task-desired property from their natural language responses typically necessitates additional processing steps. In fact, selectively comprehending the hierarchical structure of large-scale text is pivotal to understanding its substance. Aligning LLMs more closely with the classification or regression values of specific task through prompting also remains challenging. To this end, we propose a novel framework called Tree-Based Hard Attention with Self-Motivation for Large Language Models (TEAROOM). TEAROOM incorporates a tree-based hard attention mechanism for LLMs to process hierarchically structured text inputs. By leveraging prompting, it enables a frozen LLM to selectively focus on relevant leaves in relation to the root, generating a tailored symbolic representat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32452;&#21512;&#30340;LLM&#26041;&#27861;&#65292;&#30001;&#35821;&#38899;&#32534;&#30721;&#22120;&#12289;LLM&#21644;&#32447;&#24615;&#25237;&#24433;&#22120;&#32452;&#25104;&#65292;&#33021;&#22815;&#32988;&#20219;ASR&#20219;&#21153;&#65292;&#24182;&#19988;&#20855;&#26377;&#28165;&#26224;&#30340;&#35774;&#32622;&#21644;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.08846</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#24378;ASR&#33021;&#21147;&#30340;&#20196;&#20154;&#23604;&#23596;&#31616;&#21333;&#30340;LLM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Embarrassingly Simple Approach for LLM with Strong ASR Capacity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32452;&#21512;&#30340;LLM&#26041;&#27861;&#65292;&#30001;&#35821;&#38899;&#32534;&#30721;&#22120;&#12289;LLM&#21644;&#32447;&#24615;&#25237;&#24433;&#22120;&#32452;&#25104;&#65292;&#33021;&#22815;&#32988;&#20219;ASR&#20219;&#21153;&#65292;&#24182;&#19988;&#20855;&#26377;&#28165;&#26224;&#30340;&#35774;&#32622;&#21644;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#35299;&#20915;&#35821;&#38899;&#22788;&#29702;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#21363;&#20351;&#29992;&#35821;&#38899;&#22522;&#26412;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#22797;&#26434;&#30340;&#35774;&#35745;&#65292;&#20363;&#22914;&#23545;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#26102;&#38388;&#21387;&#32553;&#65292;&#22788;&#29702;&#25237;&#24433;&#20202;&#30340;&#27169;&#24577;&#23545;&#40784;&#65292;&#24182;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#23545;LLM&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24182;&#19981;&#38656;&#35201;&#31934;&#32454;&#30340;&#35774;&#35745;&#65292;&#32780;&#26159;&#19968;&#20010;&#30001;&#29616;&#25104;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#12289;LLM&#21644;&#21807;&#19968;&#21487;&#35757;&#32451;&#30340;&#32447;&#24615;&#25237;&#24433;&#22120;&#32452;&#25104;&#30340;&#31616;&#21333;&#32452;&#21512;&#23601;&#33021;&#32988;&#20219;ASR&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;LLM&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#32452;&#21512;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#25506;&#32034;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#26368;&#20339;&#30340;&#22522;&#20110;LLM&#30340;ASR&#31995;&#32479;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;SLAM-ASR&#12290;&#25152;&#25552;&#20986;&#30340;SLAM-ASR&#25552;&#20379;&#20102;&#19968;&#20010;&#28165;&#26224;&#30340;&#35774;&#32622;&#21644;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35774;&#35745;&#65292;&#21482;&#26377;&#32447;&#24615;&#25237;&#24433;&#22120;&#38656;&#35201;&#36827;&#34892;&#35757;&#32451;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SLAM-ASR&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08846v1 Announce Type: cross Abstract: In this paper, we focus on solving one of the most important tasks in the field of speech processing, i.e., automatic speech recognition (ASR), with speech foundation encoders and large language models (LLM). Recent works have complex designs such as compressing the output temporally for the speech encoder, tackling modal alignment for the projector, and utilizing parameter-efficient fine-tuning for the LLM. We found that delicate designs are not necessary, while an embarrassingly simple composition of off-the-shelf speech encoder, LLM, and the only trainable linear projector is competent for the ASR task. To be more specific, we benchmark and explore various combinations of LLMs and speech encoders, leading to the optimal LLM-based ASR system, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup and little task-specific design, where only the linear projector is trained. To the best of our knowledge, SLAM-ASR achieves t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#22238;&#24212;&#24494;&#31505;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#23545;&#35805;&#30340;&#20307;&#39564;&#24335;AI&#20195;&#29702;&#20013;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#36129;&#29486;&#22312;&#20110;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#34701;&#27965;&#33021;&#21147;&#65292;&#36890;&#36807;&#27169;&#25311;&#22238;&#24212;&#24494;&#31505;&#26469;&#24314;&#31435;&#27835;&#30103;&#22330;&#26223;&#20013;&#30340;&#29702;&#35299;&#21644;&#24314;&#31435;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.08837</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#22238;&#24212;&#24494;&#31505;&#65292;&#24212;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#23545;&#35805;&#30340;&#20307;&#39564;&#24335;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Context-Sensitive Backchannel Smiles for Embodied AI Agents with Applications in Mental Health Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#22238;&#24212;&#24494;&#31505;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#23545;&#35805;&#30340;&#20307;&#39564;&#24335;AI&#20195;&#29702;&#20013;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#36129;&#29486;&#22312;&#20110;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#34701;&#27965;&#33021;&#21147;&#65292;&#36890;&#36807;&#27169;&#25311;&#22238;&#24212;&#24494;&#31505;&#26469;&#24314;&#31435;&#27835;&#30103;&#22330;&#26223;&#20013;&#30340;&#29702;&#35299;&#21644;&#24314;&#31435;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24515;&#29702;&#20581;&#24247;&#36164;&#28304;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#26377;&#25928;&#30340;&#31579;&#26597;&#12289;&#35786;&#26029;&#21644;&#27835;&#30103;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#31181;&#31232;&#32570;&#24615;&#31361;&#26174;&#20102;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#22312;&#22686;&#24378;&#27835;&#30103;&#25903;&#25345;&#30340;&#21487;&#21450;&#24615;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#12290;&#25317;&#26377;&#20808;&#36827;&#20114;&#21160;&#33021;&#21147;&#30340;&#20307;&#39564;&#24335;&#20195;&#29702;&#21576;&#29616;&#20986;&#26377;&#26395;&#25104;&#20026;&#20256;&#32479;&#25252;&#29702;&#26041;&#27861;&#30340;&#26377;&#21147;&#34917;&#20805;&#21697;&#65292;&#24182;&#19988;&#20302;&#25104;&#26412;&#12290;&#36825;&#20123;&#20195;&#29702;&#30340;&#26377;&#25928;&#24615;&#22312;&#20110;&#23427;&#20204;&#33021;&#22815;&#27169;&#25311;&#38750;&#35328;&#35821;&#34892;&#20026;&#65292;&#22914;&#22238;&#24212;&#24494;&#31505;&#65292;&#22312;&#27835;&#30103;&#22330;&#26223;&#20013;&#24314;&#31435;&#34701;&#27965;&#21644;&#29702;&#35299;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#30446;&#21069;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#20026;&#20102;&#25552;&#39640;&#20307;&#39564;&#24335;&#20195;&#29702;&#30340;&#24314;&#31435;&#34701;&#27965;&#33021;&#21147;&#65292;&#25105;&#20204;&#23545;&#38754;&#23545;&#38754;&#20146;&#23494;&#23545;&#35805;&#30340;&#35270;&#39057;&#20013;&#30340;&#22238;&#24212;&#24494;&#31505;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#36825;&#20123;&#23545;&#35805;&#28041;&#21450;&#24515;&#29702;&#20581;&#24247;&#12289;&#30142;&#30149;&#21644;&#20154;&#38469;&#20851;&#31995;&#31561;&#20027;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#35762;&#35805;&#32773;&#21644;&#21548;&#20247;&#30340;&#34892;&#20026;&#37117;&#20250;&#24433;&#21709;&#22238;&#24212;&#24494;&#31505;&#30340;&#25345;&#32493;&#26102;&#38388;&#21644;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08837v1 Announce Type: new Abstract: Addressing the critical shortage of mental health resources for effective screening, diagnosis, and treatment remains a significant challenge. This scarcity underscores the need for innovative solutions, particularly in enhancing the accessibility and efficacy of therapeutic support. Embodied agents with advanced interactive capabilities emerge as a promising and cost-effective supplement to traditional caregiving methods. Crucial to these agents' effectiveness is their ability to simulate non-verbal behaviors, like backchannels, that are pivotal in establishing rapport and understanding in therapeutic contexts but remain under-explored. To improve the rapport-building capabilities of embodied agents we annotated backchannel smiles in videos of intimate face-to-face conversations over topics such as mental health, illness, and relationships. We hypothesized that both speaker and listener behaviors affect the duration and intensity of back
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#38598;ECInstruct&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65288;eCeLLM&#65289;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08831</link><description>&lt;p&gt;
eCeLLM&#65306;&#20174;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#20013;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#24191;&#21040;&#30005;&#23376;&#21830;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#38598;ECInstruct&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65288;eCeLLM&#65289;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;&#30005;&#23376;&#21830;&#21153;&#27169;&#22411;&#26041;&#38754;&#20570;&#20986;&#24040;&#22823;&#21162;&#21147;&#65292;&#20256;&#32479;&#30340;&#30005;&#23376;&#21830;&#21153;&#27169;&#22411;&#22312;&#36890;&#29992;&#30005;&#23376;&#21830;&#21153;&#24314;&#27169;&#19978;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#24182;&#19988;&#22312;&#26032;&#29992;&#25143;&#21644;&#26032;&#20135;&#21697;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#8212;&#8212;&#36825;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#39046;&#22495;&#22806;&#27867;&#21270;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#36890;&#29992;&#24314;&#27169;&#21644;&#39046;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;ECInstruct&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#21830;&#21153;&#30340;&#24320;&#28304;&#12289;&#22823;&#35268;&#27169;&#21644;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;ECInstruct&#65292;&#25105;&#20204;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65292;&#31216;&#20026;eCeLLM&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#21644;&#35780;&#20272;&#34920;&#26126;&#65292;eCeLLM&#27169;&#22411;&#22312;&#20869;&#37096;&#29615;&#22659;&#20013;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;GPT-4&#21644;&#26368;&#20808;&#36827;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08831v1 Announce Type: cross Abstract: With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#24207;&#21015;&#22270;&#30340;&#23454;&#29616;&#21644;&#27495;&#20041;&#38382;&#39064;&#65292;&#36890;&#36807;&#32452;&#21512;&#21644;&#35745;&#31639;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#22270;&#30340;&#31383;&#21475;&#22823;&#23567;&#12289;&#26041;&#21521;&#24615;&#21644;&#26435;&#37325;&#31561;&#22240;&#32032;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35299;&#20915;&#23454;&#29616;&#21644;&#26522;&#20030;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08830</link><description>&lt;p&gt;
&#24207;&#21015;&#22270;&#23454;&#29616;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27495;&#20041;
&lt;/p&gt;
&lt;p&gt;
Sequence graphs realizations and ambiguity in language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#24207;&#21015;&#22270;&#30340;&#23454;&#29616;&#21644;&#27495;&#20041;&#38382;&#39064;&#65292;&#36890;&#36807;&#32452;&#21512;&#21644;&#35745;&#31639;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#22270;&#30340;&#31383;&#21475;&#22823;&#23567;&#12289;&#26041;&#21521;&#24615;&#21644;&#26435;&#37325;&#31561;&#22240;&#32032;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35299;&#20915;&#23454;&#29616;&#21644;&#26522;&#20030;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#31181;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#34920;&#31034;&#20026;&#35789;&#34955;&#12290;&#36825;&#26679;&#30340;&#34920;&#31034;&#33258;&#28982;&#22320;&#36890;&#36807;&#19968;&#20010;&#24207;&#21015;&#22270;&#26469;&#32534;&#30721;&#65292;&#20854;&#20013;&#39030;&#28857;&#26159;&#20986;&#29616;&#22312;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#19981;&#21516;&#35789;&#65292;&#36793;&#34920;&#31034;&#22312;&#22823;&#23567;&#20026;w&#30340;&#28369;&#21160;&#31383;&#21475;&#20869;&#20004;&#20010;&#35789;&#30340;&#65288;&#26377;&#24207;&#65289;&#20849;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21387;&#32553;&#34920;&#31034;&#36890;&#24120;&#19981;&#26159;&#21452;&#23556;&#30340;&#65292;&#21487;&#33021;&#24341;&#20837;&#19968;&#23450;&#31243;&#24230;&#30340;&#27495;&#20041;&#12290;&#19968;&#20123;&#24207;&#21015;&#22270;&#21487;&#33021;&#20197;&#22810;&#31181;&#26041;&#24335;&#23454;&#29616;&#20026;&#19968;&#20010;&#24207;&#21015;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#20219;&#20309;&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32452;&#21512;&#21644;&#35745;&#31639;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#24207;&#21015;&#22270;&#30340;&#21487;&#23454;&#29616;&#24615;&#21644;&#27495;&#20041;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#30340;&#24207;&#21015;&#22270;&#23454;&#29616;&#30340;&#23384;&#22312;&#21644;&#26522;&#20030;&#65306;&#31383;&#21475;&#22823;&#23567;w&#12289;&#22270;&#30340;&#26041;&#21521;&#24615;&#30340;&#23384;&#22312;/&#32570;&#22833;&#21644;&#26435;&#37325;&#65288;&#37325;&#22797;&#24615;&#65289;&#30340;&#23384;&#22312;/&#32570;&#22833;&#12290;&#24403;w = 2&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#23454;&#29616;&#21644;&#26522;&#20030;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08830v1 Announce Type: cross Abstract: Several popular language models represent local contexts in an input text as bags of words. Such representations are naturally encoded by a sequence graph whose vertices are the distinct words occurring in x, with edges representing the (ordered) co-occurrence of two words within a sliding window of size w. However, this compressed representation is not generally bijective, and may introduce some degree of ambiguity. Some sequence graphs may admit several realizations as a sequence, while others may not admit any realization. In this paper, we study the realizability and ambiguity of sequence graphs from a combinatorial and computational point of view. We consider the existence and enumeration of realizations of a sequence graph under multiple settings: window size w, presence/absence of graph orientation, and presence/absence of weights (multiplicities). When w = 2, we provide polynomial time algorithms for realizability and enumeratio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25253;&#36947;&#20102;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#38899;&#33410;&#30340;&#31908;&#35821;&#35821;&#38899;&#36716;&#25991;&#26412;&#31995;&#32479;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#24110;&#21161;&#35829;&#35835;&#38556;&#30861;&#23398;&#29983;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#23558;&#20182;&#20204;&#36890;&#36807;&#35821;&#38899;&#34920;&#36798;&#30340;&#24605;&#24819;&#36716;&#25442;&#25104;&#25991;&#23383;&#30340;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.08788</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#33410;&#30340;DNN-HMM&#31908;&#35821;&#35821;&#38899;&#36716;&#25991;&#26412;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Syllable based DNN-HMM Cantonese Speech to Text System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#38899;&#33410;&#30340;&#31908;&#35821;&#35821;&#38899;&#36716;&#25991;&#26412;&#31995;&#32479;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#24110;&#21161;&#35829;&#35835;&#38556;&#30861;&#23398;&#29983;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#23558;&#20182;&#20204;&#36890;&#36807;&#35821;&#38899;&#34920;&#36798;&#30340;&#24605;&#24819;&#36716;&#25442;&#25104;&#25991;&#23383;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#25105;&#20204;&#22312;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#38899;&#33410;&#30340;&#31908;&#35821;&#35821;&#38899;&#36716;&#25991;&#26412;&#31995;&#32479;&#26041;&#38754;&#30340;&#24037;&#20316;&#12290;&#36825;&#26159;&#20026;&#20102;&#24110;&#21161;&#24739;&#26377;&#20889;&#20316;&#25216;&#24039;&#35748;&#30693;&#32570;&#38519;&#20294;&#34920;&#36798;&#33021;&#21147;&#27809;&#26377;&#38382;&#39064;&#30340;&#35829;&#35835;&#38556;&#30861;&#23398;&#29983;&#26500;&#24314;&#19968;&#20010;&#35821;&#38899;&#36716;&#25991;&#26412;&#31995;&#32479;&#30340;&#21162;&#21147;&#30340;&#19968;&#37096;&#20998;&#12290;&#23545;&#20110;&#31908;&#35821;&#35821;&#38899;&#35782;&#21035;&#65292;&#22768;&#23398;&#27169;&#22411;&#30340;&#22522;&#26412;&#21333;&#20301;&#21487;&#20197;&#26159;&#20256;&#32479;&#30340;&#23383;&#31526;&#20026;&#22522;&#30784;&#30340;&#38899;&#33410;&#65288;Initial-Final&#65292;IF&#65289;&#25110;&#32773;&#36827;&#19968;&#27493;&#23558;&#23614;&#38899;&#25286;&#20998;&#20026;&#38901;&#26680;&#21644;&#36741;&#38899;&#20197;&#21453;&#26144;&#31908;&#35821;&#38899;&#33410;&#20869;&#21464;&#20307;&#30340;&#38901;&#33050;&#26680;&#24515;&#38901;&#33276;&#65288;Onset-Nucleus-Coda&#65292;ONC&#65289;&#38899;&#33410;&#12290;&#36890;&#36807;&#20351;&#29992;Kaldi&#24037;&#20855;&#21253;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36741;&#20197;GPU&#36827;&#34892;&#28151;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;DNN-HMM&#65289;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#22522;&#20110;I&#30690;&#37327;&#30340;&#35828;&#35805;&#20154;&#33258;&#36866;&#24212;&#35757;&#32451;&#25216;&#26415;&#12290;&#30456;&#21516;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#22312;&#35757;&#32451;DNN&#30340;&#35828;&#35805;&#20154;&#33258;&#36866;&#24212;&#35757;&#32451;&#65288;GMM-SAT&#65289;&#20013;&#29992;&#20316;&#36755;&#20837;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08788v1 Announce Type: new Abstract: This paper reports our work on building up a Cantonese Speech-to-Text (STT) system with a syllable based acoustic model. This is a part of an effort in building a STT system to aid dyslexic students who have cognitive deficiency in writing skills but have no problem expressing their ideas through speech. For Cantonese speech recognition, the basic unit of acoustic models can either be the conventional Initial-Final (IF) syllables, or the Onset-Nucleus-Coda (ONC) syllables where finals are further split into nucleus and coda to reflect the intra-syllable variations in Cantonese. By using the Kaldi toolkit, our system is trained using the stochastic gradient descent optimization model with the aid of GPUs for the hybrid Deep Neural Network and Hidden Markov Model (DNN-HMM) with and without I-vector based speaker adaptive training technique. The input features of the same Gaussian Mixture Model with speaker adaptive training (GMM-SAT) to DNN
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.08787</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Rethinking Machine Unlearning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65288;MU&#65289;&#65292;&#31216;&#20026;LLM&#28040;&#38500;&#25216;&#26415;&#12290;&#36825;&#20010;&#30740;&#31350;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#25935;&#24863;&#25110;&#38750;&#27861;&#20449;&#24687;&#65289;&#20197;&#21450;&#30456;&#20851;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#22522;&#26412;&#30340;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#24182;&#19981;&#24433;&#21709;&#22240;&#26524;&#26080;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35774;&#24819;LLM&#28040;&#38500;&#25216;&#26415;&#23558;&#25104;&#20026;LLM&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#21487;&#33021;&#25104;&#20026;&#24320;&#21457;&#26082;&#23433;&#20840;&#12289;&#21487;&#38752;&#21448;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#20840;&#37325;&#35757;&#32451;&#12290;&#25105;&#20204;&#20174;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#24212;&#29992;&#31561;&#26041;&#38754;&#25506;&#32034;&#20102;LLM&#28040;&#38500;&#25216;&#26415;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#29616;&#26377;LLM&#28040;&#38500;&#25216;&#26415;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#65292;&#20363;&#22914;&#28040;&#38500;&#33539;&#22260;&#12289;&#25968;&#25454;&#27169;&#22411;&#20132;&#20114;&#21644;&#22810;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
&lt;/p&gt;</description></item><item><title>InstructGraph&#26159;&#19968;&#20010;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#21644;&#20559;&#22909;&#23545;&#40784;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22270;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#25968;&#25454;&#26684;&#24335;&#12289;&#24341;&#23548;LLM&#35299;&#20915;&#22270;&#20219;&#21153;&#21644;&#25552;&#39640;&#36755;&#20986;&#21487;&#38752;&#24615;&#26469;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08785</link><description>&lt;p&gt;
InstructGraph: &#36890;&#36807;&#22270;&#20013;&#24515;&#30340;&#25351;&#23548;&#35843;&#25972;&#21644;&#20559;&#22909;&#23545;&#40784;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08785
&lt;/p&gt;
&lt;p&gt;
InstructGraph&#26159;&#19968;&#20010;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#21644;&#20559;&#22909;&#23545;&#40784;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22270;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#25968;&#25454;&#26684;&#24335;&#12289;&#24341;&#23548;LLM&#35299;&#20915;&#22270;&#20219;&#21153;&#21644;&#25552;&#39640;&#36755;&#20986;&#21487;&#38752;&#24615;&#26469;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#21542;&#36890;&#36807;&#21442;&#25968;&#26356;&#26032;&#26356;&#22909;&#22320;&#35299;&#20915;&#22270;&#25512;&#29702;&#21644;&#29983;&#25104;&#20219;&#21153;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;InstructGraph&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#21644;&#20559;&#22909;&#23545;&#40784;&#36171;&#20104;LLM&#22270;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#26684;&#24335;&#30340;&#35821;&#35328;&#25551;&#36848;&#22120;&#65292;&#23558;&#25152;&#26377;&#30340;&#22270;&#25968;&#25454;&#32479;&#19968;&#21040;&#19968;&#31181;&#36890;&#29992;&#30340;&#31867;&#20284;&#20195;&#30721;&#30340;&#26684;&#24335;&#20013;&#65292;&#36825;&#26679;&#21487;&#20197;&#31616;&#27905;&#22320;&#34920;&#31034;&#22270;&#32780;&#19981;&#38656;&#35201;&#22806;&#37096;&#30340;&#22270;&#29305;&#23450;&#32534;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#22270;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#65292;&#24341;&#23548;LLM&#35299;&#20915;&#22270;&#25512;&#29702;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22270;&#20219;&#21153;&#20013;&#35782;&#21035;&#20986;&#28508;&#22312;&#30340;&#34394;&#26500;&#38382;&#39064;&#65292;&#24182;&#20026;&#20559;&#22909;&#23545;&#40784;&#26679;&#26412;&#36127;&#23454;&#20363;&#65292;&#20854;&#30446;&#26631;&#26159;&#25552;&#39640;&#27169;&#22411;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#12290;&#22810;&#20010;&#22270;&#20013;&#24515;&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;InstructGraph&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36229;&#36807;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08785v1 Announce Type: new Abstract: Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose InstructGraph, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the output's reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 a
&lt;/p&gt;</description></item><item><title>DNABERT-S&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21019;&#24314;&#29289;&#31181;&#24863;&#30693;&#30340;DNA&#23884;&#20837;&#30340;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#38271;&#35835;DNA&#24207;&#21015;&#30340;&#23884;&#20837;&#25928;&#26524;&#65292;&#24341;&#20837;&#20102;Manifold Instance Mixup (MI-Mix)&#23545;&#27604;&#30446;&#26631;&#26041;&#27861;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.08777</link><description>&lt;p&gt;
DNABERT-S: &#23398;&#20064;&#20855;&#26377;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#30340;&#29289;&#31181;&#24863;&#30693;DNA&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08777
&lt;/p&gt;
&lt;p&gt;
DNABERT-S&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21019;&#24314;&#29289;&#31181;&#24863;&#30693;&#30340;DNA&#23884;&#20837;&#30340;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#38271;&#35835;DNA&#24207;&#21015;&#30340;&#23884;&#20837;&#25928;&#26524;&#65292;&#24341;&#20837;&#20102;Manifold Instance Mixup (MI-Mix)&#23545;&#27604;&#30446;&#26631;&#26041;&#27861;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;DNA&#23884;&#20837;&#22312;&#22522;&#22240;&#32452;&#20998;&#26512;&#20013;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#20047;&#29992;&#20110;&#27169;&#22411;&#24494;&#35843;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#23439;&#22522;&#22240;&#32452;&#20998;&#31665;&#65292;&#36825;&#26159;&#24494;&#29983;&#29289;&#32452;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#36807;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#26469;&#33258;&#21487;&#33021;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#19981;&#21516;&#30340;&#12289;&#36890;&#24120;&#27809;&#26377;&#32463;&#36807;&#34920;&#24449;&#30340;&#29289;&#31181;&#30340;&#22797;&#26434;&#28151;&#21512;DNA&#24207;&#21015;&#30340;&#29289;&#31181;&#26469;&#23545;DNA&#24207;&#21015;&#36827;&#34892;&#20998;&#32452;&#12290;&#20026;&#20102;&#22635;&#34917;&#26377;&#25928;&#30340;DNA&#23884;&#20837;&#27169;&#22411;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DNABERT-S&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21019;&#24314;&#29289;&#31181;&#24863;&#30693;&#30340;DNA&#23884;&#20837;&#30340;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#40723;&#21169;&#23545;&#26131;&#20986;&#38169;&#30340;&#38271;&#35835;DNA&#24207;&#21015;&#36827;&#34892;&#26377;&#25928;&#23884;&#20837;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Manifold Instance Mixup(MI-Mix)&#65292;&#19968;&#31181;&#23545;&#27604;&#30446;&#26631;&#65292;&#23427;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23618;&#27425;&#20013;&#28151;&#21512;DNA&#24207;&#21015;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#20197;&#22312;&#36755;&#20986;&#23618;&#35782;&#21035;&#21644;&#21306;&#20998;&#36825;&#20123;&#28151;&#21512;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08777v1 Announce Type: cross Abstract: Effective DNA embedding remains crucial in genomic analysis, particularly in scenarios lacking labeled data for model fine-tuning, despite the significant advancements in genome foundation models. A prime example is metagenomics binning, a critical process in microbiome research that aims to group DNA sequences by their species from a complex mixture of DNA sequences derived from potentially thousands of distinct, often uncharacterized species. To fill the lack of effective DNA embedding models, we introduce DNABERT-S, a genome foundation model that specializes in creating species-aware DNA embeddings. To encourage effective embeddings to error-prone long-read DNA sequences, we introduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes the hidden representations of DNA sequences at randomly selected layers and trains the model to recognize and differentiate these mixed proportions at the output layer. We further enha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#29992;&#20110;&#26816;&#27979;&#20154;&#36523;&#38750;&#20154;&#21270;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#26159;&#22823;&#35268;&#27169;&#33258;&#21160;&#25910;&#38598;&#30340;&#35821;&#26009;&#24211;&#65292;&#21478;&#19968;&#20010;&#26159;&#23567;&#35268;&#27169;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#20998;&#26512;&#21644;&#33258;&#21160;&#20998;&#31867;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08764</link><description>&lt;p&gt;
&#29992;&#20110;&#26816;&#27979;&#20154;&#36523;&#35328;&#35770;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Dataset for the Detection of Dehumanizing Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#29992;&#20110;&#26816;&#27979;&#20154;&#36523;&#38750;&#20154;&#21270;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#26159;&#22823;&#35268;&#27169;&#33258;&#21160;&#25910;&#38598;&#30340;&#35821;&#26009;&#24211;&#65292;&#21478;&#19968;&#20010;&#26159;&#23567;&#35268;&#27169;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#20998;&#26512;&#21644;&#33258;&#21160;&#20998;&#31867;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#36523;&#38750;&#20154;&#21270;&#26159;&#19968;&#31181;&#24515;&#29702;&#36807;&#31243;&#65292;&#20351;&#24471;&#26576;&#20010;&#32676;&#20307;&#34987;&#25490;&#38500;&#21644;&#34384;&#24453;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#20154;&#36523;&#38750;&#20154;&#21270;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#26159;&#22823;&#35268;&#27169;&#33258;&#21160;&#25910;&#38598;&#30340;&#35821;&#26009;&#24211;&#65292;&#21478;&#19968;&#20010;&#26159;&#23567;&#35268;&#27169;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#25919;&#27835;&#35328;&#35770;&#21644;&#30005;&#24433;&#23383;&#24149;&#23545;&#35805;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#22810;&#26679;&#21270;&#30340;&#20154;&#36523;&#38750;&#20154;&#21270;&#25968;&#25454;&#65292;&#20415;&#20110;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#24615;&#20998;&#26512;&#21644;&#33258;&#21160;&#20998;&#31867;&#20154;&#36523;&#38750;&#20154;&#21270;&#27169;&#24335;&#12290;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#23558;&#20250;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08764v1 Announce Type: new Abstract: Dehumanization is a mental process that enables the exclusion and ill treatment of a group of people. In this paper, we present two data sets of dehumanizing text, a large, automatically collected corpus and a smaller, manually annotated data set. Both data sets include a combination of political discourse and dialogue from movie subtitles. Our methods give us a broad and varied amount of dehumanization data to work with, enabling further exploratory analysis and automatic classification of dehumanization patterns. Both data sets will be publicly released.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21407;&#22987;&#20869;&#23481;&#21644;&#27969;&#30021;&#24615;&#30340;&#21516;&#26102;&#28151;&#28102;&#20316;&#32773;&#36523;&#20221;&#12290;</title><link>https://arxiv.org/abs/2402.08761</link><description>&lt;p&gt;
JAMDEC&#65306;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26377;&#32422;&#26463;&#35299;&#30721;&#30340;&#26080;&#30417;&#30563;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;
&lt;/p&gt;
&lt;p&gt;
JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08761
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21407;&#22987;&#20869;&#23481;&#21644;&#27969;&#30021;&#24615;&#30340;&#21516;&#26102;&#28151;&#28102;&#20316;&#32773;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22686;&#24378;&#30340;&#20316;&#32773;&#36523;&#20221;&#35782;&#21035;&#25216;&#26415;&#21644;&#22312;&#32447;&#20869;&#23481;&#30340;&#27704;&#20037;&#24615;&#65292;&#38656;&#35201;&#26356;&#24378;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#20445;&#25252;&#22312;&#32447;&#20316;&#32773;&#36523;&#20221;&#30340;&#38544;&#31169;&#65292;&#20363;&#22914;&#31185;&#23398;&#35770;&#25991;&#30340;&#30450;&#23457;&#12289;&#21311;&#21517;&#22312;&#32447;&#35780;&#35770;&#25110;&#22312;&#24515;&#29702;&#20581;&#24247;&#35770;&#22363;&#19978;&#30340;&#21311;&#21517;&#20114;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25512;&#29702;&#26102;&#38388;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#30340;&#29420;&#29305;&#25361;&#25112;&#65306;&#32570;&#20047;&#22810;&#26679;&#21270;&#30340;&#20316;&#32773;&#36523;&#20221;&#21644;&#39046;&#22495;&#30340;&#30417;&#30563;&#25968;&#25454;&#65292;&#20197;&#21450;&#22312;&#28151;&#28102;&#20316;&#32773;&#36523;&#20221;&#30340;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#20869;&#23481;&#21644;&#27969;&#30021;&#24615;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08761v1 Announce Type: cross Abstract: The permanence of online content combined with the enhanced authorship identification techniques calls for stronger computational methods to protect the identity and privacy of online authorship when needed, e.g., blind reviews for scientific papers, anonymous online reviews, or anonymous interactions in the mental health forums. In this paper, we propose an unsupervised inference-time approach to authorship obfuscation to address the unique challenges of authorship obfuscation: lack of supervision data for diverse authorship and domains, and the need for a sufficient level of revision beyond simple paraphrasing to obfuscate the authorship, all the while preserving the original content and fluency.   We introduce JAMDEC, a user-controlled, inference-time algorithm for authorship obfuscation that can be in principle applied to any text and authorship. Our approach builds on small language models such as GPT2-XL in order to help avoid dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CyclePrompt&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#20248;&#21270;&#25552;&#31034;&#65292;&#22312;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#20013;&#36827;&#34892;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#30417;&#30563;&#23398;&#20064;&#12290;&#36825;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#20174;&#23436;&#25104;&#24230;&#21040;&#20219;&#21153;&#35268;&#33539;&#30340;&#21453;&#21521;&#25512;&#23548;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#27809;&#26377;&#26114;&#36149;&#24494;&#35843;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#22797;&#26434;&#22806;&#37096;&#29615;&#22659;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08756</link><description>&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#25552;&#38382;&#65306;&#24490;&#29615;&#19968;&#33268;&#24615;&#22312;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#20013;&#20248;&#21270;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CyclePrompt&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#20248;&#21270;&#25552;&#31034;&#65292;&#22312;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#20013;&#36827;&#34892;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#30417;&#30563;&#23398;&#20064;&#12290;&#36825;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#20174;&#23436;&#25104;&#24230;&#21040;&#20219;&#21153;&#35268;&#33539;&#30340;&#21453;&#21521;&#25512;&#23548;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#27809;&#26377;&#26114;&#36149;&#24494;&#35843;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#22797;&#26434;&#22806;&#37096;&#29615;&#22659;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;LLMs&#36827;&#34892;&#38646;-shot&#25512;&#26029;&#26102;&#65292;&#36890;&#24120;&#20250;&#20351;&#29992;&#19968;&#20010;&#21253;&#21547;&#20219;&#21153;&#35268;&#33539;&#30340;&#25552;&#31034;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#23436;&#25104;&#24230;&#12290;&#28982;&#32780;&#65292;&#36824;&#27809;&#26377;&#25506;&#32034;&#20174;&#23436;&#25104;&#24230;&#21040;&#20219;&#21153;&#35268;&#33539;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#37319;&#29992;&#21452;&#21521;&#24490;&#29615;&#30417;&#30563;&#23398;&#20064;&#65292;&#23436;&#20840;&#22312;&#19978;&#19979;&#25991;&#20013;&#25191;&#34892;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#21069;&#21521;&#26144;&#23556;f: X -&gt; Y&#65288;&#20363;&#22914;&#65292;&#22270;&#20687; -&gt; &#29983;&#25104;&#30340;&#26631;&#39064;&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#36870;&#21521;&#26144;&#23556;g: Y -&gt; X&#65288;&#20363;&#22914;&#65292;&#26631;&#39064; -&gt; &#29983;&#25104;&#30340;&#22270;&#20687;&#65289;&#65292;&#26469;&#26500;&#24314;&#19968;&#20010;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#8220;&#25439;&#22833;&#8221;&#65288;&#20197;&#25552;&#31034;&#30340;&#26356;&#26032;&#24418;&#24335;&#65289;&#65292;&#20197;&#24378;&#21046;g(f(X))&#32422;&#31561;&#20110;X&#12290;&#36825;&#31181;&#25216;&#26415;&#31216;&#20026;CyclePrompt&#65292;&#21033;&#29992;&#24490;&#29615;&#19968;&#33268;&#24615;&#20316;&#20026;&#33258;&#30001;&#30417;&#30563;&#20449;&#21495;&#26469;&#24490;&#29615;&#24615;&#22320;&#26500;&#24314;&#25552;&#31034;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;CyclePrompt&#21487;&#20197;&#22312;&#19981;&#26114;&#36149;&#30340;&#24494;&#35843;&#12289;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#21644;&#27809;&#26377;&#22797;&#26434;&#22806;&#37096;&#29615;&#22659;&#65288;&#22914;&#32534;&#35793;&#22120;&#12289;API&#65289;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;CyclePrompt&#65306;&#20195;&#30721;&#29983;&#25104;&#21644;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08756v1 Announce Type: new Abstract: When LLMs perform zero-shot inference, they typically use a prompt with a task specification, and generate a completion. However, there is no work to explore the possibility of the reverse - going from completion to task specification. In this paper, we employ both directions to perform cycle-supervised learning entirely in-context. Our goal is to create a forward map f : X -&gt; Y (e.g. image -&gt; generated caption), coupled with a backward map g : Y -&gt; X (e.g. caption -&gt; generated image) to construct a cycle-consistency "loss" (formulated as an update to the prompt) to enforce g(f(X)) ~= X. The technique, called CyclePrompt, uses cycle-consistency as a free supervisory signal to iteratively craft the prompt. Importantly, CyclePrompt reinforces model performance without expensive fine-tuning, without training data, and without the complexity of external environments (e.g. compilers, APIs). We demonstrate CyclePrompt in two domains: code gener
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08702</link><description>&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08702
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt optimization aims to find the best prompt for a language model (LLM) in multi-step tasks. This paper introduces a genetic algorithm framework that incorporates human feedback to automatically suggest prompt improvements. It addresses challenges such as complex prompt content analysis, evaluation of individual steps, and variations in task execution preferences.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08702v1 Announce Type: cross Abstract: Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules about potential errors to automatically offer direct suggestions for improvement. Our framework is stylized as a genetic algorithm in which an LLM generates new candidate prompts from a parent
&lt;/p&gt;</description></item><item><title>SemRel2024&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65292;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#21512;&#21487;&#20197;&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21512;&#28085;&#30422;&#20102;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#31561;14&#31181;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2402.08638</link><description>&lt;p&gt;
SemRel2024: 14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08638
&lt;/p&gt;
&lt;p&gt;
SemRel2024&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65292;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#21512;&#21487;&#20197;&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21512;&#28085;&#30422;&#20102;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#31561;14&#31181;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#26159;&#35821;&#35328;&#34920;&#36798;&#30340;&#26680;&#24515;&#12290;&#23427;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#21253;&#25324;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#25552;&#20379;&#27934;&#23519;&#12290;&#34429;&#28982;&#26089;&#26399;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#19978;&#65292;&#24448;&#24448;&#26159;&#22312;&#33521;&#35821;&#35821;&#22659;&#20013;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#26356;&#24191;&#27867;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#20540;&#24471;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SemRel&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#27597;&#35821;&#20026;14&#31181;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#30340;&#26032;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65306;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#12290;&#36825;&#20123;&#35821;&#35328;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#30340;&#35821;&#31995;&#65292;&#20027;&#35201;&#22312;&#38750;&#27954;&#21644;&#20122;&#27954;&#20351;&#29992;&#65292;&#36825;&#20123;&#22320;&#21306;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30456;&#23545;&#36739;&#23569;&#12290;SemRel&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#37117;&#26159;&#19982;&#19968;&#20010;&#34920;&#31034;&#30456;&#20851;&#24615;&#24471;&#20998;&#30340;&#21477;&#23376;&#23545;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring and quantifying semantic relatedness is central to representing language. It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of Large Language Models (LLMs). While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness. In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by a relatively limited availability of NLP resources. Each instance in the SemRel datasets is a sentence pair associated with a score that repr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#37319;&#29992;&#34164;&#28085;&#23545;&#40784;&#65292;&#20197;&#20248;&#21270;&#21487;&#34892;&#24615;&#65292;&#25552;&#21462;&#26377;&#29702;&#30340;&#26041;&#24335;&#25552;&#20379;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.08479</link><description>&lt;p&gt;
&#21487;&#20449;&#30340;&#21462;&#26679;&#21512;&#29702;&#21270;&#36890;&#36807;&#21322;&#30417;&#30563;&#30340;&#34164;&#28085;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Plausible Extractive Rationalization through Semi-Supervised Entailment Signal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#37319;&#29992;&#34164;&#28085;&#23545;&#40784;&#65292;&#20197;&#20248;&#21270;&#21487;&#34892;&#24615;&#65292;&#25552;&#21462;&#26377;&#29702;&#30340;&#26041;&#24335;&#25552;&#20379;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#21644;&#19981;&#36879;&#26126;&#30340;&#40657;&#30418;&#23376;&#27169;&#22411;&#30340;&#22686;&#21152;&#38656;&#35201;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#25514;&#26045;&#65292;&#20854;&#20013;&#19968;&#31181;&#36873;&#25321;&#26159;&#25552;&#21462;&#26377;&#29702;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#20316;&#20026;&#26356;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#36825;&#20123;&#27169;&#22411;&#65292;&#20063;&#31216;&#20026;&#20808;&#35299;&#37322;&#28982;&#21518;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#35299;&#37322;&#27169;&#22411;&#26469;&#25552;&#21462;&#26377;&#29702;&#65292;&#28982;&#21518;&#20351;&#29992;&#25552;&#21462;&#30340;&#20449;&#24687;&#26469;&#35843;&#25972;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20379;&#31934;&#30830;&#21644;&#24544;&#23454;&#30340;&#35299;&#37322;&#65292;&#30001;&#25552;&#21462;&#30340;&#26377;&#29702;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#21322;&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#25552;&#21462;&#26377;&#29702;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#20010;&#23567;&#22411;&#30340;&#26377;&#30417;&#30563;&#26377;&#29702;&#38598;&#65288;10%&#65289;&#19978;&#36827;&#19968;&#27493;&#24494;&#35843;&#23427;&#12290;&#36890;&#36807;&#34164;&#28085;&#23545;&#40784;&#65292;NLI&#39044;&#27979;&#27169;&#22411;&#34987;&#21033;&#29992;&#20316;&#20026;&#35299;&#37322;&#27169;&#22411;&#30340;&#19968;&#31181;&#30417;&#30563;&#20449;&#21495;&#28304;&#12290;&#36890;&#36807;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#24378;&#21046;&#35299;&#37322;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#23545;&#40784;&#19968;&#33268;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing use of complex and opaque black box models requires the adoption of interpretable measures, one such option is extractive rationalizing models, which serve as a more interpretable alternative. These models, also known as Explain-Then-Predict models, employ an explainer model to extract rationales and subsequently condition the predictor with the extracted information. Their primary objective is to provide precise and faithful explanations, represented by the extracted rationales. In this paper, we take a semi-supervised approach to optimize for the plausibility of extracted rationales. We adopt a pre-trained natural language inference (NLI) model and further fine-tune it on a small set of supervised rationales ($10\%$). The NLI predictor is leveraged as a source of supervisory signals to the explainer via entailment alignment. We show that, by enforcing the alignment agreement between the explanation and answer in a question-answering task, the performance can be improve
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#30340;&#30005;&#23376;&#37038;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#39640;&#25928;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08309</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#21521;&#37327;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Prompted Contextual Vectors for Spear-Phishing Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08309
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#30340;&#30005;&#23376;&#37038;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#39640;&#25928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#30340;&#30005;&#23376;&#37038;&#20214;&#24182;&#26041;&#20415;&#30446;&#26631;&#20390;&#23519;&#26469;&#21319;&#32423;&#20102;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLMs&#30340;&#38598;&#21512;&#26469;&#21019;&#24314;&#34920;&#31034;&#21521;&#37327;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#26469;&#25512;&#29702;&#21644;&#22238;&#31572;&#20154;&#24037;&#21046;&#23450;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#37327;&#21270;&#30005;&#23376;&#37038;&#20214;&#20869;&#23481;&#20013;&#24120;&#35265;&#35828;&#26381;&#21407;&#21017;&#30340;&#23384;&#22312;&#65292;&#20026;&#19979;&#28216;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#19978;&#19979;&#25991;&#25991;&#26723;&#21521;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19987;&#26377;&#31995;&#32479;&#29983;&#25104;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#33258;&#21160;&#21270;&#30446;&#26631;&#20390;&#23519;&#21644;&#38035;&#40060;&#30005;&#23376;&#37038;&#20214;&#30340;&#21019;&#24314;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20165;&#21253;&#21547;&#20256;&#32479;&#38035;&#40060;&#21644;&#33391;&#24615;&#30005;&#23376;&#37038;&#20214;&#30340;&#35757;&#32451;&#38598;&#20013;&#23454;&#29616;&#20102;91%&#30340;F1&#24471;&#20998;&#65292;&#20854;&#20013;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#19968;&#31181;&#21019;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance. To address this, we propose a detection approach based on a novel document vectorization method that utilizes an ensemble of LLMs to create representation vectors. By prompting LLMs to reason and respond to human-crafted questions, we quantify the presence of common persuasion principles in the email's content, producing prompted contextual document vectors for a downstream supervised machine learning model. We evaluate our method using a unique dataset generated by a proprietary system that automates target reconnaissance and spear-phishing email creation. Our method achieves a 91% F1 score in identifying LLM-generated spear-phishing emails, with the training set comprising only traditional phishing and benign emails. Key contributions include an innovative document vectorization method utilizin
&lt;/p&gt;</description></item><item><title>ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08303</link><description>&lt;p&gt;
ChatCell: &#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
ChatCell: Facilitating Single-Cell Analysis with Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08303
&lt;/p&gt;
&lt;p&gt;
ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#22312;&#31185;&#23398;&#20013;&#30340;&#24433;&#21709;&#26085;&#30410;&#31361;&#20986;&#12290;LLMs&#22312;&#20219;&#21153;&#27867;&#21270;&#21644;&#33258;&#30001;&#23545;&#35805;&#26041;&#38754;&#30340;&#26032;&#20852;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#25512;&#36827;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#36825;&#20010;&#26500;&#25104;&#29983;&#29289;&#20307;&#22522;&#30784;&#26500;&#20214;&#30340;&#39046;&#22495;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#24403;&#21069;&#26041;&#27861;&#22312;&#30693;&#35782;&#38376;&#27099;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;LLMs&#22312;&#25484;&#25569;&#21333;&#32454;&#32990;&#25968;&#25454;&#26041;&#38754;&#30340;&#20805;&#20998;&#21033;&#29992;&#65292;&#24433;&#21709;&#20102;&#30452;&#25509;&#21487;&#35775;&#38382;&#21644;&#24555;&#36895;&#36845;&#20195;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ChatCell&#65292;&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#22312;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#39046;&#22495;&#33719;&#24471;&#20102;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;BiasMedQA&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#20219;&#21153;&#20013;LLMs&#30340;&#35748;&#30693;&#20559;&#35265;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#21457;&#29616;LLMs&#22312;&#38754;&#23545;&#21253;&#21547;&#35748;&#30693;&#20559;&#35265;&#30340;&#20020;&#24202;&#38382;&#39064;&#26102;&#65292;&#20854;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#26126;&#26174;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.08113</link><description>&lt;p&gt;
&#35299;&#20915;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing cognitive bias in medical language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;BiasMedQA&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#20219;&#21153;&#20013;LLMs&#30340;&#35748;&#30693;&#20559;&#35265;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#21457;&#29616;LLMs&#22312;&#38754;&#23545;&#21253;&#21547;&#35748;&#30693;&#20559;&#35265;&#30340;&#20020;&#24202;&#38382;&#39064;&#26102;&#65292;&#20854;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#26126;&#26174;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#21307;&#23398;&#39046;&#22495;&#24050;&#32463;&#24341;&#36215;&#20102;&#37325;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#27169;&#25311;&#20020;&#24202;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#20934;&#30830;&#24615;&#24456;&#26377;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#20020;&#24202;&#20915;&#31574;&#27604;&#27169;&#25311;&#26356;&#22797;&#26434;&#65292;&#22240;&#20026;&#21307;&#29983;&#30340;&#20915;&#31574;&#21463;&#21040;&#35768;&#22810;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#35748;&#30693;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#38754;&#23545;&#21253;&#21547;&#35748;&#30693;&#20559;&#35265;&#30340;&#20020;&#24202;&#38382;&#39064;&#26102;&#65292;&#19982;&#19981;&#21253;&#21547;&#36825;&#20123;&#20559;&#35265;&#30340;&#38382;&#39064;&#30456;&#27604;&#65292;&#20854;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#20250;&#26126;&#26174;&#38477;&#20302;&#65292;&#36825;&#19968;&#38382;&#39064;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#30340;&#20551;&#35774;&#35748;&#20026;&#65292;&#24403;LLMs&#38754;&#23545;&#21253;&#21547;&#35748;&#30693;&#20559;&#35265;&#30340;&#20020;&#24202;&#38382;&#39064;&#26102;&#65292;&#19982;&#19981;&#21253;&#21547;&#36825;&#20123;&#20559;&#35265;&#30340;&#38382;&#39064;&#30456;&#27604;&#65292;&#20854;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#20250;&#26126;&#26174;&#38477;&#20302;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;BiasMedQA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#20351;&#29992;BiasMedQA&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20845;&#20010;LLMs&#65292;&#20998;&#21035;&#26159;GPT-4&#12289;Mixtral-8x70B&#12289;GPT-3.5&#12289;PaLM-2&#12289;Llama 2 70B-chat&#21644;&#21307;&#23398;&#19987;&#19994;&#30340;PMC Llama 13B&#12290;&#25105;&#20204;&#22312;127&#20010;&#20020;&#24202;&#38382;&#39064;&#19978;&#27979;&#35797;&#20102;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of large language models (LLMs) into the medical field has gained significant attention due to their promising accuracy in simulated clinical decision-making settings. However, clinical decision-making is more complex than simulations because physicians' decisions are shaped by many factors, including the presence of cognitive bias. However, the degree to which LLMs are susceptible to the same cognitive biases that affect human clinicians remains unexplored. Our hypothesis posits that when LLMs are confronted with clinical questions containing cognitive biases, they will yield significantly less accurate responses compared to the same questions presented without such biases.In this study, we developed BiasMedQA, a novel benchmark for evaluating cognitive biases in LLMs applied to medical tasks. Using BiasMedQA we evaluated six LLMs, namely GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B-chat, and the medically specialized PMC Llama 13B. We tested these models on 1,27
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017; ($\mathbf{UA}^2$)&#65292;&#26088;&#22312;&#23454;&#29616;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#30340;&#32479;&#19968;&#23545;&#40784;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07744</link><description>&lt;p&gt;
&#23454;&#29616;&#26234;&#33021;&#20307;&#12289;&#20154;&#31867;&#21644;&#29615;&#22659;&#20043;&#38388;&#30340;&#32479;&#19968;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Alignment Between Agents, Humans, and Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017; ($\mathbf{UA}^2$)&#65292;&#26088;&#22312;&#23454;&#29616;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#30340;&#32479;&#19968;&#23545;&#40784;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#23637;&#23548;&#33268;&#20102;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#32321;&#33635;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#36827;&#34892;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#22797;&#26434;&#12289;&#29616;&#23454;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#26102;&#65292;&#26234;&#33021;&#20307;&#30340;&#25928;&#33021;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017;&#65292;&#21363;&#21516;&#26102;&#23545;&#40784;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#65288;&#22914;&#36135;&#24065;&#39044;&#31639;&#38480;&#21046;&#65289;&#12290;&#20174;&#32479;&#19968;&#23545;&#40784; ($\mathbf{UA}^2$) &#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24403;&#21069;&#26234;&#33021;&#20307;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#26234;&#33021;&#20307;&#22522;&#20934;&#21644;&#26041;&#27861;&#20505;&#36873;&#20013;&#34987;&#24573;&#35270;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20026;WebShop&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#65292;&#21253;&#25324;&#20351;&#29992;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#26469;&#23637;&#31034;&#24847;&#22270;&#12289;&#20010;&#24615;&#21270;&#37325;&#26032;&#25490;&#21517;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#29615;&#22659;&#21160;&#24577;&#21644;&#36816;&#34892;&#26102;&#25104;&#26412;&#32479;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents ($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost stat
&lt;/p&gt;</description></item><item><title>OpenToM&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24515;&#29702;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#12289;&#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#12289;&#20197;&#21450;&#25361;&#25112;&#27169;&#22411;&#23545;&#24515;&#29702;&#29366;&#24577;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#19982;&#24515;&#29702;&#19990;&#30028;&#30340;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.06044</link><description>&lt;p&gt;
&#24320;&#25918;&#29702;&#35770;-&#24515;&#28789;&#65288;OpenToM&#65289;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#28789;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06044
&lt;/p&gt;
&lt;p&gt;
OpenToM&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24515;&#29702;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#12289;&#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#12289;&#20197;&#21450;&#25361;&#25112;&#27169;&#22411;&#23545;&#24515;&#29702;&#29366;&#24577;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#19982;&#24515;&#29702;&#19990;&#30028;&#30340;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24515;&#29702;&#29702;&#35770;&#65288;N-ToM&#65289;&#26159;&#26426;&#22120;&#29702;&#35299;&#21644;&#36319;&#36394;&#20182;&#20154;&#24515;&#29702;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#22312;&#24320;&#21457;&#20855;&#26377;&#31038;&#20132;&#26234;&#33021;&#30340;&#20195;&#29702;&#31243;&#24207;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;N-ToM&#22522;&#20934;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#27169;&#31946;&#21644;&#20154;&#24037;&#25925;&#20107;&#30340;&#23384;&#22312;&#65292;&#32570;&#20047;&#20010;&#24615;&#29305;&#24449;&#21644;&#20559;&#22909;&#65292;&#32570;&#20047;&#28041;&#21450;&#35282;&#33394;&#24515;&#29702;&#24515;&#24577;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#25552;&#20986;&#30340;&#38382;&#39064;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;OpenToM&#65292;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;N-ToM&#30340;&#22522;&#20934;&#65292;&#20197; (1) &#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#65292;(2) &#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#65292;(3) &#35302;&#21457;&#35282;&#33394;&#24847;&#22270;&#30340;&#34892;&#21160;&#65292;&#20197;&#21450; (4) &#35774;&#35745;&#26088;&#22312;&#25361;&#25112;LLMs&#23545;&#24314;&#27169;&#35282;&#33394;&#22312;&#29289;&#29702;&#21644;&#24515;&#29702;&#19990;&#30028;&#30340;&#24515;&#29702;&#29366;&#24577;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;OpenToM&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#24314;&#27169;&#29289;&#29702;&#19990;&#30028;&#30340;&#19968;&#20123;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36319;&#36394;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characte
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#21644;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#20026;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.05128</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25552;&#21319;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#21644;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#20026;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#31185;&#20070;&#38382;&#31572;&#65288;TQA&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#19978;&#19979;&#25991;&#21644;&#22810;&#27169;&#24335;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#22312;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#21644;&#26080;&#27861;&#25429;&#25417;&#38271;&#25991;&#26412;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24341;&#20837;&#38761;&#21629;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;LLMs&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;TQA&#20013;&#39046;&#22495;&#22806;&#24773;&#26223;&#65292;&#21363;&#27010;&#24565;&#20998;&#24067;&#22312;&#19981;&#21516;&#35838;&#31243;&#20013;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#24182;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;LLM&#27169;&#22411;Llama-2&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#24182;&#21152;&#20837;RAG&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#20248;&#20110;&#22522;&#32447;&#65292;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;4.12%&#65292;&#22312;&#27979;&#35797;&#38598;&#19978;&#25552;&#39640;&#20102;9.84%&#12290;
&lt;/p&gt;
&lt;p&gt;
Textbook question answering (TQA) is a challenging task in artificial intelligence due to the complex nature of context and multimodal data. Although previous research has significantly improved the task, there are still some limitations including the models' weak reasoning and inability to capture contextual information in the lengthy context. The introduction of large language models (LLMs) has revolutionized the field of AI, however, directly applying LLMs often leads to inaccurate answers. This paper proposes a methodology that handle the out-of-domain scenario in TQA where concepts are spread across different lessons by incorporating the retrieval augmented generation (RAG) technique and utilize transfer learning to handle the long context and enhance reasoning abilities. Through supervised fine-tuning of the LLM model Llama-2 and the incorporation of RAG, our architecture outperforms the baseline, achieving a 4.12% accuracy improvement on validation set and 9.84% on test set for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04838</link><description>&lt;p&gt;
PaDeLLM-NER&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24182;&#34892;&#35299;&#30721;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;LLMs&#30340;&#39640;&#24310;&#36831;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#39034;&#24207;&#35299;&#30721;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;NER&#30340;&#25152;&#26377;&#26631;&#31614;&#21644;&#25552;&#21450;&#65292;&#26174;&#33879;&#22686;&#21152;&#20102;&#24207;&#21015;&#38271;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PaDeLLM-NER&#65288;Parallel Decoding in LLM for NE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#27169;&#22359;&#25110;&#26550;&#26500;&#20462;&#25913;&#21363;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#20013;&#30340;&#26041;&#27861;&#12290;PaDeLLM-NER&#20801;&#35768;&#21516;&#26102;&#35299;&#30721;&#25152;&#26377;&#25552;&#21450;&#65292;&#20174;&#32780;&#20943;&#23569;&#29983;&#25104;&#24310;&#36831;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PaDeLLM-NER&#30340;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#23545;&#33521;&#35821;&#21644;&#20013;&#25991;&#26469;&#35828;&#27604;&#33258;&#22238;&#24402;&#26041;&#27861;&#24555;1.76&#21040;10.22&#20493;&#12290;&#19982;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#32500;&#25345;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35843;&#26597;&#20102;&#21253;&#21547;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#20027;&#24352;&#30340;NLP&#30740;&#31350;&#65292;&#21457;&#29616;&#19981;&#21516;&#35770;&#25991;&#23545;&#20110;&#36825;&#19968;&#27010;&#24565;&#30340;&#23450;&#20041;&#21644;&#26631;&#20934;&#21508;&#19981;&#30456;&#21516;&#65292;&#24341;&#20837;&#20102;&#22810;&#20010;&#32500;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#35821;&#35328;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#35821;&#35328;&#36873;&#25321;&#23384;&#22312;&#30340;&#20559;&#21521;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.04222</link><description>&lt;p&gt;
NLP&#20013;&#30340;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What is 'Typological Diversity' in NLP?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35843;&#26597;&#20102;&#21253;&#21547;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#20027;&#24352;&#30340;NLP&#30740;&#31350;&#65292;&#21457;&#29616;&#19981;&#21516;&#35770;&#25991;&#23545;&#20110;&#36825;&#19968;&#27010;&#24565;&#30340;&#23450;&#20041;&#21644;&#26631;&#20934;&#21508;&#19981;&#30456;&#21516;&#65292;&#24341;&#20837;&#20102;&#22810;&#20010;&#32500;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#35821;&#35328;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#35821;&#35328;&#36873;&#25321;&#23384;&#22312;&#30340;&#20559;&#21521;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#30740;&#31350;&#30028;&#23545;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#25237;&#20837;&#26356;&#22810;&#20851;&#27880;&#65292;&#20174;&#32780;&#22312;&#22810;&#35821;&#35328;NLP&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25913;&#36827;&#21482;&#36866;&#29992;&#20110;&#19990;&#30028;&#35821;&#35328;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#20026;&#20102;&#25193;&#23637;&#36825;&#19968;&#33539;&#22260;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35770;&#25991;&#33268;&#21147;&#20110;&#25552;&#39640;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#35821;&#35328;&#31867;&#22411;&#23398;&#24120;&#34987;&#29992;&#26469;&#36873;&#25321;&#35821;&#35328;&#65292;&#22522;&#20110;&#24191;&#27867;&#30340;&#35821;&#35328;&#31867;&#22411;&#26679;&#26412;&#24212;&#33021;&#24102;&#26469;&#23545;&#22810;&#31181;&#35821;&#35328;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#20123;&#36873;&#25321;&#36890;&#24120;&#34987;&#25551;&#36848;&#20026;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#21253;&#21547;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#20027;&#24352;&#30340;NLP&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#30830;&#20999;&#30340;&#23450;&#20041;&#25110;&#26631;&#20934;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#32500;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#36817;&#20284;&#35821;&#35328;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#21457;&#29616;&#32467;&#26524;&#22312;&#19981;&#21516;&#35770;&#25991;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#35328;&#36873;&#25321;&#30340;&#20559;&#21521;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The NLP research community has devoted increased attention to languages beyond English, resulting in considerable improvements for multilingual NLP. However, these improvements only apply to a small subset of the world's languages. Aiming to extend this, an increasing number of papers aspires to enhance generalizable multilingual performance across languages. To this end, linguistic typology is commonly used to motivate language selection, on the basis that a broad typological sample ought to imply generalization across a broad range of languages. These selections are often described as being 'typologically diverse'. In this work, we systematically investigate NLP research that includes claims regarding 'typological diversity'. We find there are no set definitions or criteria for such claims. We introduce metrics to approximate the diversity of language selection along several axes and find that the results vary considerably across papers. Furthermore, we show that skewed language sele
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.01677</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#23884;&#20837;&#26412;&#20307;
&lt;/p&gt;
&lt;p&gt;
Embedding Ontologies via Incoprorating Extensional and Intensional Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21253;&#21547;&#39046;&#22495;&#20869;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#20197;&#20998;&#20026;&#20004;&#20010;&#31867;&#21035;&#65292;&#21363;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22806;&#24310;&#30693;&#35782;&#25552;&#20379;&#20851;&#20110;&#26412;&#20307;&#20013;&#29305;&#23450;&#27010;&#24565;&#25152;&#23646;&#30340;&#20855;&#20307;&#23454;&#20363;&#30340;&#20449;&#24687;&#65292;&#32780;&#20869;&#28085;&#30693;&#35782;&#35814;&#32454;&#25551;&#36848;&#20102;&#27010;&#24565;&#20043;&#38388;&#30340;&#20869;&#22312;&#23646;&#24615;&#12289;&#29305;&#24449;&#21644;&#35821;&#20041;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#26410;&#33021;&#21516;&#26102;&#20805;&#20998;&#32771;&#34385;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EIKE&#65288;Extensional and Intensional Knowledge Embedding&#65289;&#30340;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#12290;EIKE&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#23454;&#20363;&#12289;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#23884;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#23545;&#22806;&#24310;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20869;&#28085;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can captur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65292;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#36825;&#20123;&#32467;&#26524;&#20063;&#26174;&#31034;&#20986;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2401.08273</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#38646;&#23556;&#20987;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Null-Shot Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65292;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#36825;&#20123;&#32467;&#26524;&#20063;&#26174;&#31034;&#20986;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#12290;&#38646;&#23556;&#20987;&#25552;&#31034;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#36890;&#36807;&#25351;&#31034;LLMs&#21033;&#29992;&#20174;&#8220;&#31034;&#20363;&#8221;&#37096;&#20998;&#20013;&#33719;&#21462;&#30340;&#20449;&#24687;&#65288;&#35813;&#20449;&#24687;&#22312;&#25152;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#20013;&#19981;&#23384;&#22312;&#65289;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#34429;&#28982;&#20943;&#23569;&#38169;&#35823;&#20449;&#24687;&#23545;&#20110;LLMs&#30340;&#26085;&#24120;&#21644;&#37325;&#35201;&#29992;&#36884;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#22312;&#30446;&#21069;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;LLMs&#20173;&#28982;&#20855;&#26377;&#38169;&#35823;&#20449;&#24687;&#65292;&#23454;&#38469;&#19978;&#21487;&#20197;&#21033;&#29992;&#38169;&#35823;&#20449;&#24687;&#26469;&#25552;&#39640;&#19982;&#26631;&#20934;&#38646;&#23556;&#20987;&#25552;&#31034;&#30456;&#27604;&#30340;&#20219;&#21153;&#34920;&#29616;&#12290;&#23545;&#20843;&#20010;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#20843;&#20010;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65289;&#20013;&#65292;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#35266;&#23519;&#21040;&#30340;&#19981;&#19968;&#33268;&#24615;&#22686;&#21152;&#30456;&#23545;&#24615;&#33021;&#22312;LLMs&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20063;&#21487;&#33021;&#34920;&#31034;&#27599;&#20010;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08273v2 Announce Type: replace-cross Abstract: This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the "Examples" section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across the LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25552;&#31034;&#21521;&#37327;&#26500;&#24314;&#36830;&#32493;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#36830;&#32493;&#25552;&#31034;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.10323</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25552;&#31034;&#21521;&#37327;&#32447;&#24615;&#32452;&#21512;&#29983;&#25104;&#36830;&#32493;&#25552;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continuous Prompt Generation from Linear Combination of Discrete Prompt Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25552;&#31034;&#21521;&#37327;&#26500;&#24314;&#36830;&#32493;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#36830;&#32493;&#25552;&#31034;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#25552;&#31034;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#24378;&#35843;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#25935;&#24863;&#20219;&#21153;&#65288;&#22914;&#31616;&#21382;&#31579;&#36873;&#65289;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25552;&#31034;&#21521;&#37327;&#26500;&#24314;&#36830;&#32493;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#36830;&#32493;&#25552;&#31034;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#25913;&#36827;&#12290;&#23545;&#20110;&#19968;&#32452;&#25163;&#21160;&#35774;&#35745;&#30340;&#31163;&#25955;&#25552;&#31034;$\mathcal{D}$&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#35789;&#24182;&#23884;&#20837;&#20026;&#24352;&#37327;&#24418;&#24335;&#65292;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#26435;&#37325;&#65292;&#20351;&#24471;&#36825;&#20123;&#25552;&#31034;&#30340;&#32447;&#24615;&#32452;&#21512;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10323v2 Announce Type: replace Abstract: The wayward quality of continuous prompts stresses the importance of their interpretability as unexpected and unpredictable behaviors appear following training, especially in the context of large language models automating people-sensitive tasks such as resume screening. In this paper we present a novel method of constructing continuous prompts via discrete prompt embeddings and evaluate improvements to continuous prompt interpretability and inference accuracy. For a set of manually designed discrete prompts $\mathcal{D}$, which we tokenize and embed each into tensor form, we train a model to predict the weights such that the linear combinations of those prompts correspond to higher performance on natural language understanding tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;diff&#21382;&#21490;&#30340;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;&#29615;&#22659;&#20013;&#30340;&#35266;&#27979;&#36716;&#25442;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#20415;&#23545;&#20110;&#38271;&#26399;&#25512;&#29702;&#20915;&#31574;&#30340;&#20219;&#21153;&#20013;&#30340;Neural Language Models&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.07540</link><description>&lt;p&gt;
Neural Language Agents&#30340;diff&#21382;&#21490;
&lt;/p&gt;
&lt;p&gt;
diff History for Neural Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;diff&#21382;&#21490;&#30340;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;&#29615;&#22659;&#20013;&#30340;&#35266;&#27979;&#36716;&#25442;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#20415;&#23545;&#20110;&#38271;&#26399;&#25512;&#29702;&#20915;&#31574;&#30340;&#20219;&#21153;&#20013;&#30340;Neural Language Models&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Language Models (LMs)&#20026;&#36890;&#29992;&#30340;&#20855;&#20307;&#25511;&#21046;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#22522;&#20110;LM&#30340;&#25511;&#21046;&#22120;&#26102;&#65292;&#20250;&#20986;&#29616;&#19968;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#38382;&#39064;&#65306;&#29615;&#22659;&#35266;&#27979;&#24517;&#39035;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#36825;&#19982;&#21382;&#21490;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#20887;&#38271;&#32780;&#20887;&#20313;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#22240;&#27492;&#65292;LM&#20195;&#29702;&#30340;&#20808;&#21069;&#24037;&#20316;&#23616;&#38480;&#20110;&#20855;&#26377;&#23567;&#35266;&#27979;&#22823;&#23567;&#20197;&#21450;&#23545;&#20132;&#20114;&#21382;&#21490;&#25110;&#25351;&#31034;&#35843;&#20248;&#38656;&#27714;&#36739;&#23567;&#30340;&#38480;&#21046;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;diff&#21382;&#21490;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#19988;&#38750;&#24120;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#29992;&#20110;&#25552;&#31034;LM&#31574;&#30053;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#30340;&#36830;&#32493;&#25991;&#26412;&#35266;&#27979;&#19978;&#24212;&#29992;Unix diff&#21629;&#20196;&#65292;&#25105;&#20204;&#26082;&#21487;&#20197;&#25688;&#38500;&#20887;&#20313;&#20449;&#24687;&#65292;&#21448;&#21487;&#20197;&#23558;&#25991;&#26412;&#36755;&#20837;&#30340;&#20869;&#23481;&#38598;&#20013;&#22312;&#29615;&#22659;&#20013;&#26174;&#33879;&#21464;&#21270;&#30340;&#26041;&#38754;&#12290;&#22312;&#38656;&#35201;&#38271;&#26399;&#25512;&#29702;&#36827;&#34892;&#20915;&#31574;&#30340;&#26410;&#35299;&#20915;&#30340;&#35270;&#39057;&#28216;&#25103;NetHack&#20013;&#65292;&#20351;&#29992;diff&#21382;&#21490;&#35843;&#20248;&#30340;LM&#19982;&#29366;&#24577;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07540v2 Announce Type: replace Abstract: Neural Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, results in long and verbose textual prompts. As a result, prior work in LM agents is limited to restricted domains with small observation size as well as minimal needs for interaction history or instruction tuning. In this paper, we introduce diff history, a simple and highly effective solution to these issues. By applying the Unix diff command on consecutive text observations in the interaction histories used to prompt LM policies, we can both abstract away redundant information and focus the content of textual inputs on the salient changes in the environment. On NetHack, an unsolved video game that requires long-horizon reasoning for decision-making, LMs tuned with diff history match state-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#36923;&#36753;&#20462;&#25913;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#36923;&#36753;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.17438</link><description>&lt;p&gt;
CLOMO: &#24102;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#36923;&#36753;&#20462;&#25913;
&lt;/p&gt;
&lt;p&gt;
CLOMO: Counterfactual Logical Modification with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#36923;&#36753;&#20462;&#25913;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#36923;&#36753;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22521;&#20859;LLMs&#20869;&#30340;&#21453;&#20107;&#23454;&#24605;&#32500;&#36807;&#31243;&#65292;&#24182;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#21453;&#20107;&#23454;&#36923;&#36753;&#20462;&#25913;&#65288;CLOMO&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#22522;&#20934;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;LLMs&#24517;&#39035;&#29087;&#32451;&#22320;&#25913;&#21464;&#32473;&#23450;&#30340;&#35770;&#35777;&#25991;&#26412;&#65292;&#20197;&#20445;&#25345;&#39044;&#23450;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#20026;&#20102;&#26377;&#25928;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36923;&#36753;&#24863;&#30693;&#30340;&#21453;&#20107;&#23454;&#20998;&#25968;&#65292;&#30452;&#25509;&#35780;&#20272;LLMs&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20986;&#65292;&#32780;&#19981;&#26159;&#23558;&#20219;&#21153;&#24314;&#27169;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#20559;&#22909;&#24456;&#22909;&#22320;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;LLMs&#23637;&#31034;&#20102;&#26174;&#30528;&#30340;&#36923;&#36753;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17438v3 Announce Type: replace-cross Abstract: In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the LogicAware Counterfactual Score to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for log
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28508;&#31354;&#38388;&#25805;&#25511;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#21521;&#37327;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#36981;&#24490;&#31034;&#20363;&#28436;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#21521;&#37327;&#30340;&#37327;&#32423;&#26469;&#36731;&#26494;&#25511;&#21046;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2311.06668</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#21521;&#37327;&#20013;&#65292;&#36890;&#36807;&#28508;&#31354;&#38388;&#25805;&#25511;&#20351;&#19978;&#19979;&#25991;&#23398;&#20064;&#26356;&#26377;&#25928;&#21644;&#21487;&#25511;
&lt;/p&gt;
&lt;p&gt;
In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06668
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28508;&#31354;&#38388;&#25805;&#25511;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#21521;&#37327;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#36981;&#24490;&#31034;&#20363;&#28436;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#21521;&#37327;&#30340;&#37327;&#32423;&#26469;&#36731;&#26494;&#25511;&#21046;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#26032;&#20219;&#21153;&#36866;&#24212;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#31034;&#20363;&#28436;&#31034;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#26377;&#38480;&#65292;&#38590;&#20197;&#23450;&#37327;&#25511;&#21046;&#65292;&#24182;&#21344;&#29992;&#19978;&#19979;&#25991;&#31383;&#21475;&#31354;&#38388;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#37325;&#26032;&#23450;&#20041;&#20026;&#19978;&#19979;&#25991;&#21521;&#37327;&#65288;ICV&#65289;&#12290;&#20351;&#29992;ICV&#26377;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#28436;&#31034;&#31034;&#20363;&#36827;&#34892;&#27491;&#21521;&#20256;&#36882;&#65292;&#20174;LLM&#30340;&#28508;&#22312;&#23884;&#20837;&#20013;&#21019;&#24314;&#19978;&#19979;&#25991;&#21521;&#37327;&#12290;&#36825;&#20010;&#21521;&#37327;&#25429;&#25417;&#20102;&#20851;&#20110;&#39044;&#26399;&#20219;&#21153;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#23545;&#20110;&#19968;&#20010;&#26032;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#19981;&#26159;&#23558;&#31034;&#20363;&#28155;&#21152;&#21040;&#25552;&#31034;&#20013;&#65292;&#32780;&#26159;&#20351;&#29992;ICV&#26469;&#25913;&#21464;LLM&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;ICV&#26041;&#27861;&#26377;&#20960;&#20010;&#22909;&#22788;&#65306;1&#65289;&#23427;&#20351;LLM&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#36981;&#24490;&#31034;&#20363;&#28436;&#31034;&#65307;2&#65289;&#36890;&#36807;&#35843;&#25972;ICV&#30340;&#37327;&#32423;&#65292;&#23427;&#26131;&#20110;&#25511;&#21046;&#65307;3&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06668v3 Announce Type: replace-cross Abstract: Large language models (LLMs) demonstrate emergent in-context learning capabilities, where they adapt to new tasks based on example demonstrations. However, in-context learning has seen limited effectiveness in many settings, is difficult to quantitatively control and takes up context window space. To overcome these limitations, we propose an alternative approach that recasts in-context learning as in-context vectors (ICV). Using ICV has two steps. We first use a forward pass on demonstration examples to create the in-context vector from the latent embedding of the LLM. This vector captures essential information about the intended task. On a new query, instead of adding demonstrations to the prompt, we shift the latent states of the LLM using the ICV. The ICV approach has several benefits: 1) it enables the LLM to more effectively follow the demonstration examples; 2) it's easy to control by adjusting the magnitude of the ICV; 3)
&lt;/p&gt;</description></item><item><title>LINC&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#19968;&#38454;&#36923;&#36753;&#34920;&#36798;&#24335;&#65292;&#20877;&#36890;&#36807;&#22806;&#37096;&#23450;&#29702;&#35777;&#26126;&#22120;&#36827;&#34892;&#25512;&#29702;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36923;&#36753;&#25512;&#29702;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.15164</link><description>&lt;p&gt;
LINC: &#36890;&#36807;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#19968;&#38454;&#36923;&#36753;&#35777;&#26126;&#22120;&#30456;&#32467;&#21512;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.15164
&lt;/p&gt;
&lt;p&gt;
LINC&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#19968;&#38454;&#36923;&#36753;&#34920;&#36798;&#24335;&#65292;&#20877;&#36890;&#36807;&#22806;&#37096;&#23450;&#29702;&#35777;&#26126;&#22120;&#36827;&#34892;&#25512;&#29702;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36923;&#36753;&#25512;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#25512;&#29702;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23427;&#36890;&#36807;&#20174;&#19968;&#32452;&#21069;&#25552;&#20013;&#25512;&#26029;&#32467;&#35770;&#30340;&#30495;&#20540;&#65292;&#22312;&#31185;&#23398;&#12289;&#25968;&#23398;&#21644;&#31038;&#20250;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#25552;&#31034;&#30340;&#31574;&#30053;&#26469;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#36825;&#31181;&#25512;&#29702;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#34920;&#29616;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#32463;&#24120;&#22312;&#24494;&#22937;&#21644;&#38590;&#20197;&#39044;&#27979;&#30340;&#26041;&#24335;&#19979;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#36825;&#20123;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#27169;&#22359;&#21270;&#30340;&#31070;&#32463;&#31526;&#21495;&#32534;&#31243;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;LINC&#65306;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;&#22312;LINC&#20013;&#65292;LLM&#20805;&#24403;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#30340;&#21069;&#25552;&#21644;&#32467;&#35770;&#36716;&#21270;&#20026;&#19968;&#38454;&#36923;&#36753;&#34920;&#36798;&#24335;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#34920;&#36798;&#24335;&#36716;&#31227;&#21040;&#22806;&#37096;&#23450;&#29702;&#35777;&#26126;&#22120;&#20013;&#65292;&#35813;&#35777;&#26126;&#22120;&#36890;&#36807;&#31526;&#21495;&#26041;&#24335;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.15164v2 Announce Type: replace-cross Abstract: Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gain
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#33258;&#28982;&#30340;&#12289;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;</title><link>https://arxiv.org/abs/2304.08460</link><description>&lt;p&gt;
LongForm: &#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LongForm: Effective Instruction Tuning with Reverse Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.08460
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#33258;&#28982;&#30340;&#12289;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Instruction tuning&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#27867;&#21270;&#65292;&#24182;&#26356;&#22909;&#22320;&#36981;&#24490;&#29992;&#25143;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#25351;&#20196;&#25968;&#25454;&#25104;&#26412;&#39640;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#37319;&#29992;&#20102;&#35832;&#22914;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12289;&#23384;&#22312;&#23545;&#40784;&#38382;&#39064;&#30340;&#20247;&#21253;&#25968;&#25454;&#38598;&#12289;&#20197;&#21450;&#36890;&#36807;LLMs&#29983;&#25104;&#22122;&#22768;&#31034;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LongForm-C&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#21453;&#21521;&#25351;&#20196;&#21019;&#24314;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#20026;&#20154;&#31867;&#20889;&#20316;&#35821;&#26009;&#24211;&#31034;&#20363;&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#29983;&#25104;&#25351;&#20196;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35832;&#22914;C4&#21644;Wikipedia&#30340;&#35821;&#26009;&#24211;&#20013;&#36873;&#25321;&#20102;&#19968;&#32452;&#22810;&#26679;&#24615;&#30340;&#20154;&#31867;&#25776;&#20889;&#25991;&#26723;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#20026;&#36825;&#20123;&#25991;&#26723;&#29983;&#25104;&#25351;&#20196;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#20415;&#23452;&#12289;&#26356;&#24178;&#20928;&#12289;&#36755;&#20986;&#33258;&#28982;&#20197;&#21450;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.08460v2 Announce Type: replace-cross Abstract: Instruction tuning enables language models to more effectively generalize and better follow user intent. However, obtaining instruction data is costly and challenging. Prior work employs methods such as expensive human annotation, crowd-sourced datasets with alignment issues, and generating noisy examples via LLMs. We introduce the LongForm-C dataset, which is created by reverse instructions. We generate instructions via LLMs for human-written corpus examples using reverse instructions. First we select a diverse set of human-written documents from corpora such as C4 and Wikipedia; then we generate instructions for these documents via LLMs. This approach provides a cheaper and cleaner instruction-tuning dataset with natural output and one suitable for long text generation. Our models outperform 10x larger language models without instruction tuning on tasks such as story/recipe generation and long-form question answering. Moreover
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.11624</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#31034;&#33539;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#19978;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;ICL&#33021;&#21147;&#23545;&#20110;&#23569;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#26159;&#25935;&#24863;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#36827;&#23637;&#26159;&#26816;&#32034;&#38024;&#23545;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#23450;&#21046;&#30340;&#31034;&#33539;&#12290;&#31034;&#33539;&#26816;&#32034;&#30340;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#21644;&#26816;&#32034;&#31995;&#32479;&#12290;&#36825;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;&#19988;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;&#37492;&#20110;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#21644;&#22312;&#26816;&#32034;&#31034;&#33539;&#30340;ICL&#26041;&#38754;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#32508;&#36848;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#21644;&#27604;&#36739;&#20102;&#26816;&#32034;&#27169;&#22411;&#30340;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#65292;&#26816;&#32034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#20219;&#21153;&#27010;&#24565;&#21270;&#20026;&#21382;&#21490;&#20107;&#20214;&#38142;&#20013;&#30340;&#20107;&#20214;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21644;&#32467;&#26500;&#21270;&#21382;&#21490;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#21450;&#25972;&#21512;&#21453;&#21521;&#30693;&#35782;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;SOTA&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06072</link><description>&lt;p&gt;
&#21382;&#21490;&#38142;&#30340;&#38142;&#36335;&#39044;&#27979;&#19982;&#23398;&#20064;&#65306;&#22522;&#20110;LLMs&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion. (arXiv:2401.06072v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#20219;&#21153;&#27010;&#24565;&#21270;&#20026;&#21382;&#21490;&#20107;&#20214;&#38142;&#20013;&#30340;&#20107;&#20214;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21644;&#32467;&#26500;&#21270;&#21382;&#21490;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#21450;&#25972;&#21512;&#21453;&#21521;&#30693;&#35782;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#26102;&#38388;&#32467;&#26500;&#30693;&#35782;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19978;&#32570;&#22833;&#30340;&#20107;&#20214;&#38142;&#25509;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#38142;&#36335;&#39044;&#27979;&#27010;&#24565;&#21270;&#20026;&#21382;&#21490;&#20107;&#20214;&#38142;&#20013;&#30340;&#20107;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;LLMs&#36866;&#24212;&#29305;&#23450;&#30340;&#22270;&#25991;&#20449;&#24687;&#21644;&#22312;&#26102;&#38388;&#32447;&#20013;&#21457;&#29616;&#30340;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#32467;&#26500;&#30340;&#21382;&#21490;&#25968;&#25454;&#22686;&#24378;&#21644;&#21453;&#21521;&#30693;&#35782;&#30340;&#25972;&#21512;&#65292;&#20197;&#22686;&#24378;LLMs&#23545;&#32467;&#26500;&#20449;&#24687;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#23884;&#20837;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;SOTA&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20805;&#20998;&#30340;&#28040;&#34701;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph Completion (TKGC) is a challenging task of predicting missing event links at future timestamps by leveraging established temporal structural knowledge. Given the formidable generative capabilities inherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize temporal link prediction as an event generation task within the context of a historical event chain. We employ efficient fine-tuning methods to make LLMs adapt to specific graph textual information and patterns discovered in temporal timelines. Furthermore, we introduce structure-based historical data augmentation and the integration of reverse knowledge to emphasize LLMs' awareness of structural information, thereby enhancing their reasoning capabilities. We conduct thorough experiments on multiple widely used datasets and find that our fine-tuned model outperforms existing embedding-based models on multiple metrics, achieving SOTA results. We also carry out sufficient ablation experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#30740;&#31350;&#20102;Twitter&#29992;&#25143;&#23545;ChatGPT&#30340;&#24577;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#24635;&#20307;&#24773;&#24863;&#26159;&#20013;&#24615;&#21040;&#31215;&#26497;&#30340;&#65292;&#26368;&#21463;&#20851;&#27880;&#30340;&#20027;&#39064;&#21253;&#25324;&#20154;&#24037;&#26234;&#33021;&#12289;&#25628;&#32034;&#24341;&#25806;&#12289;&#25945;&#32946;&#12289;&#20889;&#20316;&#21644;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2306.12951</link><description>&lt;p&gt;
&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#24314;&#27169;&#36319;&#36394;Twitter&#19978;ChatGPT&#30340;&#20844;&#20247;&#24577;&#24230;
&lt;/p&gt;
&lt;p&gt;
Tracking public attitudes toward ChatGPT on Twitter using sentiment analysis and topic modeling. (arXiv:2306.12951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#30740;&#31350;&#20102;Twitter&#29992;&#25143;&#23545;ChatGPT&#30340;&#24577;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#24635;&#20307;&#24773;&#24863;&#26159;&#20013;&#24615;&#21040;&#31215;&#26497;&#30340;&#65292;&#26368;&#21463;&#20851;&#27880;&#30340;&#20027;&#39064;&#21253;&#25324;&#20154;&#24037;&#26234;&#33021;&#12289;&#25628;&#32034;&#24341;&#25806;&#12289;&#25945;&#32946;&#12289;&#20889;&#20316;&#21644;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#29992;&#25143;&#22522;&#25968;&#22686;&#38271;&#26041;&#38754;&#21019;&#19979;&#20102;&#26032;&#35760;&#24405;&#12290;&#34429;&#28982;&#23427;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26368;&#26032;&#33021;&#21147;&#65292;&#20294;&#23427;&#20063;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20844;&#20247;&#20851;&#27880;&#65292;&#28041;&#21450;&#20854;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#24212;&#29992;&#20110;Twitter&#25968;&#25454;&#26469;&#35843;&#26597;&#20844;&#20247;&#23545;ChatGPT&#30340;&#24577;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#24635;&#20307;&#24773;&#24863;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#20013;&#24615;&#21040;&#31215;&#26497;&#30340;&#65292;&#36825;&#20063;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32844;&#19994;&#32676;&#20307;&#12290;&#22312;&#20247;&#22810;&#25552;&#21040;&#30340;&#20027;&#39064;&#20013;&#65292;&#26368;&#21463;&#20851;&#27880;&#30340;&#20027;&#39064;&#26159;&#20154;&#24037;&#26234;&#33021;&#12289;&#25628;&#32034;&#24341;&#25806;&#12289;&#25945;&#32946;&#12289;&#20889;&#20316;&#21644;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT sets a new record with the fastest-growing user base, as a chatbot powered by a large language model (LLM). While it demonstrates state-of-the-art capabilities in a variety of language-generating tasks, it also raises widespread public concerns regarding its societal impact. In this paper, we utilize natural language processing approaches to investigate the public attitudes towards ChatGPT by applying sentiment analysis and topic modeling techniques to Twitter data. Our result shows that the overall sentiment is largely neutral to positive, which also holds true across different occupation groups. Among a wide range of topics mentioned in tweets, the most popular topics are Artificial Intelligence, Search Engines, Education, Writing, and Question Answering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;SSD-2&#65292;&#23427;&#21487;&#20197;&#20174;0.4B&#25193;&#23637;&#21040;13B&#21442;&#25968;&#65292;&#24182;&#32463;&#36807;&#24494;&#35843;&#26469;&#36981;&#24490;&#25351;&#20196;&#12290;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#20351;&#29992;SSD-2&#21487;&#20197;&#24418;&#25104;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#21512;&#20316;&#65292;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14771</link><description>&lt;p&gt;
SSD-2&#65306;&#25193;&#23637;&#21644;&#25512;&#29702;&#26102;&#38388;&#34701;&#21512;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models. (arXiv:2305.14771v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;SSD-2&#65292;&#23427;&#21487;&#20197;&#20174;0.4B&#25193;&#23637;&#21040;13B&#21442;&#25968;&#65292;&#24182;&#32463;&#36807;&#24494;&#35843;&#26469;&#36981;&#24490;&#25351;&#20196;&#12290;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#20351;&#29992;SSD-2&#21487;&#20197;&#24418;&#25104;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#21512;&#20316;&#65292;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#35328;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26131;&#20110;&#22312;&#25512;&#29702;&#26102;&#36827;&#34892;&#25511;&#21046;&#65292;&#24182;&#19988;&#26159;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#20165;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#35268;&#27169;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;SSD-LM&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23558;&#20854;&#20174;0.4B&#25193;&#23637;&#21040;13B&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#25913;&#36827;&#20854;&#35757;&#32451;&#21644;&#25512;&#29702;&#25928;&#29575;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#21629;&#21517;&#36825;&#20010;&#26032;&#27169;&#22411;&#20026;SSD-2&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36890;&#36807;&#24494;&#35843;&#26469;&#36981;&#24490;&#25351;&#20196;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SSD-2&#21487;&#20197;&#19982;100&#20493;&#26356;&#23567;&#30340;&#27169;&#22411;&#21512;&#20316;&#24418;&#25104;&#26032;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#30001;&#20010;&#20154;&#29992;&#25143;&#36827;&#34892;&#23450;&#21046;&#21644;&#37096;&#32626;&#12290;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#26356;&#21152;&#26377;&#25928;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based language models (LMs) have been shown to be competent generative models that are easy to control at inference and are a promising alternative to autoregressive LMs. While autoregressive LMs have benefited immensely from scaling and instruction-based learning, existing studies on diffusion LMs have been conducted on a relatively smaller scale. Starting with a recently proposed diffusion model SSD-LM, in this work we explore methods to scale it from 0.4B to 13B parameters, proposing several techniques to improve its training and inference efficiency. We call the new model SSD-2. We further show that this model can be easily finetuned to follow instructions. Finally, leveraging diffusion models' capability at inference-time control, we show that SSD-2 facilitates novel ensembles with 100x smaller models that can be customized and deployed by individual users. We find that compared to autoregressive models, the collaboration between diffusion models is more effective, leadi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#26469;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;&#65292;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03584</link><description>&lt;p&gt;
&#29616;&#22312;&#23427;&#21548;&#36215;&#26469;&#20687;&#20320;&#20102;&#65306;&#22312;&#35774;&#22791;&#19978;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;
&lt;/p&gt;
&lt;p&gt;
Now It Sounds Like You: Learning Personalized Vocabulary On Device. (arXiv:2305.03584v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03584
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#26469;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;&#65292;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#36827;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#20391;&#37325;&#20110;&#24212;&#29992;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#35774;&#22791;&#31471;&#35821;&#35328;&#24314;&#27169;&#12290;&#30001;&#20110;&#20869;&#23384;&#21644;&#24310;&#36831;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#25903;&#25345;&#23376;&#21333;&#35789;&#26631;&#35760;&#25110;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#20915;&#23450;&#37096;&#32626;&#23553;&#38381;&#35789;&#27719;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23553;&#38381;&#35789;&#27719;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#29305;&#23450;&#29992;&#25143;&#30340;&#29983;&#35789;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#65292;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#23545;&#20869;&#23384;&#21644;&#24310;&#36831;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#65292;&#26377;&#25928;&#22320;&#20174;&#20013;&#22830;&#27169;&#22411;&#20256;&#36755;&#30693;&#35782;&#65292;&#24182;&#20026;&#20010;&#24615;&#21270;&#35789;&#27719;&#23398;&#20064;&#21333;&#35789;&#23884;&#20837;&#12290;&#22312;&#19968;&#32452;&#24120;&#35265;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#29983;&#35789;&#25193;&#23637;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Federated Learning (FL) has shown significant advancements in its ability to perform various natural language processing (NLP) tasks. This work focuses on applying personalized FL for on-device language modeling. Due to limitations of memory and latency, these models cannot support the complexity of sub-word tokenization or beam search decoding, resulting in the decision to deploy a closed-vocabulary language model. However, closed-vocabulary models are unable to handle out-of-vocabulary (OOV) words belonging to specific users. To address this issue, We propose a novel technique called "OOV expansion" that improves OOV coverage and increases model accuracy while minimizing the impact on memory and latency. This method introduces a personalized "OOV adapter" that effectively transfers knowledge from a central model and learns word embedding for personalized vocabulary. OOV expansion significantly outperforms standard FL personalization methods on a set of common FL benc
&lt;/p&gt;</description></item></channel></rss>