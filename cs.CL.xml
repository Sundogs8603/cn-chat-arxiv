<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>DeTiME&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#30340;LLMs&#22686;&#24378;&#25193;&#25955;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#32858;&#31867;&#30340;&#23884;&#20837;&#21644;&#20855;&#26377;&#22686;&#24378;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#33021;&#29983;&#25104;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2310.15296</link><description>&lt;p&gt;
DeTiME: &#20351;&#29992;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#30340;LLM&#22686;&#24378;&#25193;&#25955;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM. (arXiv:2310.15296v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15296
&lt;/p&gt;
&lt;p&gt;
DeTiME&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#30340;LLMs&#22686;&#24378;&#25193;&#25955;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#32858;&#31867;&#30340;&#23884;&#20837;&#21644;&#20855;&#26377;&#22686;&#24378;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#33021;&#29983;&#25104;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20805;&#28385;&#27963;&#21147;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTMs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;NTMs&#20027;&#35201;&#20351;&#29992;&#26469;&#33258;LLMs&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#36825;&#23545;&#20110;&#32858;&#31867;&#25110;&#20027;&#39064;&#29983;&#25104;&#26469;&#35828;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21517;&#20026;DeTiME&#30340;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;DeTiME&#21033;&#29992;&#32534;&#30721;-&#35299;&#30721;&#30340;LLMs&#20135;&#29983;&#39640;&#24230;&#21487;&#32858;&#31867;&#30340;&#23884;&#20837;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#33021;&#22815;&#29983;&#25104;&#26082;&#20855;&#26377;&#20248;&#36234;&#30340;&#32858;&#31867;&#24615;&#21448;&#20855;&#26377;&#22686;&#24378;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#19982;&#24050;&#35782;&#21035;&#20027;&#39064;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#21452;&#37325;&#21151;&#33021;&#20351;&#29992;&#25143;&#33021;&#22815;&#21516;&#26102;&#39640;&#25928;&#20135;&#29983;&#39640;&#24230;&#32858;&#31867;&#30340;&#20027;&#39064;&#21644;&#30456;&#20851;&#20869;&#23481;&#12290;DeTiME&#30340;&#28508;&#21147;&#36824;&#21253;&#25324;&#29983;&#25104;&#38598;&#32676;&#21270;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the burgeoning field of natural language processing, Neural Topic Models (NTMs) and Large Language Models (LLMs) have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic generation. Our study addresses this gap by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages ncoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion, our framework also provides the capability to generate content relevant to the identified topics. This dual functionality allows users to efficiently produce highly clustered topics and related content simultaneously. DeTiME's potential extends to generating clustered embeddi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.08475</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#19982;&#32534;&#36753;&#21333;&#27169;&#24335;LLMs&#30456;&#27604;&#65292;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#32534;&#36753;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#23457;&#26597;&#21644;&#24910;&#37325;&#32771;&#34385;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;MMEdit&#65292;&#29992;&#20110;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21253;&#25324;&#21508;&#31181;&#27169;&#22411;&#32534;&#36753;&#22522;&#32447;&#30340;&#32508;&#21512;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#30340;&#19981;&#21516;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#22522;&#32447;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#23454;&#29616;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#20294;&#25928;&#26524;&#20173;&#28982;&#19981;&#29702;&#24819;&#65292;&#34920;&#26126;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#23384;&#22312;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#35265;&#35299;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/EasyEdit&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#65292;&#36890;&#36807;&#19982;LLMs&#20132;&#20114;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.15074</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#24314;&#27169;&#19982;&#25512;&#29702;&#65306;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial. (arXiv:2309.15074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#65292;&#36890;&#36807;&#19982;LLMs&#20132;&#20114;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;2018&#24180;&#20197;&#26469;&#24613;&#21095;&#22686;&#38271;&#65292;&#33258;&#24341;&#20837;&#19978;&#19979;&#25991;&#24863;&#30693;&#35745;&#31639;&#31995;&#32479;20&#24180;&#21518;&#12290;&#19978;&#19979;&#25991;&#24863;&#30693;&#35745;&#31639;&#36890;&#36807;&#32771;&#34385;&#26222;&#36866;&#35774;&#22791;&#12289;&#29992;&#25143;&#21644;&#31038;&#20250;&#30340;&#24773;&#20917;&#65292;&#23454;&#29616;&#20102;&#24191;&#27867;&#30340;&#21019;&#26032;&#24212;&#29992;&#65292;&#22914;&#36741;&#21161;&#29983;&#27963;&#12289;&#22522;&#20110;&#20301;&#32622;&#30340;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#31561;&#12290;&#20026;&#20102;&#35782;&#21035;&#19978;&#19979;&#25991;&#24182;&#30456;&#24212;&#22320;&#20570;&#20986;&#20915;&#31574;&#65292;&#37319;&#29992;&#20102;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65288;&#22914;&#26412;&#20307;&#35770;&#21644;OWL&#65289;&#20316;&#20026;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;LLMs&#30340;&#23835;&#36215;&#21644;&#23427;&#20204;&#25913;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#24314;&#27169;&#19978;&#19979;&#25991;&#24182;&#36890;&#36807;&#19982;ChatGPT&#21644;GPT-4&#31561;LLMs&#20132;&#20114;&#36827;&#34892;&#19978;&#19979;&#25991;&#25512;&#29702;&#21464;&#24471;&#21487;&#34892;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#25991;&#26412;&#12289;&#25552;&#31034;&#21644;&#33258;&#20027;&#20195;&#29702;&#65288;AutoAgents&#65289;&#20351;LLMs&#33021;&#22815;&#25191;&#34892;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have become phenomenally surging, since 2018--two decades after introducing context-awareness into computing systems. Through taking into account the situations of ubiquitous devices, users and the societies, context-aware computing has enabled a wide spectrum of innovative applications, such as assisted living, location-based social network services and so on. To recognize contexts and make decisions for actions accordingly, various artificial intelligence technologies, such as Ontology and OWL, have been adopted as representations for context modeling and reasoning. Recently, with the rise of LLMs and their improved natural language understanding and reasoning capabilities, it has become feasible to model contexts using natural language and perform context reasoning by interacting with LLMs such as ChatGPT and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and autonomous agents (AutoAgents) that enable LLMs to perform context modeling 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#27602;&#24615;&#20820;&#23376;&#27934;&#26694;&#26550;&#23545;PaLM 2&#30340;&#23433;&#20840;&#21453;&#39304;&#36827;&#34892;&#20102;&#31283;&#20581;&#24615;&#23457;&#35745;&#65292;&#25581;&#31034;&#20102;PaLM 2&#29983;&#25104;&#30340;&#39640;&#24230;&#20196;&#20154;&#19981;&#23433;&#30340;&#27602;&#24615;&#20869;&#23481;&#26410;&#34987;&#23433;&#20840;&#23432;&#25252;&#26639;&#35780;&#20272;&#20026;&#39640;&#24230;&#19981;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2309.06415</link><description>&lt;p&gt;
&#28145;&#20837;&#27602;&#24615;&#20820;&#23376;&#27934;&#65306;&#36890;&#36807;PaLM 2&#30340;&#23432;&#25252;&#26639;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Down the Toxicity Rabbit Hole: Investigating PaLM 2 Guardrails. (arXiv:2309.06415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06415
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#27602;&#24615;&#20820;&#23376;&#27934;&#26694;&#26550;&#23545;PaLM 2&#30340;&#23433;&#20840;&#21453;&#39304;&#36827;&#34892;&#20102;&#31283;&#20581;&#24615;&#23457;&#35745;&#65292;&#25581;&#31034;&#20102;PaLM 2&#29983;&#25104;&#30340;&#39640;&#24230;&#20196;&#20154;&#19981;&#23433;&#30340;&#27602;&#24615;&#20869;&#23481;&#26410;&#34987;&#23433;&#20840;&#23432;&#25252;&#26639;&#35780;&#20272;&#20026;&#39640;&#24230;&#19981;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;&#8220;&#27602;&#24615;&#20820;&#23376;&#27934;&#8221;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23545;PaLM 2&#30340;&#23433;&#20840;&#21453;&#39304;&#36827;&#34892;&#20102;&#24378;&#21270;&#31283;&#20581;&#24615;&#23457;&#35745;&#12290;&#20174;&#19968;&#20010;&#21051;&#26495;&#21360;&#35937;&#24320;&#22987;&#65292;&#35813;&#26694;&#26550;&#25351;&#31034;PaLM 2&#29983;&#25104;&#27604;&#21051;&#26495;&#21360;&#35937;&#26356;&#20855;&#26377;&#27602;&#24615;&#30340;&#20869;&#23481;&#12290;&#27599;&#19968;&#27425;&#36845;&#20195;&#65292;&#23427;&#37117;&#35201;&#27714;PaLM 2&#29983;&#25104;&#27604;&#19978;&#19968;&#27425;&#36845;&#20195;&#26356;&#20855;&#26377;&#27602;&#24615;&#30340;&#20869;&#23481;&#65292;&#30452;&#21040;PaLM 2&#30340;&#23433;&#20840;&#23432;&#25252;&#26639;&#21457;&#20986;&#23433;&#20840;&#36829;&#35268;&#35686;&#25253;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#26497;&#20854;&#20196;&#20154;&#19981;&#23433;&#30340;&#21453;&#29369;&#22826;&#20027;&#20041;&#12289;&#20234;&#26031;&#20848;&#24656;&#24807;&#30151;&#12289;&#31181;&#26063;&#20027;&#20041;&#12289;&#24656;&#21516;&#21644;&#21388;&#22899;&#24773;&#32490;&#65288;&#20165;&#21015;&#20030;&#20960;&#31181;&#65289;&#30340;&#29983;&#25104;&#20869;&#23481;&#65292;&#24182;&#19988;&#36825;&#20123;&#20869;&#23481;&#22312;PaLM 2&#30340;&#23433;&#20840;&#23432;&#25252;&#26639;&#35780;&#20272;&#20013;&#24182;&#26410;&#34987;&#35270;&#20026;&#39640;&#24230;&#19981;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper conducts a robustness audit of the safety feedback of PaLM 2 through a novel toxicity rabbit hole framework introduced here. Starting with a stereotype, the framework instructs PaLM 2 to generate more toxic content than the stereotype. Every subsequent iteration it continues instructing PaLM 2 to generate more toxic content than the previous iteration until PaLM 2 safety guardrails throw a safety violation. Our experiments uncover highly disturbing antisemitic, Islamophobic, racist, homophobic, and misogynistic (to list a few) generated content that PaLM 2 safety guardrails do not evaluate as highly unsafe.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20010;&#34920;&#31034;&#36716;&#31227;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#25216;&#26415;&#65292;&#24182;&#35777;&#26126;&#20102;&#27492;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04031</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21040;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#30340;&#22810;&#37325;&#34920;&#31034;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems. (arXiv:2309.04031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20010;&#34920;&#31034;&#36716;&#31227;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#25216;&#26415;&#65292;&#24182;&#35777;&#26126;&#20102;&#27492;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#26159;&#23558;&#35821;&#35328;&#30693;&#35782;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#21482;&#33021;&#36716;&#31227;LLM&#30340;&#21333;&#20010;&#34920;&#31034;&#65288;&#20363;&#22914;&#65292;&#39044;&#35757;&#32451;BERT&#30340;&#26368;&#21518;&#19968;&#23618;&#65289;&#65292;&#32780;&#25991;&#26412;&#30340;&#34920;&#31034;&#22312;&#26412;&#36136;&#19978;&#26159;&#38750;&#21807;&#19968;&#30340;&#65292;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#23618;&#12289;&#19978;&#19979;&#25991;&#21644;&#27169;&#22411;&#20197;&#21508;&#31181;&#26041;&#24335;&#33719;&#24471;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#25216;&#26415;&#26469;&#33719;&#24471;&#21644;&#36716;&#31227;LLMs&#30340;&#22810;&#20010;&#34920;&#31034;&#21040;&#22522;&#20110;&#20256;&#23548;&#22120;&#30340;ASR&#31995;&#32479;&#20013;&#12290;&#23613;&#31649;&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;LLMs&#30340;&#22810;&#20010;&#34920;&#31034;&#36716;&#31227;&#21487;&#20197;&#26159;&#36716;&#31227;&#21333;&#20010;&#34920;&#31034;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transferring the knowledge of large language models (LLMs) is a promising technique to incorporate linguistic knowledge into end-to-end automatic speech recognition (ASR) systems. However, existing works only transfer a single representation of LLM (e.g. the last layer of pretrained BERT), while the representation of a text is inherently non-unique and can be obtained variously from different layers, contexts and models. In this work, we explore a wide range of techniques to obtain and transfer multiple representations of LLMs into a transducer-based ASR system. While being conceptually simple, we show that transferring multiple representations of LLMs can be an effective alternative to transferring only a single representation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#30340;&#22810;&#35821;&#35328;&#20064;&#35821;&#30693;&#35782;&#24211;&#65288;IdiomKB&#65289;&#65292;&#36890;&#36807;&#26816;&#32034;&#20064;&#35821;&#30340;&#27604;&#21947;&#24847;&#20041;&#65292;&#23454;&#29616;&#23545;&#20064;&#35821;&#30340;&#26356;&#22909;&#32763;&#35793;&#12290;</title><link>http://arxiv.org/abs/2308.13961</link><description>&lt;p&gt;
&#32763;&#35793;&#21547;&#20041;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35789;&#35821;&#65306;IdiomKB&#22312;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#20064;&#35821;&#32763;&#35793;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Translate Meanings, Not Just Words: IdiomKB's Role in Optimizing Idiomatic Translation with Language Models. (arXiv:2308.13961v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#30340;&#22810;&#35821;&#35328;&#20064;&#35821;&#30693;&#35782;&#24211;&#65288;IdiomKB&#65289;&#65292;&#36890;&#36807;&#26816;&#32034;&#20064;&#35821;&#30340;&#27604;&#21947;&#24847;&#20041;&#65292;&#23454;&#29616;&#23545;&#20064;&#35821;&#30340;&#26356;&#22909;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36827;&#34892;&#33391;&#22909;&#30340;&#32763;&#35793;&#65292;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#21644;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#23545;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20197;&#21450;&#25991;&#21270;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#20854;&#38750;&#32452;&#21512;&#24615;&#30340;&#29305;&#24615;&#65292;&#20064;&#35821;&#23545;&#22522;&#20110;Transformer&#30340;&#31995;&#32479;&#25552;&#20986;&#20102;&#29305;&#27530;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#30452;&#35793;&#24448;&#24448;&#20250;&#24573;&#30053;&#24847;&#22270;&#12290;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#29616;&#26377;&#30340;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#26367;&#25442;&#20064;&#35821;&#65292;&#24448;&#24448;&#32570;&#20047;&#35268;&#27169;&#21644;&#19978;&#19979;&#25991;&#24847;&#35782;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#19978;&#19979;&#25991;&#24847;&#35782;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20801;&#35768;&#22312;&#21487;&#31649;&#29702;&#30340;KB&#22823;&#23567;&#30340;&#31163;&#32447;&#23384;&#20648;&#20013;&#23384;&#20648;&#20064;&#35821;&#12290;&#36825;&#30830;&#20445;&#20102;&#26356;&#39640;&#25928;&#30340;&#23567;&#22411;&#27169;&#22411;&#26381;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20064;&#35821;&#34920;&#36798;&#30340;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#30340;&#22810;&#35821;&#35328;&#20064;&#35821;KB&#65288;IdiomKB&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#26816;&#32034;&#20064;&#35821;&#30340;&#27604;&#21947;&#24847;&#20041;&#65292;&#35813;KB&#21487;&#20197;&#24110;&#21161;&#23567;&#22411;&#27169;&#22411;&#65288;&#22914;BLOOMZ&#65288;7.1B&#65289;&#65292;Alpaca&#65288;7B&#65289;&#21644;InstructGPT&#65288;6.7B&#65289;&#65289;&#23454;&#29616;&#26356;&#22909;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
To translate well, machine translation (MT) systems and general-purposed language models (LMs) need a deep understanding of both source and target languages and cultures. Therefore, idioms, with their non-compositional nature, pose particular challenges for Transformer-based systems, as literal translations often miss the intended meaning. Traditional methods, which replace idioms using existing knowledge bases (KBs), often lack scale and context awareness. Addressing these challenges, our approach prioritizes context awareness and scalability, allowing for offline storage of idioms in a manageable KB size. This ensures efficient serving with smaller models and provides a more comprehensive understanding of idiomatic expressions. We introduce a multilingual idiom KB (IdiomKB) developed using large LMs to address this. This KB facilitates better translation by smaller models, such as BLOOMZ (7.1B), Alpaca (7B), and InstructGPT (6.7B), by retrieving idioms' figurative meanings. We presen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.11730</link><description>&lt;p&gt;
&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#8220;&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#12289;&#39044;&#27979;&#8221;&#33539;&#24335;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;OD-QA&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MD-QA&#65289;&#22330;&#26223;&#19979;&#25506;&#32034;&#36825;&#20010;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#20010;&#35201;&#27714;&#23545;&#19981;&#21516;&#25991;&#26723;&#30340;&#20869;&#23481;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#32852;&#26377;&#28145;&#20837;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#65288;KGP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;MD-QA&#20013;&#20026;LLMs&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#22270;&#26500;&#24314;&#27169;&#22359;&#21644;&#22270;&#36941;&#21382;&#27169;&#22359;&#12290;&#23545;&#20110;&#22270;&#26500;&#24314;&#65292;&#25105;&#20204;&#20351;&#29992;&#33410;&#28857;&#26469;&#34920;&#31034;&#25991;&#27573;&#25110;&#25991;&#26723;&#32467;&#26500;&#65288;&#20363;&#22914;&#65292;&#39029;&#38754;/&#34920;&#26684;&#65289;&#65292;&#32780;&#20351;&#29992;&#36793;&#26469;&#34920;&#31034;&#25991;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;/&#35789;&#27719;&#30456;&#20284;&#24615;&#25110;&#32773;&#25991;&#26723;&#20869;&#30340;&#32467;&#26500;&#20851;&#31995;&#12290;&#23545;&#20110;&#22270;&#36941;&#21382;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#23427;&#22312;&#33410;&#28857;&#20043;&#38388;&#23548;&#33322;&#24182;&#25910;&#38598;&#25903;&#25345;&#24615;&#30340;&#25991;&#27573;&#65292;&#20197;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
&lt;/p&gt;</description></item><item><title>VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.06595</link><description>&lt;p&gt;
VisIT-Bench: &#19968;&#20010;&#21463;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#21551;&#21457;&#30340;&#35270;&#35273;&#35821;&#35328;&#25351;&#31034;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06595
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;VisIT-Bench&#65288;Visual InsTruction Benchmark&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20215;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#31034;&#36981;&#24490;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#36215;&#28857;&#26159;&#31574;&#21010;&#20102;70&#20010;&#8220;&#25351;&#31034;&#23478;&#26063;&#8221;&#65292;&#25105;&#20204;&#35748;&#20026;&#25351;&#31034;&#35843;&#20248;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#23478;&#26063;&#12290;&#20219;&#21153;&#19981;&#20165;&#38480;&#20110;VQAv2&#21644;COCO&#31561;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20174;&#22522;&#26412;&#35782;&#21035;&#21040;&#28216;&#25103;&#29609;&#27861;&#21644;&#21019;&#36896;&#24615;&#29983;&#25104;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#22312;&#31574;&#21010;&#20043;&#21518;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;592&#20010;&#27979;&#35797;&#26597;&#35810;&#65292;&#27599;&#20010;&#26597;&#35810;&#37117;&#24102;&#26377;&#19968;&#20010;&#20154;&#24037;&#32534;&#20889;&#30340;&#25351;&#31034;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#12290;&#36825;&#20123;&#25551;&#36848;&#23637;&#29616;&#20102;&#29305;&#23450;&#25351;&#31034;&#22240;&#32032;&#65292;&#20363;&#22914;&#23545;&#20110;&#35810;&#38382;&#24215;&#38754;&#23545;&#20110;&#36718;&#26885;&#29992;&#25143;&#30340;&#26131;&#35775;&#38382;&#24615;&#30340;&#25351;&#31034;&#65292;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#25551;&#36848;&#20102;&#26012;&#22369;/&#28508;&#22312;&#38556;&#30861;&#29289;&#12290;&#36825;&#20123;&#25551;&#36848;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#65306;1&#65289;&#25910;&#38598;&#27599;&#20010;&#23454;&#20363;&#30340;&#20154;&#24037;&#39564;&#35777;&#30340;&#21442;&#32771;&#36755;&#20986;&#65307;2&#65289;&#20351;&#29992;&#20165;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20505;&#36873;&#22810;&#27169;&#24577;&#29983;&#25104;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps be
&lt;/p&gt;</description></item><item><title>SeACo-Paraformer&#26159;&#19968;&#31181;&#20855;&#26377;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#28909;&#35789;&#33258;&#23450;&#20041;&#33021;&#21147;&#30340;&#38750;&#33258;&#22238;&#24402;ASR&#31995;&#32479;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36807;&#28388;&#22823;&#35268;&#27169;&#28909;&#35789;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.03266</link><description>&lt;p&gt;
SeACo-Paraformer:&#19968;&#31181;&#20855;&#26377;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#28909;&#35789;&#33258;&#23450;&#20041;&#33021;&#21147;&#30340;&#38750;&#33258;&#22238;&#24402;ASR&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability. (arXiv:2308.03266v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03266
&lt;/p&gt;
&lt;p&gt;
SeACo-Paraformer&#26159;&#19968;&#31181;&#20855;&#26377;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#28909;&#35789;&#33258;&#23450;&#20041;&#33021;&#21147;&#30340;&#38750;&#33258;&#22238;&#24402;ASR&#31995;&#32479;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36807;&#28388;&#22823;&#35268;&#27169;&#28909;&#35789;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#35789;&#33258;&#23450;&#20041;&#26159;ASR&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#33258;&#23450;&#20041;&#23454;&#20307;&#12289;&#20154;&#29289;&#21644;&#20854;&#20182;&#30701;&#35821;&#30340;&#21517;&#31216;&#20855;&#26377;&#20215;&#20540;&#12290;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;ASR&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#38544;&#24335;&#21644;&#26174;&#24335;&#24314;&#27169;&#31574;&#30053;&#37117;&#24471;&#21040;&#20102;&#21457;&#23637;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#36824;&#19981;&#38169;&#65292;&#20294;&#20173;&#23384;&#22312;&#26576;&#20123;&#32570;&#28857;&#65292;&#20363;&#22914;&#22312;&#25928;&#26524;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#20041;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;Paraformer (SeACo-Paraformer)&#30340;&#38750;&#33258;&#22238;&#24402;ASR&#31995;&#32479;&#65292;&#20855;&#26377;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#28909;&#35789;&#33258;&#23450;&#20041;&#33021;&#21147;&#12290;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;AED&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#22522;&#20110;NAR&#27169;&#22411;&#30340;&#25928;&#29575;&#20197;&#21450;&#22312;&#19978;&#19979;&#25991;&#24314;&#27169;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;&#22312;50,000&#23567;&#26102;&#30340;&#24037;&#19994;&#22823;&#25968;&#25454;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#33258;&#23450;&#20041;&#21644;&#24120;&#35268;ASR&#20219;&#21153;&#20013;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#22823;&#35268;&#27169;&#30340;&#28909;&#35789;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hotword customization is one of the important issues remained in ASR field it is of value to enable users of ASR systems to customize names of entities, persons and other phrases. The past few years have seen both implicit and explicit modeling strategies for ASR contextualization developed. While these approaches have performed adequately, they still exhibit certain shortcomings such as instability in effectiveness. In this paper we propose Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) a novel NAR based ASR system with flexible and effective hotword customization ability. It combines the accuracy of the AED-based model, the efficiency of the NAR model, and the excellent performance in contextualization. In 50,000 hours industrial big data experiments, our proposed model outperforms strong baselines in customization and general ASR tasks. Besides, we explore an efficient way to filter large scale incoming hotwords for further improvement. The source codes and industrial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16082</link><description>&lt;p&gt;
EnrichEvent: &#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#20026;&#26032;&#20986;&#29616;&#30340;&#20107;&#20214;&#25552;&#20379;&#20016;&#23500;&#30340;&#31038;&#20132;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#24179;&#21488;&#24050;&#25104;&#20026;&#20256;&#25773;&#21644;&#35752;&#35770;&#30495;&#23454;&#20107;&#20214;&#20449;&#24687;&#30340;&#20851;&#38190;&#24179;&#21488;&#65292;&#20026;&#21450;&#26089;&#21457;&#29616;&#26377;&#26032;&#38395;&#20215;&#20540;&#30340;&#20107;&#20214;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#20165;&#21033;&#29992;&#20851;&#38190;&#35789;&#31361;&#21457;&#24615;&#25110;&#32593;&#32476;&#32467;&#26500;&#26469;&#26816;&#27979;&#28909;&#28857;&#20107;&#20214;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20107;&#20214;&#21644;&#31038;&#20132;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32780;&#35328;&#65292;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#22312;&#36798;&#21040;&#36235;&#21183;&#29366;&#24577;&#20043;&#21069;&#35782;&#21035;&#20986;&#26032;&#20986;&#29616;&#30340;&#31038;&#20132;&#20107;&#20214;&#12290;&#31038;&#20132;&#25968;&#25454;&#65292;&#20363;&#22914;&#25512;&#25991;&#65292;&#20855;&#26377;&#25340;&#20889;&#38169;&#35823;&#12289;&#19981;&#23436;&#25972;&#24615;&#12289;&#27495;&#20041;&#24615;&#21644;&#35821;&#35328;&#19981;&#35268;&#33539;&#24615;&#65292;&#20197;&#21450;&#24847;&#35265;&#26041;&#38754;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#26469;&#23398;&#20064;&#20107;&#20214;&#30340;&#28436;&#21464;&#29305;&#24449;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20960;&#20046;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#27969;&#24335;&#31038;&#20132;&#25968;&#25454;&#30340;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social platforms have emerged as a crucial platform for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. However, most existing approaches for event detection solely exploit keyword burstiness or network structures to detect hot events. Thus, they often fail to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, ambiguity, and irregular language, as well as variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, in this paper, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data. In particular, we leverage contextual knowledge to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.12267</link><description>&lt;p&gt;
&#38754;&#21521;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#35770;&#25991;&#30340;&#33258;&#21160;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#33021;&#22815;&#22312;&#25552;&#20379;&#20855;&#20307;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#27969;&#30021;&#22238;&#31572;&#12290;&#23613;&#31649;&#25215;&#35748;&#25216;&#26415;&#36827;&#27493;&#24102;&#26469;&#30340;&#20415;&#21033;&#65292;&#25945;&#32946;&#32773;&#20063;&#25285;&#24515;&#23398;&#29983;&#21487;&#33021;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#24182;&#23558;&#20854;&#20551;&#20882;&#20026;&#33258;&#24049;&#30340;&#21407;&#21019;&#20316;&#21697;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;AI&#20869;&#23481;&#26816;&#27979;&#30740;&#31350;&#26159;&#22522;&#20110;&#36825;&#20123;&#25285;&#24551;&#36827;&#34892;&#30340;&#65292;&#20294;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;AI&#20869;&#23481;&#26816;&#27979;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#20551;&#35774;&#19968;&#20010;&#25991;&#26412;&#35201;&#20040;&#23436;&#20840;&#30001;&#20154;&#31867;&#32534;&#20889;&#65292;&#35201;&#20040;&#23436;&#20840;&#30001;AI&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;AI&#20869;&#23481;&#26816;&#27979;&#22312;&#19968;&#20010;&#23569;&#26377;&#25506;&#32034;&#20294;&#21364;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#26816;&#27979;&#30340;&#25991;&#26412;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#28151;&#21512;&#25991;&#26412;&#65289;&#21327;&#20316;&#32534;&#20889;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#26816;&#27979;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#20174;&#32473;&#23450;&#30340;&#28151;&#21512;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#31867;&#32534;&#20889;&#20869;&#23481;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#36716;&#25442;&#28857;&#65288;&#36793;&#30028;&#26816;&#27979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary det
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#65292;&#36890;&#36807;&#35270;&#39057;&#30340;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#27450;&#39575;&#26816;&#27979;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.07516</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#65292;&#36890;&#36807;&#35270;&#39057;&#30340;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#27450;&#39575;&#26816;&#27979;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#19968;&#30452;&#26159;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#26816;&#27979;&#27450;&#39575;&#32473;&#36825;&#19968;&#26087;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#20809;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#20351;&#29992;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#21035;&#26159;&#23494;&#27463;&#26681;&#22823;&#23398;&#30340;&#30495;&#23454;&#35797;&#39564;&#25968;&#25454;&#38598;&#21644;&#36808;&#38463;&#23494;&#22823;&#23398;&#30340;&#27450;&#39575;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#35270;&#39057;&#26679;&#26412;&#34987;&#20998;&#25104;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#25163;&#31295;&#30340;&#24103;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#27169;&#24577;&#25237;&#31080;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#19977;&#20010;&#27169;&#22411;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#31532;&#20108;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#38899;&#39057;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;Mel&#39057;&#35889;&#22270;&#19978;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#31532;&#19977;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#25163;&#31295;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#19978;&#30340;Word2Vec&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#19978;&#21462;&#24471;&#30340;&#26368;&#20339;&#32467;&#26524;&#20998;&#21035;&#20026;97&#65285;&#12289;96&#65285;&#12289;9
&lt;/p&gt;
&lt;p&gt;
Automatic Deception Detection has been a hot research topic for a long time, using machine learning and deep learning to automatically detect deception, brings new light to this old field. In this paper, we proposed a voting-based method for automatic deception detection from videos using audio, visual and lexical features. Experiments were done on two datasets, the Real-life trial dataset by Michigan University and the Miami University deception detection dataset. Video samples were split into frames of images, audio, and manuscripts. Our Voting-based Multimodal proposed solution consists of three models. The first model is CNN for detecting deception from images, the second model is Support Vector Machine (SVM) on Mel spectrograms for detecting deception from audio and the third model is Word2Vec on Support Vector Machine (SVM) for detecting deception from manuscripts. Our proposed solution outperforms state of the art. Best results achieved on images, audio and text were 97%, 96%, 9
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#65292;&#21457;&#29616;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.05722</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations. (arXiv:2307.05722v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#65292;&#21457;&#29616;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#34892;&#20026;&#22270;&#30340;&#29702;&#35299;&#28508;&#21147;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34892;&#20026;&#22270;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#29702;&#35299;&#26469;&#25552;&#21319;&#22312;&#32447;&#25307;&#32856;&#20013;&#30340;&#25512;&#33616;&#65292;&#21253;&#25324;&#20419;&#36827;&#38750;&#20998;&#24067;&#24335;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#35821;&#20041;&#34920;&#31034;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#24182;&#25581;&#31034;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#36335;&#24452;&#25552;&#31034;&#26500;&#36896;&#22120;&#65292;&#21033;&#29992;LLM&#25512;&#33616;&#22120;&#39318;&#27425;&#29702;&#35299;&#34892;&#20026;&#22270;&#65292;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#36335;&#24452;&#22686;&#24378;&#27169;&#22359;&#26469;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#24207;&#21015;&#36755;&#20837;&#24341;&#20837;&#30340;&#25552;&#31034;&#20559;&#24046;&#12290;&#36890;&#36807;&#21033;&#29992;&#23558;LM&#30340;&#29305;&#28857;&#24341;&#20837;&#21040;&#34892;&#20026;&#22270;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing tasks, demonstrating their exceptional capabilities in various domains. However, their potential for behavior graph understanding in job recommendations remains largely unexplored. This paper focuses on unveiling the capability of large language models in understanding behavior graphs and leveraging this understanding to enhance recommendations in online recruitment, including the promotion of out-of-distribution (OOD) application. We present a novel framework that harnesses the rich contextual information and semantic representations provided by large language models to analyze behavior graphs and uncover underlying patterns and relationships. Specifically, we propose a meta-path prompt constructor that leverages LLM recommender to understand behavior graphs for the first time and design a corresponding path augmentation module to alleviate the prompt bias introduced by path-based sequence input. By leveragin
&lt;/p&gt;</description></item><item><title>TransDis&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#35780;&#20998;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#21457;&#25955;&#24615;&#24605;&#32500;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;TransDis&#30340;&#35780;&#20998;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#24037;&#35780;&#20998;&#65292;&#20855;&#26377;&#19982;&#20154;&#24037;&#35780;&#20998;&#30456;&#20284;&#30340;&#25928;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.14790</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#22312;&#20013;&#25991;&#21457;&#25955;&#24615;&#24605;&#32500;&#30340;&#33258;&#21160;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#8212;&#8212;TransDis
&lt;/p&gt;
&lt;p&gt;
Automatic Assessment of Divergent Thinking in Chinese Language with TransDis: A Transformer-Based Language Model Approach. (arXiv:2306.14790v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14790
&lt;/p&gt;
&lt;p&gt;
TransDis&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#35780;&#20998;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#21457;&#25955;&#24615;&#24605;&#32500;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;TransDis&#30340;&#35780;&#20998;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#24037;&#35780;&#20998;&#65292;&#20855;&#26377;&#19982;&#20154;&#24037;&#35780;&#20998;&#30456;&#20284;&#30340;&#25928;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21019;&#36896;&#21147;&#35780;&#20272;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#36890;&#36807;&#29983;&#25104;&#35821;&#20041;&#36317;&#31163;&#26469;&#23458;&#35266;&#22320;&#34913;&#37327;&#21019;&#36896;&#24615;&#24605;&#32500;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#38024;&#23545;&#20013;&#25991;&#21019;&#36896;&#24615;&#24605;&#32500;&#35780;&#20272;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TransDis&#65292;&#19968;&#20010;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20998;&#31995;&#32479;&#65292;&#33021;&#22815;&#20026;&#20013;&#25991;&#20013;&#30340;Alternate Uses Task&#65288;AUT&#65289;&#22238;&#31572;&#25552;&#20379;&#26377;&#25928;&#30340;&#29420;&#21019;&#24615;&#65288;&#36136;&#37327;&#65289;&#21644;&#28789;&#27963;&#24615;&#65288;&#22810;&#26679;&#24615;&#65289;&#35780;&#20998;&#12290;&#30740;&#31350;1&#34920;&#26126;&#65292;&#30001;&#19977;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#32452;&#25104;&#30340;&#28508;&#22312;&#27169;&#22411;&#35780;&#20998;&#30340;&#21407;&#21019;&#24615;&#22240;&#32032;&#24378;&#28872;&#39044;&#27979;&#20154;&#24037;&#21407;&#21019;&#24615;&#35780;&#20998;&#65292;&#32780;&#27169;&#22411;&#35780;&#20998;&#30340;&#28789;&#27963;&#24615;&#19982;&#20154;&#24037;&#28789;&#27963;&#24615;&#35780;&#20998;&#20063;&#24378;&#30456;&#20851;&#12290;&#20934;&#21017;&#25928;&#24230;&#20998;&#26512;&#34920;&#26126;&#65292;&#27169;&#22411;&#35780;&#20998;&#30340;&#21407;&#21019;&#24615;&#21644;&#28789;&#27963;&#24615;&#19982;&#20854;&#20182;&#21019;&#36896;&#21147;&#27979;&#37327;&#27491;&#30456;&#20851;&#65292;&#35777;&#26126;&#19982;&#20154;&#24037;&#35780;&#20998;&#20855;&#26377;&#30456;&#20284;&#30340;&#25928;&#24230;&#12290;&#30740;&#31350;2&#21644;3&#34920;&#26126;&#65292;TransDis&#33021;&#22815;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#35780;&#20272;&#20013;&#25991;&#21457;&#25955;&#24615;&#24605;&#32500;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been increasingly popular for automatic creativity assessment, generating semantic distances to objectively measure the quality of creative ideas. However, there is currently a lack of an automatic assessment system for evaluating creative ideas in the Chinese language. To address this gap, we developed TransDis, a scoring system using transformer-based language models, capable of providing valid originality (quality) and flexibility (variety) scores for Alternative Uses Task (AUT) responses in Chinese. Study 1 demonstrated that the latent model-rated originality factor, comprised of three transformer-based models, strongly predicted human originality ratings, and the model-rated flexibility strongly correlated with human flexibility ratings as well. Criterion validity analyses indicated that model-rated originality and flexibility positively correlated to other creativity measures, demonstrating similar validity to human ratings. Study 2 &amp; 3 showed that TransDis e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35282;&#24230;&#34701;&#21512;&#32467;&#26500;&#25628;&#32034;&#30340;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#65292;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#36830;&#32493;&#30340;&#35282;&#24230;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.09361</link><description>&lt;p&gt;
MFAS: &#22522;&#20110;&#22810;&#35282;&#24230;&#34701;&#21512;&#32467;&#26500;&#25628;&#32034;&#30340;&#24773;&#24863;&#35782;&#21035;&#65292;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
MFAS: Emotion Recognition through Multiple Perspectives Fusion Architecture Search Emulating Human Cognition. (arXiv:2306.09361v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09361
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35282;&#24230;&#34701;&#21512;&#32467;&#26500;&#25628;&#32034;&#30340;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#65292;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#36830;&#32493;&#30340;&#35282;&#24230;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26088;&#22312;&#35782;&#21035;&#21644;&#20998;&#26512;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;&#23436;&#32654;&#30340;&#24773;&#24863;&#35782;&#21035;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#21892;&#21508;&#31181;&#20154;&#26426;&#20132;&#20114;&#20219;&#21153;&#12290;&#21463;&#20154;&#31867;&#29702;&#35299;&#24773;&#24863;&#30340;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#37327;&#21270;&#24314;&#27169;&#30456;&#27604;&#65292;&#20174;&#36830;&#32493;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#38899;&#20869;&#23481;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#29702;&#35299;&#65292;&#33021;&#22815;&#20351;&#27169;&#22411;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#26681;&#25454;&#35821;&#38899;&#20013;&#23384;&#22312;&#30340;&#26576;&#20123;&#32447;&#32034;&#35843;&#25972;&#24773;&#24863;&#21333;&#35789;&#30340;&#25991;&#26412;&#35821;&#20041;&#30340;&#24863;&#30693;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#25628;&#32034;&#31354;&#38388;&#24182;&#25628;&#32034;&#20004;&#31181;&#20449;&#24687;&#30340;&#26368;&#20339;&#34701;&#21512;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#35843;&#25972;&#24863;&#30693;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Multiple perspectives Fusion Architecture Search(MFAS)&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition aims to identify and analyze emotional states in target speech similar to humans. Perfect emotion recognition can greatly benefit a wide range of human-machine interaction tasks. Inspired by the human process of understanding emotions, we demonstrate that compared to quantized modeling, understanding speech content from a continuous perspective, akin to human-like comprehension, enables the model to capture more comprehensive emotional information. Additionally, considering that humans adjust their perception of emotional words in textual semantic based on certain cues present in speech, we design a novel search space and search for the optimal fusion strategy for the two types of information. Experimental results further validate the significance of this perception adjustment. Building on these observations, we propose a novel framework called Multiple perspectives Fusion Architecture Search (MFAS). Specifically, we utilize continuous-based knowledge to capt
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05685</link><description>&lt;p&gt;
&#29992;MT-Bench&#21644;Chatbot Arena&#35780;&#20272;&#20197;LLM&#20026;&#22522;&#30784;&#30340;&#32842;&#22825;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05685
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32842;&#22825;&#21161;&#25163;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#32780;&#29616;&#26377;&#30340;&#22522;&#20934;&#26080;&#27861;&#34913;&#37327;&#20154;&#31867;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#22312;&#26356;&#21152;&#24320;&#25918;&#30340;&#38382;&#39064;&#19978;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#30340;&#20351;&#29992;&#21644;&#23616;&#38480;&#24615;&#65292;&#22914;&#20301;&#32622;&#21644;&#20887;&#20313;&#20559;&#35265;&#20197;&#21450;&#26377;&#38480;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#26469;&#36801;&#31227;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#65288;&#19968;&#20010;&#22810;&#36718;&#38382;&#31572;&#38598;&#21644;&#19968;&#20010;&#20247;&#21253;&#31454;&#25216;&#24179;&#21488;&#65289;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#24378;&#22823;LLM&#35780;&#21028;&#21592;&#21487;&#20197;&#24456;&#22909;&#22320;&#21305;&#37197;&#21463;&#25511;&#21644;&#20247;&#21253;&#20154;&#31867;&#20559;&#22909;&#65292;&#36798;&#21040;&#20102;80&#65285;&#20197;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#19982;&#20154;&#31867;&#19968;&#33268;&#24615;&#27700;&#24179;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#36924;&#36817;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#24335;&#65292;&#32780;&#36825;&#20123;&#20559;&#22909;&#26159;&#38750;&#24120;&#26114;&#36149;&#33719;&#21462;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, such as position and verbosity biases and limited reasoning ability, and propose solutions to migrate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15408</link><description>&lt;p&gt;
&#20174;&#29702;&#35770;&#35282;&#24230;&#25581;&#31034;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#22885;&#31192;
&lt;/p&gt;
&lt;p&gt;
Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;"&#24605;&#32500;&#38142;"&#25552;&#31034;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#25968;&#23398;&#25110;&#25512;&#29702;&#30340;&#22797;&#26434;&#20219;&#21153;&#20013;&#12290;&#23613;&#31649;&#33719;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#26426;&#21046;&#20197;&#21450;&#23427;&#22914;&#20309;&#37322;&#25918;LLMs&#30340;&#28508;&#21147;&#20173;&#28982;&#26159;&#31070;&#31192;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#20219;&#20309;&#26377;&#38480;&#28145;&#24230;&#30340;Transformer&#37117;&#19981;&#33021;&#30452;&#25509;&#36755;&#20986;&#27491;&#30830;&#30340;&#22522;&#26412;&#31639;&#26415;/&#26041;&#31243;&#20219;&#21153;&#30340;&#31572;&#26696;&#65292;&#38500;&#38750;&#27169;&#22411;&#22823;&#23567;&#38543;&#30528;&#36755;&#20837;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#36229;&#22810;&#39033;&#24335;&#22686;&#38271;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#65292;&#22823;&#23567;&#24658;&#23450;&#30340;&#33258;&#22238;&#24402;Transformer&#36275;&#20197;&#36890;&#36807;&#20351;&#29992;&#24120;&#29992;&#30340;&#25968;&#23398;&#35821;&#35328;&#24418;&#24335;&#29983;&#25104;&#8220;&#24605;&#32500;&#38142;&#8221;&#25512;&#23548;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29616;&#26377;&#30340;&#22810;&#22836;&#35821;&#35328;&#27169;&#22411;&#26816;&#26597;&#28857;&#21319;&#32423;&#20026;&#20855;&#26377;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;MQA&#65289;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#32676;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;GQA&#65289;&#26469;&#35299;&#20915;MQA&#21487;&#33021;&#23548;&#33268;&#30340;&#36136;&#37327;&#19979;&#38477;&#38382;&#39064;&#12290;&#36890;&#36807;&#21319;&#32423;&#21518;&#30340;GQA&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#36136;&#37327;&#65292;&#24182;&#20855;&#22791;&#19982;MQA&#30456;&#24403;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13245</link><description>&lt;p&gt;
GQA:&#20174;&#22810;&#22836;&#26816;&#26597;&#28857;&#35757;&#32451;&#24191;&#20041;&#22810;&#26597;&#35810;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. (arXiv:2305.13245v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29616;&#26377;&#30340;&#22810;&#22836;&#35821;&#35328;&#27169;&#22411;&#26816;&#26597;&#28857;&#21319;&#32423;&#20026;&#20855;&#26377;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;MQA&#65289;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#32676;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;GQA&#65289;&#26469;&#35299;&#20915;MQA&#21487;&#33021;&#23548;&#33268;&#30340;&#36136;&#37327;&#19979;&#38477;&#38382;&#39064;&#12290;&#36890;&#36807;&#21319;&#32423;&#21518;&#30340;GQA&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#36136;&#37327;&#65292;&#24182;&#20855;&#22791;&#19982;MQA&#30456;&#24403;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;MQA&#65289;&#20165;&#20351;&#29992;&#19968;&#20010;&#38190;&#20540;&#22836;&#65292;&#22823;&#22823;&#21152;&#24555;&#20102;&#35299;&#30721;&#22120;&#25512;&#29702;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;MQA&#21487;&#33021;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#65292;&#24182;&#19988;&#20026;&#20102;&#26356;&#24555;&#22320;&#25512;&#29702;&#32780;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#27169;&#22411;&#21487;&#33021;&#19981;&#26159;&#29702;&#24819;&#30340;&#12290;&#25105;&#20204;&#65288;1&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#21033;&#29992;&#21407;&#22987;&#39044;&#35757;&#32451;&#35745;&#31639;&#37327;&#30340;5&#65285;&#65292;&#23558;&#29616;&#26377;&#30340;&#22810;&#22836;&#35821;&#35328;&#27169;&#22411;&#26816;&#26597;&#28857;&#21319;&#32423;&#20026;&#20855;&#26377;MQA&#30340;&#27169;&#22411;&#65292;&#24182;&#65288;2&#65289;&#24341;&#20837;&#20102;&#32676;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;GQA&#65289;&#65292;&#23427;&#26159;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#30340;&#24191;&#20041;&#24418;&#24335;&#65292;&#20351;&#29992;&#20013;&#38388;&#25968;&#37327;&#30340;&#38190;&#20540;&#22836;&#65288;&#22810;&#20110;&#19968;&#20010;&#65292;&#23569;&#20110;&#26597;&#35810;&#22836;&#30340;&#25968;&#37327;&#65289;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#32463;&#36807;&#21319;&#32423;&#30340;GQA&#23454;&#29616;&#20102;&#19982;&#22810;&#22836;&#27880;&#24847;&#21147;&#30456;&#24403;&#30340;&#36895;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#25509;&#36817;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;SeeTRUE&#35780;&#20272;&#38598;&#21644;&#20004;&#31181;&#33258;&#21160;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#40784;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#22312;&#22797;&#26434;&#32452;&#21512;&#25110;&#38750;&#33258;&#28982;&#22270;&#20687;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.10400</link><description>&lt;p&gt;
&#20320;&#30475;&#21040;&#30340;&#23601;&#26159;&#20320;&#35835;&#21040;&#30340;? &#25913;&#36827;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
What You See is What You Read? Improving Text-Image Alignment Evaluation. (arXiv:2305.10400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;SeeTRUE&#35780;&#20272;&#38598;&#21644;&#20004;&#31181;&#33258;&#21160;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#40784;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#22312;&#22797;&#26434;&#32452;&#21512;&#25110;&#38750;&#33258;&#28982;&#22270;&#20687;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30830;&#23450;&#25991;&#26412;&#21644;&#30456;&#24212;&#30340;&#22270;&#20687;&#26159;&#21542;&#35821;&#20041;&#19978;&#23545;&#40784;&#26159;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#39033;&#37325;&#35201;&#25361;&#25112;&#65292;&#24212;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#25991;&#26412;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;SeeTRUE&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#38598;&#65292;&#28085;&#30422;&#20102;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20855;&#26377;&#20154;&#31867;&#30340;&#21028;&#26029;&#65292;&#21028;&#26029;&#32473;&#23450;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#26159;&#21542;&#35821;&#20041;&#19978;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20004;&#31181;&#33258;&#21160;&#30830;&#23450;&#23545;&#40784;&#30340;&#26041;&#27861;&#65306;&#31532;&#19968;&#31181;&#26159;&#22522;&#20110;&#38382;&#39064;&#29983;&#25104;&#21644;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#31649;&#36947;&#65292;&#31532;&#20108;&#31181;&#26159;&#36890;&#36807;&#24494;&#35843;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#20219;&#21153;&#20013;&#22343;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#28041;&#21450;&#22797;&#26434;&#32452;&#21512;&#25110;&#38750;&#33258;&#28982;&#22270;&#20687;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#36825;&#28608;&#21169;&#20102;&#26410;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically determining whether a text and a corresponding image are semantically aligned is a significant challenge for vision-language models, with applications in generative text-to-image and image-to-text tasks. In this work, we study methods for automatic text-image alignment evaluation. We first introduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets from both text-to-image and image-to-text generation tasks, with human judgements for whether a given text-image pair is semantically aligned. We then describe two automatic methods to determine alignment: the first involving a pipeline based on question generation and visual question answering models, and the second employing an end-to-end classification approach by finetuning multimodal pretrained models. Both methods surpass prior approaches in various text-image alignment tasks, with significant improvements in challenging cases that involve complex composition or unnatural images. Finally, we demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#35758;&#23558;&#21452;&#37325;&#20351;&#29992;&#30740;&#31350;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#24212;&#29992;&#24314;&#35758;&#65292;&#20197;&#21152;&#24378;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#24182;&#22686;&#24378;&#31038;&#20250;&#23545;&#20854;&#24433;&#21709;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.07882</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#65306;&#23454;&#29616;&#8220;&#21463;&#20851;&#27880;&#30340;&#21452;&#37325;&#20351;&#29992;&#30740;&#31350;&#8221;&#26694;&#26550;&#30340;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Dual Use Concerns of Generative AI and Large Language Models. (arXiv:2305.07882v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#35758;&#23558;&#21452;&#37325;&#20351;&#29992;&#30740;&#31350;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#24212;&#29992;&#24314;&#35758;&#65292;&#20197;&#21152;&#24378;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#24182;&#22686;&#24378;&#31038;&#20250;&#23545;&#20854;&#24433;&#21709;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#35758;&#23558;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#30340;&#8220;&#21463;&#20851;&#27880;&#30340;&#21452;&#37325;&#20351;&#29992;&#30740;&#31350;&#8221;&#65288;DURC&#65289;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#20855;&#20307;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#22312;&#29983;&#29289;&#30740;&#31350;&#39046;&#22495;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#30340;&#35777;&#26126;&#65292;&#25105;&#20204;&#30456;&#20449;DURC&#26631;&#20934;&#21487;&#20197;&#20026;LLM&#37325;&#26032;&#23450;&#20041;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#12290;&#22312;&#20351;&#29992;DURC&#26694;&#26550;&#26102;&#38656;&#35201;&#26435;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24378;&#35843;&#20854;&#22312;&#25552;&#39640;&#31038;&#20250;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#30340;&#35748;&#35782;&#26041;&#38754;&#30340;&#37325;&#35201;&#25919;&#27835;&#20316;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#31995;&#21015;&#20855;&#20307;&#30340;&#24314;&#35758;&#65292;&#20197;&#23558;DURC&#26041;&#27861;&#24212;&#29992;&#20110;LLM&#30340;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We suggest the implementation of the Dual Use Research of Concern (DURC) framework, originally designed for life sciences, to the domain of generative AI, with a specific focus on Large Language Models (LLMs). With its demonstrated advantages and drawbacks in biological research, we believe the DURC criteria can be effectively redefined for LLMs, potentially contributing to improved AI governance. Acknowledging the balance that must be struck when employing the DURC framework, we highlight its crucial political role in enhancing societal awareness of the impact of generative AI. As a final point, we offer a series of specific recommendations for applying the DURC approach to LLM research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;Selfmem&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#33258;&#25105;&#35760;&#24518;&#27744;&#24182;&#37319;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#65292;&#20351;&#26816;&#32034;&#26356;&#21152;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02437</link><description>&lt;p&gt;
&#36816;&#29992;&#33258;&#25105;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory. (arXiv:2305.02437v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;Selfmem&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#33258;&#25105;&#35760;&#24518;&#27744;&#24182;&#37319;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#65292;&#20351;&#26816;&#32034;&#26356;&#21152;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#36739;&#20110;&#20256;&#32479;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#36845;&#20195;&#20154;&#31867;&#32534;&#20889;&#30340;&#21442;&#32771;&#24211;&#65292;&#24182;&#20174;&#20013;&#26816;&#32034;&#20986;&#30456;&#24212;&#30340;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#26356;&#20248;&#36136;&#30340;&#25991;&#26412;&#12290;&#20294;&#24403;&#21069;&#25991;&#29486;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26816;&#32034;&#21040;&#30340;&#35760;&#24518;&#26469;&#33258;&#20110;&#22266;&#23450;&#30340;&#35821;&#26009;&#24211;&#65292;&#20854;&#36136;&#37327;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#21487;&#33021;&#20250;&#38480;&#21046;&#35760;&#24518;&#22686;&#24378;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Selfmem&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36845;&#20195;&#22320;&#37319;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#22120;&#33258;&#36523;&#20197;&#29983;&#25104;&#26080;&#38480;&#21046;&#30340;&#33258;&#25105;&#35760;&#24518;&#27744;&#65292;&#24182;&#20351;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#20026;&#19979;&#19968;&#36718;&#29983;&#25104;&#36873;&#25321;&#19968;&#20010;&#29983;&#25104;&#30340;&#35760;&#24518;&#12290;&#30456;&#32467;&#21512;&#65292;&#36825;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#25552;&#20986;&#20102;&#36816;&#29992;&#33258;&#25105;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation~(we define this as primal problem), previous works mainly focus on how to retrieve better memory. However, one fundamental limitation exists for current literature: the memory is retrieved from a fixed corpus and is bounded by the quality of the corpus. Due to the finite retrieval space, bounded memory would greatly limit the potential of the memory-augmented generation model. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a framework called Selfmem, which iteratively adopts a retrieval-augmented generator itself to generate an unbounded memory pool and uses a memory selector to pick one generated memory for the next generation round. By combining the primal and dual problem, a retrieval-augmented ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;q2d&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#21547;&#26377;&#26597;&#35810;&#30340;&#23545;&#35805;&#26469;&#25945;&#23548;&#27169;&#22411;&#22914;&#20309;&#21457;&#20986;&#25628;&#32034;&#26597;&#35810;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#24615;&#33021;&#25509;&#36817;&#20351;&#29992;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#32780;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#25511;&#21046;&#21644;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14318</link><description>&lt;p&gt;
q2d&#65306;&#23558;&#38382;&#39064;&#36716;&#25442;&#20026;&#23545;&#35805;&#65292;&#25945;&#23548;&#27169;&#22411;&#22914;&#20309;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
q2d: Turning Questions into Dialogs to Teach Models How to Search. (arXiv:2304.14318v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;q2d&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#21547;&#26377;&#26597;&#35810;&#30340;&#23545;&#35805;&#26469;&#25945;&#23548;&#27169;&#22411;&#22914;&#20309;&#21457;&#20986;&#25628;&#32034;&#26597;&#35810;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#24615;&#33021;&#25509;&#36817;&#20351;&#29992;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#32780;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#25511;&#21046;&#21644;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23545;&#35805;&#35821;&#35328;&#27169;&#22411;&#26377;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#33021;&#21147;&#65292;&#21363;&#33021;&#22815;&#29420;&#31435;&#22320;&#25628;&#32034;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#30830;&#23450;&#32473;&#23450;&#23545;&#35805;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#29992;&#20110;&#25945;&#23548;&#27169;&#22411;&#22914;&#20309;&#21457;&#20986;&#25628;&#32034;&#26597;&#35810;&#30340;&#35757;&#32451;&#25968;&#25454;&#26159;&#32791;&#36153;&#26102;&#38388;&#21644;&#36164;&#28304;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;q2d&#65306;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#20174;&#38382;&#39064;&#20013;&#33719;&#21462;&#20449;&#24687;&#30340;&#23545;&#35805;&#30340;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#25552;&#20379;&#32473;&#19968;&#20010;&#22823;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PaLM&#65289;&#26469;&#21019;&#24314;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23545;&#35805;&#29256;&#26412;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#25913;&#36827;&#19982;&#22806;&#37096;&#25628;&#32034;API&#36890;&#20449;&#20197;&#30830;&#23450;&#23545;&#35805;&#21709;&#24212;&#30340;&#26597;&#35810;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#20154;&#31867;&#32534;&#20889;&#30340;&#24102;&#26377;&#25628;&#32034;&#26597;&#35810;&#30340;&#23545;&#35805;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#35805;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#25511;&#21046;&#21644;&#35268;&#27169;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65306;&#65288;1&#65289;&#38024;&#23545;QReCC&#25968;&#25454;&#38598;&#30340;&#26597;&#35810;&#29983;&#25104;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#20174;MNLI&#20013;&#36827;&#34892;&#20256;&#36755;&#23398;&#20064;&#30340;&#27169;&#22411;&#24615;&#33021;&#30340;90%--97%&#65292;&#32780;&#20351;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#26597;&#35810;&#29983;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;91%--96%&#30340;&#24615;&#33021;&#12290;&#65288;2&#65289;&#38024;&#23545;QUAC&#21644;CoQA&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#21709;&#24212;&#36873;&#25321;&#65292;&#20351;&#29992;&#25105;&#20204;&#33258;&#21160;&#29983;&#25104;&#30340;&#23545;&#35805;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#20165;&#27604;&#20351;&#29992;&#24102;&#26377;&#25628;&#32034;&#26597;&#35810;&#30340;&#20154;&#31867;&#29983;&#25104;&#30340;&#23545;&#35805;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#20302;&#20102;0.8-2.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the exciting capabilities of recent language models for dialog is their ability to independently search for relevant information to ground a given dialog response. However, obtaining training data to teach models how to issue search queries is time and resource consuming. In this work, we propose q2d: an automatic data generation pipeline that generates information-seeking dialogs from questions. We prompt a large language model (PaLM) to create conversational versions of question answering datasets, and use it to improve query generation models that communicate with external search APIs to ground dialog responses. Unlike previous approaches which relied on human written dialogs with search queries, our method allows to automatically generate query-based grounded dialogs with better control and scale. Our experiments demonstrate that: (1) For query generation on the QReCC dataset, models trained on our synthetically-generated data achieve 90%--97% of the performance of models tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.01046</link><description>&lt;p&gt;
&#8220;Polytuplet Loss: &#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#8221;
&lt;/p&gt;
&lt;p&gt;
Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#23398;&#26657;&#25945;&#32946;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#20204;&#23558;&#21463;&#21040;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32771;&#39564;&#12290;&#23398;&#29983;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#23436;&#25104;&#27492;&#31867;&#32771;&#35797;&#65292;&#20854;&#20013;&#26377;&#20123;&#34987;&#35748;&#20026;&#26159;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#30340;&#12290;&#36825;&#26679;&#19968;&#31181;&#31574;&#30053;&#28041;&#21450;&#24378;&#35843;&#30456;&#23545;&#20934;&#30830;&#24615;&#32780;&#38750;&#32477;&#23545;&#20934;&#30830;&#24615;&#65292;&#29702;&#35770;&#19978;&#21487;&#20197;&#22312;&#19981;&#23436;&#20840;&#25484;&#25569;&#35299;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#35757;&#32451;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20197;&#35299;&#20915;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;ReClor&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36923;&#36753;&#25512;&#29702;&#25216;&#33021;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Polytuplet Loss&#20989;&#25968;&#65292;&#26159;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#25193;&#23637;&#65292;&#20197;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#32780;&#38750;&#23398;&#20064;&#32477;&#23545;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20381;&#36182;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#30001;&#24773;&#24863;&#24815;&#24615;&#21644;&#24863;&#26579;&#39537;&#21160;&#65288;EmotionIC&#65289;&#65292;&#29992;&#20110;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#32423;&#21035;&#19978;&#36827;&#34892;&#20250;&#35805;&#24773;&#24863;&#35782;&#21035;&#12290;&#35774;&#35745;&#20102;&#22810;&#39033;&#20855;&#20307;&#26041;&#27861;&#65292;&#21253;&#25324;&#36523;&#20221;&#25513;&#30721;&#22810;&#22836;&#27880;&#24847;&#65288;IM-MHA&#65289;&#21644;&#22522;&#20110;&#23545;&#35805;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;(DialogGRU)&#65292;&#20197;&#25235;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11117</link><description>&lt;p&gt;
EmotionIC&#65306;&#22522;&#20110;&#24773;&#24863;&#24815;&#24615;&#21644;&#24863;&#26579;&#30340;&#20381;&#36182;&#24314;&#27169;&#21487;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling for Emotion Recognition in Conversation. (arXiv:2303.11117v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20381;&#36182;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#30001;&#24773;&#24863;&#24815;&#24615;&#21644;&#24863;&#26579;&#39537;&#21160;&#65288;EmotionIC&#65289;&#65292;&#29992;&#20110;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#32423;&#21035;&#19978;&#36827;&#34892;&#20250;&#35805;&#24773;&#24863;&#35782;&#21035;&#12290;&#35774;&#35745;&#20102;&#22810;&#39033;&#20855;&#20307;&#26041;&#27861;&#65292;&#21253;&#25324;&#36523;&#20221;&#25513;&#30721;&#22810;&#22836;&#27880;&#24847;&#65288;IM-MHA&#65289;&#21644;&#22522;&#20110;&#23545;&#35805;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;(DialogGRU)&#65292;&#20197;&#25235;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#20154;&#26426;&#30028;&#38754;&#25216;&#26415;&#30340;&#36827;&#27493;&#21644;&#23454;&#26045;&#65292;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#24314;&#27169;&#26041;&#27861;&#22312;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#19979;&#25991;&#20381;&#36182;&#26041;&#38754;&#20002;&#22833;&#20102;&#20381;&#36182;&#20449;&#24687;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#32423;&#21035;&#19981;&#32771;&#34385;&#19978;&#19979;&#25991;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20381;&#36182;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#30001;&#24773;&#24863;&#24815;&#24615;&#21644;&#24863;&#26579;&#39537;&#21160;&#65288;EmotionIC&#65289;&#65292;&#29992;&#20110;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#32423;&#21035;&#19978;&#36827;&#34892;&#20250;&#35805;&#24773;&#24863;&#35782;&#21035;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#32423;&#21035;&#65292;&#25105;&#20204;&#35774;&#35745;&#30340;&#36523;&#20221;&#25513;&#30721;&#22810;&#22836;&#27880;&#24847;&#65288;IM-MHA&#65289;&#25429;&#25417;&#23545;&#35805;&#20013;&#22522;&#20110;&#36523;&#20221;&#30340;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#65292;&#20197;&#21253;&#21547;&#19981;&#21516;&#21442;&#19982;&#32773;&#30340;&#19981;&#21516;&#24433;&#21709;&#26500;&#24314;&#20840;&#23616;&#24773;&#24863;&#27675;&#22260;&#65292;&#32780;&#35774;&#35745;&#30340;&#22522;&#20110;&#23545;&#35805;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;(DialogGRU)&#21017;&#32858;&#21512;&#20102;&#20108;&#20803;&#23545;&#35805;&#30340;&#24773;&#24863;&#20542;&#21521;&#65292;&#24182;&#24212;&#29992;&#20110;&#20998;&#31867;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) has attracted growing attention in recent years as a result of the advancement and implementation of human-computer interface technologies. However, previous approaches to modeling global and local context dependencies lost the diversity of dependency information and do not take the context dependency into account at the classification level. In this paper, we propose a novel approach to dependency modeling driven by Emotional Inertia and Contagion (EmotionIC) for conversational emotion recognition at the feature extraction and classification levels. At the feature extraction level, our designed Identity Masked Multi-head Attention (IM-MHA) captures the identity-based long-distant context in the dialogue to contain the diverse influence of different participants and construct the global emotional atmosphere, while the devised Dialogue-based Gate Recurrent Unit (DialogGRU) that aggregates the emotional tendencies of dyadic dialogue is applied to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;GPT-3&#21644;GPT-3.5&#31995;&#21015;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#21457;&#23637;&#36235;&#21183;&#65292;&#24182;&#21457;&#29616;&#26368;&#26032;&#30340;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#21069;&#19968;&#20195;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2303.10420</link><description>&lt;p&gt;
GPT-3&#21644;GPT-3.5&#31995;&#21015;&#27169;&#22411;&#30340;&#20840;&#38754;&#33021;&#21147;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models. (arXiv:2303.10420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;GPT-3&#21644;GPT-3.5&#31995;&#21015;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#21457;&#23637;&#36235;&#21183;&#65292;&#24182;&#21457;&#29616;&#26368;&#26032;&#30340;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#21069;&#19968;&#20195;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT&#31995;&#21015;&#27169;&#22411;&#65292;&#22914;GPT-3&#12289;CodeX&#12289;InstructGPT&#12289;ChatGPT&#31561;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#25506;&#35752;&#20102;GPT&#31995;&#21015;&#27169;&#22411;&#19982;&#31934;&#35843;&#27169;&#22411;&#22312;&#33021;&#21147;&#19978;&#30340;&#24046;&#24322;&#65292;&#20294;&#23545;&#20110;GPT&#31995;&#21015;&#27169;&#22411;&#30340;&#33021;&#21147;&#38543;&#26102;&#38388;&#28436;&#21270;&#30340;&#30740;&#31350;&#21364;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20026;&#20102;&#20840;&#38754;&#20998;&#26512;GPT&#31995;&#21015;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#20845;&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#65292;&#21253;&#25324;&#20004;&#20010;GPT-3&#31995;&#21015;&#27169;&#22411;&#65288;&#21363;davinci&#21644;text-davinci-001&#65289;&#21644;&#22235;&#20010;GPT-3.5&#31995;&#21015;&#27169;&#22411;&#65288;&#21363;code-davinci-002&#12289;text-davinci-002&#12289;text-davinci-003&#21644;gpt-3.5-turbo&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;21&#20010;&#25968;&#25454;&#38598;&#22312;&#20061;&#20010;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#19978;&#35780;&#20272;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#27599;&#20010;&#20219;&#21153;&#20013;&#19981;&#21516;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GPT&#31995;&#21015;&#27169;&#22411;&#30340;&#25972;&#20307;&#33021;&#21147;&#32487;&#32493;&#38543;&#26102;&#38388;&#28436;&#21270;&#65292;&#26368;&#26032;&#30340;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#21069;&#19968;&#20195;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on, have gained considerable attention due to their exceptional natural language processing capabilities. However, despite the abundance of research on the difference in capabilities between GPT series models and fine-tuned models, there has been limited attention given to the evolution of GPT series models' capabilities over time. To conduct a comprehensive analysis of the capabilities of GPT series models, we select six representative models, comprising two GPT-3 series models (i.e., davinci and text-davinci-001) and four GPT-3.5 series models (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and gpt-3.5-turbo). We evaluate their performance on nine natural language understanding (NLU) tasks using 21 datasets. In particular, we compare the performance and robustness of different models for each task under zero-shot and few-shot scenarios. Our extensive experiments reveal that the overall ability of GPT s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#30340;&#25991;&#26412;&#36716;SQL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#19968;&#31995;&#21015;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;SQL&#26597;&#35810;&#12289;&#29702;&#35299;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#21450;&#21306;&#20998;&#26377;&#26080;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.07695</link><description>&lt;p&gt;
EHRSQL&#65306;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#30340;&#23454;&#29992;&#25991;&#26412;&#36716;SQL&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#30340;&#25991;&#26412;&#36716;SQL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#19968;&#31995;&#21015;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;SQL&#26597;&#35810;&#12289;&#29702;&#35299;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#21450;&#21306;&#20998;&#26377;&#26080;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#12290;&#23545;&#35805;&#26159;&#30001;222&#20010;&#21307;&#38498;&#24037;&#20316;&#20154;&#21592;&#21253;&#25324;&#21307;&#29983;&#12289;&#25252;&#22763;&#12289;&#20445;&#38505;&#23457;&#26597;&#21644;&#20581;&#24247;&#26723;&#26696;&#22242;&#38431;&#31561;&#25163;&#26426;&#32780;&#26469;&#12290;&#20026;&#20102;&#26500;&#24314;&#20851;&#20110;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#30340;QA&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;&#19968;&#25152;&#22823;&#23398;&#21307;&#38498;&#36827;&#34892;&#20102;&#19968;&#27425;&#27665;&#35843;&#24182;&#21046;&#20316;&#20102;&#27169;&#26495;&#35805;&#26415;&#20197;&#21019;&#24314;&#31181;&#23376;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25163;&#21160;&#23558;&#23427;&#20204;&#38142;&#25509;&#21040;&#20004;&#20010;&#24320;&#28304;&#30340;EHR&#25968;&#25454;&#24211;&#65288;MIMIC-III&#21644;eICU&#65289;&#20013;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20102;&#26469;&#33258;&#27665;&#24847;&#35843;&#26597;&#30340;&#21508;&#31181;&#26102;&#38388;&#34920;&#36798;&#24335;&#21644;&#26410;&#33021;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#27169;&#22411;&#38656;&#35201; 1&#65289;&#29983;&#25104;&#21453;&#26144;&#21307;&#38498;&#20013;&#21508;&#31181;&#38656;&#27714;&#30340;SQL&#26597;&#35810;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#26816;&#32034;&#21644;&#22797;&#26434;&#30340;&#25805;&#20316;&#65292;&#22914;&#35745;&#31639;&#29983;&#23384;&#29575;&#65292;2&#65289;&#29702;&#35299;&#21508;&#31181;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#22238;&#31572;&#19982;&#26102;&#38388;&#25935;&#24863;&#30340;&#21307;&#30103;&#38382;&#39064;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;3&#65289;&#26681;&#25454;&#39044;&#27979;&#21306;&#20998;&#32473;&#23450;&#38382;&#39064;&#26159;&#21487;&#22238;&#31572;&#36824;&#26159;&#19981;&#21487;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new text-to-SQL dataset for electronic health records (EHRs). The utterances were collected from 222 hospital staff, including physicians, nurses, insurance review and health records teams, and more. To construct the QA dataset on structured EHR data, we conducted a poll at a university hospital and templatized the responses to create seed questions. Then, we manually linked them to two open-source EHR databases, MIMIC-III and eICU, and included them with various time expressions and held-out unanswerable questions in the dataset, which were all collected from the poll. Our dataset poses a unique set of challenges: the model needs to 1) generate SQL queries that reflect a wide range of needs in the hospital, including simple retrieval and complex operations such as calculating survival rate, 2) understand various time expressions to answer time-sensitive questions in healthcare, and 3) distinguish whether a given question is answerable or unanswerable based on the predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2208.07316</link><description>&lt;p&gt;
MENLI: &#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#34987;&#25552;&#20986;&#30340;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26131;&#21463;&#21040;&#23545;&#20449;&#24687;&#27491;&#30830;&#24615;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#37096;&#20998;&#21407;&#22240;&#26159;&#27492;&#31867;&#27169;&#22411;&#26159;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36825;&#31181;&#25351;&#26631;&#26356;&#36866;&#21512;&#24314;&#27169;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26694;&#26550;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#27604;&#26368;&#36817;&#30340;BERT&#22522;&#30784;&#25351;&#26631;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#20248;&#20110;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#20302;&#20110;SOTA MT&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#25351;&#26631;&#19982;&#25105;&#20204;&#30340;NLI&#25351;&#26631;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#26082;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65288;15&#65285;-30&#65285;&#65289;&#65292;&#21448;&#33719;&#24471;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#26356;&#39640;&#30340;&#36136;&#37327;&#25351;&#26631;&#65288;+5&#65285;&#33267;30&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30196;&#21574;&#30417;&#27979;&#21644;&#35786;&#26029;&#12290;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#12289;&#35328;&#35821;&#21644;&#35821;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#21306;&#20998;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#24739;&#32773;&#21644;&#23545;&#29031;&#32452;&#65292;&#20174;&#32780;&#20026;&#30196;&#21574;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2109.01537</link><description>&lt;p&gt;
&#29992;&#20110;&#30196;&#21574;&#30417;&#27979;&#21644;&#35786;&#26029;&#30340;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis. (arXiv:2109.01537v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30196;&#21574;&#30417;&#27979;&#21644;&#35786;&#26029;&#12290;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#12289;&#35328;&#35821;&#21644;&#35821;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#21306;&#20998;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#24739;&#32773;&#21644;&#23545;&#29031;&#32452;&#65292;&#20174;&#32780;&#20026;&#30196;&#21574;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30196;&#21574;&#26159;&#19968;&#31995;&#21015;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#24433;&#21709;&#36234;&#26469;&#36234;&#22810;&#30340;&#20840;&#29699;&#32769;&#40836;&#20154;&#21475;&#30340;&#35760;&#24518;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;&#33258;&#21160;&#21270;&#20998;&#26512;&#35821;&#35328;&#12289;&#35328;&#35821;&#21644;&#35821;&#29992;&#25351;&#26631;&#20316;&#20026;&#35748;&#30693;&#34928;&#36864;&#30340;&#28508;&#22312;&#25351;&#26631;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#33258;&#28982;&#29615;&#22659;&#19979;&#25910;&#38598;&#20102;&#36731;&#24230;&#30196;&#21574;&#24739;&#32773;&#21644;&#37197;&#23545;&#30340;&#24180;&#40836;&#21305;&#37197;&#23545;&#29031;&#32452;&#30340;&#25968;&#25454;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;&#20960;&#20010;&#26376;&#12290;&#22810;&#27169;&#24577;&#25968;&#25454;&#21253;&#25324;&#21475;&#22836;&#20250;&#35805;&#65292;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#34987;&#36716;&#24405;&#65292;&#20197;&#21450;&#36755;&#20837;&#21644;&#20070;&#20889;&#30340;&#24605;&#32771;&#20869;&#23481;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#38750;&#35821;&#35328;&#20449;&#24687;&#65292;&#22914;&#31508;&#30011;&#21644;&#25353;&#38190;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#35813;&#25968;&#25454;&#38598;&#65292;&#24182;&#30528;&#37325;&#35752;&#35770;&#20102;&#20351;&#29992;&#35821;&#38899;&#27169;&#24577;&#30340;&#20219;&#21153;&#12290;&#21518;&#32773;&#28041;&#21450;&#21033;&#29992;&#25968;&#25454;&#30340;&#32437;&#21521;&#29305;&#24615;&#26469;&#21306;&#20998;&#23545;&#29031;&#32452;&#21644;&#30196;&#21574;&#24739;&#32773;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#20250;&#35805;&#38388;&#35821;&#38899;&#30340;&#21464;&#21270;&#22312;&#19981;&#21516;&#30340;&#20250;&#35805;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dementia is a family of neurogenerative conditions affecting memory and cognition in an increasing number of individuals in our globally aging population. Automated analysis of language, speech and paralinguistic indicators have been gaining popularity as potential indicators of cognitive decline. Here we propose a novel longitudinal multi-modal dataset collected from people with mild dementia and age matched controls over a period of several months in a natural setting. The multi-modal data consists of spoken conversations, a subset of which are transcribed, as well as typed and written thoughts and associated extra-linguistic information such as pen strokes and keystrokes. We describe the dataset in detail and proceed to focus on a task using the speech modality. The latter involves distinguishing controls from people with dementia by exploiting the longitudinal nature of the data. Our experiments showed significant differences in how the speech varied from session to session in the 
&lt;/p&gt;</description></item></channel></rss>