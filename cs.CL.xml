<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29702;&#24615;&#20013;&#24515;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#35782;&#21035;&#34394;&#20551;&#21644;&#22240;&#26524;&#20851;&#32852;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#36328;&#25991;&#26723;&#20107;&#20214;&#20851;&#32852;&#28040;&#35299;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01921</link><description>&lt;p&gt;
&#22522;&#20110;&#29702;&#24615;&#20013;&#24515;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#36328;&#25991;&#26723;&#20107;&#20214;&#20851;&#32852;&#28040;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01921
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29702;&#24615;&#20013;&#24515;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#35782;&#21035;&#34394;&#20551;&#21644;&#22240;&#26524;&#20851;&#32852;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#36328;&#25991;&#26723;&#20107;&#20214;&#20851;&#32852;&#28040;&#35299;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#20107;&#20214;&#20851;&#32852;&#28040;&#35299;&#65288;ECR&#65289;&#31995;&#32479;&#22312;&#36328;&#25991;&#26723;&#20013;&#32858;&#31867;&#25351;&#20195;&#24615;&#20107;&#20214;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#36755;&#20837;&#25552;&#21450;&#23545;&#25991;&#26412;&#20013;&#36807;&#20110;&#20381;&#36182;&#8220;&#35302;&#21457;&#35789;&#35789;&#27719;&#21305;&#37197;&#8221;&#36825;&#19968;&#34394;&#20551;&#27169;&#24335;&#12290;&#25105;&#20204;&#21033;&#29992;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#23545;&#22522;&#20934;ECR&#31995;&#32479;&#30340;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#24418;&#24335;&#21270;&#65292;&#26088;&#22312;&#35782;&#21035;ECR&#20219;&#21153;&#20013;&#30340;&#34394;&#20551;&#21644;&#22240;&#26524;&#20851;&#32852;&#65288;&#21363;&#65292;&#29702;&#24615;&#65289;&#12290;&#21033;&#29992;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#29702;&#24615;&#20013;&#24515;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;LLM&#12290;&#35813;&#26041;&#27861;&#19987;&#20026;ECR&#31995;&#32479;&#20013;&#30340;&#20004;&#20004;&#36755;&#20837;&#35774;&#35745;&#65292;&#25105;&#20204;&#22312;&#35302;&#21457;&#35789;&#21644;&#19978;&#19979;&#25991;&#19978;&#36827;&#34892;&#30452;&#25509;&#24178;&#39044;&#65292;&#20197;&#20943;&#36731;&#34394;&#20551;&#20851;&#32852;&#65292;&#24378;&#35843;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01921v1 Announce Type: new  Abstract: Based on Pre-trained Language Models (PLMs), event coreference resolution (ECR) systems have demonstrated outstanding performance in clustering coreferential events across documents. However, the existing system exhibits an excessive reliance on the `triggers lexical matching' spurious pattern in the input mention pair text. We formalize the decision-making process of the baseline ECR system using a Structural Causal Model (SCM), aiming to identify spurious and causal associations (i.e., rationales) within the ECR task. Leveraging the debiasing capability of counterfactual data augmentation, we develop a rationale-centric counterfactual data augmentation method with LLM-in-the-loop. This method is specialized for pairwise input in the ECR system, where we conduct direct interventions on triggers and context to mitigate the spurious association while emphasizing the causation. Our approach achieves state-of-the-art performance on three po
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34701;&#21512;&#23454;&#20307;&#25551;&#36848;&#36827;&#34892;&#23454;&#20307;&#28040;&#27495;&#30340;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.01626</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#23454;&#20307;&#35299;&#30721;&#36827;&#34892;&#23454;&#20307;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Entity Disambiguation via Fusion Entity Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01626
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34701;&#21512;&#23454;&#20307;&#25551;&#36848;&#36827;&#34892;&#23454;&#20307;&#28040;&#27495;&#30340;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#28040;&#27495;&#65288;ED&#65289;&#26159;&#23558;&#27169;&#31946;&#23454;&#20307;&#30340;&#25552;&#21450;&#38142;&#25509;&#21040;&#30693;&#35782;&#24211;&#20013;&#30340;&#25351;&#20195;&#23454;&#20307;&#30340;&#36807;&#31243;&#65292;&#22312;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#22312;&#26631;&#20934;&#21270;&#30340;ZELDA&#22522;&#20934;&#19979;&#23637;&#31034;&#20986;&#27604;&#20998;&#31867;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#24335;&#26041;&#27861;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#19988;&#29983;&#25104;&#25928;&#29575;&#20302;&#19979;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23454;&#20307;&#25551;&#36848;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#32780;&#36825;&#20123;&#25551;&#36848;&#21487;&#33021;&#21253;&#21547;&#21306;&#20998;&#30456;&#20284;&#23454;&#20307;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#65292;&#20197;&#26356;&#35814;&#32454;&#30340;&#23454;&#20307;&#25551;&#36848;&#26469;&#36827;&#34892;&#23454;&#20307;&#28040;&#27495;&#12290;&#32473;&#23450;&#25991;&#26412;&#21644;&#20505;&#36873;&#23454;&#20307;&#65292;&#32534;&#30721;&#22120;&#23398;&#20064;&#25991;&#26412;&#19982;&#27599;&#20010;&#20505;&#36873;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20026;&#27599;&#20010;&#23454;&#20307;&#20505;&#36873;&#20135;&#29983;&#34920;&#31034;&#12290;&#35299;&#30721;&#22120;&#38543;&#21518;&#23558;&#23454;&#20307;&#20505;&#36873;&#30340;&#34920;&#31034;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#36873;&#25321;&#27491;&#30830;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01626v1 Announce Type: new  Abstract: Entity disambiguation (ED), which links the mentions of ambiguous entities to their referent entities in a knowledge base, serves as a core component in entity linking (EL). Existing generative approaches demonstrate improved accuracy compared to classification approaches under the standardized ZELDA benchmark. Nevertheless, generative approaches suffer from the need for large-scale pre-training and inefficient generation. Most importantly, entity descriptions, which could contain crucial information to distinguish similar entities from each other, are often overlooked. We propose an encoder-decoder model to disambiguate entities with more detailed entity descriptions. Given text and candidate entities, the encoder learns interactions between the text and each candidate entity, producing representations for each entity candidate. The decoder then fuses the representations of entity candidates together and selects the correct entity. Our 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CodeBenchGen&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#20219;&#24847;&#20195;&#30721;&#36716;&#21270;&#20026;&#35780;&#20272;&#31034;&#20363;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#20195;&#30721;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;Exec-CSN&#65292;&#23637;&#31034;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00566</link><description>&lt;p&gt;
CodeBenchGen: &#21019;&#24314;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00566
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CodeBenchGen&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#20219;&#24847;&#20195;&#30721;&#36716;&#21270;&#20026;&#35780;&#20272;&#31034;&#20363;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#20195;&#30721;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;Exec-CSN&#65292;&#23637;&#31034;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CodeBenchGen&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#21019;&#24314;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#22522;&#20934;&#65292;&#20165;&#38656;&#35201;&#36731;&#24494;&#30340;&#20154;&#31867;&#25351;&#23548;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#36716;&#21270;&#20026;&#35780;&#20272;&#31034;&#20363;&#65292;&#21253;&#25324;&#29992;&#20110;&#25191;&#34892;&#35780;&#20272;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#21253;&#21547;&#26469;&#33258;CodeSearchNet&#25968;&#25454;&#38598;&#30340;367&#20010;GitHub&#23384;&#20648;&#24211;&#20013;&#30340;&#20195;&#30721;&#20462;&#25913;&#30340;293&#20010;&#24211;&#30340;1,931&#20010;&#20363;&#23376;&#30340;&#25968;&#25454;&#38598;Exec-CSN&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;Exec-CSN&#20013;&#31034;&#20363;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20154;&#31867;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;81.3%&#30340;&#20363;&#23376;&#21487;&#20197;&#34987;&#20154;&#31867;&#35299;&#20915;&#65292;61%&#34987;&#35780;&#20026;&#8220;&#38656;&#35201;&#21162;&#21147;&#35299;&#20915;&#8221;&#12290;&#25105;&#20204;&#23545;&#24320;&#28304;&#21644;&#19987;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#20195;&#30721;&#29983;&#25104;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00566v1 Announce Type: cross  Abstract: To facilitate evaluation of code generation systems across diverse scenarios, we present CodeBenchGen, a framework to create scalable execution-based benchmarks that only requires light guidance from humans. Specifically, we leverage a large language model (LLM) to convert an arbitrary piece of code into an evaluation example, including test cases for execution-based evaluation. We illustrate the usefulness of our framework by creating a dataset, Exec-CSN, which includes 1,931 examples involving 293 libraries revised from code in 367 GitHub repositories taken from the CodeSearchNet dataset. To demonstrate the complexity and solvability of examples in Exec-CSN, we present a human study demonstrating that 81.3% of the examples can be solved by humans and 61% are rated as ``requires effort to solve''. We conduct code generation experiments on open-source and proprietary models and analyze the performance of both humans and models. We will
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36870;&#21521;&#35757;&#32451;&#30340;&#26367;&#20195;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#26041;&#21521;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#20445;&#30041;&#36873;&#23450;&#23376;&#20018;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13799</link><description>&lt;p&gt;
&#36870;&#21521;&#35757;&#32451;&#20197;&#28040;&#38500;&#36870;&#36716;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Reverse Training to Nurse the Reversal Curse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36870;&#21521;&#35757;&#32451;&#30340;&#26367;&#20195;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#26041;&#21521;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#20445;&#30041;&#36873;&#23450;&#23376;&#20018;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23384;&#22312;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#29616;&#35937;&#65306;&#24403;&#35757;&#32451;&#27169;&#22411;&#20197;"A&#20855;&#26377;&#29305;&#24449;B"&#20026;&#22522;&#30784;&#26102;&#65292;&#23427;&#20204;&#26080;&#27861;&#27867;&#21270;&#21040;"B&#26159;A&#30340;&#29305;&#24449;"&#65292;&#36825;&#34987;&#31216;&#20026;&#36870;&#36716;&#35781;&#21650;&#12290;&#21363;&#20351;&#22312;&#20351;&#29992;&#25968;&#19975;&#20159;&#20196;&#29260;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#30001;&#20110;&#40784;&#22827;&#23450;&#24459;&#30340;&#23384;&#22312;&#65292;&#36825;&#20010;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#24847;&#21619;&#30528;&#21363;&#20351;&#25105;&#20204;&#22312;&#25972;&#20010;&#20114;&#32852;&#32593;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#38382;&#39064;&#20173;&#28982;&#20250;&#20986;&#29616;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36870;&#21521;&#35757;&#32451;&#30340;&#26367;&#20195;&#35757;&#32451;&#26041;&#26696;&#65292;&#22312;&#20854;&#20013;&#25152;&#26377;&#21333;&#35789;&#34987;&#20351;&#29992;&#20004;&#27425;&#65292;&#20174;&#32780;&#20351;&#21487;&#29992;&#20196;&#29260;&#25968;&#37327;&#21152;&#20493;&#12290;&#35813;LLM&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#26041;&#21521;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#39072;&#20498;&#35757;&#32451;&#23383;&#31526;&#20018;&#26469;&#39072;&#20498;&#35757;&#32451;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#30041;&#65288;&#21363;&#19981;&#39072;&#20498;&#65289;&#36873;&#23450;&#30340;&#23376;&#20018;&#65292;&#22914;&#23454;&#20307;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#21305;&#37197;&#30340;&#36870;&#21521;&#35757;&#32451;&#27169;&#22411;&#22312;&#26631;&#20934;&#20219;&#21153;&#19978;&#27604;&#26631;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#20248;&#31168;&#65292;&#24182;&#19988;&#35745;&#31639;&#21305;&#37197;&#30340;&#36870;&#21521;&#35757;&#32451;&#27169;&#22411;&#22312;&#36870;&#36716;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36828;&#36828;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13799v1 Announce Type: new  Abstract: Large language models (LLMs) have a surprising failure: when trained on "A has a feature B", they do not generalize to "B is a feature of A", which is termed the Reversal Curse. Even when training with trillions of tokens this issue still appears due to Zipf's law - hence even if we train on the entire internet. This work proposes an alternative training scheme, called reverse training, whereby all words are used twice, doubling the amount of available tokens. The LLM is trained in both forward and reverse directions by reversing the training strings while preserving (i.e., not reversing) chosen substrings, such as entities. We show that data-matched reverse-trained models provide superior performance to standard models on standard tasks, and compute-matched reverse-trained models provide far superior performance on reversal tasks, helping resolve the reversal curse issue.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#35757;&#32451;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.12403</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30340;&#21407;&#22240;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12403
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#35757;&#32451;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#29992;&#25143;&#36827;&#34892;&#20154;&#38469;&#35752;&#35770;&#21644;&#34920;&#36798;&#35266;&#28857;&#30340;&#37325;&#35201;&#22330;&#25152;&#65292;&#20294;&#31038;&#20132;&#23186;&#20307;&#25552;&#20379;&#30340;&#22806;&#31435;&#38754;&#21644;&#21311;&#21517;&#24615;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#21457;&#24067;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#12290;&#37492;&#20110;&#36825;&#20123;&#24179;&#21488;&#30340;&#24222;&#22823;&#35268;&#27169;&#65292;&#33258;&#21160;&#35782;&#21035;&#21644;&#26631;&#35760;&#20167;&#24680;&#35328;&#35770;&#30340;&#38656;&#27714;&#26085;&#30410;&#36843;&#20999;&#12290;&#23613;&#31649;&#23384;&#22312;&#20960;&#31181;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#40657;&#30418;&#26041;&#27861;&#22312;&#35774;&#35745;&#19978;&#19981;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#25110;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#35299;&#20915;&#35299;&#37322;&#24615;&#19981;&#36275;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#65292;&#35757;&#32451;&#22522;&#30784;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#36890;&#36807;&#35774;&#35745;&#23454;&#29616;&#24544;&#23454;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;LLM&#30340;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#21644;&#26368;&#20808;&#36827;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20351;&#36825;&#20123;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12403v1 Announce Type: cross  Abstract: Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech. Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#20174;&#27861;&#24459;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;&#20013;&#30830;&#23450;&#20449;&#24687;&#20844;&#27491;&#24615;&#12289;&#34920;&#29616;&#20844;&#27491;&#24615;&#21644;&#36947;&#24503;/&#20262;&#29702;&#19982;&#38544;&#31169;&#25919;&#31574;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#25919;&#31574;&#30340;&#36873;&#39033;&#12290;</title><link>https://arxiv.org/abs/2403.08115</link><description>&lt;p&gt;
&#27861;&#24459;&#32422;&#26463;&#20294;&#19981;&#20844;&#24179;&#65311;&#26397;&#21521;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#20844;&#24179;&#24615;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Legally Binding but Unfair? Towards Assessing Fairness of Privacy Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#20174;&#27861;&#24459;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;&#20013;&#30830;&#23450;&#20449;&#24687;&#20844;&#27491;&#24615;&#12289;&#34920;&#29616;&#20844;&#27491;&#24615;&#21644;&#36947;&#24503;/&#20262;&#29702;&#19982;&#38544;&#31169;&#25919;&#31574;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#25919;&#31574;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25919;&#31574;&#24212;&#24403;&#21578;&#30693;&#25968;&#25454;&#20027;&#20307;&#20854;&#25968;&#25454;&#20445;&#25252;&#26435;&#21033;&#65292;&#35299;&#37322;&#25968;&#25454;&#25511;&#21046;&#32773;&#30340;&#25968;&#25454;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#20351;&#20445;&#30041;&#26399;&#38480;&#25110;&#25968;&#25454;&#36716;&#31227;&#32473;&#31532;&#19977;&#26041;&#31561;&#20107;&#23454;&#36879;&#26126;&#21270;&#12290;&#38544;&#31169;&#25919;&#31574;&#21482;&#26377;&#22312;&#25968;&#25454;&#20027;&#20307;&#27491;&#30830;&#24863;&#30693;&#12289;&#35299;&#37322;&#12289;&#29702;&#35299;&#21644;&#20449;&#20219;&#26102;&#25165;&#33021;&#23454;&#29616;&#20854;&#30446;&#30340;&#12290;&#20854;&#20013;&#65292;&#36825;&#35201;&#27714;&#38544;&#31169;&#25919;&#31574;&#20197;&#20844;&#24179;&#30340;&#26041;&#24335;&#32534;&#20889;&#65292;&#20363;&#22914;&#19981;&#20351;&#29992;&#26497;&#31471;&#30340;&#26415;&#35821;&#65292;&#19981;&#38656;&#35201;&#29305;&#23450;&#30340;&#25945;&#32946;&#31243;&#24230;&#65292;&#25110;&#19981;&#20551;&#35774;&#29305;&#23450;&#30340;&#31038;&#20250;&#32972;&#26223;&#12290;&#22312;&#36825;&#20221;&#36827;&#34892;&#20013;&#30340;&#24037;&#20316;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25105;&#20204;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#22522;&#26412;&#27861;&#24459;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;&#20013;&#30830;&#23450;&#20449;&#24687;&#20844;&#27491;&#24615;&#12289;&#34920;&#29616;&#20844;&#27491;&#24615;&#21644;&#36947;&#24503;/&#20262;&#29702;&#19982;&#38544;&#31169;&#25919;&#31574;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#35780;&#20272;&#25919;&#31574;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08115v1 Announce Type: cross  Abstract: Privacy policies are expected to inform data subjects about their data protection rights. They should explain the data controller's data management practices, and make facts such as retention periods or data transfers to third parties transparent. Privacy policies only fulfill their purpose, if they are correctly perceived, interpreted, understood, and trusted by the data subject. Amongst others, this requires that a privacy policy is written in a fair way, e.g., it does not use polarizing terms, does not require a certain education, or does not assume a particular social background. In this work-in-progress paper, we outline our approach to assessing fairness in privacy policies. To this end, we identify from fundamental legal sources and fairness research, how the dimensions informational fairness, representational fairness and ethics/morality are related to privacy policies. We propose options to automatically assess policies in the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;(KPDDS)&#65292;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;KPMath&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#38646;-shot PASS@1&#31934;&#24230;&#20026;39.3%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.02333</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#21450;&#20854;&#22312;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;(KPDDS)&#65292;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;KPMath&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#38646;-shot PASS@1&#31934;&#24230;&#20026;39.3%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20854;&#24615;&#33021;&#36890;&#24120;&#21463;&#21040;&#39640;&#36136;&#37327;&#12289;&#20197;&#25512;&#29702;&#20026;&#37325;&#28857;&#30340;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#65288;KPDDS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#28304;&#30340;&#20851;&#38190;&#28857;&#21644;&#31034;&#20363;&#23545;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;KPDDS&#30830;&#20445;&#36890;&#36807;&#20005;&#26684;&#30340;&#36136;&#37327;&#25511;&#21046;&#21644;&#22823;&#35268;&#27169;&#24615;&#33021;&#30340;&#29983;&#25104;&#26032;&#39062;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KPMath&#65292;&#36804;&#20170;&#20026;&#27490;&#37327;&#36523;&#23450;&#21046;&#30340;&#26368;&#24191;&#27867;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19968;&#30334;&#19975;&#20010;&#20197;&#19978;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#21033;&#29992;KPMath&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#25512;&#29702;&#23494;&#38598;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#25193;&#20805;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20840;&#38754;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#12290;&#23558;Mistral-7B&#27169;&#22411;&#22312;KPMath-Plus&#19978;&#24494;&#35843;&#65292;&#20351;&#20854;&#22312;MATH&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#38646;-shot PASS@1&#31934;&#24230;&#36798;&#21040;39.3%&#65292;&#36825;&#26159;&#19968;&#39033;&#31361;&#30772;&#24615;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02333v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality, reasoning-focused training datasets. Addressing this challenge, we propose Key-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar pairs from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, the most extensive synthetic dataset tailored for mathematical reasoning to date, comprising over one million question-answer pairs. Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset. Fine-tuning the Mistral-7B model on KPMath-Plus yields a zero-shot PASS@1 accuracy of 39.3% on the MATH test set, a performance th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#19977;&#20010;&#25104;&#29087;&#30340;&#20154;&#31867;&#20915;&#31574;&#29702;&#35770;&#25972;&#21512;&#21040;&#19968;&#36215;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#30446;&#30340;&#24615;&#20154;&#31867;&#34892;&#21160;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#34892;&#21160;&#30340;&#35266;&#28857;&#24212;&#29992;&#20110;&#23545;&#35805;&#29992;&#25143;&#30028;&#38754;&#12290;&#36890;&#36807;&#29702;&#35299;ChatGPT&#30340;&#26234;&#33021;&#26469;&#28304;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#36164;&#28304;&#30340;&#21516;&#26102;&#33719;&#24471;&#23545;&#25105;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#35748;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.08403</link><description>&lt;p&gt;
LLMs&#21644;&#20154;&#31867;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
LLMs and the Human Condition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#19977;&#20010;&#25104;&#29087;&#30340;&#20154;&#31867;&#20915;&#31574;&#29702;&#35770;&#25972;&#21512;&#21040;&#19968;&#36215;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#30446;&#30340;&#24615;&#20154;&#31867;&#34892;&#21160;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#34892;&#21160;&#30340;&#35266;&#28857;&#24212;&#29992;&#20110;&#23545;&#35805;&#29992;&#25143;&#30028;&#38754;&#12290;&#36890;&#36807;&#29702;&#35299;ChatGPT&#30340;&#26234;&#33021;&#26469;&#28304;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#36164;&#28304;&#30340;&#21516;&#26102;&#33719;&#24471;&#23545;&#25105;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20154;&#31867;&#20915;&#31574;&#30340;&#19977;&#20010;&#25104;&#29087;&#29702;&#35770;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#23427;&#20204;&#25972;&#21512;&#36215;&#26469;&#25552;&#20379;&#19968;&#20010;&#30446;&#30340;&#24615;&#20154;&#31867;&#34892;&#21160;&#30340;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#34892;&#21160;&#30340;&#35266;&#28857;&#24212;&#29992;&#20110;&#23545;&#35805;&#29992;&#25143;&#30028;&#38754;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29702;&#35770;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#26412;&#25991;&#26088;&#22312;&#37325;&#26032;&#28608;&#21457;&#23545;&#29702;&#35299;LLMs&#23454;&#38469;&#25191;&#34892;&#30340;&#20852;&#36259;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#25152;&#26377;&#25968;&#25454;&#19978;&#36816;&#34892;&#38590;&#20197;&#29702;&#35299;&#30340;&#26426;&#22120;&#23398;&#20064;&#20363;&#31243;&#12290;&#24403;&#19968;&#21488;&#21806;&#20215;&#19981;&#21040;50&#32654;&#20803;&#30340;&#26641;&#33683;&#27966;&#30005;&#33041;&#27604;&#31532;&#19968;&#21488;&#21830;&#19994;Cray&#36229;&#32423;&#35745;&#31639;&#26426;&#24555;400&#20493;&#26102;&#65292;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#21487;&#20197;&#25509;&#36817;&#25317;&#26377;&#26080;&#25968;&#38543;&#26426;&#25171;&#23383;&#24182;&#29983;&#25104;&#26377;&#24847;&#20041;&#25991;&#23383;&#30340;&#29492;&#23376;&#12290;&#36890;&#36807;&#29702;&#35299;ChatGPT&#30340;&#34920;&#29616;&#26234;&#33021;&#30340;&#26469;&#28304;&#65292;&#20063;&#35768;&#25105;&#20204;&#21487;&#20197;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#36827;&#34892;&#21516;&#26679;&#30340;&#39764;&#26415;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#33719;&#24471;&#19968;&#20123;&#20851;&#20110;&#25105;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents three established theories of human decision-making and describes how they can be integrated to provide a model of purposive human action. Taking seriously the idea of language as action the model is then applied to the conversational user interfaces. Theory based AI research has had a hard time recently and the aim here is to revitalise interest in understanding what LLMs are actually doing other than running poorly understood machine learning routines over all the data the relevant Big Tech company can hoover up. When a raspberry pi computer for under 50USD is up to 400 times faster than the first commercial Cray super computer~\cite{crayVpi}, Big Tech can get really close to having an infinite number of monkeys typing at random and producing text, some of which will make sense. By understanding where ChatGPT's apparent intelligence comes from, perhaps we can perform the magic with fewer resources and at the same time gain some understanding about our relationship
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07818</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#20363;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#26222;&#21450;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20445;&#25252;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#36798;&#21040;&#28385;&#24847;&#30340;&#26435;&#34913;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;DP-SGD&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#12290;&#23613;&#31649;&#23558;DP-SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#25512;&#21040;&#20102;&#26497;&#38480;&#65292;&#20294;&#22522;&#20110;DP-SGD&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#24184;&#22320;&#21463;&#21040;&#20102;SGD&#22266;&#26377;&#20302;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#38646;&#38454;&#26041;&#27861;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#26356;&#39640;&#25928;&#30340;&#38646;&#38454;&#26799;&#24230;&#26469;&#36817;&#20284;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#12290;&#19982;&#23558;&#38646;&#38454;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21106;&#25509;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#38750;&#24120;&#25509;&#36817;&#30340;&#26041;&#24335;&#27169;&#25311;DP-SGD&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#28982;&#21518;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
&lt;/p&gt;</description></item><item><title>ResumeFlow&#26159;&#19968;&#31181;&#21033;&#29992;LLM&#25216;&#26415;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#27714;&#32844;&#32773;&#26681;&#25454;&#29305;&#23450;&#30340;&#32844;&#20301;&#35201;&#27714;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#31616;&#21382;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#23450;&#21046;&#31616;&#21382;&#30340;&#32791;&#26102;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06221</link><description>&lt;p&gt;
ResumeFlow: &#19968;&#31181;&#20010;&#24615;&#21270;&#31616;&#21382;&#29983;&#25104;&#21644;&#20462;&#35746;&#30340;LLM&#36741;&#21161;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
ResumeFlow: An LLM-facilitated Pipeline for Personalized Resume Generation and Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06221
&lt;/p&gt;
&lt;p&gt;
ResumeFlow&#26159;&#19968;&#31181;&#21033;&#29992;LLM&#25216;&#26415;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#27714;&#32844;&#32773;&#26681;&#25454;&#29305;&#23450;&#30340;&#32844;&#20301;&#35201;&#27714;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#31616;&#21382;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#23450;&#21046;&#31616;&#21382;&#30340;&#32791;&#26102;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#27714;&#32844;&#32773;&#26469;&#35828;&#65292;&#21046;&#20316;&#31526;&#21512;&#29305;&#23450;&#32844;&#20301;&#35201;&#27714;&#30340;&#29702;&#24819;&#31616;&#21382;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#21021;&#20837;&#32844;&#22330;&#30340;&#27714;&#32844;&#32773;&#26469;&#35828;&#12290;&#34429;&#28982;&#24378;&#28872;&#24314;&#35758;&#27714;&#32844;&#32773;&#26681;&#25454;&#20182;&#20204;&#30003;&#35831;&#30340;&#20855;&#20307;&#32844;&#20301;&#23450;&#21046;&#31616;&#21382;&#65292;&#20294;&#25163;&#21160;&#26681;&#25454;&#24037;&#20316;&#25551;&#36848;&#21644;&#32844;&#20301;&#35201;&#27714;&#26469;&#23450;&#21046;&#31616;&#21382;&#36890;&#24120; (1) &#38750;&#24120;&#32791;&#26102;&#65292;&#19988; (2) &#23481;&#26131;&#20986;&#38169;&#12290;&#27492;&#22806;&#65292;&#22312;&#30003;&#35831;&#22810;&#20010;&#32844;&#20301;&#26102;&#36827;&#34892;&#36825;&#26679;&#30340;&#23450;&#21046;&#27493;&#39588;&#21487;&#33021;&#23548;&#33268;&#32534;&#36753;&#31616;&#21382;&#36136;&#37327;&#19981;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#28436;&#31034;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ResumeFlow: &#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24037;&#20855;&#65292;&#20351;&#32456;&#31471;&#29992;&#25143;&#21482;&#38656;&#25552;&#20379;&#35814;&#32454;&#30340;&#31616;&#21382;&#21644;&#25152;&#38656;&#30340;&#32844;&#20301;&#21457;&#24067;&#20449;&#24687;&#65292;&#23601;&#33021;&#22312;&#20960;&#31186;&#38047;&#20869;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#35813;&#29305;&#23450;&#32844;&#20301;&#21457;&#24067;&#30340;&#20010;&#24615;&#21270;&#31616;&#21382;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27969;&#31243;&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;LLM&#65288;&#22914;OpenAI&#30340;GPT-4&#21644;Google&#30340;......&#65289;
&lt;/p&gt;
&lt;p&gt;
Crafting the ideal, job-specific resume is a challenging task for many job applicants, especially for early-career applicants. While it is highly recommended that applicants tailor their resume to the specific role they are applying for, manually tailoring resumes to job descriptions and role-specific requirements is often (1) extremely time-consuming, and (2) prone to human errors. Furthermore, performing such a tailoring step at scale while applying to several roles may result in a lack of quality of the edited resumes. To tackle this problem, in this demo paper, we propose ResumeFlow: a Large Language Model (LLM) aided tool that enables an end user to simply provide their detailed resume and the desired job posting, and obtain a personalized resume specifically tailored to that specific job posting in the matter of a few seconds. Our proposed pipeline leverages the language understanding and information extraction capabilities of state-of-the-art LLMs such as OpenAI's GPT-4 and Goog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FAQ-Gen&#65292;&#19968;&#20010;&#21033;&#29992;&#25991;&#26412;&#36716;&#25991;&#26412;&#36716;&#25442;&#27169;&#22411;&#24320;&#21457;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#30340;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#31574;&#21010;&#30340;&#31639;&#27861;&#26469;&#20248;&#21270;&#20449;&#24687;&#34920;&#31034;&#21644;&#38382;&#39064;-&#31572;&#26696;&#25490;&#21517;&#65292;&#25552;&#39640;&#20102;FAQ&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#23450;&#24615;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#29983;&#25104;&#30340;FAQs&#26500;&#36896;&#33391;&#22909;&#19988;&#21487;&#25512;&#24191;&#33267;&#19981;&#21516;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.05812</link><description>&lt;p&gt;
FAQ-Gen&#65306;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#36741;&#21161;&#29702;&#35299;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
FAQ-Gen: An automated system to generate domain-specific FAQs to aid content comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FAQ-Gen&#65292;&#19968;&#20010;&#21033;&#29992;&#25991;&#26412;&#36716;&#25991;&#26412;&#36716;&#25442;&#27169;&#22411;&#24320;&#21457;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#30340;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#31574;&#21010;&#30340;&#31639;&#27861;&#26469;&#20248;&#21270;&#20449;&#24687;&#34920;&#31034;&#21644;&#38382;&#39064;-&#31572;&#26696;&#25490;&#21517;&#65292;&#25552;&#39640;&#20102;FAQ&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#23450;&#24615;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#29983;&#25104;&#30340;FAQs&#26500;&#36896;&#33391;&#22909;&#19988;&#21487;&#25512;&#24191;&#33267;&#19981;&#21516;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#65288;FAQ&#65289;&#26159;&#25351;&#20851;&#20110;&#29305;&#23450;&#20869;&#23481;&#30340;&#26368;&#24120;&#35265;&#35810;&#38382;&#12290;&#23427;&#20204;&#36890;&#36807;&#31616;&#21270;&#20027;&#39064;&#21644;&#36890;&#36807;&#31616;&#26126;&#25212;&#35201;&#22320;&#21576;&#29616;&#20449;&#24687;&#26469;&#22686;&#24378;&#29702;&#35299;&#65292;&#20316;&#20026;&#20869;&#23481;&#29702;&#35299;&#30340;&#36741;&#21161;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#21033;&#29992;&#25991;&#26412;&#36716;&#25991;&#26412;&#36716;&#25442;&#27169;&#22411;&#26469;&#35299;&#20915;FAQ&#29983;&#25104;&#20316;&#20026;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#28085;&#30422;&#20256;&#32479;&#38382;&#31572;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24378;&#35843;&#23427;&#20204;&#22312;&#30452;&#25509;&#24212;&#29992;&#20110;FAQ&#29983;&#25104;&#20219;&#21153;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#39046;&#22495;&#30340;&#25991;&#26412;&#20869;&#23481;&#26500;&#24314;FAQs&#65292;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#25105;&#31574;&#21010;&#30340;&#31639;&#27861;&#26469;&#33719;&#24471;&#25552;&#20379;&#32473;&#36755;&#20837;&#30340;&#20449;&#24687;&#30340;&#26368;&#20339;&#34920;&#31034;&#65292;&#24182;&#23545;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#25490;&#24207;&#20197;&#26368;&#22823;&#21270;&#20154;&#31867;&#29702;&#35299;&#12290;&#23450;&#24615;&#20154;&#31867;&#35780;&#20272;&#26174;&#31034;&#29983;&#25104;&#30340;FAQs&#26500;&#36896;&#33391;&#22909;&#19988;&#21487;&#25512;&#24191;&#33267;&#19981;&#21516;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Frequently Asked Questions (FAQs) refer to the most common inquiries about specific content. They serve as content comprehension aids by simplifying topics and enhancing understanding through succinct presentation of information. In this paper, we address FAQ generation as a well-defined Natural Language Processing (NLP) task through the development of an end-to-end system leveraging text-to-text transformation models. We present a literature review covering traditional question-answering systems, highlighting their limitations when applied directly to the FAQ generation task. We propose our system capable of building FAQs from textual content tailored to specific domains, enhancing their accuracy and relevance. We utilise self-curated algorithms for obtaining optimal representation of information to be provided as input and also for ranking the question-answer pairs to maximise human comprehension. Qualitative human evaluation showcases the generated FAQs to be well-constructed and re
&lt;/p&gt;</description></item><item><title>CMMU&#26159;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#27700;&#24179;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14011</link><description>&lt;p&gt;
CMMU: &#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#19982;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. (arXiv:2401.14011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14011
&lt;/p&gt;
&lt;p&gt;
CMMU&#26159;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#27700;&#24179;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#30693;&#35782;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;MLLM&#30340;&#26234;&#33021;&#27700;&#24179;&#25152;&#38656;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25484;&#25569;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24403;&#21069;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#22810;&#39033;&#36873;&#25321;&#39064;&#19978;&#65292;&#24182;&#19988;&#22312;&#35780;&#20272;&#30340;&#20840;&#38754;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CMMU&#65292;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;CMMU&#21253;&#21547;7&#20010;&#23398;&#31185;&#30340;3603&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20998;&#20026;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31867;&#65292;&#23545;MLLMs&#25552;&#20986;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#35780;&#20272;&#31574;&#30053;&#65292;&#31216;&#20026;ShiftCheck&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models(MLLMs) have achieved remarkable progress and demonstrated powerful knowledge comprehension and reasoning abilities. However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge. Current multi-modal benchmarks for domain-specific knowledge concentrate on multiple-choice questions and are predominantly available in English, which imposes limitations on the comprehensiveness of the evaluation. To this end, we introduce CMMU, a novel benchmark for multi-modal and multi-type question understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The questions can be categorized into 3 types: multiple-choice, multiple-response, and fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we propose a rigorous evaluation strategy called ShiftCheck for assessing multiple-choice questions. The strat
&lt;/p&gt;</description></item><item><title>CIM-MLC&#26159;&#19968;&#20010;&#38754;&#21521;&#35745;&#31639;&#20869;&#23384;&#21152;&#36895;&#22120;&#30340;&#22810;&#32423;&#32534;&#35793;&#26632;&#65292;&#20026;&#20102;&#25903;&#25345;&#21508;&#31181;CIM&#26550;&#26500;&#30340;&#28508;&#21147;&#65292;&#35813;&#32534;&#35793;&#26632;&#23436;&#20840;&#20102;&#35299;CIM&#26550;&#26500;&#32454;&#33410;&#21644;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#24615;&#26469;&#25903;&#25345;&#20855;&#26377;&#19981;&#21516;&#35745;&#31639;&#31890;&#24230;&#30340;CIM&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.12428</link><description>&lt;p&gt;
CIM-MLC:&#38754;&#21521;&#35745;&#31639;&#20869;&#23384;&#21152;&#36895;&#22120;&#30340;&#22810;&#32423;&#32534;&#35793;&#26632;
&lt;/p&gt;
&lt;p&gt;
CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators. (arXiv:2401.12428v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12428
&lt;/p&gt;
&lt;p&gt;
CIM-MLC&#26159;&#19968;&#20010;&#38754;&#21521;&#35745;&#31639;&#20869;&#23384;&#21152;&#36895;&#22120;&#30340;&#22810;&#32423;&#32534;&#35793;&#26632;&#65292;&#20026;&#20102;&#25903;&#25345;&#21508;&#31181;CIM&#26550;&#26500;&#30340;&#28508;&#21147;&#65292;&#35813;&#32534;&#35793;&#26632;&#23436;&#20840;&#20102;&#35299;CIM&#26550;&#26500;&#32454;&#33410;&#21644;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#24615;&#26469;&#25903;&#25345;&#20855;&#26377;&#19981;&#21516;&#35745;&#31639;&#31890;&#24230;&#30340;CIM&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21508;&#31181;&#35745;&#31639;&#20869;&#23384;(CIM)&#22788;&#29702;&#22120;&#34987;&#25552;&#20986;&#65292;&#23637;&#31034;&#20986;&#20248;&#20110;&#20256;&#32479;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#37322;&#25918;&#21508;&#31181;CIM&#26550;&#26500;&#30340;&#28508;&#21147;&#65292;&#22914;&#35774;&#22791;&#31934;&#24230;&#12289;&#20132;&#21449;&#26639;&#22823;&#23567;&#21644;&#20132;&#21449;&#26639;&#25968;&#37327;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#23436;&#20840;&#20102;&#35299;CIM&#26550;&#26500;&#32454;&#33410;&#21644;&#23454;&#29616;&#22810;&#26679;&#24615;&#30340;&#32534;&#35793;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#27969;&#34892;&#30340;&#24320;&#28304;&#32534;&#35793;&#26632;&#22312;&#26550;&#26500;&#25903;&#25345;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#29616;&#26377;&#30340;CIM&#35774;&#35745;&#35201;&#20040;&#25163;&#21160;&#37096;&#32626;&#32593;&#32476;&#65292;&#35201;&#20040;&#26500;&#24314;&#33258;&#24049;&#30340;&#32534;&#35793;&#22120;&#65292;&#36825;&#26159;&#32791;&#26102;&#32791;&#21147;&#30340;&#12290;&#23613;&#31649;&#19968;&#20123;&#24037;&#20316;&#23558;&#29305;&#23450;&#30340;CIM&#35774;&#22791;&#32534;&#31243;&#25509;&#21475;&#20844;&#24320;&#32473;&#32534;&#35793;&#22120;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#34987;&#32465;&#23450;&#21040;&#22266;&#23450;&#30340;CIM&#26550;&#26500;&#65292;&#32570;&#20047;&#28789;&#27963;&#24615;&#26469;&#25903;&#25345;&#20855;&#26377;&#19981;&#21516;&#35745;&#31639;&#31890;&#24230;&#30340;CIM&#26550;&#26500;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#32534;&#35793;&#24037;&#20316;&#36890;&#24120;&#32771;&#34385;&#20102;&#26377;&#38480;&#30340;&#25805;&#20316;&#31867;&#22411;&#30340;&#35843;&#24230;&#65288;&#20363;&#22914;&#20132;&#21449;&#26639;&#38480;&#21046;&#30340;&#30697;&#38453;-&#21521;&#37327;&#20056;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
In recent years, various computing-in-memory (CIM) processors have been presented, showing superior performance over traditional architectures. To unleash the potential of various CIM architectures, such as device precision, crossbar size, and crossbar number, it is necessary to develop compilation tools that are fully aware of the CIM architectural details and implementation diversity. However, due to the lack of architectural support in current popular open-source compiling stacks, existing CIM designs either manually deploy networks or build their own compilers, which is time-consuming and labor-intensive. Although some works expose the specific CIM device programming interfaces to compilers, they are often bound to a fixed CIM architecture, lacking the flexibility to support the CIM architectures with different computing granularity. On the other hand, existing compilation works usually consider the scheduling of limited operation types (such as crossbar-bound matrix-vector multipl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#30340;&#21306;&#21035;&#65292;&#20197;&#35299;&#20915;&#28389;&#29992;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06712</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#36827;&#34892;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#23567;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Detection of Machine-Generated Text using Style Representations. (arXiv:2401.06712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#30340;&#21306;&#21035;&#65292;&#20197;&#35299;&#20915;&#28389;&#29992;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#25351;&#23548;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#24471;&#20154;&#31867;&#20889;&#20316;&#30340;&#36924;&#30495;&#27169;&#20223;&#38754;&#20020;&#30528;&#37325;&#22823;&#28389;&#29992;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#19968;&#27573;&#25991;&#26412;&#26159;&#30001;&#35821;&#35328;&#27169;&#22411;&#36824;&#26159;&#20154;&#31867;&#25776;&#20889;&#32780;&#25104;&#26469;&#23545;&#25239;&#27492;&#31867;&#28389;&#29992;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#24335;&#34920;&#31034;&#30340;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#22312;&#38754;&#23545;&#25968;&#25454;&#36716;&#25442;&#26102;&#30340;&#35268;&#32422;&#19981;&#36275;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#36991;&#20813;&#20102;&#22312;&#25512;&#29702;&#25110;&#26816;&#27979;&#26102;&#38656;&#35201;&#35775;&#38382;&#21487;&#33021;&#29983;&#25104;&#25991;&#26723;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that may have generated a document in question at inference or detection time, which is often impractical. In light of these challenges, we pursue a fundamentally d
&lt;/p&gt;</description></item><item><title>AMPLIFY&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#30340;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#20110;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12689</link><description>&lt;p&gt;
AMPLIFY: &#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26631;&#31614;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer. (arXiv:2309.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12689
&lt;/p&gt;
&lt;p&gt;
AMPLIFY&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#30340;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#20110;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#21407;&#22987;&#26679;&#26412;&#30340;&#32447;&#24615;&#32452;&#21512;&#29983;&#25104;&#26032;&#30340;&#22686;&#24378;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#21407;&#22987;&#26679;&#26412;&#20013;&#23384;&#22312;&#22122;&#38899;&#25110;&#24322;&#24120;&#29305;&#24449;&#65292;Mixup&#21487;&#33021;&#23558;&#20854;&#20256;&#25773;&#21040;&#22686;&#24378;&#26679;&#26412;&#20013;&#65292;&#23548;&#33268;&#27169;&#22411;&#23545;&#36825;&#20123;&#24322;&#24120;&#20540;&#36807;&#20110;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Mixup&#26041;&#27861;&#31216;&#20026;AMPLIFY&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Transformer&#33258;&#36523;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#26080;&#38656;&#22686;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#20302;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#24120;&#35265;Mixup&#26041;&#27861;&#65288;&#20363;&#22914;&#35821;&#21477;Mixup&#65289;&#20013;&#36164;&#28304;&#28040;&#32791;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26356;&#23567;&#30340;&#35745;&#31639;&#36164;&#28304;&#25104;&#26412;&#19979;&#65292;AMPLIFY&#22312;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;Mixup&#26041;&#27861;&#65292;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is an effective data augmentation method that generates new augmented samples by aggregating linear combinations of different original samples. However, if there are noises or aberrant features in the original samples, Mixup may propagate them to the augmented samples, leading to over-sensitivity of the model to these outliers . To solve this problem, this paper proposes a new Mixup method called AMPLIFY. This method uses the Attention mechanism of Transformer itself to reduce the influence of noises and aberrant values in the original samples on the prediction results, without increasing additional trainable parameters, and the computational cost is very low, thereby avoiding the problem of high resource consumption in common Mixup methods such as Sentence Mixup . The experimental results show that, under a smaller computational resource cost, AMPLIFY outperforms other Mixup methods in text classification tasks on 7 benchmark datasets, providing new ideas and new ways to further
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38669;&#22827;&#26031;&#27888;&#24503;&#25991;&#21270;&#32500;&#24230;&#26694;&#26550;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19981;&#21516;&#25991;&#21270;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#30340;&#25991;&#21270;&#23545;&#40784;&#27979;&#35797;&#65288;CAT&#65289;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25991;&#21270;&#22269;&#23478;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#35299;&#37322;&#24615;&#25991;&#21270;&#32500;&#24230;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#33021;&#37327;&#21270;&#20854;&#19982;&#29305;&#23450;&#22269;&#23478;&#30340;&#25991;&#21270;&#23545;&#40784;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.12342</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#23545;&#40784;&#65306;&#22522;&#20110;&#38669;&#22827;&#26031;&#27888;&#24503;&#25991;&#21270;&#32500;&#24230;&#30340;&#35299;&#37322;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions. (arXiv:2309.12342v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38669;&#22827;&#26031;&#27888;&#24503;&#25991;&#21270;&#32500;&#24230;&#26694;&#26550;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19981;&#21516;&#25991;&#21270;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#30340;&#25991;&#21270;&#23545;&#40784;&#27979;&#35797;&#65288;CAT&#65289;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25991;&#21270;&#22269;&#23478;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#35299;&#37322;&#24615;&#25991;&#21270;&#32500;&#24230;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#33021;&#37327;&#21270;&#20854;&#19982;&#29305;&#23450;&#22269;&#23478;&#30340;&#25991;&#21270;&#23545;&#40784;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37096;&#32626;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#25991;&#21270;&#23545;&#40784;&#21644;&#23545;&#19981;&#21516;&#25991;&#21270;&#35268;&#33539;&#20010;&#20307;&#30340;&#28508;&#22312;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#25919;&#27835;&#21644;&#31038;&#20250;&#20559;&#35265;&#20197;&#21450;&#20844;&#20247;&#24847;&#35265;&#65292;&#32780;&#26410;&#28041;&#21450;&#25991;&#21270;&#20215;&#20540;&#35266;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#21270;&#23545;&#40784;&#27979;&#35797;&#65288;CAT&#65289;&#65292;&#21033;&#29992;&#38669;&#22827;&#26031;&#27888;&#24503;&#30340;&#25991;&#21270;&#32500;&#24230;&#26694;&#26550;&#37327;&#21270;&#25991;&#21270;&#23545;&#40784;&#65292;&#36890;&#36807;&#28508;&#21464;&#37327;&#20998;&#26512;&#25552;&#20379;&#35299;&#37322;&#24615;&#30340;&#36328;&#25991;&#21270;&#27604;&#36739;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;&#22914;ChatGPT&#21644;Bard&#65289;&#22312;&#19981;&#21516;&#25991;&#21270;&#22269;&#23478;&#65288;&#32654;&#22269;&#12289;&#27801;&#29305;&#38463;&#25289;&#20271;&#12289;&#20013;&#22269;&#21644;&#26031;&#27931;&#20240;&#20811;&#65289;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#20215;&#20540;&#35266;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#39118;&#26684;&#21644;&#36229;&#21442;&#25968;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#37327;&#21270;&#20102;LLMs&#19982;&#29305;&#23450;&#22269;&#23478;&#30340;&#25991;&#21270;&#23545;&#40784;&#31243;&#24230;&#65292;&#32780;&#19988;&#25581;&#31034;&#20102;LLMs&#22312;&#35299;&#37322;&#24615;&#25991;&#21270;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;&#23613;&#31649;&#25152;&#26377;&#30340;LLMs&#37117;&#27809;&#26377;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals from various cultural norms. Existing work investigated political and social biases and public opinions rather than their cultural values. To address this limitation, the proposed Cultural Alignment Test (CAT) quantifies cultural alignment using Hofstede's cultural dimension framework, which offers an explanatory cross-cultural comparison through the latent variable analysis. We apply our approach to assess the cultural values embedded in state-of-the-art LLMs, such as: ChatGPT and Bard, across diverse cultures of countries: United States (US), Saudi Arabia, China, and Slovakia, using different prompting styles and hyperparameter settings. Our results not only quantify cultural alignment of LLMs with certain countries, but also reveal the difference between LLMs in explanatory cultural dimensions. While all LLMs did not provide satisfactory res
&lt;/p&gt;</description></item><item><title>ImpressionGPT&#26159;&#19968;&#20010;&#21033;&#29992;LLMs&#26500;&#24314;&#21160;&#24577;&#19978;&#19979;&#25991;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#29983;&#25104;&#12290;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;ImpressionGPT&#22312;&#20855;&#26377;&#36739;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.08448</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#29983;&#25104;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65306;ImpressionGPT
&lt;/p&gt;
&lt;p&gt;
ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT. (arXiv:2304.08448v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08448
&lt;/p&gt;
&lt;p&gt;
ImpressionGPT&#26159;&#19968;&#20010;&#21033;&#29992;LLMs&#26500;&#24314;&#21160;&#24577;&#19978;&#19979;&#25991;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#29983;&#25104;&#12290;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;ImpressionGPT&#22312;&#20855;&#26377;&#36739;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;"Impression"&#37096;&#20998;&#26159;&#25918;&#23556;&#31185;&#21307;&#24072;&#21644;&#20854;&#20182;&#21307;&#29983;&#20132;&#27969;&#30340;&#37325;&#35201;&#22522;&#30784;&#65292;&#36890;&#24120;&#26159;&#22522;&#20110;"Findings"&#37096;&#20998;&#32534;&#20889;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#24072;&#26469;&#35828;&#65292;&#32534;&#20889;&#22823;&#37327;&#30340;&#21360;&#35937;&#25551;&#36848;&#21487;&#33021;&#26159;&#36153;&#26102;&#36153;&#21147;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21360;&#35937;&#29983;&#25104;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20294;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#24046;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#34429;&#28982;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;&#25918;&#23556;&#23398;&#65289;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#26410;&#32463;&#35843;&#26597;&#65292;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ImpressionGPT&#65292;&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#20010;&#24615;&#21270;&#25968;&#25454;&#26500;&#24314;&#21160;&#24577;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 'Impression' section of a radiology report is a critical basis for communication between radiologists and other physicians, and it is typically written by radiologists based on the 'Findings' section. However, writing numerous impressions can be laborious and error-prone for radiologists. Although recent studies have achieved promising results in automatic impression generation using large-scale medical text data for pre-training and fine-tuning pre-trained language models, such models often require substantial amounts of medical text data and have poor generalization performance. While large language models (LLMs) like ChatGPT have shown strong generalization capabilities and performance, their performance in specific domains, such as radiology, remains under-investigated and potentially limited. To address this limitation, we propose ImpressionGPT, which leverages the in-context learning capability of LLMs by constructing dynamic contexts using domain-specific, individualized dat
&lt;/p&gt;</description></item></channel></rss>