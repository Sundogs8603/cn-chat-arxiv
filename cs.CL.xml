<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Expresso&#25968;&#25454;&#38598;&#65292;&#23545;&#31163;&#25955;&#34920;&#36798;&#24615;&#35821;&#38899;&#20877;&#21512;&#25104;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#25991;&#26412;&#30340;&#39640;&#36136;&#37327;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#38590;&#20197;&#36716;&#24405;&#30340;&#35821;&#38899;&#34920;&#36798;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2308.05725</link><description>&lt;p&gt;
EXPRESSO: &#19968;&#39033;&#23545;&#31163;&#25955;&#34920;&#36798;&#24615;&#35821;&#38899;&#20877;&#21512;&#25104;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#20998;&#26512;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis. (arXiv:2308.05725v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Expresso&#25968;&#25454;&#38598;&#65292;&#23545;&#31163;&#25955;&#34920;&#36798;&#24615;&#35821;&#38899;&#20877;&#21512;&#25104;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#25991;&#26412;&#30340;&#39640;&#36136;&#37327;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#38590;&#20197;&#36716;&#24405;&#30340;&#35821;&#38899;&#34920;&#36798;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#20381;&#36182;&#20110;&#25991;&#26412;&#65292;&#32780;&#26159;&#22522;&#20110;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#23398;&#20064;&#21040;&#30340;&#20302;&#27604;&#29305;&#29575;&#31163;&#25955;&#21333;&#20803;&#65292;&#21487;&#20197;&#37325;&#26032;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#21040;&#38590;&#20197;&#36716;&#24405;&#30340;&#35821;&#38899;&#34920;&#36798;&#26041;&#38754;&#65288;&#38901;&#24459;&#12289;&#22768;&#38899;&#39118;&#26684;&#12289;&#38750;&#35821;&#35328;&#35821;&#38899;&#21270;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#35821;&#38899;&#21512;&#25104;&#25968;&#25454;&#38598;&#37117;&#26159;&#26391;&#35835;&#30340;&#65292;&#22240;&#27492;&#23545; spontaneity &#21644; expressivity &#30340;&#35201;&#27714;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102; Expresso&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#26080;&#25991;&#26412;&#35821;&#38899;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26391;&#35835;&#35821;&#38899;&#21644;26&#31181;&#33258;&#21457;&#34920;&#36798;&#39118;&#26684;&#30340;&#21363;&#20852;&#23545;&#35805;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#34920;&#36798;&#24615;&#37325;&#26032;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#26469;&#23637;&#31034;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#21644;&#28508;&#21147;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#38656;&#35201;&#20197;&#20302;&#27604;&#29305;&#29575;&#21333;&#20803;&#23545;&#36755;&#20837;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#22312;&#30446;&#26631;&#38899;&#33394;&#20013;&#37325;&#26032;&#21512;&#25104;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#23481;&#21644;&#39118;&#26684;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#23545;&#19981;&#21516;&#30340;&#33258;&#30417;&#30563;&#31163;&#25955;&#21512;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that it is possible to resynthesize high-quality speech based, not on text, but on low bitrate discrete units that have been learned in a self-supervised fashion and can therefore capture expressive aspects of speech that are hard to transcribe (prosody, voice styles, non-verbal vocalization). The adoption of these methods is still limited by the fact that most speech synthesis datasets are read, severely limiting spontaneity and expressivity. Here, we introduce Expresso, a high-quality expressive speech dataset for textless speech synthesis that includes both read speech and improvised dialogues rendered in 26 spontaneous expressive styles. We illustrate the challenges and potentials of this dataset with an expressive resynthesis benchmark where the task is to encode the input in low-bitrate units and resynthesize it in a target voice while preserving content and style. We evaluate resynthesis quality with automatic metrics for different self-supervised discrete 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#35752;&#22797;&#26434;&#24615;&#19982;&#23545;&#40784;&#20043;&#38388;&#30340;&#22266;&#26377;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;"tree-instruct"&#26041;&#27861;&#26469;&#22686;&#24378;&#25351;&#20196;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#20197;&#22312;&#23545;&#40784;&#21040;&#20219;&#21153;&#21644;&#29992;&#25143;&#20559;&#22909;&#26041;&#38754;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05696</link><description>&lt;p&gt;
&#22797;&#26434;&#24615;&#19982;&#23545;&#40784;&#20043;&#38388;&#22266;&#26377;&#20851;&#31995;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment. (arXiv:2308.05696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#35752;&#22797;&#26434;&#24615;&#19982;&#23545;&#40784;&#20043;&#38388;&#30340;&#22266;&#26377;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;"tree-instruct"&#26041;&#27861;&#26469;&#22686;&#24378;&#25351;&#20196;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#20197;&#22312;&#23545;&#40784;&#21040;&#20219;&#21153;&#21644;&#29992;&#25143;&#20559;&#22909;&#26041;&#38754;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24320;&#25918;&#22495;&#25351;&#20196;&#25968;&#25454;&#22521;&#35757;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23545;&#40784;&#21040;&#26368;&#32456;&#20219;&#21153;&#21644;&#29992;&#25143;&#20559;&#22909;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#22823;&#37327;&#30740;&#31350;&#34920;&#26126;&#65292;&#25552;&#39640;&#25351;&#20196;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#22987;&#32456;&#33021;&#22815;&#25913;&#21892;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#25351;&#26631;&#65292;&#25968;&#25454;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#20027;&#35201;&#21253;&#25324;&#19977;&#20010;&#26041;&#38754;&#65306;(1)&#25193;&#23637;&#35268;&#24459;&#65292;&#24615;&#33021;&#25913;&#36827;&#22312;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#30340;&#21487;&#25345;&#32493;&#24615;&#23578;&#19981;&#30830;&#23450;&#65292;(2)&#39069;&#22806;&#30340;&#26631;&#35760;&#65292;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#25913;&#36827;&#26159;&#21542;&#26469;&#33258;&#24341;&#20837;&#26356;&#22810;&#35757;&#32451;&#26631;&#35760;&#65292;&#20197;&#21450;(3)&#35838;&#31243;&#35774;&#32622;&#65292;&#23558;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#25351;&#20196;&#32435;&#20837;&#26159;&#21542;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#20063;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"tree-instruct"&#65292;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#31995;&#32479;&#22320;&#22686;&#24378;&#25351;&#20196;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#25351;&#23450;&#25968;&#37327;&#30340;&#33410;&#28857;&#28155;&#21152;&#21040;&#25351;&#20196;&#35821;&#20041;&#26641;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26032;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models (LLMs) with open-domain instruction data has yielded remarkable success in aligning to end tasks and user preferences. Extensive research has highlighted that enhancing the quality and diversity of instruction data consistently improves performance. However, the impact of data complexity, as a crucial metric, remains relatively unexplored in three aspects: (1) scaling law, where the sustainability of performance improvements with increasing complexity is uncertain, (2) additional tokens, whether the improvement brought by complexity comes from introducing more training tokens, and (3) curriculum tuning, where the potential advantages of incorporating instructions ranging from easy to difficult are not yet fully understood. In this paper, we propose \textit{tree-instruct} to systematically enhance the complexity of instruction data in a controllable manner. This approach adds a specified number of nodes into the instruction semantic tree, yielding new inst
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#20197;&#21450;&#25552;&#20986;&#22810;&#38454;&#27573;&#26694;&#26550;&#26469;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#28548;&#28165;&#26816;&#32034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05680</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#38454;&#27573;&#26816;&#32034;&#25214;&#21040;&#24050;&#32463;&#34987;&#28548;&#28165;&#30340;&#21465;&#36848;&#65306;&#23454;&#29616;&#36328;&#35821;&#35328;&#12289;&#36328;&#25968;&#25454;&#38598;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning. (arXiv:2308.05680v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#20197;&#21450;&#25552;&#20986;&#22810;&#38454;&#27573;&#26694;&#26550;&#26469;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#28548;&#28165;&#26816;&#32034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#24050;&#32463;&#34987;&#28548;&#28165;&#30340;&#21465;&#36848;&#30340;&#20219;&#21153;&#26088;&#22312;&#26816;&#27979;&#24050;&#32463;&#32463;&#36807;&#20107;&#23454;&#26680;&#26597;&#30340;&#25925;&#20107;&#12290;&#25104;&#21151;&#26816;&#27979;&#21040;&#24050;&#34987;&#28548;&#28165;&#30340;&#22768;&#26126;&#19981;&#20165;&#20943;&#23569;&#20102;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#30340;&#25163;&#21160;&#21162;&#21147;&#65292;&#36824;&#21487;&#20197;&#26377;&#21161;&#20110;&#20943;&#32531;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#65292;&#36825;&#26159;&#19968;&#20010;&#30740;&#31350;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#32771;&#34385;&#36328;&#35821;&#35328;&#20219;&#21153;&#26102;&#65292;&#21363;&#22312;&#26816;&#26597;&#30340;&#22312;&#32447;&#24086;&#23376;&#30340;&#35821;&#35328;&#19982;&#20107;&#23454;&#26680;&#26597;&#25991;&#31456;&#30340;&#35821;&#35328;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26816;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65306;&#65288;i&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20801;&#35768;&#23545;&#24050;&#34987;&#28548;&#28165;&#30340;&#21465;&#36848;&#36827;&#34892;&#36328;&#35821;&#35328;&#26816;&#32034;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#25512;&#25991;&#20316;&#20026;&#23545;&#20107;&#23454;&#26680;&#26597;&#25991;&#31456;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#65307;&#65288;ii&#65289;&#23637;&#31034;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#32463;&#36807;&#24494;&#35843;&#21644;&#29616;&#25104;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65307;&#65288;iii&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#26694;&#26550;&#65292;&#23558;&#36825;&#20010;&#36328;&#35821;&#35328;&#28548;&#28165;&#26816;&#32034;&#38382;&#39064;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of retrieving already debunked narratives aims to detect stories that have already been fact-checked. The successful detection of claims that have already been debunked not only reduces the manual efforts of professional fact-checkers but can also contribute to slowing the spread of misinformation. Mainly due to the lack of readily available data, this is an understudied problem, particularly when considering the cross-lingual task, i.e. the retrieval of fact-checking articles in a language different from the language of the online post being checked. This paper fills this gap by (i) creating a novel dataset to enable research on cross-lingual retrieval of already debunked narratives, using tweets as queries to a database of fact-checking articles; (ii) presenting an extensive experiment to benchmark fine-tuned and off-the-shelf multilingual pre-trained Transformer models for this task; and (iii) proposing a novel multistage framework that divides this cross-lingual debunk ret
&lt;/p&gt;</description></item><item><title>AST-MHSA&#27169;&#22411;&#36890;&#36807;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#20174;AST&#20013;&#25552;&#21462;&#37325;&#35201;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#29983;&#25104;&#28304;&#20195;&#30721;&#30340;&#31616;&#26126;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.05646</link><description>&lt;p&gt;
AST-MHSA: &#21033;&#29992;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#36827;&#34892;&#20195;&#30721;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
AST-MHSA : Code Summarization using Multi-Head Self-Attention. (arXiv:2308.05646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05646
&lt;/p&gt;
&lt;p&gt;
AST-MHSA&#27169;&#22411;&#36890;&#36807;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#20174;AST&#20013;&#25552;&#21462;&#37325;&#35201;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#29983;&#25104;&#28304;&#20195;&#30721;&#30340;&#31616;&#26126;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#25688;&#35201;&#26088;&#22312;&#20026;&#28304;&#20195;&#30721;&#29983;&#25104;&#31616;&#26126;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#20854;&#20013;&#21033;&#29992;&#28304;&#20195;&#30721;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;AST&#27604;&#23545;&#24212;&#30340;&#28304;&#20195;&#30721;&#35201;&#38271;&#24471;&#22810;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#30452;&#25509;&#23558;&#25972;&#20010;&#32447;&#24615;&#21270;&#30340;AST&#36755;&#20837;&#21040;&#32534;&#30721;&#22120;&#20013;&#26469;&#24573;&#30053;&#36825;&#20010;&#22823;&#23567;&#32422;&#26463;&#12290;&#36825;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#20351;&#24471;&#20174;&#36807;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#20013;&#25552;&#21462;&#30495;&#27491;&#26377;&#20215;&#20540;&#30340;&#20381;&#36182;&#20851;&#31995;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#30001;&#20110;&#23545;AST&#20013;&#30340;&#25152;&#26377;&#33410;&#28857;&#24212;&#29992;&#33258;&#27880;&#24847;&#21147;&#65292;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;AST-MHSA&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#20174;AST&#20013;&#25552;&#21462;&#37325;&#35201;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#12290;&#32534;&#30721;&#22120;&#20197;&#20195;&#30721;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;
&lt;/p&gt;
&lt;p&gt;
Code summarization aims to generate concise natural language descriptions for source code. The prevailing approaches adopt transformer-based encoder-decoder architectures, where the Abstract Syntax Tree (AST) of the source code is utilized for encoding structural information. However, ASTs are much longer than the corresponding source code, and existing methods ignore this size constraint by directly feeding the entire linearized AST into the encoders. This simplistic approach makes it challenging to extract truly valuable dependency relations from the overlong input sequence and leads to significant computational overhead due to self-attention applied to all nodes in the AST.  To address this issue effectively and efficiently, we present a model, AST-MHSA that uses multi-head attention to extract the important semantic information from the AST. The model consists of two main components: an encoder and a decoder. The encoder takes as input the abstract syntax tree (AST) of the code and
&lt;/p&gt;</description></item><item><title>IIHT&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22522;&#20110;&#22270;&#20687;&#21040;&#25351;&#31034;&#22120;&#23618;&#27425;Transformer&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#21462;&#21307;&#23398;&#22270;&#20687;&#30340;&#29305;&#24449;&#24182;&#29983;&#25104;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#25351;&#31034;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.05633</link><description>&lt;p&gt;
IIHT: &#22522;&#20110;&#22270;&#20687;&#21040;&#25351;&#31034;&#22120;&#23618;&#27425;Transformer&#30340;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
IIHT: Medical Report Generation with Image-to-Indicator Hierarchical Transformer. (arXiv:2308.05633v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05633
&lt;/p&gt;
&lt;p&gt;
IIHT&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22522;&#20110;&#22270;&#20687;&#21040;&#25351;&#31034;&#22120;&#23618;&#27425;Transformer&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#21462;&#21307;&#23398;&#22270;&#20687;&#30340;&#29305;&#24449;&#24182;&#29983;&#25104;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#25351;&#31034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#22312;&#21307;&#30103;&#20998;&#26512;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23427;&#21487;&#20197;&#20135;&#29983;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#25551;&#36848;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#20943;&#36731;&#21307;&#29983;&#30340;&#24037;&#20316;&#12290;&#21463;&#21040;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21644;&#22270;&#20687;&#25551;&#36848;&#30340;&#24040;&#22823;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#21253;&#25324;&#25968;&#25454;&#19981;&#24179;&#34913;&#12289;&#25253;&#21578;&#24207;&#21015;&#30340;&#38271;&#24230;&#21644;&#30456;&#20851;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#29983;&#25104;&#30340;&#25253;&#21578;&#21487;&#33021;&#22312;&#35821;&#35328;&#27969;&#30021;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#36275;&#22815;&#30340;&#20020;&#24202;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#21040;&#25351;&#31034;&#22120;&#23618;&#27425;Transformer&#30340;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#26694;&#26550;&#65288;IIHT&#65289;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65292;&#21363;&#20998;&#31867;&#22120;&#27169;&#22359;&#12289;&#25351;&#31034;&#22120;&#25193;&#23637;&#27169;&#22359;&#21644;&#29983;&#25104;&#22120;&#27169;&#22359;&#12290;&#20998;&#31867;&#22120;&#27169;&#22359;&#39318;&#20808;&#20174;&#36755;&#20837;&#30340;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#29983;&#25104;&#19982;&#20854;&#23545;&#24212;&#29366;&#24577;&#30340;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#25351;&#31034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated medical report generation has become increasingly important in medical analysis. It can produce computer-aided diagnosis descriptions and thus significantly alleviate the doctors' work. Inspired by the huge success of neural machine translation and image captioning, various deep learning methods have been proposed for medical report generation. However, due to the inherent properties of medical data, including data imbalance and the length and correlation between report sequences, the generated reports by existing methods may exhibit linguistic fluency but lack adequate clinical accuracy. In this work, we propose an image-to-indicator hierarchical transformer (IIHT) framework for medical report generation. It consists of three modules, i.e., a classifier module, an indicator expansion module and a generator module. The classifier module first extracts image features from the input medical images and produces disease-related indicators with their corresponding states. The dise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LASIGE&#21644;UNICAGE&#22312;NASA LitCoin NLP&#31454;&#36187;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#20135;&#19994;&#30028;&#30340;&#25968;&#25454;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#19982;&#23398;&#26415;&#30028;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#25972;&#21512;&#65292;&#25104;&#21151;&#22320;&#22312;&#22823;&#35268;&#27169;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.05609</link><description>&lt;p&gt;
LASIGE&#21644;UNICAGE&#35299;&#20915;NASA LitCoin NLP&#31454;&#36187;&#30340;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
LASIGE and UNICAGE solution to the NASA LitCoin NLP Competition. (arXiv:2308.05609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LASIGE&#21644;UNICAGE&#22312;NASA LitCoin NLP&#31454;&#36187;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#20135;&#19994;&#30028;&#30340;&#25968;&#25454;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#19982;&#23398;&#26415;&#30028;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#25972;&#21512;&#65292;&#25104;&#21151;&#22320;&#22312;&#22823;&#35268;&#27169;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24448;&#24448;&#21464;&#24471;&#32321;&#29712;&#65292;&#24448;&#24448;&#26159;&#22240;&#20026;&#38656;&#35201;&#22788;&#29702;&#30340;&#25991;&#26412;&#25968;&#37327;&#21644;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#34892;&#19994;&#19981;&#26029;&#24320;&#21457;&#39640;&#25928;&#30340;&#24037;&#20855;&#24182;&#21019;&#24314;&#26356;&#28789;&#27963;&#30340;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#34892;&#19994;&#25968;&#25454;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#19982;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;LasigeUnicage_NER&#65289;&#21644;&#20851;&#31995;&#25277;&#21462;&#65288;BiOnt&#65289;&#30340;&#23398;&#26415;&#31995;&#32479;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#21453;&#26144;&#20102;&#36825;&#20123;&#32452;&#20214;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#21644;&#29983;&#29289;&#21307;&#23398;&#26412;&#20307;&#30340;&#39069;&#22806;&#35757;&#32451;&#25968;&#25454;&#30340;&#22806;&#37096;&#30693;&#35782;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#22312;2022&#24180;LitCoin NLP&#25361;&#25112;&#36187;&#20013;&#20351;&#29992;&#20102;&#36825;&#20010;&#27969;&#27700;&#32447;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;LasigeUnicage&#33719;&#24471;&#20102;&#31532;&#19971;&#21517;&#22870;&#39033;&#65292;&#32422;&#26377;200&#20010;&#21442;&#36187;&#22242;&#38431;&#65292;&#21453;&#26144;&#20102;&#23398;&#26415;&#30028;&#65288;LASIGE&#65289;&#21644;&#20135;&#19994;&#30028;&#65288;Unicage&#65289;&#20043;&#38388;&#30340;&#25104;&#21151;&#21512;&#20316;&#12290;&#25903;&#25345;&#36825;&#39033;&#24037;&#20316;&#30340;&#36719;&#20214;&#21487;&#22312;
&lt;/p&gt;
&lt;p&gt;
Biomedical Natural Language Processing (NLP) tends to become cumbersome for most researchers, frequently due to the amount and heterogeneity of text to be processed. To address this challenge, the industry is continuously developing highly efficient tools and creating more flexible engineering solutions. This work presents the integration between industry data engineering solutions for efficient data processing and academic systems developed for Named Entity Recognition (LasigeUnicage\_NER) and Relation Extraction (BiOnt). Our design reflects an integration of those components with external knowledge in the form of additional training data from other datasets and biomedical ontologies. We used this pipeline in the 2022 LitCoin NLP Challenge, where our team LasigeUnicage was awarded the 7th Prize out of approximately 200 participating teams, reflecting a successful collaboration between the academia (LASIGE) and the industry (Unicage). The software supporting this work is available at \
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#23398;&#20064;&#26469;&#35299;&#20915;&#22312;&#32447;&#26377;&#27602;&#20869;&#23481;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#27602;&#24615;&#20998;&#31867;&#12289;&#27602;&#24615;&#36328;&#24230;&#26816;&#27979;&#21644;&#21435;&#27602;&#21270;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.05596</link><description>&lt;p&gt;
&#21482;&#38656;&#19968;&#27425;&#25552;&#31034;&#65306;&#20851;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#23398;&#20064;&#22312;&#35299;&#20915;&#26377;&#27602;&#20869;&#23481;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content. (arXiv:2308.05596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#23398;&#20064;&#26469;&#35299;&#20915;&#22312;&#32447;&#26377;&#27602;&#20869;&#23481;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#27602;&#24615;&#20998;&#31867;&#12289;&#27602;&#24615;&#36328;&#24230;&#26816;&#27979;&#21644;&#21435;&#27602;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#26377;&#23475;&#20869;&#23481;&#30340;&#20256;&#25773;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#23545;&#22312;&#32447;&#29992;&#25143;&#20307;&#39564;&#21644;&#31038;&#20250;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#21463;&#21040;&#36825;&#20010;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#21644;&#24433;&#21709;&#30340;&#21551;&#21457;&#65292;&#30740;&#31350;&#38598;&#20013;&#20110;&#24320;&#21457;&#35299;&#20915;&#26041;&#26696;&#26469;&#26816;&#27979;&#26377;&#27602;&#20869;&#23481;&#65292;&#36890;&#24120;&#21033;&#29992;&#22312;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#21162;&#21147;&#24456;&#37325;&#35201;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#65292;&#24182;&#19988;&#26080;&#27861;&#24212;&#23545;&#26032;&#30340;&#36235;&#21183;&#65288;&#20363;&#22914;&#65292;&#26032;&#30340;&#26377;&#27602;&#26415;&#35821;&#30340;&#20986;&#29616;&#65289;&#12290;&#30446;&#21069;&#65292;&#25105;&#20204;&#27491;&#22312;&#30446;&#30585;&#35299;&#20915;&#22312;&#32447;&#31038;&#20250;&#38382;&#39064;&#30340;&#26041;&#27861;&#21457;&#29983;&#21464;&#21270;&#65292;&#29305;&#21035;&#26159;&#21033;&#29992;&#20687;GPT-3&#25110;T5&#36825;&#26679;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20855;&#26377;&#24456;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#23398;&#20064;&#26469;&#35299;&#20915;&#26377;&#27602;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#19977;&#20010;&#20219;&#21153;&#65306;1&#65289;&#27602;&#24615;&#20998;&#31867;&#65292;2&#65289;&#27602;&#24615;&#36328;&#24230;&#26816;&#27979;&#65292;3&#65289;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#23545;&#20116;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
The spread of toxic content online is an important problem that has adverse effects on user experience online and in our society at large. Motivated by the importance and impact of the problem, research focuses on developing solutions to detect toxic content, usually leveraging machine learning (ML) models trained on human-annotated datasets. While these efforts are important, these models usually do not generalize well and they can not cope with new trends (e.g., the emergence of new toxic terms). Currently, we are witnessing a shift in the approach to tackling societal issues online, particularly leveraging large language models (LLMs) like GPT-3 or T5 that are trained on vast corpora and have strong generalizability. In this work, we investigate how we can use LLMs and prompt learning to tackle the problem of toxic content, particularly focusing on three tasks; 1) Toxicity Classification, 2) Toxic Span Detection, and 3) Detoxification. We perform an extensive evaluation over five mo
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25351;&#31216;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20511;&#37492;&#35821;&#35328;&#21746;&#23398;&#22806;&#37096;&#20027;&#20041;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#25552;&#20986;&#20102;LMs&#21487;&#20197;&#25351;&#31216;&#30340;&#29702;&#30001;&#12290;</title><link>http://arxiv.org/abs/2308.05576</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25351;&#31216;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Language Models Refer?. (arXiv:2308.05576v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05576
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25351;&#31216;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20511;&#37492;&#35821;&#35328;&#21746;&#23398;&#22806;&#37096;&#20027;&#20041;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#25552;&#20986;&#20102;LMs&#21487;&#20197;&#25351;&#31216;&#30340;&#29702;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#29992;&#35821;&#35328;&#20570;&#20160;&#20040;&#65311;&#22823;&#23478;&#37117;&#21516;&#24847;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#65288;&#22823;&#37096;&#20998;&#65289;&#36830;&#36143;&#30340;&#21477;&#23376;&#12290;&#20294;&#26159;&#23427;&#20204;&#29992;&#36825;&#20123;&#23383;&#31526;&#20018;&#34920;&#36798;&#20102;&#20160;&#20040;&#65292;&#36824;&#26159;&#21482;&#26159;&#20197;&#19968;&#31181;&#20196;&#20154;&#20449;&#26381;&#30340;&#35821;&#35328;&#36816;&#29992;&#30340;&#27169;&#25311;&#20013;&#32993;&#35328;&#20081;&#35821;&#65311;&#36825;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#38382;&#39064;&#65292;&#26377;&#35768;&#22810;&#26041;&#27861;&#21487;&#20197;&#20351;&#20854;&#26126;&#30830;&#21270;&#12290;&#36825;&#37324;&#25105;&#20204;&#23558;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#21363;&#65292;LMs&#30340;&#35789;&#35821;&#26159;&#21542;&#25351;&#31216;&#65306;&#21363;&#65292;LMs&#30340;&#36755;&#20986;&#26159;&#21542;&#23454;&#29616;&#20102;&#8220;&#35789;&#35821;-&#19990;&#30028;&#8221;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#26377;&#21021;&#27493;&#30340;&#29702;&#30001;&#35748;&#20026;&#23427;&#20204;&#19981;&#20855;&#22791;&#25351;&#31216;&#33021;&#21147;&#65292;&#22240;&#20026;LMs&#27809;&#26377;&#20687;&#26222;&#36890;&#35821;&#35328;&#29992;&#25143;&#37027;&#26679;&#19982;&#19990;&#30028;&#20114;&#21160;&#12290;&#20511;&#37492;&#35821;&#35328;&#21746;&#23398;&#30340;&#22806;&#37096;&#20027;&#20041;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#35748;&#20026;&#34920;&#35937;&#26159;&#35823;&#23548;&#30340;&#65292;&#26377;&#20805;&#20998;&#30340;&#29702;&#30001;&#35748;&#20026;LMs&#21487;&#20197;&#25351;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;
What do language models (LMs) do with language? Everyone agrees that they produce sequences of (mostly) coherent sentences. But are they saying anything with those strings or simply babbling in a convincing simulacrum of language use? This is a vague question, and there are many ways of making it precise. Here we will address one aspect of the question, namely, whether LMs' words refer: that is, whether the outputs of LMs achieve "word-to-world" connections. There is prima facie reason to think they do not since LMs do not interact with the world in the way that ordinary language users do. Drawing on insights from the externalist tradition in philosophy of language, we argue that appearances are misleading and that there is good reason to think that LMs can refer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#24503;&#25289;&#32500;&#24503;&#35821;&#31995;&#22810;&#35821;&#35328;&#32763;&#35793;&#20013;&#65292;&#21033;&#29992;&#38899;&#35793;&#21644;&#35821;&#35328;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#35299;&#20915;&#38646;&#26679;&#26412;&#32763;&#35793;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#21333;&#19968;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#35780;&#20272;&#20854;&#24615;&#33021;&#19982;&#22522;&#20110;&#20013;&#38388;&#35821;&#35328;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#36824;&#39564;&#35777;&#20102;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#26159;&#21542;&#38656;&#35201;&#36739;&#22823;&#30340;&#35789;&#27719;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.05574</link><description>&lt;p&gt;
&#25506;&#32034;&#35821;&#35328;&#30456;&#20284;&#24615;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#22312;&#24503;&#25289;&#32500;&#24503;&#35821;&#31995;&#22810;&#35821;&#35328;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Linguistic Similarity and Zero-Shot Learning for Multilingual Translation of Dravidian Languages. (arXiv:2308.05574v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#24503;&#25289;&#32500;&#24503;&#35821;&#31995;&#22810;&#35821;&#35328;&#32763;&#35793;&#20013;&#65292;&#21033;&#29992;&#38899;&#35793;&#21644;&#35821;&#35328;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#35299;&#20915;&#38646;&#26679;&#26412;&#32763;&#35793;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#21333;&#19968;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#35780;&#20272;&#20854;&#24615;&#33021;&#19982;&#22522;&#20110;&#20013;&#38388;&#35821;&#35328;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#36824;&#39564;&#35777;&#20102;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#26159;&#21542;&#38656;&#35201;&#36739;&#22823;&#30340;&#35789;&#27719;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#32763;&#35793;&#30340;&#24403;&#21069;&#30740;&#31350;&#23384;&#22312;&#35832;&#22810;&#38382;&#39064;&#65292;&#22914;&#35745;&#31639;&#38656;&#27714;&#39640;&#12289;&#35757;&#32451;&#26102;&#38388;&#22686;&#21152;&#20197;&#21450;&#32763;&#35793;&#20559;&#31163;&#30446;&#26631;&#31561;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#25110;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#20013;&#38388;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27604;&#21333;&#19968;&#32534;&#30721;&#22120;&#27169;&#22411;&#26356;&#21463;&#38738;&#30544;&#65292;&#23613;&#31649;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#38388;&#20250;&#22686;&#21152;&#12290;&#26412;&#25991;&#21033;&#29992;&#38899;&#35793;&#21644;&#35821;&#35328;&#30456;&#20284;&#24615;&#20811;&#26381;&#20102;&#38646;&#26679;&#26412;&#32763;&#35793;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21333;&#19968;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#29992;&#20110;&#24503;&#25289;&#32500;&#24503;&#35821;&#31995;&#20043;&#38388;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#65292;&#24182;&#36827;&#34892;&#38646;&#26679;&#26412;&#32763;&#35793;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25968;&#25454;&#19982;&#38646;&#26679;&#26412;&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#35780;&#20272;&#25105;&#20204;&#30340;&#22522;&#26412;&#26041;&#27861;&#19982;&#29616;&#26377;&#22522;&#20110;&#20013;&#38388;&#35821;&#35328;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#25216;&#26415;&#38480;&#21046;&#35789;&#27719;&#34920;&#65292;&#39564;&#35777;&#20102;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#26159;&#21542;&#38656;&#35201;&#36739;&#22823;&#30340;&#35789;&#27719;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current research in zero-shot translation is plagued by several issues such as high compute requirements, increased training time and off target translations. Proposed remedies often come at the cost of additional data or compute requirements. Pivot based neural machine translation is preferred over a single-encoder model for most settings despite the increased training and evaluation time. In this work, we overcome the shortcomings of zero-shot translation by taking advantage of transliteration and linguistic similarity. We build a single encoder-decoder neural machine translation system for Dravidian-Dravidian multilingual translation and perform zero-shot translation. We compare the data vs zero-shot accuracy tradeoff and evaluate the performance of our vanilla method against the current state of the art pivot based method. We also test the theory that morphologically rich languages require large vocabularies by restricting the vocabulary using an optimal transport based technique. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#25991;&#31456;&#26088;&#22312;&#31361;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#20197;&#21450;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05502</link><description>&lt;p&gt;
&#23558;&#39034;&#24207;&#24102;&#20837;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#27861;&#24459;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bringing order into the realm of Transformer-based language models for artificial intelligence and law. (arXiv:2308.05502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#25991;&#31456;&#26088;&#22312;&#31361;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#20197;&#21450;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;TLM&#65289;&#34987;&#24191;&#27867;&#35748;&#21487;&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#25104;&#21151;&#24320;&#21457;&#20986;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38656;&#35201;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#21644;&#24212;&#29992;&#12290;&#19982;&#20854;&#20182;&#25991;&#26412;&#39046;&#22495;&#19968;&#26679;&#65292;TLM&#30830;&#23454;&#25512;&#21160;&#20102;&#27861;&#24459;&#39046;&#22495;&#35768;&#22810;&#24863;&#20852;&#36259;&#20219;&#21153;&#23545;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#23613;&#31649;&#31532;&#19968;&#20010;Transformer&#27169;&#22411;&#25552;&#20986;&#20102;&#22823;&#32422;6&#24180;&#26102;&#38388;&#65292;&#20294;&#36825;&#39033;&#25216;&#26415;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#36805;&#29467;&#21457;&#23637;&#65292;BERT&#21644;&#30456;&#20851;&#27169;&#22411;&#25104;&#20026;&#20027;&#35201;&#21442;&#32771;&#65292;&#20063;&#22312;&#27861;&#24459;&#39046;&#22495;&#21344;&#26377;&#37325;&#35201;&#22320;&#20301;&#12290;&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#27010;&#36848;&#20102;TLM&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#31361;&#20986;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#20197;&#20415;&#19968;&#26041;&#38754;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#21462;&#24471;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#26159;&#20160;&#20040;&#65292;&#21478;&#19968;&#26041;&#38754;&#20102;&#35299;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#26159;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitati
&lt;/p&gt;</description></item><item><title>LLM&#21464;&#25104;DBA&#65292;&#25552;&#20379;&#25968;&#25454;&#24211;&#32500;&#25252;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#32463;&#39564;&#21644;&#22810;&#20010;LLMs&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2308.05481</link><description>&lt;p&gt;
LLM&#21464;&#25104;DBA
&lt;/p&gt;
&lt;p&gt;
LLM As DBA. (arXiv:2308.05481v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05481
&lt;/p&gt;
&lt;p&gt;
LLM&#21464;&#25104;DBA&#65292;&#25552;&#20379;&#25968;&#25454;&#24211;&#32500;&#25252;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#32463;&#39564;&#21644;&#22810;&#20010;LLMs&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24211;&#31649;&#29702;&#21592;&#65288;DBA&#65289;&#22312;&#31649;&#29702;&#12289;&#32500;&#25252;&#21644;&#20248;&#21270;&#25968;&#25454;&#24211;&#31995;&#32479;&#20197;&#30830;&#20445;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;DBA&#26469;&#35828;&#65292;&#31649;&#29702;&#22823;&#37327;&#25968;&#25454;&#24211;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#20113;&#25968;&#25454;&#24211;&#19978;&#30340;&#25968;&#30334;&#19975;&#20010;&#23454;&#20363;&#65289;&#26159;&#22256;&#38590;&#21644;&#32321;&#29712;&#30340;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#29702;&#35299;&#26377;&#20215;&#20540;&#25991;&#20214;&#24182;&#29983;&#25104;&#21512;&#29702;&#31572;&#26696;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;D-Bot&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#24211;&#31649;&#29702;&#21592;&#65292;&#23427;&#21487;&#20197;&#25345;&#32493;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#25968;&#25454;&#24211;&#32500;&#25252;&#32463;&#39564;&#65292;&#24182;&#20026;&#30446;&#26631;&#25968;&#25454;&#24211;&#25552;&#20379;&#21512;&#29702;&#12289;&#26377;&#29702;&#12289;&#21450;&#26102;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38761;&#21629;&#24615;&#30340;&#20197;LLM&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#24211;&#32500;&#25252;&#26694;&#26550;&#65292;&#21253;&#25324;&#65288;i&#65289;&#20174;&#25991;&#26723;&#21644;&#24037;&#20855;&#20013;&#26816;&#27979;&#25968;&#25454;&#24211;&#32500;&#25252;&#30693;&#35782;&#65292;&#65288;ii&#65289;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#24605;&#32500;&#26641;&#65292;&#21644;&#65288;iii&#65289;&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05476</link><description>&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#29992;&#20110;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#65306;&#19968;&#39033;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis. (arXiv:2308.05476v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#27450;&#35784;&#25110;&#27450;&#39575;&#24615;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#29992;&#20110;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;Transformer&#27169;&#22411;&#65288;&#22914;BERT&#65292;XLNET&#65292;DistilBERT&#21644;RoBERTa&#65289;&#22312;&#26816;&#27979;&#27450;&#35784;&#24615;&#25991;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#27450;&#35784;&#24615;&#21644;&#38750;&#27450;&#35784;&#24615;&#25991;&#26412;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30446;&#30340;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#21253;&#25324;&#20934;&#30830;&#29575;&#65292;&#31934;&#30830;&#29575;&#65292;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#27450;&#35784;&#24615;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#33021;&#22815;&#22312;&#22788;&#29702;&#27450;&#35784;&#20869;&#23481;&#26102;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deceptive text classification is a critical task in natural language processing that aims to identify deceptive or fraudulent content. This study presents a comparative analysis of machine learning and transformer-based approaches for deceptive text classification. We investigate the effectiveness of traditional machine learning algorithms and state-of-the-art transformer models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive text. A labeled dataset consisting of deceptive and non-deceptive texts is used for training and evaluation purposes. Through extensive experimentation, we compare the performance metrics, including accuracy, precision, recall, and F1 score, of the different approaches. The results of this study shed light on the strengths and limitations of machine learning and transformer-based methods for deceptive text classification, enabling researchers and practitioners to make informed decisions when dealing with deceptive content
&lt;/p&gt;</description></item><item><title>WeaverBird&#26159;&#19968;&#20010;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;&#26234;&#33021;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#26412;&#22320;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#65292;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#37329;&#34701;&#26597;&#35810;&#24182;&#25552;&#20379;&#26126;&#26234;&#30340;&#22238;&#31572;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.05361</link><description>&lt;p&gt;
WeaverBird: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#22686;&#24378;&#37329;&#34701;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine. (arXiv:2308.05361v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05361
&lt;/p&gt;
&lt;p&gt;
WeaverBird&#26159;&#19968;&#20010;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;&#26234;&#33021;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#26412;&#22320;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#65292;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#37329;&#34701;&#26597;&#35810;&#24182;&#25552;&#20379;&#26126;&#26234;&#30340;&#22238;&#31572;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;WeaverBird&#65292;&#19968;&#20010;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;&#26234;&#33021;&#23545;&#35805;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21033;&#29992;GPT&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#37329;&#34701;&#30456;&#20851;&#25991;&#26412;&#30340;&#24191;&#27867;&#35821;&#26009;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#37329;&#34701;&#26597;&#35810;&#65292;&#20363;&#22914;&#8220;&#22312;&#36890;&#36135;&#33192;&#32960;&#26399;&#38388;&#22914;&#20309;&#31649;&#29702;&#25105;&#30340;&#25237;&#36164;&#65311;&#8221;&#24182;&#25552;&#20379;&#26126;&#26234;&#30340;&#22238;&#31572;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#38598;&#25104;&#20102;&#26412;&#22320;&#30340;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#20197;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#12290;&#26368;&#32456;&#30340;&#22238;&#31572;&#26159;&#22522;&#20110;&#25628;&#32034;&#32467;&#26524;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#30340;&#65292;&#24182;&#21253;&#21547;&#36866;&#24403;&#30340;&#24341;&#29992;&#26469;&#28304;&#65292;&#20174;&#32780;&#20855;&#26377;&#22686;&#24378;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#19982;&#37329;&#34701;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24050;&#32463;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#30456;&#27604;&#20854;&#20182;&#27169;&#22411;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#29992;&#25143;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;&#22312;&#32447;&#28436;&#31034;&#32593;&#31449;https://weaverbird.ttic.edu&#19982;&#25105;&#20204;&#30340;&#31995;&#32479;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#35266;&#30475;&#25105;&#20204;&#30340;2&#20998;&#38047;&#28436;&#31034;&#35270;&#39057;https://www.youtube.com/watch?v=yofgeq&#12290;
&lt;/p&gt;
&lt;p&gt;
We present WeaverBird, an intelligent dialogue system designed specifically for the finance domain. Our system harnesses a large language model of GPT architecture that has been tuned using extensive corpora of finance-related text. As a result, our system possesses the capability to understand complex financial queries, such as "How should I manage my investments during inflation?", and provide informed responses. Furthermore, our system incorporates a local knowledge base and a search engine to retrieve relevant information. The final responses are conditioned on the search results and include proper citations to the sources, thus enjoying an enhanced credibility. Through a range of finance-related questions, we have demonstrated the superior performance of our system compared to other models. To experience our system firsthand, users can interact with our live demo at https://weaverbird.ttic.edu, as well as watch our 2-min video illustration at https://www.youtube.com/watch?v=yofgeq
&lt;/p&gt;</description></item><item><title>&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.05342</link><description>&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05342
&lt;/p&gt;
&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20013;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#19968;&#30452;&#22312;&#19981;&#26029;&#25552;&#39640;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;&#25552;&#31034;&#30340;&#30740;&#31350;&#22686;&#24378;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20803;&#35748;&#30693;&#25552;&#31034; (MP)&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#20154;&#31867;&#20869;&#30465;&#25512;&#29702;&#36807;&#31243;&#21551;&#21457;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;MP&#65292;LLMs&#32463;&#21382;&#19968;&#31995;&#21015;&#26377;&#32467;&#26500;&#12289;&#33258;&#25105;&#24847;&#35782;&#30340;&#35780;&#20272;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#22312;&#30693;&#35782;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#20116;&#20010;&#24120;&#35265;&#30340;LLMs&#65306;Llama2&#12289;Vicuna&#12289;PaLM&#12289;GPT-3.5&#21644;GPT-4&#65292;&#23427;&#20204;&#37117;&#28085;&#30422;&#20102;&#26469;&#33258;GLUE&#21644;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#21508;&#31181;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299; (NLU) &#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#37197;&#22791;MP&#30340;PaLM&#25509;&#36817;&#20854;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#36328;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;MP&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. While recent research on prompting has enhanced the reasoning capabilities of LLMs, a gap remains in further improving their understanding abilities. In this study, we introduce metacognitive prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. Our experiments involve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general natural language understanding (NLU) tasks from the GLUE and SuperGLUE benchmarks. Results indicate that, although GPT-4 consistently excels in most tasks, PaLM, when equipped with MP, approaches its performance level. Furthermore, across models and datasets, MP consistently outperforms existing prompting meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20998;&#31867;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#20197;&#38590;&#20197;&#34987;&#20154;&#31867;&#36776;&#35748;&#30340;&#26041;&#24335;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#26102;&#30340;&#26356;&#39640;&#32423;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31995;&#32479;&#22312;&#21306;&#20998;&#22522;&#30784;&#21644;&#39640;&#32423;&#20154;&#24037;&#29983;&#25104;/&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26102;&#30340;F1&#20998;&#25968;&#36229;&#36807;96%&#65292;&#22312;&#21306;&#20998;&#22522;&#30784;&#21644;&#39640;&#32423;&#20154;&#24037;&#29983;&#25104;/&#20154;&#24037;&#26234;&#33021;&#37325;&#26032;&#34920;&#36798;&#25991;&#26412;&#26102;&#30340;F1&#20998;&#25968;&#36229;&#36807;78%&#12290;</title><link>http://arxiv.org/abs/2308.05341</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#31867;&#65306;&#25506;&#32034;ChatGPT&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT. (arXiv:2308.05341v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20998;&#31867;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#20197;&#38590;&#20197;&#34987;&#20154;&#31867;&#36776;&#35748;&#30340;&#26041;&#24335;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#26102;&#30340;&#26356;&#39640;&#32423;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31995;&#32479;&#22312;&#21306;&#20998;&#22522;&#30784;&#21644;&#39640;&#32423;&#20154;&#24037;&#29983;&#25104;/&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26102;&#30340;F1&#20998;&#25968;&#36229;&#36807;96%&#65292;&#22312;&#21306;&#20998;&#22522;&#30784;&#21644;&#39640;&#32423;&#20154;&#24037;&#29983;&#25104;/&#20154;&#24037;&#26234;&#33021;&#37325;&#26032;&#34920;&#36798;&#25991;&#26412;&#26102;&#30340;F1&#20998;&#25968;&#36229;&#36807;78%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#38754;&#21521;&#20844;&#20247;&#25552;&#20379;&#12290;&#36825;&#20123;&#24037;&#20855;&#21487;&#20197;&#34987;&#23398;&#29983;&#29992;&#26469;&#29983;&#25104;&#25955;&#25991;&#25110;&#25972;&#20010;&#35770;&#25991;&#12290;&#20294;&#26159;&#32769;&#24072;&#22914;&#20309;&#30693;&#36947;&#19968;&#31687;&#25991;&#26412;&#26159;&#30001;&#23398;&#29983;&#36824;&#26159;&#30001;&#20154;&#24037;&#26234;&#33021;&#32534;&#20889;&#30340;&#65311;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20256;&#32479;&#21644;&#26032;&#30340;&#29305;&#24449;&#26469;(1)&#26816;&#27979;&#30001;&#20154;&#24037;&#26234;&#33021;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;(2)&#30001;&#20154;&#24037;&#26234;&#33021;&#37325;&#26032;&#34920;&#36798;&#30340;&#25991;&#26412;&#12290;&#30001;&#20110;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#20154;&#24037;&#26234;&#33021;&#34987;&#25351;&#31034;&#20197;&#20154;&#31867;&#38590;&#20197;&#36776;&#35748;&#30340;&#26041;&#24335;&#21019;&#24314;&#25991;&#26412;&#26102;&#65292;&#20998;&#31867;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#25105;&#20204;&#36824;&#23545;&#36825;&#31181;&#26356;&#39640;&#32423;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20026;&#20102;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#21046;&#20316;&#20102;&#28085;&#30422;10&#20010;&#23398;&#26657;&#35805;&#39064;&#30340;&#26032;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#26368;&#20339;&#30340;&#22522;&#30784;&#21644;&#39640;&#32423;&#20154;&#24037;&#29983;&#25104;/&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#30340;F1&#20998;&#25968;&#36229;&#36807;96%&#12290;&#25105;&#20204;&#26368;&#20339;&#30340;&#23545;&#22522;&#30784;&#21644;&#39640;&#32423;&#20154;&#24037;&#29983;&#25104;/&#20154;&#24037;&#26234;&#33021;&#37325;&#26032;&#34920;&#36798;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#30340;&#31995;&#32479;&#30340;F1&#20998;&#25968;&#36229;&#36807;78%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, generative AIs like ChatGPT have become available to the wide public. These tools can for instance be used by students to generate essays or whole theses. But how does a teacher know whether a text is written by a student or an AI? In our work, we explore traditional and new features to (1) detect text generated by AI from scratch and (2) text rephrased by AI. Since we found that classification is more difficult when the AI has been instructed to create the text in a way that a human would not recognize that it was generated by an AI, we also investigate this more advanced case. For our experiments, we produced a new text corpus covering 10 school topics. Our best systems to classify basic and advanced human-generated/AI-generated texts have F1-scores of over 96%. Our best systems for classifying basic and advanced human-generated/AI-rephrased texts have F1-scores of more than 78%. The systems use a combination of perplexity, semantic, list lookup, error-based, readability, A
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#24320;&#21457;&#19968;&#20010;&#38750;&#27491;&#24335;-&#27491;&#24335;&#27874;&#26031;&#35821;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#65292;&#20197;&#28385;&#36275;&#38750;&#27491;&#24335;&#27874;&#26031;&#35821;&#22788;&#29702;&#24037;&#20855;&#30340;&#38656;&#27714;&#12290;&#25910;&#38598;&#20102;5&#19975;&#20010;&#21477;&#23376;&#23545;&#65292;&#24182;&#20197;&#35789;&#35821;/&#30701;&#35821;&#32423;&#21035;&#23545;&#40784;&#65292;&#28085;&#30422;&#20102;&#38750;&#27491;&#24335;&#21644;&#27491;&#24335;&#27874;&#26031;&#35821;&#20043;&#38388;&#30340;&#21508;&#31181;&#35789;&#27719;&#21644;&#21477;&#27861;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.05336</link><description>&lt;p&gt;
&#24320;&#21457;&#19968;&#31181;&#38750;&#27491;&#24335;-&#27491;&#24335;&#27874;&#26031;&#35821;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Developing an Informal-Formal Persian Corpus. (arXiv:2308.05336v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05336
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#24320;&#21457;&#19968;&#20010;&#38750;&#27491;&#24335;-&#27491;&#24335;&#27874;&#26031;&#35821;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#65292;&#20197;&#28385;&#36275;&#38750;&#27491;&#24335;&#27874;&#26031;&#35821;&#22788;&#29702;&#24037;&#20855;&#30340;&#38656;&#27714;&#12290;&#25910;&#38598;&#20102;5&#19975;&#20010;&#21477;&#23376;&#23545;&#65292;&#24182;&#20197;&#35789;&#35821;/&#30701;&#35821;&#32423;&#21035;&#23545;&#40784;&#65292;&#28085;&#30422;&#20102;&#38750;&#27491;&#24335;&#21644;&#27491;&#24335;&#27874;&#26031;&#35821;&#20043;&#38388;&#30340;&#21508;&#31181;&#35789;&#27719;&#21644;&#21477;&#27861;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27491;&#24335;&#35821;&#35328;&#26159;&#19968;&#31181;&#22312;&#38750;&#27491;&#24335;&#22330;&#21512;&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#21475;&#22836;&#25110;&#20070;&#38754;&#35821;&#35328;&#39118;&#26684;&#65292;&#21253;&#25324;&#26085;&#24120;&#23545;&#35805;&#12289;&#31038;&#20132;&#23186;&#20307;&#12289;&#21338;&#23458;&#12289;&#30005;&#23376;&#37038;&#20214;&#21644;&#30701;&#20449;&#12290;&#22312;&#38750;&#27491;&#24335;&#20889;&#20316;&#20013;&#65292;&#35821;&#35328;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#20250;&#20986;&#29616;&#19968;&#20123;&#35789;&#27719;&#21644;/&#25110;&#21477;&#27861;&#30340;&#21464;&#21270;&#12290;&#27874;&#26031;&#35821;&#26159;&#19968;&#31181;&#27491;&#24335;&#21644;&#38750;&#27491;&#24335;&#20889;&#20316;&#39118;&#26684;&#20043;&#38388;&#23384;&#22312;&#35768;&#22810;&#24046;&#24322;&#30340;&#35821;&#35328;&#65292;&#22240;&#27492;&#20026;&#35813;&#35821;&#35328;&#24320;&#21457;&#38750;&#27491;&#24335;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#26174;&#24471;&#24517;&#35201;&#12290;&#36825;&#26679;&#30340;&#36716;&#25442;&#22120;&#38656;&#35201;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#27491;&#24335;-&#38750;&#27491;&#24335;&#21477;&#23376;&#23545;&#40784;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#24110;&#21161;&#35821;&#35328;&#23398;&#23478;&#25552;&#21462;&#38750;&#27491;&#24335;&#27874;&#26031;&#35821;&#30340;&#35268;&#33539;&#35821;&#27861;&#21644;&#27491;&#23383;&#27861;&#65292;&#23601;&#20687;&#23545;&#27491;&#24335;&#35821;&#35328;&#24050;&#32463;&#20570;&#30340;&#37027;&#26679;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;5&#19975;&#20010;&#21477;&#23376;&#23545;&#19988;&#20197;&#35789;&#35821;/&#30701;&#35821;&#32423;&#21035;&#23545;&#40784;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#12290;&#21477;&#23376;&#35797;&#22270;&#28085;&#30422;&#38750;&#27491;&#24335;&#27874;&#26031;&#35821;&#21644;&#27491;&#24335;&#27874;&#26031;&#35821;&#20043;&#38388;&#20960;&#20046;&#25152;&#26377;&#31867;&#22411;&#30340;&#35789;&#27719;&#21644;&#21477;&#27861;&#21464;&#21270;&#65292;&#22240;&#27492;&#37319;&#29992;&#20102;&#25506;&#32034;&#21644;&#25910;&#38598;&#20004;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Informal language is a style of spoken or written language frequently used in casual conversations, social media, weblogs, emails and text messages. In informal writing, the language faces some lexical and/or syntactic changes varying among different languages. Persian is one of the languages with many differences between its formal and informal styles of writing, thus developing informal language processing tools for this language seems necessary. Such a converter needs a large aligned parallel corpus of colloquial-formal sentences which can be useful for linguists to extract a regulated grammar and orthography for colloquial Persian as is done for the formal language. In this paper we explain our methodology in building a parallel corpus of 50,000 sentence pairs with alignments in the word/phrase level. The sentences were attempted to cover almost all kinds of lexical and syntactic changes between informal and formal Persian, therefore both methods of exploring and collecting from th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#32479;&#19968;&#34920;&#31034;&#21644;&#22810;&#28304;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#22810;&#20219;&#21153;&#12289;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#25913;&#21892;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#24418;&#24335;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#24403;&#21069;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23545;&#20110;&#26500;&#24314;&#26356;&#36890;&#29992;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.05317</link><description>&lt;p&gt;
&#36890;&#36807;&#32479;&#19968;&#34920;&#31034;&#21644;&#22810;&#28304;&#23398;&#20064;&#23454;&#29616;&#23569;&#26679;&#26412;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Data-to-Text Generation via Unified Representation and Multi-Source Learning. (arXiv:2308.05317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#32479;&#19968;&#34920;&#31034;&#21644;&#22810;&#28304;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#22810;&#20219;&#21153;&#12289;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#25913;&#21892;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#24418;&#24335;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#24403;&#21069;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23545;&#20110;&#26500;&#24314;&#26356;&#36890;&#29992;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#19987;&#27880;&#20110;&#29305;&#23450;&#31867;&#22411;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#24418;&#24335;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#34920;&#26684;&#12289;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#21644;&#35821;&#20041;&#34920;&#31034;&#65289;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#26088;&#22312;&#25552;&#39640;&#22810;&#20219;&#21153;&#35757;&#32451;&#12289;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22330;&#26223;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#36866;&#24212;&#26032;&#30340;&#32467;&#26500;&#21270;&#24418;&#24335;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#27604;&#36739;&#24403;&#21069;&#26041;&#27861;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22312;&#34920;&#26684;&#36755;&#20837;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36716;&#31227;&#21040;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#26102;&#65292;&#38646;&#26679;&#26412;BLEU&#24471;&#20998;&#25552;&#39640;&#20102;66%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26397;&#30528;&#26356;&#36890;&#29992;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for structured data-to-text generation that addresses the limitations of existing methods that primarily focus on specific types of structured data. Our proposed method aims to improve performance in multi-task training, zero-shot and few-shot scenarios by providing a unified representation that can handle various forms of structured data such as tables, knowledge graph triples, and meaning representations. We demonstrate that our proposed approach can effectively adapt to new structured forms, and can improve performance in comparison to current methods. For example, our method resulted in a 66% improvement in zero-shot BLEU scores when transferring models trained on table inputs to a knowledge graph dataset. Our proposed method is an important step towards a more general data-to-text generation framework.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;SIR&#27169;&#22411;&#30740;&#31350;&#20102;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#30340;&#28798;&#23475;&#21709;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#20581;&#24247;&#24433;&#21709;&#12289;&#25439;&#22833;&#21644;&#25764;&#31163;&#19977;&#20010;&#20027;&#39064;&#65292;&#24182;&#20351;&#29992;SIR&#29702;&#35770;&#25506;&#32034;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.05281</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#26131;&#24863;-&#24863;&#26579;-&#24247;&#22797;&#65288;SIR&#65289;&#27169;&#22411;&#30740;&#31350;&#28798;&#23475;&#21709;&#24212;&#65306;&#20197;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season. (arXiv:2308.05281v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;SIR&#27169;&#22411;&#30740;&#31350;&#20102;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#30340;&#28798;&#23475;&#21709;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#20581;&#24247;&#24433;&#21709;&#12289;&#25439;&#22833;&#21644;&#25764;&#31163;&#19977;&#20010;&#20027;&#39064;&#65292;&#24182;&#20351;&#29992;SIR&#29702;&#35770;&#25506;&#32034;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#28798;&#23475;&#21709;&#24212;&#23545;&#21463;&#24433;&#21709;&#30340;&#31038;&#21306;&#33267;&#20851;&#37325;&#35201;&#12290;&#24212;&#24613;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#22312;&#28798;&#23475;&#26399;&#38388;&#22312;&#20102;&#35299;&#31038;&#21306;&#25152;&#38754;&#20020;&#38382;&#39064;&#30340;&#21487;&#38752;&#21644;&#21450;&#26102;&#30340;&#25351;&#26631;&#19978;&#23558;&#21463;&#30410;&#20110;&#31038;&#20132;&#23186;&#20307;&#25552;&#20379;&#30340;&#20016;&#23500;&#25968;&#25454;&#26469;&#28304;&#12290;&#31038;&#20132;&#23186;&#20307;&#21487;&#20197;&#21453;&#26144;&#20844;&#20247;&#20851;&#27880;&#21644;&#38656;&#27714;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#65292;&#20197;&#20102;&#35299;&#19981;&#26029;&#28436;&#21464;&#30340;&#24773;&#20917;&#24182;&#20248;&#21270;&#36164;&#28304;&#37197;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#65288;BERT&#65289;&#20027;&#39064;&#24314;&#27169;&#23545;Twitter&#25968;&#25454;&#36827;&#34892;&#20027;&#39064;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#26102;&#38388;-&#31354;&#38388;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;2020&#24180;&#32654;&#22269;&#35199;&#37096;&#28779;&#28798;&#23395;&#26399;&#38388;&#22312;&#19981;&#21516;&#22320;&#21306;&#30340;&#20998;&#24067;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#19977;&#20010;&#20027;&#39064;&#65306;&#8220;&#20581;&#24247;&#24433;&#21709;&#8221;&#65292;&#8220;&#25439;&#22833;&#8221;&#65292;&#8220;&#25764;&#31163;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#26131;&#24863;-&#24863;&#26579;-&#24247;&#22797;&#65288;SIR&#65289;&#29702;&#35770;&#26469;&#25506;&#32034;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;&#32467;&#26524;&#28165;&#26224;&#22320;&#26174;&#31034;&#20102;&#20027;&#39064;&#20256;&#25773;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective disaster response is critical for affected communities. Responders and decision-makers would benefit from reliable, timely measures of the issues impacting their communities during a disaster, and social media offers a potentially rich data source. Social media can reflect public concerns and demands during a disaster, offering valuable insights for decision-makers to understand evolving situations and optimize resource allocation. We used Bidirectional Encoder Representations from Transformers (BERT) topic modeling to cluster topics from Twitter data. Then, we conducted a temporal-spatial analysis to examine the distribution of these topics across different regions during the 2020 western U.S. wildfire season. Our results show that Twitter users mainly focused on three topics:"health impact," "damage," and "evacuation." We used the Susceptible-Infected-Recovered (SIR) theory to explore the magnitude and velocity of topic diffusion on Twitter. The results displayed a clear re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#20934;&#30830;&#24230;&#30340;&#20266;&#26631;&#31614;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#21333;&#35789;&#38169;&#35823;&#29575;&#65292;&#30456;&#23545;&#25913;&#36827;&#36798;&#21040;14.94%&#12290;</title><link>http://arxiv.org/abs/2308.05269</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Novel Self-training Approach for Low-resource Speech Recognition. (arXiv:2308.05269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#20934;&#30830;&#24230;&#30340;&#20266;&#26631;&#31614;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#21333;&#35789;&#38169;&#35823;&#29575;&#65292;&#30456;&#23545;&#25913;&#36827;&#36798;&#21040;14.94%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#23613;&#31649;&#33258;&#35757;&#32451;&#26041;&#27861;&#22312;&#33521;&#35821;&#31561;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#21457;&#23637;&#21644;&#35780;&#20272;&#65292;&#20294;&#22312;&#20687;&#26049;&#36974;&#26222;&#35821;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24212;&#29992;&#21364;&#24456;&#26377;&#38480;&#65292;&#23613;&#31649;&#26049;&#36974;&#26222;&#35821;&#34987;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#21475;&#20351;&#29992;&#12290;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#38459;&#30861;&#20102;&#20934;&#30830;ASR&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#26049;&#36974;&#26222;&#35821;&#21644;&#27611;&#21033;&#35821;&#65289;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#20026;&#26080;&#26631;&#31614;&#30340;&#20302;&#36164;&#28304;&#35821;&#38899;&#29983;&#25104;&#39640;&#20934;&#30830;&#24230;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#21333;&#35789;&#38169;&#35823;&#29575;&#65292;&#23454;&#29616;&#20102;14.94%&#30340;&#30456;&#23545;&#25913;&#36827;&#65292;&#21516;&#26102;&#22312;&#22235;&#20010;&#30495;&#23454;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a self-training approach for automatic speech recognition (ASR) for low-resource settings. While self-training approaches have been extensively developed and evaluated for high-resource languages such as English, their applications to low-resource languages like Punjabi have been limited, despite the language being spoken by millions globally. The scarcity of annotated data has hindered the development of accurate ASR systems, especially for low-resource languages (e.g., Punjabi and M\=aori languages). To address this issue, we propose an effective self-training approach that generates highly accurate pseudo-labels for unlabeled low-resource speech. Our experimental analysis demonstrates that our approach significantly improves word error rate, achieving a relative improvement of 14.94% compared to a baseline model across four real speech datasets. Further, our proposed approach reports the best results on the Common Voice Punjabi dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#36716;&#25442;&#22120;&#20013;&#35782;&#21035;&#25991;&#26412;&#26174;&#33879;&#24615;&#30340;&#31574;&#30053;&#65292;&#24182;&#36866;&#24212;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#26174;&#33879;&#24615;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19968;&#33268;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.05219</link><description>&lt;p&gt;
&#35821;&#35328;&#36716;&#25442;&#22120;&#20013;&#30340;&#23618;&#26174;&#33879;&#24615;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decoding Layer Saliency in Language Transformers. (arXiv:2308.05219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#36716;&#25442;&#22120;&#20013;&#35782;&#21035;&#25991;&#26412;&#26174;&#33879;&#24615;&#30340;&#31574;&#30053;&#65292;&#24182;&#36866;&#24212;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#26174;&#33879;&#24615;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19968;&#33268;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#20013;&#35782;&#21035;&#25991;&#26412;&#26174;&#33879;&#24615;&#30340;&#31574;&#30053;&#12290;&#22312;&#35270;&#35273;&#32593;&#32476;&#20013;&#65292;&#26174;&#33879;&#24615;&#24448;&#24448;&#36890;&#36807;&#21367;&#31215;&#23618;&#33258;&#28982;&#22320;&#36827;&#34892;&#23450;&#20301;&#65292;&#28982;&#32780;&#65292;&#22312;&#29992;&#20110;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#30340;&#29616;&#20195;transformer-stack&#32593;&#32476;&#20013;&#65292;&#24182;&#38750;&#22914;&#27492;&#12290;&#25105;&#20204;&#20026;&#36825;&#20123;&#32593;&#32476;&#36866;&#24212;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#26174;&#33879;&#24615;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#27599;&#23618;&#35821;&#20041;&#19968;&#33268;&#24615;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#19982;&#20854;&#20182;&#22810;&#31181;&#25991;&#26412;&#26174;&#33879;&#24615;&#26041;&#27861;&#30456;&#27604;&#30340;&#19968;&#33268;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#35775;&#38382;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#19988;&#35745;&#31639;&#25928;&#29575;&#30456;&#23545;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a strategy for identifying textual saliency in large-scale language models applied to classification tasks. In visual networks where saliency is more well-studied, saliency is naturally localized through the convolutional layers of the network; however, the same is not true in modern transformer-stack networks used to process natural language. We adapt gradient-based saliency methods for these networks, propose a method for evaluating the degree of semantic coherence of each layer, and demonstrate consistent improvement over numerous other methods for textual saliency on multiple benchmark classification datasets. Our approach requires no additional training or access to labelled data, and is comparatively very computationally efficient.
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#25512;&#29702;&#26041;&#38754;&#26080;&#33021;&#20026;&#21147;&#65292;&#23613;&#31649;&#26377;&#30528;&#20598;&#23572;&#26174;&#31034;&#30340;&#20998;&#26512;&#25165;&#26234;&#12290;</title><link>http://arxiv.org/abs/2308.03762</link><description>&lt;p&gt;
GPT-4&#26080;&#27861;&#36827;&#34892;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Can't Reason. (arXiv:2308.03762v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03762
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#25512;&#29702;&#26041;&#38754;&#26080;&#33021;&#20026;&#21147;&#65292;&#23613;&#31649;&#26377;&#30528;&#20598;&#23572;&#26174;&#31034;&#30340;&#20998;&#26512;&#25165;&#26234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-4&#20110;2023&#24180;3&#26376;&#21457;&#24067;&#65292;&#24191;&#21463;&#22909;&#35780;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;GPT-3.5&#65288;OpenAI&#20043;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;ChatGPT&#30340;&#21021;&#27425;&#21457;&#24067;&#65289;&#65292;&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25913;&#36827;&#65292;&#23545;&#20110;GPT-4&#30340;&#25512;&#29702;&#33021;&#21147;&#23384;&#22312;&#20805;&#20998;&#30340;&#24576;&#30097;&#26159;&#26377;&#36947;&#29702;&#30340;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#25512;&#29702;&#30340;&#26412;&#36136;&#65307;&#25209;&#35780;&#20102;&#24403;&#21069;NLP&#31038;&#21306;&#20013;&#25512;&#29702;&#38382;&#39064;&#30340;&#34920;&#36848;&#26041;&#24335;&#65292;&#20197;&#21450;&#30446;&#21069;LLM&#25512;&#29702;&#24615;&#33021;&#30340;&#35780;&#20272;&#26041;&#24335;&#65307;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#30001;21&#20010;&#22810;&#26679;&#21270;&#25512;&#29702;&#38382;&#39064;&#32452;&#25104;&#30340;&#38598;&#21512;&#65307;&#24182;&#23545;GPT-4&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#26412;&#25991;&#24471;&#20986;&#32467;&#35770;&#65292;&#23613;&#31649;&#20598;&#23572;&#26174;&#31034;&#20986;&#20998;&#26512;&#19978;&#30340;&#25165;&#26234;&#65292;&#20294;&#30446;&#21069;&#30340;GPT-4&#23436;&#20840;&#26080;&#27861;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-4 was released in March 2023 to wide acclaim, marking a very substantial improvement across the board over GPT-3.5 (OpenAI's previously best model, which had powered the initial release of ChatGPT). However, despite the genuinely impressive improvement, there are good reasons to be highly skeptical of GPT-4's ability to reason. This position paper discusses the nature of reasoning; criticizes the current formulation of reasoning problems in the NLP community, as well as the way in which LLM reasoning performance is currently evaluated; introduces a small collection of 21 diverse reasoning problems; and performs a detailed qualitative evaluation of GPT-4's performance on those problems. Based on this analysis, the paper concludes that, despite its occasional flashes of analytical brilliance, GPT-4 at present is utterly incapable of reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#26469;&#22686;&#24378;&#21305;&#37197;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;WMT Metrics&#22522;&#20934;&#20013;&#65292;&#22810;&#21442;&#32771;F200spBLEU&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#21333;&#21442;&#32771;&#26041;&#27861;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;7.2\%&#65292;&#36229;&#36807;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;BERTscore&#30340;3.9\%&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03131</link><description>&lt;p&gt;
&#36808;&#21521;&#22810;&#21442;&#32771;&#26102;&#20195; &#8212;&#8212; &#35299;&#20915;NLG&#35780;&#20272;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#21644;&#21442;&#32771;&#22810;&#26679;&#24615;&#26377;&#38480;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Towards Multiple References Era -- Addressing Data Leakage and Limited Reference Diversity in NLG Evaluation. (arXiv:2308.03131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#26469;&#22686;&#24378;&#21305;&#37197;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;WMT Metrics&#22522;&#20934;&#20013;&#65292;&#22810;&#21442;&#32771;F200spBLEU&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#21333;&#21442;&#32771;&#26041;&#27861;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;7.2\%&#65292;&#36229;&#36807;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;BERTscore&#30340;3.9\%&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
N-gram&#21305;&#37197;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;BLEU&#21644;chrF&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#22522;&#20110;&#21305;&#37197;&#30340;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#36739;&#24369;&#30340;&#30456;&#20851;&#24615;&#65292;&#23588;&#20854;&#26159;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#26631;&#22914;BLEURT&#30456;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#27979;&#21305;&#37197;&#25351;&#26631;&#24615;&#33021;&#29942;&#39048;&#30340;&#21407;&#22240;&#21487;&#33021;&#26159;&#21442;&#32771;&#36164;&#26009;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;"&#22810;&#20010;&#21442;&#32771;"&#26469;&#22686;&#24378;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;WMT Metrics&#22522;&#20934;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22810;&#21442;&#32771;F200spBLEU&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#21333;&#21442;&#32771;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;7.2\%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#36824;&#36229;&#36807;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;BERTscore&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;3.9\%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24471;&#21040;&#32531;&#35299;&#36890;&#36807;&#25105;&#20204;&#30340;&#22810;&#21442;&#32771;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
N-gram matching-based evaluation metrics, such as BLEU and chrF, are widely utilized across a range of natural language generation (NLG) tasks. However, recent studies have revealed a weak correlation between these matching-based metrics and human evaluations, especially when compared with neural-based metrics like BLEURT. In this paper, we conjecture that the performance bottleneck in matching-based metrics may be caused by the limited diversity of references. To address this issue, we propose to utilize \textit{multiple references} to enhance the consistency between these metrics and human evaluations. Within the WMT Metrics benchmarks, we observe that the multi-references F200spBLEU surpasses the conventional single-reference one by an accuracy improvement of 7.2\%. Remarkably, it also exceeds the neural-based BERTscore by an accuracy enhancement of 3.9\%. Moreover, we observe that the data leakage issue in large language models (LLMs) can be mitigated to a large extent by our multi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#26377;&#25928;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36924;&#30495;&#30340;&#29615;&#22659;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24050;&#23384;&#22312;&#30340;&#20195;&#29702;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;</title><link>http://arxiv.org/abs/2307.15644</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#25968;&#25454;&#29983;&#25104;&#35268;&#27169;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scaling Data Generation in Vision-and-Language Navigation. (arXiv:2307.15644v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#26377;&#25928;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36924;&#30495;&#30340;&#29615;&#22659;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24050;&#23384;&#22312;&#30340;&#20195;&#29702;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35821;&#35328;&#24341;&#23548;&#30340;&#35270;&#35273;&#23548;&#33322;&#30740;&#31350;&#20013;&#65292;&#23545;&#20110;&#36941;&#21382;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#21644;&#35757;&#32451;&#21487;&#27867;&#21270;&#20195;&#29702;&#30340;&#30417;&#30563;&#25968;&#37327;&#26377;&#20102;&#26126;&#26174;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;HM3D&#21644;Gibson&#25968;&#25454;&#38598;&#20013;&#30340;1200&#22810;&#20010;&#36924;&#30495;&#30340;&#29615;&#22659;&#65292;&#24182;&#21033;&#29992;&#32593;&#32476;&#19978;&#30340;&#36164;&#28304;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#33539;&#24335;&#20013;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#23545;&#20195;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#24688;&#24403;&#22320;&#24212;&#29992;&#25193;&#22686;&#25968;&#25454;&#26469;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20195;&#29702;&#12290;&#24471;&#30410;&#20110;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#29616;&#26377;&#20195;&#29702;&#30340;&#24615;&#33021;&#21487;&#20197;&#22823;&#24133;&#25552;&#21319;&#65288;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#32477;&#23545;&#20540;&#22686;&#21152;&#20102;11%&#65289;&#65292;&#22312;R2R&#27979;&#35797;&#38598;&#20013;&#21333;&#27425;&#36816;&#34892;&#25104;&#21151;&#29575;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in language-guided visual navigation has demonstrated a significant demand for the diversity of traversable environments and the quantity of supervision for training generalizable agents. To tackle the common data scarcity issue in existing vision-and-language navigation datasets, we propose an effective paradigm for generating large-scale data for learning, which applies 1200+ photo-realistic environments from HM3D and Gibson datasets and synthesizes 4.9 million instruction trajectory pairs using fully-accessible resources on the web. Importantly, we investigate the influence of each component in this paradigm on the agent's performance and study how to adequately apply the augmented data to pre-train and fine-tune an agent. Thanks to our large-scale dataset, the performance of an existing agent can be pushed up (+11% absolute with regard to previous SoTA) to a significantly new best of 80% single-run success rate on the R2R test split by simple imitation learning. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32479;&#35745;&#21147;&#23398;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#26641;&#32467;&#26500;&#30340;Strahler&#25968;&#30340;&#19978;&#19979;&#38480;&#65292;&#21457;&#29616;&#23427;&#20960;&#20046;&#24635;&#26159;3&#25110;4&#65292;&#24182;&#35777;&#26126;&#23427;&#26159;&#22788;&#29702;&#21477;&#23376;&#25152;&#38656;&#35760;&#24518;&#37327;&#30340;&#19979;&#38480;&#12290;&#21516;&#26102;&#65292;&#23545;&#38543;&#26426;&#26641;&#36827;&#34892;&#30340;&#20998;&#26512;&#25581;&#31034;&#20986;Strahler&#25968;&#30340;&#22686;&#38271;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#23427;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#29305;&#24449;&#30340;&#32479;&#35745;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.02697</link><description>&lt;p&gt;
Strahler&#25968;&#30340;&#32479;&#35745;&#21147;&#23398;&#65306;&#22522;&#20110;&#38543;&#26426;&#21644;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Statistical Mechanics of Strahler Number via Random and Natural Language Sentences. (arXiv:2307.02697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32479;&#35745;&#21147;&#23398;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#26641;&#32467;&#26500;&#30340;Strahler&#25968;&#30340;&#19978;&#19979;&#38480;&#65292;&#21457;&#29616;&#23427;&#20960;&#20046;&#24635;&#26159;3&#25110;4&#65292;&#24182;&#35777;&#26126;&#23427;&#26159;&#22788;&#29702;&#21477;&#23376;&#25152;&#38656;&#35760;&#24518;&#37327;&#30340;&#19979;&#38480;&#12290;&#21516;&#26102;&#65292;&#23545;&#38543;&#26426;&#26641;&#36827;&#34892;&#30340;&#20998;&#26512;&#25581;&#31034;&#20986;Strahler&#25968;&#30340;&#22686;&#38271;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#23427;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#29305;&#24449;&#30340;&#32479;&#35745;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Strahler&#25968;&#26368;&#21021;&#34987;&#25552;&#20986;&#29992;&#20110;&#25551;&#36848;&#27827;&#27969;&#20998;&#25903;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25214;&#21040;&#20102;&#21508;&#31181;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35745;&#31639;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#26641;&#32467;&#26500;&#30340;Strahler&#25968;&#19978;&#19979;&#38480;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#32467;&#26500;&#21487;&#20197;&#22312;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#32479;&#35745;&#21147;&#23398;&#20998;&#26512;&#12290;&#36890;&#36807;&#23545;&#35821;&#27861;&#27880;&#37322;&#25968;&#25454;&#30340;&#32463;&#39564;&#24615;&#27979;&#37327;&#65292;&#26174;&#31034;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#30340;Strahler&#25968;&#20960;&#20046;&#24635;&#26159;3&#25110;4&#65292;&#19982;Strahler&#65288;1957&#24180;&#65289;&#21644;Horton&#65288;1945&#24180;&#65289;&#25253;&#36947;&#30340;&#27827;&#27969;&#20998;&#27969;&#24773;&#20917;&#31867;&#20284;&#12290;&#20174;&#35813;&#25968;&#20540;&#30340;&#29702;&#35770;&#35266;&#28857;&#20986;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#26159;&#22312;&#29305;&#23450;&#27169;&#22411;&#19979;&#22788;&#29702;&#21477;&#23376;&#25152;&#38656;&#35760;&#24518;&#37327;&#30340;&#19979;&#38480;&#12290;&#23545;&#38543;&#26426;&#26641;&#36827;&#34892;&#30340;&#25968;&#23398;&#20998;&#26512;&#36827;&#19968;&#27493;&#20551;&#35774;&#20102;Strahler&#25968;&#30340;&#24615;&#36136;&#65292;&#25581;&#31034;&#20986;&#23427;&#24182;&#38750;&#24120;&#25968;&#32780;&#26159;&#20197;&#23545;&#25968;&#24418;&#24335;&#22686;&#38271;&#12290;&#36825;&#19968;&#21457;&#29616;&#25581;&#31034;&#20102;Strahler&#25968;&#20316;&#20026;&#25551;&#36848;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#29305;&#24449;&#30340;&#32479;&#35745;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Strahler number was originally proposed to characterize the complexity of river bifurcation and has found various applications. This article proposes computation of the Strahler number's upper and lower limits for natural language sentence tree structures, which are available in a large dataset allowing for statistical mechanics analysis.  Through empirical measurements across grammatically annotated data, the Strahler number of natural language sentences is shown to be almost always 3 or 4, similar to the case of river bifurcation as reported by Strahler (1957) and Horton (1945).  From the theory behind the number, we show that it is the lower limit of the amount of memory required to process sentences under a particular model. A mathematical analysis of random trees provides a further conjecture on the nature of the Strahler number, revealing that it is not a constant but grows logarithmically. This finding uncovers the statistical basics behind the Strahler number as a character
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;&#31934;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#27880;&#37322;&#65292;&#20197;&#25193;&#23637;&#20107;&#20214;&#31867;&#22411;&#26412;&#20307;&#12290;&#36890;&#36807;&#30740;&#31350;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#33258;&#21160;&#20998;&#25968;&#30340;&#24212;&#29992;&#65292;&#25552;&#39640;&#27880;&#37322;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02130</link><description>&lt;p&gt;
&#25193;&#23637;&#20107;&#20214;&#31867;&#22411;&#26412;&#20307;&#65306;&#20351;&#29992;&#31934;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#21160;&#35789;&#21644;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Extending an Event-type Ontology: Adding Verbs and Classes Using Fine-tuned LLMs Suggestions. (arXiv:2306.02130v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;&#31934;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#27880;&#37322;&#65292;&#20197;&#25193;&#23637;&#20107;&#20214;&#31867;&#22411;&#26412;&#20307;&#12290;&#36890;&#36807;&#30740;&#31350;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#33258;&#21160;&#20998;&#25968;&#30340;&#24212;&#29992;&#65292;&#25552;&#39640;&#27880;&#37322;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#30740;&#31350;&#20102;&#20351;&#29992;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#31934;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#27880;&#37322;&#20197;&#36827;&#34892;&#35789;&#27719;&#25193;&#23637;&#20219;&#21153;&#65292;&#21363;&#23558;&#25551;&#36848;&#24615;&#35789;&#27719;&#65288;&#21160;&#35789;&#65289;&#28155;&#21152;&#21040;&#29616;&#26377;&#30340;&#65288;&#20294;&#23578;&#19981;&#23436;&#21892;&#30340;&#65289;&#20107;&#20214;&#31867;&#22411;&#26412;&#20307;&#20013;&#12290;&#25105;&#20204;&#20851;&#27880;&#20102;&#20960;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#20174;&#30740;&#31350;&#21487;&#33021;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20026;&#27880;&#37322;&#32773;&#25552;&#20379;&#33267;&#23569;&#25552;&#31034;&#65292;&#21253;&#25324;&#21738;&#20123;&#21160;&#35789;&#22312;&#24403;&#21069;&#29256;&#26412;&#30340;&#26412;&#20307;&#20043;&#22806;&#65292;&#21040;&#21487;&#33021;&#20351;&#29992;&#33258;&#21160;&#20998;&#25968;&#24110;&#21161;&#27880;&#37322;&#32773;&#26356;&#39640;&#25928;&#22320;&#30830;&#23450;&#26080;&#27861;&#20998;&#37197;&#21040;&#20219;&#20309;&#29616;&#26377;&#31867;&#21035;&#30340;&#21160;&#35789;&#65292;&#20174;&#32780;&#20316;&#20026;&#26032;&#31867;&#21035;&#30340;&#31181;&#23376;&#12290;&#25105;&#20204;&#36824;&#35748;&#30495;&#30740;&#31350;&#20102;&#33258;&#21160;&#20998;&#25968;&#19982;&#20154;&#24037;&#27880;&#37322;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#23613;&#31649;&#30456;&#20851;&#24615;&#36739;&#24378;&#65292;&#20294;&#30001;&#20110;&#20854;&#36817;&#20284;&#32447;&#24615;&#20851;&#31995;&#65292;&#23545;&#27880;&#37322;&#26412;&#36523;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this project, we have investigated the use of advanced machine learning methods, specifically fine-tuned large language models, for pre-annotating data for a lexical extension task, namely adding descriptive words (verbs) to an existing (but incomplete, as of yet) ontology of event types. Several research questions have been focused on, from the investigation of a possible heuristics to provide at least hints to annotators which verbs to include and which are outside the current version of the ontology, to the possible use of the automatic scores to help the annotators to be more efficient in finding a threshold for identifying verbs that cannot be assigned to any existing class and therefore they are to be used as seeds for a new class. We have also carefully examined the correlation of the automatic scores with the human annotation. While the correlation turned out to be strong, its influence on the annotation proper is modest due to its near linearity, even though the mere fact o
&lt;/p&gt;</description></item><item><title>DomMa &#26159;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#23427;&#20998;&#20026;&#20013;&#33521;&#25991;10&#19975;&#20010;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;112&#20010;&#19968;&#32423;&#23398;&#31185;&#20998;&#31867;&#19981;&#26029;&#26356;&#26032;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.11679</link><description>&lt;p&gt;
&#39046;&#22495;&#25484;&#25569;&#27700;&#24179;&#22522;&#20934;&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20840;&#38754;&#39046;&#22495;&#30693;&#35782;&#30340;&#19981;&#26029;&#26356;&#26032;&#30340;&#22522;&#20934;&#8212;&#8212;&#21021;&#27493;&#21457;&#24067;&#12290;&#65288;arXiv:2304.11679v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Domain Mastery Benchmark: An Ever-Updating Benchmark for Evaluating Holistic Domain Knowledge of Large Language Model--A Preliminary Release. (arXiv:2304.11679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11679
&lt;/p&gt;
&lt;p&gt;
DomMa &#26159;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#23427;&#20998;&#20026;&#20013;&#33521;&#25991;10&#19975;&#20010;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;112&#20010;&#19968;&#32423;&#23398;&#31185;&#20998;&#31867;&#19981;&#26029;&#26356;&#26032;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#30693;&#35782;&#25351;&#23545;&#29305;&#23450;&#20027;&#39064;&#12289;&#34892;&#19994;&#12289;&#39046;&#22495;&#25110;&#29305;&#21035;&#20852;&#36259;&#39046;&#22495;&#30340;&#28145;&#20837;&#29702;&#35299;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#29087;&#24713;&#31243;&#24230;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#37117;&#32570;&#20047;&#23545;&#39046;&#22495;&#30693;&#35782;&#35780;&#20272;&#30340;&#25972;&#20307;&#35774;&#35745;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#39046;&#22495;&#35821;&#35328;&#29702;&#35299;&#30340;&#30495;&#27491;&#33021;&#21147;&#21482;&#33021;&#36890;&#36807;&#20840;&#38754;&#28145;&#20837;&#30340;&#22522;&#20934;&#26469;&#20844;&#24179;&#22320;&#35780;&#20272;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;Domma&#39046;&#22495;&#25484;&#25569;&#27700;&#24179;&#22522;&#20934;&#12290;DomMa&#26088;&#22312;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#39046;&#22495;&#30693;&#35782;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#39046;&#22495;&#35206;&#30422;&#12289;&#22823;&#37327;&#25968;&#25454;&#21644;&#22522;&#20110;&#20013;&#22269;112&#20010;&#19968;&#32423;&#23398;&#31185;&#20998;&#31867;&#19981;&#26029;&#26356;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;DomMa&#21253;&#21547;10&#19975;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#20013;&#25991;&#21644;&#33521;&#25991;&#65292;&#26469;&#28304;&#20110;&#20013;&#22269;&#22823;&#23398;&#30340;&#30740;&#31350;&#29983;&#20837;&#23398;&#32771;&#35797;&#21644;&#26412;&#31185;&#32771;&#35797;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26356;&#36866;&#21512;LLMs&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#27969;&#31243;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain knowledge refers to the in-depth understanding, expertise, and familiarity with a specific subject, industry, field, or area of special interest. The existing benchmarks are all lack of an overall design for domain knowledge evaluation. Holding the belief that the real ability of domain language understanding can only be fairly evaluated by an comprehensive and in-depth benchmark, we introduces the Domma, a Domain Mastery Benchmark. DomMa targets at testing Large Language Models (LLMs) on their domain knowledge understanding, it features extensive domain coverage, large data volume, and a continually updated data set based on Chinese 112 first-level subject classifications. DomMa consist of 100,000 questions in both Chinese and English sourced from graduate entrance examinations and undergraduate exams in Chinese college. We have also propose designs to make benchmark and evaluation process more suitable to LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GenExpan&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26694;&#26550;&#65292;&#21033;&#29992;&#21069;&#32512;&#26641;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03531</link><description>&lt;p&gt;
&#20174;&#26816;&#32034;&#21040;&#29983;&#25104;&#65306;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Retrieval to Generation: Efficient and Effective Entity Set Expansion. (arXiv:2304.03531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GenExpan&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26694;&#26550;&#65292;&#21033;&#29992;&#21069;&#32512;&#26641;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38598;&#25193;&#23637;&#65288;ESE&#65289;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#25193;&#23637;&#30001;&#23567;&#30340;&#31181;&#23376;&#23454;&#20307;&#38598;&#25551;&#36848;&#30340;&#30446;&#26631;&#35821;&#20041;&#31867;&#30340;&#23454;&#20307;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;ESE&#26041;&#27861;&#26159;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#38656;&#35201;&#25552;&#21462;&#23454;&#20307;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#35745;&#31639;&#31181;&#23376;&#23454;&#20307;&#21644;&#20505;&#36873;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20004;&#20010;&#30446;&#30340;&#65292;&#23427;&#20204;&#24517;&#39035;&#36845;&#20195;&#22320;&#36941;&#21382;&#35821;&#26009;&#24211;&#21644;&#25968;&#25454;&#38598;&#20013;&#25552;&#20379;&#30340;&#23454;&#20307;&#35789;&#27719;&#65292;&#23548;&#33268;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#26816;&#32034;&#30340;ESE&#26041;&#27861;&#28040;&#32791;&#30340;&#26102;&#38388;&#19982;&#23454;&#20307;&#35789;&#27719;&#21644;&#35821;&#26009;&#24211;&#30340;&#22823;&#23567;&#25104;&#32447;&#24615;&#22686;&#38271;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;ESE&#26694;&#26550;&#65292;Generative Entity Set Expansion (GenExpan)&#65292;&#23427;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;ESE&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37319;&#29992;&#21069;&#32512;&#26641;&#26469;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Set Expansion (ESE) is a critical task aiming to expand entities of the target semantic class described by a small seed entity set. Most existing ESE methods are retrieval-based frameworks that need to extract the contextual features of entities and calculate the similarity between seed entities and candidate entities. To achieve the two purposes, they should iteratively traverse the corpus and the entity vocabulary provided in the datasets, resulting in poor efficiency and scalability. The experimental results indicate that the time consumed by the retrieval-based ESE methods increases linearly with entity vocabulary and corpus size. In this paper, we firstly propose a generative ESE framework, Generative Entity Set Expansion (GenExpan), which utilizes a generative pre-trained language model to accomplish ESE task. Specifically, a prefix tree is employed to guarantee the validity of entity generation, and automatically generated class names are adopted to guide the model to gen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#28151;&#21512;&#31867;&#22411;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#28508;&#21147;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#25968;&#25454;&#36136;&#37327;&#12289;&#23454;&#29992;&#24615;&#21644;&#22686;&#24378;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#38544;&#31169;&#26041;&#38754;&#23384;&#22312;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.14679</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#28151;&#21512;&#31867;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Mixed-type Electronic Health Records using Diffusion Models. (arXiv:2302.14679v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#28151;&#21512;&#31867;&#22411;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#28508;&#21147;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#25968;&#25454;&#36136;&#37327;&#12289;&#23454;&#29992;&#24615;&#21644;&#22686;&#24378;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#38544;&#31169;&#26041;&#38754;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#21253;&#21547;&#25935;&#24863;&#30340;&#24739;&#32773;&#20449;&#24687;&#65292;&#22312;&#20849;&#20139;&#27492;&#31867;&#25968;&#25454;&#26102;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26159;&#32531;&#35299;&#36825;&#20123;&#39118;&#38505;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;GANs&#20043;&#19978;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#65292;&#20363;&#22914;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#21512;&#25104;&#25968;&#25454;&#21644;&#31283;&#23450;&#30340;&#35757;&#32451;&#20197;&#29983;&#25104;&#21253;&#25324;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#22768;&#38899;&#22312;&#20869;&#30340;&#25968;&#25454;&#27169;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#30495;&#23454;&#28151;&#21512;&#31867;&#22411;&#34920;&#26684;EHRs&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#23558;TabDDPM&#27169;&#22411;&#19982;&#29616;&#26377;&#26041;&#27861;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#25968;&#25454;&#36136;&#37327;&#12289;&#23454;&#29992;&#24615;&#12289;&#38544;&#31169;&#21644;&#22686;&#24378;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#38500;&#38544;&#31169;&#26041;&#38754;&#22806;&#65292;TabDDPM&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#36825;&#35777;&#23454;&#20102;&#38544;&#31169;&#21644;&#23454;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records (EHRs) contain sensitive patient information, which presents privacy concerns when sharing such data. Synthetic data generation is a promising solution to mitigate these risks, often relying on deep generative models such as Generative Adversarial Networks (GANs). However, recent studies have shown that diffusion models offer several advantages over GANs, such as generation of more realistic synthetic data and stable training in generating data modalities, including image, text, and sound. In this work, we investigate the potential of diffusion models for generating realistic mixed-type tabular EHRs, comparing TabDDPM model with existing methods on four datasets in terms of data quality, utility, privacy, and augmentation. Our experiments demonstrate that TabDDPM outperforms the state-of-the-art models across all evaluation metrics, except for privacy, which confirms the trade-off between privacy and utility.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#27010;&#36848;&#20197;&#21450;ChatGPT&#32972;&#21518;&#30340;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#25945;&#32946;&#21644;&#30740;&#31350;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#25351;&#20986;&#20102;&#20854;&#38544;&#31169;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#20197;&#21450;&#24403;&#21069;&#29256;&#26412;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.13817</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#26469;&#32842;&#32842;&#21543;&#65281;&#19982;ChatGPT&#30340;&#23545;&#35805;&#65306;&#25216;&#26415;&#65292;&#24212;&#29992;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations. (arXiv:2302.13817v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#27010;&#36848;&#20197;&#21450;ChatGPT&#32972;&#21518;&#30340;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#25945;&#32946;&#21644;&#30740;&#31350;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#25351;&#20986;&#20102;&#20854;&#38544;&#31169;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#20197;&#21450;&#24403;&#21069;&#29256;&#26412;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27454;&#33021;&#22815;&#29983;&#25104;&#20687;&#20154;&#31867;&#19968;&#26679;&#30340;&#21477;&#23376;&#21644;&#20889;&#20986;&#36830;&#36143;&#25991;&#31456;&#30340;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#19990;&#30028;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#27010;&#36848;&#20197;&#21450;ChatGPT&#32972;&#21518;&#30340;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#36824;&#24378;&#35843;&#20102;ChatGPT&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#65292;&#25945;&#32946;&#21644;&#30740;&#31350;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#23613;&#31649;&#26377;&#30528;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;ChatGPT&#21608;&#22260;&#23384;&#22312;&#30528;&#19968;&#20123;&#38544;&#31169;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#24403;&#21069;&#29256;&#26412;ChatGPT&#30340;&#19968;&#20123;&#37325;&#35201;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#21521;ChatGPT&#25552;&#20986;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#20197;&#20415;&#23427;&#34920;&#36798;&#33258;&#24049;&#30340;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of an AI-powered chatbot that can generate human-like sentences and write coherent essays has caught the world's attention. This paper discusses the historical overview of chatbots and the technology behind Chat Generative Pre-trained Transformer, better known as ChatGPT. Moreover, potential applications of ChatGPT in various domains, including healthcare, education, and research, are highlighted. Despite promising results, there are several privacy and ethical concerns surrounding ChatGPT. In addition, we highlight some of the important limitations of the current version of ChatGPT. We also ask ChatGPT to provide its point of view and present its responses to several questions we attempt to answer.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;GLM2FSA&#65292;&#33021;&#22815;&#33258;&#21160;&#20174;&#20219;&#21153;&#30446;&#26631;&#30340;&#31616;&#30701;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20219;&#21153;&#30693;&#35782;&#24182;&#26500;&#24314;&#19968;&#20010;&#32534;&#30721;&#39640;&#23618;&#27425;&#20219;&#21153;&#30693;&#35782;&#30340;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65292;&#26500;&#24314;&#30340;&#33258;&#21160;&#26426;&#21487;&#20197;&#34987;&#27491;&#24335;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2212.01944</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#26426;&#34920;&#31034;&#20219;&#21153;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Automaton-Based Representations of Task Knowledge from Generative Language Models. (arXiv:2212.01944v3 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01944
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;GLM2FSA&#65292;&#33021;&#22815;&#33258;&#21160;&#20174;&#20219;&#21153;&#30446;&#26631;&#30340;&#31616;&#30701;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20219;&#21153;&#30693;&#35782;&#24182;&#26500;&#24314;&#19968;&#20010;&#32534;&#30721;&#39640;&#23618;&#27425;&#20219;&#21153;&#30693;&#35782;&#30340;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65292;&#26500;&#24314;&#30340;&#33258;&#21160;&#26426;&#21487;&#20197;&#34987;&#27491;&#24335;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#26426;&#30340;&#20219;&#21153;&#30693;&#35782;&#34920;&#31034;&#22312;&#25511;&#21046;&#21644;&#35268;&#21010;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#26500;&#24314;&#27492;&#31867;&#33258;&#21160;&#26426;&#25152;&#38656;&#30340;&#39640;&#23618;&#27425;&#20219;&#21153;&#30693;&#35782;&#36890;&#24120;&#24456;&#22256;&#38590;&#12290;&#21516;&#26102;&#65292;&#22823;&#35268;&#27169;&#33258;&#21160;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#30456;&#20851;&#20219;&#21153;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#36755;&#20986;&#19981;&#33021;&#27491;&#24335;&#39564;&#35777;&#25110;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GLM2FSA&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#20174;&#20219;&#21153;&#30446;&#26631;&#30340;&#31616;&#30701;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#26500;&#24314;&#19968;&#20010;&#32534;&#30721;&#39640;&#23618;&#27425;&#20219;&#21153;&#30693;&#35782;&#30340;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;FSA&#65289;&#12290;GLM2FSA&#39318;&#20808;&#21521;GLM&#21457;&#36865;&#26597;&#35810;&#20197;&#25552;&#21462;&#25991;&#26412;&#24418;&#24335;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#28982;&#21518;&#23427;&#24314;&#31435;&#19968;&#20010;FSA&#26469;&#34920;&#31034;&#36825;&#31181;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#21644;&#33258;&#21160;&#26426;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#26500;&#24314;&#30340;FSA&#21487;&#20197;&#38024;&#23545;&#29992;&#25143;&#23450;&#20041;&#30340;&#35268;&#26684;&#36827;&#34892;&#27491;&#24335;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automaton-based representations of task knowledge play an important role in control and planning for sequential decision-making problems. However, obtaining the high-level task knowledge required to build such automata is often difficult. Meanwhile, large-scale generative language models (GLMs) can automatically generate relevant task knowledge. However, the textual outputs from GLMs cannot be formally verified or used for sequential decision-making. We propose a novel algorithm named GLM2FSA, which constructs a finite state automaton (FSA) encoding high-level task knowledge from a brief natural-language description of the task goal. GLM2FSA first sends queries to a GLM to extract task knowledge in textual form, and then it builds an FSA to represent this text-based knowledge. The proposed algorithm thus fills the gap between natural-language task descriptions and automaton-based representations, and the constructed FSA can be formally verified against user-defined specifications. We a
&lt;/p&gt;</description></item><item><title>VT-CLIP&#36890;&#36807;&#35270;&#35273;&#24341;&#23548;&#25991;&#26412;&#26469;&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;CLIP&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.02399</link><description>&lt;p&gt;
VT-CLIP: &#29992;&#35270;&#35273;&#24341;&#23548;&#25991;&#26412;&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts. (arXiv:2112.02399v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.02399
&lt;/p&gt;
&lt;p&gt;
VT-CLIP&#36890;&#36807;&#35270;&#35273;&#24341;&#23548;&#25991;&#26412;&#26469;&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;CLIP&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#20854;&#21487;&#36716;&#31227;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#65292;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#20869;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;CLIP&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#21464;&#24471;&#27425;&#20248;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#20854;&#36801;&#31227;&#24615;&#33021;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#36866;&#24212;&#36328;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35270;&#35273;&#24341;&#23548;&#25991;&#26412;&#26469;&#22686;&#24378;CLIP&#65292;&#21629;&#21517;&#20026;VT-CLIP&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#24341;&#23548;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#29305;&#24449;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#22270;&#20687;&#19978;&#30340;&#20449;&#24687;&#21306;&#22495;&#65292;&#24182;&#32858;&#21512;&#35270;&#35273;&#29305;&#24449;&#12290;&#36825;&#26679;&#65292;&#25991;&#26412;&#23601;&#25104;&#20026;&#20102;&#35270;&#35273;&#24341;&#23548;&#30340;&#65292;&#21363;&#19982;&#19979;&#28216;&#22270;&#20687;&#26356;&#35821;&#20041;&#30456;&#20851;&#65292;&#36825;&#26497;&#22823;&#22320;&#26377;&#30410;&#20110;&#31867;&#21035;&#21305;&#37197;&#30340;&#36807;&#31243;&#12290;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;VT-CLIP&#22312;11&#20010;&#30693;&#21517;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) has drawn increasing attention recently for its transferable visual representation learning. However, due to the semantic gap within datasets, CLIP's pre-trained image-text alignment becomes sub-optimal on downstream tasks, which severely harms its transferring performance. To better adapt the cross-modality embedding space, we propose to enhance CLIP via Visual-guided Texts, named VT-CLIP. Specifically, we guide textual features of different categories to adaptively explore informative regions on the image and aggregate visual features by attention mechanisms. In this way, the texts become visual-guided, namely, more semantically correlated with downstream images, which greatly benefits the category-wise matching process. In few-shot settings, we evaluate our VT-CLIP on 11 well-known classification datasets to demonstrate its effectiveness.
&lt;/p&gt;</description></item></channel></rss>