<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>ImageBind-LLM&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#32852;&#21512;&#23884;&#20837;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.03905</link><description>&lt;p&gt;
ImageBind-LLM: &#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ImageBind-LLM: Multi-modality Instruction Tuning. (arXiv:2309.03905v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03905
&lt;/p&gt;
&lt;p&gt;
ImageBind-LLM&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#32852;&#21512;&#23884;&#20837;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;ImageBind&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#35328;&#21644;&#22270;&#20687;&#25351;&#20196;&#35843;&#20248;&#26041;&#38754;&#65292;&#19982;&#27492;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;ImageBind-LLM&#21487;&#20197;&#21709;&#24212;&#22810;&#27169;&#24577;&#26465;&#20214;&#65292;&#21253;&#25324;&#38899;&#39057;&#12289;3D&#28857;&#20113;&#12289;&#35270;&#39057;&#20197;&#21450;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#31639;&#26415;&#65292;&#21482;&#38656;&#36827;&#34892;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#21487;&#23398;&#20064;&#30340;Bind&#32593;&#32476;&#26469;&#23545;&#40784;LLaMA&#21644;ImageBind&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;Bind&#32593;&#32476;&#36716;&#25442;&#30340;&#22270;&#20687;&#29305;&#24449;&#34987;&#28155;&#21152;&#21040;LLaMA&#30340;&#25152;&#26377;&#23618;&#30340;&#21333;&#35789;&#26631;&#35760;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#26080;&#27880;&#24847;&#21147;&#21644;&#38646;&#21021;&#22987;&#21270;&#30340;&#38376;&#25511;&#26426;&#21046;&#36880;&#27493;&#27880;&#20837;&#35270;&#35273;&#25351;&#20196;&#12290;&#22312;ImageBind&#30340;&#32852;&#21512;&#23884;&#20837;&#30340;&#24110;&#21161;&#19979;&#65292;&#31616;&#21333;&#30340;&#22270;&#20687;-&#25991;&#26412;&#35757;&#32451;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22810;&#27169;&#24577;&#36755;&#20837;&#34987;&#36865;&#20837;&#30456;&#24212;&#30340;ImageBind&#32534;&#30721;&#22120;&#65292;&#24182;&#34987;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#35813;&#22871;&#20214;&#21253;&#25324;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.03886</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#21151;&#33021;&#35299;&#37322;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Function Interpretation Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#35813;&#22871;&#20214;&#21253;&#25324;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21487;&#35835;&#30340;&#25551;&#36848;&#26631;&#35760;&#31070;&#32463;&#32593;&#32476;&#23376;&#27169;&#22359;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#36825;&#20123;&#25551;&#36848;&#21487;&#20197;&#26292;&#38706;&#22833;&#36133;&#12289;&#24341;&#23548;&#24178;&#39044;&#65292;&#29978;&#33267;&#21487;&#20197;&#35299;&#37322;&#37325;&#35201;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#26426;&#26800;&#21407;&#29702;&#30340;&#24050;&#35757;&#32451;&#32593;&#32476;&#25551;&#36848;&#37117;&#28041;&#21450;&#21040;&#23567;&#27169;&#22411;&#12289;&#29421;&#20041;&#29616;&#35937;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#12290;&#22312;&#19981;&#26029;&#22686;&#21152;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#20013;&#26631;&#35760;&#20986;&#25152;&#26377;&#20154;&#21487;&#35299;&#37322;&#30340;&#23376;&#35745;&#31639;&#20960;&#20046;&#32943;&#23450;&#38656;&#35201;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#39564;&#35777;&#25551;&#36848;&#30340;&#24037;&#20855;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#26631;&#35760;&#30340;&#25216;&#26415;&#24320;&#22987;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#26377;&#38480;&#19988;&#20020;&#26102;&#12290;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#39564;&#35777;&#21644;&#27604;&#36739;&#24320;&#25918;&#24335;&#26631;&#35760;&#24037;&#20855;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;FIND&#65288;&#20989;&#25968;&#35299;&#37322;&#21644;&#25551;&#36848;&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#26041;&#27861;&#26500;&#24314;&#27169;&#22359;&#30340;&#22522;&#20934;&#22871;&#20214;&#12290;FIND&#21253;&#21547;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#30340;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of tr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21548;&#35273;&#24341;&#23548;&#30340;&#38646;&#26679;&#26412;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#21305;&#37197;&#32593;&#32476;&#21644;&#25991;&#26412;&#20998;&#31867;&#22120;&#26469;&#23454;&#29616;&#29983;&#25104;&#25991;&#26412;&#27969;&#30021;&#24615;&#12289;&#24544;&#23454;&#24615;&#21644;&#21487;&#21548;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03884</link><description>&lt;p&gt;
&#36890;&#36807;&#21548;&#35273;&#24341;&#23548;&#30340;&#38646;&#26679;&#26412;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Audio Captioning via Audibility Guidance. (arXiv:2309.03884v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21548;&#35273;&#24341;&#23548;&#30340;&#38646;&#26679;&#26412;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#21305;&#37197;&#32593;&#32476;&#21644;&#25991;&#26412;&#20998;&#31867;&#22120;&#26469;&#23454;&#29616;&#29983;&#25104;&#25991;&#26412;&#27969;&#30021;&#24615;&#12289;&#24544;&#23454;&#24615;&#21644;&#21487;&#21548;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#30340;&#20219;&#21153;&#19982;&#22270;&#20687;&#21644;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#31867;&#20284;&#65292;&#20294;&#21364;&#24471;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#30340;&#35201;&#27714;&#65306;&#65288;i&#65289;&#29983;&#25104;&#25991;&#26412;&#30340;&#27969;&#30021;&#24615;&#65292;&#65288;ii&#65289;&#29983;&#25104;&#25991;&#26412;&#19982;&#36755;&#20837;&#38899;&#39057;&#30340;&#24544;&#23454;&#24615;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#21487;&#21548;&#24615;&#65292;&#21363;&#20165;&#22522;&#20110;&#38899;&#39057;&#33021;&#22815;&#34987;&#24863;&#30693;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#21363;&#25105;&#20204;&#19981;&#20250;&#23398;&#20064;&#25191;&#34892;&#23383;&#24149;&#29983;&#25104;&#12290;&#30456;&#21453;&#65292;&#23383;&#24149;&#29983;&#25104;&#20316;&#20026;&#19968;&#20010;&#25512;&#29702;&#36807;&#31243;&#21457;&#29983;&#65292;&#28041;&#21450;&#21040;&#19977;&#20010;&#23545;&#24212;&#20110;&#19977;&#20010;&#26399;&#26395;&#36136;&#37327;&#30340;&#32593;&#32476;&#65306;&#65288;i&#65289;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#36873;&#25321;&#20102; GPT-2&#65292;&#65288;ii&#65289;&#19968;&#20010;&#22312;&#38899;&#39057;&#25991;&#20214;&#21644;&#25991;&#26412;&#20043;&#38388;&#25552;&#20379;&#21305;&#37197;&#20998;&#25968;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026; ImageBind &#30340;&#22810;&#27169;&#24577;&#21305;&#37197;&#32593;&#32476;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#19968;&#31181;&#25991;&#26412;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;&#25105;&#20204;&#33258;&#21160;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#25351;&#23548; GPT-4 &#25552;&#31034;&#35774;&#35745;&#26469;&#25351;&#23548;&#21487;&#21548;&#21644;&#19981;&#21487;&#21548;&#25991;&#26412;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of audio captioning is similar in essence to tasks such as image and video captioning. However, it has received much less attention. We propose three desiderata for captioning audio -- (i) fluency of the generated text, (ii) faithfulness of the generated text to the input audio, and the somewhat related (iii) audibility, which is the quality of being able to be perceived based only on audio. Our method is a zero-shot method, i.e., we do not learn to perform captioning. Instead, captioning occurs as an inference process that involves three networks that correspond to the three desired qualities: (i) A Large Language Model, in our case, for reasons of convenience, GPT-2, (ii) A model that provides a matching score between an audio file and a text, for which we use a multimodal matching network called ImageBind, and (iii) A text classifier, trained using a dataset we collected automatically by instructing GPT-4 with prompts designed to direct the generation of both audible and in
&lt;/p&gt;</description></item><item><title>DoLa&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#23618;&#27425;&#30340;&#36923;&#36753;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20943;&#23569;&#24187;&#35273;&#65292;&#26080;&#38656;&#22806;&#37096;&#30693;&#35782;&#25110;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2309.03883</link><description>&lt;p&gt;
DoLa&#65306;&#36890;&#36807;&#23545;&#27604;&#23618;&#27425;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. (arXiv:2309.03883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03883
&lt;/p&gt;
&lt;p&gt;
DoLa&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#23618;&#27425;&#30340;&#36923;&#36753;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20943;&#23569;&#24187;&#35273;&#65292;&#26080;&#38656;&#22806;&#37096;&#30693;&#35782;&#25110;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#19982;&#39044;&#35757;&#32451;&#26399;&#38388;&#35266;&#23519;&#21040;&#30340;&#20107;&#23454;&#20559;&#31163;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#20943;&#23569;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#24187;&#35273;&#65292;&#23427;&#19981;&#38656;&#35201;&#22312;&#26816;&#32034;&#30340;&#22806;&#37096;&#30693;&#35782;&#25110;&#39069;&#22806;&#30340;&#24494;&#35843;&#19978;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23558;&#36739;&#26202;&#23618;&#21644;&#36739;&#26089;&#23618;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#24471;&#21040;&#30340;&#36923;&#36753;&#24046;&#24322;&#26469;&#33719;&#24471;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#20998;&#24067;&#65292;&#21033;&#29992;&#20102;LLMs&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#36890;&#24120;&#34987;&#35777;&#26126;&#23616;&#37096;&#21270;&#22312;&#29305;&#23450;&#30340;Transformer&#23618;&#20013;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#36890;&#36807;&#23545;&#27604;&#23618;&#27425;&#30340;&#35299;&#30721;&#65288;DoLa&#65289;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23637;&#31034;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#20943;&#23569;&#29983;&#25104;&#19981;&#27491;&#30830;&#20107;&#23454;&#30340;&#24773;&#20917;&#12290;DoLa&#22312;&#22810;&#20010;&#36873;&#25321;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#21319;&#20102;&#30495;&#23454;&#24615;&#65292;&#20363;&#22914;&#25913;&#21892;&#20102;LLaMA&#31995;&#21015;&#27169;&#22411;&#22312;TruthfulQA&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#23384;&#22312;&#36873;&#25321;&#20559;&#24046;&#65292;&#21363;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#20301;&#32622;&#19978;&#30340;&#36873;&#39033;&#12290;&#30740;&#31350;&#25351;&#20986;&#36825;&#19968;&#20559;&#24046;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#36873;&#39033;&#32534;&#21495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PriDe&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#20559;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#39640;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03882</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#30340;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Large Language Models' Selection Bias in Multi-Choice Questions. (arXiv:2309.03882v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#23384;&#22312;&#36873;&#25321;&#20559;&#24046;&#65292;&#21363;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#20301;&#32622;&#19978;&#30340;&#36873;&#39033;&#12290;&#30740;&#31350;&#25351;&#20986;&#36825;&#19968;&#20559;&#24046;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#36873;&#39033;&#32534;&#21495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PriDe&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#20559;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#39640;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQs&#65289;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30740;&#31350;&#20013;&#24120;&#35265;&#19988;&#37325;&#35201;&#30340;&#20219;&#21153;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;LLMs&#22312;MCQs&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#8220;&#36873;&#25321;&#20559;&#24046;&#8221;&#65292;&#21363;LLMs&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#20301;&#32622;&#19978;&#30340;&#36873;&#39033;&#65288;&#22914;&#8220;&#36873;&#39033;C&#8221;&#65289;&#12290;&#36825;&#31181;&#20559;&#24046;&#22312;&#21508;&#31181;LLMs&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;MCQs&#20013;&#23545;&#36873;&#39033;&#20301;&#32622;&#21464;&#21270;&#30340;&#24615;&#33021;&#21464;&#24471;&#33030;&#24369;&#12290;&#25105;&#20204;&#21457;&#29616;&#23548;&#33268;&#36873;&#25321;&#20559;&#24046;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#36873;&#39033;&#32534;&#21495;&#65292;&#21363;&#19982;&#36873;&#39033;&#30456;&#20851;&#30340;ID&#31526;&#21495;A/B/C/D&#12290;&#20026;&#20102;&#20943;&#36731;&#36873;&#25321;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#31216;&#20026;PriDe&#12290;PriDe&#39318;&#20808;&#23558;&#35266;&#23519;&#21040;&#30340;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#20998;&#35299;&#20026;&#23545;&#36873;&#39033;&#20869;&#23481;&#30340;&#20869;&#22312;&#39044;&#27979;&#21644;&#23545;&#36873;&#39033;ID&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#22312;&#23569;&#37327;&#27979;&#35797;&#26679;&#26412;&#19978;&#23545;&#36873;&#39033;&#20869;&#23481;&#36827;&#34892;&#25490;&#21015;&#32452;&#21512;&#26469;&#20272;&#35745;&#20808;&#39564;&#65292;&#20174;&#32780;&#29992;&#20110;&#28040;&#38500;&#21518;&#32493;&#27979;&#35797;&#26679;&#26412;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20316;&#20026;&#19968;&#31181;&#26080;&#26631;&#31614;&#12289;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#65292;PriDe&#21487;&#20197;&#23454;&#29616;&#39640;&#31934;&#24230;&#19988;&#31283;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-choice questions (MCQs) serve as a common yet important task format in the research of large language models (LLMs). Our work shows that LLMs exhibit an inherent "selection bias" in MCQs, which refers to LLMs' preferences to select options located at specific positions (like "Option C"). This bias is prevalent across various LLMs, making their performance vulnerable to option position changes in MCQs. We identify that one primary cause resulting in selection bias is option numbering, i.e., the ID symbols A/B/C/D associated with the options. To mitigate selection bias, we propose a new method called PriDe. PriDe first decomposes the observed model prediction distribution into an intrinsic prediction over option contents and a prior distribution over option IDs. It then estimates the prior by permutating option contents on a small number of test samples, which is used to debias the subsequent test samples. We demonstrate that, as a label-free, inference-time method, PriDe achieves 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#39044;&#27979;&#35805;&#35821;&#8221;&#27010;&#24565;&#65292;&#36890;&#36807;&#33258;&#21160;&#21644;&#20934;&#30830;&#22320;&#35299;&#37322;&#29992;&#25143;&#30340;&#35805;&#35821;&#26469;&#35299;&#20915;&#23545;&#35805;&#24335;&#39044;&#27979;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03877</link><description>&lt;p&gt;
&#24341;&#20837;&#8220;&#39044;&#27979;&#35805;&#35821;&#8221;&#29992;&#20110;&#23545;&#35805;&#24335;&#25968;&#25454;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
Introducing "Forecast Utterance" for Conversational Data Science. (arXiv:2309.03877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#39044;&#27979;&#35805;&#35821;&#8221;&#27010;&#24565;&#65292;&#36890;&#36807;&#33258;&#21160;&#21644;&#20934;&#30830;&#22320;&#35299;&#37322;&#29992;&#25143;&#30340;&#35805;&#35821;&#26469;&#35299;&#20915;&#23545;&#35805;&#24335;&#39044;&#27979;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#24819;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#30452;&#35266;&#33258;&#28982;&#30340;&#23545;&#35805;&#36741;&#21161;&#29992;&#25143;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#24213;&#23618;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36807;&#31243;&#12290;&#23545;&#20110;&#36825;&#19968;&#21162;&#21147;&#65292;&#20195;&#29702;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#26159;&#20934;&#30830;&#29702;&#35299;&#29992;&#25143;&#30340;&#39044;&#27979;&#30446;&#26631;&#65292;&#24182;&#22240;&#27492;&#21046;&#23450;&#31934;&#30830;&#30340;ML&#20219;&#21153;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;&#8220;&#39044;&#27979;&#35805;&#35821;&#8221;&#30340;&#26032;&#27010;&#24565;&#65292;&#20026;&#23454;&#29616;&#36825;&#19968;&#38596;&#24515;&#21187;&#21187;&#30340;&#30446;&#26631;&#36808;&#20986;&#20102;&#21019;&#26032;&#30340;&#19968;&#27493;&#65292;&#24182;&#30528;&#37325;&#20110;&#20174;&#36825;&#20123;&#35805;&#35821;&#20013;&#33258;&#21160;&#21644;&#20934;&#30830;&#22320;&#35299;&#37322;&#29992;&#25143;&#30340;&#39044;&#27979;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#26694;&#26550;&#21270;&#20026;&#19968;&#20010;&#27133;&#22635;&#20805;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#27133;&#23545;&#24212;&#20110;&#30446;&#26631;&#39044;&#27979;&#20219;&#21153;&#30340;&#29305;&#23450;&#26041;&#38754;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20004;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;&#26469;&#35299;&#20915;&#27133;&#22635;&#20805;&#20219;&#21153;&#65292;&#21363;&#65306;1&#65289;&#23454;&#20307;&#25552;&#21462;&#65288;EE&#65289;&#21644;2&#65289;&#38382;&#31572;&#65288;QA&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#31934;&#24515;&#21046;&#20316;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Envision an intelligent agent capable of assisting users in conducting forecasting tasks through intuitive, natural conversations, without requiring in-depth knowledge of the underlying machine learning (ML) processes. A significant challenge for the agent in this endeavor is to accurately comprehend the user's prediction goals and, consequently, formulate precise ML tasks. In this paper, we take a pioneering step towards this ambitious goal by introducing a new concept called Forecast Utterance and then focus on the automatic and accurate interpretation of users' prediction goals from these utterances. Specifically, we frame the task as a slot-filling problem, where each slot corresponds to a specific aspect of the goal prediction task. We then employ two zero-shot methods for solving the slot-filling task, namely: 1) Entity Extraction (EE), and 2) Question-Answering (QA) techniques. Our experiments, conducted with three meticulously crafted data sets, validate the viability of our am
&lt;/p&gt;</description></item><item><title>OpinionGPT&#26159;&#19968;&#20010;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;web&#28436;&#31034;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#21508;&#31181;&#20559;&#35265;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#23427;&#24847;&#22312;&#20351;&#27169;&#22411;&#30340;&#20559;&#35265;&#26174;&#24615;&#21644;&#36879;&#26126;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.03876</link><description>&lt;p&gt;
OpinionGPT: &#27169;&#25311;&#26174;&#24615;&#20559;&#35265;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)
&lt;/p&gt;
&lt;p&gt;
OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs. (arXiv:2309.03876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03876
&lt;/p&gt;
&lt;p&gt;
OpinionGPT&#26159;&#19968;&#20010;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;web&#28436;&#31034;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#21508;&#31181;&#20559;&#35265;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#23427;&#24847;&#22312;&#20351;&#27169;&#22411;&#30340;&#20559;&#35265;&#26174;&#24615;&#21644;&#36879;&#26126;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#29983;&#25104;&#19982;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30456;&#21305;&#37197;&#30340;&#22238;&#24212;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#28041;&#21450;&#35757;&#32451;&#27169;&#22411;&#21644;&#23427;&#20204;&#30340;&#22238;&#24212;&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#29992;&#20110;&#35843;&#25972;LLM&#30340;&#25968;&#25454;&#20027;&#35201;&#30001;&#20855;&#26377;&#29305;&#23450;&#25919;&#27835;&#20559;&#35265;&#30340;&#20154;&#32534;&#20889;&#65292;&#25105;&#20204;&#21487;&#33021;&#20250;&#26399;&#26395;&#29983;&#25104;&#30340;&#22238;&#31572;&#20063;&#20849;&#20139;&#36825;&#31181;&#20559;&#35265;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#24037;&#20316;&#26088;&#22312;&#38500;&#21435;&#36825;&#26679;&#30340;&#27169;&#22411;&#20559;&#35265;&#65292;&#25110;&#25233;&#21046;&#21487;&#33021;&#26377;&#20559;&#35265;&#30340;&#22238;&#31572;&#12290;&#36890;&#36807;&#36825;&#20010;&#28436;&#31034;&#65292;&#25105;&#20204;&#23545;&#25351;&#20196;&#35843;&#25972;&#20013;&#30340;&#20559;&#35265;&#25345;&#26377;&#19981;&#21516;&#30340;&#35266;&#28857;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#26159;&#25233;&#21046;&#23427;&#20204;&#65292;&#32780;&#26159;&#20351;&#23427;&#20204;&#26174;&#24615;&#21644;&#36879;&#26126;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;OpinionGPT&#65292;&#19968;&#20010;&#32593;&#32476;&#28436;&#31034;&#65292;&#29992;&#25143;&#21487;&#20197;&#25552;&#38382;&#24182;&#36873;&#25321;&#25152;&#26377;&#20182;&#20204;&#24076;&#26395;&#35843;&#26597;&#30340;&#20559;&#35265;&#12290;&#35813;&#28436;&#31034;&#23558;&#20351;&#29992;&#22312;&#20195;&#34920;&#27599;&#20010;&#36873;&#25321;&#20559;&#35265;&#30340;&#25991;&#26412;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#24182;&#25490;&#27604;&#36739;&#12290;&#20026;&#20102;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#36873;&#21462;&#20102;11&#20010;...
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable ability to generate fitting responses to natural language instructions. However, an open research question concerns the inherent biases of trained models and their responses. For instance, if the data used to tune an LLM is dominantly written by persons with a specific political bias, we might expect generated answers to share this bias. Current research work seeks to de-bias such models, or suppress potentially biased answers. With this demonstration, we take a different view on biases in instruction-tuning: Rather than aiming to suppress them, we aim to make them explicit and transparent. To this end, we present OpinionGPT, a web demo in which users can ask questions and select all biases they wish to investigate. The demo will answer this question using a model fine-tuned on text representing each of the selected biases, allowing side-by-side comparison. To train the underlying model, we identified 11 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#27169;&#22411;&#65288;FLM-101B&#65289;&#20197;&#21450;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;&#12290;&#36890;&#36807;&#37319;&#29992;&#22686;&#38271;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;LLM&#30340;&#26234;&#33021;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.03852</link><description>&lt;p&gt;
FLM-101B&#65306;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#21644;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;
&lt;/p&gt;
&lt;p&gt;
FLM-101B: An Open LLM and How to Train It with $100K Budget. (arXiv:2309.03852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#27169;&#22411;&#65288;FLM-101B&#65289;&#20197;&#21450;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;&#12290;&#36890;&#36807;&#37319;&#29992;&#22686;&#38271;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;LLM&#30340;&#26234;&#33021;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21457;&#23637;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;&#39640;&#35745;&#31639;&#25104;&#26412;&#65307;&#65288;ii&#65289;&#38590;&#20197;&#36827;&#34892;&#20844;&#24179;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;LLMs&#30340;&#20215;&#26684;&#26114;&#36149;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#23478;&#20027;&#35201;&#21442;&#19982;&#32773;&#26377;&#33021;&#21147;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#30740;&#31350;&#21644;&#24212;&#29992;&#26426;&#20250;&#12290;&#36825;&#20984;&#26174;&#20102;&#25104;&#26412;&#25928;&#30410;&#30340;LLM&#35757;&#32451;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#22686;&#38271;&#31574;&#30053;&#65292;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#19979;&#35757;&#32451;&#20855;&#26377;101B&#21442;&#25968;&#21644;0.31TB&#20196;&#29260;&#30340;LLM&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#33539;&#24335;&#65292;&#29992;&#20110;&#23545;LLMs&#36827;&#34892;&#26234;&#33021;&#30340;&#26234;&#21830;&#35780;&#20272;&#65292;&#36825;&#26159;&#38024;&#23545;&#29616;&#26377;&#35780;&#20272;&#26356;&#27880;&#37325;&#30693;&#35782;&#33021;&#21147;&#30340;&#34917;&#20805;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21253;&#25324;&#31526;&#21495;&#26144;&#23556;&#12289;&#35268;&#21017;&#29702;&#35299;&#12289;&#27169;&#24335;&#25366;&#25496;&#22312;&#20869;&#30340;&#37325;&#35201;&#26234;&#33021;&#26041;&#38754;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks. Despite these successes, their development faces two main challenges: (i) high computational cost; and (ii) difficulty in conducting fair and objective evaluations. LLMs are prohibitively expensive, making it feasible for only a few major players to undertake their training, thereby constraining both research and application opportunities. This underscores the importance of cost-effective LLM training. In this paper, we utilize a growth strategy to significantly reduce LLM training cost. We demonstrate that an LLM with 101B parameters and 0.31TB tokens can be trained on a $100K budget. We also adopt a systematic evaluation paradigm for the IQ evaluation of LLMs, in complement to existing evaluations that focus more on knowledge-oriented abilities. We introduce our benchmark including evaluations on important aspects of intelligence including symbolic mapping, itrule understanding, pattern mining,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28418;&#31227;&#12290;&#36890;&#36807;&#32534;&#30721;&#29983;&#20135;&#25968;&#25454;&#26679;&#26412;&#21644;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;&#26680;&#32479;&#35745;&#26816;&#39564;&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#36317;&#31163;&#24230;&#37327;&#26469;&#27604;&#36739;&#20998;&#24067;&#65292;&#21487;&#20197;&#26377;&#25928;&#20272;&#35745;&#28418;&#31227;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.03831</link><description>&lt;p&gt;
&#25581;&#31034;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#28418;&#31227;&#65306;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#21644;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models. (arXiv:2309.03831v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03831
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28418;&#31227;&#12290;&#36890;&#36807;&#32534;&#30721;&#29983;&#20135;&#25968;&#25454;&#26679;&#26412;&#21644;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;&#26680;&#32479;&#35745;&#26816;&#39564;&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#36317;&#31163;&#24230;&#37327;&#26469;&#27604;&#36739;&#20998;&#24067;&#65292;&#21487;&#20197;&#26377;&#25928;&#20272;&#35745;&#28418;&#31227;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28418;&#31227;&#25351;&#30340;&#26159;&#25968;&#25454;&#25110;&#27169;&#22411;&#36816;&#34892;&#19978;&#19979;&#25991;&#30340;&#32479;&#35745;&#29305;&#24615;&#38543;&#26102;&#38388;&#21464;&#21270;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#29616;&#35937;&#12290;&#22240;&#27492;&#65292;&#20445;&#25345;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#25345;&#32493;&#30417;&#25511;&#36807;&#31243;&#23545;&#20110;&#39044;&#38450;&#28508;&#22312;&#24615;&#33021;&#22238;&#36864;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26377;&#30417;&#30563;&#30340;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#65292;&#20174;&#32780;&#23548;&#33268;&#28418;&#31227;&#26816;&#27979;&#21644;&#20943;&#36731;&#36807;&#31243;&#26102;&#38388;&#36739;&#38271;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26080;&#30417;&#30563;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#20010;&#27493;&#39588;&#30340;&#27969;&#31243;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#28041;&#21450;&#23558;&#29983;&#20135;&#25968;&#25454;&#30340;&#19968;&#20010;&#26679;&#26412;&#20316;&#20026;&#30446;&#26631;&#20998;&#24067;&#65292;&#23558;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20316;&#20026;&#21442;&#32771;&#20998;&#24067;&#36827;&#34892;&#32534;&#30721;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26680;&#30340;&#32479;&#35745;&#26816;&#39564;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#36317;&#31163;&#24230;&#37327;&#26469;&#27604;&#36739;&#21442;&#32771;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#65292;&#20272;&#35745;&#20219;&#20309;&#28508;&#22312;&#30340;&#28418;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#30830;&#23450;&#29983;&#20135;&#25968;&#25454;&#23376;&#38598;&#30340;&#28418;&#31227;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drift in machine learning refers to the phenomenon where the statistical properties of data or context, in which the model operates, change over time leading to a decrease in its performance. Therefore, maintaining a constant monitoring process for machine learning model performance is crucial in order to proactively prevent any potential performance regression. However, supervised drift detection methods require human annotation and consequently lead to a longer time to detect and mitigate the drift. In our proposed unsupervised drift detection method, we follow a two step process. Our first step involves encoding a sample of production data as the target distribution, and the model training data as the reference distribution. In the second step, we employ a kernel-based statistical test that utilizes the maximum mean discrepancy (MMD) distance metric to compare the reference and target distributions and estimate any potential drift. Our method also identifies the subset of production
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#21333;&#35789;&#21644;&#25972;&#20010;&#25991;&#26412;&#20043;&#38388;&#30340;&#20114;&#30456;&#22686;&#24378;&#25928;&#24212;&#26469;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#24615;&#33021;&#30340;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#65292;&#24182;&#26500;&#24314;&#20102;&#26032;&#30340;&#24773;&#24863;&#25991;&#26412;&#20998;&#31867;&#21644;&#35789;&#24615;&#25968;&#25454;&#38598;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;gpt-3.5-t&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03787</link><description>&lt;p&gt;
&#32654;&#22269;&#65306;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#21644;&#26500;&#24314;&#26085;&#35821;&#24773;&#24863;&#25991;&#26412;&#20998;&#31867;&#21644;&#35789;&#24615;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
USA: Universal Sentiment Analysis Model &amp; Construction of Japanese Sentiment Text Classification and Part of Speech Dataset. (arXiv:2309.03787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#21333;&#35789;&#21644;&#25972;&#20010;&#25991;&#26412;&#20043;&#38388;&#30340;&#20114;&#30456;&#22686;&#24378;&#25928;&#24212;&#26469;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#24615;&#33021;&#30340;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#65292;&#24182;&#26500;&#24314;&#20102;&#26032;&#30340;&#24773;&#24863;&#25991;&#26412;&#20998;&#31867;&#21644;&#35789;&#24615;&#25968;&#25454;&#38598;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;gpt-3.5-t&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#23427;&#21253;&#25324;&#25991;&#26412;&#32423;&#24773;&#24863;&#26497;&#24615;&#20998;&#31867;&#21644;&#35789;&#32423;&#24773;&#24863;&#26497;&#24615;&#30830;&#23450;&#12290;&#36825;&#31181;&#20998;&#26512;&#25361;&#25112;&#27169;&#22411;&#22312;&#20840;&#38754;&#29702;&#35299;&#25991;&#26412;&#30340;&#21516;&#26102;&#65292;&#25552;&#21462;&#32454;&#24494;&#30340;&#20449;&#24687;&#12290;&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20852;&#36215;&#65292;&#24773;&#24863;&#20998;&#26512;&#30340;&#26032;&#36884;&#24452;&#24471;&#20197;&#24320;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#21333;&#35789;&#21644;&#25972;&#20010;&#25991;&#26412;&#20043;&#38388;&#30340;&#20114;&#30456;&#22686;&#24378;&#25928;&#24212;&#65288;MRE&#65289;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#23427;&#28145;&#20837;&#25506;&#35752;&#20102;&#21333;&#35789;&#26497;&#24615;&#23545;&#25972;&#20010;&#27573;&#33853;&#30340;&#24773;&#24863;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26631;&#27880;&#20102;&#22235;&#20010;&#26032;&#39062;&#30340;&#24773;&#24863;&#25991;&#26412;&#20998;&#31867;&#21644;&#35789;&#24615;&#65288;SCPOS&#65289;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#29616;&#26377;&#30340;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#26500;&#24314;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#25317;&#26377;70&#20159;&#21442;&#25968;&#35268;&#27169;&#30340;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#65288;USA&#65289;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;gpt-3.5-t&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is a pivotal task in the domain of natural language processing. It encompasses both text-level sentiment polarity classification and word-level Part of Speech(POS) sentiment polarity determination. Such analysis challenges models to understand text holistically while also extracting nuanced information. With the rise of Large Language Models(LLMs), new avenues for sentiment analysis have opened. This paper proposes enhancing performance by leveraging the Mutual Reinforcement Effect(MRE) between individual words and the overall text. It delves into how word polarity influences the overarching sentiment of a passage. To support our research, we annotated four novel Sentiment Text Classification and Part of Speech(SCPOS) datasets, building upon existing sentiment classification datasets. Furthermore, we developed a Universal Sentiment Analysis(USA) model, with a 7-billion parameter size. Experimental results revealed that our model surpassed the performance of gpt-3.5-t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#23545;&#35805;&#31995;&#32479;&#12290;&#22312;&#35774;&#35745;&#21644;&#24320;&#21457;&#38454;&#27573;&#65292;LLM&#21487;&#20197;&#24110;&#21161;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12289;&#25552;&#21462;&#23454;&#20307;&#21644;&#21516;&#20041;&#35789;&#12289;&#26412;&#22320;&#21270;&#21644;&#35282;&#33394;&#35774;&#35745;&#12290;&#22312;&#36816;&#33829;&#38454;&#27573;&#65292;LLM&#21487;&#20197;&#36741;&#21161;&#19978;&#19979;&#25991;&#21270;&#12289;&#24847;&#22270;&#20998;&#31867;&#12289;&#33258;&#21160;&#32416;&#27491;&#35805;&#35821;&#12289;&#25913;&#20889;&#22238;&#22797;&#12289;&#25688;&#35201;&#21644;&#20351;&#38381;&#21512;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#31169;&#20154;&#38134;&#34892;&#39046;&#22495;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03748</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Enhancing Pipeline-Based Conversational Agents with Large Language Models. (arXiv:2309.03748v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#23545;&#35805;&#31995;&#32479;&#12290;&#22312;&#35774;&#35745;&#21644;&#24320;&#21457;&#38454;&#27573;&#65292;LLM&#21487;&#20197;&#24110;&#21161;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12289;&#25552;&#21462;&#23454;&#20307;&#21644;&#21516;&#20041;&#35789;&#12289;&#26412;&#22320;&#21270;&#21644;&#35282;&#33394;&#35774;&#35745;&#12290;&#22312;&#36816;&#33829;&#38454;&#27573;&#65292;LLM&#21487;&#20197;&#36741;&#21161;&#19978;&#19979;&#25991;&#21270;&#12289;&#24847;&#22270;&#20998;&#31867;&#12289;&#33258;&#21160;&#32416;&#27491;&#35805;&#35821;&#12289;&#25913;&#20889;&#22238;&#22797;&#12289;&#25688;&#35201;&#21644;&#20351;&#38381;&#21512;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#31169;&#20154;&#38134;&#34892;&#39046;&#22495;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20195;&#29702;&#22120;&#65288;&#22914;GPT-4&#65289;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21830;&#19994;&#21270;&#23545;&#35805;&#31995;&#32479;&#24320;&#21457;&#24037;&#20855;&#26159;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#65292;&#24182;&#19988;&#22312;&#36827;&#34892;&#20154;&#31867;&#23545;&#35805;&#26102;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#22312;&#20197;&#19979;&#20004;&#20010;&#38454;&#27573;&#20013;&#22686;&#24378;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#65306;1&#65289;&#35774;&#35745;&#21644;&#24320;&#21457;&#38454;&#27573;&#65307;2&#65289;&#36816;&#33829;&#38454;&#27573;&#12290;&#22312;1&#65289;&#20013;&#65292;LLM&#21487;&#20197;&#22312;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12289;&#25552;&#21462;&#23454;&#20307;&#21644;&#21516;&#20041;&#35789;&#12289;&#26412;&#22320;&#21270;&#21644;&#35282;&#33394;&#35774;&#35745;&#26041;&#38754;&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;2&#65289;&#20013;&#65292;LLM&#21487;&#20197;&#36741;&#21161;&#19978;&#19979;&#25991;&#21270;&#12289;&#24847;&#22270;&#20998;&#31867;&#20197;&#38450;&#27490;&#23545;&#35805;&#20013;&#26029;&#21644;&#22788;&#29702;&#36229;&#20986;&#33539;&#22260;&#30340;&#38382;&#39064;&#12289;&#33258;&#21160;&#32416;&#27491;&#35805;&#35821;&#12289;&#25913;&#20889;&#22238;&#22797;&#12289;&#21046;&#23450;&#28040;&#27495;&#38382;&#21477;&#12289;&#25688;&#35201;&#21644;&#20351;&#38381;&#21512;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#31169;&#20154;&#38134;&#34892;&#39046;&#22495;&#36827;&#34892;&#20102;&#20351;&#29992;GPT-4&#30340;&#38750;&#27491;&#24335;&#23454;&#39564;&#65292;&#20197;&#23454;&#38469;&#31034;&#20363;&#35777;&#26126;&#19978;&#36848;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The latest advancements in AI and deep learning have led to a breakthrough in large language model (LLM)-based agents such as GPT-4. However, many commercial conversational agent development tools are pipeline-based and have limitations in holding a human-like conversation. This paper investigates the capabilities of LLMs to enhance pipeline-based conversational agents during two phases: 1) in the design and development phase and 2) during operations. In 1) LLMs can aid in generating training data, extracting entities and synonyms, localization, and persona design. In 2) LLMs can assist in contextualization, intent classification to prevent conversational breakdown and handle out-of-scope questions, auto-correcting utterances, rephrasing responses, formulating disambiguation questions, summarization, and enabling closed question-answering capabilities. We conducted informal experiments with GPT-4 in the private banking domain to demonstrate the scenarios above with a practical example.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20116;&#31181;&#27969;&#34892;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#21644;&#25429;&#25417;&#22522;&#26412;&#35821;&#20041;&#23646;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#21457;&#29616;Sentence-Bert&#21644;USE&#27169;&#22411;&#22312;&#25913;&#20889;&#26631;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;LASER&#22312;&#21516;&#20041;&#35789;&#26367;&#25442;&#21644;&#21453;&#20041;&#35789;&#26367;&#25442;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.03747</link><description>&lt;p&gt;
&#21477;&#23376;&#32534;&#30721;&#22120;&#38754;&#20020;&#30340;&#20005;&#23803;&#22256;&#22659;&#65306;&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#25104;&#21151;&#65292;&#22312;&#25429;&#25417;&#22522;&#26412;&#35821;&#20041;&#23646;&#24615;&#19978;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
The Daunting Dilemma with Sentence Encoders: Success on Standard Benchmarks, Failure in Capturing Basic Semantic Properties. (arXiv:2309.03747v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03747
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20116;&#31181;&#27969;&#34892;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#21644;&#25429;&#25417;&#22522;&#26412;&#35821;&#20041;&#23646;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#21457;&#29616;Sentence-Bert&#21644;USE&#27169;&#22411;&#22312;&#25913;&#20889;&#26631;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;LASER&#22312;&#21516;&#20041;&#35789;&#26367;&#25442;&#21644;&#21453;&#20041;&#35789;&#26367;&#25442;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#22238;&#39038;&#24615;&#26041;&#27861;&#30740;&#31350;&#24182;&#27604;&#36739;&#20102;&#20116;&#31181;&#29616;&#26377;&#30340;&#27969;&#34892;&#21477;&#23376;&#32534;&#30721;&#22120;&#65292;&#21363;Sentence-BERT&#12289;Universal Sentence Encoder (USE)&#12289;LASER&#12289;InferSent&#21644;Doc2vec&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#25429;&#25417;&#22522;&#26412;&#35821;&#20041;&#23646;&#24615;&#30340;&#33021;&#21147;&#26041;&#38754;&#12290;&#21021;&#22987;&#26102;&#65292;&#25105;&#20204;&#22312;&#27969;&#34892;&#30340;SentEval&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#36825;&#20116;&#31181;&#21477;&#23376;&#32534;&#30721;&#22120;&#65292;&#24182;&#21457;&#29616;&#22810;&#31181;&#21477;&#23376;&#32534;&#30721;&#22120;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#27809;&#26377;&#25214;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#20248;&#32988;&#32773;&#65292;&#22240;&#27492;&#25105;&#20204;&#35774;&#35745;&#20102;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#26469;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#30340;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#35821;&#20041;&#35780;&#20272;&#26631;&#20934;&#65292;&#21363;&#25913;&#20889;&#12289;&#21516;&#20041;&#35789;&#26367;&#25442;&#12289;&#21453;&#20041;&#35789;&#26367;&#25442;&#21644;&#21477;&#23376;&#28151;&#20081;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26631;&#20934;&#35780;&#20272;&#20102;&#21516;&#26679;&#30340;&#20116;&#31181;&#21477;&#23376;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;Sentence-Bert&#21644;USE&#27169;&#22411;&#36890;&#36807;&#20102;&#25913;&#20889;&#26631;&#20934;&#65292;&#20854;&#20013;SBERT&#22312;&#20004;&#32773;&#20043;&#38388;&#26356;&#20026;&#20248;&#36234;&#12290;LASER&#22312;&#21516;&#20041;&#35789;&#26367;&#25442;&#21644;&#21453;&#20041;&#35789;&#26367;&#25442;&#26631;&#20934;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we adopted a retrospective approach to examine and compare five existing popular sentence encoders, i.e., Sentence-BERT, Universal Sentence Encoder (USE), LASER, InferSent, and Doc2vec, in terms of their performance on downstream tasks versus their capability to capture basic semantic properties. Initially, we evaluated all five sentence encoders on the popular SentEval benchmark and found that multiple sentence encoders perform quite well on a variety of popular downstream tasks. However, being unable to find a single winner in all cases, we designed further experiments to gain a deeper understanding of their behavior. Specifically, we proposed four semantic evaluation criteria, i.e., Paraphrasing, Synonym Replacement, Antonym Replacement, and Sentence Jumbling, and evaluated the same five sentence encoders using these criteria. We found that the Sentence-Bert and USE models pass the paraphrasing criterion, with SBERT being the superior between the two. LASER dominates 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38889;&#35821;&#20013;&#30340;&#35789;&#35821;&#20998;&#21106;&#31890;&#24230;&#65292;&#24182;&#21457;&#29616;&#22312;&#30701;&#35821;&#32467;&#26500;&#35299;&#26512;&#20013;&#65292;&#20165;&#20998;&#21106;&#21151;&#33021;&#24418;&#24577;&#32032;&#24182;&#20445;&#30041;&#20854;&#20182;&#21518;&#32512;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03713</link><description>&lt;p&gt;
&#38889;&#35821;&#20013;&#30340;&#35789;&#35821;&#20998;&#21106;&#31890;&#24230;
&lt;/p&gt;
&lt;p&gt;
Word segmentation granularity in Korean. (arXiv:2309.03713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38889;&#35821;&#20013;&#30340;&#35789;&#35821;&#20998;&#21106;&#31890;&#24230;&#65292;&#24182;&#21457;&#29616;&#22312;&#30701;&#35821;&#32467;&#26500;&#35299;&#26512;&#20013;&#65292;&#20165;&#20998;&#21106;&#21151;&#33021;&#24418;&#24577;&#32032;&#24182;&#20445;&#30041;&#20854;&#20182;&#21518;&#32512;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#38889;&#35821;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#35789;&#35821;&#20998;&#21106;&#31890;&#24230;&#12290;&#20174;&#20197;&#31354;&#26684;&#20998;&#38548;&#30340;&#35789;&#35821;&#65288;&#31216;&#20026;&#36766;&#35789;&#65289;&#21040;&#38889;&#35821;&#20013;&#30340;&#19968;&#31995;&#21015;&#24418;&#24577;&#32032;&#65292;&#38889;&#35821;&#20013;&#23384;&#22312;&#22810;&#20010;&#21487;&#33021;&#30340;&#35789;&#35821;&#20998;&#21106;&#31890;&#24230;&#12290;&#38024;&#23545;&#29305;&#23450;&#30340;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#26009;&#24211;&#26631;&#27880;&#20219;&#21153;&#65292;&#24050;&#32463;&#25552;&#20986;&#24182;&#24212;&#29992;&#20102;&#22810;&#20010;&#19981;&#21516;&#30340;&#20998;&#21106;&#31890;&#24230;&#27700;&#24179;&#65292;&#22240;&#20026;&#21253;&#25324;&#38889;&#35821;&#22312;&#20869;&#30340;&#20957;&#32858;&#35821;&#35328;&#20855;&#26377;&#21151;&#33021;&#24418;&#24577;&#32032;&#21644;&#21477;&#27861;&#33539;&#30068;&#20043;&#38388;&#30340;&#19968;&#23545;&#19968;&#26144;&#23556;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#19981;&#21516;&#30340;&#20998;&#21106;&#31890;&#24230;&#27700;&#24179;&#65292;&#24182;&#25552;&#20379;&#20102;&#38889;&#35821;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#31034;&#20363;&#20197;&#20379;&#21442;&#32771;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21482;&#20998;&#38548;&#21253;&#25324;&#26684;&#26631;&#35760;&#21644;&#21160;&#35789;&#35789;&#23614;&#22312;&#20869;&#30340;&#21151;&#33021;&#24418;&#24577;&#32032;&#65292;&#24182;&#20445;&#30041;&#20854;&#20182;&#21518;&#32512;&#29992;&#20110;&#35789;&#27861;&#27966;&#29983;&#21487;&#20197;&#24471;&#21040;&#26368;&#20339;&#30340;&#30701;&#35821;&#32467;&#26500;&#35299;&#26512;&#24615;&#33021;&#12290;&#36825;&#19982;&#27492;&#21069;&#38889;&#35821;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#20339;&#23454;&#36341;&#30456;&#30683;&#30462;&#65292;&#36825;&#19968;&#23454;&#36341;&#24050;&#32463;&#25104;&#20026;
&lt;/p&gt;
&lt;p&gt;
This paper describes word {segmentation} granularity in Korean language processing. From a word separated by blank space, which is termed an eojeol, to a sequence of morphemes in Korean, there are multiple possible levels of word segmentation granularity in Korean. For specific language processing and corpus annotation tasks, several different granularity levels have been proposed and utilized, because the agglutinative languages including Korean language have a one-to-one mapping between functional morpheme and syntactic category. Thus, we analyze these different granularity levels, presenting the examples of Korean language processing systems for future reference. Interestingly, the granularity by separating only functional morphemes including case markers and verbal endings, and keeping other suffixes for morphological derivation results in the optimal performance for phrase structure parsing. This contradicts previous best practices for Korean language processing, which has been th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20174;&#25968;&#23398;&#38382;&#39064;&#20013;&#29983;&#25104;Prolog&#35859;&#35789;&#30340;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21644;&#20351;&#29992;&#24605;&#32500;&#38142;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;Prolog&#20195;&#30721;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.03667</link><description>&lt;p&gt;
&#20174;&#25968;&#23398;&#38382;&#39064;&#20013;&#29983;&#25104;Prolog&#35859;&#35789;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploring an LM to generate Prolog Predicates from Mathematics Questions. (arXiv:2309.03667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03667
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20174;&#25968;&#23398;&#38382;&#39064;&#20013;&#29983;&#25104;Prolog&#35859;&#35789;&#30340;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21644;&#20351;&#29992;&#24605;&#32500;&#38142;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;Prolog&#20195;&#30721;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;ChatGPT&#39537;&#21160;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20852;&#36259;&#24613;&#21095;&#22686;&#21152;&#12290;ChatGPT&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#22823;&#35268;&#27169;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20219;&#21153;&#19978;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#25512;&#29702;&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#36890;&#24120;&#34920;&#29616;&#36739;&#24046;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#36890;&#36807;&#24605;&#32500;&#38142;&#28608;&#21169;&#22312;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#29616;&#22312;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#26159;&#21542;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#26469;&#29983;&#25104;&#36923;&#36753;&#35821;&#35328;&#65288;Prolog&#65289;&#20195;&#30721;&#65292;&#24182;&#23558;&#36825;&#20123;&#20195;&#30721;&#20256;&#36882;&#32473;&#32534;&#35793;&#22120;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#24605;&#32500;&#38142;&#26469;&#24494;&#35843;LLaMA7B&#20316;&#20026;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20854;&#20182;&#29992;&#20110;&#29983;&#25104;Prolog&#20195;&#30721;&#12289;Prolog&#20195;&#30721;+&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#38142;+Prolog&#20195;&#30721;&#30340;&#24494;&#35843;LLaMA7B&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;Prolog&#29983;&#25104;&#27169;&#22411;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#36824;&#20381;&#28982;&#24605;&#32500;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a surge in interest in NLP driven by ChatGPT. ChatGPT, a transformer-based generative language model of substantial scale, exhibits versatility in performing various tasks based on natural language. Nevertheless, large language models often exhibit poor performance in solving mathematics questions that require reasoning. Prior research has demonstrated the effectiveness of chain-of-thought prompting in enhancing reasoning capabilities. Now, we aim to investigate whether fine-tuning a model for the generation of Prolog codes, a logic language, and subsequently passing these codes to a compiler can further improve accuracy. Consequently, we employ chain-of-thought to fine-tune LLaMA7B as a baseline model and develop other fine-tuned LLaMA7B models for the generation of Prolog code, Prolog code + chain-of-thought, and chain-of-thought + Prolog code, respectively. The results reveal that the Prolog generation model surpasses the baseline in performance, while the c
&lt;/p&gt;</description></item><item><title>BNS-Net&#26159;&#19968;&#31181;&#21452;&#36890;&#36947;&#35773;&#21050;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#34892;&#20026;&#32423;&#21644;&#21477;&#23376;&#32423;&#30340;&#20914;&#31361;&#20449;&#24687;&#65292;&#26377;&#25928;&#25429;&#25417;&#35773;&#21050;&#34920;&#36798;&#20013;&#30340;&#38544;&#21547;&#24773;&#24863;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.03658</link><description>&lt;p&gt;
BNS-Net:&#19968;&#31181;&#32771;&#34385;&#34892;&#20026;&#32423;&#21644;&#21477;&#23376;&#32423;&#20914;&#31361;&#30340;&#21452;&#36890;&#36947;&#35773;&#21050;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BNS-Net: A Dual-channel Sarcasm Detection Method Considering Behavior-level and Sentence-level Conflicts. (arXiv:2309.03658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03658
&lt;/p&gt;
&lt;p&gt;
BNS-Net&#26159;&#19968;&#31181;&#21452;&#36890;&#36947;&#35773;&#21050;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#34892;&#20026;&#32423;&#21644;&#21477;&#23376;&#32423;&#30340;&#20914;&#31361;&#20449;&#24687;&#65292;&#26377;&#25928;&#25429;&#25417;&#35773;&#21050;&#34920;&#36798;&#20013;&#30340;&#38544;&#21547;&#24773;&#24863;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35773;&#21050;&#26816;&#27979;&#26159;&#19968;&#31181;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#26088;&#22312;&#30830;&#23450;&#32473;&#23450;&#35805;&#35821;&#26159;&#21542;&#20855;&#26377;&#35773;&#21050;&#24847;&#21619;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#35773;&#21050;&#26816;&#27979;&#24050;&#20174;&#32463;&#20856;&#30340;&#27169;&#24335;&#35782;&#21035;&#21457;&#23637;&#21040;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#24120;&#29992;&#30340;&#29305;&#24449;&#21253;&#25324;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#12289;&#26631;&#28857;&#31526;&#21495;&#21644;&#24773;&#24863;&#35789;&#27719;&#12290;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#35773;&#21050;&#34920;&#36798;&#20013;&#65292;&#27809;&#26377;&#26126;&#30830;&#24773;&#24863;&#32447;&#32034;&#30340;&#34892;&#20026;&#24120;&#24120;&#20316;&#20026;&#38544;&#21547;&#24773;&#24863;&#24847;&#20041;&#30340;&#36733;&#20307;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;BNS-Net&#30340;&#21452;&#36890;&#36947;&#35773;&#21050;&#26816;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#36890;&#36947;&#20013;&#32771;&#34385;&#20102;&#34892;&#20026;&#21644;&#21477;&#23376;&#20914;&#31361;&#12290;&#36890;&#36947;1: &#34892;&#20026;&#32423;&#20914;&#31361;&#36890;&#36947;&#22522;&#20110;&#26680;&#24515;&#21160;&#35789;&#37325;&#26500;&#25991;&#26412;&#65292;&#21033;&#29992;&#20462;&#25913;&#21518;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#31361;&#20986;&#20914;&#31361;&#20449;&#24687;&#12290;&#36890;&#36947;2: &#21477;&#23376;&#32423;&#20914;&#31361;&#36890;&#36947;&#24341;&#20837;&#22806;&#37096;&#24773;&#24863;&#30693;&#35782;&#23558;&#25991;&#26412;&#20998;&#21106;&#20026;&#26126;&#30830;&#21644;&#38544;&#21547;&#21477;&#23376;&#65292;&#25429;&#25417;&#23427;&#20204;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sarcasm detection is a binary classification task that aims to determine whether a given utterance is sarcastic. Over the past decade, sarcasm detection has evolved from classical pattern recognition to deep learning approaches, where features such as user profile, punctuation and sentiment words have been commonly employed for sarcasm detection. In real-life sarcastic expressions, behaviors without explicit sentimental cues often serve as carriers of implicit sentimental meanings. Motivated by this observation, we proposed a dual-channel sarcasm detection model named BNS-Net. The model considers behavior and sentence conflicts in two channels. Channel 1: Behavior-level Conflict Channel reconstructs the text based on core verbs while leveraging the modified attention mechanism to highlight conflict information. Channel 2: Sentence-level Conflict Channel introduces external sentiment knowledge to segment the text into explicit and implicit sentences, capturing conflicts between them. To
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25506;&#32034;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.03613</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#20005;&#35880;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating ChatGPT as a Recommender System: A Rigorous Approach. (arXiv:2309.03613v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25506;&#32034;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#22823;&#22411;AI&#35821;&#35328;&#27169;&#22411;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#23427;&#20204;&#22312;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#65292;&#21253;&#25324;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#65292;&#22240;&#27492;&#23545;&#20110;&#21508;&#31181;&#29305;&#23450;&#20219;&#21153;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#37322;&#25918;&#20102;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#30740;&#31350;&#30028;&#27491;&#22312;&#31215;&#26497;&#25506;&#32034;&#23427;&#20204;&#30340;&#24212;&#29992;&#65292;ChatGPT&#20063;&#22240;&#27492;&#33719;&#24471;&#20102;&#35748;&#21487;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#26377;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#20854;&#22312;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#20173;&#24453;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#25506;&#31350;ChatGPT&#20316;&#20026;&#38646;-shot&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#21253;&#25324;&#35780;&#20272;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#29616;&#26377;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#30340;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;MovieLens Small&#12289;Last.FM&#21644;Facebook Bo&#65289;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#26469;&#35780;&#20272;ChatGPT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent popularity surrounds large AI language models due to their impressive natural language capabilities. They contribute significantly to language-related tasks, including prompt-based learning, making them valuable for various specific tasks. This approach unlocks their full potential, enhancing precision and generalization. Research communities are actively exploring their applications, with ChatGPT receiving recognition. Despite extensive research on large language models, their potential in recommendation scenarios still needs to be explored. This study aims to fill this gap by investigating ChatGPT's capabilities as a zero-shot recommender system. Our goals include evaluating its ability to use user preferences for recommendations, reordering existing recommendation lists, leveraging information from similar users, and handling cold-start situations. We assess ChatGPT's performance through comprehensive experiments using three datasets (MovieLens Small, Last.FM, and Facebook Bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22797;&#26434;&#20889;&#20316;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;&#23427;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#20869;&#23481;&#24314;&#35758;&#21644;&#29983;&#25104;&#22823;&#37327;&#35821;&#35328;&#27491;&#30830;&#30340;&#25991;&#26412;&#26469;&#21152;&#24555;&#24037;&#20316;&#27969;&#31243;&#65292;&#20294;&#23427;&#38656;&#35201;&#19987;&#23478;&#30417;&#30563;&#65292;&#24182;&#19988;&#21487;&#33021;&#20135;&#29983;&#32932;&#27973;&#25110;&#19981;&#30456;&#20851;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2309.03595</link><description>&lt;p&gt;
&#22810;&#35328;&#22810;&#35821;&#19982;&#26174;&#24615;&#24773;&#24863;&#65306;ChatGPT&#20316;&#20026;&#25919;&#31574;&#39038;&#38382;
&lt;/p&gt;
&lt;p&gt;
Loquacity and Visible Emotion: ChatGPT as a Policy Advisor. (arXiv:2309.03595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22797;&#26434;&#20889;&#20316;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;&#23427;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#20869;&#23481;&#24314;&#35758;&#21644;&#29983;&#25104;&#22823;&#37327;&#35821;&#35328;&#27491;&#30830;&#30340;&#25991;&#26412;&#26469;&#21152;&#24555;&#24037;&#20316;&#27969;&#31243;&#65292;&#20294;&#23427;&#38656;&#35201;&#19987;&#23478;&#30417;&#30563;&#65292;&#24182;&#19988;&#21487;&#33021;&#20135;&#29983;&#32932;&#27973;&#25110;&#19981;&#30456;&#20851;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#27454;&#23547;&#27714;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#33021;&#21147;&#30340;&#36719;&#20214;&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26377;&#26102;&#23427;&#34987;&#25551;&#36848;&#20026;&#19968;&#27454;&#24320;&#21019;&#24615;&#30340;&#29983;&#20135;&#21147;&#36741;&#21161;&#24037;&#20855;&#65292;&#21253;&#25324;&#29992;&#20110;&#21019;&#24847;&#24037;&#20316;&#12290;&#26412;&#25991;&#36890;&#36807;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;ChatGPT&#22312;&#22797;&#26434;&#20889;&#20316;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35201;&#27714;&#35813;&#36719;&#20214;&#20026;&#24847;&#22823;&#21033;&#38134;&#34892;&#33891;&#20107;&#20250;&#25776;&#20889;&#19968;&#20221;&#25919;&#31574;&#31616;&#25253;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#33021;&#22815;&#36890;&#36807;&#25552;&#20379;&#33391;&#22909;&#32467;&#26500;&#30340;&#20869;&#23481;&#24314;&#35758;&#20197;&#21450;&#22312;&#20960;&#31186;&#38047;&#20869;&#29983;&#25104;&#22823;&#37327;&#30340;&#35821;&#35328;&#27491;&#30830;&#30340;&#25991;&#26412;&#65292;&#21152;&#24555;&#24037;&#20316;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#23427;&#38656;&#35201;&#22823;&#37327;&#19987;&#23478;&#30417;&#30563;&#65292;&#36825;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25269;&#28040;&#20102;&#29983;&#20135;&#21147;&#30340;&#22686;&#30410;&#12290;&#22914;&#26524;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#24471;&#19981;&#22815;&#35880;&#24910;&#65292;&#36755;&#20986;&#21487;&#33021;&#20250;&#19981;&#27491;&#30830;&#12289;&#32932;&#27973;&#25110;&#19981;&#30456;&#20851;&#12290;&#32932;&#27973;&#24615;&#22312;&#38754;&#21521;&#39640;&#32423;&#21463;&#20247;&#30340;&#25919;&#31574;&#24314;&#35758;&#32972;&#26223;&#19979;&#23588;&#20854;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, a software seeking to simulate human conversational abilities, is attracting increasing attention. It is sometimes portrayed as a groundbreaking productivity aid, including for creative work. In this paper, we run an experiment to assess its potential in complex writing tasks. We ask the software to compose a policy brief for the Board of the Bank of Italy. We find that ChatGPT can accelerate workflows by providing well-structured content suggestions, and by producing extensive, linguistically correct text in a matter of seconds. It does, however, require a significant amount of expert supervision, which partially offsets productivity gains. If the app is used naively, output can be incorrect, superficial, or irrelevant. Superficiality is an especially problematic limitation in the context of policy advice intended for high-level audiences.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#32467;&#26524;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.03564</link><description>&lt;p&gt;
&#35780;&#20272;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media. (arXiv:2309.03564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#32467;&#26524;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#31867;&#20284;&#24555;&#36895;&#21457;&#23637;&#30340;GPT&#31995;&#21015;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24433;&#21709;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#24515;&#29702;&#23398;&#31561;&#21307;&#23398;&#39046;&#22495;&#23545;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#27987;&#21402;&#20852;&#36259;&#65292;&#20294;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#20855;&#20307;&#25506;&#32034;&#20173;&#28982;&#24456;&#23569;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#36234;&#26469;&#36234;&#22810;&#22320;&#34920;&#36798;&#20010;&#20154;&#24773;&#24863;&#65307;&#22312;&#29305;&#23450;&#30340;&#20027;&#39064;&#19979;&#65292;&#36825;&#20123;&#24773;&#24863;&#36890;&#24120;&#34920;&#29616;&#20026;&#28040;&#26497;&#24773;&#32490;&#65292;&#26377;&#26102;&#20250;&#21319;&#32423;&#20026;&#33258;&#26432;&#20542;&#21521;&#12290;&#21450;&#26102;&#36776;&#35782;&#36825;&#26679;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#23545;&#26377;&#25928;&#24178;&#39044;&#21644;&#28508;&#22312;&#36991;&#20813;&#20005;&#37325;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#36827;&#34892;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#33258;&#26432;&#39118;&#38505;&#21644;&#35748;&#30693;&#20559;&#24046;&#35782;&#21035;&#30340;&#23454;&#39564;&#65292;&#36827;&#20837;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#65306;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#24494;&#35843;&#65292;&#32771;&#23519;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models, particularly those akin to the rapidly progressing GPT series, are gaining traction for their expansive influence. While there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. Concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. Timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. Our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on Chinese social media platforms. Using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tunin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;One-to-All&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#36890;&#36807;&#27604;&#36739;&#36755;&#20837;&#35805;&#35821;&#19982;&#25152;&#26377;&#26631;&#31614;&#20505;&#36873;&#39033;&#26469;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#35821;&#20041;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#31574;&#30053;&#23454;&#29616;&#20102;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.03563</link><description>&lt;p&gt;
&#20840;&#37096;&#26631;&#31614;&#22312;&#19968;&#36215;&#65306;&#22522;&#20110;&#39640;&#25928;&#30340;&#26631;&#31614;&#35821;&#20041;&#32534;&#30721;&#33539;&#24335;&#30340;&#20302;&#36164;&#28304;&#24847;&#22270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
All Labels Together: Low-shot Intent Detection with an Efficient Label Semantic Encoding Paradigm. (arXiv:2309.03563v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03563
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;One-to-All&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#36890;&#36807;&#27604;&#36739;&#36755;&#20837;&#35805;&#35821;&#19982;&#25152;&#26377;&#26631;&#31614;&#20505;&#36873;&#39033;&#26469;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#35821;&#20041;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#31574;&#30053;&#23454;&#29616;&#20102;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;&#21033;&#29992;&#24847;&#22270;&#26631;&#31614;&#30340;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#20449;&#24687;&#23545;&#20110;&#23569;&#26679;&#26412;&#22330;&#26223;&#21487;&#33021;&#29305;&#21035;&#26377;&#30410;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#35201;&#20040;&#24573;&#30053;&#20102;&#24847;&#22270;&#26631;&#31614;&#65292;&#65288;&#20363;&#22914;&#23558;&#24847;&#22270;&#35270;&#20026;&#32034;&#24341;&#65289;&#65292;&#35201;&#20040;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#65288;&#20363;&#22914;&#20165;&#20351;&#29992;&#37096;&#20998;&#24847;&#22270;&#26631;&#31614;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;One-to-All&#31995;&#32479;&#65292;&#21487;&#20197;&#23558;&#36755;&#20837;&#35805;&#35821;&#19982;&#25152;&#26377;&#26631;&#31614;&#20505;&#36873;&#39033;&#36827;&#34892;&#27604;&#36739;&#12290;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#35821;&#20041;&#12290;&#22312;&#19977;&#20010;&#23569;&#26679;&#26412;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#35757;&#32451;&#36164;&#28304;&#26497;&#20026;&#26377;&#38480;&#26102;&#65292;One-to-All&#29305;&#21035;&#26377;&#25928;&#65292;&#22312;1-shot&#12289;3-shot&#21644;5-shot&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#20102;&#20174;&#37322;&#20041;&#24471;&#21040;&#30340;&#38388;&#25509;&#30417;&#30563;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#23545;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#30340;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20301;&#20110;https://github.com/jian
&lt;/p&gt;
&lt;p&gt;
In intent detection tasks, leveraging meaningful semantic information from intent labels can be particularly beneficial for few-shot scenarios. However, existing few-shot intent detection methods either ignore the intent labels, (e.g. treating intents as indices) or do not fully utilize this information (e.g. only using part of the intent labels). In this work, we present an end-to-end One-to-All system that enables the comparison of an input utterance with all label candidates. The system can then fully utilize label semantics in this way. Experiments on three few-shot intent detection tasks demonstrate that One-to-All is especially effective when the training resource is extremely scarce, achieving state-of-the-art performance in 1-, 3- and 5-shot settings. Moreover, we present a novel pretraining strategy for our model that utilizes indirect supervision from paraphrasing, enabling zero-shot cross-domain generalization on intent detection tasks. Our code is at https://github.com/jian
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24341;&#25991;&#23383;&#27573;&#23398;&#20064;&#30340;&#38170;&#23450;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;CIFAL&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38170;&#23450;&#23398;&#20064;&#20174;&#19981;&#21516;&#24341;&#25991;&#26679;&#24335;&#30340;&#25968;&#25454;&#20013;&#25429;&#25417;&#24341;&#25991;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#24341;&#25991;&#23383;&#27573;&#23398;&#20064;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;2.83%&#30340;&#23383;&#27573;&#32423;F1&#20998;&#25968;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.03559</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24341;&#25991;&#23383;&#27573;&#23398;&#20064;&#30340;&#38170;&#23450;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Anchor Learning Approach for Citation Field Learning. (arXiv:2309.03559v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03559
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24341;&#25991;&#23383;&#27573;&#23398;&#20064;&#30340;&#38170;&#23450;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;CIFAL&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38170;&#23450;&#23398;&#20064;&#20174;&#19981;&#21516;&#24341;&#25991;&#26679;&#24335;&#30340;&#25968;&#25454;&#20013;&#25429;&#25417;&#24341;&#25991;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#24341;&#25991;&#23383;&#27573;&#23398;&#20064;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;2.83%&#30340;&#23383;&#27573;&#32423;F1&#20998;&#25968;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#25991;&#23383;&#27573;&#23398;&#20064;&#26159;&#23558;&#24341;&#25991;&#23383;&#31526;&#20018;&#20998;&#21106;&#20026;&#24863;&#20852;&#36259;&#30340;&#23383;&#27573;&#65292;&#22914;&#20316;&#32773;&#12289;&#26631;&#39064;&#21644;&#22330;&#25152;&#12290;&#20174;&#24341;&#25991;&#20013;&#25552;&#21462;&#36825;&#20123;&#23383;&#27573;&#23545;&#20110;&#24341;&#25991;&#32034;&#24341;&#12289;&#30740;&#31350;&#32773;&#20010;&#20154;&#36164;&#26009;&#20998;&#26512;&#31561;&#38750;&#24120;&#37325;&#35201;&#12290;&#29992;&#25143;&#29983;&#25104;&#30340;&#36164;&#28304;&#65292;&#22914;&#23398;&#26415;&#20027;&#39029;&#21644;&#20010;&#20154;&#31616;&#21382;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24341;&#25991;&#23383;&#27573;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24341;&#25991;&#26679;&#24335;&#19981;&#19968;&#33268;&#12289;&#21477;&#27861;&#19981;&#23436;&#25972;&#21644;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#65292;&#20174;&#36825;&#20123;&#36164;&#28304;&#20013;&#25552;&#21462;&#23383;&#27573;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;CIFAL&#65288;&#36890;&#36807;&#38170;&#23450;&#23398;&#20064;&#36827;&#34892;&#24341;&#25991;&#23383;&#27573;&#23398;&#20064;&#65289;&#65292;&#20197;&#25552;&#39640;&#24341;&#25991;&#23383;&#27573;&#23398;&#20064;&#24615;&#33021;&#12290;CIFAL&#21033;&#29992;&#38170;&#23450;&#23398;&#20064;&#65292;&#23427;&#23545;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#37117;&#26159;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#65292;&#24110;&#21161;&#25429;&#25417;&#19981;&#21516;&#24341;&#25991;&#26679;&#24335;&#30340;&#24341;&#25991;&#27169;&#24335;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CIFAL&#22312;&#24341;&#25991;&#23383;&#27573;&#23398;&#20064;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22312;&#23383;&#27573;&#32423;F1&#20998;&#25968;&#19978;&#21462;&#24471;&#20102;2.83%&#30340;&#25552;&#21319;&#12290;&#22823;&#37327;&#30340;
&lt;/p&gt;
&lt;p&gt;
Citation field learning is to segment a citation string into fields of interest such as author, title, and venue. Extracting such fields from citations is crucial for citation indexing, researcher profile analysis, etc. User-generated resources like academic homepages and Curriculum Vitae, provide rich citation field information. However, extracting fields from these resources is challenging due to inconsistent citation styles, incomplete sentence syntax, and insufficient training data. To address these challenges, we propose a novel algorithm, CIFAL (citation field learning by anchor learning), to boost the citation field learning performance. CIFAL leverages the anchor learning, which is model-agnostic for any Pre-trained Language Model, to help capture citation patterns from the data of different citation styles. The experiments demonstrate that CIFAL outperforms state-of-the-art methods in citation field learning, achieving a 2.83% improvement in field-level F1-scores. Extensive an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#25581;&#31034;&#20102;&#38750;&#27861;&#25353;&#25705;&#19994;&#20013;&#21592;&#24037;&#38754;&#20020;&#30340;&#21171;&#21160;&#21387;&#21147;&#21644;&#35821;&#35328;&#38556;&#30861;&#20197;&#21450;&#36141;&#20080;&#24615;&#26381;&#21153;&#32773;&#30340;&#25910;&#20837;&#12289;&#20154;&#21475;&#32479;&#35745;&#21644;&#31038;&#20250;&#21387;&#21147;&#65292;&#20197;&#24110;&#21161;&#25171;&#20987;&#20154;&#21475;&#36137;&#21334;&#12290;</title><link>http://arxiv.org/abs/2309.03470</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20855;&#20307;&#25928;&#26524;&#20013;&#30340;&#24212;&#29992;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25581;&#31034;&#38750;&#27861;&#25353;&#25705;&#19994;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#23454;&#29616;&#35302;&#35273;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Tangible Effects: Natural Language Processing for Uncovering the Illicit Massage Industry &amp; Computer Vision for Tactile Sensing. (arXiv:2309.03470v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#25581;&#31034;&#20102;&#38750;&#27861;&#25353;&#25705;&#19994;&#20013;&#21592;&#24037;&#38754;&#20020;&#30340;&#21171;&#21160;&#21387;&#21147;&#21644;&#35821;&#35328;&#38556;&#30861;&#20197;&#21450;&#36141;&#20080;&#24615;&#26381;&#21153;&#32773;&#30340;&#25910;&#20837;&#12289;&#20154;&#21475;&#32479;&#35745;&#21644;&#31038;&#20250;&#21387;&#21147;&#65292;&#20197;&#24110;&#21161;&#25171;&#20987;&#20154;&#21475;&#36137;&#21334;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#25506;&#35752;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#35745;&#31639;&#26426;&#31185;&#23398;&#22914;&#20309;&#29992;&#20110;&#25171;&#20987;&#20154;&#21475;&#36137;&#21334;&#65311;&#35745;&#31639;&#26426;&#35270;&#35273;&#22914;&#20309;&#21019;&#24314;&#35302;&#35273;&#24863;&#30693;&#65311;&#25105;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30417;&#25511;&#32654;&#22269;&#30340;&#38750;&#27861;&#25353;&#25705;&#19994;&#65288;IMI&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20215;&#20540;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#34892;&#19994;&#65292;&#19981;&#20165;&#25552;&#20379;&#27835;&#30103;&#24615;&#25353;&#25705;&#65292;&#36824;&#25552;&#20379;&#21830;&#19994;&#24615;&#26381;&#21153;&#12290;&#35813;&#34892;&#19994;&#30340;&#21592;&#24037;&#36890;&#24120;&#26159;&#23569;&#26377;&#24037;&#20316;&#26426;&#20250;&#30340;&#31227;&#27665;&#22919;&#22899;&#65292;&#23481;&#26131;&#21463;&#21040;&#27450;&#35784;&#12289;&#32961;&#36843;&#21644;&#20854;&#20182;&#20154;&#21475;&#36137;&#21334;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#30417;&#27979;&#31354;&#38388;&#26102;&#38388;&#36235;&#21183;&#26377;&#21161;&#20110;&#39044;&#38450;&#38750;&#27861;&#25353;&#25705;&#19994;&#20013;&#30340;&#20154;&#21475;&#36137;&#21334;&#12290;&#36890;&#36807;&#21019;&#24314;&#21253;&#25324;Google Places&#12289;Rubmaps&#21644;AMPReviews&#31561;&#19977;&#20010;&#21487;&#20844;&#24320;&#35775;&#38382;&#30340;&#32593;&#31449;&#30340;&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#35832;&#22914;&#35789;&#34955;&#21644;Word2Vec&#31561;NLP&#25216;&#26415;&#65292;&#25105;&#23637;&#31034;&#20102;&#22914;&#20309;&#27934;&#23519;&#21592;&#24037;&#38754;&#20020;&#30340;&#21171;&#21160;&#21387;&#21147;&#21644;&#35821;&#35328;&#38556;&#30861;&#65292;&#20197;&#21450;&#24433;&#21709;&#36141;&#20080;&#24615;&#26381;&#21153;&#32773;&#30340;&#25910;&#20837;&#12289;&#20154;&#21475;&#32479;&#35745;&#21644;&#31038;&#20250;&#21387;&#21147;&#12290;&#25105;&#21628;&#21505;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#34892;&#21160;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
I explore two questions in this thesis: how can computer science be used to fight human trafficking? And how can computer vision create a sense of touch?  I use natural language processing (NLP) to monitor the United States illicit massage industry (IMI), a multi-billion dollar industry that offers not just therapeutic massages but also commercial sexual services. Employees of this industry are often immigrant women with few job opportunities, leaving them vulnerable to fraud, coercion, and other facets of human trafficking. Monitoring spatiotemporal trends helps prevent trafficking in the IMI. By creating datasets with three publicly-accessible websites: Google Places, Rubmaps, and AMPReviews, combined with NLP techniques such as bag-of-words and Word2Vec, I show how to derive insights into the labor pressures and language barriers that employees face, as well as the income, demographics, and societal pressures affecting sex buyers. I include a call-to-action to other researchers give
&lt;/p&gt;</description></item><item><title>XGen-7B&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20811;&#26381;&#24320;&#28304;LLMs&#22312;&#25903;&#25345;&#38271;&#24207;&#21015;&#38271;&#24230;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#21462;&#24471;&#19982;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;LLMs&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#25512;&#36827;&#20102;&#30740;&#31350;&#36827;&#23637;&#21644;&#21830;&#19994;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.03450</link><description>&lt;p&gt;
XGen-7B&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
XGen-7B Technical Report. (arXiv:2309.03450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03450
&lt;/p&gt;
&lt;p&gt;
XGen-7B&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20811;&#26381;&#24320;&#28304;LLMs&#22312;&#25903;&#25345;&#38271;&#24207;&#21015;&#38271;&#24230;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#21462;&#24471;&#19982;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;LLMs&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#25512;&#36827;&#20102;&#30740;&#31350;&#36827;&#23637;&#21644;&#21830;&#19994;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21464;&#24471;&#26222;&#36941;&#65292;&#25913;&#21464;&#20102;&#25105;&#20204;&#19982;&#20449;&#24687;&#20132;&#20114;&#21644;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#39640;&#24615;&#33021;&#30340;LLMs&#20173;&#28982;&#21463;&#38480;&#20110;&#19987;&#26377;&#22681;&#22721;&#65292;&#38459;&#30861;&#20102;&#31185;&#23398;&#36827;&#23637;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22810;&#25968;&#24320;&#28304;&#30340;LLMs&#22312;&#25903;&#25345;&#36739;&#38271;&#24207;&#21015;&#38271;&#24230;&#26041;&#38754;&#26377;&#38480;&#65292;&#32780;&#36825;&#23545;&#20110;&#35768;&#22810;&#38656;&#35201;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#36827;&#34892;&#25512;&#29702;&#30340;&#20219;&#21153;&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;XGen&#65292;&#19968;&#31995;&#21015;7B&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#21487;&#25903;&#25345;&#38271;&#24230;&#20026;8K&#30340;&#24207;&#21015;&#21644;1.5T&#20010;&#20196;&#29260;&#12290;&#25105;&#20204;&#36824;&#23545;XGen&#27169;&#22411;&#36827;&#34892;&#20102;&#20844;&#20849;&#39046;&#22495;&#25945;&#23398;&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#21019;&#24314;&#20102;&#23427;&#20204;&#30340;&#25945;&#23398;&#20248;&#21270;&#29256;&#26412;&#65288;XGen-Inst&#65289;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#24320;&#28304;&#65292;&#29992;&#20110;&#30740;&#31350;&#36827;&#23637;&#21644;&#21830;&#19994;&#24212;&#29992;&#12290;&#25105;&#20204;&#23545;&#26631;&#20934;&#22522;&#20934;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;LLMs&#30456;&#27604;&#65292;XGen&#27169;&#22411;&#23454;&#29616;&#20102;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#38024;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#36827;&#34892;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have become ubiquitous across various domains, transforming the way we interact with information and conduct research. However, most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context. To address this, we have trained XGen, a series of 7B parameter models on up to 8K sequence length for up to 1.5T tokens. We have also finetuned the XGen models on public-domain instructional data, creating their instruction-tuned counterparts (XGen-Inst). We open-source our models for both research advancements and commercial applications. Our evaluation on standard benchmarks shows that XGen models achieve comparable or better results when compared with state-of-the-art open-source LLMs. Our targeted evaluation on long sequence modeling task
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20102;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#20986;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#21644;&#28436;&#31034;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22359;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#21644;&#29983;&#25104;&#20851;&#31995;&#30340;&#33258;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.03433</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#65306;&#23545;&#28436;&#31034;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty. (arXiv:2309.03433v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20102;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#20986;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#21644;&#28436;&#31034;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22359;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#21644;&#29983;&#25104;&#20851;&#31995;&#30340;&#33258;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#65288;OIE&#65289;&#20219;&#21153;&#26088;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20107;&#23454;&#65292;&#36890;&#24120;&#20197;&#65288;&#20027;&#20307;&#65292;&#20851;&#31995;&#65292;&#23458;&#20307;&#65289;&#19977;&#20803;&#32452;&#30340;&#24418;&#24335;&#23384;&#22312;&#12290;&#23613;&#31649;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#35299;&#20915;&#22120;&#30340;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#23427;&#20204;&#22312;OIE&#20219;&#21153;&#20013;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#30340;&#65288;&#30417;&#30563;&#65289;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#30001;&#20110;&#23545;&#27169;&#22411;&#24494;&#35843;&#30340;&#38480;&#21046;&#65292;LLMs&#24456;&#38590;&#21306;&#20998;&#26080;&#20851;&#30340;&#19978;&#19979;&#25991;&#21644;&#30456;&#20851;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#20854;&#27425;&#65292;LLMs&#22522;&#20110;&#27010;&#29575;&#33258;&#22238;&#24402;&#29983;&#25104;&#21709;&#24212;&#65292;&#23548;&#33268;&#39044;&#27979;&#30340;&#20851;&#31995;&#32570;&#20047;&#33258;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#25913;&#36827;OIE&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#28436;&#31034;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22359;&#26469;&#22686;&#24378;&#29983;&#25104;&#20851;&#31995;&#30340;&#33258;&#20449;&#24230;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;OIE&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open Information Extraction (OIE) task aims at extracting structured facts from unstructured text, typically in the form of (subject, relation, object) triples. Despite the potential of large language models (LLMs) like ChatGPT as a general task solver, they lag behind state-of-the-art (supervised) methods in OIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant context from relevant relations and generate structured output due to the restrictions on fine-tuning the model. Second, LLMs generates responses autoregressively based on probability, which makes the predicted relations lack confidence. In this paper, we assess the capabilities of LLMs in improving the OIE task. Particularly, we propose various in-context learning strategies to enhance LLM's instruction-following ability and a demonstration uncertainty quantification module to enhance the confidence of the generated relations. Our experiments on three OIE benchmark datasets show that our approach hold
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#39564;&#35777;&#20102;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03412</link><description>&lt;p&gt;
&#20174;&#22522;&#30784;&#21040;&#23545;&#35805;&#24335;&#65306;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Base to Conversational: Japanese Instruction Dataset and Tuning Large Language Models. (arXiv:2309.03412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03412
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#39564;&#35777;&#20102;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#21464;&#24471;&#20132;&#20114;&#24615;&#26469;&#35828;&#65292;&#25351;&#20196;&#35843;&#25972;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#33521;&#25991;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20294;&#20854;&#20182;&#35821;&#35328;&#32570;&#20047;&#26126;&#26174;&#12290;&#32780;&#19988;&#65292;&#23427;&#20204;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;&#21644;&#31579;&#36873;&#29616;&#26377;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#35813;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#19968;&#20010;&#26085;&#35821;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#23545;&#26085;&#35821;&#21644;&#33521;&#35821;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#35843;&#25972;&#12290;&#25105;&#20204;&#20174;&#25968;&#37327;&#21644;&#36136;&#37327;&#20004;&#20010;&#35282;&#24230;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#12290;&#32467;&#26524;&#30830;&#35748;&#20102;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#30456;&#23545;&#36739;&#23567;&#30340;LLMs&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#20063;&#33021;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#12289;&#35843;&#25972;&#27169;&#22411;&#21644;&#23454;&#29616;&#22343;&#21487;&#22312;&#32593;&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is essential for large language models (LLMs) to become interactive. While many instruction tuning datasets exist in English, there is a noticeable lack in other languages. Also, their effectiveness has not been well verified in non-English languages. We construct a Japanese instruction dataset by expanding and filtering existing datasets and apply the dataset to a Japanese pre-trained base model. We performed Low-Rank Adaptation (LoRA) tuning on both Japanese and English existing models using our instruction dataset. We evaluated these models from both quantitative and qualitative perspectives. As a result, the effectiveness of Japanese instruction datasets is confirmed. The results also indicate that even with relatively small LLMs, performances in downstream tasks would be improved through instruction tuning. Our instruction dataset, tuned models, and implementation are publicly available online.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03409</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Optimizers. (arXiv:2309.03409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#34429;&#28982;&#22522;&#20110;&#23548;&#25968;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#26159;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#26159;&#27809;&#26377;&#26799;&#24230;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;&#20248;&#21270;&#20219;&#21153;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#25551;&#36848;&#12290;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#65292;LLM&#20174;&#21253;&#21547;&#20808;&#21069;&#29983;&#25104;&#30340;&#35299;&#19982;&#20854;&#20540;&#30340;&#25552;&#31034;&#20013;&#29983;&#25104;&#26032;&#30340;&#35299;&#65292;&#28982;&#21518;&#23545;&#26032;&#30340;&#35299;&#36827;&#34892;&#35780;&#20272;&#24182;&#28155;&#21152;&#21040;&#25552;&#31034;&#20013;&#65292;&#29992;&#20110;&#19979;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;OPRO&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#28982;&#21518;&#36716;&#21521;&#25552;&#31034;&#20248;&#21270;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#33021;&#26368;&#22823;&#21270;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#25351;&#20196;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;LLM&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;OPRO&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#22312;GSM8K&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;8%&#65292;&#22312;Big-Bench Hard&#20219;&#21153;&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
&lt;/p&gt;</description></item><item><title>RoDia&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#26041;&#35328;&#35782;&#21035;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#22320;&#21306;&#30340;2&#23567;&#26102;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#31454;&#20105;&#27169;&#22411;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.03378</link><description>&lt;p&gt;
RoDia: &#19968;&#20221;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#26041;&#35328;&#35782;&#21035;&#30340;&#26032;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RoDia: A New Dataset for Romanian Dialect Identification from Speech. (arXiv:2309.03378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03378
&lt;/p&gt;
&lt;p&gt;
RoDia&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#26041;&#35328;&#35782;&#21035;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#22320;&#21306;&#30340;2&#23567;&#26102;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#31454;&#20105;&#27169;&#22411;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#35328;&#35782;&#21035;&#26159;&#35821;&#38899;&#22788;&#29702;&#21644;&#35821;&#35328;&#25216;&#26415;&#20013;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#21487;&#20197;&#22686;&#24378;&#35832;&#22914;&#35821;&#38899;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#39564;&#35777;&#31561;&#21508;&#31181;&#24212;&#29992;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#24191;&#20026;&#20351;&#29992;&#30340;&#35821;&#35328;&#30340;&#26041;&#35328;&#35782;&#21035;&#19978;&#65292;&#20294;&#23545;&#20110;&#32599;&#39532;&#23612;&#20122;&#36825;&#31181;&#36164;&#28304;&#26377;&#38480;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26041;&#35328;&#35782;&#21035;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RoDia&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#32599;&#39532;&#23612;&#20122;&#26041;&#35328;&#35782;&#21035;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;RoDia&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#32599;&#39532;&#23612;&#20122;&#20116;&#20010;&#19981;&#21516;&#22320;&#21306;&#30340;&#21508;&#31181;&#35821;&#38899;&#26679;&#26412;&#65292;&#28085;&#30422;&#20102;&#22478;&#24066;&#21644;&#20892;&#26449;&#29615;&#22659;&#65292;&#24635;&#20849;&#26377;2&#23567;&#26102;&#30340;&#25163;&#21160;&#26631;&#27880;&#35821;&#38899;&#25968;&#25454;&#12290;&#38500;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#32452;&#21487;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#22522;&#20934;&#30340;&#31454;&#20105;&#27169;&#22411;&#12290;&#26368;&#39640;&#24471;&#20998;&#30340;&#27169;&#22411;&#30340;&#23439;F1&#20998;&#25968;&#20026;59.83%&#65292;&#24494;F1&#20998;&#25968;&#20026;62.08%&#65292;&#35828;&#26126;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;RoDia&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialect identification is a critical task in speech processing and language technology, enhancing various applications such as speech recognition, speaker verification, and many others. While most research studies have been dedicated to dialect identification in widely spoken languages, limited attention has been given to dialect identification in low-resource languages, such as Romanian. To address this research gap, we introduce RoDia, the first dataset for Romanian dialect identification from speech. The RoDia dataset includes a varied compilation of speech samples from five distinct regions of Romania, covering both urban and rural environments, totaling 2 hours of manually annotated speech data. Along with our dataset, we introduce a set of competitive models to be used as baselines for future research. The top scoring model achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%, indicating that the task is challenging. We thus believe that RoDia is a valuable resource
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38899;&#39057;-&#25991;&#26412;&#20849;&#20139;&#28508;&#22312;&#34920;&#31034;&#26469;&#26816;&#27979;&#24187;&#35273;&#65292;&#24182;&#37319;&#29992;&#20934;&#30830;&#35299;&#30721;&#31639;&#27861;&#65292;&#20351;&#36739;&#23567;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19982;&#36739;&#22823;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03340</link><description>&lt;p&gt;
&#20351;&#29992;&#38899;&#39057;-&#25991;&#26412;&#20849;&#20139;&#28508;&#22312;&#34920;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Parameter Efficient Audio Captioning With Faithful Guidance Using Audio-text Shared Latent Representation. (arXiv:2309.03340v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38899;&#39057;-&#25991;&#26412;&#20849;&#20139;&#28508;&#22312;&#34920;&#31034;&#26469;&#26816;&#27979;&#24187;&#35273;&#65292;&#24182;&#37319;&#29992;&#20934;&#30830;&#35299;&#30721;&#31639;&#27861;&#65292;&#20351;&#36739;&#23567;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19982;&#36739;&#22823;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#24050;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#24320;&#21457;&#39044;&#35757;&#32451;&#30340;Transformer&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#36807;&#20110;&#21442;&#25968;&#21270;&#65292;&#22240;&#27492;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#21644;&#20869;&#23384;&#21344;&#29992;&#36807;&#22823;&#30340;&#38382;&#39064;&#65292;&#20351;&#23427;&#20204;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#33258;&#21160;&#21270;&#38899;&#39057;&#23383;&#24149;&#24212;&#29992;&#20013;&#30340;&#36825;&#20004;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#24187;&#35273;&#38899;&#39057;&#23383;&#24149;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#38899;&#39057;-&#25991;&#26412;&#20849;&#20139;&#28508;&#22312;&#31354;&#38388;&#30340;&#30456;&#20284;&#24615;&#23545;&#20110;&#26816;&#27979;&#24187;&#35273;&#26159;&#21512;&#36866;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#25512;&#29702;&#26102;&#20934;&#30830;&#35299;&#30721;&#31639;&#27861;&#65292;&#20351;&#36739;&#23567;&#30340;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#19982;&#20351;&#29992;&#26356;&#22810;&#25968;&#25454;&#35757;&#32451;&#30340;&#36739;&#22823;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#22312;&#26463;&#25628;&#32034;&#35299;&#30721;&#27493;&#39588;&#20013;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#21033;&#29992;&#38899;&#39057;-&#25991;&#26412;&#20849;&#20139;&#28508;&#22312;&#34920;&#31034;&#26469;&#35821;&#20041;&#23545;&#40784;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30456;&#24212;&#30340;&#36755;&#20837;&#38899;&#39057;&#12290;&#20934;&#30830;&#30340;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
There has been significant research on developing pretrained transformer architectures for multimodal-to-text generation tasks. Albeit performance improvements, such models are frequently overparameterized, hence suffer from hallucination and large memory footprint making them challenging to deploy on edge devices. In this paper, we address both these issues for the application of automated audio captioning. First, we propose a data augmentation technique for generating hallucinated audio captions and show that similarity based on an audio-text shared latent space is suitable for detecting hallucination. Then, we propose a parameter efficient inference time faithful decoding algorithm that enables smaller audio captioning models with performance equivalent to larger models trained with more data. During the beam decoding step, the smaller model utilizes an audio-text shared latent representation to semantically align the generated text with corresponding input audio. Faithful guidance 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20805;&#20998;&#35757;&#32451;&#65292;&#19968;&#20010;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20197;&#20960;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-4&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#36890;&#36807;&#22312;&#38468;&#21152;&#30340;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#19982;GPT-4&#22312;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#19978;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03241</link><description>&lt;p&gt;
GPT&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20805;&#20998;&#35757;&#32451;&#65292;&#19968;&#20010;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20197;&#20960;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-4&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#36890;&#36807;&#22312;&#38468;&#21152;&#30340;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#19982;GPT-4&#22312;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#19978;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#25191;&#34892;&#31639;&#26415;&#36816;&#31639;&#65292;&#29305;&#21035;&#26159;&#36229;&#36807;8&#20301;&#25968;&#23383;&#30340;&#20056;&#27861;&#65292;&#20197;&#21450;&#28041;&#21450;&#23567;&#25968;&#21644;&#20998;&#25968;&#30340;&#36816;&#31639;&#12290;&#26412;&#25991;&#26088;&#22312;&#25361;&#25112;&#36825;&#31181;&#35823;&#35299;&#12290;&#36890;&#36807;&#20805;&#20998;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#19968;&#20010;&#25317;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20197;&#36817;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#32780;&#19988;&#27809;&#26377;&#25968;&#25454;&#27844;&#38706;&#65292;&#26174;&#33879;&#36229;&#36807;&#20102;GPT-4&#65288;&#20854;&#22810;&#20301;&#25968;&#20056;&#27861;&#20934;&#30830;&#29575;&#20165;&#20026;4.3%&#65289;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;MathGLM&#65292;&#23427;&#26159;&#36890;&#36807;&#22312;&#21253;&#21547;&#20102;&#25991;&#26412;&#25551;&#36848;&#30340;&#38468;&#21152;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#20174;GLM-10B&#24494;&#35843;&#32780;&#25104;&#30340;&#65292;&#23427;&#22312;&#19968;&#20010;&#21253;&#21547;5000&#20010;&#26679;&#26412;&#30340;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#27979;&#35797;&#38598;&#19978;&#30340;&#34920;&#29616;&#19982;GPT-4&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of &gt;8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#25910;&#38598;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#38750;&#20856;&#22411;&#35757;&#32451;&#25968;&#25454;&#12289;&#20998;&#26512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#27880;&#37322;&#26041;&#26696;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#25239;&#32593;&#32476;&#22788;&#29702;&#33258;&#28982;&#28151;&#28102;&#21464;&#37327;&#21644;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#24320;&#21457;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.03238</link><description>&lt;p&gt;
&#38544;&#21547;&#35774;&#35745;&#36873;&#25321;&#21450;&#20854;&#23545;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#24320;&#21457;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation. (arXiv:2309.03238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#25910;&#38598;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#38750;&#20856;&#22411;&#35757;&#32451;&#25968;&#25454;&#12289;&#20998;&#26512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#27880;&#37322;&#26041;&#26696;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#25239;&#32593;&#32476;&#22788;&#29702;&#33258;&#28982;&#28151;&#28102;&#21464;&#37327;&#21644;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#24320;&#21457;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#35782;&#21035;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#24773;&#32490;&#30340;&#24863;&#30693;&#21644;&#34920;&#36798;&#20855;&#26377;&#22266;&#26377;&#30340;&#20027;&#35266;&#24615;&#12290;&#24773;&#32490;&#30340;&#20027;&#35266;&#24615;&#22312;&#24320;&#21457;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#35745;&#31639;&#27169;&#22411;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#35770;&#25991;&#32771;&#23519;&#20102;&#24773;&#32490;&#35782;&#21035;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#20174;&#25910;&#38598;&#26088;&#22312;&#32771;&#34385;&#24773;&#32490;&#20135;&#29983;&#24515;&#29702;&#22240;&#32032;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#24320;&#22987;&#12290;&#20026;&#20102;&#24212;&#23545;&#38750;&#20856;&#22411;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25910;&#38598;&#20102;&#22810;&#27169;&#24577;&#24212;&#28608;&#24773;&#32490;&#25968;&#25454;&#38598;&#65292;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#21463;&#25511;&#30340;&#21387;&#21147;&#22240;&#32032;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#24773;&#32490;&#20135;&#29983;&#30340;&#30495;&#23454;&#29615;&#22659;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#31614;&#20027;&#35266;&#24615;&#38382;&#39064;&#65292;&#35813;&#30740;&#31350;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#27880;&#37322;&#26041;&#26696;&#22914;&#20309;&#24433;&#21709;&#24773;&#32490;&#24863;&#30693;&#21644;&#27880;&#37322;&#32773;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#32593;&#32476;&#26469;&#22788;&#29702;&#33258;&#28982;&#28151;&#28102;&#21464;&#37327;&#21644;&#21464;&#21270;&#65292;&#20197;&#38548;&#31163;&#20851;&#38190;&#22240;&#32032;&#65288;&#22914;&#21387;&#21147;&#65289;&#19982;&#23398;&#20064;&#21040;&#30340;&#24773;&#32490;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition is a complex task due to the inherent subjectivity in both the perception and production of emotions. The subjectivity of emotions poses significant challenges in developing accurate and robust computational models. This thesis examines critical facets of emotion recognition, beginning with the collection of diverse datasets that account for psychological factors in emotion production.  To handle the challenge of non-representative training data, this work collects the Multimodal Stressed Emotion dataset, which introduces controlled stressors during data collection to better represent real-world influences on emotion production. To address issues with label subjectivity, this research comprehensively analyzes how data augmentation techniques and annotation schemes impact emotion perception and annotator labels. It further handles natural confounding variables and variations by employing adversarial networks to isolate key factors like stress from learned emotion rep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#36890;&#36807;&#26500;&#24314;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#21487;&#20197;&#32508;&#21512;&#20998;&#26512;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#20026;&#33647;&#29289;&#20877;&#23450;&#20301;&#30740;&#31350;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2309.03227</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#25581;&#31034;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#30340;&#25216;&#26415;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates. (arXiv:2309.03227v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#36890;&#36807;&#26500;&#24314;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#21487;&#20197;&#32508;&#21512;&#20998;&#26512;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#20026;&#33647;&#29289;&#20877;&#23450;&#20301;&#30740;&#31350;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#20877;&#23450;&#20301;&#26159;&#19968;&#31181;&#21457;&#29616;&#29616;&#26377;&#33647;&#29289;&#26032;&#27835;&#30103;&#29992;&#36884;&#30340;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#36817;&#24180;&#26469;&#22312;&#35745;&#31639;&#31185;&#23398;&#25991;&#29486;&#20013;&#20351;&#29992;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#30340;&#25216;&#26415;&#28508;&#21147;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32508;&#21512;&#20998;&#26512;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#31561;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;s-BKG&#65289;&#65292;&#21253;&#25324;&#26469;&#33258;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30340;&#33647;&#29289;&#12289;&#30142;&#30149;&#21644;&#22522;&#22240;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35782;&#21035;&#22312;s-BKG&#20013;&#19982;&#30446;&#26631;&#30142;&#30149;&#20851;&#32852;&#26377;&#38480;&#20294;&#22312;&#31354;&#38388;&#19978;&#32039;&#23494;&#30456;&#37051;&#30340;&#33647;&#29289;&#20316;&#20026;&#28508;&#22312;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#33647;&#29289;&#19987;&#21033;&#20449;&#24687;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;p-BKG&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug repositioning-a promising strategy for discovering new therapeutic uses for existing drugs-has been increasingly explored in the computational science literature using biomedical databases. However, the technological potential of drug repositioning candidates has often been overlooked. This study presents a novel protocol to comprehensively analyse various sources such as pharmaceutical patents and biomedical databases, and identify drug repositioning candidates with both technological potential and scientific evidence. To this end, first, we constructed a scientific biomedical knowledge graph (s-BKG) comprising relationships between drugs, diseases, and genes derived from biomedical databases. Our protocol involves identifying drugs that exhibit limited association with the target disease but are closely located in the s-BKG, as potential drug candidates. We constructed a patent-informed biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information. Finally, we d
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23478;&#26063;&#21382;&#21490;&#25910;&#38598;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#38754;&#23545;&#38754;&#37319;&#35775;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#35752;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#22320;&#29702;&#21644;&#25216;&#26415;&#38480;&#21046;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#38271;&#30340;&#37319;&#35775;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.03223</link><description>&lt;p&gt;
&#30740;&#31350;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#25910;&#38598;&#23478;&#26063;&#21382;&#21490;&#20449;&#24687;&#26041;&#38754;&#19982;&#20256;&#32479;&#38754;&#23545;&#38754;&#35775;&#35848;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Examining the Effectiveness of Chatbots in Gathering Family History Information in Comparison to the Standard In-Person Interview-Based Approach. (arXiv:2309.03223v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23478;&#26063;&#21382;&#21490;&#25910;&#38598;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#38754;&#23545;&#38754;&#37319;&#35775;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#35752;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#22320;&#29702;&#21644;&#25216;&#26415;&#38480;&#21046;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#38271;&#30340;&#37319;&#35775;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#23478;&#35889;&#23398;&#23478;&#24120;&#24120;&#35201;&#20570;&#30340;&#20107;&#24773;&#20043;&#19968;&#23601;&#26159;&#36890;&#36807;&#38754;&#23545;&#38754;&#37319;&#35775;&#25110;&#20351;&#29992;&#35832;&#22914;ancestry.com&#20043;&#31867;&#30340;&#24179;&#21488;&#25910;&#38598;&#19968;&#20010;&#20154;&#30340;&#23478;&#26063;&#21382;&#21490;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#20026;&#23478;&#35889;&#23398;&#23478;&#25171;&#19979;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#36827;&#34892;&#36825;&#20123;&#37319;&#35775;&#30340;&#33021;&#21147;&#24448;&#24448;&#21463;&#21040;&#22320;&#29702;&#20301;&#32622;&#38480;&#21046;&#21644;&#34987;&#37319;&#35775;&#32773;&#30340;&#25216;&#26415;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#22312;&#36825;&#20123;&#31867;&#22411;&#30340;&#37319;&#35775;&#20013;&#65292;&#34987;&#37319;&#35775;&#32773;&#36890;&#24120;&#26159;&#19968;&#20010;&#24180;&#38271;&#19988;&#25216;&#26415;&#33021;&#21147;&#20302;&#20110;&#24179;&#22343;&#27700;&#24179;&#30340;&#20154;&#12290;&#22522;&#20110;&#36825;&#19968;&#28857;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#25454;&#25105;&#20204;&#22522;&#20110;&#20808;&#21069;&#30340;&#30740;&#31350;&#30456;&#20449;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#38754;&#21521;&#23478;&#26063;&#21382;&#21490;&#25910;&#38598;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24182;&#36890;&#36807;&#19982;&#19978;&#36848;&#26367;&#20195;&#26041;&#26696;&#30340;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#36827;&#34892;&#27604;&#36739;&#26469;&#25506;&#32034;&#21033;&#29992;&#36825;&#31181;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23613;&#31649;&#36827;&#34892;&#37319;&#35775;&#30340;&#24179;&#22343;&#26102;&#38388;&#21487;&#33021;&#36739;&#38271;&#65292;
&lt;/p&gt;
&lt;p&gt;
One of the most common things that a genealogist is tasked with is the gathering of a person's initial family history, normally via in-person interviews or with the use of a platform such as ancestry.com, as this can provide a strong foundation upon which a genealogist may build. However, the ability to conduct these interviews can often be hindered by both geographical constraints and the technical proficiency of the interviewee, as the interviewee in these types of interviews is most often an elderly person with a lower than average level of technical proficiency. With this in mind, this study presents what we believe, based on prior research, to be the first chatbot geared entirely towards the gathering of family histories, and explores the viability of utilising such a chatbot by comparing the performance and usability of such a method with the aforementioned alternatives. With a chatbot-based approach, we show that, though the average time taken to conduct an interview may be long
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#24863;&#30693;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20276;&#20387;&#21160;&#29289;&#30142;&#30149;&#35786;&#26029;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#34701;&#21512;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22270;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#37325;&#35201;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.03219</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#24863;&#30693;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#30340;&#20276;&#20387;&#21160;&#29289;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Companion Animal Disease Diagnostics based on Literal-aware Medical Knowledge Graph Representation Learning. (arXiv:2309.03219v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03219
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#24863;&#30693;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20276;&#20387;&#21160;&#29289;&#30142;&#30149;&#35786;&#26029;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#34701;&#21512;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22270;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#37325;&#35201;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#34987;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65288;&#22914;&#31508;&#35760;&#21644;&#20861;&#21307;&#35760;&#24405;&#65289;&#26469;&#21463;&#30410;&#20110;&#21160;&#29289;&#30142;&#30149;&#30340;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#29992;&#20110;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#24102;&#26377;&#25991;&#26412;&#20449;&#24687;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#34920;&#31034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#30693;&#35782;&#22270;&#35889;&#26174;&#31034;&#20986;&#24322;&#26500;&#29305;&#24615;&#21644;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#26088;&#22312;&#20445;&#30041;&#22260;&#32469;&#30446;&#26631;&#33410;&#28857;&#30340;&#22270;&#32467;&#26500;&#65292;&#32780;&#26080;&#27861;&#32771;&#34385;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#65292;&#32780;&#36825;&#20123;&#25991;&#26412;&#20063;&#21487;&#33021;&#21253;&#21547;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#25928;&#35786;&#26029;&#21160;&#29289;&#30142;&#30149;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22270;&#32467;&#26500;&#65292;&#24182;&#23558;&#23427;&#20204;&#34701;&#21512;&#20026;&#32479;&#19968;&#30340;&#34920;&#31034;&#65292;&#21363;LiteralKG&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;&#35813;&#22270;&#35889;&#26159;&#20174;&#21508;&#20010;&#21160;&#29289;&#21307;&#38498;&#25910;&#38598;&#30340;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#21450;&#25991;&#26412;&#20449;&#24687;&#26500;&#24314;&#32780;&#26469;&#12290;&#25105;&#20204;&#28982;&#21518;&#34701;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#20197;&#21450;&#25991;&#26412;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) embedding has been used to benefit the diagnosis of animal diseases by analyzing electronic medical records (EMRs), such as notes and veterinary records. However, learning representations to capture entities and relations with literal information in KGs is challenging as the KGs show heterogeneous properties and various types of literal information. Meanwhile, the existing methods mostly aim to preserve graph structures surrounding target nodes without considering different types of literals, which could also carry significant information. In this paper, we propose a knowledge graph embedding model for the efficient diagnosis of animal diseases, which could learn various types of literal information and graph structure and fuse them into unified representations, namely LiteralKG. Specifically, we construct a knowledge graph that is built from EMRs along with literal information collected from various animal hospitals. We then fuse different types of entities and no
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#23545;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"expand-guess-refine"&#30340;&#21307;&#23398;&#38382;&#31572;&#23545;&#40784;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#20351;&#29992;&#25351;&#20196;&#35843;&#20248;&#21644;in-prompt&#31574;&#30053;&#31561;&#25216;&#26415;&#26469;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02884</link><description>&lt;p&gt;
&#23545;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models for Clinical Tasks. (arXiv:2309.02884v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02884
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#23545;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"expand-guess-refine"&#30340;&#21307;&#23398;&#38382;&#31572;&#23545;&#40784;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#20351;&#29992;&#25351;&#20196;&#35843;&#20248;&#21644;in-prompt&#31574;&#30053;&#31561;&#25216;&#26415;&#26469;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36866;&#24212;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#27809;&#26377;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#33021;&#21147;&#65292;&#20294;&#26377;&#25928;&#22320;&#23545;&#40784;LLM&#20173;&#28982;&#26159;&#22312;&#29305;&#23450;&#20020;&#24202;&#24212;&#29992;&#20013;&#37096;&#32626;&#23427;&#20204;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#29983;&#25104;&#20855;&#26377;&#20107;&#23454;&#20934;&#30830;&#20869;&#23481;&#30340;&#21709;&#24212;&#21644;&#20174;&#20107;&#38750;&#24179;&#20961;&#25512;&#29702;&#27493;&#39588;&#30340;&#33021;&#21147;&#23545;&#20110;LLMs&#33021;&#21542;&#36866;&#29992;&#20110;&#20020;&#24202;&#21307;&#23398;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#21253;&#25324;&#25351;&#20196;&#35843;&#20248;&#21644;&#23569;&#37327;&#31034;&#20363;&#21644;&#24605;&#36335;&#38142;&#25509;&#31561;in-prompt&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21307;&#23398;&#38382;&#31572;&#23545;&#40784;&#31574;&#30053;&#34987;&#31216;&#20026;&#8220; expand-guess-refine&#8221;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#21021;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#22312;&#38382;&#39064;&#23376;&#38598;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#24471;&#20998;&#20026;70.63&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable adaptability, showcasing their capacity to excel in tasks for which they were not explicitly trained. However, despite their impressive natural language processing (NLP) capabilities, effective alignment of LLMs remains a crucial challenge when deploying them for specific clinical applications. The ability to generate responses with factually accurate content and to engage in non-trivial reasoning steps are crucial for the LLMs to be eligible for applications in clinical medicine. Employing a combination of techniques including instruction-tuning and in-prompt strategies like few-shot and chain of thought prompting has significantly enhanced the performance of LLMs. Our proposed alignment strategy for medical question-answering, known as 'expand-guess-refine', offers a parameter and data-efficient solution. A preliminary analysis of this method demonstrated outstanding performance, achieving a score of 70.63% on a subset of ques
&lt;/p&gt;</description></item><item><title>HAE-RAE Bench&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20351;&#29992;&#27604;GPT-3.5&#23567;&#30340;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24378;&#35843;&#20102;&#21516;&#36136;&#35821;&#26009;&#24211;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02706</link><description>&lt;p&gt;
HAE-RAE Bench: &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models. (arXiv:2309.02706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02706
&lt;/p&gt;
&lt;p&gt;
HAE-RAE Bench&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20351;&#29992;&#27604;GPT-3.5&#23567;&#30340;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24378;&#35843;&#20102;&#21516;&#36136;&#35821;&#26009;&#24211;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#23545;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20851;&#27880;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20013;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#24182;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#38889;&#35821;&#35821;&#35328;&#21644;&#25991;&#21270;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HAE-RAE Bench&#65292;&#22312;&#35789;&#27719;&#12289;&#21382;&#21490;&#21644;&#19968;&#33324;&#30693;&#35782;&#31561;6&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#35780;&#20272;&#31361;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;(LLSMs)&#19982;&#20687;GPT-3.5&#36825;&#26679;&#30340;&#20840;&#38754;&#36890;&#29992;&#27169;&#22411;&#30456;&#27604;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#27604;GPT-3.5&#32422;&#23567;13&#20493;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#23637;&#29616;&#20986;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#36825;&#19968;&#35266;&#23519;&#24378;&#35843;&#20102;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#26102;&#21516;&#36136;&#35821;&#26009;&#24211;&#30340;&#37325;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#24403;&#36825;&#20123;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;......
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) pretrained on massive corpora exhibit remarkable capabilities across a wide range of tasks, however, the attention given to non-English languages has been limited in this field of research. To address this gap and assess the proficiency of language models in the Korean language and culture, we present HAE-RAE Bench, covering 6 tasks including vocabulary, history, and general knowledge. Our evaluation of language models on this benchmark highlights the potential advantages of employing Large Language-Specific Models(LLSMs) over a comprehensive, universal model like GPT-3.5. Remarkably, our study reveals that models approximately 13 times smaller than GPT-3.5 can exhibit similar performance levels in terms of language-specific knowledge retrieval. This observation underscores the importance of homogeneous corpora for training professional-level language-specific models. On the contrary, we also observe a perplexing performance dip in these smaller LMs when th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#27979;&#35797;&#32467;&#26524;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#23384;&#22312;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#21487;&#35266;&#23519;&#21040;&#19968;&#33268;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.02553</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#32763;&#35793;&#30340;&#34892;&#20026;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#27979;&#35797;&#32467;&#26524;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#23384;&#22312;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#21487;&#35266;&#23519;&#21040;&#19968;&#33268;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#20013;&#30340;&#34892;&#20026;&#27979;&#35797;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#34892;&#20026;&#26469;&#32454;&#31890;&#24230;&#35780;&#20272;&#31995;&#32479;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#26426;&#22120;&#32763;&#35793;&#20013;&#34892;&#20026;&#27979;&#35797;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#27979;&#35797;&#33539;&#22260;&#26377;&#38480;&#12289;&#28085;&#30422;&#30340;&#35821;&#35328;&#31181;&#31867;&#20063;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#28304;&#21477;&#23376;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22791;&#36873;&#38598;&#65292;&#20197;&#39564;&#35777;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#26159;&#21542;&#34920;&#29616;&#20986;&#39044;&#26399;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#20351;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#34892;&#20026;&#27979;&#35797;&#23454;&#38469;&#21487;&#34892;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#26368;&#23569;&#30340;&#20154;&#21147;&#25237;&#20837;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#35780;&#20272;&#26694;&#26550;&#24212;&#29992;&#20110;&#22810;&#20010;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#24635;&#20307;&#19978;&#36890;&#36807;&#29575;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#21487;&#35266;&#23519;&#21040;&#30340;&#36235;&#21183;&#30456;&#31526;&#65292;&#20294;&#20173;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioral testing in NLP allows fine-grained evaluation of systems by examining their linguistic capabilities through the analysis of input-output behavior. Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations. We can then verify whether the MT model exhibits the expected behavior through matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metri
&lt;/p&gt;</description></item><item><title>Wordle&#26159;&#19968;&#27454;&#27969;&#34892;&#30340;&#22312;&#32447;&#21333;&#35789;&#28216;&#25103;&#65292;&#29609;&#23478;&#38656;&#35201;&#22312;6&#27425;&#29468;&#27979;&#20013;&#29468;&#20986;&#27599;&#26085;&#30446;&#26631;&#21333;&#35789;&#12290;&#36890;&#36807;&#25910;&#38598;&#29609;&#23478;&#30340;&#25968;&#25454;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#27599;&#22825;&#32422;&#26377;0.2-0.5%&#30340;&#29609;&#23478;&#33021;&#22312;&#19968;&#27425;&#23581;&#35797;&#20013;&#35299;&#20915;&#35868;&#39064;&#65292;&#23637;&#31034;&#20102;&#29609;&#23478;&#20204;&#30340;&#25216;&#24039;&#21644;&#36816;&#27668;&#12290;</title><link>http://arxiv.org/abs/2309.02110</link><description>&lt;p&gt;
Wordle: &#29983;&#27963;&#30340;&#32553;&#24433;&#12290;&#24184;&#36816;&#12289;&#25216;&#24039;&#12289;&#20316;&#24330;&#12289;&#24544;&#35802;&#21644;&#24433;&#21709;&#21147;&#65281;
&lt;/p&gt;
&lt;p&gt;
Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!. (arXiv:2309.02110v2 [math.HO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02110
&lt;/p&gt;
&lt;p&gt;
Wordle&#26159;&#19968;&#27454;&#27969;&#34892;&#30340;&#22312;&#32447;&#21333;&#35789;&#28216;&#25103;&#65292;&#29609;&#23478;&#38656;&#35201;&#22312;6&#27425;&#29468;&#27979;&#20013;&#29468;&#20986;&#27599;&#26085;&#30446;&#26631;&#21333;&#35789;&#12290;&#36890;&#36807;&#25910;&#38598;&#29609;&#23478;&#30340;&#25968;&#25454;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#27599;&#22825;&#32422;&#26377;0.2-0.5%&#30340;&#29609;&#23478;&#33021;&#22312;&#19968;&#27425;&#23581;&#35797;&#20013;&#35299;&#20915;&#35868;&#39064;&#65292;&#23637;&#31034;&#20102;&#29609;&#23478;&#20204;&#30340;&#25216;&#24039;&#21644;&#36816;&#27668;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wordle&#26159;&#19968;&#27454;&#30001;&#32445;&#32422;&#26102;&#25253;&#25552;&#20379;&#30340;&#27969;&#34892;&#22312;&#32447;&#21333;&#35789;&#28216;&#25103;&#12290;&#30446;&#21069;&#20840;&#29699;&#26377;&#22823;&#32422;200&#19975;&#21517;&#33521;&#25991;&#29256;&#26412;&#30340;&#29609;&#23478;&#12290;&#29609;&#23478;&#26377;6&#27425;&#26426;&#20250;&#26469;&#29468;&#27979;&#27599;&#26085;&#30340;&#30446;&#26631;&#21333;&#35789;&#65292;&#24182;&#22312;&#27599;&#27425;&#29468;&#27979;&#21518;&#65292;&#26681;&#25454;&#27599;&#20010;&#23383;&#27597;&#30340;&#20301;&#32622;&#21644;&#27491;&#30830;&#24615;&#65292;&#29609;&#23478;&#20250;&#24471;&#21040;&#24425;&#33394;&#32534;&#30721;&#30340;&#20449;&#24687;&#12290;&#22312;&#25104;&#21151;&#23436;&#25104;&#35868;&#39064;&#25110;&#26368;&#21518;&#19968;&#27425;&#26410;&#25104;&#21151;&#30340;&#23581;&#35797;&#20043;&#21518;&#65292;&#36719;&#20214;&#21487;&#20197;&#20351;&#29992;&#20449;&#24687;&#35770;&#26469;&#35780;&#20272;&#29609;&#23478;&#30340;&#36816;&#27668;&#21644;&#25216;&#24039;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#31034;&#38543;&#26426;&#25277;&#26679;&#30340;&#25152;&#26377;&#29609;&#23478;&#30340;&#31532;&#19968;&#27425;&#12289;&#31532;&#20108;&#27425;...&#31532;&#20845;&#27425;&#29468;&#27979;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#25105;&#21457;&#29616;&#21518;&#38754;&#30340;&#25968;&#25454;&#20197;&#19968;&#31181;&#26041;&#20415;&#22797;&#21046;&#31896;&#36148;&#21040;&#30005;&#23376;&#34920;&#26684;&#20013;&#30340;&#26684;&#24335;&#21576;&#29616;&#20986;&#26469;&#12290;&#25105;&#20174;2023&#24180;5&#26376;&#33267;2023&#24180;8&#26376;&#25910;&#38598;&#20102;Wordle&#29609;&#23478;&#30340;&#31532;&#19968;&#27425;&#29468;&#27979;&#25968;&#25454;&#65292;&#24182;&#25512;&#26029;&#20986;&#19968;&#20123;&#26377;&#36259;&#30340;&#26377;&#20851;Wordle&#29609;&#23478;&#30340;&#20449;&#24687;&#12290;A&#65289;&#27599;&#22825;&#32422;&#26377;0.2-0.5%&#30340;&#29609;&#23478;&#22312;&#19968;&#27425;&#23581;&#35797;&#20013;&#35299;&#20915;&#20102;&#35868;&#39064;&#12290;&#30001;&#20110;&#29468;&#23545;2315&#20010;&#20301;&#32622;&#19978;&#30340;&#21333;&#35789;&#30340;&#20960;&#29575;&#20026;1/2315&#65292;&#36825;&#19968;&#27604;&#20363;&#26174;&#31034;&#20102;&#29609;&#23478;&#20204;&#22312;&#28216;&#25103;&#20013;&#30340;&#25216;&#24039;&#21644;&#36816;&#27668;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wordle is a popular, online word game offered by the New York Times (nytimes.com). Currently there are some 2 million players of the English version worldwide. Players have 6 attempts to guess the daily word (target word) and after each attempt, the player receives color-coded information about the correctness and position of each letter in the guess. After either a successful completion of the puzzle or the final unsuccessful attempt, software can assess the player's luck and skill using Information Theory and can display data for the first, second, ..., sixth guesses of a random sample of all players. Recently, I discovered that the latter data is presented in a format that can easily be copied and pasted into a spreadsheet. I compiled data on Wordle players' first guesses from May 2023 - August 2023 and inferred some interesting information about Wordle players. A) Every day, about 0.2-0.5% of players solve the puzzle in one attempt. Because the odds of guessing the one of 2,315 pos
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;&#25918;&#23556;&#25253;&#21578;&#20013;&#26080;&#38656;&#26631;&#27880;&#25968;&#25454;&#21363;&#21487;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.01398</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#36827;&#34892;&#26080;&#38656;&#26631;&#27880;&#20449;&#24687;&#25552;&#21462;&#30340;&#25918;&#23556;&#25253;&#21578;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Zero-shot information extraction from radiological reports using ChatGPT. (arXiv:2309.01398v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01398
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;&#25918;&#23556;&#25253;&#21578;&#20013;&#26080;&#38656;&#26631;&#27880;&#25968;&#25454;&#21363;&#21487;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#21253;&#21547;&#22823;&#37327;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20294;&#24456;&#22810;&#35760;&#24405;&#37117;&#26159;&#33258;&#30001;&#25991;&#26412;&#12290;&#20449;&#24687;&#25552;&#21462;&#26159;&#23558;&#23383;&#31526;&#24207;&#21015;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#29992;&#20110;&#20108;&#27425;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20449;&#24687;&#25552;&#21462;&#32452;&#20214;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#65292;&#38656;&#35201;&#26631;&#27880;&#25968;&#25454;&#26469;&#20248;&#21270;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#24050;&#32463;&#25104;&#20026;&#26500;&#24314;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#30340;&#20027;&#35201;&#29942;&#39048;&#20043;&#19968;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#32780;&#26080;&#38656;&#21442;&#25968;&#35843;&#20248;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#20449;&#24687;&#25552;&#21462;&#20063;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#26368;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#26159;&#21542;&#33021;&#20174;&#25918;&#23556;&#25253;&#21578;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;CT&#25253;&#21578;&#20013;&#24863;&#20852;&#36259;&#30340;&#20449;&#24687;&#35774;&#35745;&#25552;&#31034;&#27169;&#26495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;&#36825;&#20123;&#25552;&#31034;&#27169;&#26495;&#30340;&#25552;&#31034;&#35821;&#21477;&#21040;ChatGPT&#23454;&#29616;&#20449;&#24687;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records contain an enormous amount of valuable information, but many are recorded in free text. Information extraction is the strategy to transform the sequence of characters into structured data, which can be employed for secondary analysis. However, the traditional information extraction components, such as named entity recognition and relation extraction, require annotated data to optimize the model parameters, which has become one of the major bottlenecks in building information extraction systems. With the large language models achieving good performances on various downstream NLP tasks without parameter tuning, it becomes possible to use large language models for zero-shot information extraction. In this study, we aim to explore whether the most popular large language model, ChatGPT, can extract useful information from the radiological reports. We first design the prompt template for the interested information in the CT reports. Then, we generate the prompts by 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#26500;&#24314;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00237</link><description>&lt;p&gt;
&#22522;&#20110;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#30340;&#20844;&#24320;&#21487;&#20849;&#20139;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes. (arXiv:2309.00237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00237
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#26500;&#24314;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21512;&#25104;&#30340;&#20020;&#24202;&#26696;&#20363;&#25253;&#21578;&#65292;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#65292;&#20197;&#35299;&#20915;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#21512;&#25104;&#35760;&#24405;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#19987;&#38376;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;Asclepius&#12290;&#34429;&#28982;Asclepius&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;Asclepius&#19982;&#21253;&#25324;GPT-3.5-turbo&#21644;&#20854;&#20182;&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#22312;&#20869;&#30340;&#20960;&#31181;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#23558;Asclepius&#19982;&#20854;&#22312;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21464;&#20307;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21147;&#22320;&#35777;&#26126;&#65292;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#22312;&#26500;&#24314;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#21487;&#20197;&#20316;&#20026;&#21487;&#34892;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models tailored for handling patients' clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations. To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature. We then use these synthetic notes to train our specialized clinical large language model, Asclepius. While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes. We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.16898</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16898
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;"Attention Is All You Need"&#20013;&#24341;&#20837;&#36716;&#25442;&#22120;&#26550;&#26500;&#20197;&#26469;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#36716;&#25442;&#22120;&#20013;&#30340;&#27880;&#24847;&#21147;&#23618;&#25509;&#21463;&#36755;&#20837;&#20196;&#29260;&#24207;&#21015;$X$&#24182;&#36890;&#36807;&#35745;&#31639;softmax$(XQK^\top X^\top)$&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#20351;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;$(K,Q)$&#26159;&#21487;&#35757;&#32451;&#30340;&#38190;-&#26597;&#35810;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#20248;&#21270;&#20960;&#20309;&#21644;&#19968;&#20010;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#23545;&#20196;&#29260;&#23545;&#30340;&#22806;&#31215;&#26045;&#21152;&#32447;&#24615;&#32422;&#26463;&#65292;&#23558;&#26368;&#20339;&#36755;&#20837;&#20196;&#29260;&#19982;&#38750;&#26368;&#20339;&#20196;&#29260;&#20998;&#31163;&#12290;&#36825;&#20010;&#24418;&#24335;&#20027;&#20041;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#24449;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#21333;&#23618;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#65306;(1)&#20248;&#21270;&#27880;&#24847;&#21147;&#23618;&#65292;&#20351;&#29992;&#21487;&#21464;&#27491;&#21017;&#21270;&#21442;&#25968;$(K,Q)$&#65292;&#25910;&#25947;&#30340;&#26041;&#21521;&#26159;&#19968;&#20010;&#26368;&#23567;&#21270;&#32508;&#21512;&#21442;&#25968;$W=KQ^\top$&#30340;&#26680;&#33539;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#35299;&#20915;&#26041;&#26696;&#12290;&#32780;&#30452;&#25509;&#20351;&#29992;$W$&#36827;&#34892;&#21442;&#25968;&#21270;&#21017;&#26368;&#23567;&#21270;&#19968;&#20010;Frobenius&#33539;&#25968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Ladder-of-Thought&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#26469;&#25552;&#21319;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#23567;&#22411;&#27169;&#22411;&#22312;&#24212;&#29992;&#20808;&#21069;&#20869;&#37096;&#30693;&#35782;&#26102;&#24615;&#33021;&#25552;&#21319;&#19981;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#25928;&#29575;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16763</link><description>&lt;p&gt;
Ladder-of-Thought: &#20351;&#29992;&#30693;&#35782;&#20316;&#20026;&#38454;&#26799;&#25552;&#21319;&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection. (arXiv:2308.16763v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Ladder-of-Thought&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#26469;&#25552;&#21319;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#23567;&#22411;&#27169;&#22411;&#22312;&#24212;&#29992;&#20808;&#21069;&#20869;&#37096;&#30693;&#35782;&#26102;&#24615;&#33021;&#25552;&#21319;&#19981;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#25928;&#29575;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#24335;&#25552;&#20379;&#65288;CoT&#65289;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#30340;&#25512;&#29702;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#20027;&#35201;&#26377;&#30410;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#22312;&#30452;&#25509;&#24212;&#29992;CoT&#26102;&#23567;&#22411;LLM&#30340;&#24615;&#33021;&#25913;&#36827;&#19981;&#26126;&#26174;&#12290;&#23613;&#31649;LLM&#20855;&#26377;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;CoT&#20027;&#35201;&#20381;&#36182;&#20110;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#20869;&#37096;&#30693;&#35782;&#65292;&#20808;&#21069;&#26410;&#30693;&#20110;&#27169;&#22411;&#30340;&#22806;&#37096;&#30693;&#35782;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#12290;&#22312;&#31435;&#22330;&#26816;&#27979;&#31561;&#20219;&#21153;&#20013;&#65292;&#22806;&#37096;&#32972;&#26223;&#30693;&#35782;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36825;&#31181;&#36951;&#28431;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;LLM&#30340;&#22823;&#35268;&#27169;&#26550;&#26500;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#25928;&#29575;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#31435;&#22330;&#26816;&#27979;&#30340;&#24605;&#32500;&#38454;&#26799;&#65288;LoT&#65289;&#12290;LoT&#22522;&#20110;&#21452;&#38454;&#27573;&#32423;&#32852;&#20248;&#21270;&#26694;&#26550;&#65292;&#25351;&#23548;&#27169;&#22411;&#25972;&#21512;&#39640;&#36136;&#37327;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#22686;&#24378;&#20013;&#38388;&#27493;&#39588;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Prompting (CoT) reinforces the reasoning capabilities of Large Language Models (LLMs) through the generation of intermediate rationales. However, these enhancements predominantly benefit large-scale models, leaving small LMs without significant performance improvements when directly applying CoT. Despite the advanced reasoning capabilities of LLMs, CoT relies primarily on their pre-trained internal knowledge. The external knowledge that is previously unknown to the model remains unexploited. This omission becomes pronounced in tasks such as stance detection, where the external background knowledge plays a pivotal role. Additionally, the large-scale architecture of LLMs inevitably present efficiency challenges during deployment. To address these challenges, we introduce the Ladder-of-Thought (LoT) for stance detection. Grounded in a dual-phase Cascaded Optimization framework, LoT directs the model to incorporate high-quality external knowledge, enhancing the intermediat
&lt;/p&gt;</description></item><item><title>LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16137</link><description>&lt;p&gt;
LM-Infinite: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21363;&#26102;&#38271;&#24230;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16137
&lt;/p&gt;
&lt;p&gt;
LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;Transformer-based&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;LLM&#22312;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#30528;&#23545;&#38271;&#26102;&#38388;&#25512;&#29702;&#36807;&#31243;&#25110;&#29702;&#35299;&#26356;&#22823;&#19978;&#19979;&#25991;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;LLM&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#26041;&#26696;&#23558;&#35757;&#32451;&#24207;&#21015;&#25130;&#26029;&#21040;&#22266;&#23450;&#38271;&#24230;&#65288;&#20363;&#22914;LLaMa&#30340;2048&#65289;&#12290;&#21363;&#20351;&#20351;&#29992;&#20102;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;LLM&#22312;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20043;&#21518;&#24448;&#24448;&#38590;&#20197;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#26356;&#19981;&#29992;&#35828;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#20102;&#12290;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#22312;&#26356;&#38271;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#24448;&#24448;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#30828;&#20214;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#38656;&#35201;&#36827;&#34892;&#20180;&#32454;&#30340;&#35757;&#32451;&#36807;&#31243;&#35774;&#35745;&#12290;&#20026;&#20102;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30740;&#31350;&#20102;&#20027;&#35201;&#30340;&#20998;&#24067;&#22806;(OOD) f
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#21644;&#32534;&#36753;&#30340;&#28304;&#20195;&#30721;&#25688;&#35201;&#26694;&#26550;(EditSum)&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#32467;&#26500;&#21270;&#12289;&#20449;&#24687;&#20016;&#23500;&#30340;&#20195;&#30721;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.13775</link><description>&lt;p&gt;
EditSum: &#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#21644;&#32534;&#36753;&#30340;&#28304;&#20195;&#30721;&#25688;&#35201;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EditSum: A Retrieve-and-Edit Framework for Source Code Summarization. (arXiv:2308.13775v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#21644;&#32534;&#36753;&#30340;&#28304;&#20195;&#30721;&#25688;&#35201;&#26694;&#26550;(EditSum)&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#32467;&#26500;&#21270;&#12289;&#20449;&#24687;&#20016;&#23500;&#30340;&#20195;&#30721;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#20195;&#30721;&#25688;&#35201;&#26377;&#21161;&#20110;&#24320;&#21457;&#20154;&#21592;&#29702;&#35299;&#21644;&#32500;&#25252;&#28304;&#20195;&#30721;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36719;&#20214;&#39033;&#30446;&#20013;&#32463;&#24120;&#32570;&#20047;&#25110;&#36807;&#26102;&#30340;&#25688;&#35201;&#12290;&#20195;&#30721;&#25688;&#35201;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#28304;&#20195;&#30721;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#20195;&#30721;&#25688;&#35201;&#38750;&#24120;&#32467;&#26500;&#21270;&#65292;&#24182;&#20855;&#26377;&#37325;&#22797;&#30340;&#27169;&#24335;&#12290;&#38500;&#20102;&#27169;&#24335;&#21270;&#30340;&#21333;&#35789;&#22806;&#65292;&#20195;&#30721;&#25688;&#35201;&#36824;&#21253;&#21547;&#37325;&#35201;&#30340;&#20851;&#38190;&#35789;&#65292;&#36825;&#20123;&#20851;&#38190;&#35789;&#26159;&#21453;&#26144;&#20195;&#30721;&#21151;&#33021;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#22312;&#39044;&#27979;&#20851;&#38190;&#35789;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#25688;&#35201;&#20449;&#24687;&#20007;&#22833;&#20102;&#20449;&#24687;&#37327;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EditSum&#30340;&#26032;&#22411;&#26816;&#32034;&#21644;&#32534;&#36753;&#26041;&#27861;&#29992;&#20110;&#20195;&#30721;&#25688;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EditSum&#39318;&#20808;&#20174;&#39044;&#23450;&#20041;&#30340;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#19968;&#20010;&#30456;&#20284;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#24182;&#23558;&#20854;&#25688;&#35201;&#35270;&#20026;&#21407;&#22411;&#25688;&#35201;&#20197;&#23398;&#20064;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;EditSum&#33258;&#21160;&#32534;&#36753;&#21407;&#22411;&#25688;&#35201;&#65292;&#20197;&#32467;&#21512;&#20854;&#20013;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing studies show that code summaries help developers understand and maintain source code. Unfortunately, these summaries are often missing or outdated in software projects. Code summarization aims to generate natural language descriptions automatically for source code. Code summaries are highly structured and have repetitive patterns. Besides the patternized words, a code summary also contains important keywords, which are the key to reflecting the functionality of the code. However, the state-of-the-art approaches perform poorly on predicting the keywords, which leads to the generated summaries suffering a loss in informativeness. To alleviate this problem, this paper proposes a novel retrieve-and-edit approach named EditSum for code summarization. Specifically, EditSum first retrieves a similar code snippet from a pre-defined corpus and treats its summary as a prototype summary to learn the pattern. Then, EditSum edits the prototype automatically to combine the pattern in the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZC3&#30340;&#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#23545;&#27604;&#20195;&#30721;&#29255;&#27573;&#39044;&#27979;&#65292;&#24418;&#25104;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#21516;&#26500;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#39046;&#22495;&#24863;&#30693;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#23398;&#20064;&#26469;&#36827;&#19968;&#27493;&#32422;&#26463;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.13754</link><description>&lt;p&gt;
ZC3: &#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ZC3: Zero-Shot Cross-Language Code Clone Detection. (arXiv:2308.13754v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZC3&#30340;&#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#23545;&#27604;&#20195;&#30721;&#29255;&#27573;&#39044;&#27979;&#65292;&#24418;&#25104;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#21516;&#26500;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#39046;&#22495;&#24863;&#30693;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#23398;&#20064;&#26469;&#36827;&#19968;&#27493;&#32422;&#26463;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20154;&#21592;&#24341;&#20837;&#20195;&#30721;&#20811;&#38534;&#20197;&#25552;&#39640;&#32534;&#31243;&#25928;&#29575;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#22312;&#21333;&#35821;&#35328;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#24320;&#21457;&#20154;&#21592;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#32534;&#20889;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#31243;&#24207;&#65292;&#20197;&#25903;&#25345;&#19981;&#21516;&#30340;&#24179;&#21488;&#65292;&#24182;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#20174;&#19968;&#31181;&#35821;&#35328;&#32763;&#35793;&#39033;&#30446;&#21040;&#21478;&#19968;&#31181;&#35821;&#35328;&#12290;&#32771;&#34385;&#21040;&#25910;&#38598;&#36328;&#35821;&#35328;&#24182;&#34892;&#25968;&#25454;&#65288;&#23588;&#20854;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#65289;&#30340;&#25104;&#26412;&#39640;&#26114;&#19988;&#32791;&#26102;&#65292;&#35774;&#35745;&#19968;&#31181;&#19981;&#20381;&#36182;&#20219;&#20309;&#24182;&#34892;&#25968;&#25454;&#30340;&#26377;&#25928;&#36328;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZC3&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#12290;ZC3&#36890;&#36807;&#35774;&#35745;&#23545;&#27604;&#20195;&#30721;&#29255;&#27573;&#39044;&#27979;&#26469;&#24418;&#25104;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#21516;&#26500;&#34920;&#31034;&#31354;&#38388;&#12290;&#22522;&#20110;&#27492;&#65292;ZC3&#21033;&#29992;&#39046;&#22495;&#24863;&#30693;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#23398;&#20064;&#36827;&#19968;&#27493;&#32422;&#26463;&#27169;&#22411;&#20197;&#29983;&#25104;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers introduce code clones to improve programming productivity. Many existing studies have achieved impressive performance in monolingual code clone detection. However, during software development, more and more developers write semantically equivalent programs with different languages to support different platforms and help developers translate projects from one language to another. Considering that collecting cross-language parallel data, especially for low-resource languages, is expensive and time-consuming, how designing an effective cross-language model that does not rely on any parallel data is a significant problem. In this paper, we propose a novel method named ZC3 for Zero-shot Cross-language Code Clone detection. ZC3 designs the contrastive snippet prediction to form an isomorphic representation space among different programming languages. Based on this, ZC3 exploits domain-aware learning and cycle consistency learning to further constrain the model to generate represen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2308.11764</link><description>&lt;p&gt;
Halo&#65306;&#35780;&#20272;&#21644;&#38477;&#20302;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#12290;&#34429;&#28982;&#23545;&#20110;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#26041;&#20415;&#65292;&#20294;&#26159;&#19982;&#20854;&#26356;&#22823;&#35268;&#27169;&#30340;&#23545;&#24212;&#27169;&#22411;&#30456;&#27604;&#65292;&#24320;&#28304;&#30340;&#21442;&#25968;&#36739;&#23569;&#30340;LLMs&#32463;&#24120;&#20986;&#29616;&#20005;&#37325;&#24187;&#35273;&#38382;&#39064;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#27979;&#37327;&#21644;&#20943;&#23569;BLOOM 7B&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#26159;&#20844;&#24320;&#25552;&#20379;&#32473;&#30740;&#31350;&#21644;&#21830;&#19994;&#24212;&#29992;&#30340;&#24369;&#24320;&#28304;LLMs&#30340;&#20195;&#34920;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HaloCheck&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26080;&#38656;&#30693;&#35782;&#30340;&#40657;&#30418;&#23376;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;LLMs&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#20302;&#21442;&#25968;LLMs&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;LLMs&#30340;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.11432</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20195;&#29702;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#23398;&#26415;&#30028;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#23545;&#26377;&#38480;&#30693;&#35782;&#30340;&#20195;&#29702;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#19982;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#20154;&#31867;&#33324;&#30340;&#20915;&#31574;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#33719;&#21462;&#22823;&#37327;&#30340;&#32593;&#32476;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20986;&#20102;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#30340;&#39640;&#28072;&#20852;&#36259;&#12290;&#20026;&#20102;&#21457;&#25381;LLM&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#21508;&#31181;&#19981;&#21516;&#24212;&#29992;&#30340;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#36825;&#20123;&#30740;&#31350;&#65292;&#20174;&#25972;&#20307;&#30340;&#35282;&#24230;&#23545;&#33258;&#20027;&#20195;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23457;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26500;&#24314;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#28909;&#21551;&#21160;&#31574;&#30053;&#23545;&#20110;&#35299;&#20915;&#20998;&#24067;&#21464;&#21270;&#21644;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.04014</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65306;&#22914;&#20309;&#65288;&#37325;&#26032;&#65289;&#28909;&#21551;&#21160;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Continual Pre-Training of Large Language Models: How to (re)warm your model?. (arXiv:2308.04014v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#28909;&#21551;&#21160;&#31574;&#30053;&#23545;&#20110;&#35299;&#20915;&#20998;&#24067;&#21464;&#21270;&#21644;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#23545;&#25968;&#21313;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#65292;&#23601;&#20250;&#37325;&#26032;&#24320;&#22987;&#36825;&#20010;&#36807;&#31243;&#12290;&#19968;&#31181;&#26356;&#24265;&#20215;&#21644;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#21363;&#29992;&#26032;&#25968;&#25454;&#26356;&#26032;&#39044;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#26159;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#36890;&#24120;&#20250;&#23548;&#33268;&#36807;&#21435;&#25968;&#25454;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#28909;&#21551;&#21160;&#31574;&#30053;&#23545;&#25345;&#32493;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#22312;&#35757;&#32451;&#26032;&#25968;&#25454;&#38598;&#26102;&#65292;&#38656;&#35201;&#37325;&#26032;&#22686;&#21152;&#23398;&#20064;&#29575;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;Pile&#65288;&#19978;&#28216;&#25968;&#25454;&#65292;300B&#26631;&#35760;&#65289;&#19978;&#25345;&#32493;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;SlimPajama&#65288;&#19979;&#28216;&#25968;&#25454;&#65292;297B&#26631;&#35760;&#65289;&#19978;&#36827;&#34892;&#20102;&#32447;&#24615;&#28909;&#21551;&#21160;&#21644;&#20313;&#24358;&#34928;&#20943;&#30340;&#35843;&#24230;&#12290;&#25105;&#20204;&#22312;Pythia 410M&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#19978;&#36827;&#34892;&#20102;&#25152;&#26377;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are routinely pre-trained on billions of tokens, only to restart the process over again once new data becomes available. A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e. updating pre-trained models with new data instead of re-training them from scratch. However, the distribution shift induced by novel data typically results in degraded performance on past data. Taking a step towards efficient continual pre-training, in this work, we examine the effect of different warm-up strategies. Our hypothesis is that the learning rate must be re-increased to improve compute efficiency when training on a new dataset. We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens), following a linear warmup and cosine decay schedule. We conduct all experiments on the Pythia 410M language model architecture and ev
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.13596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#36793;&#32536;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#24778;&#20154;&#25104;&#21151;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#32972;&#21518;&#30340;&#29702;&#35770;&#21407;&#21017;&#23578;&#19981;&#28165;&#26970;&#65292;&#29305;&#21035;&#26159;&#23427;&#30340;&#38750;&#20984;&#20248;&#21270;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21019;&#24615;&#30340;softmax-attention&#27169;&#22411;$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$&#65292;&#20854;&#20013;$\boldsymbol{X}$&#26159;&#26631;&#35760;&#24207;&#21015;&#65292;$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$&#26159;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$\boldsymbol{p}$&#25110;&#31561;&#20215;&#30340;$\boldsymbol{W}$&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#20250;&#27839;&#30528;&#26041;&#21521;&#25910;&#25947;&#21040;&#20998;&#38548;&#8220;&#23616;&#37096;&#26368;&#20248;&#8221;&#26631;&#35760;&#21644;&#8220;&#38750;&#26368;&#20248;&#8221;&#26631;&#35760;&#30340;&#26368;&#22823;&#36793;&#32536;&#35299;&#12290;&#36825;&#26126;&#30830;&#22320;&#24418;&#24335;&#21270;&#20102;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#31181;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#33324;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#23884;&#20837;$\boldsymbol{Xv}$&#21644;$\texttt{softmax}(\boldsymbol{XWp})$&#31934;&#32454;&#22320;&#34920;&#24449;&#26631;&#35760;&#30340;&#8220;&#26368;&#20248;&#24615;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
&lt;/p&gt;</description></item><item><title>ToolAlpaca&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#24037;&#20855;&#20351;&#29992;&#35821;&#26009;&#24211;&#65292;&#23454;&#29616;&#32039;&#20945;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#21152;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.05301</link><description>&lt;p&gt;
ToolAlpaca: &#36890;&#36807;3000&#20010;&#27169;&#25311;&#26696;&#20363;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases. (arXiv:2306.05301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05301
&lt;/p&gt;
&lt;p&gt;
ToolAlpaca&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#24037;&#20855;&#20351;&#29992;&#35821;&#26009;&#24211;&#65292;&#23454;&#29616;&#32039;&#20945;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#21152;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#21033;&#29992;&#29616;&#23454;&#24037;&#20855;&#23545;&#20110;&#23454;&#29616;&#20855;&#26377;&#24863;&#30693;&#33021;&#21147;&#30340;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20381;&#38752;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;GPT-4&#65292;&#22312;&#38646;-shot&#26041;&#24335;&#19979;&#33719;&#24471;&#36890;&#29992;&#30340;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#65292;&#25110;&#21033;&#29992;&#30417;&#30563;&#23398;&#20064;&#22312;&#32039;&#20945;&#30340;&#27169;&#22411;&#19978;&#35757;&#32451;&#26377;&#38480;&#31867;&#22411;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#27809;&#26377;&#29305;&#23450;&#24037;&#20855;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36890;&#29992;&#30340;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#23578;&#19981;&#30830;&#23450;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ToolAlpaca&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#24037;&#20855;&#20351;&#29992;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#26368;&#23567;&#20154;&#20026;&#24178;&#39044;&#19979;&#65292;&#22312;&#32039;&#20945;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#23398;&#20064;&#36890;&#29992;&#30340;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabling large language models to effectively utilize real-world tools is crucial for achieving embodied intelligence. Existing approaches to tool learning have primarily relied on either extremely large language models, such as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or have utilized supervised learning to train limited types of tools on compact models. However, it remains uncertain whether smaller language models can achieve generalized tool-use abilities without specific tool-specific training. To address this question, this paper introduces ToolAlpaca, a novel framework designed to automatically generate a tool-use corpus and learn generalized tool-use abilities on compact language models with minimal human intervention. Specifically, ToolAlpaca first collects a comprehensive dataset by building a multi-agent simulation environment, which contains 3938 tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Subseque
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#23545;&#40784;&#21040;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#65292;&#33021;&#22815;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.00526</link><description>&lt;p&gt;
&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#38646;&#26679;&#26412;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#25351;&#23548;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering. (arXiv:2306.00526v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#23545;&#40784;&#21040;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#65292;&#33021;&#22815;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24067;&#23616;&#24863;&#30693;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#39044;&#35757;&#32451;&#21644;&#20219;&#21153;&#24494;&#35843;&#23545;&#20110;&#39069;&#22806;&#30340;&#35270;&#35273;&#12289;&#24067;&#23616;&#21644;&#20219;&#21153;&#27169;&#22359;&#38459;&#27490;&#20102;&#20854;&#30452;&#25509;&#21033;&#29992;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#26368;&#36817;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#28508;&#21147;&#12290;&#19982;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#39046;&#22495;&#23545;&#40784;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#19982;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#23427;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#12290;&#21069;&#32773;&#36890;&#36807;&#36866;&#24403;&#30340;&#31354;&#26684;&#21644;&#25442;&#34892;&#31526;&#20174;OCR&#24037;&#20855;&#20013;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#12290;&#21518;&#32773;&#30830;&#20445;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pre-training-fine-tuning paradigm based on layout-aware multimodal pre-trained models has achieved significant progress on document image question answering. However, domain pre-training and task fine-tuning for additional visual, layout, and task modules prevent them from directly utilizing off-the-shelf instruction-tuning language foundation models, which have recently shown promising potential in zero-shot learning. Contrary to aligning language models to the domain of document image question answering, we align document image question answering to off-the-shell instruction-tuning language foundation models to utilize their zero-shot capability. Specifically, we propose layout and task aware instruction prompt called LATIN-Prompt, which consists of layout-aware document content and task-aware descriptions. The former recovers the layout information among text segments from OCR tools by appropriate spaces and line breaks. The latter ensures that the model generates answers that m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.13592</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#25552;&#31034;&#65306;&#19996;&#21335;&#20122;&#35821;&#35328;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28151;&#21512;&#20195;&#30721;&#22312;&#19990;&#30028;&#35768;&#22810;&#22320;&#21306;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#23454;&#36341;&#65292;&#20294;&#25910;&#38598;&#39640;&#36136;&#37327;&#19988;&#20302;&#25104;&#26412;&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#20173;&#28982;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#36843;&#20351;&#20154;&#20204;&#38382;&#65306;&#36825;&#20123;&#31995;&#32479;&#33021;&#29992;&#20110;&#25968;&#25454;&#29983;&#25104;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#38646;-shot&#30340;&#26041;&#24335;&#19979;&#22914;&#20309;&#25552;&#31034;LLMs&#20026;&#19996;&#21335;&#20122;&#65288;SEA&#65289;&#30340;&#20116;&#31181;&#35821;&#35328;&#65288;&#21360;&#23612;&#35821;&#65292;&#39532;&#26469;&#35821;&#65292;&#20013;&#25991;&#65292;&#22612;&#21152;&#36335;&#35821;&#65292;&#36234;&#21335;&#35821;&#65289;&#21450;&#20811;&#37324;&#22885;&#23572;&#35821;S ingl ish&#21019;&#36896;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#26174;&#31034;&#20986;&#26368;&#22823;&#30340;&#28508;&#21147;&#65292;&#24403;&#26126;&#30830;&#23450;&#20041;&#8220;&#28151;&#21512;&#20195;&#30721;&#8221;&#26415;&#35821;&#26102;&#65292;&#33021;&#22815;68%&#30340;&#26102;&#38388;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;ChatGPT&#21644;InstructGPT&#65288;davinci-003&#65289;&#29983;&#25104;S ingl ish&#25991;&#26412;&#30340;&#34920;&#29616;&#20063;&#20540;&#24471;&#27880;&#24847;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#25552;&#31034;&#19979;&#30340;&#25104;&#21151;&#29575;&#24179;&#22343;&#20026;96%&#12290;&#20294;&#26159;&#65292;ChatGPT&#21644;InstructGPT&#30340;&#28151;&#21512;&#20195;&#30721;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#35821;&#20041;&#19981;&#27491;&#30830;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The proliferation of Large Language Models (LLMs) in recent times compels one to ask: can these systems be used for data generation? In this article, we explore prompting LLMs in a zero-shot manner to create code-mixed data for five languages in South East Asia (SEA) -Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language Singlish. We find that ChatGPT shows the most potential, capable of producing code-mixed text 68% of the time when the term "code-mixing" is explicitly defined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in generating Singlish texts are noteworthy, averaging a 96% success rate across a variety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT, however, is dampened by word choice errors that lead to semant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20027;&#24352;&#20248;&#21270;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#37325;&#26032;&#25776;&#20889;&#35770;&#35777;&#24615;&#20027;&#24352;&#20197;&#20248;&#21270;&#20854;&#20256;&#36882;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#20027;&#24352;&#38598;&#21512;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#36136;&#37327;&#25351;&#26631;&#36873;&#25321;&#26368;&#20339;&#20505;&#36873;&#20027;&#24352;&#12290;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#22810;&#31181;&#22522;&#20934;&#32447;&#65292;&#25913;&#21892;&#20102;60&#65285;&#30340;&#20027;&#24352;&#12290;</title><link>http://arxiv.org/abs/2212.08913</link><description>&lt;p&gt;
&#35745;&#31639;&#36777;&#35770;&#20013;&#30340;&#20027;&#24352;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Claim Optimization in Computational Argumentation. (arXiv:2212.08913v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20027;&#24352;&#20248;&#21270;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#37325;&#26032;&#25776;&#20889;&#35770;&#35777;&#24615;&#20027;&#24352;&#20197;&#20248;&#21270;&#20854;&#20256;&#36882;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#20027;&#24352;&#38598;&#21512;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#36136;&#37327;&#25351;&#26631;&#36873;&#25321;&#26368;&#20339;&#20505;&#36873;&#20027;&#24352;&#12290;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#22810;&#31181;&#22522;&#20934;&#32447;&#65292;&#25913;&#21892;&#20102;60&#65285;&#30340;&#20027;&#24352;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#20309;&#36777;&#35770;&#20013;&#65292;&#26080;&#35770;&#26159;&#23545;&#20154;&#31867;&#36824;&#26159;AI&#31995;&#32479;&#26469;&#35828;&#65292;&#20248;&#21270;&#35770;&#25454;&#30340;&#20256;&#36882;&#26159;&#35828;&#26381;&#30340;&#20851;&#38190;&#12290;&#36825;&#35201;&#27714;&#20351;&#29992;&#19982;&#36777;&#35770;&#30456;&#20851;&#30340;&#28165;&#26224;&#27969;&#30021;&#30340;&#20027;&#24352;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#35770;&#25454;&#36136;&#37327;&#30340;&#33258;&#21160;&#35780;&#20272;&#65292;&#28982;&#32780;&#36804;&#20170;&#20026;&#27490;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#33021;&#22815;&#23454;&#38469;&#25913;&#21892;&#36136;&#37327;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20027;&#24352;&#20248;&#21270;&#30340;&#20219;&#21153;&#65306;&#37325;&#26032;&#25776;&#20889;&#35770;&#35777;&#24615;&#20027;&#24352;&#20197;&#20248;&#21270;&#20854;&#20256;&#36882;&#12290;&#30001;&#20110;&#23384;&#22312;&#22810;&#31181;&#20248;&#21270;&#31867;&#22411;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BART&#65289;&#29983;&#25104;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#20027;&#24352;&#38598;&#21512;&#65292;&#32771;&#34385;&#21040;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#21508;&#31181;&#36136;&#37327;&#25351;&#26631;&#36873;&#25321;&#26368;&#20339;&#20505;&#36873;&#20027;&#24352;&#12290;&#22312;&#33521;&#35821;&#35821;&#26009;&#24211;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#36136;&#37327;&#30340;&#20505;&#36873;&#20027;&#24352;&#36873;&#25321;&#20248;&#20110;&#20960;&#31181;&#22522;&#20934;&#32447;&#65292;&#25913;&#21892;&#20102;60&#65285;&#30340;&#25152;&#26377;&#20027;&#24352;&#65288;&#20165;&#24694;&#21270;&#20102;16&#65285;&#65289;&#12290;&#21518;&#32493;&#20998;&#26512;&#34920;&#26126;&#65292;&#38500;&#20102;&#22797;&#21046;&#32534;&#36753;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#24120;&#26356;&#21152;&#20855;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
An optimal delivery of arguments is key to persuasion in any debate, both for humans and for AI systems. This requires the use of clear and fluent claims relevant to the given debate. Prior work has studied the automatic assessment of argument quality extensively. Yet, no approach actually improves the quality so far. To fill this gap, this paper proposes the task of claim optimization: to rewrite argumentative claims in order to optimize their delivery. As multiple types of optimization are possible, we approach this task by first generating a diverse set of candidate claims using a large language model, such as BART, taking into account contextual information. Then, the best candidate is selected using various quality metrics. In automatic and human evaluation on an English-language corpus, our quality-based candidate selection outperforms several baselines, improving 60% of all claims (worsening 16% only). Follow-up analyses reveal that, beyond copy editing, our approach often speci
&lt;/p&gt;</description></item><item><title>BigText-QA&#24341;&#20837;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;QA&#26041;&#27861;&#65292;&#33021;&#22815;&#22522;&#20110;&#26377;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#30693;&#35782;&#22270;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.05798</link><description>&lt;p&gt;
BigText-QA&#65306;&#22522;&#20110;&#22823;&#35268;&#27169;&#28151;&#21512;&#30693;&#35782;&#22270;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
BigText-QA: Question Answering over a Large-Scale Hybrid Knowledge Graph. (arXiv:2212.05798v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05798
&lt;/p&gt;
&lt;p&gt;
BigText-QA&#24341;&#20837;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;QA&#26041;&#27861;&#65292;&#33021;&#22815;&#22522;&#20110;&#26377;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#30693;&#35782;&#22270;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#36164;&#28304;&#19978;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#20013;&#22810;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#24494;&#22937;&#20851;&#31995;&#26102;&#12290;&#20026;&#27492;&#65292;&#20687;YAGO&#12289;DBpedia&#12289;Freebase&#21644;Wikidata&#36825;&#26679;&#30340;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#24191;&#27867;&#20351;&#29992;&#24182;&#24471;&#21040;&#20102;&#35748;&#21487;&#65292;&#29992;&#20110;&#38382;&#31572;&#65288;QA&#65289;&#24212;&#29992;&#12290;&#34429;&#28982;&#36825;&#20123;&#30693;&#35782;&#24211;&#25552;&#20379;&#20102;&#32467;&#26500;&#21270;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#20294;&#32570;&#20047;&#33258;&#28982;&#35821;&#35328;&#26469;&#28304;&#20013;&#30340;&#19978;&#19979;&#25991;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;BigText-QA&#24341;&#20837;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;QA&#26041;&#27861;&#65292;&#33021;&#22815;&#22522;&#20110;&#26356;&#20887;&#20313;&#30340;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#22238;&#31572;&#38382;&#39064;&#65292;&#35813;&#22270;&#23558;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#65288;&#21363;&#8220;&#28151;&#21512;&#8221;&#65289;&#30693;&#35782;&#20197;&#32479;&#19968;&#30340;&#22270;&#24418;&#34920;&#31034;&#26041;&#24335;&#32452;&#32455;&#12290;&#22240;&#27492;&#65292;BigText-QA&#33021;&#22815;&#20860;&#39038;&#20004;&#20010;&#19990;&#30028;&#30340;&#20248;&#21183;&#8212;&#8212;&#19968;&#20010;&#32463;&#20856;&#30340;&#21629;&#21517;&#23454;&#20307;&#38598;&#21512;&#65292;&#26144;&#23556;&#21040;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#32972;&#26223;&#30693;&#35782;&#24211;&#65288;&#22914;YAGO&#25110;Wikidata&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#20174;&#33258;&#28982;&#35821;&#35328;&#26469;&#28304;&#20013;&#33719;&#21462;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering complex questions over textual resources remains a challenge, particularly when dealing with nuanced relationships between multiple entities expressed within natural-language sentences. To this end, curated knowledge bases (KBs) like YAGO, DBpedia, Freebase, and Wikidata have been widely used and gained great acceptance for question-answering (QA) applications in the past decade. While these KBs offer a structured knowledge representation, they lack the contextual diversity found in natural-language sources. To address this limitation, BigText-QA introduces an integrated QA approach, which is able to answer questions based on a more redundant form of a knowledge graph (KG) that organizes both structured and unstructured (i.e., "hybrid") knowledge in a unified graphical representation. Thereby, BigText-QA is able to combine the best of both worlds$\unicode{x2013}$a canonical set of named entities, mapped to a structured background KB (such as YAGO or Wikidata), as well as an o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#21270;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#65292;&#38450;&#27490;&#29305;&#23450;&#38750;&#32447;&#24615;&#23545;&#25163;&#39044;&#27979;&#27010;&#24565;&#65292;&#20294;&#26080;&#27861;&#24443;&#24213;&#35299;&#20915;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#25830;&#38500;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2201.12191</link><description>&lt;p&gt;
Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12191
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#21270;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#65292;&#38450;&#27490;&#29305;&#23450;&#38750;&#32447;&#24615;&#23545;&#25163;&#39044;&#27979;&#27010;&#24565;&#65292;&#20294;&#26080;&#27861;&#24443;&#24213;&#35299;&#20915;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#25830;&#38500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20986;&#29616;&#12290;&#29702;&#35299;&#36825;&#20123;&#34920;&#31034;&#22914;&#20309;&#32534;&#30721;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#27010;&#24565;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#35782;&#21035;&#31070;&#32463;&#34920;&#31034;&#20013;&#30340;&#27010;&#24565;&#30340;&#19968;&#31181;&#26126;&#26174;&#26041;&#27861;&#26159;&#25628;&#32034;&#19968;&#20010;&#32447;&#24615;&#23376;&#31354;&#38388;&#65292;&#20854;&#25830;&#38500;&#20250;&#38459;&#27490;&#20174;&#34920;&#31034;&#20013;&#39044;&#27979;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35768;&#22810;&#32447;&#24615;&#25830;&#38500;&#31639;&#27861;&#26159;&#21487;&#22788;&#29702;&#21644;&#21487;&#35299;&#37322;&#30340;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#26410;&#24517;&#20197;&#32447;&#24615;&#26041;&#24335;&#34920;&#31034;&#27010;&#24565;&#12290;&#20026;&#20102;&#35782;&#21035;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26680;&#21270;&#30340;&#27010;&#24565;&#25830;&#38500;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#38450;&#27490;&#29305;&#23450;&#30340;&#38750;&#32447;&#24615;&#23545;&#25163;&#39044;&#27979;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20445;&#25252;&#19981;&#20250;&#36716;&#31227;&#21040;&#19981;&#21516;&#30340;&#38750;&#32447;&#24615;&#23545;&#25163;&#12290;&#22240;&#27492;&#65292;&#24443;&#24213;&#22320;&#25830;&#38500;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation space of neural models for textual data emerges in an unsupervised manner during training. Understanding how those representations encode human-interpretable concepts is a fundamental problem. One prominent approach for the identification of concepts in neural representations is searching for a linear subspace whose erasure prevents the prediction of the concept from the representations. However, while many linear erasure algorithms are tractable and interpretable, neural networks do not necessarily represent concepts in a linear manner. To identify non-linearly encoded concepts, we propose a kernelization of a linear minimax game for concept erasure. We demonstrate that it is possible to prevent specific non-linear adversaries from predicting the concept. However, the protection does not transfer to different nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded concept remains an open problem.
&lt;/p&gt;</description></item></channel></rss>