<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26381;&#21153;&#22120;&#31471;&#23545;&#35821;&#38899;&#20013;&#24515;&#21270;&#30693;&#35782;&#26597;&#35810;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;&#30340;&#24314;&#27169;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21508;&#31181;&#26381;&#21153;&#22120;&#31471;&#35821;&#35328;&#27169;&#22411;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#21508;&#31181;&#23454;&#20307;&#20013;&#24515;&#21270;&#26597;&#35810;&#23376;&#26063;&#32676;&#30340;&#35782;&#21035;&#38169;&#35823;&#29575;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#34701;&#21512;&#21644;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#21319;VA ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#20063;&#36215;&#21040;&#20102;&#31215;&#26497;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01398</link><description>&lt;p&gt;
&#26381;&#21153;&#22120;&#31471;&#23545;&#35821;&#38899;&#20013;&#24515;&#21270;&#30693;&#35782;&#26597;&#35810;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Server-side Rescoring of Spoken Entity-centric Knowledge Queries for Virtual Assistants. (arXiv:2311.01398v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26381;&#21153;&#22120;&#31471;&#23545;&#35821;&#38899;&#20013;&#24515;&#21270;&#30693;&#35782;&#26597;&#35810;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;&#30340;&#24314;&#27169;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21508;&#31181;&#26381;&#21153;&#22120;&#31471;&#35821;&#35328;&#27169;&#22411;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#21508;&#31181;&#23454;&#20307;&#20013;&#24515;&#21270;&#26597;&#35810;&#23376;&#26063;&#32676;&#30340;&#35782;&#21035;&#38169;&#35823;&#29575;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#34701;&#21512;&#21644;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#21319;VA ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#20063;&#36215;&#21040;&#20102;&#31215;&#26497;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#39537;&#21160;&#30340;&#35774;&#22791;&#20869;&#34394;&#25311;&#21161;&#25163;&#65288;VA&#65289;&#38656;&#35201;&#26377;&#25928;&#30340;&#30693;&#35782;&#25972;&#21512;&#26469;&#24212;&#23545;&#23500;&#23454;&#20307;&#26597;&#35810;&#35782;&#21035;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;N-gram&#35789;&#35821;&#27169;&#22411;&#12289;&#23376;&#35789;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65289;&#65292;&#23545;&#26381;&#21153;&#22120;&#31471;&#23545;&#21475;&#35821;&#20449;&#24687;&#39046;&#22495;&#26597;&#35810;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;&#30340;&#24314;&#27169;&#31574;&#30053;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35774;&#22791;&#20869;&#21644;&#26381;&#21153;&#22120;&#31471;&#20449;&#21495;&#30340;&#32452;&#21512;&#65292;&#24182;&#19982;&#20165;&#22312;&#35774;&#22791;&#20869;&#36827;&#34892;ASR&#30340;&#24773;&#20917;&#30456;&#27604;&#65292;&#36890;&#36807;&#25972;&#21512;&#21508;&#31181;&#26381;&#21153;&#22120;&#31471;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#23454;&#20307;&#20013;&#24515;&#21270;&#26597;&#35810;&#23376;&#26063;&#32676;&#20013;&#21462;&#24471;&#20102;23%-35%&#30340;WER&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#23545;&#20351;&#29992;&#39046;&#22495;&#25968;&#25454;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#30001;OpenAI&#25552;&#20379;&#30340;GPT-3&#21464;&#20307;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#22810;&#20010;&#26381;&#21153;&#22120;&#31471;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#34701;&#21512;&#26368;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#27599;&#20010;&#27169;&#22411;&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#24182;&#23558;&#20174;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;VA ASR&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-device Virtual Assistants (VAs) powered by Automatic Speech Recognition (ASR) require effective knowledge integration for the challenging entity-rich query recognition. In this paper, we conduct an empirical study of modeling strategies for server-side rescoring of spoken information domain queries using various categories of Language Models (LMs) (N-gram word LMs, sub-word neural LMs). We investigate the combination of on-device and server-side signals, and demonstrate significant WER improvements of 23%-35% on various entity-centric query subpopulations by integrating various server-side LMs compared to performing ASR on-device only. We also perform a comparison between LMs trained on domain data and a GPT-3 variant offered by OpenAI as a baseline. Furthermore, we also show that model fusion of multiple server-side LMs trained from scratch most effectively combines complementary strengths of each model and integrates knowledge learned from domain-specific data to a VA ASR system.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;"&#35821;&#35328;&#24187;&#35273;"&#30456;&#20851;&#30340;&#21028;&#26029;&#20013;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#32467;&#26500;&#20381;&#36182;&#24615;&#30340;&#24187;&#35273;&#30340;&#24433;&#21709;&#65292;&#32780;&#22312;&#35821;&#20041;&#26041;&#38754;&#21017;&#36739;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2311.01386</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23481;&#26131;&#21463;&#21040;&#35821;&#35328;&#24187;&#35273;&#30340;&#27450;&#39575;&#65311;&#22312;&#35821;&#27861;&#26041;&#38754;&#23481;&#26131;&#65292;&#22312;&#35821;&#20041;&#26041;&#38754;&#22256;&#38590;&#12290;&#65288;arXiv:2311.01386v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics. (arXiv:2311.01386v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01386
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;"&#35821;&#35328;&#24187;&#35273;"&#30456;&#20851;&#30340;&#21028;&#26029;&#20013;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#32467;&#26500;&#20381;&#36182;&#24615;&#30340;&#24187;&#35273;&#30340;&#24433;&#21709;&#65292;&#32780;&#22312;&#35821;&#20041;&#26041;&#38754;&#21017;&#36739;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21028;&#26029;&#35821;&#27861;&#24615;&#26041;&#38754;&#19982;&#20154;&#31867;&#26377;&#24456;&#22823;&#37325;&#21472;&#65292;&#20294;&#26159;&#24403;&#20154;&#31867;&#22312;&#35821;&#35328;&#22788;&#29702;&#20013;&#31995;&#32479;&#24615;&#22320;&#20986;&#29616;&#38169;&#35823;&#26102;&#65292;&#25105;&#20204;&#26159;&#21542;&#26399;&#26395;LMs&#33021;&#20687;&#35821;&#35328;&#30340;&#35748;&#30693;&#27169;&#22411;&#37027;&#26679;&#27169;&#20223;&#20154;&#31867;&#34892;&#20026;&#65311;&#36890;&#36807;&#30740;&#31350;&#19982;&#8220;&#35821;&#35328;&#24187;&#35273;&#8221;&#30456;&#20851;&#30340;LMs&#30340;&#26356;&#24494;&#22937;&#21028;&#26029;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#8212;&#8212;&#36825;&#20123;&#21477;&#23376;&#22312;&#24847;&#20041;&#19978;&#27169;&#31946;&#12289;&#19981;&#21512;&#24773;&#29702;&#25110;&#35821;&#27861;&#38169;&#35823;&#65292;&#20294;&#21364;&#21463;&#21040;&#20154;&#31867;&#24847;&#22806;&#39640;&#25509;&#21463;&#24230;&#30340;&#21028;&#26029;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#24187;&#35273;&#65306;&#27604;&#36739;&#24187;&#35273;&#65288;&#20363;&#22914;&#8220;&#21435;&#36807;&#20420;&#32599;&#26031;&#30340;&#20154;&#27604;&#25105;&#22810;&#8221;&#65289;&#65292;&#28145;&#24230;&#20914;&#20987;&#24187;&#35273;&#65288;&#20363;&#22914;&#8220;&#27809;&#26377;&#36731;&#24494;&#30340;&#22836;&#37096;&#20260;&#23475;&#21487;&#20197;&#34987;&#24573;&#35270;&#8221;&#65289;&#21644;&#21542;&#23450;&#26497;&#24615;&#39033;&#65288;NPI&#65289;&#24187;&#35273;&#65288;&#20363;&#22914;&#8220;&#27809;&#26377;&#19968;&#20010;&#20065;&#26449;&#20154;&#30456;&#20449;&#26159;&#21487;&#20449;&#36182;&#30340;&#29454;&#20154;&#20250;&#21521;&#29066;&#23556;&#20987;&#8221;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LMs&#34920;&#31034;&#30340;&#27010;&#29575;&#26356;&#26377;&#21487;&#33021;&#19982;&#20154;&#31867;&#23545;&#20110;&#34987;NPI&#24187;&#35273;&#8220;&#27450;&#39575;&#8221;&#30340;&#21028;&#26029;&#19968;&#33268;&#65292;&#36825;&#19968;&#24187;&#35273;&#26816;&#39564;&#20102;&#19968;&#31181;&#32467;&#26500;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have been argued to overlap substantially with human beings in grammaticality judgment tasks. But when humans systematically make errors in language processing, should we expect LMs to behave like cognitive models of language and mimic human behavior? We answer this question by investigating LMs' more subtle judgments associated with "language illusions" -- sentences that are vague in meaning, implausible, or ungrammatical but receive unexpectedly high acceptability judgments by humans. We looked at three illusions: the comparative illusion (e.g. "More people have been to Russia than I have"), the depth-charge illusion (e.g. "No head injury is too trivial to be ignored"), and the negative polarity item (NPI) illusion (e.g. "The hunter who no villager believed to be trustworthy will ever shoot a bear"). We found that probabilities represented by LMs were more likely to align with human judgments of being "tricked" by the NPI illusion which examines a structural dep
&lt;/p&gt;</description></item><item><title>GPT-4V&#20316;&#20026;&#19968;&#20010;&#36890;&#29992;&#35780;&#20272;&#22120;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#19968;&#33268;&#24615;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01361</link><description>&lt;p&gt;
GPT-4V(ision)&#20316;&#20026;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#36890;&#29992;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks. (arXiv:2311.01361v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01361
&lt;/p&gt;
&lt;p&gt;
GPT-4V&#20316;&#20026;&#19968;&#20010;&#36890;&#29992;&#35780;&#20272;&#22120;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#19968;&#33268;&#24615;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#21453;&#26144;&#20154;&#31867;&#21028;&#26029;&#26041;&#38754;&#65292;&#38480;&#21046;&#22312;&#32771;&#34385;&#32454;&#33268;&#24046;&#24322;&#26041;&#38754;&#12290;&#23613;&#31649;GPT-4V&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23558;GPT-4V&#20316;&#20026;&#36825;&#20123;&#20219;&#21153;&#30340;&#36890;&#29992;&#35780;&#20272;&#22120;&#23578;&#26410;&#34987;&#31995;&#32479;&#22320;&#25506;&#32034;&#12290;&#25105;&#20204;&#20840;&#38754;&#39564;&#35777;&#20102;GPT-4V&#22312;&#35780;&#20272;&#30446;&#30340;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#28041;&#21450;&#22522;&#30784;&#30340;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#12289;&#39640;&#32423;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#21644;&#22810;&#22270;&#20687;&#21040;&#25991;&#26412;&#23545;&#40784;&#31561;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4V&#37319;&#29992;&#20102;&#20004;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#21333;&#19968;&#31572;&#26696;&#35780;&#20998;&#21644;&#37197;&#23545;&#27604;&#36739;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GPT-4V&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#35780;&#20272;&#26041;&#27861;&#19978;&#19982;&#20154;&#31867;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#19968;&#33268;&#24615;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#23613;&#31649;&#23384;&#22312;&#38480;&#21046;&#65292;&#22914;&#38480;&#21046;&#30340;&#35270;&#35273;&#28165;&#26224;&#24230;&#35780;&#20998;&#21644;&#30495;&#23454;&#19990;&#30028;&#22797;&#26434;&#25512;&#29702;&#65292;&#20294;&#23427;&#33021;&#22815;&#25552;&#20379;&#20154;&#31867;&#23545;&#40784;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically evaluating vision-language tasks is challenging, especially when it comes to reflecting human judgments due to limitations in accounting for fine-grained details. Although GPT-4V has shown promising results in various multi-modal tasks, leveraging GPT-4V as a generalist evaluator for these tasks has not yet been systematically explored. We comprehensively validate GPT-4V's capabilities for evaluation purposes, addressing tasks ranging from foundational image-to-text and text-to-image synthesis to high-level image-to-image translations and multi-images to text alignment. We employ two evaluation methods, single-answer grading and pairwise comparison, using GPT-4V. Notably, GPT-4V shows promising agreement with humans across various tasks and evaluation methods, demonstrating immense potential for multi-modal LLMs as evaluators. Despite limitations like restricted visual clarity grading and real-world complex reasoning, its ability to provide human-aligned scores enriched w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#33410;&#28857;&#37051;&#23621;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#21152;&#20837;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25913;&#36827;&#30693;&#35782;&#22270;&#35889;&#23436;&#21892;&#26041;&#27861;&#12290;&#22312;&#24402;&#32435;&#21644;&#20256;&#36882;&#24335;Wikidata&#23376;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KGC&#26041;&#27861;&#12290;&#37051;&#23621;&#20449;&#24687;&#23545;&#27169;&#22411;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.01326</link><description>&lt;p&gt;
&#26356;&#22909;&#22320;&#22312;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#23436;&#21892;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#37051;&#23621;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information. (arXiv:2311.01326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#33410;&#28857;&#37051;&#23621;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#21152;&#20837;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25913;&#36827;&#30693;&#35782;&#22270;&#35889;&#23436;&#21892;&#26041;&#27861;&#12290;&#22312;&#24402;&#32435;&#21644;&#20256;&#36882;&#24335;Wikidata&#23376;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KGC&#26041;&#27861;&#12290;&#37051;&#23621;&#20449;&#24687;&#23545;&#27169;&#22411;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#32463;&#24120;&#23384;&#22312;&#19981;&#23436;&#25972;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#20854;&#28508;&#22312;&#24615;&#33021;&#12290;&#30693;&#35782;&#22270;&#35889;&#23436;&#21892;&#65288;KGC&#65289;&#25216;&#26415;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;KGC&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#19978;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65292;&#19981;&#23454;&#38469;&#65292;&#38656;&#35201;&#23398;&#20064;&#23494;&#38598;&#30340;&#33410;&#28857;&#23884;&#20837;&#21644;&#35745;&#31639;&#25104;&#23545;&#36317;&#31163;&#12290;&#29983;&#25104;&#24335;&#36716;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;T5&#21644;&#26368;&#36817;&#30340;KGT5&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#30452;&#25509;&#39044;&#27979;&#23614;&#33410;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#19978;&#21253;&#21547;&#33410;&#28857;&#37051;&#23621;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#26469;&#25913;&#36827;KGC&#26041;&#27861;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#36825;&#31181;&#34917;&#20840;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#24402;&#32435;&#21644;&#20256;&#36882;&#24335;Wikidata&#23376;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;KGT5&#21644;&#20256;&#32479;&#30340;KGC&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23545;&#37051;&#23621;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#36890;&#36807;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;KGC&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world Knowledge Graphs (KGs) often suffer from incompleteness, which limits their potential performance. Knowledge Graph Completion (KGC) techniques aim to address this issue. However, traditional KGC methods are computationally intensive and impractical for large-scale KGs, necessitating the learning of dense node embeddings and computing pairwise distances. Generative transformer-based language models (e.g., T5 and recent KGT5) offer a promising solution as they can predict the tail nodes directly. In this study, we propose to include node neighborhoods as additional information to improve KGC methods based on language models. We examine the effects of this imputation and show that, on both inductive and transductive Wikidata subsets, our method outperforms KGT5 and conventional KGC approaches. We also provide an extensive analysis of the impact of neighborhood on model prediction and show its importance. Furthermore, we point the way to significantly improve KGC through more ef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32553;&#25918;&#21644;&#26816;&#32034;&#22686;&#24378;&#20004;&#31181;&#31574;&#30053;&#65292;&#20998;&#26512;&#20102;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#19981;&#19968;&#33268;&#24615;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#25928;&#26524;&#26356;&#22909;&#12290;&#21477;&#27861;&#24418;&#24335;&#21644;&#20854;&#20182;&#35780;&#20272;&#20219;&#21153;&#30340;&#26500;&#36896;&#23545;&#20110;&#19968;&#33268;&#24615;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.01307</link><description>&lt;p&gt;
&#32553;&#25918;&#12289;&#26816;&#32034;&#22686;&#24378;&#21644;&#24418;&#24335;&#23545;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models. (arXiv:2311.01307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32553;&#25918;&#21644;&#26816;&#32034;&#22686;&#24378;&#20004;&#31181;&#31574;&#30053;&#65292;&#20998;&#26512;&#20102;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#19981;&#19968;&#33268;&#24615;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#25928;&#26524;&#26356;&#22909;&#12290;&#21477;&#27861;&#24418;&#24335;&#21644;&#20854;&#20182;&#35780;&#20272;&#20219;&#21153;&#30340;&#26500;&#36896;&#23545;&#20110;&#19968;&#33268;&#24615;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#33258;&#28982;&#30340;&#20107;&#23454;&#30693;&#35782;&#25509;&#21475;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#20542;&#21521;&#20110;&#23545;&#35821;&#20041;&#31561;&#25928;&#30340;&#38382;&#39064;&#32473;&#20986;&#19981;&#19968;&#33268;&#30340;&#31572;&#26696;&#65292;&#20854;&#23454;&#29992;&#24615;&#21463;&#38480;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#19968;&#33268;&#24615;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#35780;&#20272;&#20102;&#20004;&#31181;&#32531;&#35299;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65306;&#36890;&#36807;&#22686;&#21152;&#35268;&#27169;&#21644;&#20351;&#29992;&#26816;&#32034;&#35821;&#26009;&#24211;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;LLaMA&#21644;Atlas&#27169;&#22411;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#31574;&#30053;&#37117;&#33021;&#20943;&#23569;&#19981;&#19968;&#33268;&#24615;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#25928;&#26524;&#26356;&#26174;&#33879;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#24182;&#21306;&#20998;&#20102;Atlas&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#23545;&#19968;&#33268;&#24615;&#30340;&#36129;&#29486;&#12290;&#23545;&#20110;&#25152;&#26377;&#35780;&#20272;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#21477;&#27861;&#24418;&#24335;&#21644;&#20854;&#20182;&#35780;&#20272;&#20219;&#21153;&#30340;&#26500;&#36896;&#23545;&#19968;&#33268;&#24615;&#26377;&#24433;&#21709;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#29702;&#35299;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might predict both "Anne Redpath passed away in Edinburgh." and "Anne Redpath's life ended in London." In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient. We further consider and disentangle the consistency contributions of different components of Atlas. For all LMs evaluated we find that syntactical form and other evaluation task artifacts impact consistency. Taken together, our results provide a better understanding of the factors affecting the factual consistency of language models.
&lt;/p&gt;</description></item><item><title>AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01305</link><description>&lt;p&gt;
AWEQ&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01305
&lt;/p&gt;
&lt;p&gt;
AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#20063;&#30456;&#23545;&#36739;&#39640;&#12290;&#37327;&#21270;&#36825;&#20123;&#27169;&#22411;&#26159;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#30828;&#20214;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AWEQ&#65292;&#19968;&#31181;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;AWEQ&#22312;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;(W8A8)&#37327;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#35266;&#23519;&#21040;&#26435;&#37325;&#37327;&#21270;&#27604;&#28608;&#27963;&#37327;&#21270;&#26356;&#23481;&#26131;&#12290;AWEQ&#36890;&#36807;&#36890;&#36947;&#22343;&#34913;&#23558;&#28608;&#27963;&#37327;&#21270;&#30340;&#38590;&#24230;&#36716;&#31227;&#21040;&#26435;&#37325;&#19978;&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#37327;&#21270;&#22256;&#38590;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#22343;&#34913;&#26041;&#27861;&#65292;&#20943;&#23567;&#20102;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#20687;LLaMA&#36825;&#26679;&#30340;&#27969;&#34892;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA a
&lt;/p&gt;</description></item><item><title>FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2311.01282</link><description>&lt;p&gt;
FlashDecoding++: &#22312;GPU&#19978;&#21152;&#36895;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26356;&#24555;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01282
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#26410;&#35299;&#20915;&#65306;(1) &#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12290;softmax&#25805;&#20316;&#38656;&#35201;&#21516;&#27493;&#26356;&#26032;&#27599;&#20010;&#37096;&#20998;softmax&#32467;&#26524;&#65292;&#23548;&#33268;LLM&#20013;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#24320;&#38144;&#22686;&#21152;&#32422;20%&#12290;(2) &#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#12290;&#22312;LLM&#25512;&#29702;&#20013;&#25191;&#34892;GEMM&#30340;&#30697;&#38453;&#24418;&#29366;&#26159;&#25153;&#24179;&#30340;&#65292;&#23548;&#33268;&#22312;&#20808;&#21069;&#30340;&#35774;&#35745;&#20013;&#22635;&#20805;&#38646;&#21518;&#35745;&#31639;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#24615;&#33021;&#25439;&#22833;&#36229;&#36807;50%&#12290;(3) &#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;LLM&#20013;&#30340;&#20869;&#26680;&#24615;&#33021;&#21462;&#20915;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#25968;&#25454;&#29305;&#24449;&#12289;&#30828;&#20214;&#37197;&#32622;&#31561;&#12290;&#21333;&#19968;&#21644;&#38745;&#24577;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#23548;&#33268;LLM&#25512;&#29702;&#20013;&#19981;&#21516;&#24418;&#29366;&#30340;GEMM&#30340;&#24615;&#33021;&#25439;&#22833;&#36798;&#21040;50.25%&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FlashDecoding++&#65292;&#19968;&#31181;&#24555;&#36895;&#25903;&#25345;&#20027;&#27969;LLM&#21644;&#30828;&#20214;&#21518;&#31471;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;FlashDecoding++&#23454;&#29616;&#20102;&#20197;&#19979;&#30446;&#26631;&#65306;
&lt;/p&gt;
&lt;p&gt;
As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and &gt;50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.  We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#27880;&#21644;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#25429;&#25417;&#21475;&#35821;&#23545;&#35805;&#20013;&#30340;&#20849;&#21516;&#22522;&#30784;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#21629;&#39064;&#21644;&#36319;&#36394;&#23427;&#20204;&#22312;&#20849;&#21516;&#22522;&#30784;&#20013;&#30340;&#29366;&#24577;&#30340;&#23454;&#39564;&#65292;&#23545;&#21475;&#35821;&#23545;&#35805;&#20013;&#30340;&#20849;&#21516;&#22522;&#30784;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2311.01273</link><description>&lt;p&gt;
&#21457;&#29616;&#20849;&#21516;&#28857;&#65306;&#26631;&#27880;&#21644;&#39044;&#27979;&#21475;&#35821;&#23545;&#35805;&#20013;&#30340;&#20849;&#21516;&#28857;
&lt;/p&gt;
&lt;p&gt;
Finding Common Ground: Annotating and Predicting Common Ground in Spoken Conversations. (arXiv:2311.01273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#27880;&#21644;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#25429;&#25417;&#21475;&#35821;&#23545;&#35805;&#20013;&#30340;&#20849;&#21516;&#22522;&#30784;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#21629;&#39064;&#21644;&#36319;&#36394;&#23427;&#20204;&#22312;&#20849;&#21516;&#22522;&#30784;&#20013;&#30340;&#29366;&#24577;&#30340;&#23454;&#39564;&#65292;&#23545;&#21475;&#35821;&#23545;&#35805;&#20013;&#30340;&#20849;&#21516;&#22522;&#30784;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25105;&#20204;&#19982;&#20854;&#20182;&#20154;&#20132;&#27969;&#26102;&#65292;&#25105;&#20204;&#19981;&#20165;&#20165;&#26159;&#29983;&#25104;&#19968;&#31995;&#21015;&#30340;&#35789;&#35821;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#35748;&#30693;&#29366;&#24577;&#65288;&#20449;&#24565;&#65292;&#27442;&#26395;&#65292;&#24847;&#22270;&#65289;&#21644;&#25105;&#20204;&#23545;&#21548;&#20247;&#35748;&#30693;&#29366;&#24577;&#30340;&#27169;&#22411;&#26469;&#21019;&#24314;&#23545;&#35805;&#65292;&#20197;&#39044;&#26399;&#30340;&#26041;&#24335;&#24433;&#21709;&#21548;&#20247;&#30340;&#35748;&#30693;&#29366;&#24577;&#12290;&#35748;&#30693;&#29366;&#24577;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#20849;&#21516;&#22522;&#30784;&#65292;&#21363;&#35828;&#35805;&#32773;&#30456;&#20449;&#65292;&#24182;&#19988;&#35828;&#35805;&#32773;&#35748;&#20026;&#21548;&#20247;&#30456;&#20449;&#65292;&#20197;&#27492;&#31867;&#25512;&#30340;&#20869;&#23481;&#12290;&#34429;&#28982;&#35748;&#30693;&#31185;&#23398;&#20013;&#23545;&#20849;&#21516;&#22522;&#30784;&#24050;&#32463;&#20184;&#20986;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24182;&#27809;&#26377;&#22826;&#22810;&#30340;&#30456;&#20851;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#21644;&#35821;&#26009;&#24211;&#26469;&#25429;&#25417;&#20849;&#21516;&#22522;&#30784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20123;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#21629;&#39064;&#24182;&#20174;&#27599;&#20010;&#35828;&#35805;&#32773;&#30340;&#35282;&#24230;&#36319;&#36394;&#23427;&#20204;&#22312;&#20849;&#21516;&#22522;&#30784;&#20013;&#30340;&#29366;&#24577;&#30340;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
When we communicate with other humans, we do not simply generate a sequence of words. Rather, we use our cognitive state (beliefs, desires, intentions) and our model of the audience's cognitive state to create utterances that affect the audience's cognitive state in the intended manner. An important part of cognitive state is the common ground, which is the content the speaker believes, and the speaker believes the audience believes, and so on. While much attention has been paid to common ground in cognitive science, there has not been much work in natural language processing. In this paper, we introduce a new annotation and corpus to capture common ground. We then describe some initial experiments extracting propositions from dialog and tracking their status in the common ground from the perspective of each speaker.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65288;CADs&#65289;&#19982;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#20173;&#28982;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01270</link><description>&lt;p&gt;
&#20154;&#31867;&#36827;&#34892;&#26356;&#22909;&#30340;&#32534;&#36753;&#65306;&#34913;&#37327;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#22312;&#26377;&#23475;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection. (arXiv:2311.01270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65288;CADs&#65289;&#19982;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#20173;&#28982;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#22312;&#35768;&#22810;&#37325;&#35201;&#30340;&#31038;&#20250;&#35745;&#31639;&#20219;&#21153;&#20013;&#34987;&#20351;&#29992;&#65292;&#22914;&#26816;&#27979;&#24615;&#21035;&#27495;&#35270;&#12289;&#31181;&#26063;&#27495;&#35270;&#25110;&#20854;&#20182;&#20167;&#24680;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#30340;&#24037;&#20316;&#23581;&#35797;&#35299;&#20915;&#36825;&#20123;&#34394;&#20551;&#29305;&#24449;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65288;CADs&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;CADs&#23545;&#29616;&#26377;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#26368;&#23567;&#25913;&#21160;&#24182;&#32763;&#36716;&#26631;&#31614;&#65307;&#22312;&#20854;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#29983;&#25104;CADs&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#29983;&#25104;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#33258;&#21160;&#21270;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;Polyjuice&#12289;ChatGPT&#21644;Flan-T5&#33258;&#21160;&#29983;&#25104;CADs&#65292;&#24182;&#19982;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#39046;&#22495;&#22806;&#27979;&#35797;&#38598;&#19978;&#27979;&#35797;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#25163;&#21160;CADs&#20173;&#28982;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. Therefore, it is imperative that these models are robust to spurious features. Past work has attempted to tackle such spurious features using training data augmentation, including Counterfactually Augmented Data (CADs). CADs introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. However, manually generating CADs can be time-consuming and expensive. Hence in this work, we assess if this task can be automated using generative NLP models. We automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate their usefulness in improving model robustness compared to manually-generated CADs. By testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual CADs are still the most e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#32508;&#21512;&#32771;&#34385;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21155;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#24615;&#33021;&#30456;&#36817;&#30340;&#24773;&#20917;&#19979;&#24212;&#37325;&#35270;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#26041;&#38754;&#30340;&#32771;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01256</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#28304;&#30340;&#27861;&#24459;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#24120;&#35265;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An energy-based comparative analysis of common approaches to text classification in the Legal domain. (arXiv:2311.01256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#32508;&#21512;&#32771;&#34385;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21155;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#24615;&#33021;&#30456;&#36817;&#30340;&#24773;&#20917;&#19979;&#24212;&#37325;&#35270;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#26041;&#38754;&#30340;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#35780;&#20272;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36861;&#27714;&#26368;&#20339;&#24615;&#33021;&#30340;&#31454;&#20105;&#20013;&#65292;&#32463;&#24120;&#24573;&#35270;&#35768;&#22810;&#37325;&#35201;&#22240;&#32032;&#65292;&#32780;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#22240;&#32032;&#24212;&#35813;&#34987;&#20180;&#32454;&#32771;&#34385;&#12290;&#23454;&#38469;&#19978;&#65292;&#26377;&#26102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#32780;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#22240;&#32032;&#24517;&#39035;&#32771;&#34385;&#22312;&#20869;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;NLP&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;LexGLUE&#22522;&#20934;&#19978;&#23545;LLM&#21644;&#20256;&#32479;&#26041;&#27861;&#65288;&#20363;&#22914;SVM&#65289;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#37327;&#27604;&#36739;&#65292;&#21516;&#26102;&#32771;&#34385;&#24615;&#33021;&#65288;&#26631;&#20934;&#25351;&#26631;&#65289;&#21644;&#20854;&#20182;&#25351;&#26631;&#65292;&#22914;&#26102;&#38388;&#12289;&#32791;&#33021;&#21644;&#25104;&#26412;&#65292;&#24635;&#20043;&#23601;&#26159;&#30899;&#36275;&#36857;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#20998;&#21035;&#32771;&#34385;&#20102;&#21407;&#22411;&#35774;&#35745;&#38454;&#27573;&#65288;&#36890;&#36807;&#35757;&#32451;-&#39564;&#35777;-&#27979;&#35797;&#36845;&#20195;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65289;&#21644;&#29983;&#20135;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most Machine Learning research evaluates the best solutions in terms of performance. However, in the race for the best performing model, many important aspects are often overlooked when, on the contrary, they should be carefully considered. In fact, sometimes the gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration. Large Language Models (LLMs) are extensively adopted to address NLP problems in academia and industry. In this work, we present a detailed quantitative comparison of LLM and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes into account both performance (standard indices) and alternative metrics such as timing, power consumption and cost, in a word: the carbon-footprint. In our analysis, we considered the prototyping phase (model selection by training-validation-test iterations) and in-production phases separately, since they fol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2311.01200</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#24040;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#22312;&#38543;&#26102;&#38388;&#25512;&#31227;&#32780;&#20986;&#29616;&#26032;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#26032;&#27169;&#22411;&#32780;&#19981;&#26159;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#25910;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26032;&#35821;&#35328;&#20986;&#29616;&#26102;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#22909;&#22788;&#21644;&#24330;&#31471;&#65292;&#21363;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;&#20174;&#21333;&#35821;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20986;&#21457;&#65292;&#25105;&#20204;&#36880;&#27493;&#28155;&#21152;&#20102;&#26469;&#33258;&#25386;&#23041;&#35821;&#21644;&#20912;&#23707;&#35821;&#30340;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#21069;&#21521;&#21644;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#22914;&#20309;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#39034;&#24207;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#21069;&#21521;&#36716;&#31227;&#20027;&#35201;&#26159;&#27491;&#21521;&#30340;&#65292;&#19981;&#21463;&#35821;&#35328;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#21017;&#21487;&#33021;&#26159;&#27491;&#21521;&#30340;&#25110;&#36127;&#21521;&#30340;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#24335;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20960;&#31181;&#35821;&#35328;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metr
&lt;/p&gt;</description></item><item><title>CRUSH4SQL&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26816;&#32034;&#36807;&#31243;&#65292;&#20351;&#29992;&#27169;&#24335;&#24187;&#35273;&#36827;&#34892;Text2SQL&#30340;&#38598;&#20307;&#26816;&#32034;&#65292;&#36890;&#36807;&#24187;&#35273;&#30340;&#26368;&#23567;&#25968;&#25454;&#24211;&#27169;&#24335;&#26816;&#32034;&#23454;&#38469;&#27169;&#24335;&#30340;&#23376;&#38598;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#25968;&#25454;&#24211;&#20013;&#30340;&#27169;&#24335;&#23376;&#38598;&#26816;&#32034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01173</link><description>&lt;p&gt;
CRUSH4SQL&#65306;&#20351;&#29992;&#27169;&#24335;&#24187;&#35273;&#36827;&#34892;Text2SQL&#30340;&#38598;&#20307;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL. (arXiv:2311.01173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01173
&lt;/p&gt;
&lt;p&gt;
CRUSH4SQL&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26816;&#32034;&#36807;&#31243;&#65292;&#20351;&#29992;&#27169;&#24335;&#24187;&#35273;&#36827;&#34892;Text2SQL&#30340;&#38598;&#20307;&#26816;&#32034;&#65292;&#36890;&#36807;&#24187;&#35273;&#30340;&#26368;&#23567;&#25968;&#25454;&#24211;&#27169;&#24335;&#26816;&#32034;&#23454;&#38469;&#27169;&#24335;&#30340;&#23376;&#38598;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#25968;&#25454;&#24211;&#20013;&#30340;&#27169;&#24335;&#23376;&#38598;&#26816;&#32034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;Text-to-SQL&#29983;&#25104;&#22120;&#38656;&#35201;&#23558;&#25972;&#20010;&#27169;&#24335;&#19982;&#29992;&#25143;&#25991;&#26412;&#36827;&#34892;&#32534;&#30721;&#12290;&#23545;&#20110;&#20855;&#26377;&#25104;&#21315;&#19978;&#19975;&#21015;&#30340;&#22823;&#22411;&#25968;&#25454;&#24211;&#26469;&#35828;&#65292;&#36825;&#26159;&#26114;&#36149;&#25110;&#19981;&#21487;&#34892;&#30340;&#12290;&#26631;&#20934;&#30340;&#31264;&#23494;&#26816;&#32034;&#25216;&#26415;&#23545;&#20110;&#22823;&#22411;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#30340;&#27169;&#24335;&#23376;&#38598;&#26816;&#32034;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#22240;&#20026;&#27491;&#30830;&#30340;&#26816;&#32034;&#35821;&#20041;&#35201;&#27714;&#25105;&#20204;&#23545;&#27169;&#24335;&#20803;&#32032;&#32452;&#36827;&#34892;&#25490;&#24207;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#20803;&#32032;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26816;&#32034;&#36807;&#31243;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#35206;&#30422;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25351;&#23548;LLM&#24187;&#35273;&#20986;&#19968;&#20010;&#34987;&#35748;&#20026;&#36275;&#22815;&#22238;&#31572;&#26597;&#35810;&#30340;&#26368;&#23567;&#25968;&#25454;&#24211;&#27169;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#24187;&#35273;&#30340;&#27169;&#24335;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#31264;&#23494;&#26816;&#32034;&#30340;&#32467;&#26524;&#26469;&#26816;&#32034;&#23454;&#38469;&#27169;&#24335;&#30340;&#23376;&#38598;&#12290;&#26174;&#28982;&#65292;&#24187;&#35273;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#40635;&#28902;&#65292;&#20294;&#20107;&#23454;&#35777;&#26126;&#23427;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#26377;&#29992;&#30340;&#26725;&#26753;&#26426;&#21046;&#12290;&#30001;&#20110;&#30446;&#21069;&#36824;&#27809;&#26377;&#38024;&#23545;&#22823;&#22411;&#25968;&#25454;&#24211;&#30340;&#27169;&#24335;&#23376;&#38598;&#30340;&#29616;&#26377;&#22522;&#20934;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Text-to-SQL generators require the entire schema to be encoded with the user text. This is expensive or impractical for large databases with tens of thousands of columns. Standard dense retrieval techniques are inadequate for schema subsetting of a large structured database, where the correct semantics of retrieval demands that we rank sets of schema elements rather than individual elements. In response, we propose a two-stage process for effective coverage during retrieval. First, we instruct an LLM to hallucinate a minimal DB schema deemed adequate to answer the query. We use the hallucinated schema to retrieve a subset of the actual schema, by composing the results from multiple dense retrievals. Remarkably, hallucination $\unicode{x2013}$ generally considered a nuisance $\unicode{x2013}$ turns out to be actually useful as a bridging mechanism. Since no existing benchmarks exist for schema subsetting on large databases, we introduce three benchmarks. Two semi-synthetic data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#36755;&#20837;&#33539;&#24335;GeneInput&#65292;&#32467;&#21512;&#25552;&#31034;&#21644;&#29992;&#25143;&#21453;&#39304;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#36755;&#20837;&#22788;&#29702;&#65292;&#22312;&#20013;&#25991;&#36755;&#20837;&#27861;&#24341;&#25806;&#30340;&#26500;&#24314;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01166</link><description>&lt;p&gt;
&#29983;&#25104;&#36755;&#20837;&#65306;&#36808;&#21521;&#19979;&#19968;&#20195;&#36755;&#20837;&#26041;&#27861;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Generative Input: Towards Next-Generation Input Methods Paradigm. (arXiv:2311.01166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#36755;&#20837;&#33539;&#24335;GeneInput&#65292;&#32467;&#21512;&#25552;&#31034;&#21644;&#29992;&#25143;&#21453;&#39304;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#36755;&#20837;&#22788;&#29702;&#65292;&#22312;&#20013;&#25991;&#36755;&#20837;&#27861;&#24341;&#25806;&#30340;&#26500;&#24314;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;ChatGPT&#21457;&#24067;&#20197;&#26469;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24182;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#36755;&#20837;&#27861;&#39046;&#22495;&#20013;&#65292;&#20854;&#24212;&#29992;&#20173;&#28982;&#19981;&#22815;&#28145;&#20837;&#12290;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#24050;&#34987;&#24212;&#29992;&#20110;&#20013;&#25991;&#36755;&#20837;&#27861;&#24341;&#25806;&#30340;&#26500;&#24314;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24120;&#24120;&#20551;&#35774;&#36755;&#20837;&#30340;&#25340;&#38899;&#26159;&#27491;&#30830;&#30340;&#65292;&#24182;&#20851;&#27880;&#25340;&#38899;&#21040;&#23383;&#31526;&#30340;&#36716;&#25442;&#20219;&#21153;&#65292;&#36825;&#22312;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#26041;&#38754;&#26174;&#28982;&#19981;&#36275;&#22815;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#26080;&#27861;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#26469;&#20248;&#21270;&#27169;&#22411;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeneInput&#30340;&#20840;&#26032;&#29983;&#25104;&#36755;&#20837;&#33539;&#24335;&#12290;&#23427;&#20351;&#29992;&#25552;&#31034;&#22788;&#29702;&#25152;&#26377;&#36755;&#20837;&#22330;&#26223;&#21644;&#20854;&#20182;&#26234;&#33021;&#36741;&#21161;&#36755;&#20837;&#21151;&#33021;&#65292;&#36890;&#36807;&#29992;&#25143;&#21453;&#39304;&#20248;&#21270;&#27169;&#22411;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#20840;&#27169;&#24335;&#25353;&#38190;&#24207;&#21015;&#21040;&#23383;&#31526;&#30340;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the release of ChatGPT, generative models have achieved tremendous success and become the de facto approach for various NLP tasks. However, its application in the field of input methods remains under-explored. Many neural network approaches have been applied to the construction of Chinese input method engines(IMEs).Previous research often assumed that the input pinyin was correct and focused on Pinyin-to-character(P2C) task, which significantly falls short of meeting users' demands. Moreover, previous research could not leverage user feedback to optimize the model and provide personalized results. In this study, we propose a novel Generative Input paradigm named GeneInput. It uses prompts to handle all input scenarios and other intelligent auxiliary input functions, optimizing the model with user feedback to deliver personalized results. The results demonstrate that we have achieved state-of-the-art performance for the first time in the Full-mode Key-sequence to Characters(FK2C) 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31243;&#24207;&#25191;&#34892;&#32467;&#26524;&#30340;&#39046;&#22495;&#26080;&#20851;&#31579;&#36873;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;&#35821;&#20041;&#35299;&#26512;&#20013;&#30340;&#34394;&#20551;&#31243;&#24207;&#38382;&#39064;</title><link>http://arxiv.org/abs/2311.01161</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#19979;&#22522;&#20110;&#25191;&#34892;&#30340;&#34394;&#20551;&#31243;&#24207;&#36807;&#28388;&#30340;&#35821;&#20041;&#35299;&#26512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Semantic Parsing with Execution-based Spurious Program Filtering. (arXiv:2311.01161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31243;&#24207;&#25191;&#34892;&#32467;&#26524;&#30340;&#39046;&#22495;&#26080;&#20851;&#31579;&#36873;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;&#35821;&#20041;&#35299;&#26512;&#20013;&#30340;&#34394;&#20551;&#31243;&#24207;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24369;&#30417;&#30563;&#19979;&#35757;&#32451;&#35821;&#20041;&#35299;&#26512;&#22120;&#26102;&#65292;&#34394;&#20551;&#31243;&#24207;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#28040;&#38500;&#20855;&#26377;&#38169;&#35823;&#35821;&#20041;&#20294;&#27491;&#30830;&#25351;&#31034;&#30340;&#31243;&#24207;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#30528;&#37325;&#20110;&#21033;&#29992;&#22522;&#20110;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20363;&#23376;&#30456;&#20284;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31243;&#24207;&#25191;&#34892;&#32467;&#26524;&#30340;&#39046;&#22495;&#26080;&#20851;&#31579;&#36873;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#36890;&#36807;&#25628;&#32034;&#36807;&#31243;&#33719;&#24471;&#30340;&#27599;&#20010;&#31243;&#24207;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#34920;&#31034;&#65292;&#23427;&#20197;&#21508;&#31181;&#36755;&#20837;&#19979;&#30340;&#25191;&#34892;&#32467;&#26524;&#25429;&#25417;&#31243;&#24207;&#30340;&#35821;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#34920;&#31034;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#65292;&#20197;&#35782;&#21035;&#24182;&#36807;&#28388;&#25481;&#19982;&#20854;&#20182;&#31243;&#24207;&#20855;&#26377;&#26126;&#26174;&#19981;&#21516;&#35821;&#20041;&#30340;&#31243;&#24207;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#31243;&#24207;&#25628;&#32034;&#36807;&#31243;&#27491;&#20132;&#65292;&#22240;&#27492;&#21487;&#20197;&#36731;&#26494;&#22686;&#24378;&#20219;&#20309;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#35299;&#26512;&#26694;&#26550;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#21644;WikiTableQuestions&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
The problem of spurious programs is a longstanding challenge when training a semantic parser from weak supervision. To eliminate such programs that have wrong semantics but correct denotation, existing methods focus on exploiting similarities between examples based on domain-specific knowledge. In this paper, we propose a domain-agnostic filtering mechanism based on program execution results. Specifically, for each program obtained through the search process, we first construct a representation that captures the program's semantics as execution results under various inputs. Then, we run a majority vote on these representations to identify and filter out programs with significantly different semantics from the other programs. In particular, our method is orthogonal to the program search process so that it can easily augment any of the existing weakly supervised semantic parsing frameworks. Empirical evaluations on the Natural Language Visual Reasoning and WikiTableQuestions demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ACES&#25361;&#25112;&#38598;&#23545;WMT 2023&#20013;&#30340;segment-level&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#27809;&#26377;&#26126;&#30830;&#30340;&#36194;&#23478;&#65292;&#24182;&#19988;2023&#24180;&#29256;&#21644;2022&#24180;&#29256;&#25351;&#26631;&#20043;&#38388;&#30340;&#24615;&#33021;&#21464;&#21270;&#24456;&#22823;&#12290;&#24314;&#35758;&#25351;&#26631;&#24320;&#21457;&#32773;&#24212;&#35813;&#26500;&#24314;&#19981;&#21516;&#35774;&#35745;&#23478;&#26063;&#30340;&#25351;&#26631;&#38598;&#21512;&#65292;&#24182;&#24320;&#21457;&#26356;&#21152;&#20851;&#27880;&#25972;&#20307;&#24615;&#33021;&#30340;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2311.01153</link><description>&lt;p&gt;
ACES: WMT 2023&#20013;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#25361;&#25112;&#38598;
&lt;/p&gt;
&lt;p&gt;
ACES: Translation Accuracy Challenge Sets at WMT 2023. (arXiv:2311.01153v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ACES&#25361;&#25112;&#38598;&#23545;WMT 2023&#20013;&#30340;segment-level&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#27809;&#26377;&#26126;&#30830;&#30340;&#36194;&#23478;&#65292;&#24182;&#19988;2023&#24180;&#29256;&#21644;2022&#24180;&#29256;&#25351;&#26631;&#20043;&#38388;&#30340;&#24615;&#33021;&#21464;&#21270;&#24456;&#22823;&#12290;&#24314;&#35758;&#25351;&#26631;&#24320;&#21457;&#32773;&#24212;&#35813;&#26500;&#24314;&#19981;&#21516;&#35774;&#35745;&#23478;&#26063;&#30340;&#25351;&#26631;&#38598;&#21512;&#65292;&#24182;&#24320;&#21457;&#26356;&#21152;&#20851;&#27880;&#25972;&#20307;&#24615;&#33021;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;ACES&#25361;&#25112;&#38598;&#65288;Amrhein&#31561;&#20154;&#65292;2022&#65289;&#23545;&#25552;&#20132;&#21040;WMT 2023&#30340;segment-level&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#25361;&#25112;&#38598;&#21253;&#21547;36K&#20010;&#31034;&#20363;&#65292;&#20195;&#34920;&#20102;&#26469;&#33258;68&#31181;&#29616;&#35937;&#30340;&#25361;&#25112;&#65292;&#24182;&#28085;&#30422;&#20102;146&#31181;&#35821;&#35328;&#23545;&#12290;&#36825;&#20123;&#29616;&#35937;&#30340;&#33539;&#22260;&#20174;&#21333;&#35789;/&#23383;&#31526;&#32423;&#30340;&#31616;&#21333;&#25200;&#21160;&#21040;&#22522;&#20110;&#35805;&#35821;&#21644;&#29616;&#23454;&#19990;&#30028;&#30693;&#35782;&#30340;&#26356;&#22797;&#26434;&#30340;&#38169;&#35823;&#12290;&#23545;&#20110;&#27599;&#20010;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#19968;&#31995;&#21015;&#38169;&#35823;&#31867;&#21035;&#19978;&#30340;&#35814;&#32454;&#24615;&#33021;&#27010;&#20917;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#24555;&#36895;&#27604;&#36739;&#30340;&#25972;&#20307;ACES-Score&#12290;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#25552;&#20132;&#32473;WMT 2023&#21644;2022&#30340;&#25351;&#26631;&#30340;&#22686;&#37327;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#22312;&#25552;&#20132;&#32473;WMT 2023&#30340;&#25351;&#26631;&#20013;&#27809;&#26377;&#26126;&#26174;&#30340;&#36194;&#23478;&#65292;2&#65289;2023&#24180;&#29256;&#21644;2022&#24180;&#29256;&#25351;&#26631;&#20043;&#38388;&#30340;&#24615;&#33021;&#21464;&#21270;&#24456;&#22823;&#12290;&#25105;&#20204;&#30340;&#24314;&#35758;&#19982;WMT 2022&#30340;&#24314;&#35758;&#31867;&#20284;&#12290;&#25351;&#26631;&#24320;&#21457;&#32773;&#24212;&#35813;&#19987;&#27880;&#20110;&#65306;&#20174;&#19981;&#21516;&#35774;&#35745;&#23478;&#26063;&#26500;&#24314;&#25351;&#26631;&#38598;&#21512;&#65292;&#24320;&#21457;&#26356;&#21152;&#20851;&#27880;+&#30340;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
We benchmark the performance of segmentlevel metrics submitted to WMT 2023 using the ACES Challenge Set (Amrhein et al., 2022). The challenge set consists of 36K examples representing challenges from 68 phenomena and covering 146 language pairs. The phenomena range from simple perturbations at the word/character level to more complex errors based on discourse and real-world knowledge. For each metric, we provide a detailed profile of performance over a range of error categories as well as an overall ACES-Score for quick comparison. We also measure the incremental performance of the metrics submitted to both WMT 2023 and 2022. We find that 1) there is no clear winner among the metrics submitted to WMT 2023, and 2) performance change between the 2023 and 2022 versions of the metrics is highly variable. Our recommendations are similar to those from WMT 2022. Metric developers should focus on: building ensembles of metrics from different design families, developing metrics that pay more at
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#20041;&#19968;&#33268;&#24615;&#24230;&#37327;&#21644;&#20854;&#20182;&#30456;&#20851;&#24230;&#37327;&#65292;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#38382;&#39064;&#30340;&#20934;&#30830;&#22238;&#31572;&#33021;&#21147;&#12290;&#36890;&#36807;&#25163;&#21160;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#35813;&#26694;&#26550;&#22312;&#20116;&#20010;&#24403;&#20195;LLM&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26694;&#26550;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01152</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#19968;&#33268;&#24615;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#31572;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Predicting Question-Answering Performance of Large Language Models through Semantic Consistency. (arXiv:2311.01152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#20041;&#19968;&#33268;&#24615;&#24230;&#37327;&#21644;&#20854;&#20182;&#30456;&#20851;&#24230;&#37327;&#65292;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#38382;&#39064;&#30340;&#20934;&#30830;&#22238;&#31572;&#33021;&#21147;&#12290;&#36890;&#36807;&#25163;&#21160;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#35813;&#26694;&#26550;&#22312;&#20116;&#20010;&#24403;&#20195;LLM&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26694;&#26550;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#24191;&#20041;&#19978;&#23450;&#20041;&#20026;&#27169;&#22411;&#22312;&#32473;&#23450;&#35821;&#20041;&#30456;&#31561;&#30340;&#36755;&#20837;&#26102;&#20135;&#29983;&#35821;&#20041;&#30456;&#31561;&#30340;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#25163;&#21160;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#39640;&#36136;&#37327;&#25913;&#20889;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#35813;&#25968;&#25454;&#38598;&#21457;&#24067;&#32473;&#31038;&#21306;&#65292;&#35299;&#20915;&#20102;&#35780;&#20272;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38382;&#31572;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#23558;&#35821;&#20041;&#19968;&#33268;&#24615;&#24230;&#37327;&#19982;&#20808;&#21069;&#24037;&#20316;&#20013;&#24314;&#35758;&#19982;LLM&#38382;&#31572;&#20934;&#30830;&#24615;&#30456;&#20851;&#30340;&#20854;&#20182;&#24230;&#37327;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;&#29992;&#20110;&#20107;&#23454;&#24615;&#38382;&#31572;&#26080;&#21442;&#32771;&#24615;&#33021;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#21363;&#39044;&#27979;&#35821;&#35328;&#27169;&#22411;&#27491;&#30830;&#22238;&#31572;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#20116;&#20010;&#24403;&#20195;LLM&#19978;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#12289;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic consistency of a language model is broadly defined as the model's ability to produce semantically-equivalent outputs, given semantically-equivalent inputs. We address the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and release the dataset to the community.  We further combine the semantic consistency metric with additional measurements suggested in prior work as correlating with LLM QA accuracy, for building and evaluating a framework for factual QA reference-less performance prediction -- predicting the likelihood of a language model to accurately answer a question. Evaluating the framework on five contemporary LLMs, we demonstrate encouraging, significantly outperforming baselines, results.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#65292;&#21457;&#29616;&#23558;&#26410;&#23545;&#40784;&#30340;&#38543;&#26426;&#30693;&#35782;&#27880;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20197;&#21462;&#24471;&#19982;&#23545;&#40784;&#30693;&#35782;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#27491;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01150</link><description>&lt;p&gt;
&#37325;&#35775;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Knowledge Injection Frameworks. (arXiv:2311.01150v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01150
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#65292;&#21457;&#29616;&#23558;&#26410;&#23545;&#40784;&#30340;&#38543;&#26426;&#30693;&#35782;&#27880;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20197;&#21462;&#24471;&#19982;&#23545;&#40784;&#30693;&#35782;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#27491;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT&#65292;&#24050;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#20135;&#29983;&#20102;&#24040;&#22823;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#20351;&#36825;&#20123;LLMs&#26356;&#36866;&#24212;&#22402;&#30452;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#38382;&#39064;&#23578;&#26410;&#23436;&#20840;&#35299;&#20915;&#12290;&#23454;&#38469;&#19978;&#65292;&#22312;&#36825;&#26041;&#38754;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#20123;&#24037;&#20316;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#26500;&#24314;&#23545;&#40784;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#36890;&#36807;&#23558;&#30456;&#24212;&#30340;&#30693;&#35782;&#20803;&#32452;&#27880;&#20837;&#21040;&#30456;&#20851;&#30340;&#25991;&#26412;&#26679;&#26412;&#20013;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#24076;&#26395;&#65292;&#20294;&#25105;&#20204;&#26222;&#36941;&#21457;&#29616;&#36825;&#39033;&#24037;&#20316;&#20013;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#21457;&#29616;&#23558;&#26410;&#23545;&#40784;&#65288;&#21363;&#38543;&#26426;&#65289;&#30340;&#30693;&#35782;&#20803;&#32452;&#27880;&#20837;&#21040;LLMs&#20013;&#65292;&#21487;&#20197;&#21462;&#24471;&#19982;&#27880;&#20837;&#23545;&#40784;&#30693;&#35782;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#36825;&#19968;&#20196;&#20154;&#27822;&#20007;&#30340;&#21457;&#29616;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35843;&#26597;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#21487;&#33021;&#30340;&#35299;&#37322;&#12290;&#22522;&#20110;&#36825;&#19968;&#20999;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#27491;&#25216;&#26415;&#12290;&#31616;&#35201;&#22320;&#35828;&#65292;&#36825;&#31181;&#25216;&#26415;&#30340;&#26680;&#24515;&#26681;&#26893;&#20110;...
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs), such as GPTs, have attained great impact worldwide. However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved. Indeed, there have emerged a few works on this line where most of them rely on an alignment heuristic that is built to inject the corresponding knowledge tuple into the associated text sample.  However, despite the promise, we identify a pivotal problem in this work ubiquitously. Simply put, we find that injecting unaligned (i.e., random) knowledge tuple into the LLMs achieves comparable (and sometimes better) results than the aligned knowledge being injected. We therefore take a thorough investigation of this frustrating finding on a variety of related prior work and further provide a chain of potential interpretations for the phenomenon. Based on all that, we offer a simple remediated technique. Briefly, the core of this technique is rooted in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#24037;&#20855;&#38142;EvalWeb&#65292;&#29992;&#20110;&#20174;&#32593;&#32476;&#25968;&#25454;&#20013;&#25552;&#21462;&#24178;&#20928;&#30340;&#20013;&#25991;&#25991;&#26412;&#12290;&#36890;&#36807;&#25163;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#31579;&#38500;&#22122;&#38899;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#35780;&#20272;&#27169;&#22411;&#20026;&#27599;&#20010;&#25991;&#26412;&#20998;&#37197;&#36136;&#37327;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2311.01149</link><description>&lt;p&gt;
Chinesewebtext: &#29992;&#26377;&#25928;&#30340;&#35780;&#20272;&#27169;&#22411;&#25552;&#21462;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#32593;&#32476;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Chinesewebtext: Large-scale high-quality Chinese web text extracted with effective evaluation model. (arXiv:2311.01149v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#24037;&#20855;&#38142;EvalWeb&#65292;&#29992;&#20110;&#20174;&#32593;&#32476;&#25968;&#25454;&#20013;&#25552;&#21462;&#24178;&#20928;&#30340;&#20013;&#25991;&#25991;&#26412;&#12290;&#36890;&#36807;&#25163;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#31579;&#38500;&#22122;&#38899;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#35780;&#20272;&#27169;&#22411;&#20026;&#27599;&#20010;&#25991;&#26412;&#20998;&#37197;&#36136;&#37327;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#36807;&#31243;&#20013;&#65292;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#35268;&#27169;&#21644;&#36136;&#37327;&#23545;&#20110;&#22609;&#36896;LLM&#30340;&#33021;&#21147;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#21152;&#24555;LLM&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24050;&#32463;&#21457;&#24067;&#20102;&#19968;&#20123;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;C4 [1]&#12289;Pile [2]&#12289;RefinedWeb [3]&#21644;WanJuan [4]&#31561;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24050;&#21457;&#24067;&#30340;&#35821;&#26009;&#24211;&#20027;&#35201;&#20851;&#27880;&#33521;&#25991;&#65292;&#20173;&#28982;&#32570;&#20047;&#23436;&#25972;&#30340;&#24037;&#20855;&#38142;&#26469;&#20174;&#32593;&#32476;&#25968;&#25454;&#20013;&#25552;&#21462;&#20986;&#24178;&#20928;&#30340;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#23545;&#35821;&#26009;&#24211;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#20363;&#22914;&#27599;&#20010;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23436;&#25972;&#30340;&#24037;&#20855;&#38142;EvalWeb&#65292;&#29992;&#20110;&#20174;&#22024;&#26434;&#30340;&#32593;&#32476;&#25968;&#25454;&#20013;&#25552;&#21462;&#20013;&#25991;&#24178;&#20928;&#30340;&#25991;&#26412;&#12290;&#39318;&#20808;&#65292;&#31867;&#20284;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#20351;&#29992;&#25163;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#26469;&#20002;&#24323;&#21407;&#22987;&#29228;&#21462;&#30340;&#32593;&#32476;&#20869;&#23481;&#20013;&#30340;&#26126;&#30830;&#22024;&#26434;&#30340;&#25991;&#26412;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#35780;&#20272;&#27169;&#22411;&#26469;&#35780;&#20272;&#21097;&#20313;&#30456;&#23545;&#24178;&#20928;&#30340;&#25968;&#25454;&#65292;&#24182;&#20026;&#27599;&#20010;&#25991;&#26412;&#20998;&#37197;&#19968;&#20010;&#29305;&#23450;&#30340;&#36136;&#37327;&#20998;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;EvalWeb&#24037;&#20855;&#38142;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the development of large language models (LLMs), the scale and quality of the pre-training data play a crucial role in shaping LLMs' capabilities. To accelerate the research of LLMs, several large-scale datasets, such as C4 [1], Pile [2], RefinedWeb [3] and WanJuan [4], have been released to the public. However, most of the released corpus focus mainly on English, and there is still lack of complete tool-chain for extracting clean texts from web data. Furthermore, fine-grained information of the corpus, e.g. the quality of each text, is missing. To address these challenges, we propose in this paper a new complete tool-chain EvalWeb to extract Chinese clean texts from noisy web data. First, similar to previous work, manually crafted rules are employed to discard explicit noisy texts from the raw crawled web contents. Second, a well-designed evaluation model is leveraged to assess the remaining relatively clean data, and each text is assigned a specific quality score. Finally, we 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22122;&#22768;&#26631;&#31614;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01108</link><description>&lt;p&gt;
&#36890;&#36807;&#22806;&#37096;&#24341;&#23548;&#23454;&#29616;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22122;&#22768;&#40065;&#26834;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance. (arXiv:2311.01108v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01108
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22122;&#22768;&#26631;&#31614;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#21518;&#24494;&#35843;&#30340;&#20004;&#38454;&#27573;&#33539;&#24335;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#27880;&#37322;&#36807;&#31243;&#65292;&#25968;&#25454;&#26631;&#31614;&#36890;&#24120;&#23384;&#22312;&#22122;&#22768;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#24320;&#21457;&#38024;&#23545;&#36825;&#26679;&#22122;&#22768;&#26631;&#31614;&#30340;&#24494;&#35843;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22122;&#22768;&#26631;&#31614;&#23545;PLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#26041;&#27861;&#23558;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25351;&#23548;&#32435;&#20837;&#20854;&#20013;&#12290;&#36825;&#31181;&#25351;&#23548;&#26377;&#21161;&#20110;&#20934;&#30830;&#21306;&#20998;&#24178;&#20928;&#26679;&#26412;&#21644;&#22122;&#22768;&#26679;&#26412;&#65292;&#24182;&#25552;&#20379;&#20102;&#38500;&#22122;&#22768;&#26631;&#31614;&#22806;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#24494;&#35843;PLMs&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#24110;&#21161;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#22122;&#22768;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adopting a two-stage paradigm of pretraining followed by fine-tuning, Pretrained Language Models (PLMs) have achieved substantial advancements in the field of natural language processing. However, in real-world scenarios, data labels are often noisy due to the complex annotation process, making it essential to develop strategies for fine-tuning PLMs with such noisy labels. To this end, we introduce an innovative approach for fine-tuning PLMs using noisy labels, which incorporates the guidance of Large Language Models (LLMs) like ChatGPT. This guidance assists in accurately distinguishing between clean and noisy samples and provides supplementary information beyond the noisy labels, thereby boosting the learning process during fine-tuning PLMs. Extensive experiments on synthetic and real-world noisy datasets further demonstrate the superior advantages of our framework over the state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DistilWhisper&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#36827;&#34892;&#36731;&#37327;&#32423;&#27169;&#22359;&#21270;ASR&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#25104;&#21151;&#24357;&#21512;&#20102;&#22810;&#20219;&#21153;&#35821;&#38899;&#27169;&#22411;&#22312;&#23569;&#25968;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2311.01070</link><description>&lt;p&gt;
DistilWhisper&#65306;&#36890;&#36807;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#39640;&#25928;&#21387;&#32553;&#22810;&#20219;&#21153;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts. (arXiv:2311.01070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DistilWhisper&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#36827;&#34892;&#36731;&#37327;&#32423;&#27169;&#22359;&#21270;ASR&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#25104;&#21151;&#24357;&#21512;&#20102;&#22810;&#20219;&#21153;&#35821;&#38899;&#27169;&#22411;&#22312;&#23569;&#25968;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Whisper&#26159;&#19968;&#20010;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#30340;&#35821;&#38899;&#27169;&#22411;&#65292;&#28085;&#30422;99&#31181;&#35821;&#35328;&#12290;&#23427;&#22312;&#20854;&#28085;&#30422;&#30340;&#37096;&#20998;&#35821;&#35328;&#20013;&#33719;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#32467;&#26524;&#65292;&#20294;&#22312;&#19968;&#20123;&#25968;&#37327;&#21487;&#35266;&#30340;&#23569;&#25968;&#35821;&#35328;&#20013;&#65292;&#35813;&#27169;&#22411;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#23588;&#20854;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#29256;&#26412;&#20013;&#34920;&#29616;&#26356;&#20026;&#20005;&#37325;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistilWhisper&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;ASR&#26041;&#38754;&#24357;&#21512;&#36825;&#20123;&#35821;&#35328;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#30041;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#20248;&#21183;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#31574;&#30053;&#65306;&#20351;&#29992;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#23545;whisper-small&#36827;&#34892;&#36731;&#37327;&#32423;&#27169;&#22359;&#21270;ASR&#24494;&#35843;&#65292;&#24182;&#20174;whisper-large-v2&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#36825;&#31181;&#21452;&#37325;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20445;&#25345;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#25552;&#21319;ASR&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#26631;&#20934;&#24494;&#35843;&#25110;LoRA&#36866;&#37197;&#22120;&#26356;&#26377;&#25928;&#65292;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whisper is a multitask and multilingual speech model covering 99 languages. It yields commendable automatic speech recognition (ASR) results in a subset of its covered languages, but the model still under-performs on a non-negligible number of under-represented languages, a problem exacerbated in smaller model versions. In this work, we propose DistilWhisper, an approach able to bridge the performance gap in ASR for these languages while retaining the advantages of multitask and multilingual capabilities. Our approach involves two key strategies: lightweight modular ASR fine-tuning of whisper-small using language-specific experts, and knowledge distillation from whisper-large-v2. This dual approach allows us to effectively boost ASR performance while keeping the robustness inherited from the multitask and multilingual pre-training. Results demonstrate that our approach is more effective than standard fine-tuning or LoRA adapters, boosting performance in the targeted languages for both 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#32500;&#25968;&#25454;&#31934;&#21270;&#31574;&#30053;&#65292;&#21253;&#25324;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#29983;&#25104;&#22411;AI&#24037;&#20855;&#24320;&#21457;&#25968;&#25454;&#29228;&#21462;&#33050;&#26412;&#65292;&#29992;&#20110;&#35843;&#20248;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35813;&#31574;&#30053;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#20174;&#25552;&#31034;&#29983;&#25104;&#36234;&#21335;&#26032;&#38395;&#25991;&#31456;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01049</link><description>&lt;p&gt;
&#29992;&#20110;&#26377;&#25928;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#32500;&#25968;&#25454;&#31934;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Multi-dimensional data refining strategy for effective fine-tuning LLMs. (arXiv:2311.01049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#32500;&#25968;&#25454;&#31934;&#21270;&#31574;&#30053;&#65292;&#21253;&#25324;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#29983;&#25104;&#22411;AI&#24037;&#20855;&#24320;&#21457;&#25968;&#25454;&#29228;&#21462;&#33050;&#26412;&#65292;&#29992;&#20110;&#35843;&#20248;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35813;&#31574;&#30053;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#20174;&#25552;&#31034;&#29983;&#25104;&#36234;&#21335;&#26032;&#38395;&#25991;&#31456;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#35843;&#20248;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#65292;&#20294;&#33719;&#21462;&#21512;&#36866;&#30340;&#25968;&#25454;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#25968;&#25454;&#31232;&#32570;&#12289;&#35821;&#35328;&#22810;&#26679;&#24615;&#21644;&#29305;&#23450;&#39046;&#22495;&#20869;&#23481;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#29228;&#21462;&#21644;&#31934;&#21270;&#38024;&#23545;&#35843;&#20248;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#26102;&#25152;&#23398;&#21040;&#30340;&#32463;&#39564;&#12290;&#21046;&#20316;&#36825;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#38656;&#35201;&#32454;&#33268;&#30340;&#35745;&#21010;&#65292;&#32771;&#34385;&#21040;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#22312;&#21253;&#23481;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#31574;&#30053;&#65292;&#21253;&#25324;&#21033;&#29992;&#33521;&#35821;&#20013;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#20511;&#21161;&#29983;&#25104;&#22411;AI&#24037;&#20855;&#24320;&#21457;&#23450;&#21046;&#30340;&#25968;&#25454;&#29228;&#21462;&#33050;&#26412;&#12290;&#20351;&#29992;&#30001;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#20248;&#27169;&#22411;&#65292;&#22312;&#20174;&#25552;&#31034;&#29983;&#25104;&#36234;&#21335;&#26032;&#38395;&#25991;&#31456;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#20026;&#23558;&#26469;&#35843;&#20248;&#36234;&#21335;&#35821;&#31561;&#35821;&#35328;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data is a cornerstone for fine-tuning large language models, yet acquiring suitable data remains challenging. Challenges encompassed data scarcity, linguistic diversity, and domain-specific content. This paper presents lessons learned while crawling and refining data tailored for fine-tuning Vietnamese language models. Crafting such a dataset, while accounting for linguistic intricacies and striking a balance between inclusivity and accuracy, demands meticulous planning. Our paper presents a multidimensional strategy including leveraging existing datasets in the English language and developing customized data-crawling scripts with the assistance of generative AI tools. A fine-tuned LLM model for the Vietnamese language, which was produced using resultant datasets, demonstrated good performance while generating Vietnamese news articles from prompts. The study offers practical solutions and guidance for future fine-tuning models in languages like Vietnamese.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20250;&#25298;&#32477;&#65288;L2R&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#25298;&#32477;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01041</link><description>&lt;p&gt;
&#23398;&#20250;&#25298;&#32477;&#65306;&#36890;&#36807;&#30693;&#35782;&#33539;&#22260;&#38480;&#21046;&#21644;&#25298;&#32477;&#26426;&#21046;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#21487;&#25511;&#21644;&#21487;&#38752;
&lt;/p&gt;
&lt;p&gt;
Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism. (arXiv:2311.01041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20250;&#25298;&#32477;&#65288;L2R&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#25298;&#32477;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22238;&#31572;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24182;&#19981;&#23436;&#32654;&#65292;&#32463;&#24120;&#20135;&#29983;&#21547;&#26377;&#38169;&#35823;&#25110;&#38169;&#35823;&#20449;&#24687;&#30340;&#22238;&#31572;&#12290;&#36825;&#20123;&#19981;&#20934;&#30830;&#24615;&#65292;&#36890;&#24120;&#31216;&#20026;&#24187;&#35273;&#65292;&#20351;&#24471;LLMs&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#19981;&#21487;&#38752;&#29978;&#33267;&#19981;&#21487;&#29992;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#22312;LLMs&#20013;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38382;&#31572;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#25298;&#32477;&#26426;&#21046;&#65292;&#25351;&#23548;LLMs&#25298;&#32477;&#22238;&#31572;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20197;&#36991;&#20813;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;Learn to Refuse (L2R)&#65292;&#23427;&#23558;&#25298;&#32477;&#26426;&#21046;&#32435;&#20837;&#21040;LLMs&#20013;&#65292;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#37027;&#20123;&#23427;&#20204;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#24211;&#26469;&#34920;&#31034;&#25152;&#26377;LLMs&#25152;&#38656;&#35201;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM
&lt;/p&gt;</description></item><item><title>ATHENA&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24605;&#32500;&#25193;&#23637;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#30340;&#24605;&#32500;&#25193;&#23637;&#26426;&#21046;&#26469;&#35299;&#20915;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#25361;&#25112;&#65292;&#23427;&#33021;&#22815;&#20135;&#29983;&#21512;&#29702;&#30340;&#24605;&#32771;&#36335;&#24452;&#20197;&#35299;&#20915;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01036</link><description>&lt;p&gt;
ATHENA: &#25968;&#23398;&#25512;&#29702;&#19982;&#24605;&#32500;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
ATHENA: Mathematical Reasoning with Thought Expansion. (arXiv:2311.01036v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01036
&lt;/p&gt;
&lt;p&gt;
ATHENA&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24605;&#32500;&#25193;&#23637;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#30340;&#24605;&#32500;&#25193;&#23637;&#26426;&#21046;&#26469;&#35299;&#20915;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#25361;&#25112;&#65292;&#23427;&#33021;&#22815;&#20135;&#29983;&#21512;&#29702;&#30340;&#24605;&#32771;&#36335;&#24452;&#20197;&#35299;&#20915;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#21462;&#20915;&#20110;&#22914;&#20309;&#34920;&#36798;&#38382;&#39064;&#65292;&#20197;&#21450;&#27169;&#22411;&#22914;&#20309;&#30475;&#24453;&#20154;&#31867;&#35821;&#35328;&#34920;&#36798;&#30340;&#35282;&#24230;&#12290;&#23454;&#38469;&#19990;&#30028;&#30340;&#24773;&#22659;&#26356;&#20381;&#36182;&#36825;&#31181;&#26041;&#27861;&#65292;&#22240;&#20026;&#21516;&#26679;&#30340;&#25968;&#23398;&#36816;&#31639;&#26377;&#22810;&#31181;&#23454;&#36341;&#26041;&#24335;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#36807;&#38480;&#21046;&#39044;&#27979;&#31574;&#30053;&#26469;&#38480;&#21046;&#21487;&#29992;&#30340;&#24605;&#32500;&#36807;&#31243;&#65292;&#32780;&#24573;&#30053;&#20102;&#36825;&#20123;&#31574;&#30053;&#22312;&#33719;&#21462;&#25968;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24605;&#32500;&#25193;&#23637;&#32593;&#32476;&#26550;&#26500; (ATHENA) &#26469;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25361;&#25112;&#65292;&#27169;&#20223;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#30340;&#24418;&#24335;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24605;&#32500;&#25193;&#23637;&#26426;&#21046;&#12290;&#24605;&#32500;&#25193;&#23637;&#36890;&#36807;&#36882;&#24402;&#29983;&#25104;&#20505;&#36873;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#20174;&#19978;&#19968;&#27493;&#39537;&#21160;&#24182;&#36873;&#25321;&#36890;&#21521;&#30446;&#26631;&#30340;&#26377;&#25928;&#36335;&#24452;&#65292;&#20135;&#29983;&#21512;&#29702;&#30340;&#24605;&#32500;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ATHENA &#22312;&#21508;&#31181;&#38382;&#39064;&#20013;&#37117;&#21462;&#24471;&#20102;&#20840;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#21363;&#20351;&#22312;&#20449;&#24687;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#32473;&#20986;&#20196;&#20154;&#20449;&#26381;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving math word problems depends on how to articulate the problems, the lens through which models view human linguistic expressions. Real-world settings count on such a method even more due to the diverse practices of the same mathematical operations. Earlier works constrain available thinking processes by limited prediction strategies without considering their significance in acquiring mathematical knowledge. We introduce Attention-based THought Expansion Network Architecture (ATHENA) to tackle the challenges of real-world practices by mimicking human thought expansion mechanisms in the form of neural network propagation. A thought expansion recurrently generates the candidates carrying the thoughts of possible math expressions driven from the previous step and yields reasonable thoughts by selecting the valid pathways to the goal. Our experiments show that ATHENA achieves a new state-of-the-art stage toward the ideal model that is compelling in variant questions even when the infor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21253;&#21547;&#39640;&#26031;&#25513;&#30721;&#23618;&#21644;&#21327;&#26041;&#24046;&#33258;&#27880;&#24847;&#23618;&#30340;&#23616;&#37096;&#32534;&#30721;&#22120;&#65292;&#22312;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#23616;&#37096;&#19978;&#19979;&#25991;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01030</link><description>&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Local and Global Features for Aspect-based Sentiment Classification. (arXiv:2311.01030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21253;&#21547;&#39640;&#26031;&#25513;&#30721;&#23618;&#21644;&#21327;&#26041;&#24046;&#33258;&#27880;&#24847;&#23618;&#30340;&#23616;&#37096;&#32534;&#30721;&#22120;&#65292;&#22312;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#23616;&#37096;&#19978;&#19979;&#25991;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#26088;&#22312;&#21028;&#26029;&#21477;&#23376;&#20013;&#32473;&#23450;&#26041;&#38754;&#26415;&#35821;&#25152;&#20256;&#36798;&#30340;&#24773;&#24863;&#26497;&#24615;&#12290;&#24773;&#24863;&#26497;&#24615;&#19981;&#20165;&#30001;&#23616;&#37096;&#19978;&#19979;&#25991;&#20915;&#23450;&#65292;&#36824;&#19982;&#36828;&#31163;&#32473;&#23450;&#26041;&#38754;&#26415;&#35821;&#30340;&#35789;&#27719;&#30456;&#20851;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26080;&#27861;&#36275;&#22815;&#22320;&#21306;&#20998;&#24212;&#35813;&#26356;&#20851;&#27880;&#21738;&#20123;&#35789;&#35821;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#27491;&#22312;&#36827;&#20837;&#22522;&#20110;&#26041;&#21521;&#30340;&#24773;&#24863;&#20998;&#31867;&#20197;&#32534;&#30721;&#21477;&#27861;&#20381;&#36182;&#26641;&#20449;&#24687;&#12290;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21477;&#27861;&#20381;&#36182;&#26641;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#35270;&#20102;&#23558;&#20381;&#36182;&#20851;&#31995;&#26631;&#31614;&#20449;&#24687;&#26377;&#25928;&#22320;&#25972;&#21512;&#21040;&#34920;&#31034;&#23398;&#20064;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#25928;&#22320;&#24314;&#27169;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21253;&#21547;&#39640;&#26031;&#25513;&#30721;&#23618;&#21644;&#21327;&#26041;&#24046;&#33258;&#27880;&#24847;&#23618;&#30340;&#23616;&#37096;&#32534;&#30721;&#22120;&#12290;&#39640;&#26031;&#25513;&#30721;&#23618;&#20542;&#21521;&#20110;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#21608;&#22260;&#26041;&#38754;&#26415;&#35821;&#30340;&#24863;&#21463;&#37326;&#65292;&#20197;&#20351;&#20854;&#19981;&#37325;&#35201;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment classification (ASC) aims to judge the sentiment polarity conveyed by the given aspect term in a sentence. The sentiment polarity is not only determined by the local context but also related to the words far away from the given aspect term. Most recent efforts related to the attention-based models can not sufficiently distinguish which words they should pay more attention to in some cases. Meanwhile, graph-based models are coming into ASC to encode syntactic dependency tree information. But these models do not fully leverage syntactic dependency trees as they neglect to incorporate dependency relation tag information into representation learning effectively. In this paper, we address these problems by effectively modeling the local and global features. Firstly, we design a local encoder containing: a Gaussian mask layer and a covariance self-attention layer. The Gaussian mask layer tends to adjust the receptive field around aspect terms adaptively to deemphasize 
&lt;/p&gt;</description></item><item><title>COPAL-ID&#26159;&#19968;&#20010;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#19982;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#23427;&#34701;&#20837;&#20102;&#21360;&#23612;&#26412;&#22303;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#65292;&#25552;&#20379;&#20102;&#26356;&#33258;&#28982;&#30340;&#26085;&#24120;&#22240;&#26524;&#25512;&#29702;&#25551;&#32472;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#20010;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#20294;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#26368;&#26032;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;COPAL-ID&#19978;&#30340;&#20934;&#30830;&#29575;&#36739;&#20302;&#65292;&#20165;&#20026;65.47%&#12290;</title><link>http://arxiv.org/abs/2311.01012</link><description>&lt;p&gt;
COPAL-ID: &#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#25512;&#29702;&#19982;&#26412;&#22303;&#25991;&#21270;&#21644;&#32454;&#24494;&#24046;&#21035;
&lt;/p&gt;
&lt;p&gt;
COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances. (arXiv:2311.01012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01012
&lt;/p&gt;
&lt;p&gt;
COPAL-ID&#26159;&#19968;&#20010;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#19982;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#23427;&#34701;&#20837;&#20102;&#21360;&#23612;&#26412;&#22303;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#65292;&#25552;&#20379;&#20102;&#26356;&#33258;&#28982;&#30340;&#26085;&#24120;&#22240;&#26524;&#25512;&#29702;&#25551;&#32472;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#20010;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#20294;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#26368;&#26032;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;COPAL-ID&#19978;&#30340;&#20934;&#30830;&#29575;&#36739;&#20302;&#65292;&#20165;&#20026;65.47%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;COPAL-ID&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#12290;&#19982;&#20197;&#21069;&#30340;&#21360;&#23612;COPA&#25968;&#25454;&#38598;&#65288;XCOPA-ID&#65289;&#19981;&#21516;&#65292;COPAL-ID&#34701;&#20837;&#20102;&#21360;&#23612;&#26412;&#22303;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#65292;&#22240;&#27492;&#22312;&#21360;&#23612;&#25991;&#21270;&#39046;&#22495;&#20869;&#25552;&#20379;&#20102;&#26356;&#33258;&#28982;&#30340;&#26085;&#24120;&#22240;&#26524;&#25512;&#29702;&#25551;&#32472;&#12290;COPAL-ID&#30001;&#26412;&#22303;&#20154;&#20174;&#22836;&#24320;&#22987;&#19987;&#19994;&#25776;&#20889;&#65292;&#26356;&#27969;&#21033;&#65292;&#19981;&#20687;XCOPA-ID&#30340;&#32763;&#35793;&#23384;&#22312;&#23604;&#23596;&#30340;&#35789;&#35821;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20197;&#26631;&#20934;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#21644;&#38597;&#21152;&#36798;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#65288;&#19968;&#31181;&#22312;&#26085;&#24120;&#23545;&#35805;&#20013;&#24120;&#29992;&#30340;&#26041;&#35328;&#65289;&#21576;&#29616;COPAL-ID&#12290;COPAL-ID&#23545;&#20110;&#29616;&#26377;&#30340;&#24320;&#28304;&#21644;&#38381;&#28304;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#21364;&#26159;&#38750;&#24120;&#23481;&#26131;&#30340;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#24403;&#21069;&#26368;&#22909;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#20063;&#24456;&#38590;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;COPAL-ID&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;65.47%&#65292;&#36828;&#20302;&#20110;&#27809;&#26377;&#25991;&#21270;&#32972;&#26223;&#30340;XCOPA-ID&#65288;79.40%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present publicly available COPAL-ID, a novel Indonesian language common sense reasoning dataset. Unlike the previous Indonesian COPA dataset (XCOPA-ID), COPAL-ID incorporates Indonesian local and cultural nuances, and therefore, provides a more natural portrayal of day-to-day causal reasoning within the Indonesian cultural sphere. Professionally written by natives from scratch, COPAL-ID is more fluent and free from awkward phrases, unlike the translated XCOPA-ID. In addition, we present COPAL-ID in both standard Indonesian and in Jakartan Indonesian--a dialect commonly used in daily conversation. COPAL-ID poses a greater challenge for existing open-sourced and closed state-of-the-art multilingual language models, yet is trivially easy for humans. Our findings suggest that even the current best open-source, multilingual model struggles to perform well, achieving 65.47% accuracy on COPAL-ID, significantly lower than on the culturally-devoid XCOPA-ID (79.40%). Despite GPT-4's impressiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;&#20302;&#36164;&#28304;&#26412;&#22320;&#35821;&#35328;&#19978;&#35757;&#32451;NMT&#31995;&#32479;&#30340;&#32508;&#21512;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290; &#23613;&#31649;&#35745;&#31639;&#36164;&#28304;&#21644;&#25991;&#26412;&#25968;&#25454;&#26377;&#38480;&#65292;&#20294;&#25105;&#20204;&#30340;&#20960;&#20010;NMT&#31995;&#32479;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#24615;&#33021;&#65292;&#19982;&#38646;-shot gpt-3.5-turbo&#30340;&#32763;&#35793;&#36136;&#37327;&#30456;&#23218;&#32654;&#12290; &#36825;&#20123;&#21457;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;NMT&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2311.00998</link><description>&lt;p&gt;
&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;&#20302;&#36164;&#28304;&#26412;&#22320;&#35821;&#35328;&#19978;&#21487;&#22797;&#21046;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Replicable Benchmarking of Neural Machine Translation (NMT) on Low-Resource Local Languages in Indonesia. (arXiv:2311.00998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;&#20302;&#36164;&#28304;&#26412;&#22320;&#35821;&#35328;&#19978;&#35757;&#32451;NMT&#31995;&#32479;&#30340;&#32508;&#21512;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290; &#23613;&#31649;&#35745;&#31639;&#36164;&#28304;&#21644;&#25991;&#26412;&#25968;&#25454;&#26377;&#38480;&#65292;&#20294;&#25105;&#20204;&#30340;&#20960;&#20010;NMT&#31995;&#32479;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#24615;&#33021;&#65292;&#19982;&#38646;-shot gpt-3.5-turbo&#30340;&#32763;&#35793;&#36136;&#37327;&#30456;&#23218;&#32654;&#12290; &#36825;&#20123;&#21457;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;NMT&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#20302;&#36164;&#28304;&#26412;&#22320;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#38656;&#35201;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#21644;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#20026;&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;&#22235;&#31181;&#20302;&#36164;&#28304;&#26412;&#22320;&#35821;&#35328;&#65288;&#29226;&#21703;&#35821;&#12289;&#33487;&#20025;&#23612;&#26031;&#35821;&#12289;&#31859;&#21335;&#21345;&#21338;&#21644;&#24052;&#21400;&#35821;&#65289;&#35757;&#32451;NMT&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#21508;&#31181;&#35757;&#32451;&#26041;&#27861;&#12289;&#33539;&#20363;&#12289;&#25968;&#25454;&#35268;&#27169;&#20197;&#21450;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#29983;&#25104;&#30340;&#21021;&#27493;&#30740;&#31350;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#32763;&#35793;&#23454;&#29992;&#31574;&#30053;&#30340;&#20855;&#20307;&#36235;&#21183;&#21644;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#35745;&#31639;&#36164;&#28304;&#21644;&#25991;&#26412;&#25968;&#25454;&#26377;&#38480;&#65292;&#25105;&#20204;&#30340;&#20960;&#20010;NMT&#31995;&#32479;&#22312;&#31454;&#20105;&#24615;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#19982;&#38646;-shot gpt-3.5-turbo&#30340;&#32763;&#35793;&#36136;&#37327;&#30456;&#23218;&#32654;&#12290;&#36825;&#20123;&#21457;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;NMT&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation (NMT) for low-resource local languages in Indonesia faces significant challenges, including the need for a representative benchmark and limited data availability. This work addresses these challenges by comprehensively analyzing training NMT systems for four low-resource local languages in Indonesia: Javanese, Sundanese, Minangkabau, and Balinese. Our study encompasses various training approaches, paradigms, data sizes, and a preliminary study into using large language models for synthetic low-resource languages parallel data generation. We reveal specific trends and insights into practical strategies for low-resource language translation. Our research demonstrates that despite limited computational resources and textual data, several of our NMT systems achieve competitive performances, rivaling the translation quality of zero-shot gpt-3.5-turbo. These findings significantly advance NMT for low-resource languages, offering valuable guidance for researchers in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-Language Interpreter&#65288;ViLaIn&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26426;&#22120;&#20154;&#20219;&#21153;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#31526;&#21495;&#35268;&#21010;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ViLaIn&#21644;&#31526;&#21495;&#35268;&#21010;&#22120;&#33021;&#22815;&#20934;&#30830;&#29983;&#25104;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2311.00967</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#35270;&#35273;&#35821;&#35328;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Interpreter for Robot Task Planning. (arXiv:2311.00967v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-Language Interpreter&#65288;ViLaIn&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26426;&#22120;&#20154;&#20219;&#21153;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#31526;&#21495;&#35268;&#21010;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ViLaIn&#21644;&#31526;&#21495;&#35268;&#21010;&#22120;&#33021;&#22815;&#20934;&#30830;&#29983;&#25104;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#21152;&#36895;&#35821;&#35328;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#35268;&#21010;&#22120;&#30340;&#21457;&#23637;&#12290;&#21516;&#26102;&#65292;&#31526;&#21495;&#35268;&#21010;&#22120;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#23558;&#36825;&#20004;&#31181;&#36235;&#21183;&#30456;&#32467;&#21512;&#65292;&#21363;&#22810;&#27169;&#24577;&#35268;&#21010;&#38382;&#39064;&#35268;&#33539;&#12290;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#38382;&#39064;&#25551;&#36848;&#65288;PD&#65289;&#65292;&#36825;&#26159;&#35268;&#21010;&#22120;&#29992;&#26469;&#26597;&#25214;&#35745;&#21010;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#20214;&#12290;&#36890;&#36807;&#20174;&#35821;&#35328;&#25351;&#20196;&#21644;&#22330;&#26223;&#35266;&#27979;&#20013;&#29983;&#25104;PD&#65292;&#25105;&#20204;&#21487;&#20197;&#39537;&#21160;&#31526;&#21495;&#35268;&#21010;&#22120;&#22312;&#35821;&#35328;&#24341;&#23548;&#26694;&#26550;&#19979;&#24037;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-Language Interpreter&#65288;ViLaIn&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20808;&#36827;&#30340;LLM&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;PD&#12290;ViLaIn&#21487;&#20197;&#36890;&#36807;&#31526;&#21495;&#35268;&#21010;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#21453;&#39304;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;PD&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65306;ViLaIn&#21644;&#31526;&#21495;&#35268;&#21010;&#22120;&#33021;&#22815;&#20934;&#30830;&#22320;&#29983;&#25104;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#21527;&#65311;&#20026;&#20102;&#35780;&#20272;ViLaIn&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#38382;&#39064;&#25551;&#36848;&#29983;&#25104;&#65288;ProDG&#65289;&#25968;&#25454;&#38598;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#23558;&#22312;&#35780;&#20272;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are accelerating the development of language-guided robot planners. Meanwhile, symbolic planners offer the advantage of interpretability. This paper proposes a new task that bridges these two trends, namely, multimodal planning problem specification. The aim is to generate a problem description (PD), a machine-readable file used by the planners to find a plan. By generating PDs from language instruction and scene observation, we can drive symbolic planners in a language-guided framework. We propose a Vision-Language Interpreter (ViLaIn), a new framework that generates PDs using state-of-the-art LLM and vision-language models. ViLaIn can refine generated PDs via error message feedback from the symbolic planner. Our aim is to answer the question: How accurately can ViLaIn and the symbolic planner generate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset called the problem description generation (ProDG) dataset. The framework is evaluated wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;IndoToD&#65292;&#19968;&#20010;&#29992;&#20110;&#21360;&#23612;&#35821;&#30340;&#31471;&#21040;&#31471;&#22810;&#39046;&#22495;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22522;&#20934;&#12290;&#36890;&#36807;&#23558;&#33521;&#35821;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#21360;&#23612;&#35821;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#36825;&#20010;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#38599;&#20323;&#27597;&#35821;&#20026;&#21360;&#23612;&#35821;&#30340;&#20154;&#21592;&#36827;&#34892;&#32763;&#35793;&#21644;&#25968;&#25454;&#25910;&#38598;&#65292;&#36825;&#20026;&#35780;&#20272;&#21360;&#23612;&#35821;&#21644;&#33521;&#35821;&#23545;&#35805;&#31995;&#32479;&#20197;&#21450;&#36328;&#35821;&#35328;&#21644;&#21452;&#35821;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2311.00958</link><description>&lt;p&gt;
IndoToD: &#19968;&#20010;&#29992;&#20110;&#22810;&#39046;&#22495;&#21360;&#23612;&#35821;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
IndoToD: A Multi-Domain Indonesian Benchmark For End-to-End Task-Oriented Dialogue Systems. (arXiv:2311.00958v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;IndoToD&#65292;&#19968;&#20010;&#29992;&#20110;&#21360;&#23612;&#35821;&#30340;&#31471;&#21040;&#31471;&#22810;&#39046;&#22495;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22522;&#20934;&#12290;&#36890;&#36807;&#23558;&#33521;&#35821;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#21360;&#23612;&#35821;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#36825;&#20010;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#38599;&#20323;&#27597;&#35821;&#20026;&#21360;&#23612;&#35821;&#30340;&#20154;&#21592;&#36827;&#34892;&#32763;&#35793;&#21644;&#25968;&#25454;&#25910;&#38598;&#65292;&#36825;&#20026;&#35780;&#20272;&#21360;&#23612;&#35821;&#21644;&#33521;&#35821;&#23545;&#35805;&#31995;&#32479;&#20197;&#21450;&#36328;&#35821;&#35328;&#21644;&#21452;&#35821;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#21482;&#26159;&#20026;&#39640;&#36164;&#28304;&#35821;&#35328;&#22914;&#33521;&#35821;&#21644;&#27721;&#35821;&#21019;&#24314;&#30340;&#65292;&#28982;&#32780;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#20854;&#20182;&#21306;&#22495;&#25110;&#26412;&#22320;&#35821;&#35328;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#20197;&#25193;&#23637;&#23427;&#20204;&#29702;&#35299;&#19981;&#21516;&#35821;&#35328;&#23545;&#35805;&#32972;&#26223;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IndoToD&#65292;&#19968;&#20010;&#21360;&#23612;&#35821;&#31471;&#21040;&#31471;&#22810;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35789;&#27861;&#20998;&#26512;&#23558;&#20004;&#20010;&#33521;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;&#25193;&#23637;&#21040;&#21360;&#23612;&#35821;&#65292;&#21253;&#25324;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#65292;&#20197;&#39640;&#25928;&#22320;&#20943;&#23569;&#27880;&#37322;&#30340;&#22823;&#23567;&#12290;&#20026;&#20102;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#25105;&#20204;&#38599;&#29992;&#27597;&#35821;&#20026;&#21360;&#23612;&#35821;&#30340;&#20154;&#25163;&#21160;&#32763;&#35793;&#23545;&#35805;&#12290;&#38500;&#20102;&#21407;&#22987;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#22806;&#65292;&#36825;&#20123;&#26032;&#30340;&#21360;&#23612;&#35821;&#25968;&#25454;&#38598;&#21487;&#20197;&#20316;&#20026;&#35780;&#20272;&#21360;&#23612;&#35821;&#21644;&#33521;&#35821;&#23545;&#35805;&#31995;&#32479;&#20197;&#21450;&#25506;&#32034;&#36328;&#35821;&#35328;&#21644;&#21452;&#35821;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#28508;&#22312;&#30410;&#22788;&#30340;&#26377;&#25928;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue (ToD) systems have been mostly created for high-resource languages, such as English and Chinese. However, there is a need to develop ToD systems for other regional or local languages to broaden their ability to comprehend the dialogue contexts in various languages. This paper introduces IndoToD, an end-to-end multi domain ToD benchmark in Indonesian. We extend two English ToD datasets to Indonesian, comprising four different domains by delexicalization to efficiently reduce the size of annotations. To ensure a high-quality data collection, we hire native speakers to manually translate the dialogues. Along with the original English datasets, these new Indonesian datasets serve as an effective benchmark for evaluating Indonesian and English ToD systems as well as exploring the potential benefits of cross-lingual and bilingual transfer learning approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;&#24544;&#23454;&#24230;&#25351;&#26631;&#30340;&#26032;&#22870;&#21169;&#20989;&#25968;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#24187;&#35937;&#21644;&#30693;&#35782;&#25991;&#26412;&#22810;&#20313;&#20449;&#24687;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#24179;&#34913;&#30340;&#29983;&#25104;&#23545;&#35805;&#22238;&#24212;&#36136;&#37327;&#35780;&#21028;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.00953</link><description>&lt;p&gt;
&#36890;&#36807;&#23569;&#37327;&#19987;&#23478;&#28436;&#31034;&#34701;&#21512;&#22870;&#21169;&#20989;&#25968;&#65292;&#29992;&#20110;&#24544;&#23454;&#20934;&#30830;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Blending Reward Functions via Few Expert Demonstrations for Faithful and Accurate Knowledge-Grounded Dialogue Generation. (arXiv:2311.00953v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;&#24544;&#23454;&#24230;&#25351;&#26631;&#30340;&#26032;&#22870;&#21169;&#20989;&#25968;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#24187;&#35937;&#21644;&#30693;&#35782;&#25991;&#26412;&#22810;&#20313;&#20449;&#24687;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#24179;&#34913;&#30340;&#29983;&#25104;&#23545;&#35805;&#22238;&#24212;&#36136;&#37327;&#35780;&#21028;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;&#23545;&#35805;&#20449;&#24687;&#23547;&#27714;&#31995;&#32479;&#38656;&#35201;&#33021;&#22815;&#22522;&#20110;&#30456;&#20851;&#30693;&#35782;&#25991;&#26412;&#29983;&#25104;&#24544;&#23454;&#20934;&#30830;&#22238;&#24212;&#30340;&#23545;&#35805;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#20559;&#35265;&#32780;&#20135;&#29983;&#24187;&#35273;&#12290;&#20854;&#27425;&#65292;&#30693;&#35782;&#25991;&#26412;&#36890;&#24120;&#21253;&#21547;&#22810;&#20313;&#21644;&#19981;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#36825;&#20250;&#20998;&#25955;&#27169;&#22411;&#23545;&#30456;&#20851;&#25991;&#26412;&#33539;&#22260;&#30340;&#27880;&#24847;&#21147;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#39069;&#22806;&#30340;&#25968;&#25454;&#27880;&#37322;&#22312;&#30693;&#35782;&#25991;&#26412;&#19978;&#23398;&#20064;&#30693;&#35782;&#35782;&#21035;&#27169;&#22359;&#65292;&#20197;&#32469;&#36807;&#19981;&#30456;&#20851;&#20449;&#24687;&#65292;&#20294;&#26159;&#25910;&#38598;&#36825;&#26679;&#30340;&#39640;&#36136;&#37327;&#33539;&#22260;&#27880;&#37322;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#20811;&#26381;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22870;&#21169;&#20989;&#25968;&#23558;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;&#24544;&#23454;&#24230;&#25351;&#26631;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20379;&#19968;&#20010;&#24179;&#34913;&#30340;&#29983;&#25104;&#22238;&#24212;&#36136;&#37327;&#35780;&#21028;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21327;&#21516;&#21442;&#32771;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of trustworthy conversational information-seeking systems relies on dialogue models that can generate faithful and accurate responses based on relevant knowledge texts. However, two main challenges hinder this task. Firstly, language models may generate hallucinations due to data biases present in their pretraining corpus. Secondly, knowledge texts often contain redundant and irrelevant information that distracts the model's attention from the relevant text span. Previous works use additional data annotations on the knowledge texts to learn a knowledge identification module in order to bypass irrelevant information, but collecting such high-quality span annotations can be costly. In this work, we leverage reinforcement learning algorithms to overcome the above challenges by introducing a novel reward function. Our reward function combines an accuracy metric and a faithfulness metric to provide a balanced quality judgment of generated responses, which can be used as a co
&lt;/p&gt;</description></item><item><title>E3 TTS&#26159;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#20013;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#24314;&#27169;&#27874;&#24418;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#33021;&#22815;&#36731;&#26494;&#36866;&#24212;&#38646;&#26679;&#26412;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2311.00945</link><description>&lt;p&gt;
E3 TTS: &#31616;&#21333;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
E3 TTS: Easy End-to-End Diffusion-based Text to Speech. (arXiv:2311.00945v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00945
&lt;/p&gt;
&lt;p&gt;
E3 TTS&#26159;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#20013;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#24314;&#27169;&#27874;&#24418;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#33021;&#22815;&#36731;&#26494;&#36866;&#24212;&#38646;&#26679;&#26412;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#65292;&#31216;&#20026;E3 TTS&#12290;E3 TTS&#30452;&#25509;&#25509;&#21463;&#32431;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#32454;&#21270;&#36807;&#31243;&#29983;&#25104;&#38899;&#39057;&#27874;&#24418;&#12290;&#19982;&#35768;&#22810;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;E3 TTS&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20013;&#38388;&#34920;&#31034;&#65292;&#22914;&#39057;&#35889;&#29305;&#24449;&#25110;&#23545;&#40784;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;E3 TTS&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#24314;&#27169;&#27874;&#24418;&#30340;&#26102;&#38388;&#32467;&#26500;&#12290;&#19981;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#26465;&#20214;&#20449;&#24687;&#65292;E3 TTS&#21487;&#20197;&#25903;&#25345;&#32473;&#23450;&#38899;&#39057;&#20013;&#30340;&#28789;&#27963;&#28508;&#22312;&#32467;&#26500;&#12290;&#36825;&#20351;&#24471;E3 TTS&#33021;&#22815;&#36731;&#26494;&#36866;&#24212;&#38646;&#26679;&#26412;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32534;&#36753;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;E3 TTS&#33021;&#22815;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#38899;&#39057;&#65292;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;TTS&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#38899;&#39057;&#26679;&#26412;&#21487;&#22312;https://e3tts.github.io&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Easy End-to-End Diffusion-based Text to Speech, a simple and efficient end-to-end text-to-speech model based on diffusion. E3 TTS directly takes plain text as input and generates an audio waveform through an iterative refinement process. Unlike many prior work, E3 TTS does not rely on any intermediate representations like spectrogram features or alignment information. Instead, E3 TTS models the temporal structure of the waveform through the diffusion process. Without relying on additional conditioning information, E3 TTS could support flexible latent structure within the given audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as editing without any additional training. Experiments show that E3 TTS can generate high-fidelity audio, approaching the performance of a state-of-the-art neural TTS system. Audio samples are available at https://e3tts.github.io.
&lt;/p&gt;</description></item><item><title>HyperLoRA&#26159;&#19968;&#31181;&#26080;&#20219;&#21153;&#30693;&#35782;&#30340;&#20302;&#31209;&#36866;&#37197;&#22120;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#35821;&#35328;&#30693;&#35782;&#36890;&#36807;&#36229;&#32593;&#32476;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#36866;&#24212;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#26410;&#30693;&#33521;&#35821;&#26041;&#35328;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00915</link><description>&lt;p&gt;
&#26080;&#20219;&#21153;&#30693;&#35782;&#30340;&#20302;&#31209;&#36866;&#37197;&#22120;&#29992;&#20110;&#26410;&#30693;&#30340;&#33521;&#35821;&#26041;&#35328;
&lt;/p&gt;
&lt;p&gt;
Task-Agnostic Low-Rank Adapters for Unseen English Dialects. (arXiv:2311.00915v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00915
&lt;/p&gt;
&lt;p&gt;
HyperLoRA&#26159;&#19968;&#31181;&#26080;&#20219;&#21153;&#30693;&#35782;&#30340;&#20302;&#31209;&#36866;&#37197;&#22120;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#35821;&#35328;&#30693;&#35782;&#36890;&#36807;&#36229;&#32593;&#32476;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#36866;&#24212;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#26410;&#30693;&#33521;&#35821;&#26041;&#35328;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#22522;&#20110;&#20559;&#21521;&#20110;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#20854;&#20182;&#26041;&#35328;&#30340;&#20154;&#22312;&#19982;&#36825;&#20123;&#25216;&#26415;&#20132;&#20114;&#26102;&#20250;&#36935;&#21040;&#26356;&#22810;&#30340;&#25925;&#38556;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#35762;&#35805;&#32773;&#36890;&#24120;&#20250;&#36866;&#24212;&#33258;&#24049;&#30340;&#35328;&#35821;&#20197;&#20415;&#34987;&#26356;&#22909;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35748;&#20026;&#35821;&#35328;&#25216;&#26415;&#24212;&#35813;&#34987;&#35774;&#35745;&#20026;&#36866;&#24212;&#33521;&#35821;&#26041;&#35328;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;&#19981;&#26159;&#21453;&#20854;&#36947;&#32780;&#34892;&#20043;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#20851;&#20110;&#26041;&#35328;&#30340;&#24037;&#20316;&#22312;&#25193;&#23637;&#21644;&#26032;&#20986;&#29616;&#30340;&#26041;&#35328;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;HyperLoRA&#21033;&#29992;&#19987;&#23478;&#35821;&#35328;&#30693;&#35782;&#36890;&#36807;&#36229;&#32593;&#32476;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#35299;&#24320;&#26041;&#35328;&#29305;&#23450;&#21644;&#36328;&#26041;&#35328;&#30340;&#20449;&#24687;&#65292;HyperLoRA&#20197;&#26080;&#20219;&#21153;&#30340;&#26041;&#24335;&#25552;&#39640;&#23545;&#26410;&#30693;&#26041;&#35328;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;HyperLoRA&#19981;&#20165;&#22312;&#21442;&#25968;&#25968;&#37327;&#19978;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;&#19988;&#22312;&#24615;&#33021;&#19978;&#26159;&#26368;&#20339;&#25110;&#26368;&#20855;&#31454;&#20105;&#21147;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are trained on corpora disproportionally weighted in favor of Standard American English. As a result, speakers of other dialects experience significantly more failures when interacting with these technologies. In practice, these speakers often accommodate their speech to be better understood. Our work shares the belief that language technologies should be designed to accommodate the diversity in English dialects and not the other way around. However, prior works on dialect struggle with generalizing to evolving and emerging dialects in a scalable manner. To fill this gap, our method, HyperLoRA, leverages expert linguistic knowledge to enable resource-efficient adaptation via hypernetworks. By disentangling dialect-specific and cross-dialectal information, HyperLoRA improves generalization to unseen dialects in a task-agnostic fashion. Not only is HyperLoRA more scalable in the number of parameters, but it also achieves the best or most competitive performan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRESENCE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#20027;&#24433;&#21709;&#20998;&#25968;&#20316;&#20026;&#37325;&#35201;&#24615;&#25351;&#26631;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#37325;&#21152;&#26435;&#65292;&#20419;&#36827;&#20102;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#26032;&#39062;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00913</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#20027;&#24433;&#21709;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#25968;&#25454;&#37325;&#21152;&#26435;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Influence Guided Data Reweighting for Language Model Pre-training. (arXiv:2311.00913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00913
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRESENCE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#20027;&#24433;&#21709;&#20998;&#25968;&#20316;&#20026;&#37325;&#35201;&#24615;&#25351;&#26631;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#37325;&#21152;&#26435;&#65292;&#20419;&#36827;&#20102;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#26032;&#39062;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#24320;&#21457;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#27169;&#22411;&#30340;&#40664;&#35748;&#36215;&#28857;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#34987;&#32452;&#35013;&#22909;&#65292;&#22312;LM&#39044;&#35757;&#32451;&#26399;&#38388;&#65292;&#25152;&#26377;&#25968;&#25454;&#26679;&#26412;&#37117;&#34987;&#35270;&#20026;&#20855;&#26377;&#30456;&#31561;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#21644;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#65292;&#23545;&#25152;&#26377;&#25968;&#25454;&#26679;&#26412;&#32473;&#20104;&#30456;&#31561;&#30340;&#37325;&#35201;&#24615;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#36873;&#25321;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20010;&#37325;&#35201;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRESENCE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#20027;&#24433;&#21709;&#65288;SI&#65289;&#20998;&#25968;&#20316;&#20026;&#26679;&#26412;&#37325;&#35201;&#24615;&#21644;&#39044;&#35757;&#32451;&#30340;&#25351;&#26631;&#65292;&#20849;&#21516;&#23545;&#26679;&#26412;&#36827;&#34892;&#37325;&#21152;&#26435;&#12290;PRESENCE&#20419;&#36827;&#20102;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#26032;&#39062;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#28085;&#30422;&#22810;&#20010;&#27169;&#22411;&#35268;&#27169;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;PRESENCE&#25552;&#20986;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#39318;&#35201;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) pre-trained with self-supervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of task-specific supervised learning and LM fine-tuning, model-driven reweighting for pre-training data has not been explored. We fill this important gap and propose PRESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training. PRESENCE promotes novelty and stability for model pre-training. Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present PRESENCE as an important first step in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#26631;&#35760;&#20998;&#37197;&#21160;&#24577;&#24179;&#28369;&#30340;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.00906</link><description>&lt;p&gt;
&#37325;&#26032;&#21152;&#26435;&#26631;&#35760;&#65306;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#36866;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Re-weighting Tokens: A Simple and Effective Active Learning Strategy for Named Entity Recognition. (arXiv:2311.00906v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#26631;&#35760;&#20998;&#37197;&#21160;&#24577;&#24179;&#28369;&#30340;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#27880;&#37322;&#36164;&#28304;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39046;&#22495;&#65292;&#20027;&#21160;&#23398;&#20064;&#30456;&#23545;&#36739;&#23569;&#34987;&#20851;&#27880;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#22952;&#30861;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#22240;&#20026;&#24207;&#21015;&#26631;&#35760;&#27169;&#22411;&#32570;&#20047;&#36275;&#22815;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#26032;&#21152;&#26435;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20026;&#27599;&#20010;&#26631;&#35760;&#20998;&#37197;&#20102;&#21160;&#24577;&#24179;&#28369;&#30340;&#26435;&#37325;&#12290;&#36825;&#31181;&#36866;&#24212;&#24615;&#31574;&#30053;&#19982;&#21508;&#31181;&#26631;&#35760;&#32423;&#37319;&#38598;&#20989;&#25968;&#20860;&#23481;&#65292;&#24182;&#26377;&#21161;&#20110;&#24320;&#21457;&#20581;&#22766;&#30340;&#20027;&#21160;&#23398;&#20064;&#22120;&#12290;&#22312;&#22810;&#20010;&#35821;&#26009;&#24211;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#25105;&#20204;&#30340;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#19982;&#29616;&#26377;&#30340;&#37319;&#38598;&#20989;&#25968;&#32467;&#21512;&#36215;&#26469;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#39564;&#35777;&#20102;&#20854;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning, a widely adopted technique for enhancing machine learning models in text and image classification tasks with limited annotation resources, has received relatively little attention in the domain of Named Entity Recognition (NER). The challenge of data imbalance in NER has hindered the effectiveness of active learning, as sequence labellers lack sufficient learning signals. To address these challenges, this paper presents a novel reweighting-based active learning strategy that assigns dynamic smoothed weights to individual tokens. This adaptable strategy is compatible with various token-level acquisition functions and contributes to the development of robust active learners. Experimental results on multiple corpora demonstrate the substantial performance improvement achieved by incorporating our re-weighting strategy into existing acquisition functions, validating its practical efficacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26465;&#20214;&#38899;&#39057;&#29983;&#25104;&#20013;&#30340;&#24320;&#25918;&#25552;&#31034;&#25361;&#25112;&#65292;&#36890;&#36807;&#37325;&#26032;&#32534;&#20889;&#25552;&#31034;&#24182;&#21033;&#29992;&#25991;&#26412;-&#38899;&#39057;&#23545;&#40784;&#20316;&#20026;&#21453;&#39304;&#20449;&#21495;&#65292;&#20174;&#32780;&#25913;&#21892;&#38899;&#39057;&#36136;&#37327;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.00897</link><description>&lt;p&gt;
&#20851;&#20110;&#26465;&#20214;&#38899;&#39057;&#29983;&#25104;&#20013;&#30340;&#24320;&#25918;&#25552;&#31034;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
On The Open Prompt Challenge In Conditional Audio Generation. (arXiv:2311.00897v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26465;&#20214;&#38899;&#39057;&#29983;&#25104;&#20013;&#30340;&#24320;&#25918;&#25552;&#31034;&#25361;&#25112;&#65292;&#36890;&#36807;&#37325;&#26032;&#32534;&#20889;&#25552;&#31034;&#24182;&#21033;&#29992;&#25991;&#26412;-&#38899;&#39057;&#23545;&#40784;&#20316;&#20026;&#21453;&#39304;&#20449;&#21495;&#65292;&#20174;&#32780;&#25913;&#21892;&#38899;&#39057;&#36136;&#37327;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#65288;TTA&#65289;&#36890;&#36807;&#23398;&#20064;&#38899;&#39057;&#26679;&#26412;&#21644;&#25163;&#24037;&#27880;&#37322;&#30340;&#25991;&#26412;&#23545;&#26469;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#38899;&#39057;&#12290;&#28982;&#32780;&#65292;&#21830;&#19994;&#21270;&#38899;&#39057;&#29983;&#25104;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#29992;&#25143;&#36755;&#20837;&#30340;&#25552;&#31034;&#36890;&#24120;&#19982;&#35757;&#32451;TTA&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#25991;&#26412;&#25551;&#36848;&#30456;&#27604;&#32570;&#20047;&#20855;&#20307;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;TTA&#27169;&#22411;&#35270;&#20026;&#19968;&#20010;&#8220;&#40657;&#30418;&#23376;&#8221;&#65292;&#35299;&#20915;&#20102;&#29992;&#25143;&#25552;&#31034;&#25361;&#25112;&#30340;&#20004;&#20010;&#20851;&#38190;&#27934;&#23519;&#65306;&#65288;1&#65289;&#29992;&#25143;&#25552;&#31034;&#36890;&#24120;&#32570;&#20047;&#20855;&#20307;&#24615;&#65292;&#23548;&#33268;&#29992;&#25143;&#25552;&#31034;&#19982;&#35757;&#32451;&#25552;&#31034;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#23545;&#40784;&#24046;&#36317;&#12290;&#65288;2&#65289;&#23384;&#22312;&#19968;&#31181;&#38899;&#39057;&#25551;&#36848;&#20998;&#24067;&#65292;TTA&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#38899;&#39057;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;audionese&#8221;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#25351;&#23548;&#35843;&#25972;&#27169;&#22411;&#37325;&#20889;&#25552;&#31034;&#65292;&#24182;&#25552;&#20986;&#21033;&#29992;&#25991;&#26412;-&#38899;&#39057;&#23545;&#40784;&#20316;&#20026;&#21453;&#39304;&#20449;&#21495;&#65292;&#36890;&#36807;&#36793;&#30028;&#25490;&#24207;&#23398;&#20064;&#26469;&#25913;&#21892;&#38899;&#39057;&#36136;&#37327;&#12290;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#30340;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#25991;&#26412;-&#38899;&#39057;&#23545;&#40784;&#21644;&#38899;&#20048;&#38899;&#39057;&#36136;&#37327;&#30340;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-audio generation (TTA) produces audio from a text description, learning from pairs of audio samples and hand-annotated text. However, commercializing audio generation is challenging as user-input prompts are often under-specified when compared to text descriptions used to train TTA models. In this work, we treat TTA models as a ``blackbox'' and address the user prompt challenge with two key insights: (1) User prompts are generally under-specified, leading to a large alignment gap between user prompts and training prompts. (2) There is a distribution of audio descriptions for which TTA models are better at generating higher quality audio, which we refer to as ``audionese''. To this end, we rewrite prompts with instruction-tuned models and propose utilizing text-audio alignment as feedback signals via margin ranking learning for audio improvements. On both objective and subjective human evaluations, we observed marked improvements in both text-audio alignment and music audio qual
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#32534;&#36753;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#23383;&#24149;&#20316;&#20026;&#31034;&#20363;&#26469;&#25913;&#36827;&#26465;&#20214;&#38899;&#39057;&#29983;&#25104;&#30340;&#38899;&#39057;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.00895</link><description>&lt;p&gt;
&#26465;&#20214;&#38899;&#39057;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
In-Context Prompt Editing For Conditional Audio Generation. (arXiv:2311.00895v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#32534;&#36753;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#23383;&#24149;&#20316;&#20026;&#31034;&#20363;&#26469;&#25913;&#36827;&#26465;&#20214;&#38899;&#39057;&#29983;&#25104;&#30340;&#38899;&#39057;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;&#26159;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#19981;&#36866;&#24212;&#12290;&#36825;&#22312;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#20854;&#20013;&#32534;&#30721;&#34920;&#31034;&#24456;&#23481;&#26131;&#34987;&#19981;&#21487;&#35265;&#30340;&#25552;&#31034;&#30772;&#22351;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#38899;&#39057;&#36136;&#37327;&#19979;&#38477;&#12290;&#26377;&#38480;&#30340;&#25991;&#26412;-&#38899;&#39057;&#23545;&#38598;&#21512;&#20173;&#28982;&#19981;&#36275;&#20197;&#23454;&#29616;&#26465;&#20214;&#38899;&#39057;&#29983;&#25104;&#65292;&#22240;&#20026;&#29992;&#25143;&#30340;&#25552;&#31034;&#20449;&#24687;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19982;&#35757;&#32451;&#38598;&#20013;&#30340;&#25552;&#31034;&#30456;&#27604;&#65292;&#29983;&#25104;&#30340;&#38899;&#39057;&#26679;&#26412;&#20013;&#20855;&#26377;&#29992;&#25143;&#25552;&#31034;&#30340;&#38899;&#39057;&#36136;&#37327;&#19968;&#30452;&#23384;&#22312;&#19968;&#33268;&#30340;&#38477;&#20302;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#32534;&#36753;&#26694;&#26550;&#65292;&#21033;&#29992;&#35757;&#32451;&#23383;&#24149;&#20316;&#20026;&#31034;&#20363;&#26469;&#37325;&#26032;&#23457;&#35270;&#29992;&#25143;&#25552;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#22312;&#25910;&#38598;&#21040;&#30340;&#29992;&#25143;&#25552;&#31034;&#38598;&#21512;&#19978;&#30340;&#38899;&#39057;&#36136;&#37327;&#65292;&#36825;&#20123;&#25552;&#31034;&#26159;&#21442;&#32771;&#35757;&#32451;&#23383;&#24149;&#32534;&#36753;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional shift is a central challenge in the deployment of machine learning models as they can be ill-equipped for real-world data. This is particularly evident in text-to-audio generation where the encoded representations are easily undermined by unseen prompts, which leads to the degradation of generated audio -- the limited set of the text-audio pairs remains inadequate for conditional audio generation in the wild as user prompts are under-specified. In particular, we observe a consistent audio quality degradation in generated audio samples with user prompts, as opposed to training set prompts. To this end, we present a retrieval-based in-context prompt editing framework that leverages the training captions as demonstrative exemplars to revisit the user prompts. We show that the framework enhanced the audio quality across the set of collected user prompts, which were edited with reference to the training captions as exemplars.
&lt;/p&gt;</description></item><item><title>Transformer&#27169;&#22411;&#36890;&#36807;&#39044;&#35757;&#32451;&#25968;&#25454;&#28151;&#21512;&#23454;&#29616;&#20102;&#29421;&#31364;&#30340;&#27169;&#22411;&#36873;&#25321;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#20013;&#35782;&#21035;&#21644;&#23398;&#20064;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20294;&#23545;&#20110;&#20219;&#21153;&#25110;&#20989;&#25968;&#30340;&#22788;&#29702;&#30456;&#23545;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2311.00871</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#28151;&#21512;&#20351;&#24471;Transformer&#27169;&#22411;&#20855;&#22791;&#29421;&#31364;&#30340;&#27169;&#22411;&#36873;&#25321;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models. (arXiv:2311.00871v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00871
&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#36890;&#36807;&#39044;&#35757;&#32451;&#25968;&#25454;&#28151;&#21512;&#23454;&#29616;&#20102;&#29421;&#31364;&#30340;&#27169;&#22411;&#36873;&#25321;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#20013;&#35782;&#21035;&#21644;&#23398;&#20064;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20294;&#23545;&#20110;&#20219;&#21153;&#25110;&#20989;&#25968;&#30340;&#22788;&#29702;&#30456;&#23545;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20855;&#26377;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#26174;&#33879;&#33021;&#21147;-&#22312;&#26410;&#32463;&#36807;&#20219;&#20309;&#26126;&#30830;&#27169;&#22411;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#26681;&#25454;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;-&#36755;&#20986;&#20363;&#23376;&#25191;&#34892;&#26032;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;Transformer&#27169;&#22411;&#22914;&#20309;&#26377;&#25928;&#22320;&#22312;&#20854;&#39044;&#35757;&#32451;&#25968;&#25454;&#28151;&#21512;&#20013;&#24314;&#31435;&#26725;&#26753;&#65292;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#35782;&#21035;&#21644;&#23398;&#20064;&#26082;&#21253;&#25324;&#39044;&#35757;&#32451;&#20998;&#24067;&#20869;&#21448;&#21253;&#25324;&#20854;&#22806;&#30340;&#26032;&#20219;&#21153;&#12290;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21463;&#25511;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;$(x, f(x))$&#23545;&#24207;&#21015;&#32780;&#19981;&#26159;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#27169;&#22411;&#36873;&#25321;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#25509;&#36817;&#26368;&#20248;&#65292;&#22312;&#33021;&#22815;&#39318;&#20808;&#22312;&#19978;&#19979;&#25991;&#20013;&#35782;&#21035;&#19981;&#21516;&#30340;&#20219;&#21153;&#26063;&#32676;&#24182;&#22312;&#20854;&#20013;&#36827;&#34892;&#23398;&#20064;&#26102;&#65288;&#20219;&#21153;&#26063;&#32676;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#26377;&#24456;&#22909;&#30340;&#34920;&#31034;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#20219;&#21153;&#25110;&#20989;&#25968;&#26102;&#65292;&#24773;&#20917;&#20250;&#31245;&#26377;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) -- to perform new tasks when prompted with unseen input-output examples without any explicit model training. In this work, we study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution. Building on previous work, we investigate this question in a controlled setting, where we study transformer models trained on sequences of $(x, f(x))$ pairs rather than natural language. Our empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data. However when presented with tasks or functi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#26410;&#34987;&#36716;&#24405;&#30340;&#35821;&#38899;&#20013;&#33258;&#21160;&#26816;&#27979;&#35821;&#35328;&#38169;&#20081;&#30340;&#35821;&#35328;&#12289;&#22768;&#23398;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#36716;&#24405;&#33021;&#21147;&#65292;&#20197;&#21450;&#23558;&#20854;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#26469;&#25913;&#21892;&#20020;&#24202;&#21644;&#38750;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00867</link><description>&lt;p&gt;
&#20174;&#26410;&#34987;&#36716;&#24405;&#30340;&#35821;&#38899;&#20013;&#33258;&#21160;&#26816;&#27979;&#35821;&#35328;&#38169;&#20081;
&lt;/p&gt;
&lt;p&gt;
Automatic Disfluency Detection from Untranscribed Speech. (arXiv:2311.00867v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#26410;&#34987;&#36716;&#24405;&#30340;&#35821;&#38899;&#20013;&#33258;&#21160;&#26816;&#27979;&#35821;&#35328;&#38169;&#20081;&#30340;&#35821;&#35328;&#12289;&#22768;&#23398;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#36716;&#24405;&#33021;&#21147;&#65292;&#20197;&#21450;&#23558;&#20854;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#26469;&#25913;&#21892;&#20020;&#24202;&#21644;&#38750;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#38169;&#20081;&#65292;&#22914;&#20805;&#28385;&#20572;&#39039;&#25110;&#37325;&#22797;&#65292;&#26159;&#35821;&#38899;&#20013;&#20856;&#22411;&#27969;&#30021;&#24230;&#30340;&#20013;&#26029;&#12290;&#21475;&#21507;&#26159;&#19968;&#31181;&#20197;&#39640;&#39057;&#29575;&#20986;&#29616;&#35821;&#35328;&#38169;&#20081;&#30340;&#35328;&#35821;&#38556;&#30861;&#65292;&#20294;&#25152;&#26377;&#20154;&#35828;&#35805;&#26102;&#37117;&#20250;&#20986;&#29616;&#19968;&#20123;&#35821;&#35328;&#38169;&#20081;&#65292;&#32780;&#35821;&#35328;&#38169;&#20081;&#30340;&#39057;&#29575;&#21487;&#33021;&#22240;&#35748;&#30693;&#36127;&#33655;&#31561;&#22240;&#32032;&#32780;&#22686;&#21152;&#12290;&#22312;&#20020;&#24202;&#19978;&#65292;&#33258;&#21160;&#26816;&#27979;&#38169;&#35823;&#30340;&#35821;&#35328;&#21487;&#33021;&#26377;&#21161;&#20110;&#20026;&#21475;&#21507;&#32773;&#21046;&#23450;&#27835;&#30103;&#35745;&#21010;&#12290;&#22312;&#20020;&#24202;&#22806;&#65292;&#33258;&#21160;&#26816;&#27979;&#38169;&#35823;&#30340;&#35821;&#35328;&#21487;&#33021;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#20197;&#25913;&#21892;&#21518;&#32493;&#24212;&#29992;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12290;&#32771;&#34385;&#21040;&#36825;&#20010;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#35328;&#12289;&#22768;&#23398;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#30340;&#36880;&#24103;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#31867;&#35821;&#35328;&#38169;&#20081;&#12290;&#36825;&#20123;&#26041;&#27861;&#37117;&#20197;&#38899;&#39057;&#20316;&#20026;&#36755;&#20837;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#22312;&#36716;&#24405;&#38169;&#35823;&#30340;&#33021;&#21147;&#26041;&#38754;&#65292;&#20351;&#29992;&#38169;&#35823;&#29575;&#26469;&#34913;&#37327;&#35821;&#35328;&#38169;&#20081;&#30340;&#36716;&#24405;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;ASR&#36716;&#24405;&#32467;&#26524;&#20316;&#20026;&#36755;&#20837;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Speech disfluencies, such as filled pauses or repetitions, are disruptions in the typical flow of speech. Stuttering is a speech disorder characterized by a high rate of disfluencies, but all individuals speak with some disfluencies and the rates of disfluencies may by increased by factors such as cognitive load. Clinically, automatic disfluency detection may help in treatment planning for individuals who stutter. Outside of the clinic, automatic disfluency detection may serve as a pre-processing step to improve natural language understanding in downstream applications. With this wide range of applications in mind, we investigate language, acoustic, and multimodal methods for frame-level automatic disfluency detection and categorization. Each of these methods relies on audio as an input. First, we evaluate several automatic speech recognition (ASR) systems in terms of their ability to transcribe disfluencies, measured using disfluency error rates. We then use these ASR transcripts as i
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#19978;&#19979;&#25991;N-Gram&#30340;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#19978;&#19979;&#25991;&#31070;&#32463;&#20803;&#23384;&#22312;&#20110;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;N-Gram&#30005;&#36335;&#20013;&#65292;&#36825;&#34987;&#31216;&#20026;&#20108;&#38454;&#30005;&#36335;&#12290;&#22312;&#35757;&#32451;&#26089;&#26399;&#65292;&#36825;&#20004;&#20010;&#30005;&#36335;&#20855;&#26377;&#30456;&#20114;&#29420;&#31435;&#30340;&#21151;&#33021;&#65292;&#21482;&#26377;&#22312;&#23427;&#20204;&#37117;&#24418;&#25104;&#20043;&#21518;&#25165;&#33021;&#32452;&#21512;&#25104;&#19968;&#20010;&#20108;&#38454;&#30005;&#36335;&#12290;</title><link>http://arxiv.org/abs/2311.00863</link><description>&lt;p&gt;
&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;N-Gram&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Training Dynamics of Contextual N-Grams in Language Models. (arXiv:2311.00863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00863
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#19978;&#19979;&#25991;N-Gram&#30340;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#19978;&#19979;&#25991;&#31070;&#32463;&#20803;&#23384;&#22312;&#20110;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;N-Gram&#30005;&#36335;&#20013;&#65292;&#36825;&#34987;&#31216;&#20026;&#20108;&#38454;&#30005;&#36335;&#12290;&#22312;&#35757;&#32451;&#26089;&#26399;&#65292;&#36825;&#20004;&#20010;&#30005;&#36335;&#20855;&#26377;&#30456;&#20114;&#29420;&#31435;&#30340;&#21151;&#33021;&#65292;&#21482;&#26377;&#22312;&#23427;&#20204;&#37117;&#24418;&#25104;&#20043;&#21518;&#25165;&#33021;&#32452;&#21512;&#25104;&#19968;&#20010;&#20108;&#38454;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19978;&#19979;&#25991;&#31070;&#32463;&#20803;&#65292;&#21253;&#25324;&#19968;&#20010;&#22312;&#24503;&#35821;&#25991;&#26412;&#19978;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31070;&#32463;&#20803;&#23384;&#22312;&#20110;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;N-Gram&#30005;&#36335;&#20013;&#65306;&#25105;&#20204;&#21457;&#29616;&#26202;&#26399;&#30340;&#31070;&#32463;&#20803;&#33021;&#22815;&#35782;&#21035;&#21644;&#32487;&#32493;&#24503;&#35821;&#25991;&#26412;&#20013;&#24120;&#35265;&#30340;N-Gram&#65292;&#20294;&#21482;&#26377;&#22312;&#24503;&#35821;&#31070;&#32463;&#20803;&#28608;&#27963;&#26102;&#25165;&#20250;&#34987;&#28608;&#27963;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#36825;&#20010;&#30005;&#36335;&#30340;&#24418;&#25104;&#65292;&#24182;&#21457;&#29616;&#36825;&#26159;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026;&#20108;&#38454;&#30005;&#36335;&#30340;&#31034;&#20363;&#12290;&#29305;&#21035;&#22320;&#65292;&#26089;&#26399;&#30340;&#35757;&#32451;&#20013;&#65292;&#32452;&#25104;N-Gram&#30005;&#36335;&#21644;&#26368;&#32456;&#24418;&#25104;&#24503;&#35821;&#31070;&#32463;&#20803;&#30340;&#24503;&#35821;&#26816;&#27979;&#30005;&#36335;&#20855;&#26377;&#29420;&#31435;&#30340;&#21151;&#33021;-&#24503;&#35821;&#26816;&#27979;&#30005;&#36335;&#37096;&#20998;&#36890;&#36807;&#24314;&#27169;&#24503;&#35821;&#21333;&#23383;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#24418;&#25104;&#65292;&#32780;N-Gram&#21017;&#36890;&#36807;&#25552;&#21319;&#36866;&#24403;&#30340;&#23436;&#25104;&#26469;&#24418;&#25104;&#12290;&#21482;&#26377;&#22312;&#36825;&#20004;&#20010;&#30005;&#36335;&#24050;&#32463;&#24418;&#25104;&#20043;&#21518;&#65292;&#23427;&#20204;&#25165;&#33021;&#32452;&#21512;&#25104;&#20026;&#19968;&#20010;&#20108;&#38454;&#30005;&#36335;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#20551;&#35774;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#19978;&#19979;&#25991;N-Gram&#30005;&#36335;&#36880;&#28176;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has shown the existence of contextual neurons in language models, including a neuron that activates on German text. We show that this neuron exists within a broader contextual n-gram circuit: we find late layer neurons which recognize and continue n-grams common in German text, but which only activate if the German neuron is active. We investigate the formation of this circuit throughout training and find that it is an example of what we call a second-order circuit. In particular, both the constituent n-gram circuits and the German detection circuit which culminates in the German neuron form with independent functions early in training - the German detection circuit partially through modeling German unigram statistics, and the n-grams by boosting appropriate completions. Only after both circuits have already formed do they fit together into a second-order circuit. Contrary to the hypotheses presented in prior work, we find that the contextual n-gram circuit forms gradually r
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;CASENT&#65292;&#19968;&#31181;&#20026;&#36229;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#23450;&#20041;&#35774;&#35745;&#30340;Seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#26657;&#20934;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#39044;&#27979;&#36229;&#32454;&#31890;&#24230;&#31867;&#22411;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#27169;&#22411;&#22312;UFET&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00835</link><description>&lt;p&gt;
&#20026;&#39640;&#25928;&#19988;&#21487;&#24191;&#27867;&#24212;&#29992;&#30340;&#36229;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#23450;&#20041;&#26657;&#20934;&#30340;Seq2seq&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Calibrated Seq2seq Models for Efficient and Generalizable Ultra-fine Entity Typing. (arXiv:2311.00835v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00835
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;CASENT&#65292;&#19968;&#31181;&#20026;&#36229;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#23450;&#20041;&#35774;&#35745;&#30340;Seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#26657;&#20934;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#39044;&#27979;&#36229;&#32454;&#31890;&#24230;&#31867;&#22411;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#27169;&#22411;&#22312;UFET&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#23545;&#20110;&#20449;&#24687;&#25552;&#21462;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#21487;&#20197;&#39044;&#27979;&#25991;&#26412;&#20013;&#23454;&#20307;&#25552;&#21450;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36755;&#20986;&#31354;&#38388;&#20013;&#23454;&#20307;&#31867;&#22411;&#30340;&#22823;&#37327;&#23384;&#22312;&#65292;&#36825;&#39033;&#20219;&#21153;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#26631;&#20934;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#25110;&#20132;&#21449;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#22312;&#27867;&#21270;&#24615;&#33021;&#24046;&#25110;&#25512;&#29702;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CASENT&#65292;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#36229;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#23450;&#20041;&#30340;seq2seq&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#24102;&#26377;&#26657;&#20934;&#32622;&#20449;&#24230;&#20998;&#25968;&#30340;&#36229;&#32454;&#31890;&#24230;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#23454;&#20307;&#25552;&#21450;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#37319;&#29992;&#32422;&#26463;&#30340;&#26463;&#25628;&#32034;&#26041;&#27861;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#22810;&#20010;&#31867;&#22411;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26657;&#20934;&#26041;&#27861;&#23558;&#19982;&#39044;&#27979;&#31867;&#22411;&#20851;&#32852;&#30340;&#21407;&#22987;&#24207;&#21015;&#27010;&#29575;&#36716;&#25442;&#20026;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;&#36229;&#36807;10k&#31181;&#31867;&#22411;&#30340;UFET&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultra-fine entity typing plays a crucial role in information extraction by predicting fine-grained semantic types for entity mentions in text. However, this task poses significant challenges due to the massive number of entity types in the output space. The current state-of-the-art approaches, based on standard multi-label classifiers or cross-encoder models, suffer from poor generalization performance or inefficient inference. In this paper, we present CASENT, a seq2seq model designed for ultra-fine entity typing that predicts ultra-fine types with calibrated confidence scores. Our model takes an entity mention as input and employs constrained beam search to generate multiple types autoregressively. The raw sequence probabilities associated with the predicted types are then transformed into confidence scores using a novel calibration method. We conduct extensive experiments on the UFET dataset which contains over 10k types. Our method outperforms the previous state-of-the-art in terms
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#38544;&#21947;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#24314;&#26500;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#21947;&#35782;&#21035;&#31995;&#32479;&#21487;&#20197;&#19982;&#20351;&#29992;&#23436;&#25972;&#19978;&#19979;&#25991;&#30340;&#31995;&#32479;&#30456;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2311.00790</link><description>&lt;p&gt;
&#22312;&#38544;&#21947;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#24314;&#26500;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Construction Artifacts in Metaphor Identification Datasets. (arXiv:2311.00790v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00790
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#38544;&#21947;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#24314;&#26500;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#21947;&#35782;&#21035;&#31995;&#32479;&#21487;&#20197;&#19982;&#20351;&#29992;&#23436;&#25972;&#19978;&#19979;&#25991;&#30340;&#31995;&#32479;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#35782;&#21035;&#26088;&#22312;&#29702;&#35299;&#32473;&#23450;&#34920;&#36798;&#26159;&#21542;&#22312;&#29305;&#23450;&#35821;&#22659;&#20013;&#20197;&#27604;&#21947;&#30340;&#26041;&#24335;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#38544;&#21947;&#35782;&#21035;&#25968;&#25454;&#38598;&#22914;&#20309;&#36890;&#36807;&#23436;&#20840;&#24573;&#30053;&#28508;&#22312;&#30340;&#38544;&#21947;&#34920;&#36798;&#25110;&#20854;&#25152;&#22312;&#30340;&#35821;&#22659;&#26469;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#20013;&#27979;&#35797;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#34920;&#26126;&#22522;&#20110;&#19981;&#20855;&#26377;&#23436;&#25972;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#21947;&#35782;&#21035;&#31995;&#32479;&#21487;&#20197;&#19982;&#20351;&#29992;&#23436;&#25972;&#19978;&#19979;&#25991;&#30340;&#31995;&#32479;&#30456;&#31454;&#20105;&#12290;&#36825;&#26159;&#30001;&#20110;&#26500;&#24314;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#23545;&#27491;&#36127;&#31867;&#21035;&#30340;&#19981;&#24076;&#26395;&#30340;&#20559;&#35265;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#20174;&#33258;&#28982;&#35821;&#26009;&#24211;&#20013;&#31934;&#24515;&#25277;&#26679;&#30340;&#19981;&#23384;&#22312;&#36825;&#31181;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#21516;&#26679;&#30340;&#20551;&#35774;&#65292;&#20351;&#24471;&#36825;&#20123;&#25968;&#25454;&#38598;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaphor identification aims at understanding whether a given expression is used figuratively in context. However, in this paper we show how existing metaphor identification datasets can be gamed by fully ignoring the potential metaphorical expression or the context in which it occurs. We test this hypothesis in a variety of datasets and settings, and show that metaphor identification systems based on language models without complete information can be competitive with those using the full context. This is due to the construction procedures to build such datasets, which introduce unwanted biases for positive and negative classes. Finally, we test the same hypothesis on datasets that are carefully sampled from natural corpora and where this bias is not present, making these datasets more challenging and reliable.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#20026;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#25512;&#23548;&#20986;&#39640;&#36136;&#37327;&#30340;&#36890;&#29992;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#38477;&#32500;&#25216;&#26415;&#21487;&#35270;&#21270;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#65292;&#24182;&#22312;MIMIC-III&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00768</link><description>&lt;p&gt;
&#29992;&#20110;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Language Model Training Paradigms for Clinical Feature Embeddings. (arXiv:2311.00768v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#20026;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#25512;&#23548;&#20986;&#39640;&#36136;&#37327;&#30340;&#36890;&#29992;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#38477;&#32500;&#25216;&#26415;&#21487;&#35270;&#21270;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#65292;&#24182;&#22312;MIMIC-III&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#34920;&#31034;&#23398;&#20064;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23545;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#25512;&#23548;&#20986;&#20020;&#24202;&#29305;&#24449;&#65288;&#22914;&#24515;&#29575;&#21644;&#34880;&#21387;&#65289;&#30340;&#36890;&#29992;&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#65292;&#23454;&#29616;&#27604;&#29616;&#26377;&#30340;&#26102;&#38388;&#27493;&#21644;&#24739;&#32773;&#32423;&#21035;&#34920;&#31034;&#23398;&#20064;&#26356;&#32454;&#31890;&#24230;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#38477;&#32500;&#25216;&#26415;&#21487;&#35270;&#21270;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#20808;&#21069;&#30340;&#20020;&#24202;&#30693;&#35782;&#39640;&#24230;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#22312;MIMIC-III&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21457;&#24067;&#22312;&#32593;&#19978;&#20197;&#20379;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In research areas with scarce data, representation learning plays a significant role. This work aims to enhance representation learning for clinical time series by deriving universal embeddings for clinical features, such as heart rate and blood pressure. We use self-supervised training paradigms for language models to learn high-quality clinical feature embeddings, achieving a finer granularity than existing time-step and patient-level representation learning. We visualize the learnt embeddings via unsupervised dimension reduction techniques and observe a high degree of consistency with prior clinical knowledge. We also evaluate the model performance on the MIMIC-III benchmark and demonstrate the effectiveness of using clinical feature embeddings. We publish our code online for replication.
&lt;/p&gt;</description></item><item><title>&#36816;&#29992;&#32654;&#22269;&#25163;&#35821;&#25968;&#25454;&#65292;&#35770;&#25991;&#27010;&#36848;&#20102;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#35745;&#31639;&#26426;&#25163;&#21183;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25163;&#35821;&#32773;&#38388;&#21644;&#25163;&#35821;&#32773;&#20869;&#30340;&#21516;&#27493;&#21464;&#21270;&#20197;&#21450;&#25163;&#21183;&#32467;&#26500;&#20013;&#30340;&#35821;&#35328;&#35268;&#24459;&#12290;</title><link>http://arxiv.org/abs/2311.00762</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#20174;&#36830;&#32493;&#25163;&#35821;&#20013;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#35745;&#31639;&#26426;&#25163;&#21183;&#35782;&#21035;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Challenges for Linguistically-Driven Computer-Based Sign Recognition from Continuous Signing for American Sign Language. (arXiv:2311.00762v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00762
&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#32654;&#22269;&#25163;&#35821;&#25968;&#25454;&#65292;&#35770;&#25991;&#27010;&#36848;&#20102;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#35745;&#31639;&#26426;&#25163;&#21183;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25163;&#35821;&#32773;&#38388;&#21644;&#25163;&#35821;&#32773;&#20869;&#30340;&#21516;&#27493;&#21464;&#21270;&#20197;&#21450;&#25163;&#21183;&#32467;&#26500;&#20013;&#30340;&#35821;&#35328;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35745;&#31639;&#26426;&#22522;&#20110;&#35270;&#39057;&#30340;&#23396;&#31435;&#25163;&#21183;&#35782;&#21035;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;&#36825;&#19968;&#20219;&#21153;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#25361;&#25112;&#26159;&#25163;&#35821;&#30340;&#23454;&#38469;&#20135;&#29983;&#20013;&#23384;&#22312;&#30340;&#25163;&#35821;&#32773;&#38388;&#21644;&#25163;&#35821;&#32773;&#20869;&#30340;&#21516;&#27493;&#21464;&#21270;&#65292;&#21253;&#25324;&#26576;&#20123;&#25163;&#35821;&#30340;&#31038;&#20250;&#35821;&#35328;&#23398;&#21464;&#20307;&#30340;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36830;&#32493;&#25163;&#21183;&#35782;&#21035;&#30340;&#25361;&#25112;&#26356;&#21152;&#22256;&#38590;&#65292;&#26412;&#25991;&#22522;&#20110;&#32654;&#22269;&#25163;&#35821;&#65288;ASL&#65289;&#30340;&#22823;&#22411;&#35821;&#35328;&#27880;&#37322;&#35270;&#39057;&#25968;&#25454;&#30340;&#21457;&#29616;&#65292;&#23545;&#36825;&#20123;&#25361;&#25112;&#36827;&#34892;&#20102;&#27010;&#36848;&#12290;&#36824;&#35752;&#35770;&#20102;&#25163;&#21183;&#32467;&#26500;&#20013;&#30340;&#19968;&#20123;&#35821;&#35328;&#35268;&#24459;&#65292;&#21487;&#20197;&#25552;&#39640;&#25163;&#21183;&#24418;&#29366;&#21644;&#25163;&#21183;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been recent advances in computer-based recognition of isolated, citation-form signs from video. There are many challenges for such a task, not least the naturally occurring inter- and intra- signer synchronic variation in sign production, including sociolinguistic variation in the realization of certain signs. However, there are several significant factors that make recognition of signs from continuous signing an even more difficult problem. This article presents an overview of such challenges, based in part on findings from a large corpus of linguistically annotated video data for American Sign Language (ASL). Some linguistic regularities in the structure of signs that can boost handshape and sign recognition are also discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DataSculpt&#65292;&#23427;&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#26631;&#31614;&#20989;&#25968;&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#12290;&#36890;&#36807;&#22810;&#31181;&#25216;&#26415;&#21644;&#26041;&#27861;&#30340;&#32467;&#21512;&#65292;DataSculpt&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00739</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#35774;&#35745;&#20934;&#30830;&#30340;&#26631;&#31614;&#20989;&#25968;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Design Accurate Label Functions?. (arXiv:2311.00739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DataSculpt&#65292;&#23427;&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#26631;&#31614;&#20989;&#25968;&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#12290;&#36890;&#36807;&#22810;&#31181;&#25216;&#26415;&#21644;&#26041;&#27861;&#30340;&#32467;&#21512;&#65292;DataSculpt&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#31243;&#24335;&#24369;&#30417;&#30563;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23553;&#35013;&#21551;&#21457;&#24335;&#25968;&#25454;&#28304;&#30340;&#26631;&#31614;&#20989;&#25968;&#65288;LFs&#65289;&#21152;&#36895;&#26631;&#35760;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#31934;&#30830;&#30340;LFs&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#22823;&#37327;&#21162;&#21147;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;PLMs&#33258;&#20027;&#21046;&#23450;&#20934;&#30830;&#30340;LFs&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;DataSculpt&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;PLMs&#33258;&#21160;&#29983;&#25104;LFs&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#12290;&#22312;DataSculpt&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#21508;&#31181;&#25552;&#31034;&#25216;&#26415;&#12289;&#23454;&#20363;&#36873;&#25321;&#31574;&#30053;&#21644;LF&#36807;&#28388;&#26041;&#27861;&#26469;&#25506;&#32034;&#24191;&#38420;&#30340;&#35774;&#35745;&#39046;&#22495;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#23545;DataSculpt&#22312;12&#20010;&#28085;&#30422;&#22810;&#20010;&#20219;&#21153;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#36825;&#20010;&#35780;&#20272;&#25581;&#31034;&#20102;DataSculpt&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Programmatic weak supervision methodologies facilitate the expedited labeling of extensive datasets through the use of label functions (LFs) that encapsulate heuristic data sources. Nonetheless, the creation of precise LFs necessitates domain expertise and substantial endeavors. Recent advances in pre-trained language models (PLMs) have exhibited substantial potential across diverse tasks. However, the capacity of PLMs to autonomously formulate accurate LFs remains an underexplored domain. In this research, we address this gap by introducing DataSculpt, an interactive framework that harnesses PLMs for the automated generation of LFs. Within DataSculpt, we incorporate an array of prompting techniques, instance selection strategies, and LF filtration methods to explore the expansive design landscape. Ultimately, we conduct a thorough assessment of DataSculpt's performance on 12 real-world datasets, encompassing a range of tasks. This evaluation unveils both the strengths and limitations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26816;&#27979;&#33258;&#25105;&#25253;&#21578;COVID-19&#35786;&#26029;&#25512;&#25991;&#30340;&#19981;&#21516;&#25991;&#26412;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#33719;&#24471;&#20102;&#27604;&#24179;&#22343;&#20540;&#39640;&#20986;4.1%&#30340;84.5%&#30340;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2311.00732</link><description>&lt;p&gt;
tmn&#22312;#SMM4H 2023&#19978;&#30340;&#35770;&#25991;:&#27604;&#36739;&#29992;&#20110;&#26816;&#27979;&#33258;&#25105;&#25253;&#21578;COVID-19&#35786;&#26029;&#25512;&#25991;&#30340;&#25991;&#26412;&#39044;&#22788;&#29702;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
tmn at #SMM4H 2023: Comparing Text Preprocessing Techniques for Detecting Tweets Self-reporting a COVID-19 Diagnosis. (arXiv:2311.00732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26816;&#27979;&#33258;&#25105;&#25253;&#21578;COVID-19&#35786;&#26029;&#25512;&#25991;&#30340;&#19981;&#21516;&#25991;&#26412;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#33719;&#24471;&#20102;&#27604;&#24179;&#22343;&#20540;&#39640;&#20986;4.1%&#30340;84.5%&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#29992;&#20110;SMM4H 2023&#20219;&#21153;1&#30340;&#31995;&#32479;&#12290;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#33258;&#21160;&#21306;&#20998;&#37027;&#20123;&#33258;&#25105;&#25253;&#21578;COVID-19&#35786;&#26029;&#30340;&#25512;&#25991;&#65288;&#20363;&#22914;&#65292;&#38451;&#24615;&#26816;&#27979;&#12289;&#20020;&#24202;&#35786;&#26029;&#25110;&#20303;&#38498;&#65289;&#21644;&#37027;&#20123;&#27809;&#26377;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#20351;&#29992;&#22235;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25512;&#25991;&#39044;&#22788;&#29702;&#25216;&#26415;&#12290;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#33719;&#24471;&#20102;84.5%&#30340;F1&#24471;&#20998;&#65292;&#27604;&#24179;&#22343;&#20540;&#39640;&#20986;4.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper describes a system developed for Task 1 at SMM4H 2023. The goal of the task is to automatically distinguish tweets that self-report a COVID-19 diagnosis (for example, a positive test, clinical diagnosis, or hospitalization) from those that do not. We investigate the use of different techniques for preprocessing tweets using four transformer-based models. The ensemble of fine-tuned language models obtained an F1-score of 84.5%, which is 4.1% higher than the average value.
&lt;/p&gt;</description></item><item><title>JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00286</link><description>&lt;p&gt;
JADE&#65306;&#22522;&#20110;&#35821;&#35328;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00286
&lt;/p&gt;
&lt;p&gt;
JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JADE&#65292;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#20998;&#26512;&#30340;&#27169;&#31946;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#22686;&#24378;&#31181;&#23376;&#38382;&#39064;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#24182;&#22987;&#32456;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#19977;&#31867;LLM&#65306;&#20843;&#20010;&#24320;&#28304;&#20013;&#25991;LLM&#65292;&#20845;&#20010;&#21830;&#19994;&#20013;&#25991;LLM&#21644;&#22235;&#20010;&#21830;&#19994;&#33521;&#25991;LLM&#12290;JADE&#20026;&#36825;&#19977;&#31867;LLM&#29983;&#25104;&#20102;&#19977;&#20010;&#23433;&#20840;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#65306;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#21516;&#26102;&#35302;&#21457;&#22810;&#20010;LLM&#30340;&#26377;&#23475;&#29983;&#25104;&#65292;&#24179;&#22343;&#19981;&#23433;&#20840;&#29983;&#25104;&#27604;&#20363;&#20026;70%&#65288;&#35831;&#21442;&#35265;&#19979;&#34920;&#65289;&#65292;&#21516;&#26102;&#36825;&#20123;&#38382;&#39064;&#20173;&#28982;&#26159;&#33258;&#28982;&#12289;&#27969;&#30021;&#19988;&#20445;&#30041;&#20102;&#26680;&#24515;&#30340;&#19981;&#23433;&#20840;&#35821;&#20041;&#12290;&#25105;&#20204;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#21457;&#24067;&#20102;&#23545;&#21830;&#19994;&#33521;&#25991;LLM&#21644;&#24320;&#28304;&#33521;&#25991;LLM&#29983;&#25104;&#30340;&#22522;&#20934;&#28436;&#31034;&#65306;https://github.com/whitzard-ai/jade-db&#12290;&#23545;&#20110;&#23545;JADE&#29983;&#25104;&#30340;&#26356;&#22810;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#35835;&#32773;&#65292;&#35831;&#19982;&#25105;&#20204;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present \textit{JADE}, a targeted linguistic fuzzing platform which strengthens the linguistic complexity of seed questions to simultaneously and consistently break a wide range of widely-used LLMs categorized in three groups: eight open-sourced Chinese, six commercial Chinese and four commercial English LLMs. JADE generates three safety benchmarks for the three groups of LLMs, which contain unsafe questions that are highly threatening: the questions simultaneously trigger harmful generation of multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$} (please see the table below), while are still natural questions, fluent and preserving the core unsafe semantics. We release the benchmark demos generated for commercial English LLMs and open-sourced English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For readers who are interested in evaluating on more questions generated by JADE, please contact us.  \textit{JADE} is based on Noam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#38750;&#23545;&#31216;&#20851;&#31995;&#65292;&#19988;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18348</link><description>&lt;p&gt;
&#24847;&#20041;&#34920;&#24449;&#26469;&#33258;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#38750;&#23545;&#31216;&#20851;&#31995;&#65292;&#19988;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#32771;&#34385;&#25193;&#23637;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#26469;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#12290;&#36825;&#31181;&#31574;&#30053;&#26159;&#26080;&#25552;&#31034;&#30340;&#65292;&#19981;&#38656;&#35201;&#24494;&#35843;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#21521;&#37327;&#30340;&#34920;&#24449;&#19981;&#21516;&#65292;&#22522;&#20110;&#20998;&#24067;&#30340;&#34920;&#24449;&#36824;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20284;&#28982;&#20989;&#25968;&#20043;&#38388;&#30340;&#20195;&#25968;&#36816;&#31639;&#26469;&#24314;&#27169;&#38750;&#23545;&#31216;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#36923;&#36753;&#34164;&#28085;&#30340;&#26041;&#21521;&#65292;&#19978;&#20301;&#35789;/&#19979;&#20301;&#35789;&#20851;&#31995;&#65289;&#12290;&#36825;&#20123;&#24819;&#27861;&#22522;&#20110;&#35821;&#20041;&#30340;&#20998;&#24067;&#35270;&#35282;&#65292;&#24182;&#19982;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#26631;&#20934;&#26500;&#36896;&#30456;&#36830;&#25509;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20204;&#23578;&#26410;&#24212;&#29992;&#20110;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20174;&#22823;&#22411;&#27169;&#22411;&#33719;&#24471;&#30340;&#34920;&#24449;&#19982;&#20154;&#31867;&#27880;&#37322;&#24456;&#22909;&#22320;&#19968;&#33268;&#65292;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#38646;&#26679;&#26412;&#21644;&#26080;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#34164;&#28085;&#21644;&#21253;&#21547;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to extract meaning representations from autoregressive language models by considering the distribution of all possible trajectories extending an input text. This strategy is prompt-free, does not require fine-tuning, and is applicable to any pre-trained autoregressive model. Moreover, unlike vector-based representations, distribution-based representations can also model asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym relations) by using algebraic operations between likelihood functions. These ideas are grounded in distributional perspectives on semantics and are connected to standard constructions in automata theory, but to our knowledge they have not been applied to modern language models. We empirically show that the representations obtained from large models align well with human annotations, outperform other zero-shot and prompt-free methods on semantic similarity tasks, and can be used to solve more complex entailment and containment tasks 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#30340;&#24178;&#25200;&#26469;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;LLMs&#21463;&#21040;&#24178;&#25200;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17490</link><description>&lt;p&gt;
&#25552;&#39640;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#23545;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24178;&#25200;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#30340;&#24178;&#25200;&#26469;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;LLMs&#21463;&#21040;&#24178;&#25200;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20351;&#24471;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;(ODQA)&#20013;&#23454;&#29616;&#38646;&#26679;&#26412;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#26159;&#30001;&#20110;&#38405;&#35835;&#22120;&#30456;&#23545;&#20110;&#26816;&#32034;&#22120;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#19968;&#31181;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#25104;&#26412;&#21644;&#26631;&#27880;&#25968;&#25454;&#38656;&#27714;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#30001;&#20110;&#26816;&#32034;&#21040;&#30340;&#26080;&#20851;&#25991;&#26723;&#20197;&#21450;&#20316;&#20026;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#26102;&#29983;&#25104;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#32780;&#21463;&#21040;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21542;&#23450;&#30340;&#25351;&#20196;&#21644;&#20998;&#25968;&#35843;&#25972;&#30340;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#25991;&#26723;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22788;&#29702;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;&#38754;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#32780;&#22256;&#38590;&#37325;&#37325;&#30340;&#30417;&#30563;&#24335;&#38405;&#35835;&#22120;&#19981;&#21516;&#65292;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) enable zero-shot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for labeled data. We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers. To tackle these problems, we mitigate the impact of such documents via Distraction-aware Answer Selection (DAS) with a negation-based instruction and score adjustment for proper answer selection. Experimental results show that our approach successfully handles distraction across diverse scenarios, enhancing the performance of zero-shot readers. Furthermore, unlike supervised readers struggling with unseen data, zero-shot readers demonstrate outstanding transferability without any training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31354;&#27668;&#35299;&#30721;&#30340;&#26032;&#39062;&#36731;&#37327;&#32423;&#35299;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#24314;&#23646;&#24615;&#20998;&#24067;&#26469;&#24179;&#34913;&#26435;&#37325;&#65292;&#29983;&#25104;&#26356;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#20197;&#35299;&#20915;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#23646;&#24615;&#22349;&#32553;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.14892</link><description>&lt;p&gt;
&#31354;&#27668;&#35299;&#30721;&#65306;&#35299;&#30721;&#26102;&#38388;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#23646;&#24615;&#20998;&#24067;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation. (arXiv:2310.14892v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31354;&#27668;&#35299;&#30721;&#30340;&#26032;&#39062;&#36731;&#37327;&#32423;&#35299;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#24314;&#23646;&#24615;&#20998;&#24067;&#26469;&#24179;&#34913;&#26435;&#37325;&#65292;&#29983;&#25104;&#26356;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#20197;&#35299;&#20915;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#23646;&#24615;&#22349;&#32553;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#30340;&#25991;&#26412;&#29983;&#25104;&#65288;CTG&#65289;&#26088;&#22312;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#25991;&#26412;&#65292;&#32780;&#22522;&#20110;&#35299;&#30721;&#26102;&#38388;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#21457;&#29616;&#20102;&#23646;&#24615;&#22349;&#32553;&#29616;&#35937;&#12290;&#24403;&#25511;&#21046;&#24378;&#24230;&#36229;&#36807;&#20020;&#30028;&#20540;&#26102;&#65292;&#23427;&#20250;&#23548;&#33268;&#29983;&#25104;&#25991;&#26412;&#30340;&#27969;&#30021;&#24615;&#36805;&#36895;&#38477;&#20302;&#65292;&#20351;&#25991;&#26412;&#23436;&#20840;&#26080;&#27861;&#20351;&#29992;&#12290;&#36825;&#20010;&#38480;&#21046;&#38459;&#30861;&#20102;&#35299;&#30721;&#26041;&#27861;&#22312;&#23454;&#29616;&#39640;&#27700;&#24179;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#35299;&#30721;&#26694;&#26550;&#65292;&#21517;&#20026;&#31354;&#27668;&#35299;&#30721;&#12290;&#23427;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#37325;&#24314;&#23646;&#24615;&#20998;&#24067;&#26469;&#24179;&#34913;&#23646;&#24615;&#35789;&#21644;&#38750;&#23646;&#24615;&#35789;&#20043;&#38388;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#21069;&#32512;&#24494;&#35843;&#26469;&#35757;&#32451;&#21069;&#32512;&#20197;&#33719;&#24471;&#23646;&#24615;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23646;&#24615;&#20998;&#24067;&#37325;&#24314;&#26041;&#27861;&#26469;&#24179;&#34913;&#25152;&#33719;&#24471;&#30340;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#37325;&#24314;&#21518;&#30340;&#20998;&#24067;&#36827;&#34892;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable text generation (CTG) aims to generate text with desired attributes, and decoding-time-based methods have shown promising performance on this task. However, in this paper, we identify the phenomenon of Attribute Collapse for the first time. It causes the fluency of generated text to rapidly decrease when the control strength exceeds a critical value, rendering the text completely unusable. This limitation hinders the effectiveness of decoding methods in achieving high levels of controllability. To address this problem, we propose a novel lightweight decoding framework named Air-Decoding. Its main idea is reconstructing the attribute distributions to balance the weights between attribute words and non-attribute words to generate more fluent text. Specifically, we train prefixes by prefix-tuning to obtain attribute distributions. Then we design a novel attribute distribution reconstruction method to balance the obtained distributions and use the reconstructed distributions t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#38382;&#21477;&#35752;&#35770;&#65288;QUD&#65289;&#35821;&#31687;&#35299;&#26512;&#30340;&#26694;&#26550;QUDeval&#12290;&#20351;&#29992;QUDeval&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20173;&#28982;&#38754;&#20020;&#35299;&#26512;&#25152;&#26377;QUD&#32422;&#26463;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#24456;&#24046;&#22320;&#36817;&#20284;&#35299;&#26512;&#22120;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.14520</link><description>&lt;p&gt;
QUDEVAL&#65306;&#38382;&#21477;&#35752;&#35770;&#30340;&#35821;&#31687;&#35299;&#26512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing. (arXiv:2310.14520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#38382;&#21477;&#35752;&#35770;&#65288;QUD&#65289;&#35821;&#31687;&#35299;&#26512;&#30340;&#26694;&#26550;QUDeval&#12290;&#20351;&#29992;QUDeval&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20173;&#28982;&#38754;&#20020;&#35299;&#26512;&#25152;&#26377;QUD&#32422;&#26463;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#24456;&#24046;&#22320;&#36817;&#20284;&#35299;&#26512;&#22120;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#21477;&#35752;&#35770;&#65288;QUD&#65289;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#35821;&#35328;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#65292;&#35821;&#31687;&#36890;&#36807;&#19981;&#26029;&#25552;&#38382;&#21644;&#22238;&#31572;&#32780;&#36827;&#34892;&#12290;&#23558;&#35821;&#31687;&#36827;&#34892;&#33258;&#21160;&#35299;&#26512;&#20197;&#29983;&#25104;QUD&#32467;&#26500;&#65292;&#22240;&#27492;&#28041;&#21450;&#21040;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#65306;&#32473;&#23450;&#19968;&#20010;&#25991;&#26723;&#21644;&#19968;&#20010;&#22238;&#31572;&#21477;&#23376;&#65292;&#29983;&#25104;&#28385;&#36275;QUD&#35821;&#35328;&#32422;&#26463;&#24182;&#21487;&#20197;&#22312;&#20808;&#21069;&#29615;&#22659;&#20013;&#32465;&#23450;&#21040;&#19968;&#20010;&#38170;&#23450;&#21477;&#23376;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#34987;&#31216;&#20026;&#22909;&#22855;&#24515;&#39537;&#21160;&#21644;&#24320;&#25918;&#24615;&#30340;&#12290;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;QUD&#35299;&#26512;&#30340;&#26694;&#26550;&#65292;&#23558;QUD&#30340;&#29702;&#35770;&#32422;&#26463;&#20855;&#20307;&#21270;&#20026;&#19968;&#20010;&#20855;&#20307;&#30340;&#21327;&#35758;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;QUDeval&#65292;&#19968;&#20010;&#23545;&#26469;&#33258;&#35843;&#20248;&#31995;&#32479;&#21644;LLMs&#30340;2,190&#20010;QUD&#38382;&#39064;&#36827;&#34892;&#32454;&#31890;&#24230;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;QUDeval&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28385;&#36275;&#25152;&#26377;QUD&#32422;&#26463;&#23545;&#20110;&#29616;&#20195;LLMs&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#24456;&#24046;&#22320;&#36817;&#20284;&#35299;&#26512;&#22120;&#36136;&#37327;&#12290;&#20196;&#20154;&#40723;&#33310;&#30340;&#26159;&#65292;&#20154;&#24037;&#32534;&#20889;&#30340;QUD&#24471;&#20998;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Questions Under Discussion (QUD) is a versatile linguistic framework in which discourse progresses as continuously asking questions and answering them. Automatic parsing of a discourse to produce a QUD structure thus entails a complex question generation task: given a document and an answer sentence, generate a question that satisfies linguistic constraints of QUD and can be grounded in an anchor sentence in prior context. These questions are known to be curiosity-driven and open-ended. This work introduces the first framework for the automatic evaluation of QUD parsing, instantiating the theoretical constraints of QUD in a concrete protocol. We present QUDeval, a dataset of fine-grained evaluation of 2,190 QUD questions generated from both fine-tuned systems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD is still challenging for modern LLMs, and that existing evaluation metrics poorly approximate parser quality. Encouragingly, human-authored QUDs are scored hi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#26816;&#27979;&#21051;&#26495;&#21360;&#35937;&#30340;&#35821;&#35328;&#20195;&#29702;&#26550;&#26500;&#65292;&#21487;&#33258;&#20027;&#35843;&#29992;&#21508;&#31181;&#24037;&#20855;&#26469;&#20419;&#36827;&#25972;&#20010;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#24212;&#29992;&#20110;&#21830;&#19994;&#20135;&#21697;&#21644;&#24320;&#25918;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.11778</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#26816;&#27979;&#38544;&#24615;&#21051;&#26495;&#21360;&#35937;&#30340;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale. (arXiv:2310.11778v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#26816;&#27979;&#21051;&#26495;&#21360;&#35937;&#30340;&#35821;&#35328;&#20195;&#29702;&#26550;&#26500;&#65292;&#21487;&#33258;&#20027;&#35843;&#29992;&#21508;&#31181;&#24037;&#20855;&#26469;&#20419;&#36827;&#25972;&#20010;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#24212;&#29992;&#20110;&#21830;&#19994;&#20135;&#21697;&#21644;&#24320;&#25918;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#30340;&#28608;&#22686;&#21152;&#36895;&#20102;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#21830;&#19994;&#20135;&#21697;&#20013;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#37319;&#29992;&#12290;&#34429;&#28982;&#36825;&#20123;&#20986;&#33394;&#30340;AIGC&#20135;&#21697;&#22312;&#28040;&#36153;&#32773;&#20013;&#33719;&#24471;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#35748;&#21487;&#21644;&#28608;&#21457;&#20102;&#28909;&#24773;&#65292;&#20294;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#26080;&#24847;&#20013;&#24378;&#21270;&#29616;&#26377;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#21463;&#21040;&#35821;&#35328;&#20195;&#29702;&#26368;&#36817;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;&#20195;&#29702;&#26550;&#26500;&#12290;&#36825;&#31181;&#22810;&#21151;&#33021;&#30340;&#20195;&#29702;&#26550;&#26500;&#33021;&#22815;&#36866;&#24212;&#33258;&#30001;&#24418;&#24335;&#30340;&#26816;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#33258;&#20027;&#35843;&#29992;&#21508;&#31181;&#24037;&#20855;&#26469;&#20419;&#36827;&#25972;&#20010;&#36807;&#31243;&#65292;&#20174;&#29983;&#25104;&#30456;&#24212;&#30340;&#25351;&#20196;&#21644;&#22270;&#20687;&#21040;&#26816;&#27979;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#22522;&#20110;&#22810;&#20010;&#24320;&#25918;&#25991;&#26412;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19982;&#21051;&#26495;&#21360;&#35937;&#30456;&#20851;&#30340;&#22522;&#20934;&#65292;&#24182;&#23558;&#36825;&#31181;&#26550;&#26500;&#24212;&#29992;&#20110;&#21830;&#19994;&#20135;&#21697;&#21644;&#27969;&#34892;&#30340;&#24320;&#25918;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge in the research of diffusion models has accelerated the adoption of text-to-image models in various Artificial Intelligence Generated Content (AIGC) commercial products. While these exceptional AIGC products are gaining increasing recognition and sparking enthusiasm among consumers, the questions regarding whether, when, and how these models might unintentionally reinforce existing societal stereotypes remain largely unaddressed. Motivated by recent advancements in language agents, here we introduce a novel agent architecture tailored for stereotype detection in text-to-image models. This versatile agent architecture is capable of accommodating free-form detection tasks and can autonomously invoke various tools to facilitate the entire process, from generating corresponding instructions and images, to detecting stereotypes. We build the stereotype-relevant benchmark based on multiple open-text datasets, and apply this architecture to commercial products and popular ope
&lt;/p&gt;</description></item><item><title>EMO&#25552;&#20986;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#29616;&#35937;&#12290;EMO&#21033;&#29992;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#19978;&#30028;&#26469;&#31616;&#21270;&#35757;&#32451;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;EMO&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.04691</link><description>&lt;p&gt;
EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04691
&lt;/p&gt;
&lt;p&gt;
EMO&#25552;&#20986;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#29616;&#35937;&#12290;EMO&#21033;&#29992;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#19978;&#30028;&#26469;&#31616;&#21270;&#35757;&#32451;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;EMO&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26159;&#20154;&#25991;&#26412;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#23427;&#20204;&#20027;&#35201;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#31561;&#21516;&#20110;&#26368;&#23567;&#21270;&#32463;&#39564;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#20998;&#24067;&#20043;&#38388;&#30340;&#21069;&#21521;&#20132;&#21449;&#29109;&#12290;&#28982;&#32780;&#65292;&#24403;&#20174;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#30340;&#20998;&#24067;&#35299;&#30721;&#26102;&#65292;&#20173;&#28982;&#32463;&#24120;&#35266;&#23519;&#21040;&#21508;&#31181;&#36864;&#21270;&#29616;&#35937;&#12290;&#25105;&#20204;&#30830;&#23450;&#21069;&#21521;&#20132;&#21449;&#29109;&#20316;&#20026;&#20154;&#19982;&#27169;&#22411;&#20998;&#24067;&#23545;&#40784;&#30340;&#36317;&#31163;&#24230;&#37327;&#26159;&#27425;&#20248;&#30340;&#65292;&#21407;&#22240;&#26377;&#65306;&#65288;1&#65289;&#21484;&#22238;&#20248;&#21270;&#65292;&#65288;2&#65289;&#36127;&#26679;&#26412;&#22810;&#26679;&#24615;&#24573;&#35270;&#21644;&#65288;3&#65289;&#35757;&#32451;&#27979;&#35797;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#12290;EMO&#21033;&#29992;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#20869;&#22312;&#29305;&#24615;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#30001;&#20110;&#30452;&#25509;&#35745;&#31639;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;EMO&#19978;&#30028;&#26469;&#31616;&#21270;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#32463;&#36807;&#24191;&#27867;&#35780;&#20272;&#20043;&#21518;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural language models are probabilistic models of human text. They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution. However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models. We establish that the forward cross-entropy is suboptimal as a distance metric for aligning human and model distribution due to its (1) recall-prioritization (2) negative diversity ignorance and (3) train-test mismatch. In this paper, we propose Earth Mover Distance Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on the inherent properties of earth mover distance to address the aforementioned challenges. Due to the high complexity of direct computation, we further introduce a feasible upper bound for EMO to ease end-to-end training. Upon extensive evaluation of language model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23558;&#20154;&#31867;&#25551;&#36848;&#36716;&#21270;&#20026;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#31649;&#29702;Ansible YAML&#35282;&#33394;&#21644;playbooks&#65292;&#20197;&#24212;&#23545;&#20113;&#35745;&#31639;&#21644;&#31649;&#29702;&#26041;&#27861;&#21457;&#23637;&#24341;&#36215;&#30340;&#31995;&#32479;&#26500;&#24314;&#21644;&#32500;&#25252;&#26041;&#27861;&#30340;&#21464;&#38761;&#12290;</title><link>http://arxiv.org/abs/2309.01456</link><description>&lt;p&gt;
LLM&#21644;&#22522;&#30784;&#35774;&#26045;&#21363;&#20195;&#30721;&#30340;&#29992;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM and Infrastructure as a Code use case. (arXiv:2309.01456v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23558;&#20154;&#31867;&#25551;&#36848;&#36716;&#21270;&#20026;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#31649;&#29702;Ansible YAML&#35282;&#33394;&#21644;playbooks&#65292;&#20197;&#24212;&#23545;&#20113;&#35745;&#31639;&#21644;&#31649;&#29702;&#26041;&#27861;&#21457;&#23637;&#24341;&#36215;&#30340;&#31995;&#32479;&#26500;&#24314;&#21644;&#32500;&#25252;&#26041;&#27861;&#30340;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35745;&#31639;&#21644;&#35832;&#22914;&#31934;&#30410;&#31649;&#29702;&#25110;&#25935;&#25463;&#31649;&#29702;&#31561;&#31649;&#29702;&#26041;&#27861;&#30340;&#28436;&#36827;&#65292;&#24341;&#36215;&#20102;&#31995;&#32479;&#26500;&#24314;&#21644;&#32500;&#25252;&#26041;&#27861;&#30340;&#28145;&#21051;&#21464;&#38761;&#12290;&#36825;&#20123;&#23454;&#36341;&#34987;&#32479;&#31216;&#20026;"DevOps"&#12290;&#36825;&#31181;&#25551;&#36848;&#24615;&#30340;&#20449;&#24687;&#31995;&#32479;&#25110;&#24212;&#29992;&#31243;&#24207;&#26041;&#27861;&#65292;&#20197;&#21450;&#20854;&#32452;&#25104;&#37096;&#20998;&#30340;&#37197;&#32622;&#65292;&#24050;&#32463;&#20419;&#20351;&#24320;&#21457;&#20102;&#37197;&#26377;&#19987;&#38376;&#24341;&#25806;&#30340;&#25551;&#36848;&#24615;&#35821;&#35328;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#31995;&#32479;&#31649;&#29702;&#20219;&#21153;&#12290;&#22312;&#36825;&#20123;&#24037;&#20855;&#20013;&#65292;Ansible&#65288;&#24341;&#25806;&#65289;&#21644;YAML&#65288;&#25551;&#36848;&#24615;&#35821;&#35328;&#65289;&#30340;&#32452;&#21512;&#26159;&#24066;&#22330;&#19978;&#26368;&#27969;&#34892;&#30340;&#20004;&#31181;&#24037;&#20855;&#65292;&#20027;&#35201;&#38754;&#20020;&#26469;&#33258;Terraform&#30340;&#31454;&#20105;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#20154;&#31867;&#25551;&#36848;&#36716;&#21270;&#20026;&#20195;&#30721;&#65292;&#29983;&#25104;&#21644;&#31649;&#29702;Ansible YAML&#35282;&#33394;&#21644;playbooks&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25506;&#35752;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#30830;&#23450;&#21487;&#34892;&#30340;&#26041;&#21521;&#21644;&#27010;&#36848;&#28508;&#22312;&#30340;&#24037;&#19994;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud computing and the evolution of management methodologies such as Lean Management or Agile entail a profound transformation in both system construction and maintenance approaches. These practices are encompassed within the term "DevOps." This descriptive approach to an information system or application, alongside the configuration of its constituent components, has necessitated the development of descriptive languages paired with specialized engines for automating systems administration tasks. Among these, the tandem of Ansible (engine) and YAML (descriptive language) stands out as the two most prevalent tools in the market, facing notable competition mainly from Terraform. The current document presents an inquiry into a solution for generating and managing Ansible YAML roles and playbooks, utilizing Generative LLMs (Language Models) to translate human descriptions into code. Our efforts are focused on identifying plausible directions and outlining the potential industrial applicat
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;LLM&#30340;&#29305;&#28857;&#21644;&#21151;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21457;&#29616;&#21644;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.06435</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06435
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;LLM&#30340;&#29305;&#28857;&#21644;&#21151;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21457;&#29616;&#21644;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;&#20102;&#20247;&#22810;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#35843;&#25972;&#29616;&#26377;&#30340;&#26550;&#26500;&#65292;&#22686;&#21152;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#22686;&#21152;&#35757;&#32451;&#26102;&#38388;&#20197;&#36229;&#36234;&#22522;&#32447;&#12290;&#20998;&#26512;&#26032;&#30340;&#21457;&#23637;&#23545;&#20110;&#35782;&#21035;&#22686;&#24378;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#25913;&#36827;LLM&#27867;&#21270;&#33021;&#21147;&#30340;&#21464;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;LLM&#30340;&#26550;&#26500;&#21450;&#20854;&#20998;&#31867;&#12289;&#35757;&#32451;&#31574;&#30053;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;LLM&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#21644;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;LLM&#30340;&#23436;&#25972;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#37325;&#35201;&#29305;&#28857;&#21644;&#21151;&#33021;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;LLM&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#24182;&#25972;&#21512;&#20102;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown excellent generalization capabilities that have led to the development of numerous models. These models propose various new architectures, tweaking existing architectures with refined training strategies, increasing context length, using high-quality training data, and increasing training time to outperform baselines. Analyzing new developments is crucial for identifying changes that enhance training stability and improve generalization in LLMs. This survey paper comprehensively analyses the LLMs architectures and their categorization, training strategies, training datasets, and performance evaluations and discusses future research directions. Moreover, the paper also discusses the basic building blocks and concepts behind LLMs, followed by a complete overview of LLMs, including their important features and functions. Finally, the paper summarizes significant findings from LLM research and consolidates essential architectural and training strateg
&lt;/p&gt;</description></item><item><title>VoxPoser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#22312;&#22810;&#31181;&#25805;&#20316;&#20219;&#21153;&#19979;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#21644;&#23545;&#35937;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05973</link><description>&lt;p&gt;
VoxPoser: &#29992;&#20110;&#24102;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#21487;&#32452;&#21512;&#30340;3D&#20215;&#20540;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05973
&lt;/p&gt;
&lt;p&gt;
VoxPoser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#22312;&#22810;&#31181;&#25805;&#20316;&#20219;&#21153;&#19979;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#21644;&#23545;&#35937;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20016;&#23500;&#30340;&#21487;&#34892;&#21160;&#30693;&#35782;&#65292;&#21487;&#20197;&#20197;&#25512;&#29702;&#21644;&#35268;&#21010;&#30340;&#24418;&#24335;&#25552;&#21462;&#20986;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#20173;&#28982;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#36816;&#21160;&#21407;&#35821;&#26469;&#25191;&#34892;&#19982;&#29615;&#22659;&#30340;&#29289;&#29702;&#20132;&#20114;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#24320;&#38598;&#25351;&#20196;&#21644;&#24320;&#38598;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#65292;&#21363;&#19968;&#31995;&#21015;&#23494;&#38598;&#30340;6-DoF&#26411;&#31471;&#25191;&#34892;&#22120;&#36335;&#24452;&#28857;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;LLMs&#22312;&#32473;&#23450;&#33258;&#30001;&#24418;&#24335;&#30340;&#35821;&#35328;&#25351;&#20196;&#26102;&#25797;&#38271;&#25512;&#26029;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#30340;&#20195;&#30721;&#32534;&#20889;&#33021;&#21147;&#65292;&#23427;&#20204;&#21487;&#20197;&#19982;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20132;&#20114;&#65292;&#20197;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#23558;&#30693;&#35782;&#25509;&#22320;&#21040;Agent&#30340;&#35266;&#27979;&#31354;&#38388;&#20013;&#12290;&#28982;&#21518;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#26694;&#26550;&#20013;&#20351;&#29992;&#32452;&#21512;&#30340;&#20215;&#20540;&#26144;&#23556;&#26469;&#38646;&#35797;&#21512;&#25104;&#38381;&#29615;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop ro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#34164;&#21547;&#12289;&#30456;&#20284;&#24615;&#12289;&#38382;&#31572;&#12289;&#20107;&#23454;&#19968;&#33268;&#24615;&#31561;&#12290;&#36890;&#36807;&#23545;RoBERTa&#36827;&#34892;&#36731;&#37327;&#32423;&#24494;&#35843;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02729</link><description>&lt;p&gt;
&#25991;&#26412;&#23545;&#40784;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#29992;&#20110;&#28023;&#37327;NLP&#20219;&#21153;&#30340;&#32479;&#19968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Text Alignment Is An Efficient Unified Model for Massive NLP Tasks. (arXiv:2307.02729v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#34164;&#21547;&#12289;&#30456;&#20284;&#24615;&#12289;&#38382;&#31572;&#12289;&#20107;&#23454;&#19968;&#33268;&#24615;&#31561;&#12290;&#36890;&#36807;&#23545;RoBERTa&#36827;&#34892;&#36731;&#37327;&#32423;&#24494;&#35843;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#34987;&#35774;&#35745;&#20026;&#19979;&#19968;&#20010;&#35789;&#35821;&#39044;&#27979;&#30340;&#20989;&#25968;&#65292;&#22312;&#24191;&#27867;&#30340;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23613;&#31649;&#20855;&#26377;&#24191;&#27867;&#24615;&#65292;&#20294;&#19979;&#19968;&#20010;&#35789;&#35821;&#39044;&#27979;&#23545;&#20110;&#35768;&#22810;&#20219;&#21153;&#26469;&#35828;&#36890;&#24120;&#19981;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#34920;&#36798;&#26041;&#24335;&#65292;&#38656;&#35201;&#26497;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#21442;&#25968;&#65288;&#25968;&#30334;&#20159;&#32423;&#21035;&#65289;&#65292;&#26377;&#26102;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#23454;&#38469;&#19978;&#65292;&#26500;&#24314;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#21487;&#21462;&#30340;&#8212;&#8212;&#23613;&#31649;&#19981;&#22815;&#36890;&#29992;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#36866;&#29992;&#20110;&#22823;&#37327;&#38382;&#39064;&#30340;&#23376;&#38598;&#65292;&#24182;&#20197;&#26356;&#23567;&#30340;&#27169;&#22411;&#35268;&#27169;&#23454;&#29616;&#30456;&#24403;&#25110;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25991;&#26412;&#23545;&#40784;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#29992;&#20110;&#28041;&#21450;&#25991;&#26412;&#34164;&#21547;&#12289;&#30456;&#20284;&#24615;&#12289;&#38382;&#31572;&#65288;&#21644;&#21487;&#22238;&#31572;&#24615;&#65289;&#12289;&#20107;&#23454;&#19968;&#33268;&#24615;&#31561;&#20851;&#38190;&#20219;&#21153;&#30340;&#24191;&#27867;&#33539;&#22260;&#12290;&#32473;&#23450;&#19968;&#23545;&#25991;&#26412;&#65292;&#35813;&#27169;&#22411;&#27979;&#37327;&#23427;&#20204;&#20043;&#38388;&#20449;&#24687;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;RoBERTa&#65288;3.55&#20159;&#21442;&#25968;&#65289;&#36827;&#34892;&#36731;&#37327;&#32423;&#24494;&#35843;&#26469;&#23454;&#20363;&#21270;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#65288;Align&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), typically designed as a function of next-word prediction, have excelled across extensive NLP tasks. Despite the generality, next-word prediction is often not an efficient formulation for many of the tasks, demanding an extreme scale of model parameters (10s or 100s of billions) and sometimes yielding suboptimal performance. In practice, it is often desirable to build more efficient models -- despite being less versatile, they still apply to a substantial subset of problems, delivering on par or even superior performance with much smaller model sizes. In this paper, we propose text alignment as an efficient unified model for a wide range of crucial tasks involving text entailment, similarity, question answering (and answerability), factual consistency, and so forth. Given a pair of texts, the model measures the degree of alignment between their information. We instantiate an alignment model (Align) through lightweight finetuning of RoBERTa (355M parameters)
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;EHRSHOT&#65292;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;&#12290;&#35813;&#35770;&#25991;&#21033;&#29992;EHRSHOT&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;CLMBR-T-base&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.02028</link><description>&lt;p&gt;
EHRSHOT:&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models. (arXiv:2307.02028v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;EHRSHOT&#65292;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;&#12290;&#35813;&#35770;&#25991;&#21033;&#29992;EHRSHOT&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;CLMBR-T-base&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19968;&#33324;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#31038;&#21306;&#24050;&#32463;&#21463;&#30410;&#20110;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#20294;&#26159;ML&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20849;&#20139;&#36164;&#20135;&#30340;&#32570;&#20047;&#30340;&#38459;&#30861;&#12290;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35775;&#38382;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#39564;&#35777;&#24615;&#33021;&#20248;&#21183;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#36129;&#29486;&#26469;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;EHRSHOT&#65292;&#20854;&#20013;&#21253;&#21547;6,739&#21517;&#26469;&#33258;&#26031;&#22374;&#31119;&#21307;&#23398;&#30340;&#24739;&#32773;&#30340;&#21435;&#35782;&#21035;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#25968;&#25454;&#12290;&#19982;MIMIC-III/IV&#21644;&#20854;&#20182;&#27969;&#34892;&#30340;EHR&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;EHRSHOT&#26159;&#32437;&#21521;&#30340;&#65292;&#19981;&#20165;&#23616;&#38480;&#20110;ICU/ED&#24739;&#32773;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;CLMBR-T-base&#30340;&#26435;&#37325;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#30340;141M&#21442;&#25968;&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#25968;&#25454;&#21253;&#25324;2.57M&#21517;&#24739;&#32773;&#12290;&#25105;&#20204;&#26159;&#26368;&#26089;&#23436;&#20840;&#21457;&#24067;&#36825;&#26679;&#19968;&#20010;&#29992;&#20110;&#32534;&#30721;EHR&#25968;&#25454;&#30340;&#27169;&#22411;&#20043;&#19968;&#65307;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#21457;&#24067;&#30340;&#20020;&#24202;&#25968;&#25454;&#27169;&#22411;&#65288;&#22914;GatorTron&#12289;ClinicalBER&#65289;&#24182;&#27809;&#26377;&#23436;&#20840;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains deidentified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBER
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20803;&#25512;&#29702;&#8221;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#31526;&#21495;&#35299;&#26500;&#30340;&#26041;&#24335;&#65292;&#23558;&#19981;&#21516;&#25512;&#29702;&#38382;&#39064;&#36716;&#21270;&#20026;&#31867;&#20284;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17820</link><description>&lt;p&gt;
&#20803;&#25512;&#29702;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#31526;&#21495;&#35299;&#26500;
&lt;/p&gt;
&lt;p&gt;
Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models. (arXiv:2306.17820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20803;&#25512;&#29702;&#8221;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#31526;&#21495;&#35299;&#26500;&#30340;&#26041;&#24335;&#65292;&#23558;&#19981;&#21516;&#25512;&#29702;&#38382;&#39064;&#36716;&#21270;&#20026;&#31867;&#20284;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31526;&#21495;&#21270;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#26144;&#23556;&#21040;&#26356;&#21152;&#35821;&#27861;&#23436;&#22791;&#19988;&#27809;&#26377;&#27495;&#20041;&#30340;&#24418;&#24335;&#35821;&#35328;&#65288;&#20363;&#22914;Python&#12289;SQL&#65289;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#31163;&#24320;&#20102;&#33258;&#28982;&#35821;&#35328;&#26412;&#36523;&#65292;&#20559;&#31163;&#20102;&#20154;&#31867;&#24605;&#32500;&#30340;&#20064;&#24815;&#65292;&#32780;&#26356;&#22810;&#22320;&#36814;&#21512;&#20102;&#35745;&#31639;&#26426;&#30340;&#25191;&#34892;&#24605;&#32500;&#26041;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24076;&#26395;&#20174;&#35821;&#35328;&#23398;&#20013;&#31526;&#21495;&#30340;&#27010;&#24565;&#20986;&#21457;&#26469;&#31616;&#21270;&#33258;&#28982;&#35821;&#35328;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#19981;&#21516;&#33258;&#28982;&#35821;&#20041;&#20013;&#21253;&#21547;&#30340;&#25512;&#29702;&#38382;&#39064;&#30340;&#24120;&#35265;&#34920;&#36798;&#26041;&#24335;&#21644;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#32771;&#34385;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20803;&#25512;&#29702;&#8221;&#65292;&#23427;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#23436;&#25104;&#35821;&#20041;&#31526;&#21495;&#30340;&#35299;&#26500;&#65292;&#21363;&#35821;&#20041;&#35299;&#26512;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#23558;&#26576;&#20123;&#25512;&#29702;&#20219;&#21153;&#30340;&#19981;&#21516;&#38382;&#39064;&#20943;&#23569;&#21040;&#31867;&#20284;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#33719;&#24471;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolization methods in large language models (LLMs) have been shown effective to improve LLMs' reasoning ability. However, most of these approaches hinge on mapping natural languages to formal languages (e.g., Python, SQL) that are more syntactically complete and free of ambiguity. Although effective, they depart from the natural language itself and deviate from the habits of human thinking, and instead cater more to the execution mindset of computers. In contrast, we hope to simplify natural language by starting from the concept of symbols in linguistics itself, so that LLMs can learn the common formulation and general solution of reasoning problems wrapped in different natural semantics. From this consideration, we propose \textbf{Meta-Reasoning}, which allows LLMs to automatically accomplish semantic-symbol deconstruction, i.e., semantic resolution, to maximally reduce different questions of certain reasoning tasks to similar natural language representation, thus gaining the abili
&lt;/p&gt;</description></item><item><title>&#36845;&#20195;&#20998;&#27573;&#20223;&#23556;&#25554;&#20540;&#65288;IPA&#65289;&#36924;&#36817;&#27861;&#21487;&#20197;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#65292;&#19982;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#26550;&#26500;&#31867;&#20284;&#65292;&#24182;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#19979;&#30340;&#23567;&#24207;&#21015;&#38271;&#24230;&#19979;&#20248;&#20110;&#21464;&#21387;&#22120;1.5&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.12317</link><description>&lt;p&gt;
&#36845;&#20195;&#20998;&#27573;&#20223;&#23556;&#25554;&#20540;&#65288;IPA&#65289;&#36924;&#36817;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Iterated Piecewise Affine (IPA) Approximation for Language Modeling. (arXiv:2306.12317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12317
&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#20998;&#27573;&#20223;&#23556;&#25554;&#20540;&#65288;IPA&#65289;&#36924;&#36817;&#27861;&#21487;&#20197;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#65292;&#19982;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#26550;&#26500;&#31867;&#20284;&#65292;&#24182;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#19979;&#30340;&#23567;&#24207;&#21015;&#38271;&#24230;&#19979;&#20248;&#20110;&#21464;&#21387;&#22120;1.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#19968;&#38454;&#27888;&#21202;&#23637;&#24320;&#27861;&#26469;&#36924;&#36817;&#19968;&#20010;&#36890;&#29992;&#30340;&#20989;&#25968;F: R^{n x m} -&gt; R^{n x m} &#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#12290;&#20026;&#20102;&#22686;&#24378;&#22522;&#26412;&#30340;&#27888;&#21202;&#23637;&#24320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;&#21644;&#20998;&#27573;&#24314;&#27169;&#65292;&#20174;&#32780;&#21629;&#21517;&#31639;&#27861;&#20026;&#36845;&#20195;&#20998;&#27573;&#20223;&#23556;&#25554;&#20540;&#65288;IPA&#65289;&#36924;&#36817;&#12290;&#26368;&#32456;&#31639;&#27861;&#34920;&#29616;&#20986;&#19982;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#26550;&#26500;&#30456;&#20284;&#30340;&#26377;&#36259;&#29305;&#24449;&#12290;&#36890;&#36807;&#27604;&#36739;IPA&#21644;&#21464;&#21387;&#22120;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#36739;&#23567;&#30340;&#24207;&#21015;&#38271;&#24230;&#19979;&#65292;IPA&#22312;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#20219;&#21153;&#20013;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#27604;&#21464;&#21387;&#22120;&#39640;1.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we demonstrate the application of a simple first-order Taylor expansion to approximate a generic function $F: R^{n \times m} \to R^{n \times m}$ and utilize it in language modeling. To enhance the basic Taylor expansion, we introduce iteration and piecewise modeling, leading us to name the algorithm the Iterative Piecewise Affine (IPA) approximation. The final algorithm exhibits interesting resemblances to the Transformers decoder architecture. By comparing parameter arrangements in IPA and Transformers, we observe a strikingly similar performance, with IPA outperforming Transformers by 1.5\% in the next token prediction task with cross-entropy loss for smaller sequence lengths.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#25152;&#38656;&#30340;&#22806;&#37096;&#30693;&#35782;&#33719;&#21462;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.08129</link><description>&lt;p&gt;
AVIS:&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#35270;&#35273;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
AVIS: Autonomous Visual Information Seeking with Large Language Models. (arXiv:2306.08129v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#25152;&#38656;&#30340;&#22806;&#37096;&#30693;&#35782;&#33719;&#21462;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23454;&#29616;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#30340;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLM&#21160;&#24577;&#22320;&#21046;&#23450;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31574;&#30053;&#65292;&#24182;&#35843;&#26597;&#23427;&#20204;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#33719;&#21462;&#25552;&#20379;&#25152;&#25552;&#20986;&#38382;&#39064;&#25152;&#38656;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#30693;&#35782;&#12290;&#22238;&#31572;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#39064;&#65292;&#22914;&#8220;&#36825;&#24133;&#22270;&#20687;&#25152;&#25551;&#32472;&#30340;&#24314;&#31569;&#29289;&#26159;&#20026;&#20102;&#32426;&#24565;&#21738;&#20010;&#20107;&#20214;&#65311;&#8221;&#65292;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#21576;&#29616;&#20986;&#19968;&#20010;&#32452;&#21512;&#25628;&#32034;&#31354;&#38388;&#65292;&#38656;&#35201;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#21253;&#25324;&#35843;&#29992;API&#12289;&#20998;&#26512;&#23427;&#20204;&#30340;&#21709;&#24212;&#24182;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#25910;&#38598;&#20102;&#20154;&#31867;&#38754;&#23545;&#36825;&#20010;&#20219;&#21153;&#26102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20915;&#31574;&#23454;&#20363;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#30340;&#31995;&#32479;&#65306;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#30830;&#23450;&#19979;&#19968;&#20010;&#35201;&#20351;&#29992;&#30340;&#24037;&#20855;&#65307;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#25512;&#29702;&#22120;&#65292;&#20998;&#26512;&#24182;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as "What event is commemorated by the building depicted in this image?", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information 
&lt;/p&gt;</description></item><item><title>Diable&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#22312;&#34920;&#26684;&#19978;&#36827;&#34892;&#25805;&#20316;&#26469;&#26356;&#26032;&#23545;&#35805;&#29366;&#24577;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26102;&#38388;&#25928;&#29575;&#25552;&#39640;&#20102;2.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#30446;&#26631;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17020</link><description>&lt;p&gt;
Diable: &#22312;&#34920;&#26684;&#19978;&#36827;&#34892;&#30340;&#39640;&#25928;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Diable: Efficient Dialogue State Tracking as Operations on Tables. (arXiv:2305.17020v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17020
&lt;/p&gt;
&lt;p&gt;
Diable&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#22312;&#34920;&#26684;&#19978;&#36827;&#34892;&#25805;&#20316;&#26469;&#26356;&#26032;&#23545;&#35805;&#29366;&#24577;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26102;&#38388;&#25928;&#29575;&#25552;&#39640;&#20102;2.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#30446;&#26631;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#23558;&#23436;&#25972;&#30340;&#23545;&#35805;&#21382;&#21490;&#20316;&#20026;&#36755;&#20837;&#65292;&#23558;&#24403;&#21069;&#29366;&#24577;&#34920;&#31034;&#20026;&#21253;&#21547;&#25152;&#26377;&#27133;&#30340;&#21015;&#34920;&#65292;&#24182;&#22312;&#27599;&#20010;&#23545;&#35805;&#22238;&#21512;&#20013;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#25972;&#20010;&#29366;&#24577;&#12290;&#36825;&#31181;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#65292;&#29305;&#21035;&#26159;&#24403;&#27133;&#30340;&#25968;&#37327;&#24456;&#22810;&#19988;&#23545;&#35805;&#24456;&#38271;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Diable&#65292;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#39640;&#25928;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#23884;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#23545;&#35805;&#29366;&#24577;&#34920;&#31034;&#20026;&#34920;&#26684;&#65292;&#24182;&#23558;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#24418;&#24335;&#21270;&#20026;&#34920;&#26684;&#25805;&#20316;&#20219;&#21153;&#12290;&#22312;&#27599;&#20010;&#22238;&#21512;&#20013;&#65292;&#31995;&#32479;&#36890;&#36807;&#22522;&#20110;&#23545;&#35805;&#19978;&#19979;&#25991;&#29983;&#25104;&#34920;&#26684;&#25805;&#20316;&#26469;&#26356;&#26032;&#20808;&#21069;&#30340;&#29366;&#24577;&#12290;&#22312;MultiWoz&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;Diable (i) &#20248;&#20110;&#24378;&#22823;&#30340;&#39640;&#25928;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22522;&#20934;&#65292;(ii) &#26102;&#38388;&#25928;&#29575;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;2.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;(iii) &#23545;&#26080;&#22122;&#22768;&#30340;&#36755;&#20837;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the number of slots is large and the conversation is long. We propose Diable, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models. We represent the dialogue state as a table and formalise DST as a table manipulation task. At each turn, the system updates the previous state by generating table operations based on the dialogue context. Extensive experimentation on the MultiWoz datasets demonstrates that Diable (i) outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient than current state-of-the-art methods while retaining competitive Joint Goal Accuracy, and (iii) is robust to no
&lt;/p&gt;</description></item><item><title>SOCRATIC QUESTIONING&#26159;&#19968;&#31181;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#20316;&#36827;&#34892;&#36882;&#24402;&#24605;&#32771;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.14999</link><description>&lt;p&gt;
SOCRATIC QUESTIONING&#30340;&#33402;&#26415;&#65306;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36882;&#24402;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models. (arXiv:2305.14999v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14999
&lt;/p&gt;
&lt;p&gt;
SOCRATIC QUESTIONING&#26159;&#19968;&#31181;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#20316;&#36827;&#34892;&#36882;&#24402;&#24605;&#32771;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#65288;Chain-of-Thought&#65292;CoT&#65289;&#25552;&#31034;&#33021;&#22815;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#21463;&#21046;&#20110;&#20854;&#20869;&#22312;&#30340;&#21333;&#36941;&#21644;&#39034;&#24207;&#29983;&#25104;&#36807;&#31243;&#65292;CoT&#22312;&#21021;&#22987;&#20915;&#31574;&#19978;&#36807;&#24230;&#20381;&#36182;&#65292;&#23548;&#33268;&#26089;&#26399;&#27493;&#39588;&#20013;&#30340;&#38169;&#35823;&#32047;&#31215;&#24182;&#24433;&#21709;&#26368;&#32456;&#31572;&#26696;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26102;&#37319;&#29992;&#36882;&#24402;&#24605;&#32500;&#65292;&#21363;&#23558;&#21407;&#38382;&#39064;&#36845;&#20195;&#22320;&#20998;&#35299;&#20026;&#21487;&#22788;&#29702;&#30340;&#23376;&#38382;&#39064;&#65292;&#24182;&#27719;&#24635;&#20854;&#31572;&#26696;&#20197;&#35299;&#20915;&#21407;&#38382;&#39064;&#12290;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SOCRATIC QUESTIONING&#65292;&#19968;&#31181;&#27169;&#25311;&#36882;&#24402;&#24605;&#32500;&#36807;&#31243;&#30340;&#20998;&#32780;&#27835;&#20043;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SOCRATIC QUESTIONING&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#21644;&#22238;&#31572;&#23376;&#38382;&#39064;&#65292;&#30452;&#21040;&#25910;&#38598;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#35299;&#20915;&#21407;&#38382;&#39064;&#12290;&#19982;CoT&#19981;&#21516;&#65292;SOCRATIC QUESTIONING&#26126;&#30830;&#22320;&#23548;&#33322;&#24605;&#32771;&#31354;&#38388;&#65292;&#20419;&#36827;&#26377;&#25928;&#30340;&#36882;&#24402;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) prompting enables large language models to solve complex reasoning problems by generating intermediate steps. However, confined by its inherent single-pass and sequential generation process, CoT heavily relies on the initial decisions, causing errors in early steps to accumulate and impact the final answers. In contrast, humans adopt recursive thinking when tackling complex reasoning problems, i.e., iteratively breaking the original problem into approachable sub-problems and aggregating their answers to resolve the original one. Inspired by the human cognitive process, we propose SOCRATIC QUESTIONING, a divide-and-conquer style algorithm that mimics the recursive thinking process. Specifically, SOCRATIC QUESTIONING leverages large language models to raise and answer sub-questions until collecting enough information to tackle the original question. Unlike CoT, SOCRATIC QUESTIONING explicitly navigates the thinking space, stimulates effective recursive thinking, an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27861;&#24459;&#23450;&#20041;&#20026;&#20013;&#24515;&#30340;&#12289;&#21487;&#27861;&#24459;&#24378;&#21046;&#25191;&#34892;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#65292;&#21033;&#29992;&#27861;&#24459;&#19987;&#23478;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#32467;&#21512;&#22522;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#31526;&#21512;&#30417;&#31649;&#32773;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.13677</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#20849;&#35770;&#22363;&#30340;&#21487;&#27861;&#24459;&#24378;&#21046;&#25191;&#34892;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Legally Enforceable Hate Speech Detection for Public Forums. (arXiv:2305.13677v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27861;&#24459;&#23450;&#20041;&#20026;&#20013;&#24515;&#30340;&#12289;&#21487;&#27861;&#24459;&#24378;&#21046;&#25191;&#34892;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#65292;&#21033;&#29992;&#27861;&#24459;&#19987;&#23478;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#32467;&#21512;&#22522;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#31526;&#21512;&#30417;&#31649;&#32773;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#26159;&#20844;&#20849;&#35770;&#22363;&#19978;&#30340;&#20005;&#37325;&#38382;&#39064;&#65292;&#23545;&#24694;&#24847;&#21644;&#27495;&#35270;&#24615;&#35821;&#35328;&#30340;&#36866;&#24403;&#25191;&#34892;&#26159;&#20445;&#25252;&#20154;&#32676;&#20813;&#21463;&#20260;&#23475;&#21644;&#27495;&#35270;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#20160;&#20040;&#26500;&#25104;&#20167;&#24680;&#35328;&#35770;&#26159;&#19968;&#39033;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#39640;&#24230;&#23481;&#26131;&#21463;&#21040;&#20027;&#35266;&#35299;&#37322;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#20316;&#21697;&#27809;&#26377;&#23558;&#23427;&#20204;&#30340;&#31995;&#32479;&#19982;&#21487;&#25191;&#34892;&#30340;&#20167;&#24680;&#35328;&#35770;&#23450;&#20041;&#23545;&#40784;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#23427;&#20204;&#30340;&#36755;&#20986;&#19982;&#30417;&#31649;&#32773;&#30340;&#30446;&#26631;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#20197;&#27861;&#24459;&#23450;&#20041;&#20026;&#20013;&#24515;&#30340;&#21487;&#25191;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#24182;&#20351;&#29992;&#27861;&#24459;&#19987;&#23478;&#23545;&#36829;&#21453;&#21313;&#19968;&#31181;&#21487;&#33021;&#23450;&#20041;&#36827;&#34892;&#20102;&#25968;&#25454;&#38598;&#27880;&#37322;&#12290;&#32771;&#34385;&#21040;&#30830;&#23450;&#28165;&#26224;&#12289;&#21487;&#27861;&#24459;&#24378;&#21046;&#25191;&#34892;&#30340;&#20167;&#24680;&#35328;&#35770;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#23478;&#29983;&#25104;&#30340;&#26679;&#26412;&#21644;&#33258;&#21160;&#25366;&#25496;&#30340;&#25361;&#25112;&#38598;&#22686;&#24378;&#20102;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#30340;&#25552;&#31034;&#26469;&#22522;&#20110;&#36825;&#20123;&#23450;&#20041;&#26469;&#20915;&#23450;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;&#20960;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech is a serious issue on public forums, and proper enforcement of hate speech laws is key for protecting groups of people against harmful and discriminatory language. However, determining what constitutes hate speech is a complex task that is highly open to subjective interpretations. Existing works do not align their systems with enforceable definitions of hate speech, which can make their outputs inconsistent with the goals of regulators. Our work introduces a new task for enforceable hate speech detection centred around legal definitions, and a dataset annotated on violations of eleven possible definitions by legal experts. Given the challenge of identifying clear, legally enforceable instances of hate speech, we augment the dataset with expert-generated samples and an automatically mined challenge set. We experiment with grounding the model decision in these definitions using zero-shot and few-shot prompting. We then report results on several large language models (LLMs). 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24320;&#25918;&#19990;&#30028;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24050;&#30693;&#31867;&#21644;&#26032;&#39062;&#31867;&#20013;&#36827;&#34892;&#26174;&#24335;&#21644;&#38544;&#24335;&#34920;&#31034;&#30340;&#20851;&#31995;&#20998;&#31867;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#30340;&#29305;&#24449;&#19979;&#36827;&#34892;&#20102;&#20004;&#20010;&#20851;&#38190;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.13533</link><description>&lt;p&gt;
&#23454;&#29616;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#23545;&#40784;&#30340;&#24320;&#25918;&#19990;&#30028;&#21322;&#30417;&#30563;&#24191;&#20041;&#20851;&#31995;&#21457;&#29616; (arXiv:2305.13533v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Open-world Semi-supervised Generalized Relation Discovery Aligned in a Real-world Setting. (arXiv:2305.13533v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24320;&#25918;&#19990;&#30028;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24050;&#30693;&#31867;&#21644;&#26032;&#39062;&#31867;&#20013;&#36827;&#34892;&#26174;&#24335;&#21644;&#38544;&#24335;&#34920;&#31034;&#30340;&#20851;&#31995;&#20998;&#31867;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#30340;&#29305;&#24449;&#19979;&#36827;&#34892;&#20102;&#20004;&#20010;&#20851;&#38190;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#20851;&#31995;&#25277;&#21462;(OpenRE)&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#31616;&#21270;&#20102;&#38382;&#39064;&#65292;&#20551;&#35774;&#25152;&#26377;&#26410;&#26631;&#35760;&#30340;&#25991;&#26412;&#37117;&#23646;&#20110;&#26032;&#31867;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;OpenRE&#35774;&#32622;&#24212;&#26356;&#31526;&#21512;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#25913;&#36827;:(a)&#26410;&#26631;&#35760;&#25968;&#25454;&#24212;&#21253;&#25324;&#24050;&#30693;&#21644;&#26032;&#39062;&#30340;&#31867;&#65292;&#21253;&#25324;&#38590;&#20197;&#21306;&#20998;&#30340;&#36127;&#26679;&#26412;&#23454;&#20363;;(b)&#26032;&#39062;&#30340;&#31867;&#38598;&#24212;&#35813;&#20195;&#34920;&#38271;&#23614;&#20851;&#31995;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#27969;&#34892;&#30340;&#20851;&#31995;&#65292;&#22914;&#26631;&#39064;&#21644;&#20301;&#32622;&#65292;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#30340;&#27169;&#24335;&#38544;&#21547;&#22320;&#25512;&#26029;&#65292;&#32780;&#38271;&#23614;&#30340;&#20851;&#31995;&#20542;&#21521;&#20110;&#22312;&#21477;&#23376;&#20013;&#26126;&#30830;&#34920;&#31034;&#12290;&#22312;&#36825;&#20123;&#35265;&#35299;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KNoRD&#65288;&#24050;&#30693;&#21644;&#26032;&#39062;&#20851;&#31995;&#21457;&#29616;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23545;&#24050;&#30693;&#31867;&#21644;&#26032;&#39062;&#31867;&#20013;&#30340;&#26174;&#24335;&#21644;&#38544;&#24335;&#34920;&#31034;&#30340;&#20851;&#31995;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-world Relation Extraction (OpenRE) has recently garnered significant attention. However, existing approaches tend to oversimplify the problem by assuming that all unlabeled texts belong to novel classes, thereby limiting the practicality of these methods. We argue that the OpenRE setting should be more aligned with the characteristics of real-world data. Specifically, we propose two key improvements: (a) unlabeled data should encompass known and novel classes, including hard-negative instances; and (b) the set of novel classes should represent long-tail relation types. Furthermore, we observe that popular relations such as titles and locations can often be implicitly inferred through specific patterns, while long-tail relations tend to be explicitly expressed in sentences. Motivated by these insights, we present a novel method called KNoRD (Known and Novel Relation Discovery), which effectively classifies explicitly and implicitly expressed relations from known and novel classes w
&lt;/p&gt;</description></item><item><title>SEAHORSE&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#35821;&#35328;&#12289;&#22810;&#26041;&#38754;&#25688;&#35201;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;96K&#20010;&#25688;&#35201;&#65292;&#28085;&#30422;6&#31181;&#35821;&#35328;&#12289;9&#20010;&#31995;&#32479;&#21644;4&#20010;&#25968;&#25454;&#38598;&#12290;SEAHORSE&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#24230;&#37327;&#21644;&#35757;&#32451;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#36164;&#28304;&#12290;&#20351;&#29992;SEAHORSE&#35757;&#32451;&#30340;&#24230;&#37327;&#22312;&#39046;&#22495;&#22806;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13194</link><description>&lt;p&gt;
SEAHORSE: &#22810;&#35821;&#35328;&#12289;&#22810;&#26041;&#38754;&#25688;&#35201;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation. (arXiv:2305.13194v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13194
&lt;/p&gt;
&lt;p&gt;
SEAHORSE&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#35821;&#35328;&#12289;&#22810;&#26041;&#38754;&#25688;&#35201;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;96K&#20010;&#25688;&#35201;&#65292;&#28085;&#30422;6&#31181;&#35821;&#35328;&#12289;9&#20010;&#31995;&#32479;&#21644;4&#20010;&#25968;&#25454;&#38598;&#12290;SEAHORSE&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#24230;&#37327;&#21644;&#35757;&#32451;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#36164;&#28304;&#12290;&#20351;&#29992;SEAHORSE&#35757;&#32451;&#30340;&#24230;&#37327;&#22312;&#39046;&#22495;&#22806;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#33258;&#21160;&#25688;&#35201;&#35780;&#20272;&#30001;&#20110;&#20219;&#21153;&#30340;&#22810;&#26041;&#38754;&#21644;&#20027;&#35266;&#24615;&#36136;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23588;&#20854;&#23545;&#20110;&#38500;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#65292;&#20154;&#24037;&#35780;&#20272;&#31232;&#32570;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SEAHORSE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#35821;&#35328;&#12289;&#22810;&#26041;&#38754;&#25688;&#35201;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#12290;SEAHORSE&#21253;&#21547;96K&#20010;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;6&#31181;&#35821;&#35328;&#12289;9&#20010;&#31995;&#32479;&#21644;4&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#26681;&#25454;&#25991;&#26412;&#36136;&#37327;&#30340;6&#20010;&#32500;&#24230;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20998;&#65306;&#21487;&#29702;&#35299;&#24615;&#12289;&#37325;&#22797;&#24615;&#12289;&#35821;&#27861;&#12289;&#24402;&#22240;&#12289;&#20027;&#35201;&#35266;&#28857;&#21644;&#31616;&#27905;&#24615;&#12290;&#30001;&#20110;&#20854;&#35268;&#27169;&#21644;&#33539;&#22260;&#30340;&#21407;&#22240;&#65292;SEAHORSE&#26082;&#21487;&#20197;&#20316;&#20026;&#35780;&#20272;&#23398;&#20064;&#24230;&#37327;&#30340;&#22522;&#20934;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#35757;&#32451;&#36825;&#20123;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#36164;&#28304;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;SEAHORSE&#35757;&#32451;&#30340;&#24230;&#37327;&#22312;&#39046;&#22495;&#22806;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;TRUE&#21644;mFACE&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;SEAHORSE&#25968;&#25454;&#38598;&#21644;&#24230;&#37327;&#20844;&#24320;&#25552;&#20379;&#65292;&#20197;&#20379;&#26410;&#26469;&#30340;&#22810;&#35821;&#35328;&#21644;&#22810;
&lt;/p&gt;
&lt;p&gt;
Reliable automatic evaluation of summarization systems is challenging due to the multifaceted and subjective nature of the task. This is especially the case for languages other than English, where human evaluations are scarce. In this work, we introduce SEAHORSE, a dataset for multilingual, multifaceted summarization evaluation. SEAHORSE consists of 96K summaries with human ratings along 6 dimensions of text quality: comprehensibility, repetition, grammar, attribution, main ideas, and conciseness, covering 6 languages, 9 systems and 4 datasets. As a result of its size and scope, SEAHORSE can serve both as a benchmark to evaluate learnt metrics, as well as a large-scale resource for training such metrics. We show that metrics trained with SEAHORSE achieve strong performance on the out-of-domain meta-evaluation benchmarks TRUE (Honovich et al., 2022) and mFACE (Aharoni et al., 2022). We make the SEAHORSE dataset and metrics publicly available for future research on multilingual and multi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;ICA&#25581;&#31034;&#20102;&#23884;&#20837;&#20013;&#30340;&#36890;&#29992;&#20960;&#20309;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#27599;&#20010;&#23884;&#20837;&#21487;&#20197;&#30001;&#23569;&#37327;&#20869;&#22312;&#21487;&#35299;&#37322;&#36724;&#30340;&#32452;&#21512;&#34920;&#31034;&#65292;&#24182;&#19988;&#36825;&#20123;&#35821;&#20041;&#36724;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#31639;&#27861;&#21644;&#27169;&#24577;&#19979;&#20445;&#25345;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2305.13175</link><description>&lt;p&gt;
&#20351;&#29992;ICA&#21457;&#29616;&#23884;&#20837;&#20013;&#30340;&#36890;&#29992;&#20960;&#20309;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Discovering Universal Geometry in Embeddings with ICA. (arXiv:2305.13175v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;ICA&#25581;&#31034;&#20102;&#23884;&#20837;&#20013;&#30340;&#36890;&#29992;&#20960;&#20309;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#27599;&#20010;&#23884;&#20837;&#21487;&#20197;&#30001;&#23569;&#37327;&#20869;&#22312;&#21487;&#35299;&#37322;&#36724;&#30340;&#32452;&#21512;&#34920;&#31034;&#65292;&#24182;&#19988;&#36825;&#20123;&#35821;&#20041;&#36724;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#31639;&#27861;&#21644;&#27169;&#24577;&#19979;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#25581;&#31034;&#20102;&#21333;&#35789;&#25110;&#22270;&#20687;&#23884;&#20837;&#20013;&#30340;&#19968;&#33268;&#35821;&#20041;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#20013;&#30333;&#21270;&#36807;&#31243;&#21518;&#27531;&#30041;&#30340;&#21508;&#21521;&#24322;&#24615;&#20449;&#24687;&#65292;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#20013;&#25552;&#21462;&#29420;&#31435;&#30340;&#35821;&#20041;&#25104;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#20010;&#23884;&#20837;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#23569;&#37327;&#20869;&#22312;&#21487;&#35299;&#37322;&#36724;&#30340;&#32452;&#21512;&#65292;&#24182;&#19988;&#36825;&#20123;&#35821;&#20041;&#36724;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#31639;&#27861;&#21644;&#27169;&#24577;&#19979;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#23884;&#20837;&#30340;&#20960;&#20309;&#27169;&#24335;&#20013;&#21457;&#29616;&#36890;&#29992;&#35821;&#20041;&#32467;&#26500;&#65292;&#22686;&#24378;&#20102;&#25105;&#20204;&#23545;&#23884;&#20837;&#20013;&#34920;&#31034;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study utilizes Independent Component Analysis (ICA) to unveil a consistent semantic structure within embeddings of words or images. Our approach extracts independent semantic components from the embeddings of a pre-trained model by leveraging anisotropic information that remains after the whitening process in Principal Component Analysis (PCA). We demonstrate that each embedding can be expressed as a composition of a few intrinsic interpretable axes and that these semantic axes remain consistent across different languages, algorithms, and modalities. The discovery of a universal semantic structure in the geometric patterns of embeddings enhances our understanding of the representations in embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#32463;&#39564;&#24615;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#26368;&#22810;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;Spoken&#29256;&#26412;&#30340;&#25991;&#26412;&#22522;&#20934;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#27169;&#22411;&#35780;&#20272;&#21644;&#25512;&#21160;&#26410;&#26469;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.13009</link><description>&lt;p&gt;
&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Textually Pretrained Speech Language Models. (arXiv:2305.13009v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#32463;&#39564;&#24615;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#26368;&#22810;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;Spoken&#29256;&#26412;&#30340;&#25991;&#26412;&#22522;&#20934;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#27169;&#22411;&#35780;&#20272;&#21644;&#25512;&#21160;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SpeechLMs&#65289;&#20165;&#22788;&#29702;&#21644;&#29983;&#25104;&#38899;&#39057;&#25968;&#25454;&#65292;&#27809;&#26377;&#25991;&#23383;&#30417;&#30563;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TWIST&#65292;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;SpeechLMs&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;TWIST&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#20248;&#20110;&#20919;&#21551;&#21160;&#30340;SpeechLM&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#65288;&#22914;&#35821;&#38899;&#20998;&#35789;&#22120;&#12289;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#22312;&#26500;&#24314;&#24615;&#33021;&#26356;&#22909;&#30340;SpeechLMs&#26041;&#38754;&#37117;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36804;&#20170;&#20026;&#27490;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#26368;&#22810;&#30340;SpeechLM&#65288;&#25454;&#25105;&#20204;&#25152;&#30693;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;Spoken&#29256;&#26412;&#30340;StoryCloze&#25991;&#26412;&#22522;&#20934;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#27169;&#22411;&#35780;&#20272;&#24182;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;&#25105;&#20204;&#20844;&#24320;&#25552;&#20379;&#35821;&#38899;&#26679;&#26412;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#65306;https://pages.cs.huji.ac.il/
&lt;/p&gt;
&lt;p&gt;
Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102; TempoSum &#25277;&#35937;&#25688;&#35201;&#30340;&#26102;&#38388;&#27867;&#21270;&#33021;&#21147;&#22522;&#20934;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#35777;&#26126;&#20102;&#25688;&#35201;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#23545;&#26410;&#26469;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#25688;&#35201;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01951</link><description>&lt;p&gt;
TempoSum&#65306;&#35780;&#20272;&#25277;&#35937;&#25688;&#35201;&#30340;&#26102;&#38388;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
TempoSum: Evaluating the Temporal Generalization of Abstractive Summarization. (arXiv:2305.01951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102; TempoSum &#25277;&#35937;&#25688;&#35201;&#30340;&#26102;&#38388;&#27867;&#21270;&#33021;&#21147;&#22522;&#20934;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#35777;&#26126;&#20102;&#25688;&#35201;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#23545;&#26410;&#26469;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#25688;&#35201;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#26377;&#30340;&#25277;&#35937;&#25688;&#35201;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26377; promising &#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25688;&#35201;&#22522;&#20934;&#19982;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#24494;&#35843;&#25968;&#25454;&#38598;&#22312;&#26102;&#38388;&#19978;&#37325;&#21472;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#24615;&#33021;&#21487;&#33021;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#25152;&#35760;&#24518;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25152;&#35760;&#24518;&#30340;&#30693;&#35782;&#21487;&#33021;&#24456;&#24555;&#23601;&#36807;&#26102;&#65292;&#36825;&#20250;&#24433;&#21709;&#21040;&#23427;&#20204;&#22312;&#26410;&#26469;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#20102;&#35299;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#30340;&#26102;&#38388;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; TempoSum&#65292;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20174; 2010 &#24180;&#21040; 2022 &#24180;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25688;&#35201;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#23545;&#26410;&#26469;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#26041;&#27861;&#19981;&#33021;&#21487;&#38752;&#22320;&#25552;&#39640;&#25688;&#35201;&#27169;&#22411;&#22312;&#26410;&#26469;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent pre-trained language models (PLMs) achieve promising results in existing abstractive summarization datasets. However, existing summarization benchmarks overlap in time with the standard pre-training corpora and finetuning datasets. Hence, the strong performance of PLMs may rely on the parametric knowledge that is memorized during pre-training and fine-tuning. Moreover, the knowledge memorized by PLMs may quickly become outdated, which affects the generalization performance of PLMs on future data. In this work, we propose TempoSum, a novel benchmark that contains data samples from 2010 to 2022, to understand the temporal generalization ability of abstractive summarization models. Through extensive human evaluation, we show that parametric knowledge stored in summarization models significantly affects the faithfulness of the generated summaries on future data. Moreover, existing faithfulness enhancement methods cannot reliably improve the faithfulness of summarization models on fu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>ParroT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#28304;LLM&#21644;&#20154;&#24037;&#32534;&#20889;&#30340;&#32763;&#35793;&#35780;&#20272;&#25968;&#25454;&#30340;&#32842;&#22825;&#32763;&#35793;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#32763;&#35793;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25191;&#34892;&#26679;&#24335;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#35201;&#27714;&#26469;&#35268;&#33539;&#32763;&#35793;&#36807;&#31243;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; ParroT &#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.02426</link><description>&lt;p&gt;
ParroT: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32842;&#22825;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
ParroT: Translating During Chat Using Large Language Models. (arXiv:2304.02426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02426
&lt;/p&gt;
&lt;p&gt;
ParroT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#28304;LLM&#21644;&#20154;&#24037;&#32534;&#20889;&#30340;&#32763;&#35793;&#35780;&#20272;&#25968;&#25454;&#30340;&#32842;&#22825;&#32763;&#35793;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#32763;&#35793;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25191;&#34892;&#26679;&#24335;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#35201;&#27714;&#26469;&#35268;&#33539;&#32763;&#35793;&#36807;&#31243;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; ParroT &#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914; ChatGPT &#21644; GPT-4 &#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#22312;&#32842;&#22825;&#36807;&#31243;&#20013;&#23436;&#25104;&#21508;&#31181;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#21482;&#33021;&#36890;&#36807;&#21463;&#38480;&#30340;API&#35775;&#38382;&#65292;&#36825;&#20026;&#26032;&#30340;&#30740;&#31350;&#21644;&#39046;&#22495;&#36827;&#23637;&#24102;&#26469;&#20102;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ParroT &#26694;&#26550;&#65292;&#22522;&#20110;&#24320;&#28304;LLM&#65288;&#22914;LLaMA-7b&#65289;&#21644;&#20154;&#24037;&#32534;&#20889;&#30340;&#32763;&#35793;&#35780;&#20272;&#25968;&#25454;&#26469;&#22686;&#24378;&#21644;&#35268;&#33539;&#32842;&#22825;&#32763;&#35793;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ParroT&#23558;&#32763;&#35793;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25191;&#34892;&#30340;&#26679;&#24335;&#65292;&#24182;&#24341;&#20837; "Hint " &#23383;&#27573;&#20197;&#21152;&#20837;&#39069;&#22806;&#35201;&#27714;&#26469;&#35268;&#33539;&#32763;&#35793;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#25351;&#20196;&#31867;&#22411;&#26469;&#24494;&#35843; ParroT &#27169;&#22411;&#65292;&#21253;&#25324;&#32763;&#35793;&#25351;&#20196;&#12289;&#23545;&#27604;&#25351;&#20196;&#21644;&#35823;&#24046;&#24341;&#23548;&#25351;&#20196;&#12290;&#22312;&#20004;&#20010; Flores &#23376;&#38598;&#21644; WMT22 &#27979;&#35797;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992; ParroT &#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#19988;&#38656;&#35201;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable abilities on a wide range of natural language processing (NLP) tasks, including various machine translation abilities accomplished during chat. However, these models are only accessible through restricted APIs, which creates barriers to new research and advancements in the field. Therefore, we propose the $\mathbf{ParroT}$ framework to enhance and regulate the translation abilities during chat based on open-sourced LLMs (i.e., LLaMA-7b) and human written translation and evaluation data. Specifically, ParroT reformulates translation data into the instruction-following style, and introduces a "Hint" field for incorporating extra requirements to regulate the translation process. Accordingly, we propose three instruction types for finetuning ParroT models, including translation instruction, contrastive instruction, and error-guided instruction. Experiments on two Flores subsets and WMT22 test sets suggest that tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17760</link><description>&lt;p&gt;
CAMEL: &#29992;&#20110;&#8220;&#24515;&#26234;&#8221;&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#31038;&#32676;&#30340;&#20132;&#20114;&#24335;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society. (arXiv:2303.17760v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#24050;&#21462;&#24471;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20154;&#31867;&#30340;&#25351;&#23548;&#65292;&#20197;&#24341;&#23548;&#23545;&#35805;&#65292;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26500;&#24314;&#21487;&#25193;&#23637;&#25216;&#26415;&#20197;&#20419;&#36827;&#20132;&#20114;&#24335;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#24182;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#30340;&#8220;&#35748;&#30693;&#8221;&#36807;&#31243;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#23454;&#29616;&#33258;&#20027;&#21512;&#20316;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#21551;&#21160;&#25552;&#31034;&#26469;&#24341;&#23548;&#32842;&#22825;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20154;&#31867;&#24847;&#22270;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35282;&#33394;&#25198;&#28436;&#26469;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#32842;&#22825;&#20195;&#29702;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of conversational and chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and provide insight into their "cognitive" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of chat agents, providing a valuable resource for investigating conversational language models. Our contributions include introducing a novel communicative agent framewor
&lt;/p&gt;</description></item><item><title>DeltaScore&#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#26469;&#35780;&#20272;&#25925;&#20107;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#26041;&#38754;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#25200;&#21160;&#21069;&#21518;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#26469;&#34913;&#37327;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.08991</link><description>&lt;p&gt;
DeltaScore: &#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#35780;&#20215;&#25925;&#20107;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DeltaScore: Evaluating Story Generation with Differentiating Perturbations. (arXiv:2303.08991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08991
&lt;/p&gt;
&lt;p&gt;
DeltaScore&#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#26469;&#35780;&#20272;&#25925;&#20107;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#26041;&#38754;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#25200;&#21160;&#21069;&#21518;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#26469;&#34913;&#37327;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#21508;&#31181;&#35780;&#20215;&#25351;&#26631;&#23384;&#22312;&#65292;&#20294;&#23545;&#20110;&#25925;&#20107;&#29983;&#25104;&#30340;&#23454;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#19981;&#24378;&#65292;&#20063;&#19981;&#33021;&#27979;&#37327;&#32454;&#31890;&#24230;&#30340;&#25925;&#20107;&#26041;&#38754;&#65292;&#20363;&#22914;&#27969;&#30021;&#24230;&#19982;&#30456;&#20851;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26088;&#22312;&#35780;&#20272;&#25972;&#20307;&#29983;&#25104;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;DeltaScore&#65292;&#19968;&#31181;&#21033;&#29992;&#25200;&#21160;&#26469;&#35780;&#20272;&#32454;&#31890;&#24230;&#25925;&#20107;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22522;&#20110;&#36825;&#26679;&#30340;&#20551;&#35774;&#65306;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#34920;&#29616;&#24471;&#36234;&#22909;&#65288;&#20363;&#22914;&#27969;&#30021;&#24230;&#65289;&#65292;&#23427;&#23601;&#20250;&#21463;&#21040;&#29305;&#23450;&#25200;&#21160;&#65288;&#20363;&#22914;&#24341;&#20837;&#38169;&#21035;&#23383;&#65289;&#30340;&#24433;&#21709;&#36234;&#22823;&#12290;&#20026;&#20102;&#34913;&#37327;&#24433;&#21709;&#65292;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#25200;&#21160;&#21069;&#21518;&#25925;&#20107;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#20351;&#29992;DeltaScore&#35780;&#20272;&#20102;&#22522;&#20110;&#29366;&#24577;&#30340;&#26368;&#26032;&#27169;&#22411;&#21644;&#20256;&#32479;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25351;&#26631;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#19982;&#20154;&#31867;&#22312;&#20116;&#20010;&#32454;&#31890;&#24230;&#25925;&#20107;&#26041;&#38754;&#30340;&#21028;&#26029;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various evaluation metrics exist for natural language generation tasks, but they have limited utility for story generation since they generally do not correlate well with human judgments and do not measure fine-grained story aspects, such as fluency versus relatedness, as they are intended to assess overall generation quality. In this paper, we propose deltascore, an approach that utilizes perturbation to evaluate fine-grained story aspects. Our core idea is based on the hypothesis that the better the story performs in a specific aspect (e.g., fluency), the more it will be affected by a particular perturbation (e.g., introducing typos). To measure the impact, we calculate the likelihood difference between the pre- and post-perturbation stories using a language model. We evaluate deltascore against state-of-the-art model-based and traditional similarity-based metrics across multiple story domains, and investigate its correlation with human judgments on five fine-grained story aspects: f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04391</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#26032;&#26631;&#31614;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#24320;&#21457;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;90&#20998;&#20197;&#19978;&#30340;&#25104;&#32489;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#20986;&#22122;&#22768;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#20154;&#31867;&#26631;&#35760;&#30340;&#21442;&#32771;&#26469;&#37325;&#26032;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24819;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24207;&#21015;&#26631;&#35760;&#12289;&#29289;&#20307;&#26816;&#27979;&#12289;&#24207;&#21015;&#29983;&#25104;&#12289;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;ChatGPT&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#22312;&#39640;&#36164;&#28304;&#27431;&#27954;&#35821;&#35328;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#25110;&#36828;&#31243;&#35821;&#35328;&#19978;&#34920;&#29616;&#28382;&#21518;&#65307;&#37319;&#29992;&#26530;&#36724;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36828;&#31243;&#35821;&#35328;&#32763;&#35793;&#30340;&#24615;&#33021;&#65307;&#22312;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#25110;Reddit&#35780;&#35770;&#26041;&#38754;&#65292;ChatGPT&#30340;&#34920;&#29616;&#19981;&#22914;&#21830;&#19994;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2301.08745</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#32763;&#35793;&#24341;&#25806;&#65292;&#20381;&#36182;&#20110;GPT-4&#65292;&#26159;&#19968;&#31181;&#22909;&#30340;&#32763;&#35793;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine. (arXiv:2301.08745v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;ChatGPT&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#22312;&#39640;&#36164;&#28304;&#27431;&#27954;&#35821;&#35328;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#25110;&#36828;&#31243;&#35821;&#35328;&#19978;&#34920;&#29616;&#28382;&#21518;&#65307;&#37319;&#29992;&#26530;&#36724;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36828;&#31243;&#35821;&#35328;&#32763;&#35793;&#30340;&#24615;&#33021;&#65307;&#22312;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#25110;Reddit&#35780;&#35770;&#26041;&#38754;&#65292;ChatGPT&#30340;&#34920;&#29616;&#19981;&#22914;&#21830;&#19994;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#23545;ChatGPT&#36827;&#34892;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#21021;&#27493;&#35780;&#20272;&#65292;&#21253;&#25324;&#32763;&#35793;&#25552;&#31034;&#12289;&#22810;&#35821;&#35328;&#32763;&#35793;&#21644;&#32763;&#35793;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;ChatGPT&#24314;&#35758;&#30340;&#25552;&#31034;&#26469;&#35302;&#21457;&#20854;&#32763;&#35793;&#33021;&#21147;&#65292;&#21457;&#29616;&#20505;&#36873;&#25552;&#31034;&#36890;&#24120;&#36816;&#34892;&#33391;&#22909;&#65292;&#34920;&#29616;&#20986;&#36731;&#24494;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#22312;&#39640;&#36164;&#28304;&#30340;&#27431;&#27954;&#35821;&#35328;&#19978;&#65292;ChatGPT&#30340;&#34920;&#29616;&#19982;&#21830;&#19994;&#32763;&#35793;&#20135;&#21697;&#65288;&#20363;&#22914;Google&#32763;&#35793;&#65289;&#30456;&#24403;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#25110;&#36828;&#31243;&#35821;&#35328;&#19978;&#34920;&#29616;&#26174;&#33879;&#28382;&#21518;&#12290;&#23545;&#20110;&#36828;&#31243;&#35821;&#35328;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26377;&#36259;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;$\mathbf{&#26530;&#36724;&#25552;&#31034;}$&#65292;&#21363;&#35753;ChatGPT&#20808;&#23558;&#28304;&#35821;&#35328;&#21477;&#23376;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#30340;&#36724;&#35821;&#35328;&#65292;&#20877;&#32763;&#35793;&#30446;&#26631;&#35821;&#35328;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#32763;&#35793;&#24615;&#33021;&#12290;&#33267;&#20110;&#32763;&#35793;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#25110;Reddit&#35780;&#35770;&#26041;&#38754;&#65292;ChatGPT&#30340;&#34920;&#29616;&#19981;&#22914;&#21830;&#19994;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report provides a preliminary evaluation of ChatGPT for machine translation, including translation prompt, multilingual translation, and translation robustness. We adopt the prompts advised by ChatGPT to trigger its translation ability and find that the candidate prompts generally work well and show minor performance differences. By evaluating on a number of benchmark test sets, we find that ChatGPT performs competitively with commercial translation products (e.g., Google Translate) on high-resource European languages but lags behind significantly on low-resource or distant languages. For distant languages, we explore an interesting strategy named $\mathbf{pivot~prompting}$ that asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, which improves the translation performance significantly. As for the translation robustness, ChatGPT does not perform as well as the commercial systems on biomedical abstracts or Reddit commen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#36339;&#23383;&#27169;&#22411;&#21644;&#36127;&#37319;&#26679;&#26041;&#27861;&#20013;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#24179;&#26041;&#33539;&#25968;&#32534;&#30721;&#20102;&#35789;&#25152;&#20256;&#36798;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#36890;&#36807;&#19982;&#35821;&#26009;&#24211;&#20013;&#21333;&#35789;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#23450;&#20041;&#65292;&#21487;&#29992;&#20110;&#20851;&#38190;&#35789;&#25552;&#21462;&#12289;&#35789;&#24615;&#21306;&#20998;&#21644;&#19978;&#20301;&#35789;&#20998;&#31867;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2212.09663</link><description>&lt;p&gt;
&#35789;&#21521;&#37327;&#30340;&#33539;&#25968;&#32534;&#30721;&#20449;&#24687;&#22686;&#30410;
&lt;/p&gt;
&lt;p&gt;
Norm of Word Embedding Encodes Information Gain. (arXiv:2212.09663v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#36339;&#23383;&#27169;&#22411;&#21644;&#36127;&#37319;&#26679;&#26041;&#27861;&#20013;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#24179;&#26041;&#33539;&#25968;&#32534;&#30721;&#20102;&#35789;&#25152;&#20256;&#36798;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#36890;&#36807;&#19982;&#35821;&#26009;&#24211;&#20013;&#21333;&#35789;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#23450;&#20041;&#65292;&#21487;&#29992;&#20110;&#20851;&#38190;&#35789;&#25552;&#21462;&#12289;&#35789;&#24615;&#21306;&#20998;&#21644;&#19978;&#20301;&#35789;&#20998;&#31867;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#32534;&#30721;&#20102;&#35789;&#27719;&#35821;&#20041;&#20449;&#24687;&#65292;&#20294;&#26159;&#32534;&#30721;&#20102;&#21738;&#20123;&#31867;&#22411;&#30340;&#20449;&#24687;&#65311;&#20197;&#21450;&#22914;&#20309;&#32534;&#30721;&#65311;&#26412;&#25991;&#38024;&#23545;&#36339;&#23383;&#27169;&#22411;&#21644;&#36127;&#37319;&#26679;&#26041;&#27861;&#65292;&#21457;&#29616;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#24179;&#26041;&#33539;&#25968;&#32534;&#30721;&#20102;&#35789;&#25152;&#20256;&#36798;&#30340;&#20449;&#24687;&#22686;&#30410;&#65307;&#32780;&#20449;&#24687;&#22686;&#30410;&#26159;&#36890;&#36807;&#35789;&#22312;&#20849;&#29616;&#20998;&#24067;&#21644;&#35821;&#26009;&#24211;&#30340;&#21333;&#35789;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#23450;&#20041;&#30340;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;&#36890;&#36807;&#25351;&#25968;&#26063;&#27010;&#29575;&#20998;&#24067;&#30340;&#29702;&#35770;&#26694;&#26550;&#35828;&#26126;&#30340;&#65292;&#24182;&#36890;&#36807;&#28040;&#38500;&#35789;&#39057;&#24341;&#36215;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#31934;&#23494;&#23454;&#39564;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#26080;&#35770;&#26159;KL&#25955;&#24230;&#36824;&#26159;&#35789;&#23884;&#20837;&#30340;&#24179;&#26041;&#33539;&#25968;&#65292;&#22312;&#20851;&#38190;&#35789;&#25552;&#21462;&#12289;&#35789;&#24615;&#21306;&#20998;&#21644;&#19978;&#20301;&#35789;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#37117;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#35789;&#20449;&#24687;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed representations of words encode lexical semantic information, but what type of information is encoded, and how? Focusing on the skip-gram with negative-sampling method, we found that the squared norm of static word embedding encodes the information gain conveyed by the word; the information gain is defined by the Kullback-Leibler divergence of the co-occurrence distribution of the word to the unigram distribution of the corpus. Our findings are explained by the theoretical framework of the exponential family of probability distributions and confirmed through precise experiments that remove spurious correlations arising from word frequency. We demonstrate that both the KL divergence and the squared norm of embedding provide a useful metric of a word's informativeness in tasks such as keyword extraction, part-of-speech discrimination, and hypernym classification.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAINPROOF&#30340;&#30456;&#23545;&#20449;&#24687;&#25237;&#24433;OOD&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#21033;&#29992;&#36719;&#27010;&#29575;&#36827;&#34892;&#26816;&#27979;&#12290;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#23454;&#38469;&#30340;OOD&#26816;&#27979;&#35780;&#20272;&#35774;&#32622;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;OOD&#26816;&#27979;&#19981;&#19968;&#23450;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#24230;&#37327;&#30456;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2212.09171</link><description>&lt;p&gt;
Rainproof:&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#25991;&#26412;&#29983;&#25104;&#22120;&#20813;&#21463;&#26469;&#33258;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#38632;&#20254;
&lt;/p&gt;
&lt;p&gt;
Rainproof: An Umbrella To Shield Text Generators From Out-Of-Distribution Data. (arXiv:2212.09171v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09171
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAINPROOF&#30340;&#30456;&#23545;&#20449;&#24687;&#25237;&#24433;OOD&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#21033;&#29992;&#36719;&#27010;&#29575;&#36827;&#34892;&#26816;&#27979;&#12290;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#23454;&#38469;&#30340;OOD&#26816;&#27979;&#35780;&#20272;&#35774;&#32622;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;OOD&#26816;&#27979;&#19981;&#19968;&#23450;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#24230;&#37327;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#30340;NLP&#27169;&#22411;&#20013;&#65292;&#20174;&#32763;&#35793;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#23454;&#26045;&#26377;&#25928;&#30340;&#25511;&#21046;&#26426;&#21046;&#20197;&#30830;&#20445;&#27491;&#30830;&#36816;&#34892;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#30830;&#20445;&#23433;&#20840;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;Out-Of-Distribution&#65288;OOD&#65289;&#26816;&#27979;&#65292;&#26088;&#22312;&#26816;&#27979;&#36755;&#20837;&#26679;&#26412;&#26159;&#21542;&#19982;&#35757;&#32451;&#20998;&#24067;&#32479;&#35745;&#19978;&#36807;&#20110;&#20559;&#31163;&#12290;&#23613;&#31649;OOD&#26816;&#27979;&#26159;&#20998;&#31867;&#20219;&#21153;&#20013;&#24191;&#27867;&#35752;&#35770;&#30340;&#35805;&#39064;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#32534;&#30721;&#22120;&#36755;&#20986;&#30340;&#38544;&#34255;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#40657;&#30418;&#26694;&#26550;&#20013;&#21033;&#29992;&#36719;&#27010;&#29575;&#65292;&#21363;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#36719;&#39044;&#27979;&#20294;&#19981;&#33021;&#35775;&#38382;&#27169;&#22411;&#30340;&#20869;&#37096;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;i&#65289;RAINPROOF&#30456;&#23545;&#20449;&#24687;&#25237;&#24433;OOD&#26816;&#27979;&#26694;&#26550;&#65307;&#21644;&#65288;ii&#65289;&#19968;&#31181;&#26356;&#23454;&#38469;&#30340;OOD&#26816;&#27979;&#35780;&#20272;&#35774;&#32622;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;OOD&#26816;&#27979;&#19981;&#19968;&#23450;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#24230;&#37327;&#30456;&#19968;&#33268;&#12290;OOD&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#36807;&#28388;&#25481;&#27169;&#22411;&#22788;&#29702;&#24471;&#24456;&#22909;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#19968;&#20123;&#26679;&#26412;&#65292;&#36825;&#20123;&#26679;&#26412;&#20854;&#23454;&#27169;&#22411;&#22788;&#29702;&#24471;&#19981;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implementing effective control mechanisms to ensure the proper functioning and security of deployed NLP models, from translation to chatbots, is essential. A key ingredient to ensure safe system behaviour is Out-Of-Distribution (OOD) detection, which aims to detect whether an input sample is statistically far from the training distribution. Although OOD detection is a widely covered topic in classification tasks, most methods rely on hidden features output by the encoder. In this work, we focus on leveraging soft-probabilities in a black-box framework, i.e. we can access the soft-predictions but not the internal states of the model. Our contributions include: (i) RAINPROOF a Relative informAItioN Projection OOD detection framework; and (ii) a more operational evaluation setting for OOD detection. Surprisingly, we find that OOD detection is not necessarily aligned with task-specific measures. The OOD detector may filter out samples well processed by the model and keep samples that are n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#30697;&#38453;&#25913;&#36827;&#20102;&#35789;&#31227;&#36317;&#31163;&#65288;Word Mover's Distance&#65292;WMD&#65289;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#32771;&#34385;&#21477;&#23376;&#32467;&#26500;&#21644;&#35789;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#65292;&#23454;&#29616;&#20102;&#22312;&#36817;&#20041;&#35789;&#35782;&#21035;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#20013;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.06229</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#30697;&#38453;&#25552;&#39640;&#35789;&#31227;&#36317;&#31163;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving word mover's distance by leveraging self-attention matrix. (arXiv:2211.06229v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#30697;&#38453;&#25913;&#36827;&#20102;&#35789;&#31227;&#36317;&#31163;&#65288;Word Mover's Distance&#65292;WMD&#65289;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#32771;&#34385;&#21477;&#23376;&#32467;&#26500;&#21644;&#35789;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#65292;&#23454;&#29616;&#20102;&#22312;&#36817;&#20041;&#35789;&#35782;&#21035;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#20013;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34913;&#37327;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#35789;&#31227;&#36317;&#31163;&#65288;WMD&#65289;&#36890;&#36807;&#35745;&#31639;&#35789;&#21521;&#37327;&#38598;&#20043;&#38388;&#30340;&#26368;&#20248;&#23545;&#40784;&#26469;&#35745;&#31639;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;WMD&#27809;&#26377;&#21033;&#29992;&#35789;&#24207;&#65292;&#36825;&#20351;&#24471;&#23427;&#38590;&#20197;&#21306;&#20998;&#20855;&#26377;&#30456;&#20284;&#35789;&#27719;&#37325;&#21472;&#30340;&#21477;&#23376;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#35821;&#20041;&#19978;&#38750;&#24120;&#19981;&#21516;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#34701;&#21512;BERT&#30340;&#33258;&#27880;&#24847;&#21147;&#30697;&#38453;&#65288;SAM&#65289;&#26469;&#25913;&#36827;WMD&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#34701;&#21512;Gromov-Wasserstein&#36317;&#31163;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#35789;&#23884;&#20837;&#21644;SAM&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#35745;&#31639;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#26368;&#20248;&#36716;&#36816;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36817;&#20041;&#35789;&#35782;&#21035;&#26041;&#38754;&#25552;&#39640;&#20102;WMD&#21450;&#20854;&#21464;&#20307;&#65292;&#19982;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20013;&#30340;&#20960;&#20046;&#30456;&#31561;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/ymgw55/WSMD}&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring the semantic similarity between two sentences is still an important task. The word mover's distance (WMD) computes the similarity via the optimal alignment between the sets of word embeddings. However, WMD does not utilize word order, making it challenging to distinguish sentences with significant overlaps of similar words, even if they are semantically very different. Here, we attempt to improve WMD by incorporating the sentence structure represented by BERT's self-attention matrix (SAM). The proposed method is based on the Fused Gromov-Wasserstein distance, which simultaneously considers the similarity of the word embedding and the SAM for calculating the optimal transport between two sentences. Experiments demonstrate the proposed method enhances WMD and its variants in paraphrase identification with near-equivalent performance in semantic textual similarity. Our code is available at \url{https://github.com/ymgw55/WSMD}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#31359;&#26797;&#24335;&#31163;&#23376;&#38449;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#37327;&#23376;&#30005;&#36335;&#32534;&#35793;&#22120;&#65292;&#33021;&#22815;&#23558;&#37327;&#23376;&#30005;&#36335;&#36716;&#25442;&#21644;&#20248;&#21270;&#20026;&#29305;&#23450;&#30340;&#26412;&#22320;&#38376;&#24207;&#21015;&#65292;&#19982;&#26631;&#20934;&#32534;&#35793;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#23558;&#38376;&#35745;&#25968;&#20943;&#23569;&#21040;5.1&#20493;&#12290;</title><link>http://arxiv.org/abs/2207.01964</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#31359;&#26797;&#24335;&#31163;&#23376;&#38449;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#37327;&#23376;&#30005;&#36335;&#32534;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
Quantum Circuit Compiler for a Shuttling-Based Trapped-Ion Quantum Computer. (arXiv:2207.01964v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#31359;&#26797;&#24335;&#31163;&#23376;&#38449;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#37327;&#23376;&#30005;&#36335;&#32534;&#35793;&#22120;&#65292;&#33021;&#22815;&#23558;&#37327;&#23376;&#30005;&#36335;&#36716;&#25442;&#21644;&#20248;&#21270;&#20026;&#29305;&#23450;&#30340;&#26412;&#22320;&#38376;&#24207;&#21015;&#65292;&#19982;&#26631;&#20934;&#32534;&#35793;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#23558;&#38376;&#35745;&#25968;&#20943;&#23569;&#21040;5.1&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37327;&#23376;&#35745;&#31639;&#30828;&#20214;&#33021;&#21147;&#30340;&#22686;&#24378;&#21644;&#23454;&#29616;&#28145;&#24230;&#37327;&#23376;&#30005;&#36335;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#23436;&#20840;&#33258;&#21160;&#21270;&#21644;&#39640;&#25928;&#30340;&#24037;&#20855;&#26469;&#32534;&#35793;&#37327;&#23376;&#30005;&#36335;&#12290;&#20026;&#20102;&#22312;&#29305;&#23450;&#20110;&#37327;&#23376;&#35745;&#31639;&#26426;&#26550;&#26500;&#30340;&#26412;&#22320;&#38376;&#24207;&#21015;&#20013;&#34920;&#31034;&#20219;&#24847;&#30005;&#36335;&#65292;&#38656;&#35201;&#20351;&#31639;&#27861;&#22312;&#37327;&#23376;&#30828;&#20214;&#20379;&#24212;&#21830;&#30340;&#33539;&#22260;&#20869;&#21487;&#31227;&#26893;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#35793;&#22120;&#65292;&#21487;&#20197;&#23558;&#37327;&#23376;&#30005;&#36335;&#36716;&#25442;&#21644;&#20248;&#21270;&#20026;&#38024;&#23545;&#31359;&#26797;&#24335;&#31163;&#23376;&#38449;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#30446;&#26631;&#30005;&#36335;&#12290;&#23427;&#30001;&#22522;&#20110;&#37327;&#23376;&#30005;&#36335;&#26694;&#26550;Pytket&#30340;&#23450;&#21046;&#31639;&#27861;&#32452;&#25104;&#12290;&#23545;&#24191;&#27867;&#30340;&#37327;&#23376;&#30005;&#36335;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;Pytket&#30456;&#27604;&#65292;&#38376;&#35745;&#25968;&#21487;&#20197;&#20943;&#23569;&#22810;&#36798;5.1&#20493;&#65292;&#19982;&#26631;&#20934;Qiskit&#32534;&#35793;&#30456;&#27604;&#21487;&#20197;&#20943;&#23569;&#22810;&#36798;2.2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing capabilities of quantum computing hardware and the challenge of realizing deep quantum circuits require fully automated and efficient tools for compiling quantum circuits. To express arbitrary circuits in a sequence of native gates specific to the quantum computer architecture, it is necessary to make algorithms portable across the landscape of quantum hardware providers. In this work, we present a compiler capable of transforming and optimizing a quantum circuit targeting a shuttling-based trapped-ion quantum processor. It consists of custom algorithms set on top of the quantum circuit framework Pytket. The performance was evaluated for a wide range of quantum circuits and the results show that the gate counts can be reduced by factors up to 5.1 compared to standard Pytket and up to 2.2 compared to standard Qiskit compilation.
&lt;/p&gt;</description></item></channel></rss>