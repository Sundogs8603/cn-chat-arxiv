<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RiskCards&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#35780;&#20272;&#21644;&#35760;&#24405;&#19982;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30456;&#20851;&#30340;&#21508;&#31181;&#39118;&#38505;&#65292;&#24182;&#23558;&#36825;&#20123;&#39118;&#38505;&#20256;&#36798;&#32473;&#25216;&#26415;&#21644;&#38750;&#25216;&#26415;&#30456;&#20851;&#32773;&#12290;</title><link>http://arxiv.org/abs/2303.18190</link><description>&lt;p&gt;
&#20351;&#29992;&#39118;&#38505;&#21345;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Assessing Language Model Deployment with Risk Cards. (arXiv:2303.18190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RiskCards&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#35780;&#20272;&#21644;&#35760;&#24405;&#19982;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30456;&#20851;&#30340;&#21508;&#31181;&#39118;&#38505;&#65292;&#24182;&#23558;&#36825;&#20123;&#39118;&#38505;&#20256;&#36798;&#32473;&#25216;&#26415;&#21644;&#38750;&#25216;&#26415;&#30456;&#20851;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RiskCards&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#32467;&#26500;&#21270;&#35780;&#20272;&#21644;&#35760;&#24405;&#19982;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30456;&#20851;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;&#19982;&#25152;&#26377;&#35821;&#35328;&#19968;&#26679;&#65292;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#65292;&#25110;&#29992;&#20110;&#36896;&#25104;&#20260;&#23475;&#12290;&#33258;&#21160;&#21270;&#35821;&#35328;&#29983;&#25104;&#19981;&#20165;&#22686;&#21152;&#20102;&#35268;&#27169;&#30340;&#22240;&#32032;&#65292;&#36824;&#20351;&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#26356;&#24494;&#22937;&#25110;&#31361;&#21457;&#30340;&#19981;&#33391;&#36235;&#21183;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#30830;&#23450;&#20102;&#35768;&#22810;&#19981;&#21516;&#35282;&#33394;&#25152;&#38754;&#20020;&#30340;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#21361;&#23475;&#12290;&#29616;&#26377;&#30340;&#20998;&#31867;&#27861;&#30830;&#23450;&#20102;&#35821;&#35328;&#27169;&#22411;&#25152;&#36896;&#25104;&#30340;&#21508;&#31181;&#21361;&#23475;&#31867;&#21035;&#65307;&#22522;&#20934;&#27979;&#35797;&#30830;&#31435;&#20102;&#23545;&#36825;&#20123;&#21361;&#23475;&#30340;&#33258;&#21160;&#21270;&#27979;&#35797;&#65307;&#32780;&#23545;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#25991;&#26723;&#26631;&#20934;&#21017;&#40723;&#21169;&#36879;&#26126;&#25253;&#21578;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#19968;&#20010;&#20197;&#39118;&#38505;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35760;&#24405;&#19968;&#20010;&#22797;&#26434;&#30340;&#39118;&#38505;&#23616;&#21183;&#65292;&#20854;&#20013;&#26576;&#20123;&#39118;&#38505;&#36328;&#36234;&#27169;&#22411;&#21644;&#29615;&#22659;&#65292;&#32780;&#20854;&#20182;&#39118;&#38505;&#21017;&#26159;&#29305;&#23450;&#30340;&#65292;&#20854;&#20013;&#26576;&#20123;&#26465;&#20214;&#21487;&#33021;&#38656;&#35201;&#25165;&#33021;&#23558;&#39118;&#38505;&#36716;&#21270;&#20026;&#21361;&#23475;&#12290;RiskCards&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#30830;&#23450;&#12289;&#35780;&#20272;&#21644;&#35760;&#24405;&#19982;&#35821;&#35328;&#27169;&#22411;&#37096;&#32626;&#30456;&#20851;&#30340;&#39118;&#38505;&#65292;&#24182;&#21521;&#25216;&#26415;&#21644;&#38750;&#25216;&#26415;&#30456;&#20851;&#32773;&#20256;&#36798;&#36825;&#20123;&#39118;&#38505;&#65292;&#22635;&#34917;&#20102;&#36825;&#20010;&#26041;&#27861;&#35770;&#19978;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces RiskCards, a framework for structured assessment and documentation of risks associated with an application of language models. As with all language, text generated by language models can be harmful, or used to bring about harm. Automating language generation adds both an element of scale and also more subtle or emergent undesirable tendencies to the generated text. Prior work establishes a wide variety of language model harms to many different actors: existing taxonomies identify categories of harms posed by language models; benchmarks establish automated tests of these harms; and documentation standards for models, tasks and datasets encourage transparent reporting. However, there is no risk-centric framework for documenting the complexity of a landscape in which some risks are shared across models and contexts, while others are specific, and where certain conditions may be required for risks to manifest as harms. RiskCards address this methodological gap by prov
&lt;/p&gt;</description></item><item><title>ViMMRC 2.0&#26159;&#19968;&#20010;&#38024;&#23545;&#36234;&#21335;&#25945;&#26448;&#20013;&#30340;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#65292;&#20849;&#26377;699&#31687;&#25955;&#25991;&#21644;&#35799;&#27468;&#20197;&#21450;5,273&#20010;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#36873;&#39033;&#19981;&#22266;&#23450;&#20026;&#22235;&#20010;&#65292;&#19988;&#38382;&#39064;&#38590;&#24230;&#22686;&#21152;&#65292;&#38656;&#35201;&#20351;&#29992;&#22810;&#27493;&#27880;&#24847;&#21147;&#32593;&#32476;&#19982;&#21464;&#21387;&#22120;&#30456;&#32467;&#21512;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;&#26469;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.18162</link><description>&lt;p&gt;
&#29992;&#20110;&#36234;&#21335;&#35821;&#25945;&#32946;&#30340;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
A Multiple Choices Reading Comprehension Corpus for Vietnamese Language Education. (arXiv:2303.18162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18162
&lt;/p&gt;
&lt;p&gt;
ViMMRC 2.0&#26159;&#19968;&#20010;&#38024;&#23545;&#36234;&#21335;&#25945;&#26448;&#20013;&#30340;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#65292;&#20849;&#26377;699&#31687;&#25955;&#25991;&#21644;&#35799;&#27468;&#20197;&#21450;5,273&#20010;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#36873;&#39033;&#19981;&#22266;&#23450;&#20026;&#22235;&#20010;&#65292;&#19988;&#38382;&#39064;&#38590;&#24230;&#22686;&#21152;&#65292;&#38656;&#35201;&#20351;&#29992;&#22810;&#27493;&#27880;&#24847;&#21147;&#32593;&#32476;&#19982;&#21464;&#21387;&#22120;&#30456;&#32467;&#21512;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;&#26469;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#26159;&#36817;&#24180;&#26469;&#19968;&#20010;&#26377;&#36259;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#30446;&#30340;&#22312;&#20110;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ViMMRC 2.0&#65292;&#36825;&#26159;&#23545;&#20043;&#21069;ViMMRC&#30340;&#25193;&#23637;&#65292;&#29992;&#20110;&#36234;&#21335;&#25945;&#26448;&#20013;&#30340;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#65292;&#36825;&#20123;&#25945;&#26448;&#21253;&#21547;&#20102;&#19968;&#24180;&#32423;&#33267;&#21313;&#20108;&#24180;&#32423;&#23398;&#29983;&#30340;&#38405;&#35835;&#25991;&#31456;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;699&#31687;&#25955;&#25991;&#21644;&#35799;&#27468;&#65292;&#20197;&#21450;5,273&#20010;&#38382;&#39064;&#12290;&#19982;&#20043;&#21069;&#30340;&#29256;&#26412;&#19981;&#21516;&#65292;&#26032;&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#36873;&#39033;&#19981;&#22266;&#23450;&#20026;&#22235;&#20010;&#65292;&#21516;&#26102;&#36824;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#38590;&#24230;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#38656;&#35201;&#23547;&#25214;&#27491;&#30830;&#30340;&#36873;&#25321;&#12290;&#30005;&#33041;&#24517;&#39035;&#29702;&#35299;&#25972;&#20010;&#38405;&#35835;&#25991;&#31456;&#30340;&#19978;&#19979;&#25991;&#12289;&#38382;&#39064;&#20197;&#21450;&#27599;&#20010;&#36873;&#39033;&#30340;&#20869;&#23481;&#25165;&#33021;&#25552;&#21462;&#27491;&#30830;&#31572;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#22810;&#27493;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;MAN&#65289;&#19982;&#21464;&#21387;&#22120;&#30456;&#32467;&#21512;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine reading comprehension has been an interesting and challenging task in recent years, with the purpose of extracting useful information from texts. To attain the computer ability to understand the reading text and answer relevant information, we introduce ViMMRC 2.0 - an extension of the previous ViMMRC for the task of multiple-choice reading comprehension in Vietnamese Textbooks which contain the reading articles for students from Grade 1 to Grade 12. This dataset has 699 reading passages which are prose and poems, and 5,273 questions. The questions in the new dataset are not fixed with four options as in the previous version. Moreover, the difficulty of questions is increased, which challenges the models to find the correct choice. The computer must understand the whole context of the reading passage, the question, and the content of each choice to extract the right answers. Hence, we propose the multi-stage approach that combines the multi-step attention network (MAN) with the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#21542;&#36890;&#36807;&#24037;&#31243;&#22522;&#30784;&#65288;FE&#65289;&#21644;&#24037;&#31243;&#21407;&#29702;&#19982;&#23454;&#36341;&#65288;PE&#65289;&#32771;&#35797;&#65292;&#30740;&#31350;&#21457;&#29616;ChatGPT-4&#22312;FE&#32771;&#35797;&#20013;&#24471;&#20998;70.9&#65285;&#65292;&#22312;PE&#32771;&#35797;&#20013;&#24471;&#20998;46.2&#65285;&#65292;&#24182;&#19988;&#26377;&#26395;&#36890;&#36807;PE&#32771;&#35797;&#12290;</title><link>http://arxiv.org/abs/2303.18149</link><description>&lt;p&gt;
AI&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#21542;&#33021;&#36890;&#36807;&#24037;&#31243;&#22522;&#30784;&#65288;FE&#65289;&#21644;&#24037;&#31243;&#21407;&#29702;&#19982;&#23454;&#36341;&#65288;PE&#65289;&#32467;&#26500;&#32771;&#35797;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI Chatbots Pass the Fundamentals of Engineering (FE) and Principles and Practice of Engineering (PE) Structural Exams?. (arXiv:2303.18149v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#21542;&#36890;&#36807;&#24037;&#31243;&#22522;&#30784;&#65288;FE&#65289;&#21644;&#24037;&#31243;&#21407;&#29702;&#19982;&#23454;&#36341;&#65288;PE&#65289;&#32771;&#35797;&#65292;&#30740;&#31350;&#21457;&#29616;ChatGPT-4&#22312;FE&#32771;&#35797;&#20013;&#24471;&#20998;70.9&#65285;&#65292;&#22312;PE&#32771;&#35797;&#20013;&#24471;&#20998;46.2&#65285;&#65292;&#24182;&#19988;&#26377;&#26395;&#36890;&#36807;PE&#32771;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#31243;&#30028;&#65292;&#38543;&#30528;OpenAI ChatGPT-4&#21644;Google Bard&#30340;&#21457;&#24067;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#36817;&#24180;&#26469;&#36805;&#36895;&#21457;&#23637;&#12290;&#34429;&#28982;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#34987;&#25253;&#36947;&#34920;&#29616;&#33391;&#22909;&#65292;&#29978;&#33267;&#36890;&#36807;&#20102;&#21508;&#31181;&#26631;&#20934;&#21270;&#32771;&#35797;&#65292;&#21253;&#25324;&#21307;&#23398;&#21644;&#27861;&#24459;&#32771;&#35797;&#65292;&#20294;&#26412;&#35770;&#25991;&#25506;&#35752;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#21542;&#20063;&#33021;&#36890;&#36807;&#24037;&#31243;&#22522;&#30784;&#65288;FE&#65289;&#21644;&#24037;&#31243;&#21407;&#29702;&#19982;&#23454;&#36341;&#65288;PE&#65289;&#32771;&#35797;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#22303;&#26408;&#21644;&#29615;&#22659;&#24037;&#31243;&#38382;&#39064;&#21644;&#24773;&#26223;&#26469;&#35780;&#20272;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#65292;&#22312;FE&#21644;PE&#32771;&#35797;&#20013;&#24120;&#35265;&#12290;&#22522;&#20110;&#30456;&#20851;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#28165;&#26224;&#24230;&#65292;&#20998;&#26512;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21709;&#24212;&#65292;&#28982;&#21518;&#19982;National Council of Examiners for Engineering and Surveying (NCEES)&#30340;&#24314;&#35758;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#25253;&#21578;&#26174;&#31034;&#65292;ChatGPT-4&#21644;Bard&#22312;FE&#32771;&#35797;&#20013;&#24471;&#20998;&#20998;&#21035;&#20026;70.9&#65285;&#21644;39.2&#65285;&#65292;&#22312;PE&#32771;&#35797;&#20013;&#24471;&#20998;&#20998;&#21035;&#20026;46.2&#65285;&#21644;41&#65285;&#12290;&#26174;&#28982;&#65292;&#30446;&#21069;&#29256;&#26412;&#30340;ChatGPT-4&#26377;&#21487;&#33021;&#36890;&#36807;PE&#32771;&#35797;&#65292;&#20294;&#22312;FE&#32771;&#35797;&#20013;&#25104;&#32489;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
The engineering community has recently witnessed the emergence of chatbot technology with the release of OpenAI ChatGPT-4 and Google Bard. While these chatbots have been reported to perform well and even pass various standardized tests, including medical and law exams, this forum paper explores whether these chatbots can also pass the Fundamentals of Engineering (FE) and Principles and Practice of Engineering (PE) exams. A diverse range of civil and environmental engineering questions and scenarios are used to evaluate the chatbots' performance, as commonly present in the FE and PE exams. The chatbots' responses were analyzed based on their relevance, accuracy, and clarity and then compared against the recommendations of the National Council of Examiners for Engineering and Surveying (NCEES). Our report shows that ChatGPT-4 and Bard, respectively scored 70.9% and 39.2% in the FE exam and 46.2% and 41% in the PE exam. It is evident that the current version of ChatGPT-4 could potentially
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BERTino&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;DistilBERT&#27169;&#22411;&#65292;&#26159;&#29992;&#20110;&#24847;&#22823;&#21033;&#35821;&#30340;&#31532;&#19968;&#20010;&#26367;&#20195;BERT&#20307;&#31995;&#32467;&#26500;&#30340;&#36873;&#25321;&#65292;&#20854;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;F1&#20998;&#25968;&#19982;BERTBASE&#30456;&#24403;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.18121</link><description>&lt;p&gt;
BERTino&#65306;&#19968;&#31181;&#24847;&#22823;&#21033;DistilBERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BERTino: an Italian DistilBERT model. (arXiv:2303.18121v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BERTino&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;DistilBERT&#27169;&#22411;&#65292;&#26159;&#29992;&#20110;&#24847;&#22823;&#21033;&#35821;&#30340;&#31532;&#19968;&#20010;&#26367;&#20195;BERT&#20307;&#31995;&#32467;&#26500;&#30340;&#36873;&#25321;&#65292;&#20854;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;F1&#20998;&#25968;&#19982;BERTBASE&#30456;&#24403;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#30340;Transformer&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#34429;&#28982;&#24778;&#20154;&#65292;&#20294;&#30001;&#20110;&#26500;&#25104;&#20854;&#32593;&#32476;&#30340;&#21442;&#25968;&#36807;&#22810;&#65292;&#23548;&#33268;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#39640;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BERTino&#65292;&#19968;&#31181;DistilBERT&#27169;&#22411;&#65292;&#23427;&#26159;&#29992;&#20110;&#24847;&#22823;&#21033;&#35821;&#30340;&#31532;&#19968;&#20010;&#36731;&#37327;&#32423;&#26367;&#20195;BERT&#20307;&#31995;&#32467;&#26500;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#23545;BERTino&#22312;&#24847;&#22823;&#21033;ISDT&#12289;&#24847;&#22823;&#21033;ParTUT&#12289;&#24847;&#22823;&#21033;WikiNER&#21644;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#33719;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#24182;&#33719;&#24471;&#20102;&#19982;BERTBASE&#30456;&#24403;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent introduction of Transformers language representation models allowed great improvements in many natural language processing (NLP) tasks. However, if on one hand the performances achieved by this kind of architectures are surprising, on the other their usability is limited by the high number of parameters which constitute their network, resulting in high computational and memory demands. In this work we present BERTino, a DistilBERT model which proposes to be the first lightweight alternative to the BERT architecture specific for the Italian language. We evaluated BERTino on the Italian ISDT, Italian ParTUT, Italian WikiNER and multiclass classification tasks, obtaining F1 scores comparable to those obtained by a BERTBASE with a remarkable improvement in training and inference speed.
&lt;/p&gt;</description></item><item><title>UKP-SQuARE v3&#26159;&#19968;&#20010;&#25903;&#25345;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;QA&#30740;&#31350;&#24179;&#21488;&#65292;&#19982;&#22810;&#25968;&#25454;&#38598;&#27169;&#22411;&#30456;&#27604;&#65292;&#32467;&#21512;&#19987;&#23478;&#26234;&#33021;&#20307;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.18120</link><description>&lt;p&gt;
UKP-SQuARE v3&#65306;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;QA&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
UKP-SQuARE v3: A Platform for Multi-Agent QA Research. (arXiv:2303.18120v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18120
&lt;/p&gt;
&lt;p&gt;
UKP-SQuARE v3&#26159;&#19968;&#20010;&#25903;&#25345;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;QA&#30740;&#31350;&#24179;&#21488;&#65292;&#19982;&#22810;&#25968;&#25454;&#38598;&#27169;&#22411;&#30456;&#27604;&#65292;&#32467;&#21512;&#19987;&#23478;&#26234;&#33021;&#20307;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#30340;&#19981;&#26029;&#21457;&#23637;&#24050;&#24341;&#36215;&#30740;&#31350;&#30028;&#23545;&#22810;&#39046;&#22495;&#27169;&#22411;&#30340;&#20851;&#27880;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#22810;&#25968;&#25454;&#38598;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#65292;&#20197;&#23398;&#20064;&#23427;&#20204;&#30340;&#35268;&#24459;&#24182;&#38450;&#27490;&#23545;&#21333;&#20010;&#25968;&#25454;&#38598;&#36807;&#24230;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22312;&#32447;&#20195;&#30721;&#24211;&#65288;&#22914;GitHub&#25110;Hugging Face&#65289;&#20013;QA&#27169;&#22411;&#30340;&#28608;&#22686;&#65292;&#21478;&#19968;&#31181;&#26041;&#27861;&#27491;&#22312;&#21464;&#24471;&#21487;&#34892;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32467;&#21512;&#19987;&#23478;&#26234;&#33021;&#20307;&#21487;&#20197;&#27604;&#22810;&#25968;&#25454;&#38598;&#27169;&#22411;&#33719;&#24471;&#26356;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20026;&#20102;&#26041;&#20415;&#22810;&#26234;&#33021;&#20307;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;UKP-SQuARE&#25193;&#23637;&#20026;&#25903;&#25345;&#19977;&#31181;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65306;i&#65289;&#26234;&#33021;&#20307;&#36873;&#25321;&#65292;ii&#65289;&#26234;&#33021;&#20307;&#30340;&#26089;&#26399;&#34701;&#21512;&#65292;&#20197;&#21450;iii&#65289;&#26234;&#33021;&#20307;&#30340;&#21518;&#26399;&#34701;&#21512;&#12290;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#20197;&#35780;&#20272;&#23427;&#20204;&#30340;&#25512;&#26029;&#36895;&#24230;&#65292;&#24182;&#19982;&#22810;&#25968;&#25454;&#38598;&#27169;&#22411;&#36827;&#34892;&#24615;&#33021;&#19982;&#36895;&#24230;&#26435;&#34913;&#30340;&#35752;&#35770;&#12290;UKP-SQuARE&#26159;&#24320;&#28304;&#30340;&#65292;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The continuous development of Question Answering (QA) datasets has drawn the research community's attention toward multi-domain models. A popular approach is to use multi-dataset models, which are models trained on multiple datasets to learn their regularities and prevent overfitting to a single dataset. However, with the proliferation of QA models in online repositories such as GitHub or Hugging Face, an alternative is becoming viable. Recent works have demonstrated that combining expert agents can yield large performance gains over multi-dataset models. To ease research in multi-agent models, we extend UKP-SQuARE, an online platform for QA research, to support three families of multi-agent systems: i) agent selection, ii) early-fusion of agents, and iii) late-fusion of agents. We conduct experiments to evaluate their inference speed and discuss the performance vs. speed trade-off compared to multi-dataset models. UKP-SQuARE is open-source and publicly available at this http URL
&lt;/p&gt;</description></item><item><title>&#29233;&#19969;&#22561;&#22269;&#38469;&#33521;&#35821;&#21475;&#38899;&#35821;&#26009;&#24211;&#65288;EdAcc&#65289;&#21457;&#24067;&#65292;&#21253;&#25324;&#33521;&#35821;&#30340;&#24191;&#27867;&#22810;&#26679;&#24615;&#21644;&#27599;&#20010;&#35828;&#35805;&#32773;&#30340;&#35821;&#35328;&#32972;&#26223;&#27010;&#20917;&#12290;&#22312; EdAcc &#19978;&#35757;&#32451;&#30340;&#26368;&#20339;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#65292;&#36825;&#31361;&#26174;&#20102;&#24403;&#21069;&#33521;&#35821;ASR&#27169;&#22411;&#30340;&#32570;&#28857;&#12290;&#36825;&#19968;&#25968;&#25454;&#38598;&#30340;&#20844;&#24320;&#20849;&#20139;&#23558;&#26377;&#21161;&#20110;&#27665;&#20027;&#21270;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2303.18110</link><description>&lt;p&gt;
&#29233;&#19969;&#22561;&#22269;&#38469;&#33521;&#35821;&#21475;&#38899;&#35821;&#26009;&#24211;&#65306;&#36808;&#21521;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
The Edinburgh International Accents of English Corpus: Towards the Democratization of English ASR. (arXiv:2303.18110v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18110
&lt;/p&gt;
&lt;p&gt;
&#29233;&#19969;&#22561;&#22269;&#38469;&#33521;&#35821;&#21475;&#38899;&#35821;&#26009;&#24211;&#65288;EdAcc&#65289;&#21457;&#24067;&#65292;&#21253;&#25324;&#33521;&#35821;&#30340;&#24191;&#27867;&#22810;&#26679;&#24615;&#21644;&#27599;&#20010;&#35828;&#35805;&#32773;&#30340;&#35821;&#35328;&#32972;&#26223;&#27010;&#20917;&#12290;&#22312; EdAcc &#19978;&#35757;&#32451;&#30340;&#26368;&#20339;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#65292;&#36825;&#31361;&#26174;&#20102;&#24403;&#21069;&#33521;&#35821;ASR&#27169;&#22411;&#30340;&#32570;&#28857;&#12290;&#36825;&#19968;&#25968;&#25454;&#38598;&#30340;&#20844;&#24320;&#20849;&#20139;&#23558;&#26377;&#21161;&#20110;&#27665;&#20027;&#21270;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#26159;&#19990;&#30028;&#19978;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#35821;&#35328;&#65292;&#27599;&#22825;&#26377;&#25968;&#30334;&#19975;&#20154;&#20351;&#29992;&#33521;&#35821;&#20316;&#20026;&#31532;&#19968;&#25110;&#31532;&#20108;&#35821;&#35328;&#65292;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#35821;&#22659;&#20013;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#33521;&#35821;&#26377;&#35768;&#22810;&#21464;&#20307;&#12290;&#34429;&#28982;&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#33521;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#36890;&#24120;&#22522;&#20110;&#27979;&#35797;&#25968;&#25454;&#38598;&#25253;&#21578;&#30340;&#32467;&#26524;&#26410;&#33021;&#20195;&#34920;&#20170;&#22825;&#20840;&#29699;&#20351;&#29992;&#30340;&#22810;&#26679;&#21270;&#33521;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#29233;&#19969;&#22561;&#22269;&#38469;&#33521;&#35821;&#21475;&#38899;&#35821;&#26009;&#24211;&#65288;EdAcc&#65289;&#30340;&#29256;&#26412;&#12290;&#27492;&#25968;&#25454;&#38598;&#35797;&#22270;&#26356;&#22909;&#22320;&#20195;&#34920;&#33521;&#35821;&#30340;&#24191;&#27867;&#22810;&#26679;&#24615;&#65292;&#21253;&#25324;&#32422;40&#23567;&#26102;&#30340;&#26379;&#21451;&#20043;&#38388;&#30340;&#20108;&#20803;&#35270;&#39057;&#36890;&#35805;&#12290;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;EdAcc&#21253;&#25324;&#24191;&#27867;&#30340;&#33521;&#35821;&#31532;&#19968;&#35821;&#35328;&#21644;&#31532;&#20108;&#35821;&#35328;&#21464;&#20307;&#20197;&#21450;&#27599;&#20010;&#20154;&#30340;&#35821;&#35328;&#32972;&#26223;&#27010;&#20917;&#12290;&#26368;&#26032;&#20844;&#20849;&#21644;&#21830;&#19994;&#27169;&#22411;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;EdAcc&#24378;&#35843;&#20102;&#24403;&#21069;&#33521;&#35821;ASR&#27169;&#22411;&#30340;&#32570;&#28857;&#12290;&#22312;680&#20010;&#36716;&#24405;&#30340;EdAcc&#23545;&#35805;&#19978;&#35757;&#32451;&#30340;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#20844;&#24320;&#30340;&#65292;&#25105;&#20204;&#24076;&#26395;&#23427;&#33021;&#20026;&#27665;&#20027;&#21270;&#33521;&#35821;ASR&#30740;&#31350;&#21644;&#24320;&#21457;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
English is the most widely spoken language in the world, used daily by millions of people as a first or second language in many different contexts. As a result, there are many varieties of English. Although the great many advances in English automatic speech recognition (ASR) over the past decades, results are usually reported based on test datasets which fail to represent the diversity of English as spoken today around the globe. We present the first release of The Edinburgh International Accents of English Corpus (EdAcc). This dataset attempts to better represent the wide diversity of English, encompassing almost 40 hours of dyadic video call conversations between friends. Unlike other datasets, EdAcc includes a wide range of first and second-language varieties of English and a linguistic background profile of each speaker. Results on latest public, and commercial models show that EdAcc highlights shortcomings of current English ASR models. The best performing model, trained on 680 t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#35206;&#30422;14&#31181;&#35821;&#35328;&#12289;&#22810;&#20803;&#21270;&#30340;&#26102;&#38388;&#21644;&#25968;&#23383;&#34920;&#36798;&#24335;&#65292;&#21253;&#25324;&#25552;&#21462;&#12289;&#35268;&#33539;&#21270;&#21644;&#35299;&#26512;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;(NTX)&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#20316;&#20026;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.18103</link><description>&lt;p&gt;
&#22810;&#35821;&#31181;&#26102;&#38388;&#21644;&#25968;&#23383;&#34920;&#36798;&#24335;&#30340;&#25277;&#21462;&#21644;&#35268;&#33539;&#21270;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Dataset and Baseline System for Multi-lingual Extraction and Normalization of Temporal and Numerical Expressions. (arXiv:2303.18103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#35206;&#30422;14&#31181;&#35821;&#35328;&#12289;&#22810;&#20803;&#21270;&#30340;&#26102;&#38388;&#21644;&#25968;&#23383;&#34920;&#36798;&#24335;&#65292;&#21253;&#25324;&#25552;&#21462;&#12289;&#35268;&#33539;&#21270;&#21644;&#35299;&#26512;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;(NTX)&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#20316;&#20026;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;&#26102;&#38388;&#21644;&#25968;&#23383;&#34920;&#36798;&#24335;&#30340;&#29702;&#35299;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#24037;&#20316;&#20165;&#28085;&#30422;&#20102;&#23569;&#37327;&#30340;&#23376;&#31867;&#22411;&#65292;&#24182;&#19988;&#21482;&#20851;&#27880;&#23454;&#20307;&#25277;&#21462;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#35782;&#21035;&#21040;&#30340;&#25552;&#21450;&#30340;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#22312;&#19979;&#28216;&#22330;&#26223;&#20013;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#65292;&#23376;&#31867;&#22411;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#31890;&#24230;&#24456;&#37325;&#35201;&#65307;&#24182;&#19988;&#26356;&#21152;&#37325;&#35201;&#30340;&#26159;&#65292;&#25552;&#20379;&#21487;&#20197;&#25805;&#20316;&#30340;&#20855;&#20307;&#20540;&#35299;&#26512;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#20165;&#22788;&#29702;&#20960;&#31181;&#35821;&#35328;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#35780;&#20272;&#25968;&#25454;&#38598;-NTX-&#28085;&#30422;&#20102;14&#31181;&#35821;&#35328;&#30340;&#21508;&#31181;&#26102;&#38388;&#21644;&#25968;&#23383;&#34920;&#36798;&#24335;&#65292;&#24182;&#35206;&#30422;&#20102;&#25552;&#21462;&#65292;&#35268;&#33539;&#21270;&#21644;&#35299;&#26512;&#12290;&#38500;&#20102;&#25968;&#25454;&#38598;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#20316;&#20026;&#19982;&#22312;&#35813;&#25968;&#25454;&#38598;&#20013;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#30340;&#24378;&#22823;&#22522;&#20934;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#22312; \url{https://aka.ms/NTX}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal and numerical expression understanding is of great importance in many downstream Natural Language Processing (NLP) and Information Retrieval (IR) tasks. However, much previous work covers only a few sub-types and focuses only on entity extraction, which severely limits the usability of identified mentions. In order for such entities to be useful in downstream scenarios, coverage and granularity of sub-types are important; and, even more so, providing resolution into concrete values that can be manipulated. Furthermore, most previous work addresses only a handful of languages. Here we describe a multi-lingual evaluation dataset - NTX - covering diverse temporal and numerical expressions across 14 languages and covering extraction, normalization, and resolution. Along with the dataset we provide a robust rule-based system as a strong baseline for comparisons against other models to be evaluated in this dataset. Data and code are available at \url{https://aka.ms/NTX}.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#20013;&#30340;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#20043;&#21069;&#19981;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#27604;&#12290;</title><link>http://arxiv.org/abs/2303.18062</link><description>&lt;p&gt;
&#35299;&#20915;&#24418;&#24577;&#23398;&#31867;&#27604;&#38382;&#39064;&#65306;&#20174;&#26816;&#32034;&#21040;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Solving morphological analogies: from retrieval to generation. (arXiv:2303.18062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#20013;&#30340;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#20043;&#21069;&#19981;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#25512;&#29702;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#19968;&#31181;&#38750;&#20961;&#33021;&#21147;&#65292;&#24182;&#19988;&#24050;&#34987;&#29992;&#26469;&#35299;&#20915;&#38590;&#20197;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290; &#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#65288;AR&#65289;&#21463;&#21040;&#20102;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20854;&#28508;&#21147;&#65292;&#20363;&#22914;&#20998;&#31867;&#65292;&#20915;&#31574;&#21644;&#20855;&#26377;&#31454;&#20105;&#24615;&#32467;&#26524;&#30340;&#25512;&#33616;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;AR&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#12290;&#35813;&#26694;&#26550;&#22312;&#25972;&#20010;Siganalogies&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#21333;&#35789;&#20043;&#38388;&#30340;&#24418;&#24577;&#23398;&#31867;&#27604;&#27604;&#20363;&#65288;APs&#65289;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#26174;&#31034;&#20986;&#20248;&#20110;&#31526;&#21495;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290; &#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#31867;&#27604;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#65288;ANNc&#65289;&#21644;&#26816;&#32034;&#38382;&#39064;&#19978;&#30340;&#31867;&#27604;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#65288;ANNr&#65289;&#65292;&#20197;&#21450;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#22312;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#21333;&#35789;&#19978;&#30340;&#28508;&#21147;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#24635;&#32467;&#24182;&#25193;&#23637;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20849;&#21516;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#22312;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#19981;&#23384;&#22312;&#30340;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical inference is a remarkable capability of human reasoning, and has been used to solve hard reasoning tasks. Analogy based reasoning (AR) has gained increasing interest from the artificial intelligence community and has shown its potential in multiple machine learning tasks such as classification, decision making and recommendation with competitive results. We propose a deep learning (DL) framework to address and tackle two key tasks in AR: analogy detection and solving. The framework is thoroughly tested on the Siganalogies dataset of morphological analogical proportions (APs) between words, and shown to outperform symbolic approaches in many languages. Previous work have explored the behavior of the Analogy Neural Network for classification (ANNc) on analogy detection and of the Analogy Neural Network for retrieval (ANNr) on analogy solving by retrieval, as well as the potential of an autoencoder (AE) for analogy solving by generating the solution word. In this article we sum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;&#65292;&#20174;&#35821;&#20041;&#12289;&#24773;&#24863;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35282;&#24230;&#25366;&#25496;&#20102;&#26032;&#38395;&#20013;&#30340;&#29420;&#29305;&#20851;&#38190;&#29305;&#24449;&#21644;&#28436;&#21270;&#27169;&#24335;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#32771;&#34385;&#35780;&#35770;&#30340;&#35821;&#20041;&#21644;&#24773;&#24863;&#30340;&#21452;&#37325;&#28145;&#24230;&#20132;&#20114;&#36890;&#36947;&#32593;&#32476;&#26469;&#33719;&#21462;&#26356;&#20840;&#38754;&#21644;&#32454;&#31890;&#24230;&#30340;&#26032;&#38395;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#25968;&#25454;&#22686;&#24378;&#27169;&#22359;&#35299;&#20915;&#26679;&#26412;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.18049</link><description>&lt;p&gt;
&#27809;&#26377;&#34255;&#36523;&#20043;&#22320;&#65306;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#21452;&#37325;&#28145;&#24230;&#20132;&#20114;&#36890;&#36947;&#32593;&#32476;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
No Place to Hide: Dual Deep Interaction Channel Network for Fake News Detection based on Data Augmentation. (arXiv:2303.18049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;&#65292;&#20174;&#35821;&#20041;&#12289;&#24773;&#24863;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35282;&#24230;&#25366;&#25496;&#20102;&#26032;&#38395;&#20013;&#30340;&#29420;&#29305;&#20851;&#38190;&#29305;&#24449;&#21644;&#28436;&#21270;&#27169;&#24335;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#32771;&#34385;&#35780;&#35770;&#30340;&#35821;&#20041;&#21644;&#24773;&#24863;&#30340;&#21452;&#37325;&#28145;&#24230;&#20132;&#20114;&#36890;&#36947;&#32593;&#32476;&#26469;&#33719;&#21462;&#26356;&#20840;&#38754;&#21644;&#32454;&#31890;&#24230;&#30340;&#26032;&#38395;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#25968;&#25454;&#22686;&#24378;&#27169;&#22359;&#35299;&#20915;&#26679;&#26412;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#65288;OSN&#65289;&#22240;&#20449;&#24687;&#20256;&#25773;&#25104;&#26412;&#20302;&#32780;&#25104;&#20026;&#20551;&#26032;&#38395;&#30340;&#28201;&#24202;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#22312;&#26032;&#38395;&#20869;&#23481;&#21644;&#20256;&#25773;&#32467;&#26500;&#26041;&#38754;&#24050;&#32463;&#20570;&#20986;&#20102;&#35768;&#22810;&#23581;&#35797;&#65292;&#20294;&#26159;&#26816;&#27979;&#20551;&#26032;&#38395;&#20173;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#19968;&#26159;&#22914;&#20309;&#25366;&#25496;&#29420;&#29305;&#30340;&#20851;&#38190;&#29305;&#24449;&#21644;&#28436;&#21270;&#27169;&#24335;&#65292;&#20108;&#26159;&#22914;&#20309;&#35299;&#20915;&#23567;&#26679;&#26412;&#38382;&#39064;&#26469;&#26500;&#24314;&#39640;&#24615;&#33021;&#27169;&#22411;&#12290;&#19982;&#21033;&#29992;&#20256;&#25773;&#25299;&#25169;&#32467;&#26500;&#30340;&#27969;&#34892;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#20174;&#35821;&#20041;&#12289;&#24773;&#24863;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;&#65292;&#25366;&#25496;&#20102;&#26032;&#38395;&#21442;&#19982;&#32773;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#24773;&#24863;&#28436;&#21270;&#27169;&#24335;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#35821;&#20041;&#21644;&#24773;&#24863;&#30340;&#21452;&#37325;&#28145;&#24230;&#20132;&#20114;&#36890;&#36947;&#32593;&#32476;&#65292;&#32771;&#34385;&#20102;&#35780;&#35770;&#20197;&#33719;&#24471;&#26356;&#20840;&#38754;&#21644;&#32454;&#31890;&#24230;&#30340;&#26032;&#38395;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#25968;&#25454;&#22686;&#24378;&#27169;&#22359;&#26469;&#35299;&#20915;&#35757;&#32451;&#26679;&#26412;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Social Network (OSN) has become a hotbed of fake news due to the low cost of information dissemination. Although the existing methods have made many attempts in news content and propagation structure, the detection of fake news is still facing two challenges: one is how to mine the unique key features and evolution patterns, and the other is how to tackle the problem of small samples to build the high-performance model. Different from popular methods which take full advantage of the propagation topology structure, in this paper, we propose a novel framework for fake news detection from perspectives of semantic, emotion and data enhancement, which excavates the emotional evolution patterns of news participants during the propagation process, and a dual deep interaction channel network of semantic and emotion is designed to obtain a more comprehensive and fine-grained news representation with the consideration of comments. Meanwhile, the framework introduces a data enhancement mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;GPT-4&#12289;ChatGPT&#21644;GPT-3&#22312;&#26085;&#26412;&#21307;&#30103;&#25191;&#29031;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;GPT-4&#20248;&#20110;&#20854;&#20182;&#20004;&#32773;&#65292;&#21576;&#29616;&#20986;LLMs&#22312;&#19982;&#33521;&#35821;&#36828;&#31163;&#30340;&#35821;&#35328;&#20013;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;LLM API&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#24314;&#35758;&#23454;&#26045;&#31105;&#27490;&#30340;&#21307;&#30103;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2303.18027</link><description>&lt;p&gt;
&#22312;&#26085;&#26412;&#21307;&#30103;&#25191;&#29031;&#32771;&#35797;&#20013;&#35780;&#20272;GPT-4&#21644;ChatGPT
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations. (arXiv:2303.18027v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;GPT-4&#12289;ChatGPT&#21644;GPT-3&#22312;&#26085;&#26412;&#21307;&#30103;&#25191;&#29031;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;GPT-4&#20248;&#20110;&#20854;&#20182;&#20004;&#32773;&#65292;&#21576;&#29616;&#20986;LLMs&#22312;&#19982;&#33521;&#35821;&#36828;&#31163;&#30340;&#35821;&#35328;&#20013;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;LLM API&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#24314;&#35758;&#23454;&#26045;&#31105;&#27490;&#30340;&#21307;&#30103;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25105;&#20204;&#35748;&#20026;&#23545;&#23427;&#20204;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#20013;&#30340;&#34892;&#20026;&#12289;&#22833;&#35823;&#21644;&#38480;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLM API&#65288;ChatGPT&#12289;GPT-3&#21644;GPT-4&#65289;&#22312;&#36807;&#21435;5&#24180;&#30340;&#26085;&#26412;&#22269;&#23478;&#21307;&#30103;&#25191;&#29031;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#21253;&#25324;&#20197;&#26085;&#35821;&#20026;&#27597;&#35821;&#30340;NLP&#30740;&#31350;&#20154;&#21592;&#21644;&#22312;&#26085;&#26412;&#24037;&#20316;&#30340;&#19968;&#21517;&#23454;&#36341;&#24515;&#33039;&#30149;&#21307;&#24072;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GPT-4&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;GPT-3&#65292;&#24182;&#36890;&#36807;&#20102;&#25152;&#26377;&#20116;&#24180;&#30340;&#32771;&#35797;&#65292;&#31361;&#26174;LLMs&#22312;&#19982;&#33521;&#35821;&#36828;&#31163;&#30340;&#35821;&#35328;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#36824;&#26292;&#38706;&#20102;&#24403;&#21069;LLM API&#30340;&#19968;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;LLMs&#26377;&#26102;&#20250;&#36873;&#25321;&#22312;&#26085;&#26412;&#21307;&#30103;&#23454;&#36341;&#20013;&#24212;&#35813;&#20005;&#26684;&#36991;&#20813;&#30340;&#31105;&#27490;&#36873;&#25321;&#65292;&#20363;&#22914;&#24314;&#35758;&#23454;&#26045;&#23433;&#20048;&#27515;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;API&#25104;&#26412;&#26222;&#36941;&#36739;&#39640;&#65292;&#26368;&#22823;&#19978;&#19979;&#25991;&#22823;&#23567;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) gain popularity among speakers of diverse languages, we believe that it is crucial to benchmark them to better understand model behaviors, failures, and limitations in languages beyond English. In this work, we evaluate LLM APIs (ChatGPT, GPT-3, and GPT-4) on the Japanese national medical licensing examinations from the past five years. Our team comprises native Japanese-speaking NLP researchers and a practicing cardiologist based in Japan. Our experiments show that GPT-4 outperforms ChatGPT and GPT-3 and passes all five years of the exams, highlighting LLMs' potential in a language that is typologically distant from English. However, our evaluation also exposes critical limitations of the current LLM APIs. First, LLMs sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia. Further, our analysis shows that the API costs are generally higher and the maximum context size is smaller fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#30340;&#21477;&#23376;&#25554;&#20540;&#26041;&#27861;&#65292;&#20197;&#21450;&#21033;&#29992;&#22810;&#35821;&#31181;&#27169;&#22411;&#29983;&#25104;&#21477;&#23376;&#36827;&#34892;&#22870;&#21169;&#35745;&#31639;&#30340;Wasserstein-GAN&#26041;&#27861;&#26469;&#20248;&#21270;&#22810;&#35821;&#31181;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18011</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#21033;&#29992;&#22810;&#35821;&#31181;&#26469;&#36827;&#34892;&#20302;&#36164;&#28304;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Exploiting Multilingualism in Low-resource Neural Machine Translation via Adversarial Learning. (arXiv:2303.18011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#30340;&#21477;&#23376;&#25554;&#20540;&#26041;&#27861;&#65292;&#20197;&#21450;&#21033;&#29992;&#22810;&#35821;&#31181;&#27169;&#22411;&#29983;&#25104;&#21477;&#23376;&#36827;&#34892;&#22870;&#21169;&#35745;&#31639;&#30340;Wasserstein-GAN&#26041;&#27861;&#26469;&#20248;&#21270;&#22810;&#35821;&#31181;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20026;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#23558;&#22810;&#20010;&#24418;&#24577;&#35821;&#35328;&#25552;&#20379;&#32473;&#21333;&#20010;&#27169;&#22411;&#20250;&#38477;&#20302;NMT&#30340;&#24615;&#33021;&#12290;&#22312;GAN&#20013;&#65292;&#19982;&#21452;&#35821;&#27169;&#22411;&#31867;&#20284;&#65292;&#22810;&#35821;&#31181;NMT&#20165;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#32771;&#34385;&#19968;&#20010;&#21442;&#32771;&#32763;&#35793;&#30340;&#27599;&#20010;&#21477;&#23376;&#12290;&#36825;&#20010;&#21333;&#19968;&#30340;&#21442;&#32771;&#32763;&#35793;&#38480;&#21046;&#20102;GAN&#27169;&#22411;&#20174;&#28304;&#21477;&#23376;&#34920;&#31034;&#20013;&#23398;&#20064;&#36275;&#22815;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#30340;&#21477;&#23376;&#25554;&#20540;&#65288;DAASI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#35821;&#31181;&#35821;&#35328;&#23545;&#30340;&#28304;&#21477;&#23376;&#21644;&#30446;&#26631;&#21477;&#23376;&#30340;&#20013;&#38388;&#28508;&#22312;&#34920;&#31034;&#26469;&#25191;&#34892;&#21477;&#23376;&#25554;&#20540;&#12290;&#38500;&#20102;&#28508;&#22312;&#34920;&#31034;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;Wasserstein-GAN&#26041;&#27861;&#36827;&#34892;&#22810;&#35821;&#31181;NMT&#27169;&#22411;&#65292;&#23558;&#22810;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#29983;&#25104;&#21477;&#23376;&#24182;&#19982;&#21442;&#32771;&#32763;&#35793;&#19968;&#36215;&#29992;&#20110;&#22870;&#21169;&#35745;&#31639;&#12290;&#36825;&#20010;&#35745;&#31639;&#20986;&#30340;&#22870;&#21169;&#20248;&#21270;&#20102;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GAN) offer a promising approach for Neural Machine Translation (NMT). However, feeding multiple morphologically languages into a single model during training reduces the NMT's performance. In GAN, similar to bilingual models, multilingual NMT only considers one reference translation for each sentence during model training. This single reference translation limits the GAN model from learning sufficient information about the source sentence representation. Thus, in this article, we propose Denoising Adversarial Auto-encoder-based Sentence Interpolation (DAASI) approach to perform sentence interpolation by learning the intermediate latent representation of the source and target sentences of multilingual language pairs. Apart from latent representation, we also use the Wasserstein-GAN approach for the multilingual NMT model by incorporating the model generated sentences of multiple languages for reward computation. This computed reward optimizes the perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#23588;&#40065;&#24052;&#25991;&#21270;&#38382;&#20505;&#35821;&#65288;$\mathcal{E}$ k\'u [MASK]&#65289;&#25972;&#21512;&#21040;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#36890;&#36807;IkiniYor\`ub\'a&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#26080;&#27861;&#20934;&#30830;&#32763;&#35793;&#65292;&#32780;&#24494;&#35843;&#29616;&#26377;&#30340;NMT&#27169;&#22411;&#22312;&#32763;&#35793;&#20013;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.17972</link><description>&lt;p&gt;
$\mathcal{E}$ K\'U [MASK]: &#23558;&#23588;&#40065;&#24052;&#25991;&#21270;&#38382;&#20505;&#35821;&#25972;&#21512;&#21040;&#26426;&#22120;&#32763;&#35793;&#20013;
&lt;/p&gt;
&lt;p&gt;
$\mathcal{E}$ K\'U [MASK]: Integrating Yor\`ub\'a cultural greetings into machine translation. (arXiv:2303.17972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#23588;&#40065;&#24052;&#25991;&#21270;&#38382;&#20505;&#35821;&#65288;$\mathcal{E}$ k\'u [MASK]&#65289;&#25972;&#21512;&#21040;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#36890;&#36807;IkiniYor\`ub\'a&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#26080;&#27861;&#20934;&#30830;&#32763;&#35793;&#65292;&#32780;&#24494;&#35843;&#29616;&#26377;&#30340;NMT&#27169;&#22411;&#22312;&#32763;&#35793;&#20013;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#22312;&#23558;&#23588;&#40065;&#24052;&#35821;&#38382;&#20505;&#35821;&#65288;$\mathcal{E}$ k\'u [MASK]&#65289;&#32763;&#35793;&#25104;&#33521;&#25991;&#26102;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23588;&#40065;&#24052;&#35821;-&#33521;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;IkiniYor\`ub\'a&#65292;&#20854;&#20013;&#21253;&#21547;&#23588;&#40065;&#24052;&#35821;&#38382;&#20505;&#35821;&#21644;&#26679;&#20363;&#29992;&#20363;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21253;&#25324;Google&#21644;NLLB&#22312;&#20869;&#30340;&#19981;&#21516;&#22810;&#35821;&#35328;NMT&#31995;&#32479;&#30340;&#34920;&#29616;&#65292;&#24182;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#20934;&#30830;&#32763;&#35793;&#23588;&#40065;&#24052;&#35821;&#38382;&#20505;&#35821;&#21040;&#33521;&#35821;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;IkiniYor\`ub\'a&#30340;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#29616;&#26377;&#30340;NMT&#27169;&#22411;&#26469;&#35757;&#32451;&#19968;&#20010;&#23588;&#40065;&#24052;&#35821;-&#33521;&#35821;&#27169;&#22411;&#65292;&#19982;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;NMT&#27169;&#22411;&#30456;&#27604;&#65292;&#20854;&#34920;&#29616;&#26356;&#22909;&#65292;&#23613;&#31649;&#23427;&#20204;&#32463;&#36807;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the performance of massively multilingual neural machine translation (NMT) systems in translating Yor\`ub\'a greetings ($\mathcal{E}$ k\'u [MASK]), which are a big part of Yor\`ub\'a language and culture, into English. To evaluate these models, we present IkiniYor\`ub\'a, a Yor\`ub\'a-English translation dataset containing some Yor\`ub\'a greetings, and sample use cases. We analysed the performance of different multilingual NMT systems including Google and NLLB and show that these models struggle to accurately translate Yor\`ub\'a greetings into English. In addition, we trained a Yor\`ub\'a-English model by finetuning an existing NMT model on the training split of IkiniYor\`ub\'a and this achieved better performance when compared to the pre-trained multilingual NMT models, although they were trained on a large volume of data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#27604;&#36739;&#35821;&#35328;&#23398;&#20013;&#65292;&#36890;&#36807;&#20462;&#21098;&#35821;&#38899;&#23545;&#40784;&#26469;&#33258;&#21160;&#25913;&#36827;&#23545;&#20934;&#21516;&#28304;&#35789;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#32463;&#27979;&#35797;&#20854;&#25928;&#26524;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20174;&#22810;&#35821;&#31181;&#35789;&#27719;&#34920;&#20013;&#27491;&#30830;&#25512;&#26029;&#38899;&#23545;&#24212;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.17932</link><description>&lt;p&gt;
&#20462;&#21098;&#35821;&#38899;&#23545;&#40784;&#25913;&#21892;&#20174;&#22810;&#35821;&#31181;&#35789;&#27719;&#34920;&#20013;&#25512;&#26029;&#38899;&#23545;&#24212;&#27169;&#24335;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Trimming Phonetic Alignments Improves the Inference of Sound Correspondence Patterns from Multilingual Wordlists. (arXiv:2303.17932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#27604;&#36739;&#35821;&#35328;&#23398;&#20013;&#65292;&#36890;&#36807;&#20462;&#21098;&#35821;&#38899;&#23545;&#40784;&#26469;&#33258;&#21160;&#25913;&#36827;&#23545;&#20934;&#21516;&#28304;&#35789;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#32463;&#27979;&#35797;&#20854;&#25928;&#26524;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20174;&#22810;&#35821;&#31181;&#35789;&#27719;&#34920;&#20013;&#27491;&#30830;&#25512;&#26029;&#38899;&#23545;&#24212;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#23545;&#24212;&#27169;&#24335;&#26159;&#21382;&#21490;&#35821;&#35328;&#27604;&#36739;&#20013;&#21516;&#28304;&#35789;&#26816;&#27979;&#21644;&#38899;&#31995;&#37325;&#24314;&#30340;&#22522;&#30784;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20174;&#35821;&#38899;&#23545;&#40784;&#21516;&#28304;&#35789;&#38598;&#20013;&#33258;&#21160;&#25512;&#26029;&#23545;&#24212;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#20294;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#22810;&#35821;&#31181;&#35789;&#27719;&#34920;&#38656;&#35201;&#38750;&#24120;&#22909;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#30001;&#20110;&#27880;&#37322;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#65292;&#22240;&#27492;&#25214;&#21040;&#33258;&#21160;&#25913;&#36827;&#23545;&#20934;&#21516;&#28304;&#35789;&#25968;&#25454;&#30340;&#26041;&#27861;&#26159;&#21487;&#21462;&#30340;&#12290;&#21463;&#36827;&#21270;&#29983;&#29289;&#23398;&#20013;&#25913;&#36827;&#23545;&#20934;&#30340;&#20462;&#21098;&#25216;&#26415;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25512;&#26029;&#23545;&#24212;&#20851;&#31995;&#20043;&#21069;&#20462;&#21098;&#27604;&#36739;&#35821;&#35328;&#23398;&#20013;&#30340;&#35821;&#38899;&#23545;&#40784;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#22312;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#23478;&#26063;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#22823;&#22411;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#36825;&#20123;&#25216;&#26415;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#20339;&#20462;&#21098;&#25216;&#26415;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#40784;&#30340;&#25972;&#20307;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#22312;&#20174;&#22810;&#35821;&#31181;&#35789;&#27719;&#34920;&#20013;&#27491;&#30830;&#25512;&#26029;&#38899;&#23545;&#24212;&#27169;&#24335;&#30340;&#33021;&#21147;&#19978;&#26377;&#26126;&#26174;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sound correspondence patterns form the basis of cognate detection and phonological reconstruction in historical language comparison. Methods for the automatic inference of correspondence patterns from phonetically aligned cognate sets have been proposed, but their application to multilingual wordlists requires extremely well annotated datasets. Since annotation is tedious and time consuming, it would be desirable to find ways to improve aligned cognate data automatically. Taking inspiration from trimming techniques in evolutionary biology, which improve alignments by excluding problematic sites, we propose a workflow that trims phonetic alignments in comparative linguistics prior to the inference of correspondence patterns. Testing these techniques on a large standardized collection of ten datasets with expert annotations from different language families, we find that the best trimming technique substantially improves the overall consistency of the alignments. The results show a clear 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;JobHam-place&#30340;&#26234;&#33021;&#27714;&#32844;&#31995;&#32479;&#65292;&#20854;&#21253;&#21547;&#24037;&#20316;&#25512;&#33616;&#12289;&#31616;&#21382;&#25490;&#21517;&#21450;&#32844;&#20301;&#20202;&#34920;&#26495;&#31561;&#21151;&#33021;&#65292;&#36890;&#36807;&#33258;&#21160;&#20851;&#38190;&#23383;&#25552;&#21462;&#21644;Job/CV&#25490;&#21517;&#31639;&#27861;&#23454;&#29616;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35789;&#23884;&#20837;&#21644;&#20313;&#24358;&#30456;&#20284;&#24615;&#30340;&#31639;&#27861;&#26469;&#21305;&#37197;&#24037;&#20316;&#35201;&#27714;&#21644;&#27714;&#32844;&#32773;&#25216;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31995;&#32479;&#20855;&#26377;&#20934;&#30830;&#30340;&#24037;&#20316;&#25512;&#33616;&#21644;&#31616;&#21382;&#25490;&#21517;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.17930</link><description>&lt;p&gt;
&#36890;&#36807;&#26234;&#33021;&#25512;&#33616;&#24037;&#20316;&#21644;&#36807;&#28388;&#27714;&#32844;&#32773;&#30340;&#36873;&#39033;&#30340;JobHam-place&#12290;
&lt;/p&gt;
&lt;p&gt;
JobHam-place with smart recommend job options and candidate filtering options. (arXiv:2303.17930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;JobHam-place&#30340;&#26234;&#33021;&#27714;&#32844;&#31995;&#32479;&#65292;&#20854;&#21253;&#21547;&#24037;&#20316;&#25512;&#33616;&#12289;&#31616;&#21382;&#25490;&#21517;&#21450;&#32844;&#20301;&#20202;&#34920;&#26495;&#31561;&#21151;&#33021;&#65292;&#36890;&#36807;&#33258;&#21160;&#20851;&#38190;&#23383;&#25552;&#21462;&#21644;Job/CV&#25490;&#21517;&#31639;&#27861;&#23454;&#29616;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35789;&#23884;&#20837;&#21644;&#20313;&#24358;&#30456;&#20284;&#24615;&#30340;&#31639;&#27861;&#26469;&#21305;&#37197;&#24037;&#20316;&#35201;&#27714;&#21644;&#27714;&#32844;&#32773;&#25216;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31995;&#32479;&#20855;&#26377;&#20934;&#30830;&#30340;&#24037;&#20316;&#25512;&#33616;&#21644;&#31616;&#21382;&#25490;&#21517;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27605;&#19994;&#29983;&#20154;&#25968;&#30340;&#22686;&#21152;&#65292;&#35768;&#22810;&#27714;&#32844;&#24212;&#32856;&#32773;&#32463;&#24120;&#36935;&#21040;&#25214;&#24037;&#20316;&#30340;&#38590;&#39064;&#65292;&#32780;&#38599;&#20027;&#21017;&#32463;&#24120;&#36935;&#21040;&#38590;&#20197;&#36807;&#28388;&#27714;&#32844;&#32773;&#30340;&#24773;&#20917;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#20182;&#20204;&#30340;&#25928;&#29575;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27714;&#32844;&#32593;&#31449;&#32570;&#20047;&#24037;&#20316;&#25512;&#33616;&#12289;&#31616;&#21382;&#36807;&#28388;&#25110;&#25490;&#21517;&#21151;&#33021;&#65292;&#36825;&#20123;&#21151;&#33021;&#27809;&#26377;&#25972;&#21512;&#21040;&#31995;&#32479;&#20013;&#12290;&#22240;&#27492;&#65292;&#26412;&#39033;&#30446;&#23558;&#23454;&#29616;&#19968;&#20010;&#26234;&#33021;&#27714;&#32844;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#24037;&#20316;&#25512;&#33616;&#12289;&#31616;&#21382;&#25490;&#21517;&#29978;&#33267;&#24102;&#26377;&#25216;&#33021;&#21644;&#27714;&#32844;&#32773;&#21151;&#33021;&#30340;&#32844;&#20301;&#20202;&#34920;&#26495;&#12290;&#24037;&#20316;&#25512;&#33616;&#21644;&#31616;&#21382;&#25490;&#21517;&#20174;&#33258;&#21160;&#20851;&#38190;&#23383;&#25552;&#21462;&#24320;&#22987;&#65292;&#20197;Job/CV&#25490;&#21517;&#31639;&#27861;&#32467;&#26463;&#12290;&#33258;&#21160;&#20851;&#38190;&#23383;&#25552;&#21462;&#30001;Job2Skill&#21644;&#22522;&#20110;Bert&#30340;CV2Skill&#27169;&#22411;&#23454;&#29616;&#12290;Job2Skill&#30001;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#23618;&#32452;&#25104;&#65292;&#32780;CV2Skill&#20027;&#35201;&#22522;&#20110;Bert&#65292;&#24182;&#36890;&#36807;&#31616;&#21382;&#23454;&#20307;&#25968;&#25454;&#38598;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23558;&#24037;&#20316;&#35201;&#27714;&#20013;&#30340;&#25216;&#33021;&#19982;&#27714;&#32844;&#32773;&#25216;&#33021;&#21305;&#37197;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#23884;&#20837;&#21644;&#20313;&#24358;&#30456;&#20284;&#24615;&#30340;&#26032;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#24037;&#20316;&#25512;&#33616;&#21644;&#31616;&#21382;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the increasing number of graduates, many applicants experience the situation about finding a job, and employers experience difficulty filtering job applicants, which might negatively impact their effectiveness. However, most job-hunting websites lack job recommendation and CV filtering or ranking functionality, which are not integrated into the system. Thus, a smart job hunter combined with the above functionality will be conducted in this project, which contains job recommendations, CV ranking and even a job dashboard for skills and job applicant functionality. Job recommendation and CV ranking starts from the automatic keyword extraction and end with the Job/CV ranking algorithm. Automatic keyword extraction is implemented by Job2Skill and the CV2Skill model based on Bert. Job2Skill consists of two components, text encoder and Gru-based layers, while CV2Skill is mainly based on Bert and fine-tunes the pre-trained model by the Resume- Entity dataset. Besides, to match skills fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#25991;&#21270;&#36801;&#31227;&#23398;&#20064;&#22312;&#20013;&#25991;&#24694;&#24847;&#35821;&#35328;&#26816;&#27979;&#19978;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#29305;&#23450;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#20559;&#35265;&#20250;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#27492;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#22312;&#27169;&#22411;&#35757;&#32451;&#26102;&#32771;&#34385;&#22810;&#20803;&#25991;&#21270;&#32972;&#26223;&#65292;&#20197;&#25552;&#39640;&#24694;&#24847;&#35821;&#35328;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17927</link><description>&lt;p&gt;
&#36328;&#25991;&#21270;&#36801;&#31227;&#23398;&#20064;&#22312;&#20013;&#25991;&#24694;&#24847;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-Cultural Transfer Learning for Chinese Offensive Language Detection. (arXiv:2303.17927v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#25991;&#21270;&#36801;&#31227;&#23398;&#20064;&#22312;&#20013;&#25991;&#24694;&#24847;&#35821;&#35328;&#26816;&#27979;&#19978;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#29305;&#23450;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#20559;&#35265;&#20250;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#27492;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#22312;&#27169;&#22411;&#35757;&#32451;&#26102;&#32771;&#34385;&#22810;&#20803;&#25991;&#21270;&#32972;&#26223;&#65292;&#20197;&#25552;&#39640;&#24694;&#24847;&#35821;&#35328;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#35821;&#35328;&#30340;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#32780;&#22312;&#19981;&#21516;&#25991;&#21270;&#21644;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#27010;&#25324;&#21017;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65306;&#38500;&#20102;&#35789;&#27719;&#12289;&#21477;&#27861;&#21644;&#35821;&#20041;&#19978;&#30340;&#24046;&#24322;&#22806;&#65292;&#25991;&#21270;&#35268;&#33539;&#21644;&#25935;&#24863;&#24615;&#31561;&#35821;&#29992;&#26041;&#38754;&#30340;&#22240;&#32032;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21464;&#21270;&#24456;&#22823;&#12290;&#26412;&#25991;&#38024;&#23545;&#20013;&#25991;&#24694;&#24847;&#35821;&#35328;&#26816;&#27979;&#65292;&#26088;&#22312;&#25506;&#31350;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#65288;&#38889;&#35821;&#21644;&#33521;&#35821;&#65289;&#30340;&#25968;&#25454;&#23454;&#29616;&#36801;&#31227;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29305;&#23450;&#25991;&#21270;&#23545;&#20160;&#20040;&#34987;&#35270;&#20026;&#24694;&#24847;&#30340;&#20559;&#35265;&#20250;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#22810;&#20803;&#25991;&#21270;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#20013;&#25991;&#24694;&#24847;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#19981;&#21516;&#29305;&#24449;&#26159;&#25935;&#24863;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#37327;&#36164;&#28304;&#30340; Few-shot &#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#38750;&#33521;&#35821;&#24694;&#24847;&#35821;&#35328;&#26816;&#27979;&#30340;&#26377;&#21069;&#36884;&#30340;&#21069;&#26223;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#36328;&#25991;&#21270;&#32763;&#35793;&#23398;&#20064;&#22312;&#24694;&#24847;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#24212;&#35813;&#32435;&#20837;&#22810;&#20803;&#25991;&#21270;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting offensive language is a challenging task. Generalizing across different cultures and languages becomes even more challenging: besides lexical, syntactic and semantic differences, pragmatic aspects such as cultural norms and sensitivities, which are particularly relevant in this context, vary greatly. In this paper, we target Chinese offensive language detection and aim to investigate the impact of transfer learning using offensive language detection data from different cultural backgrounds, specifically Korean and English. We find that culture-specific biases in what is considered offensive negatively impact the transferability of language models (LMs) and that LMs trained on diverse cultural data are sensitive to different features in Chinese offensive language detection. In a few-shot learning scenario, however, our study shows promising prospects for non-English offensive language detection with limited resources. Our findings highlight the importance of cross-cultural tra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#22312;&#38750;&#33258;&#22238;&#24402;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#24341;&#20837;&#36873;&#25321;&#24615;&#30693;&#35782;&#33976;&#39311;&#21644;&#28176;&#36827;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;NAT&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#21644;&#22797;&#26434;&#24230;&#20043;&#38388;&#23454;&#29616;&#28789;&#27963;&#26435;&#34913;&#65292;&#26377;&#21161;&#20110;NAT&#36229;&#36234;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2303.17910</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#36873;&#25321;&#24615;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation. (arXiv:2303.17910v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#22312;&#38750;&#33258;&#22238;&#24402;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#24341;&#20837;&#36873;&#25321;&#24615;&#30693;&#35782;&#33976;&#39311;&#21644;&#28176;&#36827;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;NAT&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#21644;&#22797;&#26434;&#24230;&#20043;&#38388;&#23454;&#29616;&#28789;&#27963;&#26435;&#34913;&#65292;&#26377;&#21161;&#20110;NAT&#36229;&#36234;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#65292;&#38750;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#65288;NAT&#65289;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#23384;&#22312;&#21103;&#20316;&#29992;&#65292;&#22914;&#23558;&#25945;&#24072;&#26426;&#20013;&#30340;&#38169;&#35823;&#20256;&#25773;&#21040;NAT&#23398;&#29983;&#20013;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;NAT&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#65292;&#24182;&#19988;&#24456;&#23569;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#35752;&#35770;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;NAT&#35780;&#20272;&#22120;&#26469;&#36827;&#34892;&#36873;&#25321;&#24615;&#30693;&#35782;&#33976;&#39311;&#65292;&#36873;&#25321;&#39640;&#36136;&#37327;&#19988;&#26131;&#20110;&#23398;&#20064;&#30340;NAT&#21451;&#22909;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28176;&#36827;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;NAT&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;WMT&#35821;&#35328;&#26041;&#21521;&#21644;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;NAT&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;NAT&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#21644;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#28789;&#27963;&#26435;&#34913;&#65292;&#36798;&#21040;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21482;&#33976;&#39311;5&#65285;&#30340;&#21407;&#22987;&#32763;&#35793;&#23601;&#21487;&#20197;&#24110;&#21161;NAT&#36229;&#36234;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benefiting from the sequence-level knowledge distillation, the Non-Autoregressive Transformer (NAT) achieves great success in neural machine translation tasks. However, existing knowledge distillation has side effects, such as propagating errors from the teacher to NAT students, which may limit further improvements of NAT models and are rarely discussed in existing research. In this paper, we introduce selective knowledge distillation by introducing an NAT evaluator to select NAT-friendly targets that are of high quality and easy to learn. In addition, we introduce a simple yet effective progressive distillation method to boost NAT performance. Experiment results on multiple WMT language directions and several representative NAT models show that our approach can realize a flexible trade-off between the quality and complexity of training data for NAT models, achieving strong performances. Further analysis shows that distilling only 5% of the raw translations can help an NAT outperform i
&lt;/p&gt;</description></item><item><title>WebQAmGaze&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#20302;&#25104;&#26412;&#30340;&#38405;&#35835;&#26102;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;332&#20301;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#65292;&#23545;&#30456;&#20851;&#27573;&#33853;&#30340;&#27880;&#35270;&#20284;&#20046;&#33021;&#22815;&#21453;&#26144;&#22238;&#31572;&#29702;&#35299;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20221;&#25968;&#25454;&#21487;&#20197;&#25512;&#21160;&#22522;&#20110;&#32593;&#32476;&#25668;&#20687;&#22836;&#30340;&#38405;&#35835;&#30740;&#31350;&#24182;&#24320;&#36767;&#26356;&#20415;&#23452;&#12289;&#26356;&#26131;&#33719;&#24471;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.17876</link><description>&lt;p&gt;
WebQAmGaze: &#19968;&#20221;&#22810;&#35821;&#35328;Webcam&#38405;&#35835;&#26102;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset. (arXiv:2303.17876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17876
&lt;/p&gt;
&lt;p&gt;
WebQAmGaze&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#20302;&#25104;&#26412;&#30340;&#38405;&#35835;&#26102;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;332&#20301;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#65292;&#23545;&#30456;&#20851;&#27573;&#33853;&#30340;&#27880;&#35270;&#20284;&#20046;&#33021;&#22815;&#21453;&#26144;&#22238;&#31572;&#29702;&#35299;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20221;&#25968;&#25454;&#21487;&#20197;&#25512;&#21160;&#22522;&#20110;&#32593;&#32476;&#25668;&#20687;&#22836;&#30340;&#38405;&#35835;&#30740;&#31350;&#24182;&#24320;&#36767;&#26356;&#20415;&#23452;&#12289;&#26356;&#26131;&#33719;&#24471;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21019;&#24314;&#20102;WebQAmGaze&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35821;&#31181;&#20302;&#25104;&#26412;&#30340;&#38405;&#35835;&#26102;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#20844;&#24179;&#36879;&#26126;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;WebQAmGaze&#21253;&#25324;&#20102;&#26469;&#33258;332&#20301;&#21442;&#19982;&#32773;&#38405;&#35835;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#24503;&#35821;&#25991;&#26412;&#26102;&#30340;&#32593;&#32476;&#25668;&#20687;&#22836;&#30524;&#21160;&#25968;&#25454;&#12290;&#27599;&#20010;&#21442;&#19982;&#32773;&#37117;&#20250;&#23436;&#25104;&#20004;&#20010;&#38405;&#35835;&#20219;&#21153;&#65292;&#21253;&#25324;&#20116;&#31687;&#25991;&#31456;&#30340;&#27491;&#24120;&#38405;&#35835;&#21644;&#20449;&#24687;&#23547;&#25214;&#20219;&#21153;&#12290;&#32463;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#30456;&#20851;&#27573;&#33853;&#30340;&#27880;&#35270;&#20284;&#20046;&#24847;&#21619;&#30528;&#22238;&#31572;&#29702;&#35299;&#38382;&#39064;&#30340;&#27491;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#19982;&#39640;&#36136;&#37327;&#30340;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;Webcam-ET&#33719;&#24471;&#30340;&#29305;&#24449;&#19982;&#21830;&#19994;ET&#35774;&#22791;&#30340;&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#20013;&#31561;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20221;&#25968;&#25454;&#21487;&#20197;&#25512;&#21160;&#22522;&#20110;&#32593;&#32476;&#25668;&#20687;&#22836;&#30340;&#38405;&#35835;&#30740;&#31350;&#24182;&#24320;&#36767;&#26356;&#20415;&#23452;&#12289;&#26356;&#26131;&#33719;&#24471;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#24335;&#12290;WebQAmGaze&#23545;&#20110;&#20102;&#35299;&#38382;&#39064;&#22238;&#31572;&#30340;&#35748;&#30693;&#36807;&#31243;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#20844;&#24179;&#36879;&#26126;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We create WebQAmGaze, a multilingual low-cost eye-tracking-while-reading dataset, designed to support the development of fair and transparent NLP models. WebQAmGaze includes webcam eye-tracking data from 332 participants naturally reading English, Spanish, and German texts. Each participant performs two reading tasks composed of five texts, a normal reading and an information-seeking task. After preprocessing the data, we find that fixations on relevant spans seem to indicate correctness when answering the comprehension questions. Additionally, we perform a comparative analysis of the data collected to high-quality eye-tracking data. The results show a moderate correlation between the features obtained with the webcam-ET compared to those of a commercial ET device. We believe this data can advance webcam-based reading studies and open a way to cheaper and more accessible data collection. WebQAmGaze is useful to learn about the cognitive processes behind question answering (QA) and to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#21019;&#20316;&#19968;&#31687;&#20551;&#30340;&#31185;&#23398;&#35770;&#25991;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#39564;&#35777;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20165;&#22522;&#20110;&#35821;&#35328;&#20449;&#24687;&#35299;&#37322;&#22825;&#25991;&#35266;&#27979;&#21644;&#28304;&#65292;&#24182;&#20026;&#21516;&#34892;&#35780;&#23457;&#35782;&#21035;&#27450;&#35784;&#24615;&#29983;&#25104;&#30340;&#31185;&#23398;&#35770;&#25991;&#25552;&#20379;&#28508;&#22312;&#25163;&#27573;&#12290;&#32467;&#35770;&#26159;&#65292;&#30446;&#21069;&#22825;&#25991;&#23398;&#23478;&#30340;&#24037;&#20316;&#26159;&#23433;&#20840;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.17853</link><description>&lt;p&gt;
AI&#33021;&#21542;&#35753;&#20285;&#29595;&#23556;&#32447;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#22833;&#19994;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI Put Gamma-Ray Astrophysicists Out of a Job?. (arXiv:2303.17853v1 [physics.pop-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#21019;&#20316;&#19968;&#31687;&#20551;&#30340;&#31185;&#23398;&#35770;&#25991;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#39564;&#35777;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20165;&#22522;&#20110;&#35821;&#35328;&#20449;&#24687;&#35299;&#37322;&#22825;&#25991;&#35266;&#27979;&#21644;&#28304;&#65292;&#24182;&#20026;&#21516;&#34892;&#35780;&#23457;&#35782;&#21035;&#27450;&#35784;&#24615;&#29983;&#25104;&#30340;&#31185;&#23398;&#35770;&#25991;&#25552;&#20379;&#28508;&#22312;&#25163;&#27573;&#12290;&#32467;&#35770;&#26159;&#65292;&#30446;&#21069;&#22825;&#25991;&#23398;&#23478;&#30340;&#24037;&#20316;&#26159;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#21019;&#20316;&#19968;&#31687;&#25991;&#31456;&#30340;&#33021;&#21147;&#12290;&#36825;&#31687;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#23384;&#22312;&#30340;&#25104;&#20687;&#22823;&#27668;&#20999;&#20262;&#31185;&#22827;&#26395;&#36828;&#38236;(IATC)&#38453;&#21015;&#26816;&#27979;&#33033;&#20914;&#26143;&#39118;&#26143;&#20113;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#20165;&#22522;&#20110;&#35821;&#35328;&#20449;&#24687;&#35299;&#37322;&#22825;&#25991;&#35266;&#27979;&#21644;&#28304;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#22312;&#21516;&#34892;&#35780;&#35758;&#36807;&#31243;&#20013;&#22914;&#20309;&#35782;&#21035;&#35784;&#39575;&#29983;&#25104;&#30340;&#31185;&#23398;&#35770;&#25991;&#30340;&#28508;&#22312;&#25163;&#27573;&#65288;&#32771;&#34385;&#21040;&#36825;&#20123;&#24037;&#20855;&#23578;&#26410;&#37096;&#32626;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#23383;&#27700;&#21360;&#65289;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22825;&#25991;&#23398;&#23478;&#30340;&#24037;&#20316;&#30446;&#21069;&#26159;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In what will likely be a litany of generative-model-themed arXiv submissions celebrating April the 1st, we evaluate the capacity of state-of-the-art transformer models to create a paper detailing the detection of a Pulsar Wind Nebula with a non-existent Imaging Atmospheric Cherenkov Telescope (IACT) Array. We do this to evaluate the ability of such models to interpret astronomical observations and sources based on language information alone, and to assess potential means by which fraudulently generated scientific papers could be identified during peer review (given that reliable generative model watermarking has yet to be deployed for these tools). We conclude that our jobs as astronomers are safe for the time being. From this point on, prompts given to ChatGPT and Stable Diffusion are shown in orange, text generated by ChatGPT is shown in black, whereas analysis by the (human) authors is in blue.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#20013;&#23398;&#20064;&#36807;&#31243;&#24863;&#30693;&#30340;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#65292;&#32852;&#21512;&#23398;&#20064;&#35270;&#39057;&#34920;&#31034;&#21644;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#21487;&#20197;&#22686;&#24378;&#36807;&#31243;&#25512;&#29702;&#30340;&#26032;&#21151;&#33021;&#65292;&#21516;&#26102;&#23545;&#20010;&#20307;&#27493;&#39588;&#30340;&#35782;&#21035;&#20063;&#33021;&#24471;&#21040;&#21152;&#24378;&#12290;</title><link>http://arxiv.org/abs/2303.17839</link><description>&lt;p&gt;
&#20174;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#20013;&#23398;&#20064;&#36807;&#31243;&#24863;&#30693;&#30340;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations. (arXiv:2303.17839v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#20013;&#23398;&#20064;&#36807;&#31243;&#24863;&#30693;&#30340;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#65292;&#32852;&#21512;&#23398;&#20064;&#35270;&#39057;&#34920;&#31034;&#21644;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#21487;&#20197;&#22686;&#24378;&#36807;&#31243;&#25512;&#29702;&#30340;&#26032;&#21151;&#33021;&#65292;&#21516;&#26102;&#23545;&#20010;&#20307;&#27493;&#39588;&#30340;&#35782;&#21035;&#20063;&#33021;&#24471;&#21040;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#30340;&#20016;&#23500;&#36164;&#28304;&#20026;&#29702;&#35299;&#36807;&#31243;&#27963;&#21160;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#35270;&#39057;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#34920;&#31034;&#23545;&#22522;&#20110;&#22823;&#35268;&#27169;&#32593;&#32476;&#25945;&#23398;&#35270;&#39057;&#21450;&#20854;&#35299;&#35828;&#30340;&#20010;&#20307;&#27493;&#39588;&#21450;&#20854;&#26102;&#38388;&#39034;&#24207;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#19981;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#12290;&#26041;&#27861;&#32852;&#21512;&#23398;&#20064;&#35270;&#39057;&#34920;&#31034;&#21644;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#65292;&#20197;&#25429;&#33719;&#27493;&#39588;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#24040;&#22823;&#20010;&#20307;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23398;&#20064;&#26102;&#38388;&#25490;&#24207;&#19981;&#20165;&#33021;&#22815;&#22686;&#24378;&#36807;&#31243;&#25512;&#29702;&#30340;&#26032;&#21151;&#33021;&#65292;&#36824;&#21487;&#20197;&#21152;&#24378;&#23545;&#20010;&#20307;&#27493;&#39588;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27493;&#39588;&#20998;&#31867;&#65288;&#22312;COIN/EPIC-Kitchens&#19978;&#20998;&#21035;&#22686;&#21152;2.8% / 3.3%&#65289;&#21644;&#27493;&#39588;&#39044;&#27979;&#65288;&#22312;COIN&#19978;&#22686;&#21152;7.4%&#65289;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27493;&#39588;&#25552;&#21462;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abundance of instructional videos and their narrations over the Internet offers an exciting avenue for understanding procedural activities. In this work, we propose to learn video representation that encodes both action steps and their temporal ordering, based on a large-scale dataset of web instructional videos and their narrations, without using human annotations. Our method jointly learns a video representation to encode individual step concepts, and a deep probabilistic model to capture both temporal dependencies and immense individual variations in the step ordering. We empirically demonstrate that learning temporal ordering not only enables new capabilities for procedure reasoning, but also reinforces the recognition of individual steps. Our model significantly advances the state-of-the-art results on step classification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting (+7.4% on COIN). Moreover, our model attains promising results in zero-shot inference for step c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24212;&#29992;&#20256;&#32479;&#38889;&#21307;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#24212;&#29992;&#38889;&#22269;&#22269;&#23478;&#20013;&#21307;&#21307;&#29983;&#25191;&#29031;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;57.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.17807</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#32479;&#38889;&#21307;&#20013;&#30340;&#28508;&#21147;&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#25991;&#21270;&#36866;&#24212;&#20445;&#20581;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language models in Traditional Korean Medicine: A Foundation Model Approach to Culturally-Adapted Healthcare. (arXiv:2303.17807v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24212;&#29992;&#20256;&#32479;&#38889;&#21307;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#24212;&#29992;&#38889;&#22269;&#22269;&#23478;&#20013;&#21307;&#21307;&#29983;&#25191;&#29031;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;57.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#38889;&#21307;&#27880;&#37325;&#20010;&#20307;&#21270;&#35786;&#26029;&#21644;&#27835;&#30103;&#65292;&#25968;&#25454;&#26377;&#38480;&#19988;&#36807;&#31243;&#38544;&#24615;&#65292;&#20351;AI&#24314;&#27169;&#22256;&#38590;&#12290;GPT-3.5&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23613;&#31649;&#32570;&#20047;&#21307;&#23398;&#19987;&#19994;&#22521;&#35757;&#65292;&#20294;&#24050;&#26174;&#31034;&#20986;&#20986;&#33394;&#30340;&#21307;&#30103;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;GPT-3.5&#21644;GPT-4&#22312;&#24212;&#29992;&#38889;&#22269;&#22269;&#23478;&#20013;&#21307;&#21307;&#29983;&#25191;&#29031;&#32771;&#35797;&#20013;&#30340;&#28508;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#21462;&#24471;&#20102;42.06%&#21644;57.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#20854;&#20013;GPT-4&#25509;&#36817;&#21450;&#26684;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction: Traditional Korean medicine (TKM) emphasizes individualized diagnosis and treatment, making AI modeling difficult due to limited data and implicit processes. GPT-3.5 and GPT-4, large language models, have shown impressive medical knowledge despite lacking medicine-specific training. This study aimed to assess the capabilities of GPT-3.5 and GPT-4 for TKM using the Korean National Licensing Examination for Korean Medicine Doctors. Methods: GPT-3.5 (February 2023) and GPT-4 (March 2023) models answered 340 questions from the 2022 examination across 12 subjects. Each question was independently evaluated five times in an initialized session. Results: GPT-3.5 and GPT-4 achieved 42.06% and 57.29% accuracy, respectively, with GPT-4 nearing passing performance. There were significant differences in accuracy by subjects, with 83.75% accuracy for neuropsychiatry compared to 28.75% for internal medicine (2). Both models showed high accuracy in recall-based and diagnosis-based questi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#36866;&#37197;&#22120;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#29992;&#25143;&#30446;&#24405;&#21644;&#23545;&#35805;&#34892;&#20026;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22810;&#36718;&#23545;&#35805;&#30340;&#20010;&#24615;&#21270;&#35821;&#38899;&#35782;&#21035;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#26080;&#19978;&#19979;&#25991;&#27169;&#22411;&#23454;&#29616;&#20102;58%&#30340;&#24179;&#22343;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2303.17799</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#35805;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#36866;&#37197;&#22120;&#29992;&#20110;&#20010;&#24615;&#21270;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Dialog act guided contextual adapter for personalized speech recognition. (arXiv:2303.17799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#36866;&#37197;&#22120;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#29992;&#25143;&#30446;&#24405;&#21644;&#23545;&#35805;&#34892;&#20026;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22810;&#36718;&#23545;&#35805;&#30340;&#20010;&#24615;&#21270;&#35821;&#38899;&#35782;&#21035;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#26080;&#19978;&#19979;&#25991;&#27169;&#22411;&#23454;&#29616;&#20102;58%&#30340;&#24179;&#22343;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;E2E ASR&#65289;&#27169;&#22411;&#20013;&#30340;&#22810;&#36718;&#23545;&#35805;&#30340;&#20010;&#24615;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#20851;&#20110;&#19978;&#19979;&#25991;&#36866;&#37197;&#22120;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;&#20351;&#29992;&#29992;&#25143;&#30446;&#24405;&#30340;&#32597;&#35265;&#35789;&#27719;&#35782;&#21035;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#36866;&#24212;&#24615;&#27809;&#26377;&#25972;&#21512;&#19968;&#20010;&#37325;&#35201;&#32447;&#32034;&#65292;&#21363;&#22312;&#22810;&#36718;&#23545;&#35805;&#22330;&#26223;&#20013;&#21487;&#29992;&#30340;&#23545;&#35805;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#36866;&#37197;&#22120;&#32593;&#32476;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#21033;&#29992;&#23545;&#35805;&#34892;&#20026;&#26469;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#29992;&#25143;&#30446;&#24405;&#65292;&#24182;&#22522;&#20110;&#36733;&#20307;&#30701;&#35821;&#21644;&#29992;&#25143;&#30446;&#24405;&#20043;&#38388;&#30340;&#38899;&#39057;&#21644;&#35821;&#20041;&#20851;&#31995;&#21019;&#24314;&#26597;&#35810;&#65292;&#20197;&#26356;&#22909;&#22320;&#24341;&#23548;&#19978;&#19979;&#25991;&#20559;&#32622;&#12290;&#22312;&#24037;&#19994;&#35821;&#38899;&#21161;&#25163;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65288;&#20165;&#23545;&#35805;&#34892;&#20026;&#32534;&#30721;&#22120;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#36866;&#24212;&#27169;&#22411;&#65289;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26080;&#19978;&#19979;&#25991;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#22823;&#30340;&#25913;&#36827;&#65306;&#22312;&#22810;&#36718;&#23545;&#35805;&#22330;&#26223;&#20013;&#24179;&#22343;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;58&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization in multi-turn dialogs has been a long standing challenge for end-to-end automatic speech recognition (E2E ASR) models. Recent work on contextual adapters has tackled rare word recognition using user catalogs. This adaptation, however, does not incorporate an important cue, the dialog act, which is available in a multi-turn dialog scenario. In this work, we propose a dialog act guided contextual adapter network. Specifically, it leverages dialog acts to select the most relevant user catalogs and creates queries based on both -- the audio as well as the semantic relationship between the carrier phrase and user catalogs to better guide the contextual biasing. On industrial voice assistant datasets, our model outperforms both the baselines - dialog act encoder-only model, and the contextual adaptation, leading to the most improvement over the no-context model: 58% average relative word error rate reduction (WERR) in the multi-turn dialog scenario, in comparison to the prior
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17760</link><description>&lt;p&gt;
CAMEL: &#29992;&#20110;&#8220;&#24515;&#26234;&#8221;&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#31038;&#32676;&#30340;&#20132;&#20114;&#24335;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society. (arXiv:2303.17760v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#24050;&#21462;&#24471;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20154;&#31867;&#30340;&#25351;&#23548;&#65292;&#20197;&#24341;&#23548;&#23545;&#35805;&#65292;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26500;&#24314;&#21487;&#25193;&#23637;&#25216;&#26415;&#20197;&#20419;&#36827;&#20132;&#20114;&#24335;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#24182;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#30340;&#8220;&#35748;&#30693;&#8221;&#36807;&#31243;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#23454;&#29616;&#33258;&#20027;&#21512;&#20316;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#21551;&#21160;&#25552;&#31034;&#26469;&#24341;&#23548;&#32842;&#22825;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20154;&#31867;&#24847;&#22270;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35282;&#33394;&#25198;&#28436;&#26469;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#32842;&#22825;&#20195;&#29702;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of conversational and chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and provide insight into their "cognitive" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of chat agents, providing a valuable resource for investigating conversational language models. Our contributions include introducing a novel communicative agent framewor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#32422;&#30340;&#37327;&#23376;&#36719;&#20214;&#35774;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#30830;&#20445;&#37327;&#23376;&#30005;&#36335;&#26500;&#24314;&#36807;&#31243;&#30340;&#27491;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#39564;&#35777;&#37327;&#23376;&#30005;&#36335;&#30340;&#27491;&#30830;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.17750</link><description>&lt;p&gt;
&#22522;&#20110;&#21512;&#32422;&#30340;&#37327;&#23376;&#36719;&#20214;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Design by Contract Framework for Quantum Software. (arXiv:2303.17750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#32422;&#30340;&#37327;&#23376;&#36719;&#20214;&#35774;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#30830;&#20445;&#37327;&#23376;&#30005;&#36335;&#26500;&#24314;&#36807;&#31243;&#30340;&#27491;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#39564;&#35777;&#37327;&#23376;&#30005;&#36335;&#30340;&#27491;&#30830;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#30340;&#37327;&#23376;&#36719;&#20214;&#65292;&#26368;&#36817;&#30740;&#31350;&#20102;&#33258;&#21160;&#30830;&#20445;&#37327;&#23376;&#36719;&#20214;&#27491;&#30830;&#24615;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#22266;&#23450;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#32780;&#19981;&#26159;&#26500;&#24314;&#37327;&#23376;&#30005;&#36335;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#36825;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#20294;&#20351;&#29992;&#19981;&#21516;&#21442;&#25968;&#25353;&#29031;&#30456;&#21516;&#30340;&#36807;&#31243;&#26500;&#24314;&#30005;&#36335;&#30340;&#27491;&#30830;&#24615;&#24182;&#19981;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#36719;&#20214;&#30340;&#21512;&#32422;&#35774;&#35745;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#23884;&#20837;python&#30340;&#35821;&#35328;&#65292;&#29992;&#20110;&#32534;&#20889;&#20851;&#20110;&#30001;&#26576;&#20123;&#36807;&#31243;&#26500;&#24314;&#30340;&#25152;&#26377;&#37327;&#23376;&#30005;&#36335;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#29366;&#24577;&#30340;&#26029;&#35328;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#32534;&#20889;&#20851;&#20110;&#27979;&#37327;&#32467;&#26524;&#30340;&#32479;&#35745;&#22788;&#29702;&#30340;&#26029;&#35328;&#65292;&#20197;&#30830;&#20445;&#33719;&#24471;&#26368;&#32456;&#32467;&#26524;&#30340;&#36807;&#31243;&#30340;&#27491;&#30830;&#24615;&#12290;&#36825;&#20123;&#26029;&#35328;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#27169;&#25311;&#22120;&#33258;&#21160;&#26816;&#26597;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#20026;&#19968;&#20123;&#24191;&#27867;&#20351;&#29992;&#30340;&#37327;&#23376;&#31639;&#27861;&#32534;&#20889;&#20102;assertions&#12290;
&lt;/p&gt;
&lt;p&gt;
To realize reliable quantum software, techniques to automatically ensure the quantum software's correctness have recently been investigated. However, they primarily focus on fixed quantum circuits rather than the procedure of building quantum circuits. Despite being a common approach, the correctness of building circuits using different parameters following the same procedure is not guaranteed. To this end, we propose a design-by-contract framework for quantum software. Our framework provides a python-embedded language to write assertions on the input and output states of all quantum circuits built by certain procedures. Additionally, it provides a method to write assertions about the statistical processing of measurement results to ensure the procedure's correctness for obtaining the final result. These assertions are automatically checked using a quantum computer simulator. For evaluation, we implemented our framework and wrote assertions for some widely used quantum algorithms. Cons
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(GPT&#21644;BERT)&#35782;&#21035;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#24615;&#33021;, &#32467;&#26524;&#26174;&#31034;BERT&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#20854;&#20013;PubMedBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#31934;&#24230;&#21644;F1&#20998;&#25968;&#65292;BioM-ALBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.17728</link><description>&lt;p&gt;
&#22522;&#20110;GPT&#21644;BERT&#30340;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#37492;&#23450;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text. (arXiv:2303.17728v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17728
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(GPT&#21644;BERT)&#35782;&#21035;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#24615;&#33021;, &#32467;&#26524;&#26174;&#31034;BERT&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#20854;&#20013;PubMedBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#31934;&#24230;&#21644;F1&#20998;&#25968;&#65292;BioM-ALBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;(PPIs)&#23545;&#20110;&#29702;&#35299;&#36951;&#20256;&#26426;&#21046;&#12289;&#30142;&#30149;&#21457;&#30149;&#26426;&#29702;&#21644;&#33647;&#29289;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#21644;&#20934;&#30830;&#25552;&#21462;PPIs&#20197;&#20419;&#36827;&#31185;&#23398;&#30693;&#35782;&#30340;&#21457;&#25496;&#12290;&#24050;&#32463;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;(GPT)&#21644;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#21464;&#21387;&#22120;(BERT)&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#25163;&#21160;&#32534;&#21046;&#30340;LLL&#22522;&#20934;&#35821;&#26009;&#24211;&#35780;&#20272;&#20102;&#21508;&#31181;GPT&#21644;BERT&#27169;&#22411;&#30340;PPI&#35782;&#21035;&#24615;&#33021;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;77&#20010;&#21477;&#23376;&#20013;&#30340;164&#20010;PPIs&#12290;BERT&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;PubMedBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#31934;&#24230;(85.17%)&#21644;F1&#20998;&#25968;(86.47%)&#65292;BioM-ALBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#21484;&#22238;&#29575;(93.83%)&#12290;&#23613;&#31649;GPT-4&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#20854;&#24615;&#33021;&#21487;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting protein-protein interactions (PPIs) is crucial for understanding genetic mechanisms, disease pathogenesis, and drug design. However, with the fast-paced growth of biomedical literature, there is a growing need for automated and accurate extraction of PPIs to facilitate scientific knowledge discovery. Pre-trained language models, such as generative pre-trained transformer (GPT) and bidirectional encoder representations from transformers (BERT), have shown promising results in natural language processing (NLP) tasks. We evaluated the PPI identification performance of various GPT and BERT models using a manually curated benchmark corpus of 164 PPIs in 77 sentences from learning language in logic (LLL). BERT-based models achieved the best overall performance, with PubMedBERT achieving the highest precision (85.17%) and F1-score (86.47%) and BioM-ALBERT achieving the highest recall (93.83%). Despite not being explicitly trained for biomedical texts, GPT-4 achieved comparable perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21738;&#20123;&#27169;&#31946;&#24320;&#25918;&#24615;&#38382;&#39064;&#26368;&#36866;&#21512;&#36890;&#36807;&#23545;&#35805;&#22238;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#38382;&#39064;&#39640;&#24230;&#31038;&#20132;&#21644;&#20010;&#20154;&#21270;&#65292;&#23545;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2303.17710</link><description>&lt;p&gt;
&#26377;&#21738;&#20123;&#38382;&#39064;&#38656;&#35201;&#36827;&#34892;&#20132;&#35848;&#25165;&#33021;&#22238;&#31572;&#65311;&#19968;&#20010; AskReddit &#38382;&#39064;&#26696;&#20363;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
What Types of Questions Require Conversation to Answer? A Case Study of AskReddit Questions. (arXiv:2303.17710v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21738;&#20123;&#27169;&#31946;&#24320;&#25918;&#24615;&#38382;&#39064;&#26368;&#36866;&#21512;&#36890;&#36807;&#23545;&#35805;&#22238;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#38382;&#39064;&#39640;&#24230;&#31038;&#20132;&#21644;&#20010;&#20154;&#21270;&#65292;&#23545;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20250;&#35805;&#31995;&#32479;&#65288;&#22914;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#35821;&#38899;&#23545;&#35805;&#31995;&#32479;&#21644;&#26234;&#33021;&#38899;&#31665;&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#24050;&#32463;&#28145;&#21051;&#22320;&#24433;&#21709;&#20102;&#29616;&#20195;&#25968;&#23383;&#29983;&#27963;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#22238;&#31572;&#26126;&#30830;&#23450;&#20041;&#30340;&#38382;&#39064;&#65292;&#32780;&#38750;&#25903;&#25345;&#29992;&#25143;&#25506;&#32034;&#22797;&#26434;&#30340;&#12289;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#21738;&#20123;&#27169;&#31946;&#30340;&#12289;&#24320;&#25918;&#24615;&#38382;&#39064;&#26368;&#36866;&#21512;&#36890;&#36807;&#23545;&#35805;&#26469;&#22238;&#31572;&#65292;&#25512;&#21160;&#20250;&#35805;&#31995;&#32479;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174; AskReddit &#19978;&#21457;&#24067;&#30340;100&#19975;&#20010;&#24320;&#25918;&#24335;&#35831;&#27714;&#20013;&#38543;&#26426;&#25277;&#21462;&#20102;500&#20010;&#38382;&#39064;&#65292;&#28982;&#21518;&#25307;&#21215;&#22312;&#32447;&#24037;&#20154;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#30340;8&#20010;&#35810;&#38382;&#12290;&#25105;&#20204;&#36824;&#25191;&#34892;&#24320;&#25918;&#24335;&#32534;&#30721;&#65292;&#23558;&#38382;&#39064;&#20998;&#31867;&#20026;27&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20154;&#20204;&#35748;&#20026;&#38656;&#35201;&#20132;&#35848;&#25165;&#33021;&#28385;&#24847;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#39640;&#24230;&#31038;&#20132;&#21644;&#20010;&#20154;&#21270;&#30340;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#22914;&#20309;&#36866;&#24212;&#29992;&#25143;&#38656;&#27714;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of automated conversational systems such as chatbots, spoken-dialogue systems, and smart speakers, has significantly impacted modern digital life. However, these systems are primarily designed to provide answers to well-defined questions rather than to support users in exploring complex, ill-defined questions. In this paper, we aim to push the boundaries of conversational systems by examining the types of nebulous, open-ended questions that can best be answered through conversation. We first sampled 500 questions from one million open-ended requests posted on AskReddit, and then recruited online crowd workers to answer eight inquiries about these questions. We also performed open coding to categorize the questions into 27 different domains. We found that the issues people believe require conversation to resolve satisfactorily are highly social and personal. Our work provides insights into how future research could be geared to align with users' needs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30693;&#35782;&#36873;&#25321;&#27169;&#22359;&#30340;&#23454;&#20307;&#26816;&#32034;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#20851;&#38190;&#23383;&#25552;&#21462;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#23548;&#21521;&#20132;&#20114;&#24314;&#27169;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17695</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#20027;&#35266;&#30693;&#35782;&#20132;&#20114;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Task Oriented Conversational Modelling With Subjective Knowledge. (arXiv:2303.17695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30693;&#35782;&#36873;&#25321;&#27169;&#22359;&#30340;&#23454;&#20307;&#26816;&#32034;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#20851;&#38190;&#23383;&#25552;&#21462;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#23548;&#21521;&#20132;&#20114;&#24314;&#27169;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23545;&#35805;&#27169;&#22411;&#37117;&#26159;&#22522;&#20110;&#25968;&#25454;&#24211;&#21644;API&#30340;&#31995;&#32479;&#26469;&#22788;&#29702;&#30340;&#12290;&#20294;&#26159;&#65292;&#29992;&#25143;&#30340;&#38382;&#39064;&#32463;&#24120;&#38656;&#35201;&#22788;&#29702;&#36825;&#20123;&#31995;&#32479;&#26080;&#27861;&#22788;&#29702;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38382;&#39064;&#30340;&#31572;&#26696;&#21487;&#20197;&#22312;&#23458;&#25143;&#35780;&#20215;&#21644;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#20013;&#25214;&#21040;&#12290;DSTC-11&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#37096;&#20998;&#32452;&#25104;&#30340;&#31649;&#36947;&#65292;&#21253;&#25324;&#30693;&#35782;&#23547;&#27714;&#22238;&#21512;&#26816;&#27979;&#12289;&#30693;&#35782;&#36873;&#25321;&#21644;&#21709;&#24212;&#29983;&#25104;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#22522;&#20110;&#20027;&#35266;&#30693;&#35782;&#30340;&#20132;&#20114;&#24335;&#27169;&#22411;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#25913;&#36827;&#30693;&#35782;&#36873;&#25321;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#25972;&#20010;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20307;&#26816;&#32034;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#26356;&#24555;&#30340;&#30693;&#35782;&#25628;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#30340;&#23454;&#20307;&#26816;&#32034;&#26041;&#27861;&#27604;&#22522;&#32447;&#27169;&#22411;&#24555;&#20102;7&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#20851;&#38190;&#23383;&#25552;&#21462;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#30693;&#35782;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#20102;4\%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing conversational models are handled by a database(DB) and API based systems. However, very often users' questions require information that cannot be handled by such systems. Nonetheless, answers to these questions are available in the form of customer reviews and FAQs. DSTC-11 proposes a three stage pipeline consisting of knowledge seeking turn detection, knowledge selection and response generation to create a conversational model grounded on this subjective knowledge. In this paper, we focus on improving the knowledge selection module to enhance the overall system performance. In particular, we propose entity retrieval methods which result in an accurate and faster knowledge search. Our proposed Named Entity Recognition (NER) based entity retrieval method results in 7X faster search compared to the baseline model. Additionally, we also explore a potential keyword extraction method which can improve the accuracy of knowledge selection. Preliminary results show a 4 \% improvement
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#23383;&#31526;&#32423;&#22122;&#38899;&#24494;&#35843;BERT&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#26041;&#35328;&#21644;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#21482;&#26377;&#22312;&#20219;&#21153;&#20381;&#36182;&#34920;&#38754;&#32423;&#21035;&#25552;&#31034;&#24182;&#19988;&#28304;-&#30446;&#26631;&#36328;&#35821;&#35328;&#23545;&#20855;&#26377;&#30456;&#23545;&#36739;&#39640;&#30340;&#35789;&#27719;&#37325;&#21472;&#26102;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;&#23383;&#31526;&#32423;&#22122;&#38899;&#23545;&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#25928;&#26524;&#25165;&#29305;&#21035;&#31361;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.17683</link><description>&lt;p&gt;
&#29992;&#23383;&#31526;&#32423;&#22122;&#38899;&#24494;&#35843;BERT&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#26041;&#35328;&#21450;&#30456;&#20851;&#35821;&#35328;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning BERT with Character-Level Noise for Zero-Shot Transfer to Dialects and Closely-Related Languages. (arXiv:2303.17683v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#23383;&#31526;&#32423;&#22122;&#38899;&#24494;&#35843;BERT&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#26041;&#35328;&#21644;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#21482;&#26377;&#22312;&#20219;&#21153;&#20381;&#36182;&#34920;&#38754;&#32423;&#21035;&#25552;&#31034;&#24182;&#19988;&#28304;-&#30446;&#26631;&#36328;&#35821;&#35328;&#23545;&#20855;&#26377;&#30456;&#23545;&#36739;&#39640;&#30340;&#35789;&#27719;&#37325;&#21472;&#26102;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;&#23383;&#31526;&#32423;&#22122;&#38899;&#23545;&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#25928;&#26524;&#25165;&#29305;&#21035;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#24418;&#24335;&#30340;&#23383;&#31526;&#32423;&#22122;&#38899;&#36827;&#34892;BERT&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#26041;&#35328;&#21644;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21477;&#23376;&#32423;&#20998;&#31867;&#20219;&#21153;&#19978;&#24494;&#35843;BERT&#65292;&#24182;&#22312;&#19968;&#20123;&#26410;&#35265;&#26041;&#35328;&#21644;&#35821;&#35328;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#23383;&#31526;&#32423;&#22122;&#38899;&#21487;&#20197;&#26159;&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#26497;&#20854;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#21017;&#19981;&#22826;&#26377;&#24110;&#21161;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#20219;&#21153;&#30340;&#24615;&#36136;&#21644;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#25506;&#35752;&#20102;&#36825;&#20123;&#24046;&#24322;&#65292;&#21457;&#29616;&#22312;&#20219;&#21153;&#20381;&#36182;&#34920;&#38754;&#32423;&#21035;&#25552;&#31034;&#24182;&#19988;&#28304;-&#30446;&#26631;&#36328;&#35821;&#35328;&#23545;&#20855;&#26377;&#30456;&#23545;&#36739;&#39640;&#30340;&#35789;&#27719;&#37325;&#21472;&#26102;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;&#23383;&#31526;&#32423;&#22122;&#38899;&#29305;&#21035;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we induce character-level noise in various forms when fine-tuning BERT to enable zero-shot cross-lingual transfer to unseen dialects and languages. We fine-tune BERT on three sentence-level classification tasks and evaluate our approach on an assortment of unseen dialects and languages. We find that character-level noise can be an extremely effective agent of cross-lingual transfer under certain conditions, while it is not as helpful in others. Specifically, we explore these differences in terms of the nature of the task and the relationships between source and target languages, finding that introduction of character-level noise during fine-tuning is particularly helpful when a task draws on surface level cues and the source-target cross-lingual pair has a relatively high lexical overlap with shorter (i.e., less meaningful) unseen tokens on average.
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#26159;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LLMs&#21021;&#22987;&#36755;&#20986;&#20248;&#21270;&#26041;&#27861;&#65292;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#65292;&#34987;&#35777;&#23454;&#22312;7&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.17651</link><description>&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#65306;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LM&#25913;&#36827;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Self-Refine: Iterative Refinement with Self-Feedback. (arXiv:2303.17651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17651
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#26159;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LLMs&#21021;&#22987;&#36755;&#20986;&#20248;&#21270;&#26041;&#27861;&#65292;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#65292;&#34987;&#35777;&#23454;&#22312;7&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19981;&#24635;&#26159;&#33021;&#22312;&#31532;&#19968;&#27425;&#33391;&#22909;&#22320;&#35299;&#20915;&#29983;&#25104;&#38382;&#39064;&#65288;&#22914;&#25688;&#35201;&#12289;&#31572;&#26696;&#12289;&#35299;&#37322;&#31561;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#65288;SELF-REFINE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#21644;&#31934;&#28860;&#30456;&#20284;&#22320;&#20248;&#21270;LLMs&#30340;&#21021;&#22987;&#36755;&#20986;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#65306;&#20351;&#29992;LLM&#29983;&#25104;&#36755;&#20986;&#65292;&#28982;&#21518;&#20801;&#35768;&#21516;&#19968;&#27169;&#22411;&#25552;&#20379;&#20854;&#33258;&#36523;&#36755;&#20986;&#30340;&#22810;&#26041;&#38754;&#21453;&#39304;&#65292;&#26368;&#21518;&#21033;&#29992;&#21453;&#39304;&#20351;&#30456;&#21516;&#27169;&#22411;&#31934;&#28860;&#20808;&#21069;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#31934;&#28860;&#26694;&#26550;&#19982;&#26089;&#26399;&#24037;&#20316;&#19981;&#21516;&#65292;&#26080;&#38656;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#25110;&#21152;&#24378;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#21333;&#20010;LLM&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#23545;&#19971;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#33539;&#22260;&#20174;&#35780;&#35770;&#37325;&#20889;&#21040;&#25968;&#23398;&#25512;&#29702;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#12290;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;SELF-REFINE&#29983;&#25104;&#30340;&#36755;&#20986;&#34987;&#20154;&#31867;&#21644;&#33258;&#21160;&#21270;&#25351;&#26631;&#20248;&#20808;&#20110;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#30452;&#25509;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like people, LLMs do not always generate the best text for a given generation problem on their first try (e.g., summaries, answers, explanations). Just as people then refine their text, we introduce SELF-REFINE, a framework for similarly improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an output using an LLM, then allow the same model to provide multi-aspect feedback for its own output; finally, the same model refines its previously generated output given its own feedback. Unlike earlier work, our iterative refinement framework does not require supervised training data or reinforcement learning, and works with a single LLM. We experiment with 7 diverse tasks, ranging from review rewriting to math reasoning, demonstrating that our approach outperforms direct generation. In all tasks, outputs generated with SELF-REFINE are preferred by humans and by automated metrics over those generated directly with GPT-3.5 and GPT-4, improving
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#25277;&#35937;&#27010;&#25324;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#30450;&#23457;&#20154;&#21592;&#35780;&#20272;&#26174;&#31034;ChatGPT&#29983;&#25104;&#30340;&#25688;&#35201;&#22312;&#20154;&#31867;&#35270;&#35282;&#19979;&#38590;&#20197;&#20998;&#36776;&#30495;&#20551;&#12290;</title><link>http://arxiv.org/abs/2303.17650</link><description>&lt;p&gt;
&#36890;&#36807;&#30450;&#23457;&#35780;&#20272;&#21644;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#27604;&#36739;ChatGPT&#29983;&#25104;&#30340;&#25277;&#35937;&#25688;&#35201;&#21644;&#30495;&#23454;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Comparing Abstractive Summaries Generated by ChatGPT to Real Summaries Through Blinded Reviewers and Text Classification Algorithms. (arXiv:2303.17650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#25277;&#35937;&#27010;&#25324;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#30450;&#23457;&#20154;&#21592;&#35780;&#20272;&#26174;&#31034;ChatGPT&#29983;&#25104;&#30340;&#25688;&#35201;&#22312;&#20154;&#31867;&#35270;&#35282;&#19979;&#38590;&#20197;&#20998;&#36776;&#30495;&#20551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;OpenAI&#24320;&#21457;&#30340;ChatGPT&#26159;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#30340;&#26368;&#26032;&#25104;&#21592;&#65292;&#30001;&#20110;&#20854;&#31867;&#20154;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#34987;&#19968;&#20123;&#20154;&#31216;&#20026;&#19968;&#39033;&#39072;&#35206;&#24615;&#25216;&#26415;&#12290;&#23613;&#31649;&#32593;&#32476;&#19978;&#26377;&#35768;&#22810;ChatGPT&#30340;&#20363;&#23376;&#26469;&#35780;&#20272;&#20854;&#24378;&#24369;&#20043;&#22788;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#31995;&#32479;&#24615;&#30340;&#30740;&#31350;&#23384;&#22312;&#12290;&#20026;&#20102;&#20026;ChatGPT&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#20570;&#20986;&#36129;&#29486;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#30450;&#23457;&#20154;&#21592;&#35780;&#20272;&#20102;ChatGPT&#22312;&#25277;&#35937;&#27010;&#25324;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#21487;&#20197;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#20294;&#20154;&#31867;&#26080;&#27861;&#21306;&#20998;&#30495;&#23454;&#25688;&#35201;&#21644;ChatGPT&#29983;&#25104;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have gathered significant attention due to their impressive performance on a variety of tasks. ChatGPT, developed by OpenAI, is a recent addition to the family of language models and is being called a disruptive technology by a few, owing to its human-like text-generation capabilities. Although, many anecdotal examples across the internet have evaluated ChatGPT's strength and weakness, only a few systematic research studies exist. To contribute to the body of literature of systematic research on ChatGPT, we evaluate the performance of ChatGPT on Abstractive Summarization by the means of automated metrics and blinded human reviewers. We also build automatic text classifiers to detect ChatGPT generated summaries. We found that while text classification algorithms can distinguish between real and generated summaries, humans are unable to distinguish between real summaries and those produced by ChatGPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#31572;&#26696;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#65292;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17649</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23558;&#19968;&#20010;&#20013;&#31561;&#22823;&#23567;&#30340;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;
&lt;/p&gt;
&lt;p&gt;
Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning. (arXiv:2303.17649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#33521;&#25991;GPT&#27169;&#22411;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#31572;&#26696;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#65292;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#21407;&#26412;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#30340;&#20013;&#31561;&#22823;&#23567;&#33521;&#25991;GPT&#27169;&#22411;&#65292;&#23545;&#40784;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#23567;&#23553;&#38381;&#39046;&#22495;&#12290;&#35813;&#27169;&#22411;&#34987;&#31934;&#32454;&#35843;&#25972;&#29992;&#20110;&#38382;&#31572;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#36824;&#38656;&#35201;&#35757;&#32451;&#21644;&#23454;&#29616;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#22870;&#21169;&#27169;&#22411;&#65289;&#65292;&#20197;&#35780;&#20998;&#24182;&#30830;&#23450;&#31572;&#26696;&#26159;&#21542;&#36866;&#29992;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#35813;&#32452;&#20214;&#26377;&#21161;&#20110;&#25913;&#36827;&#31995;&#32479;&#22238;&#31572;&#30340;&#35299;&#30721;&#21644;&#29983;&#25104;&#12290; BLEU&#21644;perplexity&#31561;&#25968;&#23383;&#24230;&#37327;&#26631;&#20934;&#34987;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#20351;&#29992;&#20154;&#31867;&#21028;&#26029;&#26469;&#27604;&#36739;&#35299;&#30721;&#25216;&#26415;&#19982;&#20854;&#20182;&#25216;&#26415;&#12290;&#26368;&#32456;&#65292;&#32467;&#26524;&#25903;&#25345;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20351;&#29992;&#22870;&#21169;&#27169;&#22411;&#26469;&#23545;&#40784;&#29983;&#25104;&#22238;&#31572;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a methodology to align a medium-sized GPT model, originally trained in English for an open domain, to a small closed domain in Spanish. The application for which the model is finely tuned is the question answering task. To achieve this we also needed to train and implement another neural network (which we called the reward model) that could score and determine whether an answer is appropriate for a given question. This component served to improve the decoding and generation of the answers of the system. Numerical metrics such as BLEU and perplexity were used to evaluate the model, and human judgment was also used to compare the decoding technique with others. Finally, the results favored the proposed method, and it was determined that it is feasible to use a reward model to align the generation of responses.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598; VIST-Character &#65292;&#35813;&#25968;&#25454;&#38598;&#20026;&#20197;&#35282;&#33394;&#20026;&#20013;&#24515;&#30340;&#27880;&#37322;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#65292;&#38024;&#23545;&#35813;&#25968;&#25454;&#38598;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#37325;&#35201;&#20154;&#29289;&#26816;&#27979;&#21644;&#35270;&#35273;&#25925;&#20107;&#20013;&#35282;&#33394;&#30340;&#23450;&#20301;&#65292;&#24182;&#22522;&#20110;&#20998;&#24067;&#30456;&#20284;&#24615;&#21644;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.17647</link><description>&lt;p&gt;
&#26816;&#27979;&#21644;&#23450;&#20301;&#35270;&#35273;&#25925;&#20107;&#20013;&#30340;&#37325;&#35201;&#20154;&#29289;
&lt;/p&gt;
&lt;p&gt;
Detecting and Grounding Important Characters in Visual Stories. (arXiv:2303.17647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598; VIST-Character &#65292;&#35813;&#25968;&#25454;&#38598;&#20026;&#20197;&#35282;&#33394;&#20026;&#20013;&#24515;&#30340;&#27880;&#37322;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#65292;&#38024;&#23545;&#35813;&#25968;&#25454;&#38598;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#37325;&#35201;&#20154;&#29289;&#26816;&#27979;&#21644;&#35270;&#35273;&#25925;&#20107;&#20013;&#35282;&#33394;&#30340;&#23450;&#20301;&#65292;&#24182;&#22522;&#20110;&#20998;&#24067;&#30456;&#20284;&#24615;&#21644;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#29289;&#23545;&#20110;&#20219;&#20309;&#25925;&#20107;&#30340;&#24773;&#33410;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#25776;&#20889;&#25925;&#20107;&#20043;&#21069;&#24314;&#31435;&#20154;&#29289;&#21487;&#20197;&#25552;&#39640;&#24773;&#33410;&#30340;&#28165;&#26224;&#24230;&#21644;&#25972;&#20307;&#21465;&#20107;&#30340;&#27969;&#30021;&#24615;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#20851;&#20110;&#35270;&#35273;&#21465;&#20107;&#30340;&#30740;&#31350;&#24448;&#24448;&#32858;&#28966;&#20110;&#22312;&#22270;&#20687;&#20013;&#26816;&#27979;&#29289;&#20307;&#24182;&#21457;&#29616;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#24403;&#20154;&#29289;&#36827;&#20837;&#29983;&#25104;&#31649;&#36947;&#26102;&#19981;&#20250;&#19982;&#20854;&#20182;&#29289;&#20307;&#21306;&#20998;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#36830;&#36143;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#20197;&#35282;&#33394;&#20026;&#20013;&#24515;&#30340;&#25925;&#20107;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; VIST-Character &#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20197;&#35282;&#33394;&#20026;&#20013;&#24515;&#30340;&#27880;&#37322;&#65292;&#21253;&#25324;&#35270;&#35273;&#21644;&#25991;&#26412;&#20849;&#25351;&#38142;&#21644;&#35282;&#33394;&#30340;&#37325;&#35201;&#24615;&#35780;&#32423;&#12290;&#22522;&#20110;&#27492;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#20219;&#21153;&#65306;&#37325;&#35201;&#20154;&#29289;&#26816;&#27979;&#21644;&#35270;&#35273;&#25925;&#20107;&#20013;&#35282;&#33394;&#30340;&#23450;&#20301;&#12290;&#38024;&#23545;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#22522;&#20110;&#20998;&#24067;&#30456;&#20284;&#24615;&#21644;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Characters are essential to the plot of any story. Establishing the characters before writing a story can improve the clarity of the plot and the overall flow of the narrative. However, previous work on visual storytelling tends to focus on detecting objects in images and discovering relationships between them. In this approach, characters are not distinguished from other objects when they are fed into the generation pipeline. The result is a coherent sequence of events rather than a character-centric story. In order to address this limitation, we introduce the VIST-Character dataset, which provides rich character-centric annotations, including visual and textual co-reference chains and importance ratings for characters. Based on this dataset, we propose two new tasks: important character detection and character grounding in visual stories. For both tasks, we develop simple, unsupervised models based on distributional similarity and pre-trained vision-and-language models. Our new datas
&lt;/p&gt;</description></item><item><title>oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17612</link><description>&lt;p&gt;
oBERTa: &#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#21644;&#21098;&#26525;&#26469;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17612
&lt;/p&gt;
&lt;p&gt;
oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;oBERTa&#35821;&#35328;&#27169;&#22411;&#30340;&#33539;&#22260;&#65292;&#23427;&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#19994;&#32773;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;3.8&#21040;24.3&#20493;&#30340;&#26356;&#24555;&#36895;&#30340;&#27169;&#22411;&#12290;oBERTa&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#21098;&#26525;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#24037;&#20316;&#65292;&#24182;&#21033;&#29992;&#20923;&#32467;&#30340;&#23884;&#20837;&#26469;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#65292;&#24182;&#25913;&#36827;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#20197;&#22312;&#24191;&#27867;&#30340;&#20256;&#36882;&#20219;&#21153;&#19978;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#29983;&#25104;oBERTa&#26102;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;RoBERTa&#19982;BERT&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26399;&#38388;&#21098;&#26525;&#26041;&#38754;&#30340;&#19981;&#21516;&#20043;&#22788;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#24494;&#35843;&#26399;&#38388;&#19981;&#22826;&#36866;&#21512;&#21387;&#32553;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;oBERTa&#22312;&#19971;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;NLP&#20219;&#21153;&#19978;&#30340;&#20351;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#36827;&#30340;&#21387;&#32553;&#25216;&#26415;&#20351;&#24471;&#32463;&#36807;&#21098;&#26525;&#30340;oBERTa&#27169;&#22411;&#33021;&#22815;&#21305;&#37197;BERTBASE&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;SQUAD V1.1&#38382;&#31572;&#25968;&#25454;&#30340;Prune OFA Large&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the range of oBERTa language models, an easy-to-use set of language models, which allows Natural Language Processing (NLP) practitioners to obtain between 3.8 and 24.3 times faster models without expertise in model compression. Specifically, oBERTa extends existing work on pruning, knowledge distillation, and quantization and leverages frozen embeddings to improve knowledge distillation, and improved model initialization to deliver higher accuracy on a a broad range of transfer tasks. In generating oBERTa, we explore how the highly optimized RoBERTa differs from the BERT with respect to pruning during pre-training and fine-tuning and find it less amenable to compression during fine-tuning. We explore the use of oBERTa on a broad seven representative NLP tasks and find that the improved compression techniques allow a pruned oBERTa model to match the performance of BERTBASE and exceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;ChatGPT&#23545;&#26088;&#22312;&#37327;&#21270;&#20154;&#31867;&#25991;&#21270;&#24046;&#24322;&#30340;&#38382;&#39064;&#30340;&#22238;&#31572;&#65292;&#35780;&#20272;&#20102;&#20854;&#25991;&#21270;&#36866;&#24212;&#33021;&#21147;&#12290;&#21457;&#29616;ChatGPT&#22312;&#20197;&#32654;&#22269;&#32972;&#26223;&#20026;&#25552;&#31034;&#26102;&#34920;&#29616;&#20986;&#19982;&#32654;&#22269;&#25991;&#21270;&#30340;&#24378;&#28872;&#23545;&#40784;&#65292;&#20294;&#20854;&#23545;&#20854;&#20182;&#25991;&#21270;&#30340;&#36866;&#24212;&#33021;&#21147;&#36739;&#24046;&#65292;&#24182;&#19988;&#33521;&#25991;&#25552;&#31034;&#20250;&#25273;&#24179;&#25991;&#21270;&#24046;&#24322;&#24182;&#20559;&#21521;&#32654;&#22269;&#25991;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.17466</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#19982;&#20154;&#31867;&#31038;&#20250;&#30340;&#36328;&#25991;&#21270;&#23545;&#40784;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study. (arXiv:2303.17466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;ChatGPT&#23545;&#26088;&#22312;&#37327;&#21270;&#20154;&#31867;&#25991;&#21270;&#24046;&#24322;&#30340;&#38382;&#39064;&#30340;&#22238;&#31572;&#65292;&#35780;&#20272;&#20102;&#20854;&#25991;&#21270;&#36866;&#24212;&#33021;&#21147;&#12290;&#21457;&#29616;ChatGPT&#22312;&#20197;&#32654;&#22269;&#32972;&#26223;&#20026;&#25552;&#31034;&#26102;&#34920;&#29616;&#20986;&#19982;&#32654;&#22269;&#25991;&#21270;&#30340;&#24378;&#28872;&#23545;&#40784;&#65292;&#20294;&#20854;&#23545;&#20854;&#20182;&#25991;&#21270;&#30340;&#36866;&#24212;&#33021;&#21147;&#36739;&#24046;&#65292;&#24182;&#19988;&#33521;&#25991;&#25552;&#31034;&#20250;&#25273;&#24179;&#25991;&#21270;&#24046;&#24322;&#24182;&#20559;&#21521;&#32654;&#22269;&#25991;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#21457;&#24067;&#30340;ChatGPT&#22240;&#20854;&#22312;&#23545;&#35805;&#20013;&#29983;&#25104;&#31867;&#20154;&#22238;&#24212;&#30340;&#21331;&#36234;&#33021;&#21147;&#32780;&#24191;&#21463;&#35748;&#21487;&#12290;&#32771;&#34385;&#21040;&#20854;&#34987;&#21508;&#22269;&#29992;&#25143;&#20351;&#29992;&#20197;&#21450;&#20854;&#35757;&#32451;&#20102;&#21253;&#21547;&#22810;&#26679;&#25991;&#21270;&#21644;&#31038;&#20250;&#35268;&#33539;&#30340;&#24222;&#22823;&#22810;&#35821;&#26009;&#24211;&#65292;&#35780;&#20272;&#20854;&#25991;&#21270;&#36866;&#24212;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;ChatGPT&#23545;&#26088;&#22312;&#37327;&#21270;&#20154;&#31867;&#25991;&#21270;&#24046;&#24322;&#30340;&#38382;&#39064;&#30340;&#22238;&#31572;&#26469;&#35843;&#26597;&#20854;&#28508;&#22312;&#30340;&#25991;&#21270;&#32972;&#26223;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20197;&#32654;&#22269;&#32972;&#26223;&#20026;&#25552;&#31034;&#26102;&#65292;ChatGPT&#34920;&#29616;&#20986;&#19982;&#32654;&#22269;&#25991;&#21270;&#30340;&#24378;&#28872;&#23545;&#40784;&#65292;&#20294;&#20854;&#23545;&#20854;&#20182;&#25991;&#21270;&#32972;&#26223;&#30340;&#36866;&#24212;&#33021;&#21147;&#36739;&#24046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#25506;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#33521;&#25991;&#25552;&#31034;&#20250;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#30340;&#24046;&#24322;&#65292;&#25273;&#24179;&#25991;&#21270;&#24046;&#24322;&#24182;&#20559;&#21521;&#32654;&#22269;&#25991;&#21270;&#12290;&#26412;&#30740;&#31350;&#23545;ChatGPT&#30340;&#25991;&#21270;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#24320;&#21457;&#26356;&#20855;&#25991;&#21270;&#36866;&#24212;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like responses in dialogue. Given its usage by users from various nations and its training on a vast multilingual corpus that incorporates diverse cultural and societal norms, it is crucial to evaluate its effectiveness in cultural adaptation. In this paper, we investigate the underlying cultural background of ChatGPT by analyzing its responses to questions designed to quantify human cultural differences. Our findings suggest that, when prompted with American context, ChatGPT exhibits a strong alignment with American culture, but it adapts less effectively to other cultural contexts. Furthermore, by using different prompts to probe the model, we show that English prompts reduce the variance in model responses, flattening out cultural differences and biasing them towards American culture. This study provides valuable insights into the cultural implications of ChatGPT and highl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#26657;&#20934;&#30340;&#32622;&#20449;&#20998;&#25968;&#65292;&#24179;&#34913;&#35299;&#26512;&#20219;&#21153;&#20013;&#30340;&#25104;&#26412;&#12289;&#26631;&#27880;&#21592;&#36127;&#25285;&#12289;&#20934;&#30830;&#24615;&#12289;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#31561;&#22810;&#20010;&#26435;&#34913;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;DidYouMean&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.16857</link><description>&lt;p&gt;
&#8220;&#20320;&#25351;&#30340;&#26159;...&#65311;&#8221;&#65306;&#35821;&#20041;&#35299;&#26512;&#20013;&#30340;&#32622;&#20449;&#24230;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Did You Mean...? Confidence-based Trade-offs in Semantic Parsing. (arXiv:2303.16857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#26657;&#20934;&#30340;&#32622;&#20449;&#20998;&#25968;&#65292;&#24179;&#34913;&#35299;&#26512;&#20219;&#21153;&#20013;&#30340;&#25104;&#26412;&#12289;&#26631;&#27880;&#21592;&#36127;&#25285;&#12289;&#20934;&#30830;&#24615;&#12289;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#31561;&#22810;&#20010;&#26435;&#34913;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;DidYouMean&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#19968;&#20010;&#26657;&#20934;&#22909;&#30340;&#27169;&#22411;&#26469;&#24179;&#34913;&#20219;&#21153;&#23548;&#21521;&#35299;&#26512;&#20013;&#30340;&#24120;&#35265;&#26435;&#34913;&#12290;&#22312;&#19968;&#20010;&#27169;&#25311;&#30340;&#26631;&#27880;&#21592;&#20132;&#20114;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26657;&#20934;&#30340;&#32622;&#20449;&#20998;&#25968;&#22914;&#20309;&#24179;&#34913;&#25104;&#26412;&#21644;&#26631;&#27880;&#21592;&#36127;&#25285;&#65292;&#29992;&#36739;&#23569;&#30340;&#20132;&#20114;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32622;&#20449;&#24230;&#20998;&#25968;&#22914;&#20309;&#24110;&#21161;&#20248;&#21270;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#38408;&#20540;&#30340;&#35299;&#26512;&#38169;&#35823;&#25968;&#37327;&#22823;&#24133;&#20943;&#23569;&#30340;&#31995;&#32479;DidYouMean&#65292;&#28982;&#32780;&#36825;&#20063;&#29306;&#29298;&#20102;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We illustrate how a calibrated model can help balance common trade-offs in task-oriented parsing. In a simulated annotator-in-the-loop experiment, we show that well-calibrated confidence scores allow us to balance cost with annotator load, improving accuracy with a small number of interactions. We then examine how confidence scores can help optimize the trade-off between usability and safety. We show that confidence-based thresholding can substantially reduce the number of incorrect low-confidence programs executed; however, this comes at a cost to usability. We propose the DidYouMean system which better balances usability and safety.
&lt;/p&gt;</description></item><item><title>&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16166</link><description>&lt;p&gt;
&#27809;&#26377;&#27491;&#30830;&#24615;&#30340;&#21487;&#37325;&#22797;&#24615;&#24182;&#19981;&#37325;&#35201;&#65306;&#22312;NLP&#39046;&#22495;&#20013;&#27979;&#35797;&#20195;&#30721;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16166
&lt;/p&gt;
&lt;p&gt;
&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20854;&#22312;&#30740;&#31350;&#23454;&#39564;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20195;&#30721;&#27491;&#30830;&#24615;&#24448;&#24448;&#20165;&#22522;&#20110;&#32467;&#26524;&#30340;&#24863;&#30693;&#36136;&#37327;&#32780;&#34987;&#20551;&#23450;&#12290;&#36825;&#24102;&#26469;&#20102;&#38169;&#35823;&#32467;&#26524;&#21644;&#28508;&#22312;&#35823;&#23548;&#24615;&#21457;&#29616;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#20851;&#27880;&#32467;&#26524;&#37325;&#29616;&#24212;&#35813;&#19982;&#24378;&#35843;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#30456;&#36741;&#30456;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#25903;&#25345;&#25105;&#20204;&#21521;NLP&#31038;&#21306;&#21457;&#20986;&#30340;&#21495;&#21484;&#65292;&#22312;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#24182;&#32416;&#27491;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;Conformer&#26550;&#26500;&#30340;&#24320;&#28304;&#23454;&#29616;&#20013;&#30340;&#19977;&#20010;Bug&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Bug&#30340;&#23384;&#22312;&#24182;&#19981;&#20250;&#22952;&#30861;&#33719;&#24471;&#33391;&#22909;&#30340;&#21644;&#21487;&#37325;&#22797;&#30340;&#32467;&#26524;&#65292;&#21453;&#32780;&#21487;&#33021;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#33021;&#25552;&#20379;&#38169;&#35823;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#39033;&#30740;&#31350;&#21628;&#21505;&#37319;&#29992;&#26088;&#22312;&#20419;&#36827;NLP&#30740;&#31350;&#20013;&#27491;&#30830;&#24615;&#30340;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.13592</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#25552;&#31034;&#65306;&#19996;&#21335;&#20122;&#35821;&#35328;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28151;&#21512;&#20195;&#30721;&#22312;&#19990;&#30028;&#35768;&#22810;&#22320;&#21306;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#23454;&#36341;&#65292;&#20294;&#25910;&#38598;&#39640;&#36136;&#37327;&#19988;&#20302;&#25104;&#26412;&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#20173;&#28982;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#36843;&#20351;&#20154;&#20204;&#38382;&#65306;&#36825;&#20123;&#31995;&#32479;&#33021;&#29992;&#20110;&#25968;&#25454;&#29983;&#25104;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#38646;-shot&#30340;&#26041;&#24335;&#19979;&#22914;&#20309;&#25552;&#31034;LLMs&#20026;&#19996;&#21335;&#20122;&#65288;SEA&#65289;&#30340;&#20116;&#31181;&#35821;&#35328;&#65288;&#21360;&#23612;&#35821;&#65292;&#39532;&#26469;&#35821;&#65292;&#20013;&#25991;&#65292;&#22612;&#21152;&#36335;&#35821;&#65292;&#36234;&#21335;&#35821;&#65289;&#21450;&#20811;&#37324;&#22885;&#23572;&#35821;S ingl ish&#21019;&#36896;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#26174;&#31034;&#20986;&#26368;&#22823;&#30340;&#28508;&#21147;&#65292;&#24403;&#26126;&#30830;&#23450;&#20041;&#8220;&#28151;&#21512;&#20195;&#30721;&#8221;&#26415;&#35821;&#26102;&#65292;&#33021;&#22815;68%&#30340;&#26102;&#38388;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;ChatGPT&#21644;InstructGPT&#65288;davinci-003&#65289;&#29983;&#25104;S ingl ish&#25991;&#26412;&#30340;&#34920;&#29616;&#20063;&#20540;&#24471;&#27880;&#24847;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#25552;&#31034;&#19979;&#30340;&#25104;&#21151;&#29575;&#24179;&#22343;&#20026;96%&#12290;&#20294;&#26159;&#65292;ChatGPT&#21644;InstructGPT&#30340;&#28151;&#21512;&#20195;&#30721;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#35821;&#20041;&#19981;&#27491;&#30830;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The proliferation of Large Language Models (LLMs) in recent times compels one to ask: can these systems be used for data generation? In this article, we explore prompting LLMs in a zero-shot manner to create code-mixed data for five languages in South East Asia (SEA) -Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language Singlish. We find that ChatGPT shows the most potential, capable of producing code-mixed text 68% of the time when the term "code-mixing" is explicitly defined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in generating Singlish texts are noteworthy, averaging a 96% success rate across a variety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT, however, is dampened by word choice errors that lead to semant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#29983;&#25104;&#25991;&#26412;&#30340;ChatGPT&#27169;&#22411;&#65292;&#35813;&#25216;&#26415;&#34987;&#35748;&#20026;&#21487;&#20197;&#25104;&#20026;&#33258;&#21160;&#20934;&#22791;&#23398;&#26415;&#35770;&#25991;&#21450;&#25163;&#31295;&#30340;&#28508;&#22312;&#27169;&#22411;&#65292;&#28982;&#32780;&#65292;&#20854;&#19982;&#31867;&#20284;&#27169;&#22411;&#28508;&#22312;&#30340;&#20262;&#29702;&#38382;&#39064;&#38656;&#35201;&#32771;&#34385;&#21644;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2303.13367</link><description>&lt;p&gt;
ChatGPT&#21644;&#26032;&#30340;&#23398;&#26415;&#29616;&#23454;&#65306;AI&#25776;&#20889;&#30340;&#30740;&#31350;&#35770;&#25991;&#21450;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#26415;&#20986;&#29256;&#20013;&#30340;&#20262;&#29702;&#36947;&#24503;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and a New Academic Reality: AI-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing. (arXiv:2303.13367v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#29983;&#25104;&#25991;&#26412;&#30340;ChatGPT&#27169;&#22411;&#65292;&#35813;&#25216;&#26415;&#34987;&#35748;&#20026;&#21487;&#20197;&#25104;&#20026;&#33258;&#21160;&#20934;&#22791;&#23398;&#26415;&#35770;&#25991;&#21450;&#25163;&#31295;&#30340;&#28508;&#22312;&#27169;&#22411;&#65292;&#28982;&#32780;&#65292;&#20854;&#19982;&#31867;&#20284;&#27169;&#22411;&#28508;&#22312;&#30340;&#20262;&#29702;&#38382;&#39064;&#38656;&#35201;&#32771;&#34385;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;OpenAI&#30340;ChatGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#28385;&#36275;&#22522;&#20110;&#25991;&#26412;&#30340;&#29992;&#25143;&#35831;&#27714;&#65288;&#21363;&#32842;&#22825;&#26426;&#22120;&#20154;&#65289;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#12290;&#35752;&#35770;&#20102;ChatGPT&#21450;&#31867;&#20284;&#27169;&#22411;&#32972;&#21518;&#30340;&#21382;&#21490;&#21644;&#21407;&#21017;&#12290;&#28982;&#21518;&#35752;&#35770;&#20102;&#36825;&#31181;&#25216;&#26415;&#23545;&#23398;&#26415;&#21644;&#23398;&#26415;&#30740;&#31350;&#20986;&#29256;&#29289;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;ChatGPT&#34987;&#35270;&#20026;&#33258;&#21160;&#20934;&#22791;&#35770;&#25991;&#21644;&#20854;&#20182;&#31867;&#22411;&#23398;&#26415;&#25163;&#31295;&#30340;&#28508;&#22312;&#27169;&#22411;&#12290;&#35752;&#35770;&#20102;&#21487;&#33021;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#32972;&#21518;&#30340;&#22522;&#30784;&#25216;&#26415;GPT-3&#65289;&#30340;&#20986;&#29616;&#21644;&#20854;&#34987;&#23398;&#26415;&#30028;&#21644;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#32780;&#20986;&#29616;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#23558;&#20854;&#32622;&#20110;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#30740;&#31350;&#21644;&#23398;&#26415;&#20986;&#29256;&#26041;&#38754;&#30340;&#26356;&#24191;&#27867;&#36827;&#23637;&#30340;&#32972;&#26223;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer, which uses natural language processing to fulfill text-based user requests (i.e., a chatbot). The history and principles behind ChatGPT and similar models are discussed. This technology is then discussed in relation to its potential impact on academia and scholarly research and publishing. ChatGPT is seen as a potential model for the automated preparation of essays and other types of scholarly manuscripts. Potential ethical issues that could arise with the emergence of large language models like GPT-3, the underlying technology behind ChatGPT, and its usage by academics and researchers, are discussed and situated within the context of broader advancements in artificial intelligence, machine learning, and natural language processing for research and scholarly publishing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;-FairPrompt&#65292;&#22312;&#20445;&#35777;&#20844;&#27491;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#35780;&#20272;&#25552;&#31034;&#39044;&#27979;&#20559;&#24046;&#65292;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13217</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20844;&#27491;&#24341;&#23548;&#23569;&#26679;&#26412;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Fairness-guided Few-shot Prompting for Large Language Models. (arXiv:2303.13217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;-FairPrompt&#65292;&#22312;&#20445;&#35777;&#20844;&#27491;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#35780;&#20272;&#25552;&#31034;&#39044;&#27979;&#20559;&#24046;&#65292;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#36890;&#36807;&#20960;&#20010;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#26500;&#24314;&#30340;&#25552;&#31034;&#36827;&#34892;&#30452;&#25509;&#24212;&#29992;&#26469;&#35299;&#20915;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#35757;&#32451;&#31034;&#20363;&#65292;&#31034;&#20363;&#39034;&#24207;&#21644;&#25552;&#31034;&#26684;&#24335;&#30340;&#21464;&#21270;&#23548;&#33268;&#19978;&#19979;&#25991;&#23398;&#20064;&#23481;&#26131;&#20986;&#29616;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#26500;&#24314;&#36866;&#24403;&#30340;&#25552;&#31034;&#23545;&#20110;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20174;&#39044;&#27979;&#20559;&#24046;&#30340;&#35282;&#24230;&#37325;&#26032;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25351;&#26631;&#26469;&#35780;&#20272;&#22266;&#23450;&#25552;&#31034;&#30456;&#23545;&#20110;&#26631;&#31614;&#25110;&#32473;&#23450;&#23646;&#24615;&#30340;&#39044;&#27979;&#20559;&#24046;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#39044;&#27979;&#20559;&#24046;&#36739;&#22823;&#30340;&#25552;&#31034;&#24635;&#26159;&#23548;&#33268;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22522;&#20110;&#36138;&#23146;&#25628;&#32034;&#26469;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21483;&#20570;"&#20844;&#27491;&#25552;&#31034;"&#65292;&#20854;&#20013;&#34701;&#20837;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#20197;&#25351;&#23548;&#25628;&#32034;&#19981;&#23637;&#29616;&#20986;&#23545;&#26576;&#20123;&#20154;&#32676;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;FairPrompt&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#26469;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#30041;&#23384;&#33021;&#21147;&#12290;&#20855;&#20307;&#26041;&#27861;&#26159;&#20351;&#29992;&#33258;&#21160;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#19968;&#31867;&#30340;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#25928;&#26524;&#12290;&#22312;&#35797;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#25552;&#39640;70%&#12290;</title><link>http://arxiv.org/abs/2303.06135</link><description>&lt;p&gt;
&#22522;&#20110;&#30334;&#19975;&#29992;&#25143;&#30340;&#29616;&#23454;&#19990;&#30028;&#20114;&#21160;&#26469;&#22870;&#21169;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Rewarding Chatbots for Real-World Engagement with Millions of Users. (arXiv:2303.06135v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#26469;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#30041;&#23384;&#33021;&#21147;&#12290;&#20855;&#20307;&#26041;&#27861;&#26159;&#20351;&#29992;&#33258;&#21160;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#19968;&#31867;&#30340;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#25928;&#26524;&#12290;&#22312;&#35797;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#25552;&#39640;70%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23548;&#33268;&#37096;&#32626;&#20102;&#19968;&#31995;&#21015;&#30340;&#31038;&#20132;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#34429;&#28982;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#20854;&#35821;&#35328;&#33021;&#21147;&#21644;&#27969;&#30021;&#24615;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#33021;&#20445;&#35777;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#24456;&#23481;&#26131;&#22833;&#21435;&#29992;&#25143;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#21457;&#20248;&#20808;&#32771;&#34385;&#29992;&#25143;&#21442;&#19982;&#24230;&#20197;&#22686;&#24378;&#30041;&#23384;&#30340;&#31038;&#20132;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20855;&#20307;&#25506;&#35752;&#20102;&#20351;&#29992;&#20154;&#24037;&#21453;&#39304;&#20197;&#39640;&#25928;&#22320;&#24320;&#21457;&#39640;&#24230;&#26377;&#21560;&#24341;&#21147;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20174;&#29992;&#25143;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#33258;&#21160;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#25512;&#29702;&#26102;&#25298;&#32477;&#20302;&#24471;&#20998;&#30340;&#26679;&#26412;&#21709;&#24212;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#24341;&#20837;&#20102;&#30452;&#35266;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20363;&#22914;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230;&#65288;MCL&#65289;&#65292;&#20316;&#20026;&#34913;&#37327;&#24050;&#37096;&#32626;&#32842;&#22825;&#26426;&#22120;&#20154;&#21442;&#19982;&#24230;&#27700;&#24179;&#30340;&#20195;&#29702;&#12290;&#22312;Chai Research&#24179;&#21488;&#19978;&#23545;&#27599;&#26085;&#30340;10,000&#20010;&#26032;&#32842;&#22825;&#26426;&#22120;&#20154;&#29992;&#25143;&#36827;&#34892;A/B&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20351;MCL&#22686;&#21152;70&#65285;&#65292;&#36825;&#30456;&#24403;&#20110;&#23558;&#30041;&#23384;&#26102;&#38388;&#24310;&#38271;1.5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of pretrained large language models has led to the deployment of a range of social chatbots for chitchat. Although these chatbots demonstrate language ability and fluency, they are not guaranteed to be engaging and can struggle to retain users. This work investigates the development of social chatbots that prioritize user engagement to enhance retention, specifically examining the use of human feedback to efficiently develop highly engaging chatbots. The proposed approach uses automatic pseudo-labels collected from user interactions to train a reward model that can be used to reject low-scoring sample responses generated by the chatbot model at inference time. Intuitive evaluation metrics, such as mean conversation length (MCL), are introduced as proxies to measure the level of engagement of deployed chatbots. A/B testing on groups of 10,000 new daily chatbot users on the Chai Research platform shows that this approach increases the MCL by up to 70%, which translates to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CT-TN&#30340;&#27169;&#22411;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#36827;&#34892;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#65292;&#21033;&#29992;&#20102;&#20219;&#21153;&#30340;&#31038;&#20132;&#24615;&#36136;&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#27169;&#24577;&#23884;&#20837;&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;&#24773;&#22659;&#19979;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#28304;-&#30446;&#26631;&#30446;&#26631;&#23545;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2301.04535</link><description>&lt;p&gt;
&#34701;&#21512;&#22810;&#27169;&#24577;&#23884;&#20837;&#30340;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-shot Learning for Cross-Target Stance Detection by Aggregating Multimodal Embeddings. (arXiv:2301.04535v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CT-TN&#30340;&#27169;&#22411;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#36827;&#34892;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#65292;&#21033;&#29992;&#20102;&#20219;&#21153;&#30340;&#31038;&#20132;&#24615;&#36136;&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#27169;&#24577;&#23884;&#20837;&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;&#24773;&#22659;&#19979;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#28304;-&#30446;&#26631;&#30446;&#26631;&#23545;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38480;&#20110;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#25991;&#26412;&#20869;&#23481;&#36827;&#34892;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#20219;&#21153;&#30340;&#31038;&#20132;&#24615;&#36136;&#12290;&#22312;&#36328;&#30446;&#26631;&#20998;&#31867;&#22330;&#26223;&#20013;&#65292;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#21464;&#24471;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#22312;&#23569;&#26679;&#26412;&#35757;&#32451;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#20063;&#38656;&#35201;&#39044;&#27979;&#23545;&#20110;&#20854;&#22312;&#35757;&#32451;&#26399;&#38388;&#20165;&#30475;&#21040;&#23569;&#37327;&#30456;&#20851;&#31034;&#20363;&#30340;&#26032;&#30446;&#26631;&#30340;&#31435;&#22330;&#12290;&#20026;&#20102;&#21033;&#29992;&#20219;&#21153;&#30340;&#31038;&#20132;&#24615;&#36136;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CT-TN&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#32858;&#21512;&#20102;&#25968;&#25454;&#30340;&#25991;&#26412;&#21644;&#32593;&#32476;&#29305;&#24449;&#27966;&#29983;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#20845;&#31181;&#19981;&#21516;&#30340;&#28304;-&#30446;&#26631;&#30446;&#26631;&#23545;&#30340;&#23569;&#26679;&#26412;&#36328;&#30446;&#26631;&#22330;&#26223;&#19979;&#36827;&#34892;&#23454;&#39564;&#12290;&#36890;&#36807;&#23558;CT-TN&#19982;&#26368;&#20808;&#36827;&#30340;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;CT-TN&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the increasing popularity of the stance detection task, existing approaches are predominantly limited to using the textual content of social media posts for the classification, overlooking the social nature of the task. The stance detection task becomes particularly challenging in cross-target classification scenarios, where even in few-shot training settings the model needs to predict the stance towards new targets for which the model has only seen few relevant samples during training. To address the cross-target stance detection in social media by leveraging the social nature of the task, we introduce CT-TN, a novel model that aggregates multimodal embeddings derived from both textual and network features of the data. We conduct experiments in a few-shot cross-target scenario on six different combinations of source-destination target pairs. By comparing CT-TN with state-of-the-art cross-target stance detection models, we demonstrate the effectiveness of our model by achieving
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#21153;&#21521;&#37327;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#26032;&#33539;&#24335;&#65292;&#20219;&#21153;&#21521;&#37327;&#21487;&#36890;&#36807;&#31639;&#26415;&#25805;&#20316;&#36827;&#34892;&#20462;&#25913;&#21644;&#32452;&#21512;&#65292;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#24615;&#33021;&#19988;&#23545;&#25511;&#21046;&#20219;&#21153;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2212.04089</link><description>&lt;p&gt;
&#20351;&#29992;&#20219;&#21153;&#31639;&#26415;&#32534;&#36753;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Editing Models with Task Arithmetic. (arXiv:2212.04089v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#21153;&#21521;&#37327;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#26032;&#33539;&#24335;&#65292;&#20219;&#21153;&#21521;&#37327;&#21487;&#36890;&#36807;&#31639;&#26415;&#25805;&#20316;&#36827;&#34892;&#20462;&#25913;&#21644;&#32452;&#21512;&#65292;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#24615;&#33021;&#19988;&#23545;&#25511;&#21046;&#20219;&#21153;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#21464;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#26041;&#24335;&#65288;&#27604;&#22914;&#25552;&#39640;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#25110;&#20943;&#36731;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#20559;&#24046;&#65289;&#26159;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26102;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22260;&#32469;&#8220;&#20219;&#21153;&#21521;&#37327;&#8221;&#26469;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#26032;&#33539;&#24335;&#12290;&#20219;&#21153;&#21521;&#37327;&#25351;&#23450;&#20102;&#19968;&#20010;&#26041;&#21521;&#65292;&#21363;&#39044;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65292;&#27839;&#30528;&#35813;&#26041;&#21521;&#31227;&#21160;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#32463;&#36807;&#24494;&#35843;&#20219;&#21153;&#21518;&#30340;&#30456;&#21516;&#27169;&#22411;&#30340;&#26435;&#37325;&#20013;&#20943;&#21435;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#26469;&#26500;&#24314;&#20219;&#21153;&#21521;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#21521;&#37327;&#21487;&#20197;&#36890;&#36807;&#21542;&#23450;&#21644;&#21152;&#27861;&#31561;&#31639;&#26415;&#25805;&#20316;&#36827;&#34892;&#20462;&#25913;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#21542;&#23450;&#20219;&#21153;&#21521;&#37327;&#20250;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#23545;&#25511;&#21046;&#20219;&#21153;&#30340;&#27169;&#22411;&#34892;&#20026;&#24433;&#21709;&#19981;&#22823;&#12290;&#27492;&#22806;&#65292;&#23558;&#20219;&#21153;&#21521;&#37327;&#30456;&#21152;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#25511;&#21046;&#20219;&#21153;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performanc
&lt;/p&gt;</description></item><item><title>CoP&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25511;&#21046;&#20559;&#22909;&#26469;&#26816;&#27979;&#25991;&#26412;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#12290; &#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.01611</link><description>&lt;p&gt;
CoP: &#36890;&#36807;&#25511;&#21046;&#20559;&#22909;&#26816;&#27979;&#25991;&#26412;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
CoP: Factual Inconsistency Detection by Controlling the Preference. (arXiv:2212.01611v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01611
&lt;/p&gt;
&lt;p&gt;
CoP&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25511;&#21046;&#20559;&#22909;&#26469;&#26816;&#27979;&#25991;&#26412;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#12290; &#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#21270;&#25688;&#35201;&#26159;&#26681;&#25454;&#36755;&#20837;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#25991;&#26723;&#19982;&#29983;&#25104;&#30340;&#25688;&#35201;&#20043;&#38388;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#20173;&#28982;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#20998;&#31163;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#20559;&#22909;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;CoP&#65292;&#36890;&#36807;&#25511;&#21046;&#25552;&#31034;&#26469;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#30340;&#20559;&#22909;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#25191;&#34892;&#19968;&#20010;&#39069;&#22806;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#22312;&#20854;&#20013;&#24341;&#20837;&#19968;&#20010;&#25991;&#26412;&#25552;&#31034;&#20316;&#20026;&#39069;&#22806;&#30340;&#36755;&#20837;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21478;&#19968;&#20010;&#20559;&#22909;&#30001;&#36825;&#20010;&#39069;&#22806;&#25512;&#29702;&#36807;&#31243;&#30340;&#29983;&#25104;&#27010;&#29575;&#25551;&#36848;&#12290;&#21033;&#29992;&#19978;&#36848;&#20004;&#20010;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21363;&#24341;&#20837;&#25991;&#26412;&#25552;&#31034;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#29983;&#25104;&#27169;&#22411;&#20998;&#37197;&#30340;&#27010;&#29575;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26469;&#26816;&#27979;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;CoP&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstractive summarization is the process of generating a summary given a document as input. Although significant progress has been made, the factual inconsistency between the document and the generated summary still limits its practical applications. Previous work found that the probabilities assigned by the generation model reflect its preferences for the generated summary, including the preference for factual consistency, and the preference for the language or knowledge prior as well. To separate the preference for factual consistency, we propose an unsupervised framework named CoP by controlling the preference of the generation model with the help of prompt. More specifically, the framework performs an extra inference step in which a text prompt is introduced as an additional input. In this way, another preference is described by the generation probability of this extra inference process. The difference between the above two preferences, i.e. the difference between the probabilities
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#36328;&#35821;&#35328;&#30340;&#39046;&#22495;&#24863;&#30693;&#35821;&#20041;&#19987;&#19994;&#21270;&#31995;&#32479;&#65292;&#22312;&#19981;&#37319;&#38598;&#26032;&#27495;&#35270;&#25968;&#25454;&#25110;&#25645;&#24314;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#23545;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#20013;&#25991;&#24615;&#21035;&#27495;&#35270;&#30340;&#33258;&#21160;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2211.08447</link><description>&lt;p&gt;
SexWEs: &#36890;&#36807;&#36328;&#35821;&#35328;&#35821;&#20041;&#19987;&#19994;&#21270;&#23454;&#29616;&#39046;&#22495;&#24863;&#30693;&#35789;&#21521;&#37327;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#20013;&#25991;&#24615;&#21035;&#27495;&#35270;
&lt;/p&gt;
&lt;p&gt;
SexWEs: Domain-Aware Word Embeddings via Cross-lingual Semantic Specialisation for Chinese Sexism Detection in Social Media. (arXiv:2211.08447v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#36328;&#35821;&#35328;&#30340;&#39046;&#22495;&#24863;&#30693;&#35821;&#20041;&#19987;&#19994;&#21270;&#31995;&#32479;&#65292;&#22312;&#19981;&#37319;&#38598;&#26032;&#27495;&#35270;&#25968;&#25454;&#25110;&#25645;&#24314;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#23545;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#20013;&#25991;&#24615;&#21035;&#27495;&#35270;&#30340;&#33258;&#21160;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#30340;&#30446;&#26631;&#26159;&#20943;&#23569;&#38024;&#23545;&#26576;&#20123;&#24615;&#21035;&#32676;&#20307;&#30340;&#36127;&#38754;&#22312;&#32447;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#24102;&#26377;&#24615;&#21035;&#27495;&#35270;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#21487;&#29992;&#24615;&#20351;&#24471;&#22312;&#32570;&#20047;&#36164;&#28304;&#30340;&#35821;&#35328;&#20013;&#35782;&#21035;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#33258;&#21160;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#20013;&#20013;&#25991;&#24615;&#21035;&#27495;&#35270;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#30340;&#39046;&#22495;&#24863;&#30693;&#35821;&#20041;&#19987;&#19994;&#21270;&#31995;&#32479;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#12290;&#35821;&#20041;&#19987;&#19994;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#22806;&#37096;&#35821;&#35328;&#30693;&#35782;&#65288;&#22914;&#35789;&#27719;&#35821;&#20041;&#20851;&#31995;&#65289;&#38598;&#25104;&#21040;&#19987;&#19994;&#21270;&#29305;&#24449;&#31354;&#38388;&#20013;&#26469;&#25913;&#36827;&#39044;&#20808;&#35757;&#32451;&#30340;&#20998;&#24067;&#24335;&#35789;&#21521;&#37327;&#30340;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;&#33521;&#35821;&#65289;&#20013;&#30340;&#24615;&#21035;&#27495;&#35270;&#35821;&#20041;&#36164;&#28304;&#26469;&#19987;&#19994;&#21270;&#30446;&#26631;&#35821;&#35328;&#65288;&#20013;&#25991;&#65289;&#20013;&#30340;&#39044;&#20808;&#35757;&#32451;&#30340;&#35789;&#21521;&#37327;&#20197;&#27880;&#20837;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of sexism detection is to mitigate negative online content targeting certain gender groups of people. However, the limited availability of labeled sexism-related datasets makes it problematic to identify online sexism for low-resource languages. In this paper, we address the task of automatic sexism detection in social media for one low-resource language -- Chinese. Rather than collecting new sexism data or building cross-lingual transfer learning models, we develop a cross-lingual domain-aware semantic specialisation system in order to make the most of existing data. Semantic specialisation is a technique for retrofitting pre-trained distributional word vectors by integrating external linguistic knowledge (such as lexico-semantic relations) into the specialised feature space. To do this, we leverage semantic resources for sexism from a high-resource language (English) to specialise pre-trained word vectors in the target language (Chinese) to inject domain knowledge. We demons
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24120;&#35265;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#19978;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#19982;&#26657;&#20934;&#35823;&#24046;&#30456;&#20851;&#30340;&#22240;&#32032;&#12290;&#20026;&#20102;&#26041;&#20415;&#23558;&#26657;&#20934;&#32435;&#20837;&#35821;&#20041;&#35299;&#26512;&#35780;&#20272;&#20013;&#65292;&#20316;&#32773;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#35745;&#31639;&#26657;&#20934;&#24230;&#37327;&#30340;&#24211;&#12290;</title><link>http://arxiv.org/abs/2211.07443</link><description>&lt;p&gt;
&#26657;&#20934;&#35299;&#37322;&#65306;&#35821;&#20041;&#35299;&#26512;&#20013;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Calibrated Interpretation: Confidence Estimation in Semantic Parsing. (arXiv:2211.07443v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07443
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24120;&#35265;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#19978;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#19982;&#26657;&#20934;&#35823;&#24046;&#30456;&#20851;&#30340;&#22240;&#32032;&#12290;&#20026;&#20102;&#26041;&#20415;&#23558;&#26657;&#20934;&#32435;&#20837;&#35821;&#20041;&#35299;&#26512;&#35780;&#20272;&#20013;&#65292;&#20316;&#32773;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#35745;&#31639;&#26657;&#20934;&#24230;&#37327;&#30340;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#36234;&#26469;&#36234;&#34987;&#29992;&#26469;&#23558;&#35821;&#35328;&#32763;&#35793;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#65292;&#21363;&#25191;&#34892;&#35821;&#20041;&#35299;&#26512;&#12290;&#35821;&#20041;&#35299;&#26512;&#26088;&#22312;&#25191;&#34892;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#21160;&#20316;&#65292;&#22240;&#27492;&#24320;&#21457;&#23433;&#20840;&#31995;&#32479;&#26159;&#26377;&#24517;&#35201;&#30340;&#65292;&#32780;&#27979;&#37327;&#26657;&#20934;&#21017;&#26159;&#23433;&#20840;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#27492;&#23588;&#20854;&#37325;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#24120;&#35265;&#29983;&#25104;&#27169;&#22411;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#19978;&#30340;&#26657;&#20934;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20043;&#38388;&#21464;&#21270;&#24040;&#22823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#19982;&#26657;&#20934;&#35823;&#24046;&#30456;&#20851;&#30340;&#22240;&#32032;&#65292;&#24182;&#21457;&#24067;&#20102;&#20004;&#20010;&#35299;&#26512;&#25968;&#25454;&#38598;&#30340;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#25361;&#25112;&#25286;&#20998;&#12290;&#20026;&#20102;&#26041;&#20415;&#23558;&#26657;&#20934;&#32435;&#20837;&#35821;&#20041;&#35299;&#26512;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#26657;&#20934;&#24230;&#37327;&#30340;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence generation models are increasingly being used to translate language into executable programs, i.e. to perform executable semantic parsing. The fact that semantic parsing aims to execute actions in the real world motivates developing safe systems, which in turn makes measuring calibration -- a central component to safety -- particularly important. We investigate the calibration of common generation models across four popular semantic parsing datasets, finding that it varies across models and datasets. We then analyze factors associated with calibration error and release new confidence-based challenge splits of two parsing datasets. To facilitate the inclusion of calibration in semantic parsing evaluations, we release a library for computing calibration metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#21152;&#20837;&#19981;&#27969;&#30021;&#29305;&#24449;&#25552;&#39640;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#65292;&#26368;&#39640;&#20934;&#30830;&#29575;&#36798;&#21040;95.1%&#12290;</title><link>http://arxiv.org/abs/2210.16539</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#23398;&#20064;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploiting prompt learning with pre-trained language models for Alzheimer's Disease detection. (arXiv:2210.16539v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#21152;&#20837;&#19981;&#27969;&#30021;&#29305;&#24449;&#25552;&#39640;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#65292;&#26368;&#39640;&#20934;&#30830;&#29575;&#36798;&#21040;95.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#26089;&#26399;&#35786;&#26029;&#23545;&#20110;&#20419;&#36827;&#39044;&#38450;&#24615;&#25252;&#29702;&#21644;&#24310;&#32531;&#30142;&#30149;&#36827;&#31243;&#38750;&#24120;&#20851;&#38190;&#65292;&#22522;&#20110;&#35821;&#38899;&#30340;&#33258;&#21160;AD&#31579;&#26597;&#31995;&#32479;&#20026;&#20854;&#20182;&#20020;&#24202;&#31579;&#26597;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#19988;&#26356;&#20855;&#25193;&#23637;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22914;BERT&#20135;&#29983;&#30340;&#25991;&#26412;&#23884;&#20837;&#29305;&#24449;&#34987;&#24191;&#27867;&#24212;&#29992;&#22312;&#36825;&#26679;&#30340;&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;PLM&#39046;&#22495;&#24494;&#35843;&#36890;&#24120;&#22522;&#20110;&#25513;&#34109;&#35789;&#25110;&#21477;&#23376;&#39044;&#27979;&#25104;&#26412;&#65292;&#36825;&#19982;&#21518;&#31471;AD&#26816;&#27979;&#20219;&#21153;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;PLM&#24494;&#35843;&#65292;&#36825;&#31181;&#24494;&#35843;&#19968;&#33268;&#22320;&#20351;&#29992;AD&#20998;&#31867;&#38169;&#35823;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;PLM&#24494;&#35843;&#26399;&#38388;&#65292;&#22312;&#25552;&#31034;&#30701;&#35821;&#20013;&#36827;&#19968;&#27493;&#21152;&#20837;&#20102;&#22522;&#20110;&#29369;&#35947;&#25110;&#26242;&#20572;&#22635;&#20805;&#31526;&#20196;&#29260;&#39057;&#29575;&#30340;&#19981;&#27969;&#30021;&#29305;&#24449;&#12290;&#23545;&#20110;&#20351;&#29992;&#19981;&#21516;PLMs&#65288;BERT&#21644;RoBERTa&#65289;&#25110;&#20351;&#29992;&#19981;&#21516;&#24494;&#35843;&#33539;&#20363;&#65288;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#25552;&#31034;&#23398;&#20064;&#65289;&#30340;&#31995;&#32479;&#65292;&#22522;&#20110;&#20915;&#31574;&#25237;&#31080;&#30340;&#32452;&#21512;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;AD&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#22312;&#22522;&#20934;AD&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36798;&#21040;&#20102;&#39640;&#36798;95.1%&#30340;&#20934;&#30830;&#29575;&#65292;&#34920;&#29616;&#22788;&#20110;&#21516;&#31867;&#30740;&#31350;&#30340;&#26368;&#21069;&#27839;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating preventive care and to delay further progression. Speech based automatic AD screening systems provide a non-intrusive and more scalable alternative to other clinical screening techniques. Textual embedding features produced by pre-trained language models (PLMs) such as BERT are widely used in such systems. However, PLM domain fine-tuning is commonly based on the masked word or sentence prediction costs that are inconsistent with the back-end AD detection task. To this end, this paper investigates the use of prompt-based fine-tuning of PLMs that consistently uses AD classification errors as the training objective function. Disfluency features based on hesitation or pause filler token frequencies are further incorporated into prompt phrases during PLM fine-tuning. The decision voting based combination among systems using different PLMs (BERT and RoBERTa) or systems with different fine-tuning paradigms (conventional ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#24773;&#24863;&#35782;&#21035;&#25968;&#25454;&#38598;M-MELD&#65292;&#25193;&#23637;&#20102;MELD&#25968;&#25454;&#38598;&#21040;&#33521;&#35821;&#20043;&#22806;&#30340;4&#31181;&#35821;&#35328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;DiscLSTM&#65292;&#35813;&#26550;&#26500;&#22312;&#23545;&#35805;&#20013;&#20351;&#29992;&#39034;&#24207;&#21644;&#20132;&#38469;&#35821;&#22659;&#36827;&#34892;ERC&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#36328;&#35821;&#35328;&#20256;&#36755;&#31561;&#29305;&#28857;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2203.16799</link><description>&lt;p&gt;
M-MELD&#65306;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
M-MELD: A Multilingual Multi-Party Dataset for Emotion Recognition in Conversations. (arXiv:2203.16799v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#24773;&#24863;&#35782;&#21035;&#25968;&#25454;&#38598;M-MELD&#65292;&#25193;&#23637;&#20102;MELD&#25968;&#25454;&#38598;&#21040;&#33521;&#35821;&#20043;&#22806;&#30340;4&#31181;&#35821;&#35328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;DiscLSTM&#65292;&#35813;&#26550;&#26500;&#22312;&#23545;&#35805;&#20013;&#20351;&#29992;&#39034;&#24207;&#21644;&#20132;&#38469;&#35821;&#22659;&#36827;&#34892;ERC&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#36328;&#35821;&#35328;&#20256;&#36755;&#31561;&#29305;&#28857;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#34920;&#36798;&#26159;&#26085;&#24120;&#20154;&#31867;&#20132;&#27969;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#20027;&#35201;&#20219;&#21153;&#26159;&#35782;&#21035;&#23545;&#35805;&#20013;&#27599;&#20010;&#35805;&#35821;&#32972;&#21518;&#30340;&#24773;&#24863;&#12290;&#23613;&#31649;&#36807;&#21435;&#24050;&#32463;&#23545;ERC&#36827;&#34892;&#20102;&#24456;&#22810;&#24037;&#20316;&#65292;&#20294;&#36825;&#20123;&#24037;&#20316;&#20165;&#20851;&#27880;&#33521;&#35821;&#35821;&#35328;&#30340;ERC&#65292;&#24573;&#30053;&#20102;&#20854;&#20182;&#20219;&#20309;&#35821;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multilingual MELD&#65288;M-MELD&#65289;&#65292;&#23558;Multimodal EmotionLines&#25968;&#25454;&#38598;&#65288;MELD&#65289;&#25193;&#23637;&#21040;&#33521;&#35821;&#20043;&#22806;&#30340;4&#31181;&#20854;&#20182;&#35821;&#35328;&#65292;&#20998;&#21035;&#20026;&#24076;&#33098;&#35821;&#12289;&#27874;&#20848;&#35821;&#12289;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#38500;&#20102;&#20026;&#25152;&#26377;&#36825;4&#31181;&#35821;&#35328;&#24314;&#31435;&#24378;&#22823;&#30340;&#22522;&#32447;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;DiscLSTM&#65292;&#23427;&#22312;&#23545;&#35805;&#20013;&#20351;&#29992;&#39034;&#24207;&#21644;&#20132;&#38469;&#35821;&#22659;&#36827;&#34892;ERC&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21487;&#20197;&#20351;&#29992;&#36328;&#35821;&#35328;&#32534;&#30721;&#22120;&#36328;&#35821;&#35328;&#20256;&#36755;&#65292;&#24182;&#19988;&#27604;&#22823;&#22810;&#25968;&#21333;&#27169;&#24577;&#25991;&#26412;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expression of emotions is a crucial part of daily human communication. Emotion recognition in conversations (ERC) is an emerging field of study, where the primary task is to identify the emotion behind each utterance in a conversation. Though a lot of work has been done on ERC in the past, these works only focus on ERC in the English language, thereby ignoring any other languages. In this paper, we present Multilingual MELD (M-MELD), where we extend the Multimodal EmotionLines Dataset (MELD) \cite{poria2018meld} to 4 other languages beyond English, namely Greek, Polish, French, and Spanish. Beyond just establishing strong baselines for all of these 4 languages, we also propose a novel architecture, DiscLSTM, that uses both sequential and conversational discourse context in a conversational dialogue for ERC. Our proposed approach is computationally efficient, can transfer across languages using just a cross-lingual encoder, and achieves better performance than most uni-modal text approa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33258;&#21160;&#21270;&#33021;&#21147;&#24212;&#29992;&#21040;&#29289;&#27969;&#31995;&#32479;&#27169;&#25311;&#24314;&#27169;&#20013;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;GPT-3 Codex&#30340;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#21151;&#33021;&#26377;&#25928;&#30340;&#25490;&#38431;&#21644;&#24211;&#23384;&#25511;&#21046;&#31995;&#32479;&#30340;&#27169;&#25311;&#27169;&#22411;&#65292;&#20026;&#31616;&#21270;&#27169;&#25311;&#27169;&#22411;&#24320;&#21457;&#24037;&#20316;&#27969;&#31243;&#24320;&#21551;&#20102;&#37325;&#35201;&#22823;&#38376;&#12290;</title><link>http://arxiv.org/abs/2202.12107</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#21040;&#27169;&#25311;&#65306;&#24212;&#29992;GPT-3 Codex&#33258;&#21160;&#21270;&#29289;&#27969;&#31995;&#32479;&#27169;&#25311;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems. (arXiv:2202.12107v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12107
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33258;&#21160;&#21270;&#33021;&#21147;&#24212;&#29992;&#21040;&#29289;&#27969;&#31995;&#32479;&#27169;&#25311;&#24314;&#27169;&#20013;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;GPT-3 Codex&#30340;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#21151;&#33021;&#26377;&#25928;&#30340;&#25490;&#38431;&#21644;&#24211;&#23384;&#25511;&#21046;&#31995;&#32479;&#30340;&#27169;&#25311;&#27169;&#22411;&#65292;&#20026;&#31616;&#21270;&#27169;&#25311;&#27169;&#22411;&#24320;&#21457;&#24037;&#20316;&#27969;&#31243;&#24320;&#21551;&#20102;&#37325;&#35201;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33258;&#21160;&#21270;&#24320;&#21457;&#29289;&#27969;&#31995;&#32479;&#27169;&#25311;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22522;&#20110;&#32463;&#36807;&#24494;&#35843;&#30340;GPT-3 Codex&#30340;&#26694;&#26550;&#19978;&#33021;&#22815;&#26681;&#25454;&#21475;&#22836;&#25551;&#36848;&#29983;&#25104;&#21151;&#33021;&#26377;&#25928;&#30340;&#25490;&#38431;&#21644;&#24211;&#23384;&#25511;&#21046;&#31995;&#32479;&#27169;&#25311;&#27169;&#22411;&#12290;&#22312;&#25152;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#65292;GPT-3 Codex&#23637;&#29616;&#20986;&#23545;Python&#32534;&#31243;&#30340;&#28145;&#21402;&#25216;&#33021;&#20197;&#21450;&#23545;&#34892;&#19994;&#29305;&#23450;&#35789;&#27719;&#30340;&#29702;&#35299;&#12290;&#32467;&#26524;&#65292;&#35813;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#32473;&#23450;&#34892;&#19994;&#29305;&#23450;&#22330;&#26223;&#19979;&#65292;&#26681;&#25454;&#27969;&#31243;&#35828;&#26126;&#21644;&#21464;&#37327;&#20540;&#21015;&#34920;&#29983;&#25104;&#21333;&#21697;&#24211;&#23384;&#25511;&#21046;&#31995;&#32479;&#21644;&#21333;&#26381;&#21153;&#22120;&#25490;&#38431;&#31995;&#32479;&#30340;&#27169;&#25311;&#27169;&#22411;&#12290;&#36825;&#20123;&#32467;&#26524;&#30340;&#21576;&#29616;&#65292;&#20197;&#21450;&#35821;&#35328;&#27169;&#22411;&#25345;&#32493;&#30340;&#36805;&#36895;&#36827;&#27493;&#65292;&#25171;&#24320;&#20102;&#31616;&#21270;&#27169;&#25311;&#27169;&#22411;&#24320;&#21457;&#24037;&#20316;&#27969;&#31243;&#30340;&#37325;&#35201;&#22823;&#38376;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#21152;&#24555;&#33258;&#21160;&#21270;&#29289;&#27969;&#31995;&#32479;&#30340;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work is the first attempt to apply Natural Language Processing to automate the development of simulation models of systems vitally important for logistics. We demonstrated that the framework built on top of the fine-tuned GPT-3 Codex, a Transformer-based language model, could produce functionally valid simulations of queuing and inventory control systems given the verbal description. In conducted experiments, GPT-3 Codex demonstrated convincing expertise in Python as well as an understanding of the domain-specific vocabulary. As a result, the language model could produce simulations of a single-product inventory-control system and single-server queuing system given the domain-specific context, a detailed description of the process, and a list of variables with the corresponding values. The demonstrated results, along with the rapid improvement of language models, open the door for significant simplification of the workflow behind the simulation model development, which will allow e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65288;ASPR&#65289;&#30340;&#27010;&#24565;&#21644;&#27969;&#31243;&#65292;&#32508;&#36848;&#20102;&#23454;&#29616;&#20840;&#38754;&#35745;&#31639;&#26426;&#21270;&#23457;&#31295;&#27969;&#31243;&#30340;&#30456;&#20851;&#25991;&#29486;&#21644;&#25216;&#26415;&#65292;&#21516;&#26102;&#25351;&#20986;&#23454;&#29616;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22914;&#25991;&#26723;&#35299;&#26512;&#21644;&#34920;&#36798;&#19981;&#23436;&#32654;&#12289;&#25968;&#25454;&#19981;&#36275;&#12289;&#20154;&#26426;&#20132;&#20114;&#32570;&#38519;&#21644;&#21457;&#29616;&#20302;&#36136;&#37327;&#25991;&#31456;&#30340;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2111.07533</link><description>&lt;p&gt;
&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65306;&#27010;&#24565;&#12289;&#25216;&#26415;&#19982;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated scholarly paper review: Concepts, technologies, and challenges. (arXiv:2111.07533v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07533
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65288;ASPR&#65289;&#30340;&#27010;&#24565;&#21644;&#27969;&#31243;&#65292;&#32508;&#36848;&#20102;&#23454;&#29616;&#20840;&#38754;&#35745;&#31639;&#26426;&#21270;&#23457;&#31295;&#27969;&#31243;&#30340;&#30456;&#20851;&#25991;&#29486;&#21644;&#25216;&#26415;&#65292;&#21516;&#26102;&#25351;&#20986;&#23454;&#29616;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22914;&#25991;&#26723;&#35299;&#26512;&#21644;&#34920;&#36798;&#19981;&#23436;&#32654;&#12289;&#25968;&#25454;&#19981;&#36275;&#12289;&#20154;&#26426;&#20132;&#20114;&#32570;&#38519;&#21644;&#21457;&#29616;&#20302;&#36136;&#37327;&#25991;&#31456;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#34892;&#35780;&#23457;&#26159;&#30740;&#31350;&#35780;&#20215;&#30340;&#24191;&#27867;&#25509;&#21463;&#26426;&#21046;&#65292;&#22312;&#23398;&#26415;&#20986;&#29256;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#25928;&#29575;&#20302;&#19979;&#21644;&#21487;&#37325;&#22797;&#24615;&#24046;&#65292;&#36825;&#19968;&#26426;&#21046;&#38271;&#26399;&#20197;&#26469;&#22791;&#21463;&#25209;&#35780;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#36741;&#21161;&#21516;&#34892;&#35780;&#23457;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#28041;&#21450;&#20154;&#21592;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#38480;&#21046;&#20173;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65288;ASPR&#65289;&#30340;&#27010;&#24565;&#21644;&#27969;&#31243;&#65292;&#24182;&#32508;&#36848;&#20102;&#23454;&#29616;&#20840;&#38754;&#35745;&#31639;&#26426;&#21270;&#23457;&#31295;&#27969;&#31243;&#30340;&#30456;&#20851;&#25991;&#29486;&#21644;&#25216;&#26415;&#12290;&#22312;&#23457;&#26597;&#21644;&#35752;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;ASPR &#30340;&#27599;&#20010;&#38454;&#27573;&#24050;&#32463;&#26377;&#30456;&#24212;&#30340;&#30740;&#31350;&#21644;&#21021;&#27493;&#23454;&#26045;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;ASPR&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#20027;&#35201;&#22256;&#38590;&#22312;&#20110;&#25991;&#26723;&#35299;&#26512;&#21644;&#34920;&#36798;&#19981;&#23436;&#32654;&#12289;&#25968;&#25454;&#19981;&#36275;&#12289;&#20154;&#26426;&#20132;&#20114;&#32570;&#38519;&#21644;&#21457;&#29616;&#20302;&#36136;&#37327;&#25991;&#31456;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer review is a widely accepted mechanism for research evaluation, playing a pivotal role in academic publishing. However, criticisms have long been leveled on this mechanism, mostly because of its poor efficiency and low reproducibility. Recent years have seen the application of artificial intelligence (AI) in assisting the peer review process. Nonetheless, with the involvement of humans, such limitations remain inevitable. In this paper, we propose the concept and pipeline of automated scholarly paper review (ASPR) and review the relevant literature and technologies of achieving a full-scale computerized review process. On the basis of the review and discussion, we conclude that there is already corresponding research and preliminary implementation at each stage of ASPR. We further look into the challenges in ASPR with the existing technologies. The major difficulties lie in imperfect document parsing and representation, inadequate data, defective human-computer interaction, and fla
&lt;/p&gt;</description></item></channel></rss>