<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#20197;&#32435;&#20837;&#26032;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16218</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Editing for Large Language Models: A Survey. (arXiv:2310.16218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16218
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#20197;&#32435;&#20837;&#26032;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36817;&#26399;&#20197;&#20854;&#20986;&#33394;&#30340;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26681;&#25454;&#20854;&#24191;&#21338;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#25913;&#21464;&#20102;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#30340;&#26684;&#23616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#26102;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#20026;&#20854;&#21442;&#25968;&#25968;&#37327;&#21069;&#25152;&#26410;&#26377;&#12290;&#24403;&#38656;&#35201;&#39057;&#32321;&#24341;&#20837;&#26032;&#30693;&#35782;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26102;&#65292;&#36825;&#20010;&#32570;&#28857;&#26356;&#21152;&#26174;&#33879;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20256;&#32479;&#26041;&#27861;&#26159;&#36890;&#36807;&#30452;&#25509;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#32534;&#30721;&#21040;&#39044;&#35757;&#32451;LLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#37325;&#26032;&#35757;&#32451;LLMs&#21487;&#33021;&#35745;&#31639;&#36164;&#28304;&#23494;&#38598;&#65292;&#24182;&#19988;&#23384;&#22312;&#23558;&#19982;&#27169;&#22411;&#26356;&#26032;&#26080;&#20851;&#30340;&#26377;&#20215;&#20540;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#36864;&#21270;&#30340;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#32534;&#36753;(KME)&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#31934;&#30830;&#20462;&#25913;LLMs&#20197;&#32435;&#20837;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, wit
&lt;/p&gt;</description></item><item><title>SteloCoder&#26159;&#19968;&#20010;&#20165;&#35299;&#30721;&#30340;&#22522;&#20110;StarCoder&#30340;LLM&#65292;&#22312;&#22810;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23427;&#37319;&#29992;Mixture-of-Experts&#65288;MoE&#65289;&#25216;&#26415;&#21644;&#38376;&#25511;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;StarCoder&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#19987;&#23478;&#65292;&#24182;&#20351;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;LoRA&#65289;&#25216;&#26415;&#26469;&#38480;&#21046;&#27599;&#20010;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.15539</link><description>&lt;p&gt;
SteloCoder:&#19968;&#31181;&#20165;&#35299;&#30721;&#30340;&#29992;&#20110;&#22810;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation. (arXiv:2310.15539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15539
&lt;/p&gt;
&lt;p&gt;
SteloCoder&#26159;&#19968;&#20010;&#20165;&#35299;&#30721;&#30340;&#22522;&#20110;StarCoder&#30340;LLM&#65292;&#22312;&#22810;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23427;&#37319;&#29992;Mixture-of-Experts&#65288;MoE&#65289;&#25216;&#26415;&#21644;&#38376;&#25511;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;StarCoder&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#19987;&#23478;&#65292;&#24182;&#20351;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;LoRA&#65289;&#25216;&#26415;&#26469;&#38480;&#21046;&#27599;&#20010;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#27880;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;StarCoder&#21644;Code Llama&#20998;&#21035;&#23637;&#31034;&#20102;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#20195;&#30721;&#32763;&#35793;&#21151;&#33021;&#19978;&#20173;&#28982;&#38656;&#35201;&#25913;&#36827;&#21644;&#26377;&#25928;&#35757;&#32451;&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SteloCoder&#65292;&#19968;&#31181;&#20165;&#35299;&#30721;&#30340;&#22522;&#20110;StarCoder&#30340;LLM&#65292;&#19987;&#20026;&#22810;&#32534;&#31243;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#32780;&#35774;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SteloCoder&#23454;&#29616;&#20102;C ++&#65292;C&#65283;&#65292;JavaScript&#65292;Java&#25110;PHP&#21040;Python&#20195;&#30721;&#32763;&#35793;&#65292;&#32780;&#26080;&#38656;&#25351;&#23450;&#36755;&#20837;&#32534;&#31243;&#35821;&#35328;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#32452;&#28151;&#21512;&#65288;Mixture-of-Experts&#65292;MoE&#65289;&#25216;&#26415;&#21644;&#19968;&#20010;&#25511;&#21046;&#22810;&#20219;&#21153;&#30340;&#38376;&#25511;&#32593;&#32476;&#26469;&#20462;&#25913;StarCoder&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;StarCoder&#36827;&#34892;&#24494;&#35843;&#26469;&#33719;&#24471;&#19987;&#23478;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;Low-Rank Adaptive Method&#65292;LoRA&#65289;&#25216;&#26415;&#65292;&#23558;&#27599;&#20010;&#19987;&#23478;&#30340;&#22823;&#23567;&#38480;&#21046;&#20026;StarCoder&#21442;&#25968;&#25968;&#37327;&#30340;&#20165;0.06&#65285;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#22686;&#24378;tr
&lt;/p&gt;
&lt;p&gt;
With the recent focus on Large Language Models (LLMs), both StarCoder (Li et al., 2023) and Code Llama (Rozi\`ere et al., 2023) have demonstrated remarkable performance in code generation. However, there is still a need for improvement in code translation functionality with efficient training techniques. In response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM designed specifically for multi-programming language-to-Python code translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or PHP-to-Python code translation without specifying the input programming language. We modified StarCoder model architecture by incorporating a Mixture-of-Experts (MoE) technique featuring five experts and a gating network for multi-task handling. Experts are obtained by StarCoder fine-tuning. Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each expert size as only 0.06% of number of StarCoder's parameters. At the same time, to enhance tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SeqXGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#21477;&#23376;&#32423;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#30333;&#30418;LLMs&#30340;&#23545;&#25968;&#27010;&#29575;&#21015;&#34920;&#20316;&#20026;&#29305;&#24449;&#65292;SeqXGPT&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;AIGT&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08903</link><description>&lt;p&gt;
SeqXGPT: &#21477;&#23376;&#32423;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SeqXGPT: Sentence-Level AI-Generated Text Detection. (arXiv:2310.08903v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SeqXGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#21477;&#23376;&#32423;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#30333;&#30418;LLMs&#30340;&#23545;&#25968;&#27010;&#29575;&#21015;&#34920;&#20316;&#20026;&#29305;&#24449;&#65292;SeqXGPT&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;AIGT&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#24212;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#65292;&#24341;&#21457;&#20102;&#23545;LLMs&#28389;&#29992;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#24378;&#22823;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#65288;AIGT&#65289;&#26816;&#27979;&#22120;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#21482;&#32771;&#34385;&#25991;&#26723;&#32423;&#21035;&#30340;AIGT&#26816;&#27979;&#65292;&#22240;&#27492;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21512;&#25104;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#30001;LLMs&#20462;&#25913;&#36807;&#30340;&#21477;&#23376;&#21644;&#30001;&#20154;&#31867;&#32534;&#20889;&#30340;&#21477;&#23376;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21477;&#23376;&#32423;&#21035;&#30340;&#26816;&#27979;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SeqXGPT&#65292;&#19968;&#31181;&#21033;&#29992;&#30333;&#30418;LLMs&#30340;&#23545;&#25968;&#27010;&#29575;&#21015;&#34920;&#20316;&#20026;&#21477;&#23376;&#32423;AIGT&#26816;&#27979;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#29305;&#24449;&#31867;&#20284;&#20110;&#35821;&#38899;&#22788;&#29702;&#20013;&#30340;&#8220;&#27874;&#28010;&#8221;&#65292;LLMs&#26080;&#27861;&#30740;&#31350;&#20854;&#32452;&#25104;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#26500;&#24314;&#20102;SeqXGPT&#12290;&#25105;&#20204;&#22312;&#21477;&#23376;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#26816;&#27979;&#25361;&#25112;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;&#26816;&#27979;&#20013;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs, that is, the documents contain sentences written by humans and sentences modified by LLMs. Then we propose \textbf{Seq}uence \textbf{X} (Check) \textbf{GPT}, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection. These features are composed like \textit{waves} in speech processing and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution and self-attention networks. We test it in both sentence and document-level detection challenges. Experimental results show that previous methods struggle in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#26222;&#36890;&#35805;&#35328;&#35821;&#30196;&#21574;&#35780;&#20272;&#31995;&#32479;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#24182;&#25552;&#21462;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#21644;&#20020;&#24202;&#30196;&#21574;&#35780;&#20998;&#39044;&#27979;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.03985</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;&#36827;&#34892;&#26222;&#36890;&#35805;&#35328;&#35821;&#30340;&#30196;&#21574;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dementia Assessment Using Mandarin Speech with an Attention-based Speech Recognition Encoder. (arXiv:2310.03985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#26222;&#36890;&#35805;&#35328;&#35821;&#30196;&#21574;&#35780;&#20272;&#31995;&#32479;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#24182;&#25552;&#21462;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#21644;&#20020;&#24202;&#30196;&#21574;&#35780;&#20998;&#39044;&#27979;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30196;&#21574;&#35786;&#26029;&#38656;&#35201;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#36825;&#26159;&#22797;&#26434;&#19988;&#32791;&#26102;&#30340;&#12290;&#30196;&#21574;&#30340;&#26089;&#26399;&#26816;&#27979;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#38450;&#27490;&#30149;&#24773;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#26412;&#25991;&#21033;&#29992;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#38024;&#23545;&#26222;&#36890;&#35805;&#20351;&#29992;&#32773;&#22312;&#22270;&#29255;&#25551;&#36848;&#20219;&#21153;&#20013;&#30340;&#30196;&#21574;&#35780;&#20272;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;&#19982;&#30495;&#23454;&#19990;&#30028;&#24773;&#22659;&#38750;&#24120;&#30456;&#20284;&#30340;&#35821;&#38899;&#25968;&#25454;&#19978;&#35757;&#32451;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20174;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#25552;&#21462;&#32534;&#30721;&#22120;&#65292;&#24182;&#28155;&#21152;&#20102;&#19968;&#20010;&#32447;&#24615;&#23618;&#29992;&#20110;&#30196;&#21574;&#35780;&#20272;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;99&#21517;&#34987;&#35797;&#30340;&#26222;&#36890;&#35805;&#35821;&#38899;&#25968;&#25454;&#65292;&#24182;&#20174;&#24403;&#22320;&#21307;&#38498;&#33719;&#21462;&#20102;&#20182;&#20204;&#30340;&#20020;&#24202;&#35780;&#20272;&#25968;&#25454;&#12290;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;92.04%&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#20020;&#24202;&#30196;&#21574;&#35780;&#20998;&#39044;&#27979;&#20013;&#36798;&#21040;&#20102;9%&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dementia diagnosis requires a series of different testing methods, which is complex and time-consuming. Early detection of dementia is crucial as it can prevent further deterioration of the condition. This paper utilizes a speech recognition model to construct a dementia assessment system tailored for Mandarin speakers during the picture description task. By training an attention-based speech recognition model on voice data closely resembling real-world scenarios, we have significantly enhanced the model's recognition capabilities. Subsequently, we extracted the encoder from the speech recognition model and added a linear layer for dementia assessment. We collected Mandarin speech data from 99 subjects and acquired their clinical assessments from a local hospital. We achieved an accuracy of 92.04% in Alzheimer's disease detection and a mean absolute error of 9% in clinical dementia rating score prediction.
&lt;/p&gt;</description></item><item><title>C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2309.07597</link><description>&lt;p&gt;
C-Pack: &#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#25171;&#21253;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07597
&lt;/p&gt;
&lt;p&gt;
C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;C-Pack&#65292;&#36825;&#26159;&#19968;&#22871;&#26174;&#33879;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#12290;C-Pack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#36164;&#28304;&#12290;1&#65289;C-MTEB&#26159;&#19968;&#20010;&#28085;&#30422;6&#20010;&#20219;&#21153;&#21644;35&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12290;2&#65289;C-MTP&#26159;&#19968;&#20010;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#27721;&#35821;&#35821;&#26009;&#24211;&#20013;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12290;3&#65289;C-TEM&#26159;&#19968;&#20010;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;C-MTEB&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#25152;&#26377;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#36798;&#21040;&#20102;&#21457;&#24067;&#26102;&#30340;&#26368;&#39640;+10%&#12290;&#25105;&#20204;&#36824;&#25972;&#21512;&#21644;&#20248;&#21270;&#20102;C-TEM&#30340;&#25972;&#22871;&#35757;&#32451;&#26041;&#27861;&#12290;&#38500;&#20102;&#25105;&#20204;&#20851;&#20110;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#36164;&#28304;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#36825;&#20123;&#33521;&#35821;&#27169;&#22411;&#22312;MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#24067;&#30340;&#33521;&#35821;&#25968;&#25454;&#27604;&#27721;&#35821;&#25968;&#25454;&#22823;2&#20493;&#12290;&#25152;&#26377;&#36825;&#20123;&#36164;&#28304;&#37117;&#21487;&#20197;&#22312;https://github.com/FlagOpen/FlagEmbedding&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CIF-Transducer&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23558;&#36830;&#32493;&#31215;&#20998;&#21644;&#28779;&#26426;&#21046;&#19982;RNN-T&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#24182;&#25918;&#24323;&#20102;RNN-T Loss&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#65292;&#24182;&#20351;&#39044;&#27979;&#32593;&#32476;&#21457;&#25381;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;CIF-T&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14132</link><description>&lt;p&gt;
&#21578;&#21035;RNN-T Loss&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;CIF&#30340;&#36716;&#24405;&#22120;&#26550;&#26500;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition. (arXiv:2307.14132v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CIF-Transducer&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23558;&#36830;&#32493;&#31215;&#20998;&#21644;&#28779;&#26426;&#21046;&#19982;RNN-T&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#24182;&#25918;&#24323;&#20102;RNN-T Loss&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#65292;&#24182;&#20351;&#39044;&#27979;&#32593;&#32476;&#21457;&#25381;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;CIF-T&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RNN-T&#27169;&#22411;&#22312;ASR&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#20381;&#38752;RNN-T Loss&#23454;&#29616;&#36755;&#20837;&#38899;&#39057;&#21644;&#30446;&#26631;&#24207;&#21015;&#30340;&#38271;&#24230;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;RNN-T Loss&#30340;&#23454;&#29616;&#22797;&#26434;&#24615;&#21644;&#22522;&#20110;&#23545;&#40784;&#30340;&#20248;&#21270;&#30446;&#26631;&#23548;&#33268;&#35745;&#31639;&#20887;&#20313;&#21644;&#39044;&#27979;&#32593;&#32476;&#35282;&#33394;&#30340;&#20943;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CIF-Transducer&#65288;CIF-T&#65289;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23427;&#23558;&#36830;&#32493;&#31215;&#20998;&#21644;&#28779;&#65288;CIF&#65289;&#26426;&#21046;&#19982;RNN-T&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25918;&#24323;&#20102;RNN-T Loss&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#65292;&#24182;&#20351;&#39044;&#27979;&#32593;&#32476;&#21457;&#25381;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Funnel-CIF&#12289;Context Blocks&#12289;Unified Gating&#21644;Bilinear Pooling&#32852;&#21512;&#32593;&#32476;&#20197;&#21450;&#36741;&#21161;&#35757;&#32451;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;178&#23567;&#26102;&#30340;AISHELL-1&#21644;10000&#23567;&#26102;&#30340;WenetSpeech&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;RNN-T&#27169;&#22411;&#30456;&#27604;&#65292;CIF-T&#20197;&#26356;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
RNN-T models are widely used in ASR, which rely on the RNN-T loss to achieve length alignment between input audio and target sequence. However, the implementation complexity and the alignment-based optimization target of RNN-T loss lead to computational redundancy and a reduced role for predictor network, respectively. In this paper, we propose a novel model named CIF-Transducer (CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanism with the RNN-T model to achieve efficient alignment. In this way, the RNN-T loss is abandoned, thus bringing a computational reduction and allowing the predictor network a more significant role. We also introduce Funnel-CIF, Context Blocks, Unified Gating and Bilinear Pooling joint network, and auxiliary training strategy to further improve performance. Experiments on the 178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achieves state-of-the-art results with lower computational overhead compared to RNN-T models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-3 Davinci-003&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#29305;&#36136;&#65292;&#20294;&#22312;&#19981;&#21516;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.04308</link><description>&lt;p&gt;
GPT-3&#30340;&#20154;&#26684;&#27979;&#35797;&#65306;&#26102;&#38388;&#21487;&#38752;&#24615;&#26377;&#38480;&#65292;&#20294;&#20984;&#26174;&#20102;&#31038;&#20132;&#28212;&#26395;&#30340;&#20154;&#26684;&#24037;&#20855;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results. (arXiv:2306.04308v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-3 Davinci-003&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#29305;&#36136;&#65292;&#20294;&#22312;&#19981;&#21516;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35780;&#20272;&#32842;&#22825;&#26426;&#22120;&#20154;GPT-3 Davinci-003&#30340;&#28508;&#22312;&#24212;&#29992;&#21644;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#21450;&#20854;&#20010;&#24615;&#21270;&#36164;&#26009;&#30340;&#20154;&#26684;&#38382;&#21367;&#30340;&#26102;&#38388;&#21487;&#38752;&#24615;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22330;&#21512;&#65292;&#24515;&#29702;&#38382;&#21367;&#34987;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#28982;&#21518;&#23558;&#22238;&#31572;&#19982;&#20154;&#31867;&#22522;&#20934;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#65292;&#26377;&#20123;&#37327;&#34920;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#26377;&#20123;&#21017;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#19968;&#33268;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;Davinci-003&#26174;&#31034;&#20986;&#19968;&#20010;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#23588;&#20854;&#26159;&#22312;&#20146;&#21644;&#21147;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#31572;&#30340;&#22522;&#30784;&#65292;&#26080;&#35770;&#26159;&#30001;&#20027;&#35266;&#33258;&#25105;&#21453;&#24605;&#36824;&#26159;&#39044;&#23450;&#31639;&#27861;&#39537;&#21160;&#65292;&#23578;&#19981;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
To assess the potential applications and limitations of chatbot GPT-3 Davinci-003, this study explored the temporal reliability of personality questionnaires applied to the chatbot and its personality profile. Psychological questionnaires were administered to the chatbot on two separate occasions, followed by a comparison of the responses to human normative data. The findings revealed varying levels of agreement in the chatbot's responses over time, with some scales displaying excellent while others demonstrated poor agreement. Overall, Davinci-003 displayed a socially desirable and pro-social personality profile, particularly in the domain of communion. However, the underlying basis of the chatbot's responses, whether driven by conscious self-reflection or predetermined algorithms, remains uncertain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26041;&#27861;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#35745;&#31639;&#21152;&#36895;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.18396</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#29702;&#35299;&#21152;&#23494;&#25552;&#31034;&#65306;&#38754;&#21521;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;Transformers
&lt;/p&gt;
&lt;p&gt;
LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers. (arXiv:2305.18396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26041;&#27861;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#35745;&#31639;&#21152;&#36895;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#26381;&#21153;&#22120;&#23458;&#25143;&#31471;&#29615;&#22659;&#20013;&#20026;&#22522;&#20110;transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26500;&#24314;&#31169;&#26377;&#25512;&#26029;&#26694;&#26550;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#25345;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#23458;&#25143;&#31471;&#36755;&#20837;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#24403;&#31169;&#26377;&#36755;&#20837;&#36890;&#36807;&#21407;&#22987;LLMs&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#26102;&#65292;&#36825;&#20123;&#26694;&#26550;&#20250;&#20135;&#29983;&#26174;&#30528;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#65292;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;&#19982;&#26368;&#26032;&#30340;Iron&#65288;NeurIPS 2022&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#27169;&#22411;&#25512;&#26029;&#31649;&#36947;&#22312;&#35745;&#31639;&#19978;&#23454;&#29616;&#20102;$5 \times$&#30340;&#21152;&#36895;&#65292;&#22312;&#36890;&#20449;&#24320;&#38144;&#19978;&#23454;&#29616;&#20102;80\%&#30340;&#38477;&#20302;&#65292;&#21516;&#26102;&#20960;&#20046;&#20445;&#25345;&#20102;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior works have attempted to build private inference frameworks for transformer-based large language models (LLMs) in a server-client setting, where the server holds the model parameters and the client inputs the private data for inference. However, these frameworks impose significant overhead when the private inputs are forward propagated through the original LLMs. In this paper, we show that substituting the computation- and communication-heavy operators in the transformer architecture with privacy-computing friendly approximations can greatly reduce the private inference costs with minor impact on model performance. Compared to the state-of-the-art Iron (NeurIPS 2022), our privacy-computing friendly model inference pipeline achieves a $5\times$ acceleration in computation and an 80\% reduction in communication overhead, while retaining nearly identical accuracy.
&lt;/p&gt;</description></item></channel></rss>