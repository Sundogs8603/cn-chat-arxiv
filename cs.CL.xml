<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>POMP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#30340;&#12289;&#22522;&#20110;&#25277;&#26679;&#30340;&#22810;&#36741;&#21161;&#35821;&#35328;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05596</link><description>&lt;p&gt;
POMP:&#29992;&#20110;&#20302;&#36164;&#28304;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#27010;&#29575;&#39537;&#21160;&#20803;&#22270;&#25552;&#31034;&#22120;
&lt;/p&gt;
&lt;p&gt;
POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation. (arXiv:2401.05596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05596
&lt;/p&gt;
&lt;p&gt;
POMP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#30340;&#12289;&#22522;&#20110;&#25277;&#26679;&#30340;&#22810;&#36741;&#21161;&#35821;&#35328;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#26377;&#38480;&#30340;&#24179;&#34892;&#25968;&#25454;&#19979;&#38754;&#20020;&#30528;&#22312;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25361;&#25112;&#65292;&#22240;&#27492;&#30740;&#31350;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#21253;&#25324;&#21453;&#21521;&#32763;&#35793;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#22522;&#20110;&#26530;&#36724;&#30340;&#32763;&#35793;&#65292;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#32763;&#35793;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#20204;&#21463;&#21040;&#21512;&#25104;&#25968;&#25454;&#22122;&#22768;&#12289;&#35821;&#35328;&#20559;&#24046;&#21644;&#38169;&#35823;&#20256;&#25773;&#31561;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32531;&#35299;&#12290;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#25913;&#36827;&#20102;NMT&#65292;&#20294;&#26159;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#20351;&#24471;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#25105;&#20204;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36741;&#21161;&#35821;&#35328;&#20943;&#23569;&#35821;&#35328;&#22122;&#22768;&#65292;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POMP&#30340;&#27010;&#29575;&#39537;&#21160;&#20803;&#22270;&#25552;&#31034;&#22120;&#65292;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#21160;&#24577;&#25277;&#26679;&#30340;&#22810;&#20010;&#36741;&#21161;&#35821;&#35328;&#30340;&#22270;&#24418;&#65292;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-resource languages (LRLs) face challenges in supervised neural machine translation due to limited parallel data, prompting research into unsupervised methods. Unsupervised neural machine translation (UNMT) methods, including back-translation, transfer learning, and pivot-based translation, offer practical solutions for LRL translation, but they are hindered by issues like synthetic data noise, language bias, and error propagation, which can potentially be mitigated by Large Language Models (LLMs). LLMs have advanced NMT with in-context learning (ICL) and supervised fine-tuning methods, but insufficient training data results in poor performance in LRLs. We argue that LLMs can mitigate the linguistic noise with auxiliary languages to improve translations in LRLs. In this paper, we propose Probability-driven Meta-graph Prompter (POMP), a novel approach employing a dynamic, sampling-based graph of multiple auxiliary languages to enhance LLMs' translation capabilities for LRLs. POMP inv
&lt;/p&gt;</description></item><item><title>TrustLLM&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#21487;&#20449;&#24615;&#21407;&#21017;&#30340;&#25552;&#20986;&#12289;&#24314;&#31435;&#22522;&#20934;&#30340;&#26041;&#27861;&#12289;&#35780;&#20272;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#26410;&#26469;&#25361;&#25112;&#30340;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.05561</link><description>&lt;p&gt;
TrustLLM: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21487;&#20449;&#24615;
&lt;/p&gt;
&lt;p&gt;
TrustLLM: Trustworthiness in Large Language Models. (arXiv:2401.05561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05561
&lt;/p&gt;
&lt;p&gt;
TrustLLM&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#21487;&#20449;&#24615;&#21407;&#21017;&#30340;&#25552;&#20986;&#12289;&#24314;&#31435;&#22522;&#20934;&#30340;&#26041;&#27861;&#12289;&#35780;&#20272;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#26410;&#26469;&#25361;&#25112;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#22312;&#21487;&#20449;&#24615;&#26041;&#38754;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;LLMs&#30340;&#21487;&#20449;&#24615;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#35805;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TrustLLM&#65292;&#23427;&#26159;&#23545;LLMs&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#19981;&#21516;&#32500;&#24230;&#30340;&#21487;&#20449;&#24615;&#21407;&#21017;&#12289;&#24314;&#31435;&#22522;&#20934;&#12289;&#35780;&#20272;&#21644;&#20998;&#26512;&#20027;&#27969;LLMs&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#24320;&#25918;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#28085;&#30422;&#20843;&#20010;&#19981;&#21516;&#32500;&#24230;&#30340;&#21487;&#20449;LLMs&#21407;&#21017;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#20102;&#19968;&#20010;&#36328;&#20845;&#20010;&#32500;&#24230;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#24615;&#21644;&#26426;&#22120;&#20262;&#29702;&#23398;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;TrustLLM&#20013;&#23637;&#31034;&#20102;&#19968;&#20010;&#35780;&#20272;16&#20010;&#20027;&#27969;LLMs&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;30&#22810;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our find
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25512;&#29702;&#27493;&#39588;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#21017;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04925</link><description>&lt;p&gt;
&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Reasoning Step Length on Large Language Models. (arXiv:2401.04925v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25512;&#29702;&#27493;&#39588;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#21017;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#23545;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;CoT&#30340;&#26377;&#25928;&#24615;&#19982;&#25552;&#31034;&#20013;&#25512;&#29702;&#27493;&#39588;&#30340;&#38271;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#20010;&#23454;&#35777;&#23454;&#39564;&#26469;&#25506;&#32034;&#36825;&#20123;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#23454;&#39564;&#65292;&#25193;&#23637;&#21644;&#21387;&#32553;CoT&#28436;&#31034;&#20013;&#30340;&#21512;&#29702;&#25512;&#29702;&#27493;&#39588;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#22240;&#32032;&#19981;&#21464;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#20197;&#19979;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#31034;&#20013;&#24310;&#38271;&#25512;&#29702;&#27493;&#39588;&#65292;&#21363;&#20351;&#27809;&#26377;&#21521;&#25552;&#31034;&#20013;&#28155;&#21152;&#26032;&#20449;&#24687;&#65292;&#20063;&#20250;&#26174;&#33879;&#25552;&#39640;LLM&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#32553;&#30701;&#25512;&#29702;&#27493;&#39588;&#65292;&#21363;&#20351;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#65292;&#20063;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#31361;&#26174;&#20102;CoT&#25552;&#31034;&#20013;&#27493;&#39588;&#25968;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Lightning Attention-2&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#35745;&#31639;&#20248;&#21183;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#36890;&#36807;&#21033;&#29992;&#24179;&#38138;&#30340;&#24605;&#24819;&#65292;&#20998;&#21035;&#22788;&#29702;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#20869;&#37096;&#22359;&#21644;&#22806;&#37096;&#22359;&#32452;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#37319;&#29992;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26426;&#21046;&#22788;&#29702;&#20869;&#37096;&#22359;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#32047;&#31215;&#27714;&#21644;&#26041;&#27861;&#22788;&#29702;&#22806;&#37096;&#22359;&#12290;</title><link>http://arxiv.org/abs/2401.04658</link><description>&lt;p&gt;
Lightning Attention-2:&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22788;&#29702;&#26080;&#38480;&#24207;&#21015;&#38271;&#24230;&#30340;"&#20813;&#36153;&#21320;&#39184;"
&lt;/p&gt;
&lt;p&gt;
Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. (arXiv:2401.04658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Lightning Attention-2&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#35745;&#31639;&#20248;&#21183;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#36890;&#36807;&#21033;&#29992;&#24179;&#38138;&#30340;&#24605;&#24819;&#65292;&#20998;&#21035;&#22788;&#29702;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#20869;&#37096;&#22359;&#21644;&#22806;&#37096;&#22359;&#32452;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#37319;&#29992;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26426;&#21046;&#22788;&#29702;&#20869;&#37096;&#22359;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#32047;&#31215;&#27714;&#21644;&#26041;&#27861;&#22788;&#29702;&#22806;&#37096;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#27880;&#24847;&#21147;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#20256;&#32479;softmax&#27880;&#24847;&#21147;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#19978;&#33021;&#22815;&#22312;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22788;&#29702;&#26080;&#38480;&#38271;&#24230;&#30340;&#24207;&#21015;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#36895;&#24230;&#65292;&#21363;&#22312;&#22266;&#23450;&#30340;&#20869;&#23384;&#28040;&#32791;&#19979;&#65292;&#33021;&#22815;&#20197;&#24658;&#23450;&#30340;&#35757;&#32451;&#36895;&#24230;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32047;&#31215;&#27714;&#21644;&#65288;cumsum&#65289;&#30340;&#38382;&#39064;&#65292;&#24403;&#21069;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#31639;&#27861;&#26080;&#27861;&#22312;&#22240;&#26524;&#35774;&#32622;&#19979;&#23637;&#29616;&#20854;&#29702;&#35770;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Lightning Attention-2&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#35745;&#31639;&#20248;&#21183;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#24179;&#38138;&#65288;tiling&#65289;&#30340;&#24605;&#24819;&#65292;&#20998;&#21035;&#22788;&#29702;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#20869;&#37096;&#22359;&#21644;&#22806;&#37096;&#22359;&#32452;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26426;&#21046;&#26469;&#22788;&#29702;&#20869;&#37096;&#22359;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#32047;&#31215;&#27714;&#21644;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22806;&#37096;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#23398;&#26415;&#35770;&#25991;&#26032;&#39062;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#24863;&#30693;&#30340;"&#24778;&#21916;"&#31243;&#24230;&#65292;&#32467;&#21512;&#20102;&#38754;&#21521;&#31185;&#23398;&#24120;&#35782;&#21644;&#39046;&#22495;&#19987;&#23478;&#35780;&#20272;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12289;&#32454;&#31890;&#24230;&#21644;&#26131;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03642</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#23398;&#26415;&#35770;&#25991;&#26032;&#39062;&#24615;&#24230;&#37327;&#26041;&#27861;&#65306;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
A Content-Based Novelty Measure for Scholarly Publications: A Proof of Concept. (arXiv:2401.03642v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#23398;&#26415;&#35770;&#25991;&#26032;&#39062;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#24863;&#30693;&#30340;"&#24778;&#21916;"&#31243;&#24230;&#65292;&#32467;&#21512;&#20102;&#38754;&#21521;&#31185;&#23398;&#24120;&#35782;&#21644;&#39046;&#22495;&#19987;&#23478;&#35780;&#20272;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12289;&#32454;&#31890;&#24230;&#21644;&#26131;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#39062;&#24615;&#65292;&#31867;&#20284;&#20110;&#36827;&#21270;&#20013;&#30340;&#22522;&#22240;&#31361;&#21464;&#65292;&#20026;&#23398;&#26415;&#36827;&#27493;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#23613;&#31649;&#21516;&#34892;&#35780;&#23457;&#20173;&#28982;&#26159;&#35780;&#20272;&#23398;&#26415;&#20132;&#27969;&#21644;&#36164;&#28304;&#20998;&#37197;&#20013;&#26032;&#39062;&#24615;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#24222;&#22823;&#30340;&#25237;&#31295;&#37327;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#23398;&#26415;&#26032;&#39062;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#23558;&#26032;&#39062;&#24615;&#35270;&#20026;&#29616;&#26377;&#30693;&#35782;&#30340;&#38750;&#20856;&#22411;&#32452;&#21512;&#30340;&#35266;&#28857;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#23398;&#26415;&#35770;&#25991;&#26032;&#39062;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;&#35813;&#24230;&#37327;&#26041;&#27861;&#37327;&#21270;&#20102;&#34920;&#31034;&#23398;&#26415;&#35752;&#35770;&#30340;&#35789;&#20998;&#24067;&#30340;&#35821;&#35328;&#27169;&#22411;&#25152;&#24863;&#30693;&#30340;&#8220;&#24778;&#21916;&#8221;&#31243;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#20276;&#38543;&#30528;&#38754;&#21521;&#31185;&#23398;&#24120;&#35782;&#30340;&#22806;&#35266;&#21644;&#26500;&#36896;&#26377;&#25928;&#24615;&#35777;&#25454;&#65307;&#21069;&#32773;&#23637;&#31034;&#20102;&#19982;&#31185;&#23398;&#24120;&#35782;&#30340;&#19968;&#33268;&#24615;&#65292;&#21518;&#32773;&#36890;&#36807;&#19982;&#39046;&#22495;&#19987;&#23478;&#23567;&#32452;&#30340;&#26032;&#39062;&#24615;&#35780;&#20272;&#30456;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#24230;&#37327;&#26041;&#27861;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12289;&#32454;&#31890;&#24230;&#21644;&#26131;&#29992;&#24615;&#30340;&#29305;&#28857;&#65292;&#24357;&#34917;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novelty, akin to gene mutation in evolution, opens possibilities for scholarly advancement. Although peer review remains the gold standard for evaluating novelty in scholarly communication and resource allocation, the vast volume of submissions necessitates an automated measure of scholarly novelty. Adopting a perspective that views novelty as the atypical combination of existing knowledge, we introduce an information-theoretic measure of novelty in scholarly publications. This measure quantifies the degree of 'surprise' perceived by a language model that represents the word distribution of scholarly discourse. The proposed measure is accompanied by face and construct validity evidence; the former demonstrates correspondence to scientific common sense, and the latter is endorsed through alignment with novelty evaluations from a select panel of domain experts. Additionally, characterized by its interpretability, fine granularity, and accessibility, this measure addresses gaps prevalent 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02987</link><description>&lt;p&gt;
&#20320;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#25913;&#36827;&#21527;&#65311;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach. (arXiv:2401.02987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#31561;&#39046;&#22495;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#22914;&#20309;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#19982;&#27599;&#20010;&#23454;&#20307;&#30456;&#20851;&#30340;&#20803;&#29305;&#24449;&#20316;&#20026;&#19990;&#30028;&#30693;&#35782;&#30340;&#26469;&#28304;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#21644;&#20803;&#29305;&#24449;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20855;&#26377;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of pretrained models has significantly impacted from Natural Language Processing (NLP) and Computer Vision to relational datasets. Traditionally, these models are assessed through fine-tuned downstream tasks. However, this raises the question of how to evaluate these models more efficiently and more effectively. In this study, we explore a novel approach where we leverage the meta features associated with each entity as a source of worldly knowledge and employ entity representations from the models. We propose using the consistency between these representations and the meta features as a metric for evaluating pretrained models. Our method's effectiveness is demonstrated across various domains, including models with relational datasets, large language models and images models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.02333</link><description>&lt;p&gt;
&#36229;&#36234;&#25552;&#21462;&#65306;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19978;&#19979;&#25991;&#21270;&#30340;&#34920;&#26684;&#25968;&#25454;&#20197;&#23454;&#29616;&#39640;&#25928;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models. (arXiv:2401.02333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104; (RAG) &#26550;&#26500;&#22312;&#20174;&#21508;&#31181;&#25991;&#20214;&#20013;&#26816;&#32034;&#20449;&#24687;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#21253;&#21547;&#22797;&#26434;&#34920;&#26684;&#32467;&#26500;&#30340; PDF &#25991;&#26723;&#20013;&#30340;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#26102;&#20250;&#36935;&#21040;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558; PDF &#23384;&#20648;&#22312;&#26816;&#32034;&#25968;&#25454;&#24211;&#20013;&#65292;&#24182;&#21333;&#29420;&#25552;&#21462;&#34920;&#26684;&#20869;&#23481;&#12290;&#25552;&#21462;&#30340;&#34920;&#26684;&#32463;&#36807;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#22788;&#29702;&#65292;&#23558;&#26631;&#39064;&#19982;&#30456;&#24212;&#30340;&#20540;&#36830;&#25509;&#36215;&#26469;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#20016;&#23500;&#25968;&#25454;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340; Llama-2-chat &#35821;&#35328;&#27169;&#22411;&#22312; RAG &#26550;&#26500;&#20013;&#36827;&#34892;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#27425;&#24615;&#25552;&#31034;&#20351;&#29992; ChatGPT 3.5 API &#22686;&#24378;&#34920;&#26684;&#25968;&#25454;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#20016;&#23500;&#30340;&#25968;&#25454;&#19982;&#20854;&#20182; PDF &#25991;&#20214;&#19968;&#36215;&#36755;&#20837;&#26816;&#32034;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents. However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures.This research introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately. The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values. To ensure a comprehensive understanding of the enriched data, we employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG architecture. Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt. This enriched data is then fed into the retrieval database alongside other PDFs. Our appr
&lt;/p&gt;</description></item><item><title>LLaVA-$\phi$&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#21363;&#20351;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#65292;&#23427;&#20063;&#33021;&#26377;&#25928;&#22320;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#20026;&#26102;&#38388;&#25935;&#24863;&#30340;&#29615;&#22659;&#21644;&#38656;&#35201;&#23454;&#26102;&#20132;&#20114;&#30340;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2401.02330</link><description>&lt;p&gt;
LLaVA-$\phi$: &#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#19982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-$\phi$: Efficient Multi-Modal Assistant with Small Language Model. (arXiv:2401.02330v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02330
&lt;/p&gt;
&lt;p&gt;
LLaVA-$\phi$&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#21363;&#20351;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#65292;&#23427;&#20063;&#33021;&#26377;&#25928;&#22320;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#20026;&#26102;&#38388;&#25935;&#24863;&#30340;&#29615;&#22659;&#21644;&#38656;&#35201;&#23454;&#26102;&#20132;&#20114;&#30340;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LLaVA-$\phi$&#65288;LLaVA-Phi&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#20808;&#36827;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35805;&#30340;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;&#12290;LLaVA-Phi&#22312;&#32039;&#20945;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#39046;&#22495;&#20013;&#26631;&#24535;&#30528;&#37325;&#35201;&#36827;&#23637;&#12290;&#23427;&#35777;&#26126;&#20102;&#21363;&#20351;&#26159;&#20010;&#21442;&#25968;&#21482;&#26377;27&#20159;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#26377;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#21442;&#19982;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#30340;&#22797;&#26434;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21253;&#25324;&#35270;&#35273;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#24863;&#30693;&#31561;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#38500;&#20102;&#22312;&#22810;&#27169;&#24577;&#23545;&#35805;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20026;&#26102;&#38388;&#25935;&#24863;&#30340;&#29615;&#22659;&#21644;&#38656;&#35201;&#23454;&#26102;&#20132;&#20114;&#30340;&#31995;&#32479;&#65288;&#22914;&#23454;&#20307;&#20195;&#29702;&#65289;&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#36884;&#24452;&#12290;&#23427;&#31361;&#26174;&#20102;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#39640;&#32423;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce LLaVA-$\phi$ (LLaVA-Phi), an efficient multi-modal assistant that harnesses the power of the recently advanced small language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a notable advancement in the realm of compact multi-modal models. It demonstrates that even smaller language models, with as few as 2.7B parameters, can effectively engage in intricate dialogues that integrate both textual and visual elements, provided they are trained with high-quality corpora. Our model delivers commendable performance on publicly available benchmarks that encompass visual comprehension, reasoning, and knowledge-based perception. Beyond its remarkable performance in multi-modal dialogue tasks, our model opens new avenues for applications in time-sensitive environments and systems that require real-time interaction, such as embodied agents. It highlights the potential of smaller language models to achieve sophisticated levels of understanding and inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;WordArt&#35774;&#35745;&#24072;API&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#22411;&#33539;&#22260;&#19978;&#36827;&#34892;&#29992;&#25143;&#39537;&#21160;&#30340;&#33402;&#26415;&#23383;&#20307;&#21512;&#25104;&#12290;&#36890;&#36807;&#25552;&#20379;&#21160;&#24577;&#12289;&#33258;&#36866;&#24212;&#21644;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28385;&#36275;&#38750;&#19987;&#19994;&#20154;&#22763;&#31616;&#21270;&#33402;&#26415;&#23383;&#20307;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#30452;&#35266;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;&#19982;&#29616;&#26377;&#31995;&#32479;&#30456;&#27604;&#65292;&#35813;API&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#28385;&#24847;&#24230;&#12289;&#35774;&#35745;&#28789;&#27963;&#24615;&#21644;&#21019;&#36896;&#24615;&#34920;&#36798;&#65292;&#24182;&#20026;&#20010;&#24615;&#21270;&#25968;&#23383;&#36890;&#20449;&#21644;&#35774;&#35745;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01699</link><description>&lt;p&gt;
WordArt&#35774;&#35745;&#24072;API&#65306;&#21033;&#29992;&#27169;&#22411;&#33539;&#22260;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29992;&#25143;&#39537;&#21160;&#30340;&#33402;&#26415;&#23383;&#20307;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
WordArt Designer API: User-Driven Artistic Typography Synthesis with Large Language Models on ModelScope. (arXiv:2401.01699v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;WordArt&#35774;&#35745;&#24072;API&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#22411;&#33539;&#22260;&#19978;&#36827;&#34892;&#29992;&#25143;&#39537;&#21160;&#30340;&#33402;&#26415;&#23383;&#20307;&#21512;&#25104;&#12290;&#36890;&#36807;&#25552;&#20379;&#21160;&#24577;&#12289;&#33258;&#36866;&#24212;&#21644;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28385;&#36275;&#38750;&#19987;&#19994;&#20154;&#22763;&#31616;&#21270;&#33402;&#26415;&#23383;&#20307;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#30452;&#35266;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;&#19982;&#29616;&#26377;&#31995;&#32479;&#30456;&#27604;&#65292;&#35813;API&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#28385;&#24847;&#24230;&#12289;&#35774;&#35745;&#28789;&#27963;&#24615;&#21644;&#21019;&#36896;&#24615;&#34920;&#36798;&#65292;&#24182;&#20026;&#20010;&#24615;&#21270;&#25968;&#23383;&#36890;&#20449;&#21644;&#35774;&#35745;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;WordArt&#35774;&#35745;&#24072;API&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#27169;&#22411;&#33539;&#22260;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29992;&#25143;&#39537;&#21160;&#30340;&#33402;&#26415;&#23383;&#20307;&#21512;&#25104;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#21160;&#24577;&#12289;&#33258;&#36866;&#24212;&#21644;&#35745;&#31639;&#25928;&#26524;&#39640;&#30340;&#26367;&#20195;&#20256;&#32479;&#21018;&#24615;&#27169;&#26495;&#30340;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#38750;&#19987;&#19994;&#20154;&#22763;&#31616;&#21270;&#33402;&#26415;&#23383;&#20307;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#26469;&#29702;&#35299;&#21644;&#35299;&#37322;&#29992;&#25143;&#36755;&#20837;&#65292;&#20419;&#36827;&#26356;&#30452;&#35266;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;&#36890;&#36807;&#21508;&#31181;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#25143;&#22914;&#20309;&#34920;&#36798;&#20182;&#20204;&#30340;&#23457;&#32654;&#20559;&#22909;&#21644;&#21151;&#33021;&#38656;&#27714;&#65292;&#31995;&#32479;&#28982;&#21518;&#23558;&#20854;&#36716;&#21270;&#20026;&#29420;&#29305;&#19988;&#23500;&#26377;&#21019;&#24847;&#30340;&#23383;&#20307;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#31995;&#32479;&#30456;&#27604;&#65292;&#29992;&#25143;&#28385;&#24847;&#24230;&#12289;&#35774;&#35745;&#28789;&#27963;&#24615;&#21644;&#21019;&#36896;&#24615;&#34920;&#36798;&#37117;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;WordArt&#35774;&#35745;&#24072;API&#19981;&#20165;&#20351;&#23383;&#20307;&#33402;&#26415;&#27665;&#20027;&#21270;&#65292;&#36824;&#20026;&#20010;&#24615;&#21270;&#25968;&#23383;&#36890;&#20449;&#21644;&#35774;&#35745;&#25171;&#24320;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the WordArt Designer API, a novel framework for user-driven artistic typography synthesis utilizing Large Language Models (LLMs) on ModelScope. We address the challenge of simplifying artistic typography for non-professionals by offering a dynamic, adaptive, and computationally efficient alternative to traditional rigid templates. Our approach leverages the power of LLMs to understand and interpret user input, facilitating a more intuitive design process. We demonstrate through various case studies how users can articulate their aesthetic preferences and functional requirements, which the system then translates into unique and creative typographic designs. Our evaluations indicate significant improvements in user satisfaction, design flexibility, and creative expression over existing systems. The WordArt Designer API not only democratizes the art of typography but also opens up new possibilities for personalized digital communication and design.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#22270;&#29983;&#25104;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#23545;&#40784;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01326</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#22270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction. (arXiv:2401.01326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01326
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#22270;&#29983;&#25104;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#23545;&#40784;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#30340;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#19982;&#20256;&#32479;&#30340;&#29983;&#25104;&#24335;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#36328;&#24230;&#30340;&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#32447;&#24615;&#21270;&#30340;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#25991;&#26412;&#36328;&#24230;&#65292;&#36793;&#34920;&#31034;&#20851;&#31995;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#25351;&#21521;&#26426;&#21046;&#30340;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#21160;&#24577;&#35789;&#27719;&#34920;&#26469;&#34920;&#31034;&#36328;&#24230;&#21644;&#20851;&#31995;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36328;&#24230;&#34920;&#31034;&#25429;&#25417;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#32467;&#26500;&#29305;&#24449;&#21644;&#36793;&#30028;&#65292;&#21516;&#26102;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/urchade/ATG&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method for joint entity and relation extraction from unstructured text by framing it as a conditional sequence generation problem. In contrast to conventional generative information extraction models that are left-to-right token-level generators, our approach is \textit{span-based}. It generates a linearized graph where nodes represent text spans and edges represent relation triplets. Our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types. Our model can capture the structural characteristics and boundaries of entities and relations through span representations while simultaneously grounding the generated output in the original text thanks to the pointing mechanism. Evaluation on benchmark datasets validates the effectiveness of our approach, demonstrating competitive results. Code is available at https://github.com/urchade/ATG.
&lt;/p&gt;</description></item><item><title>ToolEyes&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#26223;&#20013;&#30340;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#19971;&#20010;&#30495;&#23454;&#24773;&#26223;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#30340;&#20116;&#20010;&#20851;&#38190;&#32500;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25317;&#26377;600&#31181;&#24037;&#20855;&#30340;&#24037;&#20855;&#24211;&#20316;&#20026;&#20013;&#20171;&#12290;</title><link>http://arxiv.org/abs/2401.00741</link><description>&lt;p&gt;
ToolEyes&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24773;&#26223;&#20013;&#30340;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios. (arXiv:2401.00741v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00741
&lt;/p&gt;
&lt;p&gt;
ToolEyes&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#26223;&#20013;&#30340;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#19971;&#20010;&#30495;&#23454;&#24773;&#26223;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#30340;&#20116;&#20010;&#20851;&#38190;&#32500;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25317;&#26377;600&#31181;&#24037;&#20855;&#30340;&#24037;&#20855;&#24211;&#20316;&#20026;&#20013;&#20171;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36873;&#25321;&#30340;&#24037;&#20855;&#19982;&#26399;&#26395;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#26377;&#38480;&#30340;&#24773;&#26223;&#65292;&#22312;&#36825;&#20123;&#24773;&#26223;&#20013;&#31572;&#26696;&#21487;&#20197;&#20107;&#20808;&#30830;&#23450;&#65292;&#19982;&#30495;&#23454;&#38656;&#27714;&#32972;&#36947;&#32780;&#39536;&#12290;&#27492;&#22806;&#65292;&#20165;&#20851;&#27880;&#32467;&#26524;&#24573;&#35270;&#20102;LLMs&#26377;&#25928;&#21033;&#29992;&#24037;&#20855;&#25152;&#38656;&#30340;&#22797;&#26434;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToolEyes&#65292;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#38024;&#23545;LLMs&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#22312;&#30495;&#23454;&#24773;&#26223;&#20013;&#35780;&#20272;&#30340;&#32454;&#31890;&#24230;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#35814;&#32454;&#20998;&#26512;&#20102;&#19971;&#20010;&#30495;&#23454;&#24773;&#26223;&#65292;&#20998;&#26512;&#20102;&#23545;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20116;&#20010;&#32500;&#24230;&#65306;&#26684;&#24335;&#23545;&#40784;&#65292;&#24847;&#22270;&#29702;&#35299;&#65292;&#34892;&#20026;&#35268;&#21010;&#65292;&#24037;&#20855;&#36873;&#25321;&#21644;&#31572;&#26696;&#32452;&#32455;&#12290;&#27492;&#22806;&#65292;ToolEyes&#36824;&#21253;&#21547;&#19968;&#20010;&#25317;&#26377;&#32422;600&#31181;&#24037;&#20855;&#30340;&#24037;&#20855;&#24211;&#65292;&#20316;&#20026;LLMs&#19982;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#20013;&#20171;&#12290;&#22312;&#28041;&#21450;&#19977;&#20010;&#31867;&#21035;&#30340;&#21313;&#20010;LLMs&#30340;&#35780;&#20272;&#20013;&#65292;ToolEyes&#21462;&#24471;&#20102;&#22914;&#19979;&#30340;&#21019;&#26032;&#19982;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing evaluations of tool learning primarily focus on validating the alignment of selected tools for large language models (LLMs) with expected outcomes. However, these approaches rely on a limited set of scenarios where answers can be pre-determined, diverging from genuine needs. Furthermore, a sole emphasis on outcomes disregards the intricate capabilities essential for LLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a fine-grained system tailored for the evaluation of the LLMs' tool learning capabilities in authentic scenarios. The system meticulously examines seven real-world scenarios, analyzing five dimensions crucial to LLMs in tool learning: format alignment, intent comprehension, behavior planning, tool selection, and answer organization. Additionally, ToolEyes incorporates a tool library boasting approximately 600 tools, serving as an intermediary between LLMs and the physical world. Evaluations involving ten LLMs across three categories revea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#23494;&#38598;&#26816;&#32034;&#20013;&#20551;&#38452;&#24615;&#24433;&#21709;&#30340;&#23545;&#27604;&#32622;&#20449;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#30340;&#30828;&#36127;&#20363;&#37319;&#26679;&#38382;&#39064;&#65292;&#20351;&#24471;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#26356;&#21152;&#31283;&#20581;&#12290;</title><link>http://arxiv.org/abs/2401.00165</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#32622;&#20449;&#24230;&#27491;&#21017;&#21270;&#26469;&#20943;&#36731;&#23494;&#38598;&#26816;&#32034;&#20013;&#30340;&#20551;&#38452;&#24615;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Impact of False Negatives in Dense Retrieval with Contrastive Confidence Regularization. (arXiv:2401.00165v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00165
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#23494;&#38598;&#26816;&#32034;&#20013;&#20551;&#38452;&#24615;&#24433;&#21709;&#30340;&#23545;&#27604;&#32622;&#20449;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#30340;&#30828;&#36127;&#20363;&#37319;&#26679;&#38382;&#39064;&#65292;&#20351;&#24471;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#65292;&#23494;&#38598;&#26816;&#32034;&#23545;&#20110;&#25214;&#21040;&#30456;&#20851;&#27573;&#33853;&#20197;&#29983;&#25104;&#31572;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#24120;&#65292;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#23558;&#27573;&#33853;&#21644;&#26597;&#35810;&#26144;&#23556;&#21040;&#30456;&#21516;&#35821;&#20041;&#31354;&#38388;&#30340;&#26816;&#32034;&#27169;&#22411;&#12290;&#20854;&#30446;&#26631;&#26159;&#20351;&#30456;&#20284;&#30340;&#26356;&#21152;&#25509;&#36817;&#65292;&#19981;&#30456;&#20284;&#30340;&#26356;&#21152;&#36828;&#31163;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34394;&#20551;&#38452;&#24615;&#38382;&#39064;&#65292;&#35757;&#32451;&#36825;&#26679;&#30340;&#31995;&#32479;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#21363;&#22312;&#25968;&#25454;&#27880;&#37322;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#38169;&#36807;&#30456;&#20851;&#27573;&#33853;&#12290;&#36890;&#24120;&#29992;&#20110;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#30340;&#30828;&#36127;&#20363;&#37319;&#26679;&#21487;&#33021;&#20250;&#22312;&#35757;&#32451;&#20013;&#24341;&#20837;&#26356;&#22810;&#22122;&#22768;&#12290;&#36825;&#26159;&#22240;&#20026;&#30828;&#36127;&#20363;&#26159;&#37027;&#20123;&#38752;&#36817;&#32473;&#23450;&#26597;&#35810;&#30340;&#26679;&#26412;&#65292;&#22240;&#27492;&#26356;&#26377;&#21487;&#33021;&#26159;&#34394;&#20551;&#38452;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#32622;&#20449;&#24230;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#24120;&#29992;&#30340;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#27491;&#21017;&#21270;&#22120;&#33021;&#22815;&#24110;&#21161;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#26356;&#21152;&#31283;&#20581;&#22320;&#24212;&#23545;&#34394;&#20551;&#38452;&#24615;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In open-domain Question Answering (QA), dense retrieval is crucial for finding relevant passages for answer generation. Typically, contrastive learning is used to train a retrieval model that maps passages and queries to the same semantic space. The objective is to make similar ones closer and dissimilar ones further apart. However, training such a system is challenging due to the false negative issue, where relevant passages may be missed during data annotation. Hard negative sampling, which is commonly used to improve contrastive learning, can introduce more noise in training. This is because hard negatives are those closer to a given query, and thus more likely to be false negatives. To address this issue, we propose a novel contrastive confidence regularizer for Noise Contrastive Estimation (NCE) loss, a commonly used loss for dense retrieval. Our analysis shows that the regularizer helps dense retrieval models be more robust against false negatives with a theoretical guarantee. Ad
&lt;/p&gt;</description></item><item><title>MosaicBERT&#26159;&#19968;&#31181;&#20248;&#21270;&#30340;BERT&#39118;&#26684;&#21452;&#21521;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#39033;&#21019;&#26032;&#25216;&#26415;&#21644;&#20248;&#21270;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2312.17482</link><description>&lt;p&gt;
MosaicBERT&#65306;&#19968;&#31181;&#38024;&#23545;&#24555;&#36895;&#39044;&#35757;&#32451;&#36827;&#34892;&#20248;&#21270;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining. (arXiv:2312.17482v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17482
&lt;/p&gt;
&lt;p&gt;
MosaicBERT&#26159;&#19968;&#31181;&#20248;&#21270;&#30340;BERT&#39118;&#26684;&#21452;&#21521;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#39033;&#21019;&#26032;&#25216;&#26415;&#21644;&#20248;&#21270;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;BERT&#39118;&#26684;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#22312;NLP&#30740;&#31350;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#19981;&#20250;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#33258;&#24049;&#30340;BERT&#12290;&#22312;BERT&#39318;&#27425;&#23853;&#38706;&#22836;&#35282;&#30340;&#36807;&#21435;&#21322;-decade&#65292;&#24050;&#32463;&#23545;&#20854;&#20182;&#21464;&#21387;&#22120;&#26550;&#26500;&#21644;&#35757;&#32451;&#37197;&#32622;&#36827;&#34892;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#23578;&#26410;&#31995;&#32479;&#22320;&#32435;&#20837;BERT&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MosaicBERT&#65292;&#19968;&#31181;&#32463;&#39564;&#20248;&#21270;&#29992;&#20110;&#24555;&#36895;&#39044;&#35757;&#32451;&#30340;BERT&#39118;&#26684;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#31181;&#39640;&#25928;&#30340;&#26550;&#26500;&#23558;FlashAttention&#12289;&#24102;&#26377;&#32447;&#24615;&#20559;&#24046;&#30340;Attention (ALiBi)&#12289;&#38376;&#25511;&#32447;&#24615;&#21333;&#20803; (GLU)&#12289;&#21160;&#24577;&#31227;&#38500;&#22635;&#20805;&#20196;&#29260;&#30340;&#27169;&#22359;&#21644;&#20302;&#31934;&#24230;LayerNorm&#31561;&#24341;&#20837;&#20102;&#32463;&#20856;&#30340;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#22359;&#12290;&#35757;&#32451;&#26041;&#27861;&#36824;&#21253;&#25324;30%&#30340;&#25513;&#30721;&#27604;&#29575;&#29992;&#20110;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169; (MLM) &#30446;&#26631;&#65292;bfloat16&#31934;&#24230;&#65292;&#20197;&#21450;&#38024;&#23545;GPU&#21534;&#21520;&#37327;&#36827;&#34892;&#20248;&#21270;&#30340;&#35789;&#27719;&#22823;&#23567;&#65292;&#27492;&#22806;&#36824;&#37319;&#29992;&#20102;RoBERTa&#21644;&#20854;&#20182;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although BERT-style encoder models are heavily used in NLP research, many researchers do not pretrain their own BERTs from scratch due to the high cost of training. In the past half-decade since BERT first rose to prominence, many advances have been made with other transformer architectures and training configurations that have yet to be systematically incorporated into BERT. Here, we introduce MosaicBERT, a BERT-style encoder architecture and training recipe that is empirically optimized for fast pretraining. This efficient architecture incorporates FlashAttention, Attention with Linear Biases (ALiBi), Gated Linear Units (GLU), a module to dynamically remove padded tokens, and low precision LayerNorm into the classic transformer encoder block. The training recipe includes a 30% masking ratio for the Masked Language Modeling (MLM) objective, bfloat16 precision, and vocabulary size optimized for GPU throughput, in addition to best-practices from RoBERTa and other encoder models. When pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MVRE&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#22810;&#35270;&#35282;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.17267</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35270;&#35282;&#35299;&#32806;&#23398;&#20064;&#25913;&#36827;&#20302;&#36164;&#28304;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#20851;&#31995;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving Low-resource Prompt-based Relation Representation with Multi-view Decoupling Learning. (arXiv:2312.17267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17267
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MVRE&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#22810;&#35270;&#35282;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#20219;&#21153;&#30340;&#22686;&#24378;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#23545;&#20851;&#31995;&#30340;&#34920;&#23618;&#29702;&#35299;&#65292;&#20808;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#33021;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24378;&#35843;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#20851;&#31995;&#34920;&#31034;&#23545;&#20110;RE&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#20851;&#31995;&#34920;&#31034;&#26041;&#27861;&#65292;&#21517;&#20026;MVRE&#65288;&#22810;&#35270;&#35282;&#20851;&#31995;&#25277;&#21462;&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;PLMs&#30340;&#33021;&#21147;&#26469;&#25913;&#21892;&#20302;&#36164;&#28304;&#25552;&#31034;&#35843;&#25972;&#33539;&#24335;&#19979;&#30340;RE&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MVRE&#23558;&#27599;&#20010;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#20197;&#21253;&#21547;&#22810;&#35270;&#35282;&#30340;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#31995;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#20284;&#28982;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#23616;&#24615;&#30340;&#20302;&#39046;&#22495;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20851;&#31995;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, prompt-tuning with pre-trained language models (PLMs) has demonstrated the significantly enhancing ability of relation extraction (RE) tasks. However, in low-resource scenarios, where the available training data is scarce, previous prompt-based methods may still perform poorly for prompt-based representation learning due to a superficial understanding of the relation. To this end, we highlight the importance of learning high-quality relation representation in low-resource scenarios for RE, and propose a novel prompt-based relation representation method, named MVRE (\underline{M}ulti-\underline{V}iew \underline{R}elation \underline{E}xtraction), to better leverage the capacity of PLMs to improve the performance of RE within the low-resource prompt-tuning paradigm. Specifically, MVRE decouples each relation into different perspectives to encompass multi-view relation representations for maximizing the likelihood during relation inference. Furthermore, we also design a Global-Lo
&lt;/p&gt;</description></item><item><title>T-Eval&#26159;&#19968;&#31181;&#36880;&#27493;&#35780;&#20272;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#24037;&#20855;&#21033;&#29992;&#35780;&#20272;&#35299;&#32806;&#20026;&#22810;&#20010;&#23376;&#39046;&#22495;&#65292;&#20174;&#32780;&#33021;&#22815;&#26356;&#32454;&#33268;&#22320;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.14033</link><description>&lt;p&gt;
T-Eval: &#36880;&#27493;&#35780;&#20272;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
T-Eval: Evaluating the Tool Utilization Capability Step by Step. (arXiv:2312.14033v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14033
&lt;/p&gt;
&lt;p&gt;
T-Eval&#26159;&#19968;&#31181;&#36880;&#27493;&#35780;&#20272;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#24037;&#20855;&#21033;&#29992;&#35780;&#20272;&#35299;&#32806;&#20026;&#22810;&#20010;&#23376;&#39046;&#22495;&#65292;&#20174;&#32780;&#33021;&#22815;&#26356;&#32454;&#33268;&#22320;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#24037;&#20855;&#36827;&#34892;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35780;&#20272;&#21644;&#20998;&#26512;LLM&#30340;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#19982;&#20197;&#24448;&#35780;&#20272;&#27169;&#22411;&#25972;&#20307;&#24615;&#33021;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#24037;&#20855;&#21033;&#29992;&#20840;&#38754;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#36807;&#31243;&#65292;&#21253;&#25324;&#25351;&#20196;&#36319;&#38543;&#12289;&#35268;&#21010;&#12289;&#25512;&#29702;&#12289;&#26816;&#32034;&#12289;&#29702;&#35299;&#21644;&#22797;&#26597;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;T-Eval&#26469;&#36880;&#27493;&#35780;&#20272;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#12290;T-Eval&#23558;&#24037;&#20855;&#21033;&#29992;&#35780;&#20272;&#35299;&#32806;&#20026;&#22810;&#20010;&#23376;&#39046;&#22495;&#65292;&#26377;&#21161;&#20110;&#23545;LLM&#30340;&#25972;&#20307;&#21644;&#29420;&#31435;&#33021;&#21147;&#36827;&#34892;&#20869;&#37096;&#29702;&#35299;&#12290;&#25105;&#20204;&#23545;T-Eval&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#21644;&#21508;&#31181;LLM&#30340;&#28145;&#20837;&#20998;&#26512;&#12290;T-Eval&#19981;&#20165;&#23637;&#29616;&#20102;&#19982;&#32467;&#26524;&#23548;&#21521;&#35780;&#20272;&#30340;&#19968;&#33268;&#24615;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;LLM&#33021;&#21147;&#26356;&#32454;&#33268;&#30340;&#20998;&#26512;&#65292;&#34920;&#26126;LLM&#20855;&#22791;&#20102;&#19968;&#23450;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool-utilization capability of LLMs is still under-explored. In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review. Based on that, we further introduce T-Eval to evaluate the tool utilization capability step by step. T-Eval disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of various LLMs. T-Eval not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#20114;&#24335;&#23545;&#27604;&#23398;&#20064;&#30340;&#30693;&#35782;&#22270;&#35889;&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;CCA&#65292;&#36890;&#36807;&#32508;&#21512;&#25991;&#26412;&#21644;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#26356;&#22909;&#22320;&#21306;&#20998;&#35821;&#20041;&#65292;&#20174;&#32780;&#22312;&#26816;&#27979;&#22122;&#22768;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.12108</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#32622;&#20449;&#24230;&#35843;&#25972;&#30340;&#30693;&#35782;&#22270;&#35889;&#38169;&#35823;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Error Detection with Contrastive Confidence Adaption. (arXiv:2312.12108v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#20114;&#24335;&#23545;&#27604;&#23398;&#20064;&#30340;&#30693;&#35782;&#22270;&#35889;&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;CCA&#65292;&#36890;&#36807;&#32508;&#21512;&#25991;&#26412;&#21644;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#26356;&#22909;&#22320;&#21306;&#20998;&#35821;&#20041;&#65292;&#20174;&#32780;&#22312;&#26816;&#27979;&#22122;&#22768;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#24120;&#24120;&#21253;&#21547;&#21508;&#31181;&#38169;&#35823;&#12290;&#20197;&#24448;&#20851;&#20110;KG&#38169;&#35823;&#26816;&#27979;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#22270;&#32467;&#26500;&#20013;&#23884;&#20837;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21306;&#20998;&#22122;&#22768;&#21644;&#35821;&#20041;&#30456;&#20284;&#30340;&#27491;&#30830;&#19977;&#20803;&#32452;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KG&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;CCA&#65292;&#36890;&#36807;&#20174;&#19977;&#20803;&#32452;&#37325;&#26500;&#20013;&#32508;&#21512;&#25991;&#26412;&#21644;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#26356;&#22909;&#22320;&#21306;&#20998;&#35821;&#20041;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20132;&#20114;&#24335;&#23545;&#27604;&#23398;&#20064;&#26469;&#25429;&#25417;&#25991;&#26412;&#21644;&#32467;&#26500;&#27169;&#24335;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20855;&#26377;&#35821;&#20041;&#30456;&#20284;&#22122;&#22768;&#21644;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CCA&#22312;&#26816;&#27979;&#35821;&#20041;&#30456;&#20284;&#22122;&#22768;&#21644;&#23545;&#25239;&#24615;&#22122;&#22768;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) often contain various errors. Previous works on detecting errors in KGs mainly rely on triplet embedding from graph structure. We conduct an empirical study and find that these works struggle to discriminate noise from semantically-similar correct triplets. In this paper, we propose a KG error detection model CCA to integrate both textual and graph structural information from triplet reconstruction for better distinguishing semantics. We design interactive contrastive learning to capture the differences between textual and structural patterns. Furthermore, we construct realistic datasets with semantically-similar noise and adversarial noise. Experimental results demonstrate that CCA outperforms state-of-the-art baselines, especially in detecting semantically-similar noise and adversarial noise.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#21407;&#25991;&#25913;&#20889;"&#30340;&#20219;&#21153;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#38382;&#31572;&#65292;&#36890;&#36807;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#25104;&#21151;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#33267;32k&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.11193</link><description>&lt;p&gt;
"&#21407;&#25991;&#25913;&#20889;"&#25552;&#39640;&#20102;&#39640;&#31934;&#24230;&#38271;&#25991;&#26412;&#38382;&#31572;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
"Paraphrasing The Original Text" Makes High Accuracy Long-Context QA. (arXiv:2312.11193v6 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#21407;&#25991;&#25913;&#20889;"&#30340;&#20219;&#21153;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#38382;&#31572;&#65292;&#36890;&#36807;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#25104;&#21151;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#33267;32k&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38754;&#23545;&#38271;&#25991;&#26412;&#26102;&#65292;&#22823;&#22810;&#25968;&#24320;&#28304;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#38480;&#21046;&#22312;4k&#20197;&#20869;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#21363;&#20351;&#26159;&#20855;&#26377;&#26356;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#27169;&#22411;&#20063;&#26080;&#27861;&#22312;&#38271;&#19978;&#19979;&#25991;&#38382;&#39064;&#19978;&#20445;&#35777;&#20196;&#20154;&#28385;&#24847;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25552;&#39640;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#38656;&#35201;&#30340;&#26159;"&#26377;&#25928;"&#32780;&#19981;&#20165;&#20165;&#26159;"&#38271;"&#30340;&#25968;&#25454;&#12290;&#22522;&#20110;&#36825;&#20010;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;"&#21407;&#25991;&#25913;&#20889;"&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#23558;&#29616;&#26377;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;32k&#12290;&#25105;&#20204;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;&#20855;&#26377;&#30456;&#36817;&#35268;&#27169;&#30340;&#27169;&#22411;&#20013;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#24050;&#32463;&#22312;HuggingFace&#65288;https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k&#65289;&#21644;WiseModel&#65288;https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k&#65289;&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most open-source generative language models currently have a context window of no more than 4k, limiting their ability when facing long text. Even models with longer context windows cannot guarantee satisfactory accuracy on long-context problems. To tackle this issue, we explore from the perspective of training data and theoretically demonstrate that improving the capability to handle long contexts requires "effective" rather than simply "long" data. Based on this insight, we propose using the "original text paraphrasing" task and successfully extend the context window of existing models to 32k through a low-cost and effective method. Our fine-tuned model achieves state-of-the-art accuracy in multi-document-QA among models of comparable scale. The model and training data have been made available on HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#30340;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#21516;&#20041;&#35789;&#25968;&#25454;&#65292;&#22312;&#35299;&#20915;&#19978;&#19979;&#25991;&#29305;&#23450;&#35789;&#20041;&#30340;&#25361;&#25112;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.10048</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#30340;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Enhanced Aspect-Level Sentiment Analysis. (arXiv:2312.10048v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#30340;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#21516;&#20041;&#35789;&#25968;&#25454;&#65292;&#22312;&#35299;&#20915;&#19978;&#19979;&#25991;&#29305;&#23450;&#35789;&#20041;&#30340;&#25361;&#25112;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#19978;&#19979;&#25991;&#29305;&#23450;&#35789;&#20041;&#30340;&#25361;&#25112;&#65292;&#22686;&#24378;&#24773;&#24863;&#20998;&#26512;&#12290;&#23427;&#23558;BERT&#27169;&#22411;&#30340;&#20248;&#21183;&#19982;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21516;&#20041;&#35789;&#25968;&#25454;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#21327;&#21516;&#20316;&#29992;&#21033;&#29992;&#21160;&#24577;&#27880;&#24847;&#26426;&#21046;&#26469;&#26500;&#24314;&#19968;&#20010;&#30693;&#35782;&#39537;&#21160;&#30340;&#29366;&#24577;&#21521;&#37327;&#12290;&#20026;&#20102;&#23545;&#29305;&#23450;&#26041;&#38754;&#38142;&#25509;&#30340;&#24773;&#24863;&#36827;&#34892;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#20301;&#32622;&#25968;&#25454;&#30340;&#35760;&#24518;&#24211;&#12290;&#28982;&#21518;&#20351;&#29992;DCGRU&#20998;&#26512;&#25968;&#25454;&#65292;&#20197;&#30830;&#23450;&#19982;&#29305;&#23450;&#26041;&#38754;&#26415;&#35821;&#30456;&#20851;&#30340;&#24773;&#24863;&#29305;&#24449;&#12290;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#31867;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method to enhance sentiment analysis by addressing the challenge of context-specific word meanings. It combines the advantages of a BERT model with a knowledge graph based synonym data. This synergy leverages a dynamic attention mechanism to develop a knowledge-driven state vector. For classifying sentiments linked to specific aspects, the approach constructs a memory bank integrating positional data. The data are then analyzed using a DCGRU to pinpoint sentiment characteristics related to specific aspect terms. Experiments on three widely used datasets demonstrate the superior performance of our method in sentiment classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20302;&#31934;&#24230;&#24494;&#35843;&#20013;&#20943;&#23569;&#24322;&#24120;&#28608;&#27963;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#29992;8&#20301;&#25972;&#25968;&#34920;&#31034;&#24322;&#24120;&#28608;&#27963;&#20540;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25972;&#25968;&#21644;&#36816;&#31639;&#31526;&#20999;&#29255;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.09211</link><description>&lt;p&gt;
&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#20302;&#31934;&#24230;&#24494;&#35843;&#20013;&#30340;&#24322;&#24120;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language Models. (arXiv:2312.09211v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20302;&#31934;&#24230;&#24494;&#35843;&#20013;&#20943;&#23569;&#24322;&#24120;&#28608;&#27963;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#29992;8&#20301;&#25972;&#25968;&#34920;&#31034;&#24322;&#24120;&#28608;&#27963;&#20540;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25972;&#25968;&#21644;&#36816;&#31639;&#31526;&#20999;&#29255;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31934;&#24230;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#33021;&#28304;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37096;&#32626;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#28608;&#27963;&#20013;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;&#28608;&#27963;&#20013;&#30340;&#24322;&#24120;&#20540;&#20250;&#23545;&#20302;&#31934;&#24230;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#20204;&#24433;&#21709;&#20102;&#32553;&#25918;&#22240;&#23376;&#65292;&#20351;&#24471;&#34920;&#31034;&#36739;&#23567;&#30340;&#20540;&#21464;&#24471;&#26356;&#22256;&#38590;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20302;&#31934;&#24230;&#25972;&#25968;&#24494;&#35843;&#20013;&#20943;&#23569;&#24322;&#24120;&#28608;&#27963;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26032;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#29992;8&#20301;&#25972;&#25968;&#32780;&#19981;&#26159;&#28014;&#28857;&#65288;FP16&#65289;&#20540;&#34920;&#31034;&#24322;&#24120;&#28608;&#27963;&#20540;&#12290;&#20351;&#29992;&#25972;&#25968;&#26469;&#34920;&#31034;&#24322;&#24120;&#20540;&#30340;&#22909;&#22788;&#26159;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#36816;&#31639;&#31526;&#20999;&#29255;&#26469;&#36991;&#20813;&#25191;&#34892;16&#20301;&#25972;&#25968;&#30697;&#38453;&#20056;&#27861;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#25903;&#25345;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-precision fine-tuning of language models has gained prominence as a cost-effective and energy-efficient approach to deploying large-scale models in various applications. However, this approach is susceptible to the existence of outlier values in activation. The outlier values in the activation can negatively affect the performance of fine-tuning language models in the low-precision regime since they affect the scaling factor and thus make representing smaller values harder. This paper investigates techniques for mitigating outlier activation in low-precision integer fine-tuning of the language models. Our proposed novel approach enables us to represent the outlier activation values in 8-bit integers instead of floating-point (FP16) values. The benefit of using integers for outlier values is that it enables us to use operator tiling to avoid performing 16-bit integer matrix multiplication to address this problem effectively. We provide theoretical analysis and supporting experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;33881&#20221;&#19987;&#21033;&#25991;&#20214;&#30340;&#26679;&#26412;&#65292;&#23558;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#38416;&#37322;&#20026;&#30693;&#35782;&#22270;&#35889;&#65292;&#20174;&#32780;&#25581;&#31034;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2312.06355</link><description>&lt;p&gt;
&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Linguistic and Structural Basis of Engineering Design Knowledge. (arXiv:2312.06355v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;33881&#20221;&#19987;&#21033;&#25991;&#20214;&#30340;&#26679;&#26412;&#65292;&#23558;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#38416;&#37322;&#20026;&#30693;&#35782;&#22270;&#35889;&#65292;&#20174;&#32780;&#25581;&#31034;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#21697;&#25551;&#36848;&#26159;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#20027;&#35201;&#36733;&#20307;&#65292;&#26082;&#26159;&#35774;&#35745;&#36807;&#31243;&#30340;&#20135;&#29289;&#65292;&#20063;&#26159;&#39537;&#21160;&#35774;&#35745;&#36807;&#31243;&#30340;&#22240;&#32032;&#12290;&#23613;&#31649;&#29289;&#21697;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#20869;&#28085;&#36827;&#34892;&#25551;&#36848;&#65292;&#20294;&#35774;&#35745;&#36807;&#31243;&#38656;&#35201;&#19968;&#31181;&#25551;&#36848;&#26469;&#20307;&#29616;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#65292;&#36825;&#36890;&#36807;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#23433;&#25490;&#22312;&#25991;&#26412;&#20013;&#34920;&#29616;&#20986;&#26469;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20174;&#21508;&#31181;&#25991;&#26412;&#20013;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#29983;&#25104;&#20307;&#29616;&#26126;&#30830;&#30340;&#24037;&#31243;&#35774;&#35745;&#20107;&#23454;&#30340;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;&#26412;&#20307;&#35770;&#35774;&#35745;&#29702;&#35770;&#24456;&#23569;&#33021;&#25351;&#23548;&#30446;&#21069;&#20165;&#38480;&#20110;&#26500;&#24605;&#21644;&#23398;&#20064;&#30446;&#30340;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#20174;33881&#20221;&#19987;&#21033;&#25991;&#20214;&#30340;&#22823;&#26679;&#26412;&#20013;&#23558;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#38416;&#37322;&#20026;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#30740;&#31350;&#36825;&#20123;&#30693;&#35782;&#22270;&#35889;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#29702;&#35299;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artefact descriptions are the primary carriers of engineering design knowledge that is both an outcome and a driver of the design process. While an artefact could be described in different connotations, the design process requires a description to embody engineering design knowledge, which is expressed in the text through intricate placement of entities and relationships. As large-language models learn from all kinds of text merely as a sequence of characters/tokens, these are yet to generate text that embodies explicit engineering design facts. Existing ontological design theories are less likely to guide the large-language models whose applications are currently limited to ideation and learning purposes. In this article, we explicate engineering design knowledge as knowledge graphs from a large sample of 33,881 patent documents. We examine the constituents of these knowledge graphs to understand the linguistic and structural basis of engineering design knowledge. In terms of linguist
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;CLadder&#65292;&#24182;&#21033;&#29992;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#23558;&#31526;&#21495;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22810;&#20010;LLMs&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2312.04350</link><description>&lt;p&gt;
CLadder: &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models. (arXiv:2312.04350v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;CLadder&#65292;&#24182;&#21033;&#29992;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#23558;&#31526;&#21495;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22810;&#20010;LLMs&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#30340;&#33021;&#21147;&#34987;&#24191;&#27867;&#35270;&#20026;&#26234;&#33021;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#21542;&#36830;&#36143;&#22320;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;LLMs&#20013;&#30340;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;&#65292;&#26410;&#33021;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25353;&#29031;&#19968;&#32452;&#26126;&#30830;&#23450;&#20041;&#30340;&#24418;&#24335;&#35268;&#21017;&#25191;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#21463;&#21040;Judea Pearl&#31561;&#20154;&#25552;&#20986;&#30340;&#8220;&#22240;&#26524;&#25512;&#26029;&#24341;&#25806;&#8221;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;10K&#20010;&#26679;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLadder&#65292;&#36890;&#36807;&#19968;&#31181;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#65292;&#22522;&#20110;&#19968;&#32452;&#22240;&#26524;&#22270;&#21644;&#26597;&#35810;(&#32852;&#21512;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;)&#65292;&#24471;&#21040;&#31526;&#21495;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#24182;&#23558;&#20854;&#32763;&#35793;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#20010;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24341;&#20837;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the "causal inference engine" postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-th
&lt;/p&gt;</description></item><item><title>ChatGPT&#19968;&#21608;&#24180;&#32426;&#24565;&#65292;&#35843;&#26597;&#20102;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#36214;&#19978;&#20102;&#23553;&#38381;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#23613;&#31649;&#21518;&#32773;&#20173;&#28982;&#20248;&#20110;&#21069;&#32773;&#65292;&#20294;&#24320;&#28304;&#27169;&#22411;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#24050;&#32463;&#36798;&#21040;&#20102;&#30456;&#21516;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.16989</link><description>&lt;p&gt;
ChatGPT&#19968;&#21608;&#24180;&#32426;&#24565;&#65306;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#22312;&#36214;&#19978;&#20102;&#65311;
&lt;/p&gt;
&lt;p&gt;
ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?. (arXiv:2311.16989v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16989
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#19968;&#21608;&#24180;&#32426;&#24565;&#65292;&#35843;&#26597;&#20102;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#36214;&#19978;&#20102;&#23553;&#38381;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#23613;&#31649;&#21518;&#32773;&#20173;&#28982;&#20248;&#20110;&#21069;&#32773;&#65292;&#20294;&#24320;&#28304;&#27169;&#22411;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#24050;&#32463;&#36798;&#21040;&#20102;&#30456;&#21516;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20110;2022&#24180;&#24213;&#21457;&#24067;&#21518;&#65292;&#24102;&#26469;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30740;&#31350;&#21644;&#21830;&#19994;&#39046;&#22495;&#30340;&#24040;&#22823;&#36716;&#21464;&#12290;&#36890;&#36807;&#20351;&#29992;&#26377;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#25351;&#20196;&#65292;&#23427;&#23637;&#31034;&#20102;&#27169;&#22411;&#21487;&#20197;&#22238;&#31572;&#20154;&#31867;&#38382;&#39064;&#24182;&#36981;&#24490;&#24191;&#27867;&#20219;&#21153;&#38754;&#26495;&#19978;&#30340;&#25351;&#20196;&#12290;&#22312;&#36825;&#19968;&#25104;&#21151;&#20043;&#21518;&#65292;&#23545;&#20110;LLMs&#30340;&#20852;&#36259;&#25345;&#32493;&#22686;&#21152;&#65292;&#21253;&#25324;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20013;&#39057;&#32321;&#28044;&#29616;&#30340;&#26032;&#22411;LLMs&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#19987;&#27880;&#20110;LLMs&#30340;&#21021;&#21019;&#20225;&#19994;&#12290;&#23613;&#31649;&#23553;&#38381;&#28304;&#20195;&#30721;&#30340;LLMs&#65288;&#22914;OpenAI&#30340;GPT&#12289;Anthropic&#30340;Claude&#65289;&#36890;&#24120;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#24320;&#28304;&#23545;&#24212;&#29289;&#65292;&#20294;&#21518;&#32773;&#30340;&#36827;&#23637;&#36805;&#36895;&#65292;&#22768;&#31216;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25110;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#19981;&#20165;&#23545;&#30740;&#31350;&#65292;&#36824;&#23545;&#19994;&#21153;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#22312;ChatGPT&#19968;&#21608;&#24180;&#20043;&#38469;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#19968;&#25104;&#21151;&#30340;&#35814;&#23613;&#27010;&#36848;&#65292;&#24182;&#35843;&#26597;&#20102;&#25152;&#26377;&#24320;&#28304;LLM&#22768;&#31216;&#24050;&#32463;&#23454;&#29616;&#20102;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Upon its release in late 2022, ChatGPT has brought a seismic shift in the entire landscape of AI, both in research and commerce. Through instruction-tuning a large language model (LLM) with supervised fine-tuning and reinforcement learning from human feedback, it showed that a model could answer human questions and follow instructions on a broad panel of tasks. Following this success, interests in LLMs have intensified, with new LLMs flourishing at frequent interval across academia and industry, including many start-ups focused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's Claude) generally outperform their open-source counterparts, the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks. This has crucial implications not only on research but also on business. In this work, on the first anniversary of ChatGPT, we provide an exhaustive overview of this success, surveying all tasks where an open-source LLM has claimed
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20934;&#30830;&#21487;&#38752;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25925;&#38556;&#33410;&#28857;&#20256;&#25773;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2311.16522</link><description>&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#20013;&#21160;&#24577;&#25925;&#38556;&#29305;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20934;&#30830;&#21487;&#38752;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25925;&#38556;&#33410;&#28857;&#20256;&#25773;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#24378;&#36816;&#32500;&#30340;&#26234;&#33021;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#30693;&#35782;&#22270;&#35889;&#26469;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#12290;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#20026;&#20102;&#39564;&#35777;&#33410;&#28857;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#36827;&#34892;&#20102;&#27599;&#20010;&#33410;&#28857;&#36755;&#20986;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20223;&#30495;&#22330;&#26223;&#20013;&#20934;&#30830;&#22320;&#23450;&#20301;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#24314;&#27169;&#21487;&#20197;&#23450;&#24615;&#22320;&#32771;&#23519;&#25925;&#38556;&#22914;&#20309;&#22312;&#33410;&#28857;&#38388;&#20256;&#25773;&#65292;&#20026;&#20998;&#26512;&#25925;&#38556;&#33410;&#28857;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance the intelligence degree in operation and maintenance, a novel method for fault detection in power grids is proposed. The proposed GNN-based approach first identifies fault nodes through a specialized feature extraction method coupled with a knowledge graph. By incorporating temporal data, the method leverages the status of nodes from preceding and subsequent time periods to help current fault detection. To validate the effectiveness of the node features, a correlation analysis of the output features from each node was conducted. The results from experiments show that this method can accurately locate fault nodes in simulation scenarios with a remarkable accuracy. Additionally, the graph neural network based feature modeling allows for a qualitative examination of how faults spread across nodes, which provides valuable insights for analyzing fault nodes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#20889;&#20316;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24212;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#26816;&#27979;&#21040;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#20889;&#20316;&#20043;&#38388;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2311.15565</link><description>&lt;p&gt;
&#35780;&#20272;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text. (arXiv:2311.15565v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#20889;&#20316;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24212;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#26816;&#27979;&#21040;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#20889;&#20316;&#20043;&#38388;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#30340;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26469;&#20934;&#30830;&#21306;&#20998;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#20889;&#20316;&#12290;&#25105;&#24212;&#29992;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#26041;&#27861;&#35770;&#65292;&#21033;&#29992;&#20102;&#19968;&#20010;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;AI&#21644;&#20154;&#31867;&#25991;&#26412;&#65292;&#27599;&#20010;&#25991;&#26412;&#37117;&#26631;&#26377;&#25351;&#31034;&#12290;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20415;&#20110;&#23545;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#32467;&#21512;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20010;&#23450;&#21046;&#27169;&#22411;&#20351;&#24471;&#23427;&#33021;&#22815;&#26816;&#27979;&#20986;AI&#21644;&#20154;&#31867;&#20869;&#23481;&#20043;&#38388;&#24494;&#22937;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
My research investigates the use of cutting-edge hybrid deep learning models to accurately differentiate between AI-generated text and human writing. I applied a robust methodology, utilising a carefully selected dataset comprising AI and human texts from various sources, each tagged with instructions. Advanced natural language processing techniques facilitated the analysis of textual features. Combining sophisticated neural networks, the custom model enabled it to detect nuanced differences between AI and human content.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21464;&#30005;&#31449;&#21160;&#24577;&#25925;&#38556;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#36890;&#36807;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#65292;&#21033;&#29992;Neo4j&#22270;&#25968;&#25454;&#24211;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#23454;&#29616;&#23545;&#21464;&#30005;&#31449;&#20013;&#38544;&#34255;&#21361;&#38505;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2311.13708</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21464;&#30005;&#31449;&#21160;&#24577;&#25925;&#38556;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Dynamic Fault Analysis in Substations Based on Knowledge Graphs. (arXiv:2311.13708v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13708
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21464;&#30005;&#31449;&#21160;&#24577;&#25925;&#38556;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#36890;&#36807;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#65292;&#21033;&#29992;Neo4j&#22270;&#25968;&#25454;&#24211;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#23454;&#29616;&#23545;&#21464;&#30005;&#31449;&#20013;&#38544;&#34255;&#21361;&#38505;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#35782;&#21035;&#21464;&#30005;&#31449;&#38544;&#34255;&#21361;&#38505;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#20998;&#26512;&#26041;&#27861;&#12290;&#39318;&#20808;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#28982;&#21518;&#21033;&#29992;&#22522;&#20110;Elastic-Search&#26500;&#24314;&#30340;&#28789;&#27963;&#20998;&#24067;&#24335;&#25628;&#32034;&#24341;&#25806;&#22788;&#29702;&#25968;&#25454;&#12290;&#25509;&#19979;&#26469;&#65292;&#20351;&#29992;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#26469;&#35757;&#32451;&#24341;&#25806;&#20013;&#30340;&#25968;&#25454;&#12290;&#32500;&#29305;&#27604;&#31639;&#27861;&#34987;&#25972;&#21512;&#36827;&#26469;&#35299;&#23494;&#38544;&#34255;&#29366;&#24577;&#24207;&#21015;&#65292;&#20415;&#20110;&#23545;&#19982;&#38544;&#34255;&#21361;&#38505;&#30456;&#20851;&#30340;&#23454;&#20307;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#27880;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;Neo4j&#22270;&#25968;&#25454;&#24211;&#21160;&#24577;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#26469;&#21487;&#35270;&#21270;&#21464;&#30005;&#31449;&#20013;&#30340;&#38544;&#34255;&#21361;&#38505;&#12290;&#36890;&#36807;&#23545;&#25991;&#26412;&#35760;&#24405;&#20013;&#25581;&#31034;&#30340;&#20855;&#20307;&#21464;&#30005;&#31449;&#30340;&#38544;&#34255;&#21361;&#38505;&#36827;&#34892;&#26696;&#20363;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the challenge of identifying hidden danger in substations from unstructured text, a novel dynamic analysis method is proposed. We first extract relevant information from the unstructured text, and then leverages a flexible distributed search engine built on Elastic-Search to handle the data. Following this, the hidden Markov model is employed to train the data within the engine. The Viterbi algorithm is integrated to decipher the hidden state sequences, facilitating the segmentation and labeling of entities related to hidden dangers. The final step involves using the Neo4j graph database to dynamically create a knowledge graph that visualizes hidden dangers in the substation. The effectiveness of the proposed method is demonstrated through a case analysis from a specific substation with hidden dangers revealed in the text records.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#23454;&#29616;&#33258;&#25105;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#21644;&#21512;&#25104;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#21644;&#23384;&#20648;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#31232;&#30095;&#27880;&#37322;&#21644;&#26377;&#38480;&#30340;&#35774;&#22791;&#23384;&#20648;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2311.12275</link><description>&lt;p&gt;
&#22312;&#35774;&#22791;&#19978;&#23454;&#29616;&#33258;&#25105;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#21644;&#21512;&#25104;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enabling On-Device Large Language Model Personalization with Self-Supervised Data Selection and Synthesis. (arXiv:2311.12275v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#23454;&#29616;&#33258;&#25105;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#21644;&#21512;&#25104;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#21644;&#23384;&#20648;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#31232;&#30095;&#27880;&#37322;&#21644;&#26377;&#38480;&#30340;&#35774;&#22791;&#23384;&#20648;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#21518;&#65292;&#24076;&#26395;&#36825;&#20123;&#35774;&#22791;&#33021;&#20174;&#29992;&#25143;&#29983;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#20197;&#23454;&#26102;&#29983;&#25104;&#38024;&#23545;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#25935;&#24863;&#21644;&#31169;&#23494;&#20449;&#24687;&#65292;&#32780;&#23558;&#27492;&#31867;&#25968;&#25454;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#27880;&#37322;&#24182;&#19981;&#34987;&#25512;&#33616;&#65292;&#29978;&#33267;&#26159;&#31105;&#27490;&#30340;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#35810;&#38382;&#29992;&#25143;&#25552;&#20379;&#39318;&#36873;&#22238;&#24212;&#26469;&#22312;&#26412;&#22320;&#33719;&#21462;&#27880;&#37322;&#65292;&#20294;&#36825;&#31181;&#27880;&#37322;&#24517;&#39035;&#31232;&#30095;&#20197;&#19981;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#12290;&#27492;&#22806;&#65292;&#36793;&#32536;&#35774;&#22791;&#30340;&#23384;&#20648;&#36890;&#24120;&#22826;&#26377;&#38480;&#65292;&#26080;&#27861;&#36827;&#34892;&#20840;&#38754;&#30340;&#22823;&#35268;&#27169;&#24494;&#35843;&#12290;&#22914;&#20309;&#22312;&#32771;&#34385;&#31232;&#30095;&#27880;&#37322;&#21644;&#21463;&#38480;&#30340;&#35774;&#22791;&#23384;&#20648;&#26465;&#20214;&#19979;&#23454;&#29616;&#22312;&#35774;&#22791;&#19978;&#30340;LLM&#20010;&#24615;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#22312;&#32447;&#36873;&#25321;&#21644;&#23384;&#20648;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#25968;&#25454;&#12290;&#36825;&#31181;&#25968;&#25454;&#20855;&#26377;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#24182;&#20801;&#35768;&#24456;&#23569;&#30340;&#23384;&#20648;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
After a large language model (LLM) is deployed on edge devices, it is desirable for these devices to learn from user-generated conversation data to generate user-specific and personalized responses in real-time. However, user-generated data usually contains sensitive and private information, and uploading such data to the cloud for annotation is not preferred if not prohibited. While it is possible to obtain annotation locally by directly asking users to provide preferred responses, such annotations have to be sparse to not affect user experience. In addition, the storage of edge devices is usually too limited to enable large-scale fine-tuning with full user-generated data. It remains an open question how to enable on-device LLM personalization, considering sparse annotation and limited on-device storage. In this paper, we propose a novel framework to select and store the most representative data online in a self-supervised way. Such data has a small memory footprint and allows infrequ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#26816;&#27979;&#23459;&#20256;&#24615;&#25991;&#26412;&#36328;&#24230;&#30340;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#35813;&#27169;&#22411;&#25910;&#38598;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#26631;&#27880;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.09812</link><description>&lt;p&gt;
&#29992;&#20110;&#23459;&#20256;&#24615;&#36328;&#24230;&#27880;&#37322;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Propaganda Span Annotation. (arXiv:2311.09812v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#26816;&#27979;&#23459;&#20256;&#24615;&#25991;&#26412;&#36328;&#24230;&#30340;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#35813;&#27169;&#22411;&#25910;&#38598;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#26631;&#27880;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#32447;&#20869;&#23481;&#20013;&#20351;&#29992;&#23459;&#20256;&#25163;&#27861;&#30340;&#24773;&#20917;&#26377;&#25152;&#22686;&#21152;&#65292;&#26088;&#22312;&#25805;&#32437;&#22312;&#32447;&#21463;&#20247;&#12290;&#38024;&#23545;&#21508;&#31181;&#24314;&#27169;&#22330;&#26223;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#33258;&#21160;&#26816;&#27979;&#21644;&#25581;&#38706;&#27492;&#31867;&#20869;&#23481;&#30340;&#21162;&#21147;&#12290;&#36825;&#20123;&#22330;&#26223;&#21253;&#25324;&#30830;&#23450;&#20869;&#23481;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#22810;&#27169;&#24577;&#65289;&#26159;&#21542;&#20855;&#26377;&#20197;&#19979;&#29305;&#24449;&#65306;&#65288;i&#65289;&#20855;&#26377;&#23459;&#20256;&#24615;&#65292;&#65288;ii&#65289;&#20351;&#29992;&#19968;&#31181;&#25110;&#22810;&#31181;&#23459;&#20256;&#25163;&#27861;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#21253;&#21547;&#20855;&#26377;&#21487;&#35782;&#21035;&#33539;&#22260;&#30340;&#25216;&#24039;&#12290;&#19982;&#21069;&#20004;&#31181;&#22330;&#26223;&#30456;&#27604;&#65292;&#24050;&#32463;&#23545;&#21518;&#19968;&#31181;&#22330;&#26223;&#36827;&#34892;&#20102;&#36739;&#22823;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#26816;&#27979;&#23459;&#20256;&#24615;&#25991;&#26412;&#36328;&#24230;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#26159;&#21542;&#33021;&#22815;&#26377;&#25928;&#25191;&#34892;&#35813;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#35813;&#27169;&#22411;&#25910;&#38598;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#26631;&#27880;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#20102;&#19968;&#22871;&#22823;&#35268;&#27169;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;&#20855;&#26377;&#19981;&#21516;&#19987;&#19994;&#27700;&#24179;&#30340;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#27880;&#37322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
The use of propagandistic techniques in online contents has increased in recent years aiming to manipulate online audiences. Efforts to automatically detect and debunk such content have been made addressing various modeling scenarios. These include determining whether the content (text, image, or multimodal) (i) is propagandistic, (ii) employs one or more propagandistic techniques, and (iii) includes techniques with identifiable spans. Significant research efforts have been devoted to the first two scenarios compared to the latter. Therefore, in this study, we focus on the task of detecting propagandistic textual spans. Specifically, we investigate whether large language models (LLMs), such as GPT-4, can effectively perform the task. Moreover, we study the potential of employing the model to collect more cost-effective annotations. Our experiments use a large-scale in-house dataset consisting of annotations from human annotators with varying expertise levels. The results suggest that p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#20307;&#29305;&#24449;&#65292;&#22312;&#20998;&#37197;&#32593;&#32476;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20998;&#37197;&#25991;&#26412;&#20013;&#36827;&#34892;&#21305;&#37197;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#30005;&#21147;&#20998;&#37197;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#20013;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.08724</link><description>&lt;p&gt;
&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Construction in Power Distribution Networks. (arXiv:2311.08724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#20307;&#29305;&#24449;&#65292;&#22312;&#20998;&#37197;&#32593;&#32476;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20998;&#37197;&#25991;&#26412;&#20013;&#36827;&#34892;&#21305;&#37197;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#30005;&#21147;&#20998;&#37197;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#20013;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#20307;&#29305;&#24449;&#65292;&#21253;&#25324;&#20854;&#35821;&#20041;&#12289;&#38899;&#38901;&#21644;&#21477;&#27861;&#29305;&#24449;&#65292;&#22312;&#20998;&#37197;&#32593;&#32476;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20998;&#37197;&#25991;&#26412;&#20013;&#36827;&#34892;&#21305;&#37197;&#12290;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22686;&#24378;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#23558;&#20998;&#37197;&#25991;&#26412;&#23454;&#20307;&#19982;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#21305;&#37197;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#30005;&#21147;&#20998;&#37197;&#22330;&#26223;&#20013;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38142;&#25509;&#21508;&#31181;&#23454;&#20307;&#31867;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#30005;&#21147;&#20998;&#37197;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a method for knowledge graph construction in power distribution networks. This method leverages entity features, which involve their semantic, phonetic, and syntactic characteristics, in both the knowledge graph of distribution network and the dispatching texts. An enhanced model based on Convolutional Neural Network, is utilized for effectively matching dispatch text entities with those in the knowledge graph. The effectiveness of this model is evaluated through experiments in real-world power distribution dispatch scenarios. The results indicate that, compared with the baselines, the proposed model excels in linking a variety of entity types, demonstrating high overall accuracy in power distribution knowledge graph construction task.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.04131</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#23450;&#20301;&#36328;&#20219;&#21153;&#24207;&#21015;&#32487;&#32493;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04131
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#22797;&#26434;&#30340;&#26550;&#26500;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#23558;Transformer&#27169;&#22411;&#36824;&#21407;&#20026;&#21487;&#35835;&#30340;&#30005;&#36335;&#34920;&#31034;&#65292;&#29992;&#20110;&#23454;&#29616;&#31639;&#27861;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#26469;&#25193;&#23637;&#36825;&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#23383;&#12289;&#25968;&#23383;&#35789;&#21644;&#26376;&#20221;&#30340;&#36882;&#22686;&#24207;&#21015;&#12290;&#36890;&#36807;&#24212;&#29992;&#30005;&#36335;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36127;&#36131;&#26816;&#27979;&#24207;&#21015;&#25104;&#21592;&#21644;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#25104;&#21592;&#30340;&#20851;&#38190;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35821;&#20041;&#30456;&#20851;&#24207;&#21015;&#20381;&#36182;&#20110;&#20855;&#26377;&#31867;&#20284;&#20316;&#29992;&#30340;&#20849;&#20139;&#30005;&#36335;&#23376;&#22270;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35760;&#24405;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#65292;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#26356;&#23433;&#20840;&#30340;&#32534;&#36753;&#36807;&#31243;&#12290;&#36825;&#31181;&#23545;Transformer&#30340;&#26426;&#26800;&#29702;&#35299;&#26159;&#26500;&#24314;&#26356;&#20581;&#22766;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#26356;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#35843;&#26597;&#35774;&#35745;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#23637;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.04076</link><description>&lt;p&gt;
LLMs&#26159;&#21542;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#20506;&#65311;&#19968;&#39033;&#20851;&#20110;&#35843;&#26597;&#35774;&#35745;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do LLMs exhibit human-like response biases? A case study in survey design. (arXiv:2311.04076v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#35843;&#26597;&#35774;&#35745;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#23637;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22686;&#24378;&#65292;&#20154;&#20204;&#23545;&#23558;LLMs&#29992;&#20316;&#20195;&#29702;&#20154;&#31867;&#36827;&#34892;&#20027;&#35266;&#26631;&#31614;&#20219;&#21153;&#65288;&#22914;&#35843;&#26597;&#21644;&#33286;&#35770;&#35843;&#26597;&#65289;&#30340;&#21487;&#33021;&#24615;&#36234;&#26469;&#36234;&#20852;&#22859;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#25514;&#36766;&#30340;&#25935;&#24863;&#24615;&#26159;&#20854;&#24191;&#27867;&#24341;&#36848;&#30340;&#38480;&#21046;&#20043;&#19968;&#65292;&#20294;&#26377;&#36259;&#30340;&#26159;&#65292;&#20154;&#31867;&#22312;&#22238;&#24212;&#20013;&#20063;&#26174;&#31034;&#20986;&#23545;&#25351;&#20196;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#65292;&#34920;&#29616;&#20026;&#21453;&#24212;&#20559;&#20506;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#35201;&#20351;&#29992;LLMs&#36817;&#20284;&#20154;&#31867;&#24847;&#35265;&#65292;&#26377;&#24517;&#35201;&#35843;&#26597;LLMs&#26159;&#21542;&#20063;&#21453;&#26144;&#20102;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20197;&#35843;&#26597;&#35774;&#35745;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#35843;&#26597;&#38382;&#21367;&#20013;&#30001;&#20110;&#8220;&#25552;&#31034;&#8221;&#25514;&#36766;&#30340;&#21464;&#21270;&#23548;&#33268;&#30340;&#20154;&#31867;&#21453;&#24212;&#20559;&#24046;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#20511;&#37492;&#31038;&#20250;&#24515;&#29702;&#23398;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;LLMs&#26159;&#21542;&#22312;&#35843;&#26597;&#38382;&#21367;&#20013;&#23637;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs is their sensitivity to prompt wording - but interestingly, humans also display sensitivities to instruction changes in the form of response biases. As such, we argue that if LLMs are going to be used to approximate human opinions, it is necessary to investigate the extent to which LLMs also reflect human response biases, if at all. In this work, we use survey design as a case study, where human response biases caused by permutations in wordings of "prompts" have been extensively studied. Drawing from prior work in social psychology, we design a dataset and propose a framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Alympics&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20154;&#36827;&#34892;&#21338;&#24328;&#35770;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#27169;&#25311;&#26694;&#26550;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#25112;&#30053;&#20114;&#21160;&#65292;&#26694;&#26550;&#33021;&#22815;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#20998;&#26512;&#28216;&#25103;&#20915;&#23450;&#22240;&#32032;&#12289;&#31574;&#30053;&#21644;&#32467;&#26524;&#65292;&#24182;&#23545;&#20195;&#29702;&#20154;&#22312;&#25112;&#30053;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2311.03220</link><description>&lt;p&gt;
ALYMPICS&#65306;&#35821;&#35328;&#20195;&#29702;&#20154;&#19982;&#21338;&#24328;&#35770;&#30456;&#36935;&#8212;&#8212;&#29992;AI&#20195;&#29702;&#20154;&#25506;&#32034;&#25112;&#30053;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
ALYMPICS: Language Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents. (arXiv:2311.03220v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Alympics&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20154;&#36827;&#34892;&#21338;&#24328;&#35770;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#27169;&#25311;&#26694;&#26550;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#25112;&#30053;&#20114;&#21160;&#65292;&#26694;&#26550;&#33021;&#22815;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#20998;&#26512;&#28216;&#25103;&#20915;&#23450;&#22240;&#32032;&#12289;&#31574;&#30053;&#21644;&#32467;&#26524;&#65292;&#24182;&#23545;&#20195;&#29702;&#20154;&#22312;&#25112;&#30053;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Alympics&#65288;&#20195;&#29702;&#20154;&#30340;&#22885;&#36816;&#20250;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#20154;&#36827;&#34892;&#21338;&#24328;&#35770;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#27169;&#25311;&#26694;&#26550;&#12290;Alympics&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#21338;&#24328;&#35770;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#25511;&#21046;&#29615;&#22659;&#26469;&#27169;&#25311;&#19982;LLM&#20195;&#29702;&#20154;&#36827;&#34892;&#31867;&#20284;&#20154;&#31867;&#30340;&#25112;&#30053;&#20114;&#21160;&#65292;&#24357;&#21512;&#20102;&#29702;&#35770;&#21338;&#24328;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#25105;&#20204;&#30340;&#35797;&#28857;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#8220;&#27700;&#36164;&#28304;&#20998;&#37197;&#25361;&#25112;&#8221;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20851;&#27880;&#31232;&#32570;&#29983;&#23384;&#36164;&#28304;&#22810;&#36718;&#25293;&#21334;&#30340;&#25361;&#25112;&#24615;&#25112;&#30053;&#28216;&#25103;&#26469;&#25506;&#32034;Alympics&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#28216;&#25103;&#20915;&#23450;&#22240;&#32032;&#12289;&#31574;&#30053;&#21644;&#32467;&#26524;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#21644;&#23545;LLM&#20195;&#29702;&#20154;&#22312;&#25112;&#30053;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#28145;&#20837;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19981;&#20165;&#25193;&#23637;&#20102;&#23545;LLM&#20195;&#29702;&#20154;&#27169;&#25311;&#20154;&#31867;&#25112;&#30053;&#34892;&#20026;&#33021;&#21147;&#30340;&#29702;&#35299;&#65292;&#36824;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Alympics (Olympics for Agents), a systematic simulation framework utilizing Large Language Model (LLM) agents for game theory research. Alympics creates a versatile platform for studying complex game theory problems, bridging the gap between theoretical game theory and empirical investigations by providing a controlled environment for simulating human-like strategic interactions with LLM agents. In our pilot case study, the "Water Allocation Challenge," we explore Alympics through a challenging strategic game focused on the multi-round auction on scarce survival resources. This study demonstrates the framework's ability to qualitatively and quantitatively analyze game determinants, strategies, and outcomes. Additionally, we conduct a comprehensive human assessment and an in-depth evaluation of LLM agents in strategic decision-making scenarios. Our findings not only expand the understanding of LLM agents' proficiency in emulating human strategic behavior but also h
&lt;/p&gt;</description></item><item><title>&#36817;&#26399;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20135;&#29983;&#20102;&#19968;&#31181;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#25490;&#21517;&#36739;&#39640;&#12290;&#36825;&#31181;&#8220;&#26469;&#28304;&#20559;&#35265;&#8221;&#21487;&#33021;&#23545;&#20449;&#24687;&#35775;&#38382;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.20501</link><description>&lt;p&gt;
LLM&#21487;&#33021;&#20027;&#23548;&#20449;&#24687;&#35775;&#38382;&#65306;&#31070;&#32463;&#26816;&#32034;&#22120;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts. (arXiv:2310.20501v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20501
&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20135;&#29983;&#20102;&#19968;&#31181;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#25490;&#21517;&#36739;&#39640;&#12290;&#36825;&#31181;&#8220;&#26469;&#28304;&#20559;&#35265;&#8221;&#21487;&#33021;&#23545;&#20449;&#24687;&#35775;&#38382;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#32593;&#32476;&#25628;&#32034;&#26041;&#38754;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33539;&#24335;&#12290;&#30001;&#20110;&#20854;&#22312;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;LLMs&#22312;&#20114;&#32852;&#32593;&#19978;&#21019;&#36896;&#20102;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#22240;&#27492;&#65292;LLMs&#26102;&#20195;&#30340;IR&#31995;&#32479;&#38754;&#20020;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65306;&#32034;&#24341;&#30340;&#25991;&#26723;&#19981;&#20165;&#26159;&#30001;&#20154;&#31867;&#25776;&#20889;&#30340;&#65292;&#32780;&#19988;&#36824;&#21253;&#25324;&#30001;LLMs&#33258;&#21160;&#29983;&#25104;&#30340;&#25991;&#26723;&#12290;&#36825;&#20123;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#22914;&#20309;&#24433;&#21709;IR&#31995;&#32479;&#26159;&#19968;&#20010;&#32039;&#36843;&#19988;&#23578;&#26410;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#28041;&#21450;&#20154;&#31867;&#32534;&#20889;&#21644;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#19981;&#21516;IR&#27169;&#22411;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#20542;&#21521;&#20110;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#25490;&#21517;&#36739;&#39640;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#23545;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#20559;&#35265;&#31216;&#20026;&#8220;&#26469;&#28304;&#20559;&#35265;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20559;&#35265;&#19981;&#20165;&#38480;&#20110;f&#26041;&#30456;&#24403;&#30340;&#24773;&#20917;&#65292;&#32780;&#19988;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#20063;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search. With their remarkable capabilities in generating human-like texts, LLMs have created enormous texts on the Internet. As a result, IR systems in the LLMs era are facing a new challenge: the indexed documents now are not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of different IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher. We refer to this category of biases in neural retrieval models towards the LLM-generated text as the \textbf{source bias}. Moreover, we discover that this bias is not confined to the f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;PiNMT&#27169;&#22411;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#37096;&#20998;&#21644;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;IW&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19680</link><description>&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;
&lt;/p&gt;
&lt;p&gt;
Integrating Pre-trained Language Model into Neural Machine Translation. (arXiv:2310.19680v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19680
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;PiNMT&#27169;&#22411;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#37096;&#20998;&#21644;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;IW&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#39640;&#36136;&#37327;&#30340;&#21452;&#35821;&#35821;&#35328;&#23545;&#25968;&#25454;&#30340;&#19981;&#36275;&#20173;&#28982;&#26159;&#25552;&#39640;NMT&#24615;&#33021;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19968;&#30452;&#22312;&#25506;&#32034;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;PLM&#21644;NMT&#27169;&#22411;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PLM&#25972;&#21512;&#30340;NMT&#65288;PiNMT&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;PiNMT&#27169;&#22411;&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65292;&#20998;&#21035;&#26159;PLM&#22810;&#23618;&#36716;&#25442;&#22120;&#65292;&#23884;&#20837;&#34701;&#21512;&#21644;&#20313;&#24358;&#23545;&#40784;&#65292;&#27599;&#20010;&#37096;&#20998;&#22312;&#21521;NMT&#25552;&#20379;&#26377;&#25928;&#30340;PLM&#20449;&#24687;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;&#20998;&#31163;&#23398;&#20064;&#29575;&#21644;&#21452;&#27493;&#35757;&#32451;&#12290;&#36890;&#36807;&#23454;&#26045;&#25152;&#25552;&#20986;&#30340;PiNMT&#27169;&#22411;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;IW&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation (NMT) has become a significant technology in natural language processing through extensive research and development. However, the deficiency of high-quality bilingual language pair data still poses a major challenge to improving NMT performance. Recent studies have been exploring the use of contextual information from pre-trained language model (PLM) to address this problem. Yet, the issue of incompatibility between PLM and NMT model remains unresolved. This study proposes PLM-integrated NMT (PiNMT) model to overcome the identified problems. PiNMT model consists of three critical components, PLM Multi Layer Converter, Embedding Fusion, and Cosine Alignment, each playing a vital role in providing effective PLM information to NMT. Furthermore, two training strategies, Separate Learning Rates and Dual Step Training, are also introduced in this paper. By implementing the proposed PiNMT model and training strategy, we achieve state-of-the-art performance on the IW
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#35835;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#24320;&#28304;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CXR-LLaVA&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23558;&#22270;&#20687;&#19982;&#25918;&#23556;&#23398;&#24322;&#24120;&#23545;&#40784;&#65292;&#24182;&#20351;&#29992;GPT-4&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#22238;&#31572;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18341</link><description>&lt;p&gt;
CXR-LLaVA&#65306;&#29992;&#20110;&#35299;&#37322;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray Images. (arXiv:2310.18341v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#35835;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#24320;&#28304;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CXR-LLaVA&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23558;&#22270;&#20687;&#19982;&#25918;&#23556;&#23398;&#24322;&#24120;&#23545;&#40784;&#65292;&#24182;&#20351;&#29992;GPT-4&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#22238;&#31572;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#20197;&#22810;&#27169;&#24577;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#65292;&#21487;&#33021;&#22797;&#21046;&#20154;&#31867;&#25918;&#23556;&#31185;&#21307;&#24072;&#23545;&#22270;&#20687;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#29992;&#20110;&#35299;&#37322;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#24320;&#28304;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CXR-LLaVA&#65289;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#27169;&#22411;&#21442;&#25968;&#65288;&#22914;&#28201;&#24230;&#21644;&#26680;&#24515;&#37319;&#26679;&#65289;&#30340;&#24433;&#21709;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#25105;&#20204;&#25910;&#38598;&#20102;659,287&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65306;417,336&#20010;&#22270;&#20687;&#24102;&#26377;&#26576;&#20123;&#25918;&#23556;&#23398;&#24322;&#24120;&#26631;&#31614;&#65288;&#25968;&#25454;&#38598;1&#65289;&#65307;241,951&#20010;&#22270;&#20687;&#24102;&#26377;&#33258;&#30001;&#25991;&#26412;&#25918;&#23556;&#23398;&#25253;&#21578;&#65288;&#25968;&#25454;&#38598;2&#65289;&#12290;&#22312;&#39044;&#35757;&#32451;Resnet50&#20316;&#20026;&#22270;&#20687;&#32534;&#30721;&#22120;&#20043;&#21518;&#65292;&#37319;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26469;&#23545;&#40784;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#25918;&#23556;&#23398;&#24322;&#24120;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#25968;&#25454;&#38598;2&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Meta AI-2&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#32463;&#36807;GPT-4&#30340;&#25913;&#36827;&#65292;&#29983;&#25104;&#21508;&#31181;&#38382;&#39064;&#22238;&#31572;&#24773;&#26223;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;ht&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
Purpose: Recent advancements in large language models (LLMs) have expanded their capabilities in a multimodal fashion, potentially replicating the image interpretation of human radiologists. This study aimed to develop open-source multimodal large language model for interpreting chest X-ray images (CXR-LLaVA). We also examined the effect of prompt engineering and model parameters such as temperature and nucleus sampling.  Materials and Methods: For training, we collected 659,287 publicly available CXRs: 417,336 CXRs had labels for certain radiographic abnormalities (dataset 1); 241,951 CXRs provided free-text radiology reports (dataset 2). After pre-training the Resnet50 as an image encoder, the contrastive language-image pre-training was used to align CXRs and corresponding radiographic abnormalities. Then, the Large Language Model Meta AI-2 was fine-tuned using dataset 2, which were refined using GPT-4, with generating various question answering scenarios. The code can be found at ht
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#33258;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#30446;&#21069;&#30340;&#27169;&#22411;&#22312;&#33258;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.14053</link><description>&lt;p&gt;
&#36229;&#36234;&#20934;&#30830;&#24615;&#65306;&#29992;IdentityChain&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain. (arXiv:2310.14053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#33258;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#30446;&#21069;&#30340;&#27169;&#22411;&#22312;&#33258;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;(Code LLMs)&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#22240;&#27492;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#20934;&#30830;&#24615;&#35780;&#20272;&#26041;&#27861;&#35780;&#20272;Code LLMs&#22312;&#19968;&#31995;&#21015;&#29420;&#31435;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20294;&#24573;&#35270;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#33258;&#19968;&#33268;&#24615;&#12290;&#30452;&#35266;&#26469;&#35762;&#65292;&#19968;&#20010;&#21487;&#20449;&#36182;&#30340;&#27169;&#22411;&#22312;&#20026;&#20854;&#33258;&#36523;&#30340;&#20195;&#30721;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20197;&#21450;&#20026;&#20854;&#33258;&#36523;&#30340;&#35268;&#33539;&#29983;&#25104;&#20195;&#30721;&#26102;&#24212;&#35813;&#26159;&#33258;&#19968;&#33268;&#30340;&#12290;&#26410;&#33021;&#20445;&#25345;&#33258;&#19968;&#33268;&#24615;&#25581;&#31034;&#20102;&#23545;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20849;&#20139;&#35821;&#20041;&#30340;&#29702;&#35299;&#30340;&#19981;&#36275;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#39318;&#20808;&#27491;&#24335;&#23450;&#20041;&#20102;Code LLMs&#30340;&#33258;&#19968;&#33268;&#24615;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;IdentityChain&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#33258;&#19968;&#33268;&#24615;&#21644;&#20256;&#32479;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;11&#20010;Code LLMs&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#26410;&#33021;&#20445;&#25345;&#33258;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indee
&lt;/p&gt;</description></item><item><title>MolCA&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#23454;&#29616;&#20998;&#23376;&#22270;&#21644;&#35821;&#35328;&#30340;&#24314;&#27169;&#31995;&#32479;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#36830;&#25509;&#22270;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#24418;&#30340;&#20998;&#23376;&#20869;&#23481;&#65292;&#24182;&#36890;&#36807;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#39640;&#25928;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.12798</link><description>&lt;p&gt;
MolCA: &#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#30340;&#20998;&#23376;&#22270;-&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. (arXiv:2310.12798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12798
&lt;/p&gt;
&lt;p&gt;
MolCA&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#23454;&#29616;&#20998;&#23376;&#22270;&#21644;&#35821;&#35328;&#30340;&#24314;&#27169;&#31995;&#32479;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#36830;&#25509;&#22270;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#24418;&#30340;&#20998;&#23376;&#20869;&#23481;&#65292;&#24182;&#36890;&#36807;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#39640;&#25928;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19982;&#25991;&#26412;&#30456;&#20851;&#30340;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#23545;&#20998;&#23376;&#30340;&#21331;&#36234;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#32570;&#20047;&#20154;&#31867;&#19987;&#19994;&#20154;&#21592;&#22312;&#29702;&#35299;&#20998;&#23376;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#20851;&#38190;&#33021;&#21147; - 2D&#22270;&#24418;&#24863;&#30693;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MolCA: &#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#36827;&#34892;&#20998;&#23376;&#22270;-&#35821;&#35328;&#24314;&#27169;&#12290;MolCA&#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#20351;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Galactica&#65289;&#33021;&#22815;&#29702;&#35299;&#22522;&#20110;&#25991;&#26412;&#21644;&#22270;&#24418;&#30340;&#20998;&#23376;&#20869;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36328;&#27169;&#24577;&#25237;&#24433;&#22120;&#34987;&#23454;&#29616;&#20026;&#19968;&#20010;Q-Former&#65292;&#36830;&#25509;&#19968;&#20010;&#22270;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#21644;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;MolCA&#20351;&#29992;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#65288;&#21363;LoRA&#65289;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#32534;&#30721;&#22120;&#32806;&#21512;&#19981;&#21516;&#65292;MolCA&#20445;&#30041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#22686;&#21152;&#20102;2D&#22270;&#24418;&#20449;&#24687;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have demonstrated impressive molecule understanding ability on various 1D text-related tasks. However, they inherently lack 2D graph perception - a critical ability of human professionals in comprehending molecules' topological structures. To bridge this gap, we propose MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and graph-based molecular contents via the cross-modal projector. Specifically, the cross-modal projector is implemented as a Q-Former to connect a graph encoder's representation space and an LM's text space. Further, MolCA employs a uni-modal adapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks. Unlike previous studies that couple an LM with a graph encoder via cross-modal contrastive learning, MolCA retains the LM's ability of open-ended text generation and augments it with 2D graph information. To showcase its effectivenes
&lt;/p&gt;</description></item><item><title>VeRA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#30340;&#38543;&#26426;&#30697;&#38453;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;GLUE&#21644;E2E&#22522;&#20934;&#27979;&#35797;&#12289;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20197;&#21450;&#25351;&#20196;&#35843;&#20248;&#20013;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11454</link><description>&lt;p&gt;
VeRA: &#22522;&#20110;&#21521;&#37327;&#30340;&#38543;&#26426;&#30697;&#38453;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
VeRA: Vector-based Random Matrix Adaptation. (arXiv:2310.11454v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11454
&lt;/p&gt;
&lt;p&gt;
VeRA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#30340;&#38543;&#26426;&#30697;&#38453;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;GLUE&#21644;E2E&#22522;&#20934;&#27979;&#35797;&#12289;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20197;&#21450;&#25351;&#20196;&#35843;&#20248;&#20013;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#20294;&#22312;&#25193;&#23637;&#21040;&#26356;&#22823;&#27169;&#22411;&#25110;&#37096;&#32626;&#22823;&#37327;&#29305;&#23450;&#29992;&#25143;&#25110;&#20219;&#21153;&#33258;&#36866;&#24212;&#27169;&#22411;&#26102;&#20173;&#28982;&#38754;&#20020;&#20005;&#37325;&#30340;&#23384;&#20648;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21521;&#37327;&#30340;&#38543;&#26426;&#30697;&#38453;&#33258;&#36866;&#24212;&#65288;VeRA&#65289;&#65292;&#19982;LoRA&#30456;&#27604;&#65292;&#23427;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#19968;&#23545;&#22312;&#25152;&#26377;&#23618;&#20043;&#38388;&#20849;&#20139;&#30340;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#23398;&#20064;&#23567;&#30340;&#32553;&#25918;&#21521;&#37327;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;GLUE&#21644;E2E&#22522;&#20934;&#27979;&#35797;&#12289;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;7B&#21644;13B&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SD-HuBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#25105;&#33976;&#39311;&#30446;&#26631;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#23398;&#20064;&#35821;&#38899;&#21477;&#23376;&#32423;&#34920;&#31034;&#26102;&#38899;&#33410;&#32452;&#32455;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#35821;&#38899;&#20013;&#21010;&#23450;&#26126;&#30830;&#30340;&#36793;&#30028;&#65292;&#24182;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#38899;&#33410;&#32467;&#26500;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#20219;&#21153;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#30340;&#21477;&#23376;&#32423;&#34920;&#31034;&#65292;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#26080;&#30417;&#30563;&#38899;&#33410;&#21457;&#29616;&#21644;&#23398;&#20064;&#21477;&#23376;&#32423;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.10803</link><description>&lt;p&gt;
SD-HuBERT: &#33258;&#25105;&#33976;&#39311;&#35825;&#23548;HuBERT&#20013;&#30340;&#38899;&#33410;&#32452;&#32455;
&lt;/p&gt;
&lt;p&gt;
SD-HuBERT: Self-Distillation Induces Syllabic Organization in HuBERT. (arXiv:2310.10803v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SD-HuBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#25105;&#33976;&#39311;&#30446;&#26631;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#23398;&#20064;&#35821;&#38899;&#21477;&#23376;&#32423;&#34920;&#31034;&#26102;&#38899;&#33410;&#32452;&#32455;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#35821;&#38899;&#20013;&#21010;&#23450;&#26126;&#30830;&#30340;&#36793;&#30028;&#65292;&#24182;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#38899;&#33410;&#32467;&#26500;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#20219;&#21153;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#30340;&#21477;&#23376;&#32423;&#34920;&#31034;&#65292;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#26080;&#30417;&#30563;&#38899;&#33410;&#21457;&#29616;&#21644;&#23398;&#20064;&#21477;&#23376;&#32423;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#21333;&#20803;&#21457;&#29616;&#24320;&#21551;&#20102;&#21475;&#35821;&#35821;&#35328;&#22788;&#29702;&#30340;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#30340;&#21333;&#20803;&#24448;&#24448;&#20173;&#22788;&#20110;&#38899;&#32032;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;SSL&#34920;&#31034;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23398;&#20064;&#35821;&#38899;&#30340;&#21477;&#23376;&#32423;&#34920;&#31034;&#26102;&#65292;&#38899;&#33410;&#32452;&#32455;&#30340;&#20986;&#29616;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#8220;&#33258;&#25105;&#33976;&#39311;&#8221;&#30446;&#26631;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;HuBERT&#65292;&#24182;&#21152;&#20837;&#19968;&#20010;&#27719;&#32858;&#26631;&#35760;&#26469;&#24635;&#32467;&#25972;&#20010;&#21477;&#23376;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#35821;&#38899;&#20013;&#21010;&#23450;&#20102;&#26126;&#30830;&#30340;&#36793;&#30028;&#65292;&#24182;&#19988;&#24103;&#38388;&#30340;&#34920;&#31034;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#38899;&#33410;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#20986;&#29616;&#30340;&#32467;&#26500;&#24456;&#22823;&#31243;&#24230;&#19978;&#19982;&#30495;&#23454;&#38899;&#33410;&#23545;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;Spoken Speech ABX&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#30340;&#21477;&#23376;&#32423;&#34920;&#31034;&#12290;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#38899;&#33410;&#21457;&#29616;&#21644;&#23398;&#20064;&#21477;&#23376;&#32423;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space, limiting the utility of SSL representations. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt "self-distillation" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames show salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#33258;&#25105;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#20855;&#26377;&#25512;&#26029;&#35821;&#38899;&#21457;&#38899;&#36816;&#21160;&#23398;&#30340;&#33021;&#21147;&#65292;&#24182;&#26174;&#31034;&#20986;&#36825;&#19968;&#23646;&#24615;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#20855;&#26377;&#37325;&#21472;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31616;&#21333;&#21464;&#25442;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#21516;&#35828;&#35805;&#32773;&#12289;&#24615;&#21035;&#12289;&#35821;&#35328;&#21644;&#26041;&#35328;&#20043;&#38388;&#36716;&#25442;&#65292;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#25299;&#23485;&#20102;&#25105;&#20204;&#23545;&#35821;&#38899;&#22788;&#29702;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.10788</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#25512;&#26029;&#20986;&#26222;&#36866;&#30340;&#21457;&#38899;&#36816;&#21160;&#23398;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Models of Speech Infer Universal Articulatory Kinematics. (arXiv:2310.10788v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#33258;&#25105;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#20855;&#26377;&#25512;&#26029;&#35821;&#38899;&#21457;&#38899;&#36816;&#21160;&#23398;&#30340;&#33021;&#21147;&#65292;&#24182;&#26174;&#31034;&#20986;&#36825;&#19968;&#23646;&#24615;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#20855;&#26377;&#37325;&#21472;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31616;&#21333;&#21464;&#25442;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#21516;&#35828;&#35805;&#32773;&#12289;&#24615;&#21035;&#12289;&#35821;&#35328;&#21644;&#26041;&#35328;&#20043;&#38388;&#36716;&#25442;&#65292;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#25299;&#23485;&#20102;&#25105;&#20204;&#23545;&#35821;&#38899;&#22788;&#29702;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#19968;&#30452;&#26159;&#40657;&#21283;&#23376;&#65292;&#20294;&#26368;&#36817;&#30340;&#35768;&#22810;&#30740;&#31350;&#24320;&#22987;&#23545;&#20687;HuBERT&#36825;&#26679;&#30340;&#27169;&#22411;&#36827;&#34892;"&#25506;&#27979;"&#65292;&#20197;&#23558;&#20854;&#20869;&#37096;&#34920;&#31034;&#19982;&#35821;&#38899;&#30340;&#19981;&#21516;&#26041;&#38754;&#30456;&#20851;&#32852;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#25105;&#30417;&#30563;&#27169;&#22411;&#30340;&#19968;&#20010;&#22522;&#26412;&#23646;&#24615;&#65292;&#21363;"&#25512;&#26029;&#21457;&#38899;&#36816;&#21160;&#23398;"&#65292;&#21363;&#36825;&#20123;&#27169;&#22411;&#23558;&#22768;&#23398;&#36716;&#21270;&#20026;&#35821;&#38899;&#20449;&#21495;&#24213;&#23618;&#30340;&#22240;&#26524;&#24615;&#21457;&#38899;&#21160;&#24577;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#26174;&#31034;&#20986;&#65292;&#35813;&#25277;&#35937;&#22312;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#25454;&#30340;&#35821;&#35328;&#20043;&#38388;&#26377;&#36739;&#22823;&#30340;&#37325;&#21472;&#65292;&#22312;&#31867;&#20284;&#38899;&#31995;&#30340;&#35821;&#35328;&#20013;&#26377;&#26356;&#39640;&#30340;&#20559;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26174;&#31034;&#20986;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#20223;&#23556;&#21464;&#25442;&#65292;&#22768;&#23398;&#21040;&#21457;&#38899;&#30340;&#36870;&#36716;&#25442;&#65288;AAI&#65289;&#22312;&#35828;&#35805;&#32773;&#20043;&#38388;&#20855;&#26377;&#21487;&#20256;&#36882;&#24615;&#65292;&#29978;&#33267;&#22312;&#24615;&#21035;&#12289;&#35821;&#35328;&#21644;&#26041;&#35328;&#20043;&#38388;&#20063;&#20855;&#26377;&#21487;&#20256;&#36882;&#24615;&#65292;&#26174;&#31034;&#20102;&#35813;&#23646;&#24615;&#30340;&#26222;&#36866;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#20123;&#32467;&#26524;&#20026;&#25105;&#20204;&#23545;&#35821;&#38899;&#22788;&#29702;&#30340;&#35748;&#35782;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning (SSL) based models of speech have shown remarkable performance on a range of downstream tasks. These state-of-the-art models have remained blackboxes, but many recent studies have begun "probing" models like HuBERT, to correlate their internal representations to different aspects of speech. In this paper, we show "inference of articulatory kinematics" as fundamental property of SSL models, i.e., the ability of these models to transform acoustics into the causal articulatory dynamics underlying the speech signal. We also show that this abstraction is largely overlapping across the language of the data used to train the model, with preference to the language with similar phonological system. Furthermore, we show that with simple affine transformations, Acoustic-to-Articulatory inversion (AAI) is transferrable across speakers, even across genders, languages, and dialects, showing the generalizability of this property. Together, these results shed new light on the 
&lt;/p&gt;</description></item><item><title>CP-KGC&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32422;&#26463;&#24335;&#25552;&#31034;&#26469;&#34917;&#20840;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08279</link><description>&lt;p&gt;
CP-KGC: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#24335;&#25552;&#31034;&#23545;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large Language Models. (arXiv:2310.08279v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08279
&lt;/p&gt;
&lt;p&gt;
CP-KGC&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32422;&#26463;&#24335;&#25552;&#31034;&#26469;&#34917;&#20840;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#25512;&#26029;&#21644;&#25512;&#27979;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#30340;&#36830;&#25509;&#12290;SimKGC&#31561;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#24050;&#32463;&#36229;&#36807;&#20102;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#30340;&#25928;&#26524;&#21462;&#20915;&#20110;&#23454;&#20307;&#25991;&#26412;&#25551;&#36848;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#20943;&#36731;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#24187;&#35273;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#20316;&#20026;&#19978;&#19979;&#25991;&#32422;&#26463;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#32422;&#26463;&#24335;&#25552;&#31034;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65288;CP-KGC&#65289;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#26377;&#25928;&#30340;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36229;&#36807;&#20102;WN18RR&#21644;FB15K237&#25968;&#25454;&#38598;&#19978;&#30340;&#20043;&#21069;&#32467;&#26524;&#12290;&#36825;&#23637;&#31034;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#30340;&#25972;&#21512;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce and infer missing connections within knowledge graphs. Text-based approaches, like SimKGC, have outperformed graph embedding methods, showcasing the promise of inductive KGC. However, the efficacy of text-based methods hinges on the quality of entity textual descriptions. In this paper, we identify the key issue of whether large language models (LLMs) can generate effective text. To mitigate hallucination in LLM-generated text in this paper, we introduce a constraint-based prompt that utilizes the entity and its textual description as contextual constraints to enhance data quality. Our Constrained-Prompt Knowledge Graph Completion (CP-KGC) method demonstrates effective inference under low resource computing conditions and surpasses prior results on the WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC tasks and provides new directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#30001;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20027;&#35266;&#24615;&#32780;&#24102;&#26469;&#30340;&#22256;&#38590;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05782</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Language Models with Human Preferences via a Bayesian Approach. (arXiv:2310.05782v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#30001;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20027;&#35266;&#24615;&#32780;&#24102;&#26469;&#30340;&#22256;&#38590;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#36827;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31995;&#32479;&#30340;&#36807;&#31243;&#20013;&#65292;&#30830;&#20445;NLG&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#23545;&#40784;&#65292;&#30446;&#21069;&#27969;&#34892;&#30340;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26469;&#33258;&#20154;&#31867;&#30340;&#21453;&#39304;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20027;&#35266;&#24615;&#32780;&#24102;&#26469;&#30340;&#22266;&#26377;&#20998;&#27495;&#23545;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;NLG&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#22810;&#25968;&#25237;&#31080;&#25110;&#24179;&#22343;&#20540;&#26469;&#23558;&#22810;&#20010;&#19981;&#19968;&#33268;&#30340;&#20559;&#22909;&#21512;&#24182;&#25104;&#19968;&#20010;&#21512;&#24182;&#30340;&#20559;&#22909;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#26131;&#20110;&#29702;&#35299;&#21644;&#25191;&#34892;&#65292;&#20294;&#26159;&#23427;&#20204;&#19981;&#33021;&#25429;&#25417;&#21040;&#20154;&#31867;&#20043;&#38388;&#32454;&#24494;&#30340;&#20998;&#27495;&#31243;&#24230;&#65292;&#24182;&#19988;&#21487;&#33021;&#21482;&#20195;&#34920;&#20102;&#20010;&#21035;&#29305;&#23450;&#20154;&#32676;&#65292;&#20174;&#32780;&#32570;&#20047;&#23450;&#37327;&#25259;&#38706;&#20154;&#31867;&#20559;&#22909;&#30340;&#26222;&#36866;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the quest to advance human-centric natural language generation (NLG) systems, ensuring alignment between NLG models and human preferences is crucial. For this alignment, current popular methods leverage a reinforcement learning (RL) approach with a reward model trained on feedback from humans. However, inherent disagreements due to the subjective nature of human preferences pose a significant challenge for training the reward model, resulting in a deterioration of the NLG performance. To tackle this issue, previous approaches typically rely on majority voting or averaging to consolidate multiple inconsistent preferences into a merged one. Although straightforward to understand and execute, such methods suffer from an inability to capture the nuanced degrees of disaggregation among humans and may only represent a specialized subset of individuals, thereby lacking the ability to quantitatively disclose the universality of human preferences. To address this challenge, this paper propos
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#25277;&#21462;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#21462;&#20102;&#20844;&#21496;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20013;&#30340;&#32467;&#26500;&#21270;ESG&#30456;&#20851;&#20449;&#24687;&#65292;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#31616;&#27905;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#21487;&#34892;&#21160;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.05628</link><description>&lt;p&gt;
&#20174;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20013;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#23548;&#20986;&#32467;&#26500;&#21270;&#35265;&#35299;&#65306;&#38378;&#20809;&#36824;&#26159;&#40644;&#37329;&#65311;
&lt;/p&gt;
&lt;p&gt;
Glitter or Gold? Deriving Structured Insights from Sustainability Reports via Large Language Models. (arXiv:2310.05628v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05628
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#25277;&#21462;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#21462;&#20102;&#20844;&#21496;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20013;&#30340;&#32467;&#26500;&#21270;ESG&#30456;&#20851;&#20449;&#24687;&#65292;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#31616;&#27905;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#21487;&#34892;&#21160;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#37492;&#20110;&#25237;&#36164;&#32773;&#23545;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;&#65288;ESG&#65289;&#38382;&#39064;&#36234;&#26469;&#36234;&#20851;&#27880;&#65292;&#19968;&#20123;&#30417;&#31649;&#26426;&#26500;&#24320;&#22987;&#35201;&#27714;&#19978;&#24066;&#20844;&#21496;&#25259;&#38706;&#38750;&#36130;&#21153;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#20197;&#21508;&#31181;&#38750;&#32467;&#26500;&#21270;&#30340;&#22810;&#27169;&#24577;&#25991;&#26723;&#24418;&#24335;&#20844;&#24320;&#21457;&#24067;&#12290;&#22240;&#27492;&#65292;&#23558;&#36825;&#20123;&#25968;&#25454;&#32858;&#21512;&#21644;&#25972;&#21512;&#21040;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#20013;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#23548;&#20986;&#36328;&#20844;&#21496;&#21644;&#24066;&#22330;&#30340;&#21487;&#25345;&#32493;&#24615;&#23454;&#36341;&#35265;&#35299;&#24182;&#19981;&#30452;&#35266;&#12290;&#37492;&#20110;&#36825;&#20123;&#21069;&#25552;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#37319;&#29992;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#25216;&#26415;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#31616;&#27905;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#21487;&#34892;&#21160;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31361;&#30772;&#20102;&#20256;&#32479;&#30340;&#25991;&#26412;&#22788;&#29702;&#25216;&#26415;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#32467;&#21512;&#31361;&#20986;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25216;&#26415;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#65292;&#20174;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20013;&#25552;&#21462;&#20855;&#26377;&#35821;&#20041;&#32467;&#26500;&#30340;&#19982;ESG&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, several regulatory bodies have started requiring the disclosure of non-financial information from publicly listed companies, in light of the investors' increasing attention to Environmental, Social, and Governance (ESG) issues. Such information is publicly released in a variety of non-structured and multi-modal documentation. Hence, it is not straightforward to aggregate and consolidate such data in a cohesive framework to further derive insights about sustainability practices across companies and markets. Given these premises, it is natural to resort to Information Extraction (IE) techniques to provide concise, informative, and actionable data to the stakeholders. Moving beyond traditional text processing techniques, in this work we leverage Large Language Models (LLMs), along with the prominent in-context learning technique and the Retrieved Augmented Generation (RAG) paradigm, to extract semantically structured ESG-related information from companies' sustainabi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22320;&#29702;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#26080;&#27861;&#38544;&#21547;&#23398;&#20064;&#24635;&#20307;&#25991;&#26412;&#20027;&#39064;&#12290;&#30456;&#27604;&#20110;&#25429;&#25417;&#22797;&#26434;&#38271;&#26399;&#20381;&#36182;&#24615;&#30340;&#32593;&#32476;&#65292;&#22312;&#32447;&#25968;&#25454;&#30340;&#27169;&#22411;&#32570;&#20047;&#32467;&#26500;&#65292;&#38656;&#35201;&#22312;&#32858;&#21512;&#20013;&#26816;&#27979;&#28508;&#22312;&#30340;&#32467;&#26500;&#12290;&#27492;&#30740;&#31350;&#20351;&#29992;&#20102;20&#20159;&#26465;&#25512;&#25991;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#22478;&#24066;&#30340;&#35789;&#34955;&#23884;&#20837;&#34920;&#31034;&#65292;&#21457;&#29616;&#22320;&#29702;&#20301;&#32622;&#23545;&#22312;&#32447;&#27807;&#36890;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.05378</link><description>&lt;p&gt;
&#36229;&#36234;&#27880;&#24847;&#21147;&#33539;&#24335;&#65306;&#20174;&#22320;&#29702;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Transcending the Attention Paradigm: Representation Learning from Geospatial Social Media Data. (arXiv:2310.05378v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22320;&#29702;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#26080;&#27861;&#38544;&#21547;&#23398;&#20064;&#24635;&#20307;&#25991;&#26412;&#20027;&#39064;&#12290;&#30456;&#27604;&#20110;&#25429;&#25417;&#22797;&#26434;&#38271;&#26399;&#20381;&#36182;&#24615;&#30340;&#32593;&#32476;&#65292;&#22312;&#32447;&#25968;&#25454;&#30340;&#27169;&#22411;&#32570;&#20047;&#32467;&#26500;&#65292;&#38656;&#35201;&#22312;&#32858;&#21512;&#20013;&#26816;&#27979;&#28508;&#22312;&#30340;&#32467;&#26500;&#12290;&#27492;&#30740;&#31350;&#20351;&#29992;&#20102;20&#20159;&#26465;&#25512;&#25991;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#22478;&#24066;&#30340;&#35789;&#34955;&#23884;&#20837;&#34920;&#31034;&#65292;&#21457;&#29616;&#22320;&#29702;&#20301;&#32622;&#23545;&#22312;&#32447;&#27807;&#36890;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;transformers&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#30340;&#22522;&#30707;&#24320;&#21019;&#20102;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#26550;&#26500;&#65292;&#20294;&#23427;&#20204;&#23545;&#26174;&#24335;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#20381;&#36182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#26263;&#31034;&#24615;&#23398;&#20064;&#24635;&#20307;&#25991;&#26412;&#20027;&#39064;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#30740;&#31350;&#25361;&#25112;&#20102;&#24615;&#33021;&#22522;&#20934;&#30340;&#21551;&#21457;&#24335;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20316;&#20026;&#20998;&#24067;&#24335;&#27169;&#24335;&#30340;&#28304;&#22836;&#36827;&#34892;&#30740;&#31350;&#12290;&#19982;&#20381;&#36182;&#20110;&#25429;&#25417;&#22797;&#26434;&#38271;&#26399;&#20381;&#36182;&#24615;&#30340;&#32593;&#32476;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#22312;&#32447;&#25968;&#25454;&#30340;&#27169;&#22411;&#26412;&#36136;&#19978;&#32570;&#20047;&#32467;&#26500;&#65292;&#34987;&#36843;&#22312;&#32858;&#21512;&#20013;&#26816;&#27979;&#28508;&#22312;&#30340;&#32467;&#26500;&#12290;&#20026;&#20102;&#27491;&#30830;&#22320;&#34920;&#31034;&#36825;&#20123;&#25277;&#35937;&#20851;&#31995;&#65292;&#26412;&#30740;&#31350;&#20998;&#35299;&#20102;&#32463;&#39564;&#31038;&#20132;&#23186;&#20307;&#35821;&#26009;&#24211;&#25104;&#20026;&#20854;&#20803;&#32032;&#32452;&#25104;&#37096;&#20998;&#65292;&#20998;&#26512;&#20102;&#36229;&#36807;20&#20159;&#26465;&#21457;&#34920;&#22312;&#20154;&#21475;&#23494;&#38598;&#22320;&#21306;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#38024;&#23545;&#27599;&#20010;&#22478;&#24066;&#30340;&#35789;&#34955;&#23884;&#20837;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#34920;&#31034;&#12290;&#21457;&#29616;&#21363;&#20351;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#65292;&#22320;&#29702;&#20301;&#32622;&#23545;&#22312;&#32447;&#27807;&#36890;&#26377;&#30528;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformers have pioneered attention-driven architectures as a cornerstone of language modeling, their dependence on explicitly contextual information underscores limitations in their abilities to tacitly learn overarching textual themes. This study challenges the heuristic paradigm of performance benchmarking by investigating social media data as a source of distributed patterns. In stark contrast to networks that rely on capturing complex long-term dependencies, models of online data inherently lack structure and are forced to detect latent structures in the aggregate. To properly represent these abstract relationships, this research dissects empirical social media corpora into their elemental components, analyzing over two billion tweets across population-dense locations. We create Bag-of-Word embedding specific to each city and compare their respective representations. This finds that even amidst noisy data, geographic location has a considerable influence on online communic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.00100</link><description>&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;--&#25688;&#35201;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65281;
&lt;/p&gt;
&lt;p&gt;
Multilingual Natural Language ProcessingModel for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21360;&#35937;&#37096;&#20998;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#22312;&#21521;&#21307;&#29983;&#20256;&#36798;&#36825;&#20123;&#21457;&#29616;&#26102;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#26469;&#35828;&#65292;&#20934;&#22791;&#36825;&#20123;&#25688;&#35201;&#26082;&#32791;&#26102;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#33021;&#22815;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#24635;&#32467;&#36825;&#20123;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#33258;&#21160;&#21270;&#22320;&#29983;&#25104;&#20102;&#19981;&#21516;&#35821;&#35328;&#30340;&#25918;&#23556;&#23398;&#21360;&#35937;&#65292;&#20197;&#24635;&#32467;&#33521;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#24503;&#35821;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#21457;&#29616;&#12290;&#22312;&#19968;&#39033;&#30450;&#27979;&#20013;&#65292;&#20004;&#20301;&#26377;&#25191;&#19994;&#36164;&#26684;&#30340;&#25918;&#23556;&#31185;&#21307;&#29983;&#34920;&#31034;&#65292;&#23545;&#20110;&#33267;&#23569;70%&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#20854;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
The impression section of a radiology report summarizes important radiology findings and plays a critical role in communicating these findings to physicians. However, the preparation of these summaries is time-consuming and error-prone for radiologists. Recently, numerous models for radiology report summarization have been developed. Nevertheless, there is currently no model that can summarize these reports in multiple languages. Such a model could greatly improve future research and the development of Deep Learning models that incorporate data from patients with different ethnic backgrounds. In this study, the generation of radiology impressions in different languages was automated by fine-tuning a model, publicly available, based on a multilingual text-to-text Transformer to summarize findings available in English, Portuguese, and German radiology reports. In a blind test, two board-certified radiologists indicated that for at least 70% of the system-generated summaries, the quality 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SFAVEL&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#12290;&#36825;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.16540</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Fact Verification by Language Model Distillation. (arXiv:2309.16540v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SFAVEL&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#12290;&#36825;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#26088;&#22312;&#36890;&#36807;&#21487;&#38752;&#30693;&#35782;&#24211;&#20013;&#30340;&#35777;&#25454;&#26469;&#39564;&#35777;&#20027;&#24352;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#31639;&#27861;&#24517;&#39035;&#20026;&#27599;&#20010;&#20027;&#24352;&#29983;&#25104;&#26082;&#35821;&#20041;&#26126;&#30830;&#21448;&#32039;&#20945;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#19982;&#28304;&#20449;&#24687;&#36827;&#34892;&#35821;&#20041;&#23545;&#40784;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#21069;&#32773;&#36890;&#36807;&#23398;&#20064;&#21253;&#21547;&#20027;&#24352;&#21450;&#20854;&#30456;&#24212;&#26631;&#31614;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#26469;&#35299;&#20915;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SFAVEL&#65288;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#33976;&#39311;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#32780;&#26080;&#38656;&#27880;&#37322;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#30340;&#65292;&#35813;&#20989;&#25968;&#40723;&#21169;&#29305;&#24449;&#22312;&#20445;&#25345;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;&#21644;&#35777;&#25454;&#23545;&#40784;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36798;&#21040;&#26032;&#39062;&#30340;&#29366;&#24577;&#19968;.
&lt;/p&gt;
&lt;p&gt;
Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#23545;&#20154;&#31867;&#21453;&#39304;&#22312;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#20351;&#29992;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20204;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#20851;&#38190;&#38169;&#35823;&#26631;&#20934;&#65292;&#32780;&#19988;&#23481;&#26131;&#21463;&#21040;&#20027;&#35266;&#20559;&#35265;&#21644;&#28151;&#26434;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.16349</link><description>&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#19981;&#26159;&#40644;&#37329;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Human Feedback is not Gold Standard. (arXiv:2309.16349v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16349
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#23545;&#20154;&#31867;&#21453;&#39304;&#22312;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#20351;&#29992;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20204;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#20851;&#38190;&#38169;&#35823;&#26631;&#20934;&#65292;&#32780;&#19988;&#23481;&#26131;&#21463;&#21040;&#20027;&#35266;&#20559;&#35265;&#21644;&#28151;&#26434;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24050;&#25104;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#24182;&#36234;&#26469;&#36234;&#34987;&#29992;&#20316;&#35757;&#32451;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#19981;&#28165;&#26970;&#36825;&#20010;&#21333;&#19968;&#30340;&#8220;&#20559;&#22909;&#8221;&#20998;&#25968;&#25429;&#25417;&#21040;&#29983;&#25104;&#36755;&#20986;&#30340;&#21738;&#20123;&#29305;&#24615;&#12290;&#25105;&#20204;&#20551;&#35774;&#20559;&#22909;&#20998;&#25968;&#26159;&#20027;&#35266;&#30340;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#19981;&#33391;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23545;&#20154;&#31867;&#21453;&#39304;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#20013;&#30340;&#20351;&#29992;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#20197;&#39564;&#35777;&#23427;&#26159;&#21542;&#23436;&#20840;&#25429;&#25417;&#21040;&#19968;&#31995;&#21015;&#20851;&#38190;&#38169;&#35823;&#26631;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#20559;&#22909;&#20998;&#25968;&#30340;&#35206;&#30422;&#33539;&#22260;&#30456;&#24403;&#22909;&#65292;&#20294;&#23427;&#20204;&#22312;&#20107;&#23454;&#24615;&#31561;&#37325;&#35201;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20551;&#35774;&#20559;&#22909;&#20998;&#25968;&#21644;&#38169;&#35823;&#27880;&#37322;&#21487;&#33021;&#21463;&#21040;&#28151;&#26434;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#24182;&#21033;&#29992;&#35843;&#35797;&#27169;&#22411;&#29983;&#25104;&#22312;&#20004;&#20010;&#21487;&#33021;&#30340;&#28151;&#26434;&#32500;&#24230;&#19978;&#21464;&#21270;&#30340;&#36755;&#20986;&#65306;&#22362;&#23450;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36755;&#20986;&#30340;&#22362;&#23450;&#24615;&#20250;&#20351;&#20107;&#23454;&#38169;&#35823;&#30340;&#24863;&#30693;&#29575;&#20559;&#24046;&#65292;&#34920;&#26126;&#20154;&#31867;&#27880;&#37322;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human feedback has become the de facto standard for evaluating the performance of Large Language Models, and is increasingly being used as a training objective. However, it is not clear which properties of a generated output this single `preference' score captures. We hypothesise that preference scores are subjective and open to undesirable biases. We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. We further hypothesise that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#36827;&#34892;&#35821;&#38899;&#36716;&#25442;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20462;&#25913;&#35821;&#38899;&#30340;&#38901;&#24459;&#21644;&#24773;&#24863;&#20449;&#24687;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#25351;&#20196;&#24182;&#20135;&#29983;&#21512;&#29702;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.14324</link><description>&lt;p&gt;
&#36890;&#29992;&#25991;&#26412;&#25351;&#23548;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards General-Purpose Text-Instruction-Guided Voice Conversion. (arXiv:2309.14324v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#36827;&#34892;&#35821;&#38899;&#36716;&#25442;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20462;&#25913;&#35821;&#38899;&#30340;&#38901;&#24459;&#21644;&#24773;&#24863;&#20449;&#24687;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#25351;&#20196;&#24182;&#20135;&#29983;&#21512;&#29702;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#22914;"&#24930;&#24930;&#28165;&#26224;&#22320;&#35828;&#35805;&#65292;&#22768;&#38899;&#20302;&#27785;"&#25110;"&#20197;&#24555;&#20048;&#30340;&#23569;&#24180;&#22768;&#38899;&#35828;&#35805;"&#26469;&#25351;&#23548;&#36716;&#25442;&#36807;&#31243;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#22686;&#21152;&#20102;&#35821;&#38899;&#36716;&#25442;&#30340;&#36890;&#29992;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;&#35813;&#25552;&#20986;&#30340;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#26159;&#19968;&#20010;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#19968;&#31995;&#21015;&#31163;&#25955;&#32534;&#30721;&#65292;&#24471;&#21040;&#36716;&#25442;&#21518;&#30340;&#35821;&#38899;&#32534;&#30721;&#24207;&#21015;&#12290;&#23427;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20316;&#20026;&#26679;&#24335;&#25552;&#31034;&#65292;&#20462;&#25913;&#32473;&#23450;&#35821;&#38899;&#30340;&#38901;&#24459;&#21644;&#24773;&#24863;&#20449;&#24687;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#22788;&#29702;&#35821;&#38899;&#30340;&#21508;&#31181;&#20449;&#24687;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#29420;&#31435;&#30340;&#32534;&#30721;&#22120;&#22788;&#29702;&#28304;&#35821;&#38899;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#38901;&#24459;&#21644;&#20869;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#29702;&#35299;&#25351;&#20196;&#24182;&#20135;&#29983;&#21512;&#29702;&#32467;&#26524;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel voice conversion (VC) model, guided by text instructions such as "articulate slowly with a deep tone" or "speak in a cheerful boyish voice". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.
&lt;/p&gt;</description></item><item><title>AMPLIFY&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#30340;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#20110;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12689</link><description>&lt;p&gt;
AMPLIFY: &#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26631;&#31614;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer. (arXiv:2309.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12689
&lt;/p&gt;
&lt;p&gt;
AMPLIFY&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#30340;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#20110;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#21407;&#22987;&#26679;&#26412;&#30340;&#32447;&#24615;&#32452;&#21512;&#29983;&#25104;&#26032;&#30340;&#22686;&#24378;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#21407;&#22987;&#26679;&#26412;&#20013;&#23384;&#22312;&#22122;&#38899;&#25110;&#24322;&#24120;&#29305;&#24449;&#65292;Mixup&#21487;&#33021;&#23558;&#20854;&#20256;&#25773;&#21040;&#22686;&#24378;&#26679;&#26412;&#20013;&#65292;&#23548;&#33268;&#27169;&#22411;&#23545;&#36825;&#20123;&#24322;&#24120;&#20540;&#36807;&#20110;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Mixup&#26041;&#27861;&#31216;&#20026;AMPLIFY&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Transformer&#33258;&#36523;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#26080;&#38656;&#22686;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#20302;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#24120;&#35265;Mixup&#26041;&#27861;&#65288;&#20363;&#22914;&#35821;&#21477;Mixup&#65289;&#20013;&#36164;&#28304;&#28040;&#32791;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26356;&#23567;&#30340;&#35745;&#31639;&#36164;&#28304;&#25104;&#26412;&#19979;&#65292;AMPLIFY&#22312;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;Mixup&#26041;&#27861;&#65292;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is an effective data augmentation method that generates new augmented samples by aggregating linear combinations of different original samples. However, if there are noises or aberrant features in the original samples, Mixup may propagate them to the augmented samples, leading to over-sensitivity of the model to these outliers . To solve this problem, this paper proposes a new Mixup method called AMPLIFY. This method uses the Attention mechanism of Transformer itself to reduce the influence of noises and aberrant values in the original samples on the prediction results, without increasing additional trainable parameters, and the computational cost is very low, thereby avoiding the problem of high resource consumption in common Mixup methods such as Sentence Mixup . The experimental results show that, under a smaller computational resource cost, AMPLIFY outperforms other Mixup methods in text classification tasks on 7 benchmark datasets, providing new ideas and new ways to further
&lt;/p&gt;</description></item><item><title>CB-Whisper&#26159;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#36827;&#35782;&#21035;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#23454;&#20307;&#21484;&#22238;&#29575;&#30340;&#21516;&#26102;&#20250;&#30053;&#24494;&#22686;&#21152;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.09552</link><description>&lt;p&gt;
CB-Whisper: &#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#32622;&#30340;Whisper
&lt;/p&gt;
&lt;p&gt;
CB-Whisper: Contextual Biasing Whisper using Open-Vocabulary Keyword-Spotting. (arXiv:2309.09552v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09552
&lt;/p&gt;
&lt;p&gt;
CB-Whisper&#26159;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#36827;&#35782;&#21035;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#23454;&#20307;&#21484;&#22238;&#29575;&#30340;&#21516;&#26102;&#20250;&#30053;&#24494;&#22686;&#21152;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#12289;&#32452;&#32455;&#26426;&#26500;&#21644;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#32463;&#24120;&#36935;&#21040;&#30340;&#19987;&#19994;&#26415;&#35821;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;Contextual Biasing Whisper&#65288;CB-Whisper&#65289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;Whisper&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#29366;&#24577;&#25191;&#34892;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#26469;&#35782;&#21035;&#29992;&#25143;&#23450;&#20041;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#35782;&#21035;&#20986;&#30340;&#23454;&#20307;&#34987;&#29992;&#20316;Whisper&#35299;&#30721;&#22120;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;OV-KWS&#21644;ASR&#20219;&#21153;&#36827;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#21407;&#22987;Whisper&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20013;&#22269;Aishell&#28909;&#35789;&#23376;&#38598;&#21644;&#20004;&#20010;&#20869;&#37096;&#20195;&#30721;&#20999;&#25442;&#27979;&#35797;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#23454;&#20307;&#21484;&#22238;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#20869;&#37096;&#27979;&#35797;&#38598;&#19978;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#30053;&#24494;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;Whisper&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end automatic speech recognition (ASR) systems often struggle to recognize rare name entities, such as personal names, organizations, and terminologies not frequently encountered in the training data. This paper presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on OpenAI's Whisper model that can recognize user-defined name entities by performing open-vocabulary keyword-spotting (OV-KWS) using the hidden states of Whisper encoder. The recognized entities are used as prompts for the Whisper decoder. We first propose a multitask training approach with OV-KWS and ASR tasks to optimize the model. Experiments show that this approach substantially improves the entity recalls compared to the original Whisper model on Chinese Aishell hot word subsets and two internal code-switch test sets. However, we observed a slight increase in mixed-error-rate (MER) on internal test sets due to catastrophic forgetting. To address this problem and use different sizes of the Wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#22833;&#35782;&#30151;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21644;&#32531;&#35299;&#30340;&#26694;&#26550;EMMA&#12290;&#36890;&#36807;&#31867;&#27604;&#31070;&#32463;&#24515;&#29702;&#23398;&#20013;&#30340;&#22833;&#35782;&#30151;&#29616;&#35937;&#65292;&#23450;&#20041;&#20102;MLLM&#20013;&#30340;&#22833;&#35782;&#30151;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#35780;&#20272;&#21644;&#27835;&#30103;&#26041;&#27861;&#12290;&#35780;&#20272;&#27169;&#22359;&#36890;&#36807;&#21019;&#24314;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#38382;&#31572;&#31034;&#20363;&#26469;&#35780;&#20272;&#22833;&#35782;&#30151;&#31243;&#24230;&#65292;&#27835;&#30103;&#27169;&#22359;&#21017;&#37319;&#29992;&#20462;&#27491;&#21644;&#22686;&#24378;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#21644;&#32416;&#27491;&#22833;&#35782;&#30151;&#12290;</title><link>http://arxiv.org/abs/2309.04041</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#32531;&#35299;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22833;&#35782;&#30151;
&lt;/p&gt;
&lt;p&gt;
Evaluation and Mitigation of Agnosia in Multimodal Large Language Models. (arXiv:2309.04041v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#22833;&#35782;&#30151;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21644;&#32531;&#35299;&#30340;&#26694;&#26550;EMMA&#12290;&#36890;&#36807;&#31867;&#27604;&#31070;&#32463;&#24515;&#29702;&#23398;&#20013;&#30340;&#22833;&#35782;&#30151;&#29616;&#35937;&#65292;&#23450;&#20041;&#20102;MLLM&#20013;&#30340;&#22833;&#35782;&#30151;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#35780;&#20272;&#21644;&#27835;&#30103;&#26041;&#27861;&#12290;&#35780;&#20272;&#27169;&#22359;&#36890;&#36807;&#21019;&#24314;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#38382;&#31572;&#31034;&#20363;&#26469;&#35780;&#20272;&#22833;&#35782;&#30151;&#31243;&#24230;&#65292;&#27835;&#30103;&#27169;&#22359;&#21017;&#37319;&#29992;&#20462;&#27491;&#21644;&#22686;&#24378;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#21644;&#32416;&#27491;&#22833;&#35782;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#20294;&#35266;&#23519;&#21040;&#23427;&#20204;&#26377;&#26102;&#20250;&#35823;&#35299;&#35270;&#35273;&#36755;&#20837;&#65292;&#29978;&#33267;&#22312;&#31616;&#21333;&#24773;&#20917;&#19979;&#26410;&#33021;&#36981;&#24490;&#25991;&#26412;&#25351;&#20196;&#65292;&#23548;&#33268;&#26080;&#20851;&#30340;&#22238;&#22797;&#12289;&#38169;&#35823;&#21644;&#26080;&#26681;&#25454;&#30340;&#20027;&#24352;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#35266;&#23519;&#31867;&#27604;&#20110;&#31070;&#32463;&#24515;&#29702;&#23398;&#20013;&#30340;&#19968;&#31181;&#29616;&#35937;&#65292;&#21363;&#22833;&#35782;&#30151;&#65292;&#21363;&#26080;&#27861;&#27491;&#30830;&#22788;&#29702;&#24863;&#35273;&#27169;&#24577;&#21644;&#35748;&#35782;&#20107;&#29289;&#65288;&#20363;&#22914;&#65292;&#23545;&#35937;&#12289;&#39068;&#33394;&#12289;&#20851;&#31995;&#65289;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#36825;&#19968;&#31867;&#20284;&#30340;&#27010;&#24565;&#26469;&#23450;&#20041;&#8220;MLLM&#20013;&#30340;&#22833;&#35782;&#30151;&#8221;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20840;&#38754;&#35780;&#20272;&#21644;&#32531;&#35299;MLLM&#20013;&#30340;&#22833;&#35782;&#30151;&#12290;&#21463;&#21040;&#31070;&#32463;&#24515;&#29702;&#23398;&#20013;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;EMMA&#65288;&#35780;&#20272;&#21644;&#32531;&#35299;&#22810;&#27169;&#24577;&#22833;&#35782;&#30151;&#65289;&#12290;&#22312;EMMA&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#27169;&#22359;&#65292;&#29992;&#20110;&#33258;&#21160;&#21019;&#24314;&#32454;&#31890;&#24230;&#21644;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#38382;&#31572;&#31034;&#20363;&#65292;&#20840;&#38754;&#35780;&#20272;MLLM&#20013;&#30340;&#22833;&#35782;&#30151;&#31243;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27835;&#30103;&#27169;&#22359;&#65292;&#20351;&#29992;&#20462;&#27491;&#21644;&#22686;&#24378;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#21644;&#32416;&#27491;MLLM&#20013;&#30340;&#22833;&#35782;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Multimodal Large Language Models (MLLMs) are widely used for a variety of vision-language tasks, one observation is that they sometimes misinterpret visual inputs or fail to follow textual instructions even in straightforward cases, leading to irrelevant responses, mistakes, and ungrounded claims. This observation is analogous to a phenomenon in neuropsychology known as Agnosia, an inability to correctly process sensory modalities and recognize things (e.g., objects, colors, relations). In our study, we adapt this similar concept to define "agnosia in MLLMs", and our goal is to comprehensively evaluate and mitigate such agnosia in MLLMs. Inspired by the diagnosis and treatment process in neuropsychology, we propose a novel framework EMMA (Evaluation and Mitigation of Multimodal Agnosia). In EMMA, we develop an evaluation module that automatically creates fine-grained and diverse visual question answering examples to assess the extent of agnosia in MLLMs comprehensively. We also d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#31574;&#30053;&#26799;&#24230;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#22810;&#36718;&#23545;&#35805;&#23545;&#40784;&#31574;&#30053;&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#31579;&#36873;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#25552;&#31034;&#38598;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.07272</link><description>&lt;p&gt;
&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#31163;&#25955;&#25552;&#31034;&#29983;&#25104;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#23545;&#35805;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Generation for Few-shot Learning. (arXiv:2308.07272v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#31574;&#30053;&#26799;&#24230;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#22810;&#36718;&#23545;&#35805;&#23545;&#40784;&#31574;&#30053;&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#31579;&#36873;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#25552;&#31034;&#38598;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23569;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#33539;&#24335;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#26469;&#35774;&#35745;&#22522;&#26412;&#25552;&#31034;&#38598;&#24182;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#65292;&#36825;&#26082;&#36153;&#26102;&#21448;&#20302;&#25928;&#65292;&#32780;&#19988;&#20027;&#35266;&#24615;&#36739;&#24378;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#36830;&#32493;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;PLMs&#30340;&#26799;&#24230;&#20449;&#24687;&#26469;&#25913;&#36827;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#21487;&#35835;&#24615;&#21644;&#36890;&#29992;&#24615;&#20302;&#24120;&#24120;&#26159;&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#31574;&#30053;&#26799;&#24230;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;($DP_2O$)&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-4&#30340;&#22810;&#36718;&#23545;&#35805;&#23545;&#40784;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#35835;&#24615;&#25552;&#31034;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25552;&#31034;&#31579;&#36873;&#24230;&#37327;&#65292;&#20197;&#32447;&#24615;&#22797;&#26434;&#24230;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;(RL)&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based pre-trained language models (PLMs) paradigm have succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization ($DP_2O$) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26080;&#20851;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21442;&#25968;&#37327;&#21270;&#26041;&#27861;QuIP&#65292;&#36890;&#36807;&#20351;&#26435;&#37325;&#21644;Hessian&#30697;&#38453;&#19982;&#22352;&#26631;&#36724;&#19981;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#37327;&#21270;&#32467;&#26524;&#12290;&#32463;&#36807;&#32463;&#39564;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29616;&#26377;&#30340;&#37327;&#21270;&#31639;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#22312;&#20165;&#20351;&#29992;&#20004;&#27604;&#29305;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#21487;&#34892;&#30340;LLM&#37327;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13304</link><description>&lt;p&gt;
QuIP&#65306;&#20855;&#26377;&#20445;&#35777;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;2&#27604;&#29305;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QuIP: 2-Bit Quantization of Large Language Models With Guarantees. (arXiv:2307.13304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26080;&#20851;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21442;&#25968;&#37327;&#21270;&#26041;&#27861;QuIP&#65292;&#36890;&#36807;&#20351;&#26435;&#37325;&#21644;Hessian&#30697;&#38453;&#19982;&#22352;&#26631;&#36724;&#19981;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#37327;&#21270;&#32467;&#26524;&#12290;&#32463;&#36807;&#32463;&#39564;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29616;&#26377;&#30340;&#37327;&#21270;&#31639;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#22312;&#20165;&#20351;&#29992;&#20004;&#27604;&#29305;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#21487;&#34892;&#30340;LLM&#37327;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#35757;&#32451;&#21518;&#21442;&#25968;&#37327;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26080;&#20851;&#22788;&#29702;&#65288;QuIP&#65289;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#20197;&#19979;&#35265;&#35299;&#65306;&#37327;&#21270;&#20174;&#19981;&#30456;&#20851;&#30340;&#26435;&#37325;&#21644; Hessian &#30697;&#38453;&#20013;&#25910;&#30410;&#65292;&#21363;&#36890;&#36807;&#20934;&#30830;&#22320;&#23558;&#23427;&#20204;&#33293;&#20837;&#20026;&#19982;&#22352;&#26631;&#36724;&#19981;&#23545;&#40784;&#30340;&#26041;&#21521;&#65292;&#20351;&#24471;&#33719;&#21462;&#37325;&#35201;&#30340;&#37327;&#21270;&#32467;&#26524;&#12290;QuIP &#21253;&#21547;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#26368;&#23567;&#21270;&#20108;&#27425;&#36817;&#20284;&#30446;&#26631;&#30340;&#33258;&#36866;&#24212;&#33293;&#20837;&#36807;&#31243;&#65307;&#65288;2&#65289;&#36890;&#36807;&#19982;&#38543;&#26426;&#27491;&#20132;&#30697;&#38453;&#30456;&#20056;&#26469;&#30830;&#20445;&#26435;&#37325;&#21644; Hessian &#26080;&#20851;&#30340;&#39640;&#25928;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#31532;&#19968;&#27425;&#38024;&#23545; LLM &#35268;&#27169;&#30340;&#37327;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#19988;&#35777;&#26126;&#25105;&#20204;&#30340;&#29702;&#35770;&#20063;&#36866;&#29992;&#20110;&#29616;&#26377;&#26041;&#27861; OPTQ&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26080;&#20851;&#39044;&#22788;&#29702;&#25913;&#21892;&#20102;&#29616;&#26377;&#30340;&#22810;&#20010;&#37327;&#21270;&#31639;&#27861;&#65292;&#24182;&#39318;&#27425;&#23454;&#29616;&#20102;&#20165;&#20351;&#29992;&#27599;&#20010;&#26435;&#37325;2&#27604;&#29305;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#20351;&#29992;&#22270;&#22686;&#24378;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#20174;&#24322;&#26500;&#22270;&#20013;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#12289;&#22270;&#27880;&#24847;&#21147;&#21644;&#20851;&#31995;&#31867;&#22411;&#32771;&#34385;&#65292;&#20248;&#21270;&#20102;&#23454;&#20307;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.10443</link><description>&lt;p&gt;
&#20351;&#29992;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#23558;&#24322;&#26500;&#22270;&#19982;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#30340;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model. (arXiv:2307.10443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#20351;&#29992;&#22270;&#22686;&#24378;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#20174;&#24322;&#26500;&#22270;&#20013;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#12289;&#22270;&#27880;&#24847;&#21147;&#21644;&#20851;&#31995;&#31867;&#22411;&#32771;&#34385;&#65292;&#20248;&#21270;&#20102;&#23454;&#20307;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36755;&#20837;&#24207;&#21015;&#20013;&#32570;&#23569;&#26174;&#24335;&#30693;&#35782;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#26469;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#23427;&#21033;&#29992;&#22686;&#24378;&#22270;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#30001;&#24322;&#26500;&#22270;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#12290;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;&#21333;&#35789;&#26631;&#35760;&#30340;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#65292;&#23545;&#23454;&#20307;&#26631;&#35760;&#30340;&#22270;&#27880;&#24847;&#21147;&#65292;&#23454;&#20307;&#26631;&#35760;&#23545;&#30456;&#20851;&#32852;&#30340;&#26631;&#35760;&#26174;&#31034;&#24378;&#28872;&#30340;&#27880;&#24847;&#21147;&#32780;&#23545;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#26174;&#31034;&#36739;&#24369;&#30340;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32771;&#34385;&#27599;&#20010;&#23454;&#20307;&#26631;&#35760;&#19982;&#21333;&#35789;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#31867;&#22411;&#12290;&#36825;&#26679;&#65292;&#22914;&#26524;&#23384;&#22312;&#20851;&#31995;&#65292;&#21017;&#21487;&#20197;&#20248;&#21270;&#20004;&#32773;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#29305;&#27530;&#30340;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant progress made by transformer models in machine reading comprehension tasks, they still face limitations in handling complex reasoning tasks due to the absence of explicit knowledge in the input sequence. This paper proposes a novel attention pattern to overcome this limitation, which integrates reasoning knowledge derived from a heterogeneous graph into the transformer architecture using a graph-enhanced self-attention mechanism. The proposed attention pattern comprises three key elements: global-local attention for word tokens, graph attention for entity tokens that exhibit strong attention towards tokens connected in the graph as opposed to those unconnected, and the consideration of the type of relationship between each entity token and word token. This results in optimized attention between the two if a relationship exists. The pattern is coupled with special relative position labels, allowing it to integrate with LUKE's entity-aware self-attention mechanism
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20986;&#22810;&#20013;&#24515;&#35782;&#21035;&#27169;&#22411;&#65292;&#25972;&#20307; F1&#24471;&#20998;&#20026;84.77%&#12290;&#35813;&#27169;&#22411;&#23558;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05323</link><description>&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#24212;&#29992;&#65306;&#26041;&#27861;&#35770;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#22810;&#20013;&#24515;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application. (arXiv:2306.05323v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20986;&#22810;&#20013;&#24515;&#35782;&#21035;&#27169;&#22411;&#65292;&#25972;&#20307; F1&#24471;&#20998;&#20026;84.77%&#12290;&#35813;&#27169;&#22411;&#23558;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#38498;&#24341;&#20837;&#35745;&#31639;&#26426;&#21270;&#21307;&#30103;&#35760;&#24405;&#26377;&#21161;&#20110;&#20943;&#23569;&#25163;&#20889;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#32321;&#29712;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#21307;&#30103;&#35760;&#24405;&#20013;&#25552;&#21462;&#25968;&#25454;&#38656;&#35201;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#22240;&#27492;&#21307;&#30103;&#35760;&#24405;&#20013;&#21253;&#21547;&#30340;&#25968;&#25454;&#20173;&#28982;&#34987;&#20805;&#20998;&#21033;&#29992;&#31243;&#24230;&#20302;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#39046;&#22495;&#20449;&#24687;&#25552;&#21462;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#20351;&#29992;&#33258;&#21160;&#21270;&#25991;&#26412;&#25366;&#25496;&#27969;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598; PsyNIT&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#24320;&#21457;&#36825;&#19968;&#20219;&#21153;&#30340;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#20351;&#29992;&#19977;&#20010;&#22806;&#37096;&#29420;&#31435;&#25968;&#25454;&#38598;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#20013;&#24515;&#27169;&#22411;&#65292;&#25972;&#20307; F1 &#24471;&#20998;&#20026; 84.77%&#65292;&#31934;&#30830;&#29575;&#20026; 83.16%&#65292;&#21484;&#22238;&#29575;&#20026; 86.44%&#12290;&#25105;&#20204;&#23398;&#21040;&#30340;&#32463;&#39564;&#26159;: (i) &#19968;&#33268;&#30340;&#27880;&#37322;&#36807;&#31243;&#30340;&#20851;&#38190;&#20316;&#29992;&#21644; (ii) &#32467;&#21512;&#32463;&#20856;&#26041;&#27861;&#21644;&#8220;&#23569;&#37327;&#35757;&#32451;&#8221;&#30340; fine-tuning &#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of computerized medical records in hospitals has reduced burdensome operations like manual writing and information fetching. However, the data contained in medical records are still far underutilized, primarily because extracting them from unstructured textual medical records takes time and effort. Information Extraction, a subfield of Natural Language Processing, can help clinical practitioners overcome this limitation, using automated text-mining pipelines. In this work, we created the first Italian neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to develop a Large Language Model for this task. Moreover, we conducted several experiments with three external independent datasets to implement an effective multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned are: (i) the crucial role of a consistent annotation process and (ii) a fine-tuning strategy that combines classical methods with a "few-shot" a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#65292;&#24182;&#38477;&#20302;&#26631;&#31614;&#33719;&#21462;&#30340;&#30740;&#31350;&#25104;&#26412;80&#65285;&#65292;&#21516;&#26102;&#20445;&#35777;CSS&#30740;&#31350;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04746</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#36827;&#34892;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#26377;&#25928;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;: &#22522;&#20110;&#35774;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning. (arXiv:2306.04746v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04746
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#65292;&#24182;&#38477;&#20302;&#26631;&#31614;&#33719;&#21462;&#30340;&#30740;&#31350;&#25104;&#26412;80&#65285;&#65292;&#21516;&#26102;&#20445;&#35777;CSS&#30740;&#31350;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#65288;CSS&#65289;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#25991;&#26723;&#26469;&#35299;&#37322;&#31038;&#20250;&#21644;&#25919;&#27835;&#29616;&#35937;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;CSS&#30740;&#31350;&#20154;&#21592;&#39318;&#20808;&#33719;&#21462;&#25991;&#26723;&#30340;&#26631;&#31614;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22238;&#24402;&#20998;&#26512;&#26469;&#35299;&#37322;&#26631;&#31614;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#36827;&#23637;&#21487;&#20197;&#36890;&#36807;&#22312;&#35268;&#27169;&#19978;&#20415;&#23452;&#22320;&#27880;&#37322;&#25991;&#26723;&#26469;&#38477;&#20302;CSS&#30740;&#31350;&#25104;&#26412;&#65292;&#20294;&#36825;&#20123;&#26367;&#20195;&#26631;&#31614;&#36890;&#24120;&#26159;&#19981;&#23436;&#32654;&#21644;&#26377;&#20559;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;LLMs&#30340;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#21516;&#26102;&#20445;&#35777;&#19982;CSS&#30740;&#31350;&#22522;&#26412;&#30456;&#20851;&#30340;&#32479;&#35745;&#23646;&#24615;-&#22914;&#28176;&#36817;&#26080;&#20559;&#24615;&#21644;&#27491;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#30452;&#25509;&#22312;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#20013;&#20351;&#29992;LLM&#39044;&#27979;&#30340;&#26367;&#20195;&#26631;&#31614;&#20250;&#23548;&#33268;&#23454;&#36136;&#24615;&#20559;&#24046;&#21644;&#26080;&#25928;&#32622;&#20449;&#21306;&#38388;&#65292;&#21363;&#20351;&#26367;&#20195;&#20934;&#30830;&#24615;&#39640;&#36798;80-90&#65285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#22522;&#20110;&#35774;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;D-SSL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;LLM&#27880;&#37322;&#19982;&#26377;&#38024;&#23545;&#24615;&#30340;&#37319;&#26679;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#26631;&#31614;&#33719;&#21462;&#30340;CSS&#30740;&#31350;&#25104;&#26412;&#38477;&#20302;80&#65285;&#65292;&#32780;&#19981;&#24433;&#21709;&#32479;&#35745;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#34920;&#26126;&#65292;&#19982;&#30452;&#25509;&#20351;&#29992;LLM&#39044;&#27979;&#26631;&#31614;&#30456;&#27604;&#65292;D-SSL&#21487;&#20197;&#23558;&#22238;&#24402;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#22810;&#36798;40&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. The recent advancements in large language models (LLMs) can lower costs for CSS research by annotating documents cheaply at scale, but such surrogate labels are often imperfect and biased. We present a new algorithm for using outputs from LLMs for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to CSS research. We show that direct use of LLM-predicted surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80--90\%. To address this, we build on debiased machine learning to propose the design-based semi-supervised le
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18342</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#32534;&#31243;&#20013;&#31070;&#32463;&#20219;&#21153;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#26032;&#30340;&#20869;&#23481;&#65292;&#29983;&#25104;&#24335;&#31070;&#32463;&#27169;&#22411;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31070;&#32463;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#21487;&#35270;&#21270;&#32534;&#31243;&#29615;&#22659;&#19979;&#32473;&#23450;&#30340;&#35268;&#33539;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#20687; GPT-4 &#36825;&#26679;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21512;&#25104;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#36923;&#36753;&#21644;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415; NeurTaskSyn&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#21512;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;NeurTaskSyn &#30001;&#20004;&#20010;&#37096;&#20998;&#26500;&#25104;&#65306;&#31532;&#19968;&#20010;&#37096;&#20998;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#29983;&#25104;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#25351;&#23548;&#24213;&#23618;&#31526;&#21495;&#25191;&#34892;&#24341;&#25806;&#29983;&#25104;&#21487;&#35270;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#23545;&#35805;&#35748;&#30693;&#30340;&#30452;&#35273;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#35805;&#35748;&#30693;&#27169;&#22411;&#65292;&#20197;&#35299;&#37322;&#27599;&#20010;&#35805;&#35821;&#22914;&#20309;&#36882;&#24402;&#22320;&#33719;&#24471;&#21644;&#28608;&#27963;&#20449;&#24687;&#36890;&#36947;&#12290;&#36890;&#36807;&#23558;&#20854;&#20195;&#25968;&#36716;&#21270;&#20026;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#35813;&#27169;&#22411;&#22312;&#35805;&#35821;&#32423;&#21035;&#30340;&#20851;&#31995;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#65292;&#23427;&#35299;&#20915;&#20102;&#38544;&#21547;&#21407;&#22240;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35777;&#25454;&#19979;&#30028;&#37325;&#26500;&#20102;&#35805;&#35821;&#30340;&#22240;&#26524;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.17727</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#23398;&#20064;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#30452;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning a Structural Causal Model for Intuition Reasoning in Conversation. (arXiv:2305.17727v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#23545;&#35805;&#35748;&#30693;&#30340;&#30452;&#35273;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#35805;&#35748;&#30693;&#27169;&#22411;&#65292;&#20197;&#35299;&#37322;&#27599;&#20010;&#35805;&#35821;&#22914;&#20309;&#36882;&#24402;&#22320;&#33719;&#24471;&#21644;&#28608;&#27963;&#20449;&#24687;&#36890;&#36947;&#12290;&#36890;&#36807;&#23558;&#20854;&#20195;&#25968;&#36716;&#21270;&#20026;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#35813;&#27169;&#22411;&#22312;&#35805;&#35821;&#32423;&#21035;&#30340;&#20851;&#31995;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#65292;&#23427;&#35299;&#20915;&#20102;&#38544;&#21547;&#21407;&#22240;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35777;&#25454;&#19979;&#30028;&#37325;&#26500;&#20102;&#35805;&#35821;&#30340;&#22240;&#26524;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#20294;&#26159;&#30446;&#21069;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#22788;&#29702;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#23545;&#35805;&#25512;&#29702;&#20316;&#20026;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#30001;&#20110;&#32570;&#20047;&#33391;&#22909;&#35774;&#35745;&#30340;&#35748;&#30693;&#27169;&#22411;&#32780;&#19968;&#30452;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#21463;&#21040;&#23545;&#35805;&#35748;&#30693;&#30340;&#30452;&#35273;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#27599;&#20010;&#35805;&#35821;&#22914;&#20309;&#36882;&#24402;&#22320;&#33719;&#24471;&#21644;&#28608;&#27963;&#20449;&#24687;&#36890;&#36947;&#30340;&#23545;&#35805;&#35748;&#30693;&#27169;&#22411;&#65288;CCM&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;CCM&#36827;&#34892;&#20102;&#20195;&#25968;&#21464;&#25442;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#65292;&#22312;&#19968;&#20123;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;SCM&#19982;&#21508;&#31181;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20860;&#23481;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;SCM&#22312;&#35805;&#35821;&#32423;&#20851;&#31995;&#25512;&#29702;&#20013;&#30340;&#27010;&#29575;&#23454;&#29616;&#12290;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#65292;&#23427;&#25506;&#32034;&#20102;&#38544;&#21547;&#21407;&#22240;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#20854;&#19981;&#21487;&#35266;&#23519;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35777;&#25454;&#19979;&#30028;&#37325;&#26500;&#20102;&#35805;&#35821;&#30340;&#22240;&#26524;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Reasoning, a crucial aspect of NLP research, has not been adequately addressed by prevailing models including Large Language Model. Conversation reasoning, as a critical component of it, remains largely unexplored due to the absence of a well-designed cognitive model. In this paper, inspired by intuition theory on conversation cognition, we develop a conversation cognitive model (CCM) that explains how each utterance receives and activates channels of information recursively. Besides, we algebraically transformed CCM into a structural causal model (SCM) under some mild assumptions, rendering it compatible with various causal discovery methods. We further propose a probabilistic implementation of the SCM for utterance-level relation reasoning. By leveraging variational inference, it explores substitutes for implicit causes, addresses the issue of their unobservability, and reconstructs the causal representations of utterances through the evidence lower bounds. Moreover, we constructed s
&lt;/p&gt;</description></item><item><title>Translatotron 3&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#36827;&#34892;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65292;&#26080;&#38656;&#37197;&#23545;&#30340;&#25968;&#25454;&#25110;&#19987;&#19994;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#20445;&#30041;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.17547</link><description>&lt;p&gt;
Translatotron 3: &#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#36827;&#34892;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Translatotron 3: Speech to Speech Translation with Monolingual Data. (arXiv:2305.17547v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17547
&lt;/p&gt;
&lt;p&gt;
Translatotron 3&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#36827;&#34892;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65292;&#26080;&#38656;&#37197;&#23545;&#30340;&#25968;&#25454;&#25110;&#19987;&#19994;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#20445;&#30041;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Translatotron 3&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#12289;&#26080;&#30417;&#30563;&#30340;&#23884;&#20837;&#26144;&#23556;&#21644;&#22238;&#35793;&#23558;&#30452;&#25509;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#20174;&#21333;&#22768;&#36947;&#35821;&#38899;-&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#23436;&#20840;&#26080;&#30417;&#30563;&#22320;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#33521;&#35821;&#20043;&#38388;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Translatotron 3&#20248;&#20110;&#22522;&#20934;&#32423;&#32852;&#31995;&#32479;&#65292;&#22312; synthesized Unpaired-Conversational &#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;18.14 BLEU&#20998;&#25968;&#30340;&#25552;&#39640;&#12290;&#19982;&#38656;&#35201;&#30495;&#23454;&#37197;&#23545;&#25968;&#25454;&#25110;&#19987;&#19994;&#24314;&#27169;&#26469;&#22797;&#21046;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#30417;&#30563;&#26041;&#27861;&#19981;&#21516;&#65292;Translatotron 3&#23637;&#31034;&#20102;&#23427;&#20445;&#30041;&#20102;&#20687;&#26242;&#20572;&#12289;&#35828;&#35805;&#36895;&#24230;&#21644;&#35828;&#35805;&#20154;&#36523;&#20221;&#31561;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Translatotron 3, a novel approach to train a direct speech-to-speech translation model from monolingual speech-text datasets only in a fully unsupervised manner. Translatotron 3 combines masked autoencoder, unsupervised embedding mapping, and back-translation to achieve this goal. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting 18.14 BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, which is unavailable, or specialized modeling to replicate para-/non-linguistic information, Translatotron 3 showcases its capability to retain para-/non-linguistic such as pauses, speaking rates, and speaker identity. Audio samples can be found in our website this http URL
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#21644;&#25197;&#30697;&#30340;&#25163;&#35821;&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;&#65292;&#33021;&#22815;&#36873;&#20986;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#20851;&#38190;&#24103;&#12290;</title><link>http://arxiv.org/abs/2305.16801</link><description>&lt;p&gt;
&#22522;&#20110;&#26354;&#29575;&#21644;&#25197;&#30697;&#30340;&#25163;&#35821;&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Motion-Based Sign Language Video Summarization using Curvature and Torsion. (arXiv:2305.16801v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16801
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#21644;&#25197;&#30697;&#30340;&#25163;&#35821;&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;&#65292;&#33021;&#22815;&#36873;&#20986;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#20851;&#38190;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;&#22312;&#24456;&#22810;&#22522;&#20110;&#35270;&#39057;&#30340;&#24212;&#29992;&#20013;&#37117;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25163;&#35821;&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#20174;&#35270;&#39057;&#24103;&#20013;&#25552;&#21462;&#30340;&#19977;&#32500;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#26469;&#27169;&#22411;&#21270;&#27599;&#19968;&#24103;&#20013;&#30340;&#19977;&#32500;&#36816;&#21160;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#21644;&#25197;&#30697;&#30340;&#26032;&#22411;&#20449;&#24687;&#20989;&#25968;&#65292;&#20197;&#20415;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#20851;&#38190;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;
An interesting problem in many video-based applications is the generation of short synopses by selecting the most informative frames, a procedure which is known as video summarization. For sign language videos the benefits of using the $t$-parameterized counterpart of the curvature of the 2-D signer's wrist trajectory to identify keyframes, have been recently reported in the literature. In this paper we extend these ideas by modeling the 3-D hand motion that is extracted from each frame of the video. To this end we propose a new informative function based on the $t$-parameterized curvature and torsion of the 3-D trajectory. The method to characterize video frames as keyframes depends on whether the motion occurs in 2-D or 3-D space. Specifically, in the case of 3-D motion we look for the maxima of the harmonic mean of the curvature and torsion of the target's trajectory; in the planar motion case we seek for the maxima of the trajectory's curvature. The proposed 3-D feature is experime
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11554</link><description>&lt;p&gt;
ToolkenGPT&#65306;&#36890;&#36807;&#24037;&#20855;&#23884;&#20837;&#25193;&#20805;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#29992;&#24037;&#20855;&#28436;&#31034;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#26082;&#36153;&#26102;&#21448;&#21463;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#24037;&#20855;&#38598;&#12290;&#26368;&#36817;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#20363;&#32531;&#35299;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#26159;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21482;&#20801;&#35768;&#28436;&#31034;&#20960;&#27425;&#65292;&#23548;&#33268;&#23545;&#24037;&#20855;&#30340;&#29702;&#35299;&#19981;&#22815;&#20805;&#20998;&#12290;&#27492;&#22806;&#65292;&#24403;&#26377;&#22823;&#37327;&#24037;&#20855;&#21487;&#20379;&#36873;&#25321;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#33021;&#23436;&#20840;&#26080;&#27861;&#27491;&#24120;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$\textbf{ToolkenGPT}$&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;$\underline{&#24037;&#20855;}$&#34920;&#31034;&#20026;&#19968;&#20010;$\underline{token}$&#65288;$\textit{toolken}$&#65289;&#65292;&#24182;&#20026;&#20854;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#65292;&#20351;&#24471;&#24037;&#20855;&#35843;&#29992;&#19982;&#29983;&#25104;&#24120;&#35268;&#21333;&#35789;&#26631;&#35760;&#30340;&#26041;&#24335;&#30456;&#21516;&#12290;&#19968;&#26086;&#35302;&#21457;&#20102;toolken&#65292;LLM&#34987;&#25552;&#31034;&#23436;&#25104;&#24037;&#20855;&#25191;&#34892;&#25152;&#38656;&#30340;&#21442;&#25968;&#12290;ToolkenGPT&#25552;&#20379;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;1&#65289;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#20197;&#25193;&#20805;LLM&#19982;&#22806;&#37096;&#24037;&#20855;&#30340;&#20132;&#20114;&#65292;2&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#20363;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;3&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\underline{tool}$ as a to$\underline{ken}$ ($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10818</link><description>&lt;p&gt;
&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26377;&#28508;&#22312;&#22909;&#22788;&#65292;&#20294;&#30446;&#21069;&#20844;&#24320;&#30340;&#23454;&#29616;&#12289;&#35757;&#32451;&#27169;&#22411;&#25110;&#21487;&#37325;&#29616;&#30340;&#35757;&#32451;&#31243;&#24207;&#24182;&#19981;&#23384;&#22312;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;C4&#25968;&#25454;&#38598;&#31616;&#21270;&#30340;DDLM&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#36895;&#24230;&#26356;&#24555;&#30340;&#37319;&#26679;&#30340;&#26032;&#22411;&#26089;&#26399;&#36864;&#20986;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#38024;&#23545;&#20351;&#29992;&#24471;&#20998;&#25554;&#20540;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#27492;&#21069;&#27809;&#26377;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;LM&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#31867;&#20219;&#21153;&#65289;&#65292;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;DDLM&#30340;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20379;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#30340;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#39044;&#20808;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#22312;&#26410;&#26469;&#30340;D&#30456;&#20851;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the potential benefits of Diffusion Models for NLP applications, publicly available implementations, trained models, or reproducible training procedures currently need to be publicly available. We present the Democratized Diffusion Language Model (DDLM), based on the Continuous Diffusion for Categorical Data (CDCD) framework, to address these challenges. We propose a simplified training procedure for DDLM using the C4 dataset and perform an in-depth analysis of the trained model's behavior. Furthermore, we introduce a novel early-exiting strategy for faster sampling with models trained with score interpolation. Since no previous works aimed at solving downstream tasks with pre-trained Diffusion LM (e.g., classification tasks), we experimented with GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this paper, we propose available training and evaluation pipelines to other researchers and pre-trained DDLM models, which could be used in future research with D
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DinoSR&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#12289;&#33258;&#33976;&#39311;&#21644;&#22312;&#32447;&#32858;&#31867;&#31561;&#27010;&#24565;&#65292;&#33021;&#22815;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#20013;&#20135;&#29983;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.10005</link><description>&lt;p&gt;
DinoSR&#65306;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#33258;&#33976;&#39311;&#21644;&#22312;&#32447;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
DinoSR: Self-Distillation and Online Clustering for Self-supervised Speech Representation Learning. (arXiv:2305.10005v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DinoSR&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#12289;&#33258;&#33976;&#39311;&#21644;&#22312;&#32447;&#32858;&#31867;&#31561;&#27010;&#24565;&#65292;&#33021;&#22815;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#20013;&#20135;&#29983;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#33976;&#39311;&#21644;&#22312;&#32447;&#32858;&#31867;&#29992;&#20110;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#30340;DinoSR&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#12289;&#33258;&#33976;&#39311;&#21644;&#22312;&#32447;&#32858;&#31867;&#36825;&#20123;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#30456;&#20114;&#34917;&#20805;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;DinoSR&#39318;&#20808;&#20351;&#29992;&#25945;&#24072;&#32593;&#32476;&#20174;&#36755;&#20837;&#38899;&#39057;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#21270;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#22312;&#23884;&#20837;&#21521;&#37327;&#19978;&#36816;&#34892;&#22312;&#32447;&#32858;&#31867;&#31995;&#32479;&#20197;&#20135;&#29983;&#26426;&#22120;&#21457;&#29616;&#30340;&#38899;&#32032;&#24211;&#23384;&#65292;&#26368;&#21518;&#20351;&#29992;&#24050;&#31163;&#25955;&#21270;&#30340;&#26631;&#35760;&#25351;&#23548;&#23398;&#29983;&#32593;&#32476;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DinoSR&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#27169;&#22411;&#21644;&#23398;&#20064;&#31163;&#25955;&#21333;&#20803;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;&#21311;&#21517;&#26399;&#32467;&#26463;&#21518;&#65292;&#25105;&#20204;&#23558;&#25552;&#20379;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce self-distillation and online clustering for self-supervised speech representation learning (DinoSR) which combines masked language modeling, self-distillation, and online clustering. We show that these concepts complement each other and result in a strong representation learning model for speech. DinoSR first extracts contextualized embeddings from the input audio with a teacher network, then runs an online clustering system on the embeddings to yield a machine-discovered phone inventory, and finally uses the discretized tokens to guide a student network. We show that DinoSR surpasses previous state-of-the-art performance in several downstream tasks, and provide a detailed analysis of the model and the learned discrete units. The source code will be made available after the anonymity period.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SwissBERT&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#22788;&#29702;&#29790;&#22763;&#30456;&#20851;&#25991;&#26412;&#32780;&#21019;&#24314;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;SwissBERT&#22312;&#19982;&#29790;&#22763;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.13310</link><description>&lt;p&gt;
SwissBERT&#65306;&#29790;&#22763;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SwissBERT: The Multilingual Language Model for Switzerland. (arXiv:2303.13310v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SwissBERT&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#22788;&#29702;&#29790;&#22763;&#30456;&#20851;&#25991;&#26412;&#32780;&#21019;&#24314;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;SwissBERT&#22312;&#19982;&#29790;&#22763;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SwissBERT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#22788;&#29702;&#19982;&#29790;&#22763;&#30456;&#20851;&#30340;&#25991;&#26412;&#32780;&#21019;&#24314;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#12290; SwissBERT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#35843;&#25972;&#20026;&#33021;&#22815;&#22788;&#29702;&#29790;&#22763;&#22269;&#23478;&#35821;&#35328; -&#24503;&#35821;&#12289;&#27861;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#32599;&#26364;&#20160;&#35821;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;SwissBERT&#22312;&#19982;&#29790;&#22763;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#23427;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#24448;&#24448;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#24403;&#20195;&#26032;&#38395;&#21644;/&#25110;&#32599;&#26364;&#20160;&#35821;&#26684;&#37324;&#26031;&#26118;&#26102;&#12290;&#30001;&#20110;SwissBERT&#20351;&#29992;&#35821;&#35328;&#36866;&#37197;&#22120;&#65292;&#22240;&#27492;&#26410;&#26469;&#30340;&#24037;&#20316;&#21487;&#33021;&#23558;&#20854;&#25193;&#23637;&#21040;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;&#20013;&#12290;&#35813;&#27169;&#22411;&#21644;&#25105;&#20204;&#30340;&#24320;&#28304;&#20195;&#30721;&#20844;&#24320;&#21457;&#24067;&#22312;https://github.com/ZurichNLP/swissbert&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SwissBERT, a masked language model created specifically for processing Switzerland-related text. SwissBERT is a pre-trained model that we adapted to news articles written in the national languages of Switzerland -German, French, Italian, and Romansh. We evaluate SwissBERT on natural language understanding tasks related to Switzerland and find that it tends to outperform previous models on these tasks, especially when processing contemporary news and/or Romansh Grischun. Since SwissBERT uses language adapters, it may be extended to Swiss German dialects in future work. The model and our open-source code are publicly released at https://github.com/ZurichNLP/swissbert.
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.11593</link><description>&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#25163;&#24615;&#26102;&#23384;&#22312;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Difficulty in learning chirality for Transformer fed with SMILES. (arXiv:2303.11593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11593
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23545;&#26497;&#20854;&#22810;&#26679;&#30340;&#20998;&#23376;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#25551;&#36848;&#31526;&#29983;&#25104;&#24050;&#32463;&#24471;&#21040;&#20102;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;SMILES&#65292;&#21363;&#20998;&#23376;&#32467;&#26500;&#30340;&#25991;&#23383;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21270;&#23398;&#32467;&#26500;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;NLP&#27169;&#22411;&#8212;&#8212;Transformer&#65292;&#22312;&#23398;&#20064;SMILES&#21644;&#21270;&#23398;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;Transformer&#24555;&#36895;&#23398;&#20064;&#20998;&#23376;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#25165;&#33021;&#29702;&#35299;&#25972;&#20307;&#32467;&#26500;&#12290;&#19982;&#20043;&#19968;&#33268;&#30340;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#23398;&#20064;&#27493;&#39588;&#20013;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;Transformer&#38656;&#35201;&#29305;&#21035;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#25165;&#33021;&#23398;&#20064;&#25163;&#24615;&#65292;&#24182;&#19988;&#26377;&#26102;&#20250;&#20986;&#29616;&#20302;&#32763;&#35793;&#20934;&#30830;&#29575;&#30340;&#20572;&#28382;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen development of descriptor generation based on representation learning of extremely diverse molecules, especially those that apply natural language processing (NLP) models to SMILES, a literal representation of molecular structure. However, little research has been done on how these models understand chemical structure. To address this, we investigated the relationship between the learning progress of SMILES and chemical structure using a representative NLP model, the Transformer. The results suggest that while the Transformer learns partial structures of molecules quickly, it requires extended training to understand overall structures. Consistently, the accuracy of molecular property predictions using descriptors generated from models at different learning steps was similar from the beginning to the end of training. Furthermore, we found that the Transformer requires particularly long training to learn chirality and sometimes stagnates with low translation accura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;Twitter&#25968;&#25454;&#36827;&#34892;&#20102;&#23545;&#20234;&#26391;&#20844;&#20247;&#23545;COVID-19&#30123;&#33495;&#30340;&#24847;&#35265;&#36827;&#34892;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#30123;&#33495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#28041;&#21450;&#25919;&#24220;&#38382;&#39064;&#12289;&#23433;&#20840;&#24615;&#12289;&#29369;&#35947;&#19981;&#20915;&#21644;&#21103;&#20316;&#29992;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2302.04511</link><description>&lt;p&gt;
&#20851;&#20110;COVID-19&#30123;&#33495;&#30340;&#27874;&#26031;&#25512;&#25991;&#30340;&#22823;&#35268;&#27169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Large-Scale Analysis of Persian Tweets Regarding Covid-19 Vaccination. (arXiv:2302.04511v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;Twitter&#25968;&#25454;&#36827;&#34892;&#20102;&#23545;&#20234;&#26391;&#20844;&#20247;&#23545;COVID-19&#30123;&#33495;&#30340;&#24847;&#35265;&#36827;&#34892;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#30123;&#33495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#28041;&#21450;&#25919;&#24220;&#38382;&#39064;&#12289;&#23433;&#20840;&#24615;&#12289;&#29369;&#35947;&#19981;&#20915;&#21644;&#21103;&#20316;&#29992;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#22823;&#27969;&#34892;&#23545;&#25105;&#20204;&#30340;&#29983;&#27963;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#20154;&#20204;&#30340;&#20114;&#21160;&#12290;&#24341;&#20837;COVID-19&#30123;&#33495;&#21518;&#65292;&#20851;&#20110;&#25509;&#31181;&#30123;&#33495;&#19982;&#21542;&#30340;&#27491;&#21453;&#24847;&#35265;&#37117;&#26377;&#25152;&#25552;&#20986;&#12290;&#26412;&#25991;&#21033;&#29992;&#20174;Twitter&#33719;&#21462;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#25512;&#25991;&#21644;&#29992;&#25143;&#36164;&#26009;&#65292;&#23545;&#20234;&#26391;&#20844;&#20247;&#23545;&#20896;&#29366;&#30149;&#27602;&#30123;&#33495;&#30340;&#24847;&#35265;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#25628;&#32034;&#26597;&#35810;&#25216;&#26415;&#21644;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#26469;&#25552;&#21462;&#19982;&#30123;&#33495;&#30456;&#20851;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#23545;&#25512;&#25991;&#20869;&#23481;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25552;&#21462;&#22260;&#32469;&#25509;&#31181;&#30340;&#20027;&#39064;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#20844;&#20247;&#22312;&#36825;&#19968;&#35805;&#39064;&#19978;&#30340;&#24555;&#20048;&#21644;&#24868;&#24594;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;COVID-19&#30123;&#33495;&#24050;&#24341;&#36215;&#20102;&#21508;&#31181;&#19981;&#21516;&#35282;&#24230;&#30340;&#39640;&#24230;&#20851;&#27880;&#65292;&#22914;&#25919;&#24220;&#38382;&#39064;&#12289;&#23433;&#20840;&#24615;&#25110;&#29369;&#35947;&#19981;&#20915;&#20197;&#21450;&#21103;&#20316;&#29992;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Covid-19 pandemic had an enormous effect on our lives, especially on people's interactions. By introducing Covid-19 vaccines, both positive and negative opinions were raised over the subject of taking vaccines or not. In this paper, using data gathered from Twitter, including tweets and user profiles, we offer a comprehensive analysis of public opinion in Iran about the Coronavirus vaccines. For this purpose, we applied a search query technique combined with a topic modeling approach to extract vaccine-related tweets. We utilized transformer-based models to classify the content of the tweets and extract themes revolving around vaccination. We also conducted an emotion analysis to evaluate the public happiness and anger around this topic. Our results demonstrate that Covid-19 vaccination has attracted considerable attention from different angles, such as governmental issues, safety or hesitancy, and side effects. Moreover, Coronavirus-relevant phenomena like public vaccination and t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04391</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#26032;&#26631;&#31614;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#24320;&#21457;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;90&#20998;&#20197;&#19978;&#30340;&#25104;&#32489;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#20986;&#22122;&#22768;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#20154;&#31867;&#26631;&#35760;&#30340;&#21442;&#32771;&#26469;&#37325;&#26032;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24819;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24207;&#21015;&#26631;&#35760;&#12289;&#29289;&#20307;&#26816;&#27979;&#12289;&#24207;&#21015;&#29983;&#25104;&#12289;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#29305;&#21035;&#20851;&#27880;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.13709</link><description>&lt;p&gt;
NLP&#20013;&#30340;&#19981;&#33391;&#20559;&#35265;&#65306;&#36991;&#20813;&#34913;&#37327;&#21361;&#26426;
&lt;/p&gt;
&lt;p&gt;
Undesirable biases in NLP: Averting a crisis of measurement. (arXiv:2211.13709v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13709
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#29305;&#21035;&#20851;&#27880;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#26222;&#21450;&#65292;&#39044;&#27979;&#20854;&#20351;&#29992;&#21487;&#33021;&#23545;&#20154;&#20204;&#36896;&#25104;&#20260;&#23475;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#19968;&#20010;&#21463;&#21040;&#20851;&#27880;&#30340;&#38382;&#39064;&#26159;&#36825;&#19968;&#25216;&#26415;&#22312;&#34892;&#20026;&#20013;&#26174;&#31034;&#20986;&#26377;&#23475;&#20559;&#35265;&#12290;&#23613;&#31649;&#24050;&#32463;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#35780;&#20272;&#21644;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#65292;&#20294;&#25105;&#20204;&#34913;&#37327;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#36890;&#24120;&#19981;&#28165;&#26970;&#23427;&#20204;&#21040;&#24213;&#34913;&#37327;&#20102;&#20160;&#20040;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#35752;&#35770;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#24515;&#29702;&#27979;&#37327;&#23398;&#19987;&#27880;&#20110;&#34913;&#37327;&#19981;&#30452;&#25509;&#21487;&#35266;&#23519;&#21040;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20004;&#20010;&#26680;&#24515;&#27010;&#24565;&#65292;&#21363;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#24182;&#35752;&#35770;&#23427;&#20204;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models and Natural Language Processing (NLP) technology rapidly develops and spreads into daily life, it becomes crucial to anticipate how its use could harm people. One problem that has received a lot of attention in recent years is that this technology has displayed harmful biases in its behavior. Although a lot of effort has been invested in assessing and mitigating these biases, our methods of measuring the biases of NLP models have serious problems (e.g., it is often unclear what they actually measure). In this paper, we provide an interdisciplinary approach to discussing the issue of NLP model bias by adopting the lens of psychometrics -- a field specialized in the measurement of concepts like bias that are not directly observable. In particular, we will explore two central notions from psychometrics, the construct validity and the reliability of measurement tools, and discuss how they can be applied in the context of measuring model bias. Our goal is to provide
&lt;/p&gt;</description></item><item><title>NormSAGE&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#35805;&#39537;&#21160;&#30340;&#22810;&#35821;&#35328;&#22810;&#25991;&#21270;&#35268;&#33539;&#21457;&#29616;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;GPT-3&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#24341;&#21457;&#65292;&#24182;&#36890;&#36807;&#33258;&#39564;&#35777;&#26426;&#21046;&#30830;&#20445;&#21457;&#29616;&#30340;&#35268;&#33539;&#27491;&#30830;&#19988;&#19982;&#28304;&#23545;&#35805;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2210.08604</link><description>&lt;p&gt;
NormSAGE: &#22810;&#35821;&#35328;&#22810;&#25991;&#21270;&#23545;&#35805;&#20013;&#30340;&#35268;&#33539;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
NormSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly. (arXiv:2210.08604v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08604
&lt;/p&gt;
&lt;p&gt;
NormSAGE&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#35805;&#39537;&#21160;&#30340;&#22810;&#35821;&#35328;&#22810;&#25991;&#21270;&#35268;&#33539;&#21457;&#29616;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;GPT-3&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#24341;&#21457;&#65292;&#24182;&#36890;&#36807;&#33258;&#39564;&#35777;&#26426;&#21046;&#30830;&#20445;&#21457;&#29616;&#30340;&#35268;&#33539;&#27491;&#30830;&#19988;&#19982;&#28304;&#23545;&#35805;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#33539;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#21644;&#25512;&#29702;&#20154;&#31867;&#20132;&#27969;&#21644;&#20114;&#21160;&#20013;&#21487;&#25509;&#21463;&#30340;&#34892;&#20026;&#21644;&#28508;&#22312;&#36829;&#35268;&#34892;&#20026;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;NormSage&#65292;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#26032;&#39062;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#21644;&#33258;&#39564;&#35777;&#30340;&#23545;&#35805;&#39537;&#21160;&#30340;&#22810;&#35821;&#35328;&#22810;&#25991;&#21270;&#35268;&#33539;&#21457;&#29616;&#12290;NormSAGE&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;GPT-3&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#38544;&#24335;&#30693;&#35782;&#65292;&#36890;&#36807;&#38024;&#23545;&#35268;&#33539;&#21457;&#29616;&#20219;&#21153;&#21644;&#23545;&#35805;&#32972;&#26223;&#30340;&#26377;&#21521;&#38382;&#39064;&#24341;&#21457;&#20851;&#20110;&#35268;&#33539;&#30340;&#30693;&#35782;&#12290;&#23427;&#36824;&#36890;&#36807;&#33258;&#39564;&#35777;&#26426;&#21046;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#34394;&#26500;&#30340;&#39118;&#38505;&#65292;&#30830;&#20445;&#21457;&#29616;&#30340;&#35268;&#33539;&#26159;&#27491;&#30830;&#30340;&#65292;&#24182;&#19988;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19982;&#23427;&#20204;&#30340;&#28304;&#23545;&#35805;&#30456;&#20851;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#23454;&#26102;&#23545;&#35805;&#20013;&#21457;&#29616;&#26356;&#22810;&#30456;&#20851;&#19988;&#26377;&#27934;&#23519;&#21147;&#30340;&#35268;&#33539;&#65288;Likert&#35780;&#20998;&#20013;&#22686;&#21152;&#20102;10%&#20197;&#19978;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Norm discovery is important for understanding and reasoning about the acceptable behaviors and potential violations in human communication and interactions. We introduce NormSage, a framework for addressing the novel task of conversation-grounded multi-lingual, multi-cultural norm discovery, based on language model prompting and self-verification. NormSAGE leverages the expressiveness and implicit knowledge of the pretrained GPT-3 language model backbone, to elicit knowledge about norms through directed questions representing the norm discovery task and conversation context. It further addresses the risk of language model hallucination with a self-verification mechanism ensuring that the norms discovered are correct and are substantially grounded to their source conversations. Evaluation results show that our approach discovers significantly more relevant and insightful norms for conversations on-the-fly compared to baselines (&gt;10+% in Likert scale rating). The norms discovered from Ch
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#30740;&#31350;&#22238;&#39038;&#20102;&#36229;&#36807;110&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#20174;&#24544;&#23454;&#24615;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#32508;&#21512;&#12290;&#30740;&#31350;&#20171;&#32461;&#20102;&#24544;&#23454;&#35299;&#37322;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#35752;&#35770;&#20102;&#21508;&#20010;&#26041;&#27861;&#30340;&#20248;&#28857;&#12289;&#32570;&#28857;&#20197;&#21450;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2209.11326</link><description>&lt;p&gt;
&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24544;&#23454;&#30340;&#27169;&#22411;&#35299;&#37322;&#30340;&#25506;&#32034;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful Model Explanation in NLP: A Survey. (arXiv:2209.11326v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#30740;&#31350;&#22238;&#39038;&#20102;&#36229;&#36807;110&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#20174;&#24544;&#23454;&#24615;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#32508;&#21512;&#12290;&#30740;&#31350;&#20171;&#32461;&#20102;&#24544;&#23454;&#35299;&#37322;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#35752;&#35770;&#20102;&#21508;&#20010;&#26041;&#27861;&#30340;&#20248;&#28857;&#12289;&#32570;&#28857;&#20197;&#21450;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#19968;&#30452;&#20197;&#26469;&#37117;&#38590;&#20197;&#29702;&#35299;&#12290;&#36825;&#24341;&#21457;&#20102;&#36817;&#24180;&#26469;&#35768;&#22810;&#20851;&#20110;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#21162;&#21147;&#12290;&#20854;&#20013;&#19968;&#20010;&#35299;&#37322;&#27169;&#22411;&#30340;&#35201;&#27714;&#26159;&#24544;&#23454;&#24615;&#65292;&#21363;&#35299;&#37322;&#24212;&#20934;&#30830;&#22320;&#34920;&#36798;&#27169;&#22411;&#39044;&#27979;&#32972;&#21518;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#26412;&#35843;&#26597;&#36890;&#36807;&#24544;&#23454;&#24615;&#30340;&#35270;&#35282;&#23545;&#36229;&#36807;110&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#24544;&#23454;&#24615;&#30340;&#23450;&#20041;&#21644;&#35780;&#20272;&#65292;&#20197;&#21450;&#20854;&#23545;&#21487;&#35299;&#37322;&#24615;&#30340;&#24847;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24544;&#23454;&#35299;&#37322;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#23558;&#29616;&#26377;&#26041;&#27861;&#20998;&#20026;&#20116;&#20010;&#31867;&#21035;&#65306;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12289;&#27169;&#22411;&#20869;&#37096;&#32467;&#26500;&#30340;&#20998;&#26512;&#12289;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#12289;&#21453;&#20107;&#23454;&#24178;&#39044;&#21644;&#33258;&#35299;&#37322;&#27169;&#22411;&#12290;&#23545;&#20110;&#27599;&#20010;&#31867;&#21035;&#65292;&#25105;&#20204;&#32508;&#21512;&#20102;&#20854;&#20195;&#34920;&#24615;&#30740;&#31350;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#20849;&#21516;&#20248;&#28857;&#21644;&#25361;&#25112;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, i.e. an explanation should accurately represent the reasoning process behind the model's prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity-based methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future wo
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35821;&#35328;&#30456;&#20851;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#28145;&#24230;NLP&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#20123;&#31361;&#20986;&#30340;&#31070;&#32463;&#20803;&#65292;&#36825;&#20123;&#31070;&#32463;&#20803;&#33021;&#22815;&#25429;&#25417;&#29305;&#23450;&#30340;&#35821;&#35328;&#23646;&#24615;&#65292;&#32780;&#19988;&#20449;&#24687;&#20445;&#23384;&#20887;&#20313;&#31243;&#24230;&#36739;&#39640;&#12290;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#24433;&#21709;&#20197;&#21450;&#19981;&#21516;&#26550;&#26500;&#23398;&#20064;&#19981;&#21516;&#35821;&#35328;&#23646;&#24615;&#30340;&#24046;&#24322;&#20063;&#24471;&#21040;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2206.13288</link><description>&lt;p&gt;
&#28145;&#24230;NLP&#27169;&#22411;&#20013;&#31361;&#20986;&#31070;&#32463;&#20803;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discovering Salient Neurons in Deep NLP Models. (arXiv:2206.13288v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13288
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#30456;&#20851;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#28145;&#24230;NLP&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#20123;&#31361;&#20986;&#30340;&#31070;&#32463;&#20803;&#65292;&#36825;&#20123;&#31070;&#32463;&#20803;&#33021;&#22815;&#25429;&#25417;&#29305;&#23450;&#30340;&#35821;&#35328;&#23646;&#24615;&#65292;&#32780;&#19988;&#20449;&#24687;&#20445;&#23384;&#20887;&#20313;&#31243;&#24230;&#36739;&#39640;&#12290;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#24433;&#21709;&#20197;&#21450;&#19981;&#21516;&#26550;&#26500;&#23398;&#20064;&#19981;&#21516;&#35821;&#35328;&#23646;&#24615;&#30340;&#24046;&#24322;&#20063;&#24471;&#21040;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35299;&#28145;&#24230;NLP&#27169;&#22411;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#21644;&#23427;&#20204;&#25152;&#25429;&#25417;&#21040;&#30340;&#30693;&#35782;&#26041;&#38754;&#65292;&#24050;&#32463;&#20570;&#20102;&#24456;&#22810;&#24037;&#20316;&#65292;&#20294;&#23545;&#20110;&#20010;&#21035;&#31070;&#32463;&#20803;&#30340;&#20851;&#27880;&#24456;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35821;&#35328;&#30456;&#20851;&#20998;&#26512;&#30340;&#25216;&#26415;&#26469;&#25552;&#21462;&#27169;&#22411;&#20013;&#30340;&#31361;&#20986;&#31070;&#32463;&#20803;&#65292;&#20197;&#20102;&#35299;&#36825;&#31181;&#30693;&#35782;&#22312;&#31070;&#32463;&#20803;&#20013;&#26159;&#22914;&#20309;&#20445;&#30041;&#30340;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31934;&#32454;&#30340;&#20998;&#26512;&#26469;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;i&#65289;&#25105;&#20204;&#33021;&#21542;&#35782;&#21035;&#20986;&#32593;&#32476;&#20013;&#25429;&#25417;&#29305;&#23450;&#35821;&#35328;&#23646;&#24615;&#30340;&#31070;&#32463;&#20803;&#23376;&#38598;&#65311;&#65288;ii&#65289;&#31070;&#32463;&#20803;&#22312;&#32593;&#32476;&#20013;&#26159;&#22914;&#20309;&#20998;&#24067;&#30340;&#65311;&#65288;iii&#65289;&#20449;&#24687;&#20445;&#23384;&#24471;&#26377;&#22810;&#20887;&#20313;&#65311;&#65288;iv&#65289;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#35843;&#25972;&#21040;&#19979;&#28216;&#30340;NLP&#20219;&#21153;&#65292;&#22914;&#20309;&#24433;&#21709;&#25152;&#23398;&#30340;&#35821;&#35328;&#30693;&#35782;&#65311;&#65288;v&#65289;&#19981;&#21516;&#30340;&#26550;&#26500;&#22312;&#23398;&#20064;&#19981;&#21516;&#30340;&#35821;&#35328;&#23646;&#24615;&#26041;&#38754;&#26377;&#20309;&#19981;&#21516;&#65311;&#25105;&#20204;&#30340;&#25968;&#25454;&#39537;&#21160;&#12289;&#23450;&#37327;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;
&lt;/p&gt;
&lt;p&gt;
While a lot of work has been done in understanding representations learned within deep NLP models and what knowledge they capture, little attention has been paid towards individual neurons. We present a technique called as Linguistic Correlation Analysis to extract salient neurons in the model, with respect to any extrinsic property - with the goal of understanding how such a knowledge is preserved within neurons. We carry out a fine-grained analysis to answer the following questions: (i) can we identify subsets of neurons in the network that capture specific linguistic properties? (ii) how localized or distributed neurons are across the network? iii) how redundantly is the information preserved? iv) how fine-tuning pre-trained models towards downstream NLP tasks, impacts the learned linguistic knowledge? iv) how do architectures vary in learning different linguistic properties? Our data-driven, quantitative analysis illuminates interesting findings: (i) we found small subsets of neuro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#30417;&#30563;&#22522;&#32447;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#20998;&#26512;&#20102;&#20256;&#32479;&#25163;&#24037;&#29305;&#24449;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2105.00815</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Weakly Supervised Relation Extraction. (arXiv:2105.00815v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.00815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#30417;&#30563;&#22522;&#32447;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#20998;&#26512;&#20102;&#20256;&#32479;&#25163;&#24037;&#29305;&#24449;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20449;&#24687;&#25277;&#21462;&#20197;&#21450;&#20854;&#20013;&#30340;&#23376;&#20219;&#21153;&#20851;&#31995;&#25277;&#21462;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#20851;&#31995;&#25277;&#21462;&#33021;&#22815;&#22312;&#21477;&#23376;&#20013;&#26816;&#27979;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#30446;&#21069;&#65292;&#35768;&#22810;&#39640;&#25928;&#30340;&#26041;&#27861;&#24050;&#34987;&#24212;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#12290;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#23588;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#38590;&#39064;&#12290;&#20854;&#20013;&#26368;&#20005;&#37325;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#38590;&#20197;&#33719;&#21462;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#31561;&#20110;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#21482;&#26377;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20851;&#27880;&#22914;&#20309;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#25105;&#20204;&#30340;&#30417;&#30563;&#22522;&#32447;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#29305;&#24449;&#26159;&#25913;&#36827;&#30417;&#30563;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#25163;&#24037;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#21644;&#26114;&#36149;&#30340;&#20154;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#21487;&#33021;&#20250;&#21463;&#21040;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen rapid development in Information Extraction, as well as its subtask, Relation Extraction. Relation Extraction is able to detect semantic relations between entities in sentences. Currently, many efficient approaches have been applied to relation extraction tasks. Supervised learning approaches especially have good performance. However, there are still many difficult challenges. One of the most serious problems is that manually labeled data is difficult to acquire. In most cases, limited data for supervised approaches equals lousy performance. Thus here, under the situation with only limited training data, we focus on how to improve the performance of our supervised baseline system with unsupervised pre-training. Feature is one of the key components in improving the supervised approaches. Traditional approaches usually apply hand-crafted features, which require expert knowledge and expensive human labor. However, this type of feature might suffer from data sparsity
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#23481;&#37327;&#27133;&#27880;&#24847;&#21147;&#27169;&#22411;&#20174;&#23383;&#31526;&#24207;&#21015;&#20013;&#23398;&#20064;&#25277;&#35937;&#26377;&#24847;&#20041;&#21333;&#20803;&#65292;&#25104;&#21151;&#21457;&#29616;&#20102;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#21333;&#20803;&#30456;&#20284;&#30340;&#12289;&#36866;&#29992;&#20110;&#26356;&#39640;&#32423;&#21035;&#25277;&#35937;&#30340;&#21487;&#25429;&#25417;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#21333;&#20803;&#12290;</title><link>http://arxiv.org/abs/2102.01223</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#23481;&#37327;&#27133;&#27880;&#24847;&#21147;&#20174;&#23383;&#31526;&#24207;&#21015;&#20013;&#35825;&#23548;&#26377;&#24847;&#20041;&#30340;&#21333;&#20803;
&lt;/p&gt;
&lt;p&gt;
Inducing Meaningful Units from Character Sequences with Dynamic Capacity Slot Attention. (arXiv:2102.01223v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.01223
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#23481;&#37327;&#27133;&#27880;&#24847;&#21147;&#27169;&#22411;&#20174;&#23383;&#31526;&#24207;&#21015;&#20013;&#23398;&#20064;&#25277;&#35937;&#26377;&#24847;&#20041;&#21333;&#20803;&#65292;&#25104;&#21151;&#21457;&#29616;&#20102;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#21333;&#20803;&#30456;&#20284;&#30340;&#12289;&#36866;&#29992;&#20110;&#26356;&#39640;&#32423;&#21035;&#25277;&#35937;&#30340;&#21487;&#25429;&#25417;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#31526;&#26412;&#36523;&#24182;&#19981;&#20256;&#36798;&#24847;&#20041;&#65292;&#20294;&#23383;&#31526;&#24207;&#21015;&#21364;&#21487;&#20197;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#23383;&#31526;&#24207;&#21015;&#20013;&#30340;&#25277;&#35937;&#26377;&#24847;&#20041;&#21333;&#20803;&#12290;&#25105;&#20204;&#30340;&#21160;&#24577;&#23481;&#37327;&#27133;&#27880;&#24847;&#21147;&#27169;&#22411;&#19981;&#26159;&#23545;&#24207;&#21015;&#36827;&#34892;&#20998;&#21106;&#65292;&#32780;&#26159;&#21457;&#29616;&#24207;&#21015;&#20013;&#23545;&#35937;&#30340;&#36830;&#32493;&#34920;&#31034;&#65292;&#25193;&#23637;&#20102;&#22270;&#20687;&#20013;&#23545;&#35937;&#21457;&#29616;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#35821;&#35328;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#21644;&#21453;&#21521;&#25506;&#27979;&#20998;&#31867;&#22120;&#35780;&#20272;&#25152;&#24471;&#21040;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#21457;&#29616;&#20102;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#21333;&#20803;&#22312;&#24418;&#24335;&#12289;&#20869;&#23481;&#21644;&#25277;&#35937;&#32423;&#21035;&#19978;&#30456;&#20284;&#30340;&#21333;&#20803;&#65292;&#24182;&#26174;&#31034;&#20102;&#22312;&#26356;&#39640;&#32423;&#21035;&#30340;&#25277;&#35937;&#20013;&#25429;&#25417;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Characters do not convey meaning, but sequences of characters do. We propose an unsupervised distributional method to learn the abstract meaningful units in a sequence of characters. Rather than segmenting the sequence, our Dynamic Capacity Slot Attention model discovers continuous representations of the objects in the sequence, extending an architecture for object discovery in images. We train our model on different languages and evaluate the quality of the obtained representations with forward and reverse probing classifiers. These experiments show that our model succeeds in discovering units which are similar to those proposed previously in form, content and level of abstraction, and which show promise for capturing meaningful information at a higher level of abstraction.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20154;&#31867;&#30699;&#27491;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26631;&#27880;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#65292;&#25910;&#38598;&#32416;&#38169;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#33267;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#23558;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24230;&#25552;&#21319;&#20102;1.7&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2102.00225</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#30340;&#32416;&#38169;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning From How Humans Correct. (arXiv:2102.00225v14 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.00225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20154;&#31867;&#30699;&#27491;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26631;&#27880;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#65292;&#25910;&#38598;&#32416;&#38169;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#33267;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#23558;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24230;&#25552;&#21319;&#20102;1.7&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#19968;&#23450;&#25968;&#37327;&#30340;&#22122;&#22768;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#21040;&#22122;&#22768;&#25968;&#25454;&#24182;&#25163;&#21160;&#37325;&#26032;&#26631;&#27880;&#23427;&#20204;&#65292;&#21516;&#26102;&#25910;&#38598;&#32416;&#38169;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#32416;&#38169;&#20449;&#24687;&#34701;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#20154;&#31867;&#30693;&#36947;&#22914;&#20309;&#32416;&#27491;&#22122;&#22768;&#25968;&#25454;&#65292;&#22240;&#27492;&#32416;&#38169;&#20449;&#24687;&#21487;&#20197;&#27880;&#20837;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#22312;&#33258;&#24049;&#30340;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#25163;&#21160;&#26631;&#27880;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#37325;&#26032;&#26631;&#27880;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#65292;&#20197;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#24037;&#19994;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20998;&#31867;&#20934;&#30830;&#24230;&#20174;91.7%&#25552;&#21319;&#21040;92.5%&#12290;91.7%&#30340;&#20934;&#30830;&#24230;&#26159;&#22312;&#20462;&#27491;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#23427;&#23558;&#22522;&#32447;&#20934;&#30830;&#24230;&#20174;83.3%&#25552;&#21319;&#21040;91.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry NLP application, our manually labeled data has a certain number of noisy data. We present a simple method to find the noisy data and re-label them manually, meanwhile we collect the correction information. Then we present novel method to incorporate the human correction information into deep learning model. Human know how to correct noisy data. So the correction information can be inject into deep learning model. We do the experiment on our own text classification dataset, which is manually labeled, because we re-label the noisy data in our dataset for our industry application. The experiment result shows that our method improve the classification accuracy from 91.7% to 92.5%. The 91.7% accuracy is trained on the corrected dataset, which improve the baseline from 83.3% to 91.7%.
&lt;/p&gt;</description></item></channel></rss>