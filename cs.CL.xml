<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20351;&#29992;&#20195;&#30721;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#21644;&#25512;&#23548;&#65292;&#20174;&#32780;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21253;&#21547;&#25968;&#23398;&#38382;&#39064;&#21644;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#20102;&#23450;&#21046;&#30340;&#24494;&#35843;&#21644;&#25512;&#29702;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#19978;&#29983;&#25104;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340; MathCoder &#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03731</link><description>&lt;p&gt;
MathCoder: &#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#20013; LLMs &#20013;&#26080;&#32541;&#20195;&#30721;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning. (arXiv:2310.03731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20351;&#29992;&#20195;&#30721;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#21644;&#25512;&#23548;&#65292;&#20174;&#32780;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21253;&#21547;&#25968;&#23398;&#38382;&#39064;&#21644;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#20102;&#23450;&#21046;&#30340;&#24494;&#35843;&#21644;&#25512;&#29702;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#19978;&#29983;&#25104;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340; MathCoder &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340; GPT-4 &#20195;&#30721;&#35299;&#37322;&#22120;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#25512;&#29702;&#65292;&#29983;&#25104;&#20195;&#30721;&#65292;&#25191;&#34892;&#20195;&#30721;&#65292;&#24182;&#26681;&#25454;&#25191;&#34892;&#36755;&#20986;&#32487;&#32493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#24494;&#35843;&#24320;&#28304;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20351;&#29992;&#20195;&#30721;&#26469;&#24314;&#27169;&#21644;&#25512;&#23548;&#25968;&#23398;&#26041;&#31243;&#65292;&#24182;&#20174;&#32780;&#22686;&#24378;&#20854;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21253;&#21547;&#25968;&#23398;&#38382;&#39064;&#21450;&#20854;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#39062;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; MathCodeInstruct&#12290;&#27599;&#20010;&#35299;&#20915;&#26041;&#26696;&#37117;&#20132;&#38169;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25191;&#34892;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#30417;&#30563;&#24494;&#35843;&#21644;&#25512;&#29702;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#24471;&#21040;&#20102; MathCoder &#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#22522;&#20110;&#20195;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;MathCoder &#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#35821;&#38899;&#21040;&#25991;&#23383;&#32763;&#35793;&#26041;&#27861;&#65292;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#22522;&#30784;&#19978;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#24615;&#33021;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#35821;&#38899;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.03724</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#35821;&#38899;&#21040;&#25991;&#23383;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer. (arXiv:2310.03724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#35821;&#38899;&#21040;&#25991;&#23383;&#32763;&#35793;&#26041;&#27861;&#65292;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#22522;&#30784;&#19978;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#24615;&#33021;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#35821;&#38899;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20849;&#20139;&#22266;&#23450;&#22823;&#23567;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#29420;&#31435;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32467;&#21512;&#36215;&#26469;&#21487;&#20197;&#22312;&#35821;&#38899;&#21040;&#25991;&#23383;&#32763;&#35793;&#20013;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#22810;&#35821;&#35328;&#35757;&#32451;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#35821;&#38899;&#32763;&#35793;&#20013;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#29978;&#33267;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#32988;&#36807;&#22522;&#20110;XLSR&#30340;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown that independently trained encoders and decoders, combined through a shared fixed-size representation, can achieve competitive performance in speech-to-text translation. In this work, we show that this type of approach can be further improved with multilingual training. We observe significant improvements in zero-shot cross-modal speech translation, even outperforming a supervised approach based on XLSR for several languages.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;RLHF&#20013;&#22870;&#21169;&#21644;&#38271;&#24230;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20248;&#21270;&#21709;&#24212;&#38271;&#24230;&#26159;RLHF&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.03716</link><description>&lt;p&gt;
&#19968;&#26465;&#28459;&#38271;&#20043;&#36335;&#65306;&#25506;&#31350;RLHF&#20013;&#30340;&#38271;&#24230;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Long Way to Go: Investigating Length Correlations in RLHF. (arXiv:2310.03716v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03716
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;RLHF&#20013;&#22870;&#21169;&#21644;&#38271;&#24230;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20248;&#21270;&#21709;&#24212;&#38271;&#24230;&#26159;RLHF&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#24320;&#28304;&#20559;&#22909;&#25968;&#25454;&#38598;&#21644;&#22870;&#21169;&#27169;&#22411;&#20351;&#24471;&#22312;&#36890;&#29992;&#32842;&#22825;&#35774;&#32622;&#20043;&#22806;&#36827;&#34892;&#26356;&#24191;&#27867;&#30340;&#23454;&#39564;&#25104;&#20026;&#21487;&#33021;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#20351;&#31995;&#32479;&#22312;&#32593;&#39029;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#22810;&#36718;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#26356;&#21152;&#8220;&#26377;&#29992;&#8221;&#12290;&#24403;&#20248;&#21270;&#26377;&#29992;&#24615;&#26102;&#65292;&#25105;&#20204;&#19968;&#30452;&#35266;&#23519;&#21040;RLHF&#20250;&#39537;&#20351;&#27169;&#22411;&#20135;&#29983;&#26356;&#38271;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#21709;&#24212;&#38271;&#24230;&#36827;&#34892;&#20248;&#21270;&#26159;RLHF&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#21462;&#24471;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38024;&#23545;&#26377;&#29992;&#24615;&#35757;&#32451;&#30340;&#19977;&#20010;&#24320;&#28304;&#20559;&#22909;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#30340;&#22870;&#21169;&#19982;&#38271;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#38271;&#24230;&#19982;&#22870;&#21169;&#24378;&#28872;&#30456;&#20851;&#65292;&#22870;&#21169;&#20998;&#25968;&#30340;&#25552;&#39640;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#36890;&#36807;&#25913;&#21464;&#36755;&#20986;&#38271;&#24230;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;RL&#21644;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#24178;&#39044;&#65292;&#30475;&#26159;&#21542;&#33021;&#22815;&#36798;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Great successes have been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models. Open-source preference datasets and reward models have enabled wider experimentation beyond generic chat settings, particularly to make systems more "helpful" for tasks like web question answering, summarization, and multi-turn dialogue. When optimizing for helpfulness, RLHF has been consistently observed to drive models to produce longer outputs. This paper demonstrates that optimizing for response length is a significant factor behind RLHF's reported improvements in these settings. First, we study the relationship between reward and length for reward models trained on three open-source preference datasets for helpfulness. Here, length correlates strongly with reward, and improvements in reward score are driven in large part by shifting the distribution over output lengths. We then explore interventions during both RL and reward model learning to see if we can ach
&lt;/p&gt;</description></item><item><title>DSPy&#26159;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#65292;&#23558;LM&#27969;&#27700;&#32447;&#25277;&#35937;&#20026;&#25991;&#26412;&#36716;&#25442;&#22270;&#65292;&#36890;&#36807;&#22768;&#26126;&#24615;&#27169;&#22359;&#35843;&#29992;LM&#23454;&#29616;&#20248;&#21270;&#65292;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#30340;&#25512;&#29702;&#38382;&#39064;&#21644;&#25968;&#23398;&#38382;&#39064;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.03714</link><description>&lt;p&gt;
DSPy: &#23558;&#22768;&#26126;&#24615;&#35821;&#35328;&#27169;&#22411;&#35843;&#29992;&#32534;&#35793;&#25104;&#33258;&#25105;&#25913;&#36827;&#30340;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. (arXiv:2310.03714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03714
&lt;/p&gt;
&lt;p&gt;
DSPy&#26159;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#65292;&#23558;LM&#27969;&#27700;&#32447;&#25277;&#35937;&#20026;&#25991;&#26412;&#36716;&#25442;&#22270;&#65292;&#36890;&#36807;&#22768;&#26126;&#24615;&#27169;&#22359;&#35843;&#29992;LM&#23454;&#29616;&#20248;&#21270;&#65292;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#30340;&#25512;&#29702;&#38382;&#39064;&#21644;&#25968;&#23398;&#38382;&#39064;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ML&#31038;&#21306;&#27491;&#22312;&#24555;&#36895;&#25506;&#32034;&#29992;&#20110;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;(LMs)&#21644;&#23558;&#23427;&#20204;&#22534;&#21472;&#25104;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#27969;&#27700;&#32447;&#30340;&#25216;&#26415;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;LM&#27969;&#27700;&#32447;&#36890;&#24120;&#20351;&#29992;&#30828;&#32534;&#30721;&#30340;"&#25552;&#31034;&#27169;&#26495;"&#26469;&#23454;&#29616;&#65292;&#21363;&#36890;&#36807;&#35797;&#38169;&#21457;&#29616;&#30340;&#20887;&#38271;&#23383;&#31526;&#20018;&#12290;&#20026;&#20102;&#26356;&#31995;&#32479;&#22320;&#24320;&#21457;&#21644;&#20248;&#21270;LM&#27969;&#27700;&#32447;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DSPy&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#25991;&#26412;&#36716;&#25442;&#22270;&#30340;&#24418;&#24335;&#25277;&#35937;LM&#27969;&#27700;&#32447;&#30340;&#32534;&#31243;&#27169;&#22411;&#65292;&#21363;&#36890;&#36807;&#22768;&#26126;&#24615;&#27169;&#22359;&#35843;&#29992;LM&#30340;&#21629;&#20196;&#24335;&#35745;&#31639;&#22270;&#12290;DSPy&#27169;&#22359;&#26159;&#21442;&#25968;&#21270;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#21019;&#24314;&#21644;&#25910;&#38598;&#31034;&#20363;&#26469;&#23398;&#20064;&#22914;&#20309;&#24212;&#29992;&#25552;&#31034;&#12289;&#24494;&#35843;&#12289;&#22686;&#24378;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32534;&#35793;&#22120;&#65292;&#21487;&#20197;&#20248;&#21270;&#20219;&#20309;DSPy&#27969;&#27700;&#32447;&#20197;&#26368;&#22823;&#21270;&#32473;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#26174;&#31034;&#20986;&#31616;&#27905;&#30340;DSPy&#31243;&#24207;&#21487;&#20197;&#34920;&#36798;&#21644;&#20248;&#21270;&#22797;&#26434;&#30340;&#25512;&#29702;&#25968;&#23398;&#38382;&#39064;&#12289;&#30331;&#24405;&#26085;&#24535;&#38382;&#39064;&#31561;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded "prompt templates", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#25351;&#23548;&#30340;&#26041;&#24335;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03710</link><description>&lt;p&gt;
&#20195;&#29702;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#36890;&#29992;&#30340;&#38646;-shot&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Agent Instructs Large Language Models to be General Zero-Shot Reasoners. (arXiv:2310.03710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#25351;&#23548;&#30340;&#26041;&#24335;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#20027;&#20195;&#29702;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#36827;&#19968;&#27493;&#37322;&#25918;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#26356;&#22810;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#29983;&#25104;&#12289;&#20998;&#31867;&#21644;&#25512;&#29702;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#65292;&#24182;&#22312;&#25105;&#20204;&#35780;&#20272;&#30340;29&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#22312;20&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;Vicuna-13b&#65288;13.3%&#65289;&#65292;Llama-2-70b-chat&#65288;23.2%&#65289;&#21644;GPT-3.5 Turbo&#65288;17.0%&#65289;&#12290;&#19982;&#38646;-shot&#24605;&#32500;&#38142;&#30456;&#27604;&#65292;&#25105;&#20204;&#23545;&#25512;&#29702;&#30340;&#25913;&#36827;&#24456;&#26126;&#26174;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;10.5%&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Llama-2-70b-chat&#30340;&#24615;&#33021;&#36229;&#36807;&#38646;-shot GPT-3.5 Turbo 10.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.
&lt;/p&gt;</description></item><item><title>&#24494;&#35843;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#20250;&#29306;&#29298;&#23433;&#20840;&#24615;&#65292;&#21363;&#20351;&#29992;&#25143;&#27809;&#26377;&#24694;&#24847;&#24847;&#22270;&#12290;&#36890;&#36807;&#25932;&#23545;&#35774;&#35745;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#25968;10&#20010;&#65292;&#20063;&#21487;&#20197;&#30772;&#22351;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#12290;&#36825;&#31181;&#23433;&#20840;&#39118;&#38505;&#23384;&#22312;&#20110;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.03693</link><description>&lt;p&gt;
&#35843;&#25972;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#20250;&#29306;&#29298;&#23433;&#20840;&#24615;&#65292;&#21363;&#20351;&#29992;&#25143;&#27809;&#26377;&#24847;&#22270;&#65281;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!. (arXiv:2310.03693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03693
&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#20250;&#29306;&#29298;&#23433;&#20840;&#24615;&#65292;&#21363;&#20351;&#29992;&#25143;&#27809;&#26377;&#24694;&#24847;&#24847;&#22270;&#12290;&#36890;&#36807;&#25932;&#23545;&#35774;&#35745;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#25968;10&#20010;&#65292;&#20063;&#21487;&#20197;&#30772;&#22351;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#12290;&#36825;&#31181;&#23433;&#20840;&#39118;&#38505;&#23384;&#22312;&#20110;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19979;&#28216;&#29992;&#20363;&#30340;&#20248;&#21270;&#36890;&#24120;&#28041;&#21450;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#26469;&#23450;&#21046;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;Meta&#21457;&#24067;Llama&#27169;&#22411;&#21644;OpenAI&#30340;GPT-3.5 Turbo&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#24494;&#35843;API&#20063;&#40723;&#21169;&#36825;&#31181;&#20570;&#27861;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#33258;&#23450;&#20041;&#24494;&#35843;&#30340;&#23433;&#20840;&#25104;&#26412;&#26159;&#22810;&#23569;&#65311;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#23433;&#20840;&#23545;&#40784;&#22522;&#30784;&#35774;&#26045;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#23475;&#34892;&#20026;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#28085;&#30422;&#24403;&#24494;&#35843;&#29305;&#26435;&#25193;&#23637;&#32473;&#32456;&#31471;&#29992;&#25143;&#26102;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#32418;&#38431;&#30740;&#31350;&#21457;&#29616;&#65292;&#21482;&#38656;&#20960;&#20010;&#25932;&#23545;&#35774;&#35745;&#30340;&#35757;&#32451;&#26679;&#26412;&#23601;&#21487;&#20197;&#30772;&#22351;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21482;&#20351;&#29992;10&#20010;&#36825;&#26679;&#30340;&#31034;&#20363;&#22312;OpenAI&#30340;API&#20013;&#20197;&#19981;&#21040;0.20&#32654;&#20803;&#30340;&#25104;&#26412;&#23558;GPT-3.5 Turbo&#30340;&#23433;&#20840;&#20445;&#25252;&#35299;&#38500;&#20102;&#65292;&#20351;&#27169;&#22411;&#23545;&#20960;&#20046;&#20219;&#20309;&#26377;&#23475;&#25351;&#20196;&#37117;&#26377;&#21709;&#24212;&#12290;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#21363;&#20351;&#27809;&#26377;&#24694;&#24847;&#24847;&#22270;&#65292;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#20063;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious 
&lt;/p&gt;</description></item><item><title>DecoderLens&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#27169;&#22411;&#20013;&#20869;&#37096;&#29366;&#24577;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#35753;&#35299;&#30721;&#22120;&#36328;&#36234;&#20013;&#38388;&#32534;&#30721;&#22120;&#23618;&#30340;&#34920;&#31034;&#36827;&#34892;&#20132;&#21449;&#27880;&#24847;&#65292;DecoderLens&#23558;&#20808;&#21069;&#26080;&#27861;&#35299;&#37322;&#30340;&#21521;&#37327;&#34920;&#31034;&#26144;&#23556;&#21040;&#21487;&#35299;&#37322;&#30340;&#21333;&#35789;&#25110;&#31526;&#21495;&#24207;&#21015;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22312;&#20302;&#23618;&#25110;&#20013;&#38388;&#23618;&#35299;&#20915;&#30340;&#29305;&#23450;&#23376;&#20219;&#21153;&#65292;&#20026;&#20449;&#24687;&#27969;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.03686</link><description>&lt;p&gt;
DecoderLens: &#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#30340;&#36880;&#23618;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers. (arXiv:2310.03686v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03686
&lt;/p&gt;
&lt;p&gt;
DecoderLens&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#27169;&#22411;&#20013;&#20869;&#37096;&#29366;&#24577;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#35753;&#35299;&#30721;&#22120;&#36328;&#36234;&#20013;&#38388;&#32534;&#30721;&#22120;&#23618;&#30340;&#34920;&#31034;&#36827;&#34892;&#20132;&#21449;&#27880;&#24847;&#65292;DecoderLens&#23558;&#20808;&#21069;&#26080;&#27861;&#35299;&#37322;&#30340;&#21521;&#37327;&#34920;&#31034;&#26144;&#23556;&#21040;&#21487;&#35299;&#37322;&#30340;&#21333;&#35789;&#25110;&#31526;&#21495;&#24207;&#21015;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22312;&#20302;&#23618;&#25110;&#20013;&#38388;&#23618;&#35299;&#20915;&#30340;&#29305;&#23450;&#23376;&#20219;&#21153;&#65292;&#20026;&#20449;&#24687;&#27969;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#24110;&#21161;&#35299;&#37322;Transformer&#27169;&#22411;&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#22797;&#26434;&#24615;&#32423;&#21035;&#19978;&#24037;&#20316;&#12290;&#22312;&#36825;&#37324;&#65292;&#20026;&#20102;&#20998;&#26512;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;DecoderLens&#12290;&#21463;&#21040;&#20102;LogitLens&#65288;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#30340;Transformer&#65289;&#30340;&#21551;&#21457;&#65292;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#35299;&#30721;&#22120;&#36328;&#36234;&#20013;&#38388;&#32534;&#30721;&#22120;&#23618;&#30340;&#34920;&#31034;&#36827;&#34892;&#20132;&#21449;&#27880;&#24847;&#65292;&#32780;&#19981;&#26159;&#20687;&#24120;&#35268;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20013;&#37027;&#26679;&#20351;&#29992;&#26368;&#32456;&#30340;&#32534;&#30721;&#22120;&#36755;&#20986;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#20808;&#21069;&#26080;&#27861;&#35299;&#37322;&#30340;&#21521;&#37327;&#34920;&#31034;&#26144;&#23556;&#21040;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21333;&#35789;&#25110;&#31526;&#21495;&#24207;&#21015;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24212;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#12289;&#36923;&#36753;&#25512;&#29702;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#26426;&#22120;&#32763;&#35793;&#35757;&#32451;&#27169;&#22411;&#30340;DecoderLens&#30340;&#32467;&#26524;&#12290;DecoderLens&#22312;&#20302;&#23618;&#25110;&#20013;&#38388;&#23618;&#25581;&#31034;&#20102;&#35299;&#20915;&#30340;&#20960;&#20010;&#29305;&#23450;&#23376;&#20219;&#21153;&#65292;&#20026;&#36825;&#20010;&#37325;&#35201;&#27169;&#22411;&#31867;&#21035;&#20013;&#30340;&#32534;&#30721;&#22120;&#32452;&#20214;&#20869;&#30340;&#20449;&#24687;&#27969;&#25552;&#20379;&#20102;&#26032;&#30340;&#20809;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, many interpretability methods have been proposed to help interpret the internal states of Transformer-models, at different levels of precision and complexity. Here, to analyze encoder-decoder Transformers, we propose a simple, new method: DecoderLens. Inspired by the LogitLens (for decoder-only Transformers), this method involves allowing the decoder to cross-attend representations of intermediate encoder layers instead of using the final encoder output, as is normally done in encoder-decoder models. The method thus maps previously uninterpretable vector representations to human-interpretable sequences of words or symbols. We report results from the DecoderLens applied to models trained on question answering, logical reasoning, speech recognition and machine translation. The DecoderLens reveals several specific subtasks that are solved at low or intermediate layers, shedding new light on the information flow inside the encoder component of this important class of model
&lt;/p&gt;</description></item><item><title>GoLLIE &#26159;&#19968;&#20010;&#36981;&#24490;&#27880;&#37322;&#25351;&#21335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#20197;&#25913;&#36827;&#26410;&#35265;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03668</link><description>&lt;p&gt;
GoLLIE:&#27880;&#37322;&#25351;&#21335;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction. (arXiv:2310.03668v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03668
&lt;/p&gt;
&lt;p&gt;
GoLLIE &#26159;&#19968;&#20010;&#36981;&#24490;&#27880;&#37322;&#25351;&#21335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#20197;&#25913;&#36827;&#26410;&#35265;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#32467;&#21512;&#25351;&#23548;&#35843;&#20248;&#24050;&#32463;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462; (IE) &#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#33853;&#21518;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;IE &#20219;&#21153;&#30340;&#29305;&#28857;&#26159;&#22797;&#26434;&#30340;&#27880;&#37322;&#25351;&#21335;&#65292;&#25551;&#36848;&#20219;&#21153;&#24182;&#32473;&#20986;&#31034;&#20363;&#32473;&#20154;&#31867;&#12290;&#20808;&#21069;&#21033;&#29992;&#36825;&#26679;&#30340;&#20449;&#24687;&#30340;&#23581;&#35797;&#37117;&#22833;&#36133;&#20102;&#65292;&#21363;&#20351;&#20351;&#29992;&#26368;&#22823;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#20063;&#19981;&#33021;&#30452;&#25509;&#36981;&#24490;&#25351;&#21335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20449;&#24687;&#25277;&#21462;&#30340;&#25351;&#21335;&#36981;&#24490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; GoLLIE (Guideline-following Large Language Model for IE)&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24494;&#35843;&#20197;&#36981;&#23432;&#27880;&#37322;&#25351;&#21335;&#65292;&#20174;&#32780;&#33021;&#22815;&#25913;&#36827;&#26410;&#35265; IE &#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;&#20840;&#38754;&#30340;&#35780;&#20272;&#23454;&#35777;&#34920;&#26126;&#65292;GoLLIE &#33021;&#22815;&#27867;&#21270;&#24182;&#36981;&#24490;&#26410;&#35265;&#25351;&#21335;&#65292;&#22312;&#38646;&#26679;&#26412;&#20449;&#24687;&#25277;&#21462;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#23581;&#35797;&#12290;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#35814;&#32454;&#30340;&#25351;&#21335;&#26159;&#21462;&#24471;&#33391;&#22909;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines which describe the task and give examples to humans. Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out-of-the-box. In this paper we propose GoLLIE (Guideline-following Large Language Model for IE), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines. Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed guidelines is key for good results.
&lt;/p&gt;</description></item><item><title>MapperGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38142;&#25509;&#21644;&#26144;&#23556;&#24037;&#20855;&#65292;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#26144;&#23556;&#20013;&#30340;&#35789;&#27719;&#27169;&#31946;&#38382;&#39064;&#21644;&#25163;&#21160;&#26144;&#23556;&#32454;&#21270;&#30340;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2310.03666</link><description>&lt;p&gt;
MapperGPT:&#29992;&#20110;&#38142;&#25509;&#21644;&#26144;&#23556;&#23454;&#20307;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MapperGPT: Large Language Models for Linking and Mapping Entities. (arXiv:2310.03666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03666
&lt;/p&gt;
&lt;p&gt;
MapperGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38142;&#25509;&#21644;&#26144;&#23556;&#24037;&#20855;&#65292;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#26144;&#23556;&#20013;&#30340;&#35789;&#27719;&#27169;&#31946;&#38382;&#39064;&#21644;&#25163;&#21160;&#26144;&#23556;&#32454;&#21270;&#30340;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#65292;&#23545;&#40784;&#26415;&#35821;&#36164;&#28304;&#65288;&#21253;&#25324;&#26412;&#20307;&#12289;&#21463;&#25511;&#35789;&#27719;&#12289;&#20998;&#31867;&#27861;&#21644;&#20540;&#38598;&#65289;&#26159;&#25968;&#25454;&#38598;&#25104;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#23454;&#20307;&#26144;&#23556;&#26159;&#30830;&#23450;&#36825;&#20123;&#36164;&#28304;&#20013;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#30340;&#36807;&#31243;&#65292;&#20363;&#22914;&#22522;&#22240;&#26631;&#35782;&#31526;&#12289;&#30142;&#30149;&#27010;&#24565;&#25110;&#21270;&#23398;&#23454;&#20307;&#26631;&#35782;&#31526;&#12290;&#35768;&#22810;&#24037;&#20855;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#65292;&#22522;&#20110;&#24120;&#35265;&#32467;&#26500;&#29305;&#24449;&#21644;&#35789;&#27719;&#20449;&#24687;&#65288;&#22914;&#26631;&#31614;&#21644;&#21516;&#20041;&#35789;&#65289;&#26469;&#35745;&#31639;&#36825;&#31181;&#26144;&#23556;&#12290;&#29305;&#21035;&#26159;&#35789;&#27719;&#26041;&#27861;&#36890;&#24120;&#25552;&#20379;&#38750;&#24120;&#39640;&#30340;&#21484;&#22238;&#29575;&#65292;&#20294;&#30001;&#20110;&#35789;&#20041;&#27169;&#31946;&#65292;&#31934;&#24230;&#36739;&#20302;&#12290;&#22240;&#27492;&#65292;&#26144;&#23556;&#24037;&#20316;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#20154;&#24037;&#31574;&#21010;&#36827;&#34892;&#32321;&#29712;&#32780;&#36153;&#26102;&#30340;&#25163;&#21160;&#26144;&#23556;&#32454;&#21270;&#36807;&#31243;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;ChatGPT&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#36890;&#29992;&#33021;&#21147;&#65292;&#21487;&#20197;&#25191;&#34892;&#24191;&#27867;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#38382;&#31572;&#21644;&#20449;&#24687;&#25552;&#21462;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MapperGPT&#65292;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38142;&#25509;&#21644;&#26144;&#23556;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning terminological resources, including ontologies, controlled vocabularies, taxonomies, and value sets is a critical part of data integration in many domains such as healthcare, chemistry, and biomedical research. Entity mapping is the process of determining correspondences between entities across these resources, such as gene identifiers, disease concepts, or chemical entity identifiers. Many tools have been developed to compute such mappings based on common structural features and lexical information such as labels and synonyms. Lexical approaches in particular often provide very high recall, but low precision, due to lexical ambiguity. As a consequence of this, mapping efforts often resort to a labor intensive manual mapping refinement through a human curator.  Large Language Models (LLMs), such as the ones employed by ChatGPT, have generalizable abilities to perform a wide range of tasks, including question-answering and information extraction. Here we present MapperGPT, an a
&lt;/p&gt;</description></item><item><title>TRAM&#26159;&#19968;&#31181;&#26725;&#25509;&#20449;&#20219;&#21306;&#22495;&#21644;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25439;&#22833;&#26354;&#38754;&#30340;&#26354;&#29575;&#26469;&#25552;&#20379;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#23427;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20248;&#21270;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#26469;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;&#21644;SAM&#39118;&#26684;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#32479;&#19968;&#21442;&#25968;&#21644;&#34920;&#31034;&#31354;&#38388;&#24179;&#28369;&#26041;&#27861;&#12290;TRAM&#22312;&#20445;&#25345;&#39044;&#35757;&#32451;&#32467;&#26500;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#21644;&#24179;&#28369;&#12289;&#26377;&#20449;&#24687;&#37327;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.03646</link><description>&lt;p&gt;
TRAM: &#26725;&#25509;&#20449;&#20219;&#21306;&#22495;&#21644;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
TRAM: Bridging Trust Regions and Sharpness Aware Minimization. (arXiv:2310.03646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03646
&lt;/p&gt;
&lt;p&gt;
TRAM&#26159;&#19968;&#31181;&#26725;&#25509;&#20449;&#20219;&#21306;&#22495;&#21644;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25439;&#22833;&#26354;&#38754;&#30340;&#26354;&#29575;&#26469;&#25552;&#20379;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#23427;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20248;&#21270;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#26469;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;&#21644;SAM&#39118;&#26684;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#32479;&#19968;&#21442;&#25968;&#21644;&#34920;&#31034;&#31354;&#38388;&#24179;&#28369;&#26041;&#27861;&#12290;TRAM&#22312;&#20445;&#25345;&#39044;&#35757;&#32451;&#32467;&#26500;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#21644;&#24179;&#28369;&#12289;&#26377;&#20449;&#24687;&#37327;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#31354;&#38388;&#20013;&#25439;&#22833;&#26354;&#38754;&#30340;&#26354;&#29575;&#65292;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#19981;&#26159;&#20851;&#27880;&#21442;&#25968;&#65292;&#32780;&#26159;&#32771;&#34385;&#21040;&#34920;&#31034;&#30340;&#21487;&#36716;&#31227;&#24615;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#22312;&#24494;&#35843;&#35774;&#32622;&#20013;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;&#20026;&#20102;&#40723;&#21169;&#20445;&#30041;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25216;&#33021;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#39044;&#35757;&#32451;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20449;&#20219;&#21306;&#22495;&#36793;&#30028;&#22312;&#36825;&#20004;&#31181;&#20248;&#21270;&#34920;&#38754;&#19978;&#36890;&#30693;SAM&#39118;&#26684;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#32479;&#19968;&#20102;&#21442;&#25968;&#21644;&#34920;&#31034;&#31354;&#38388;&#24179;&#28369;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Trust Region Aware Minimization (TRAM)&#65292;&#19968;&#31181;&#20248;&#21270;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#21644;&#24179;&#28369;&#12289;&#26377;&#20449;&#24687;&#37327;&#30340;&#34920;&#31034;&#30340;&#24494;&#35843;&#31639;&#27861;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#39044;&#20808;&#35757;&#32451;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;TRAM&#20248;&#20110;&#38160;&#24230;&#24863;&#30693;&#21644;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
By reducing the curvature of the loss surface in the parameter space, Sharpness-aware minimization (SAM) yields widespread robustness improvement under domain transfer. Instead of focusing on parameters, however, this work considers the transferability of representations as the optimization target for out-of-domain generalization in a fine-tuning setup. To encourage the retention of transferable representations, we consider trust region-based fine-tuning methods, which exploit task-specific skills without forgetting task-agnostic representations from pre-training. We unify parameter- and representation-space smoothing approaches by using trust region bounds to inform SAM-style regularizers on both of these optimization surfaces. We propose Trust Region Aware Minimization (TRAM), a fine-tuning algorithm that optimizes for flat minima and smooth, informative representations without forgetting pre-trained structure. We find that TRAM outperforms both sharpness-aware and trust region-based
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#22312;&#32654;&#27954;&#22303;&#33879;&#35821;&#35328;&#19978;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;ASR&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#26174;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03639</link><description>&lt;p&gt;
&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23545;&#32654;&#27954;&#22303;&#33879;&#35821;&#35328;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating Self-Supervised Speech Representations for Indigenous American Languages. (arXiv:2310.03639v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#22312;&#32654;&#27954;&#22303;&#33879;&#35821;&#35328;&#19978;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;ASR&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#26174;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#37327;&#30340;&#26080;&#26631;&#27880;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#35780;&#20272;&#26041;&#38754;&#65292;&#30446;&#21069;&#30340;&#36827;&#23637;&#20165;&#38598;&#20013;&#22312;&#32771;&#34385;&#33521;&#35821;&#30340;&#21333;&#35821;&#27169;&#22411;&#19978;&#12290;&#24456;&#23569;&#26377;&#27169;&#22411;&#32771;&#34385;&#20854;&#20182;&#35821;&#35328;&#65292;&#23588;&#20854;&#26159;&#22303;&#33879;&#35821;&#35328;&#12290;&#25105;&#20204;&#22312;ASRU 2023 ML-SUPERB Challenge&#30340;&#26032;&#35821;&#35328;&#36187;&#36947;&#20013;&#25552;&#20132;&#20102;&#19968;&#20010;&#29992;&#20110;Quechua&#30340;ASR&#35821;&#26009;&#24211;&#65292;&#23427;&#26159;&#19968;&#31181;&#21335;&#32654;&#22303;&#33879;&#35821;&#35328;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;Quechua&#21644;&#20854;&#20182;6&#31181;&#22303;&#33879;&#35821;&#35328;&#65288;&#22914;Guarani&#21644;Bribri&#65289;&#30340;&#20302;&#36164;&#28304;ASR&#19978;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of self-supervision to speech representation learning has garnered significant interest in recent years, due to its scalability to large amounts of unlabeled data. However, much progress, both in terms of pre-training and downstream evaluation, has remained concentrated in monolingual models that only consider English. Few models consider other languages, and even fewer consider indigenous ones. In our submission to the New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR corpus for Quechua, an indigenous South American Language. We benchmark the efficacy of large SSL models on Quechua, along with 6 other indigenous languages such as Guarani and Bribri, on low-resource ASR. Our results show surprisingly strong performance by state-of-the-art SSL models, showing the potential generalizability of large-scale models to real-world data.
&lt;/p&gt;</description></item><item><title>CLEVRER-Humans&#26159;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#21028;&#26029;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#26469;&#35299;&#20915;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#35821;&#35328;&#25551;&#36848;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#21644;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03635</link><description>&lt;p&gt;
CLEVRER-Humans: &#29992;&#20154;&#31867;&#30340;&#26041;&#24335;&#25551;&#36848;&#29289;&#29702;&#21644;&#22240;&#26524;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
CLEVRER-Humans: Describing Physical and Causal Events the Human Way. (arXiv:2310.03635v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03635
&lt;/p&gt;
&lt;p&gt;
CLEVRER-Humans&#26159;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#21028;&#26029;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#26469;&#35299;&#20915;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#35821;&#35328;&#25551;&#36848;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#21644;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#33021;&#22815;&#25512;&#29702;&#29289;&#29702;&#20107;&#20214;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#30340;&#26426;&#22120;&#23545;&#20110;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#28789;&#27963;&#20114;&#21160;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#29289;&#29702;&#21644;&#22240;&#26524;&#25512;&#29702;&#22522;&#20934;&#37117;&#20165;&#22522;&#20110;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#31181;&#35774;&#35745;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;&#19968;&#26159;&#20107;&#20214;&#31867;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#32570;&#20047;&#22810;&#26679;&#24615;&#65307;&#20108;&#26159;&#22522;&#20110;&#25163;&#21160;&#23450;&#20041;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#30340;&#22240;&#26524;&#20851;&#31995;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLEVRER-Humans&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23545;&#29289;&#29702;&#20107;&#20214;&#30340;&#22240;&#26524;&#21028;&#26029;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#65306;&#39318;&#20808;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#20219;&#21153;&#65292;&#20197; eliciting &#35270;&#39057;&#20013;&#20107;&#20214;&#30340;&#26032;&#34920;&#31034;&#26041;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22240;&#26524;&#20107;&#20214;&#22270; (CEGs)&#65307;&#20854;&#27425;&#65292;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37325;&#26032;&#23450;&#20041;&#25968;&#23383;&#20581;&#24247;&#30028;&#38754;&#30340;&#26041;&#27861;&#65292;&#23558;LLMs&#19982;&#22806;&#37096;&#24037;&#20855;&#32467;&#21512;&#20351;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19982;&#20020;&#24202;&#25216;&#26415;&#30340;&#20114;&#21160;&#25928;&#26524;&#65292;&#25913;&#21892;&#20102;&#25968;&#23383;&#21307;&#30103;&#24037;&#20855;&#21644;AI&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;LLMs&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03560</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#23450;&#20041;&#25968;&#23383;&#20581;&#24247;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Redefining Digital Health Interfaces with Large Language Models. (arXiv:2310.03560v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37325;&#26032;&#23450;&#20041;&#25968;&#23383;&#20581;&#24247;&#30028;&#38754;&#30340;&#26041;&#27861;&#65292;&#23558;LLMs&#19982;&#22806;&#37096;&#24037;&#20855;&#32467;&#21512;&#20351;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19982;&#20020;&#24202;&#25216;&#26415;&#30340;&#20114;&#21160;&#25928;&#26524;&#65292;&#25913;&#21892;&#20102;&#25968;&#23383;&#21307;&#30103;&#24037;&#20855;&#21644;AI&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;LLMs&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#20581;&#24247;&#24037;&#20855;&#20855;&#26377;&#26174;&#33879;&#25913;&#21892;&#21307;&#30103;&#26381;&#21153;&#20256;&#36882;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#29992;&#24615;&#21644;&#20449;&#20219;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#30340;&#20351;&#29992;&#20173;&#28982;&#30456;&#23545;&#26377;&#38480;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20855;&#26377;&#22788;&#29702;&#22797;&#26434;&#20449;&#24687;&#21644;&#29983;&#25104;&#20154;&#31867;&#36136;&#37327;&#25991;&#26412;&#33021;&#21147;&#30340;&#36890;&#29992;&#27169;&#22411;&#20986;&#29616;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#30452;&#25509;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#24212;&#29992;LLMs&#24182;&#19981;&#31616;&#21333;&#65292;&#22240;&#20026;LLMs&#23481;&#26131;&#25552;&#20379;&#19981;&#19968;&#33268;&#25110;&#26080;&#24847;&#20041;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#20351;LLMs&#22312;&#20020;&#24202;&#21307;&#30103;&#25216;&#26415;&#20114;&#21160;&#20013;&#25552;&#20379;&#20840;&#26032;&#30028;&#38754;&#12290;&#36825;&#22686;&#24378;&#20102;&#25968;&#23383;&#21307;&#30103;&#24037;&#20855;&#21644;AI&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#21644;&#23454;&#38469;&#24433;&#21709;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;LLMs&#30340;&#24403;&#21069;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;&#31958;&#23615;&#30149;&#39118;&#38505;&#39044;&#27979;&#30340;&#20363;&#23376;&#38416;&#36848;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital health tools have the potential to significantly improve the delivery of healthcare services. However, their use remains comparatively limited due, in part, to challenges surrounding usability and trust. Recently, Large Language Models (LLMs) have emerged as general-purpose models with the ability to process complex information and produce human-quality text, presenting a wealth of potential applications in healthcare. Directly applying LLMs in clinical settings is not straightforward, with LLMs susceptible to providing inconsistent or nonsensical answers. We demonstrate how LLMs can utilize external tools to provide a novel interface between clinicians and digital technologies. This enhances the utility and practical impact of digital healthcare tools and AI models while addressing current issues with using LLM in clinical settings such as hallucinations. We illustrate our approach with examples from cardiovascular disease and diabetes risk prediction, highlighting the benefit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#36755;&#20837;&#22122;&#22768;&#30340;&#40065;&#26834;&#21644;&#21487;&#25512;&#24191;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#22024;&#26434;&#30340;&#27133;&#22635;&#20805;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#40065;&#26834;&#24615;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#26694;&#26550;&#65292;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03518</link><description>&lt;p&gt;
&#26397;&#30528;&#40065;&#26834;&#21644;&#21487;&#25512;&#24191;&#30340;&#35757;&#32451;&#26041;&#27861;&#65306;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#22024;&#26434;&#27133;&#22635;&#20805;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Robust and Generalizable Training: An Empirical Study of Noisy Slot Filling for Input Perturbations. (arXiv:2310.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03518
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#36755;&#20837;&#22122;&#22768;&#30340;&#40065;&#26834;&#21644;&#21487;&#25512;&#24191;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#22024;&#26434;&#30340;&#27133;&#22635;&#20805;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#40065;&#26834;&#24615;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#26694;&#26550;&#65292;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#23545;&#35805;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#35805;&#35821;&#20013;&#23384;&#22312;&#26410;&#30693;&#30340;&#36755;&#20837;&#22122;&#22768;&#65292;&#29616;&#26377;&#30340;&#30417;&#30563;&#24335;&#27133;&#22635;&#20805;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#36890;&#24120;&#36739;&#24046;&#12290;&#34429;&#28982;&#26377;&#19968;&#20123;&#20851;&#20110;&#22122;&#22768;&#40065;&#26834;&#24615;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#21482;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#31181;&#38480;&#21046;&#24615;&#23548;&#33268;&#20102;&#22122;&#22768;&#40065;&#26834;&#26041;&#27861;&#30740;&#31350;&#30340;&#25512;&#24191;&#22256;&#38590;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Noise-SF&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20116;&#31181;&#31867;&#22411;&#30340;&#20154;&#24037;&#26631;&#27880;&#22122;&#22768;&#65292;&#24182;&#19988;&#25152;&#26377;&#36825;&#20123;&#22122;&#22768;&#37117;&#30830;&#23454;&#23384;&#22312;&#20110;&#30495;&#23454;&#30340;&#22823;&#35268;&#27169;&#40065;&#26834;&#24615;&#35757;&#32451;&#26041;&#27861;&#20013;&#12290;&#36890;&#36807;&#23545;Noise-SF&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#35777;&#35780;&#20272;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#32447;&#27169;&#22411;&#22312;&#40065;&#26834;&#24615;&#35780;&#20272;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#32780;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#23454;&#35777;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#21069;&#30651;&#24615;&#30340;&#24314;&#35758;&#26469;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real dialogue scenarios, as there are unknown input noises in the utterances, existing supervised slot filling models often perform poorly in practical applications. Even though there are some studies on noise-robust models, these works are only evaluated on rule-based synthetic datasets, which is limiting, making it difficult to promote the research of noise-robust methods. In this paper, we introduce a noise robustness evaluation dataset named Noise-SF for slot filling task. The proposed dataset contains five types of human-annotated noise, and all those noises are exactly existed in real extensive robust-training methods of slot filling into the proposed framework. By conducting exhaustive empirical evaluation experiments on Noise-SF, we find that baseline models have poor performance in robustness evaluation, and the proposed framework can effectively improve the robustness of models. Based on the empirical experimental results, we make some forward-looking suggestions to fuel t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36716;&#25442;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#35821;&#35328;&#26631;&#35760;&#26144;&#23556;&#21040;&#35821;&#20041;&#30456;&#20284;&#30340;&#28304;&#35821;&#35328;&#26631;&#35760;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#35757;&#32451;&#21333;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#21021;&#22987;&#21270;&#36807;&#31243;&#65292;&#24182;&#22312;&#33655;&#20848;&#35821;&#21644;&#24343;&#37324;&#26031;&#20848;&#35821;&#31561;&#22810;&#31181;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03477</link><description>&lt;p&gt;
Tik-to-Tok:&#19968;&#27425;&#32763;&#35793;&#19968;&#20010;&#26631;&#35760;&#65306;&#19968;&#31181;&#29992;&#20110;&#26377;&#25928;&#35821;&#35328;&#36866;&#24212;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Tik-to-Tok: Translating Language Models One Token at a Time: An Embedding Initialization Strategy for Efficient Language Adaptation. (arXiv:2310.03477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36716;&#25442;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#35821;&#35328;&#26631;&#35760;&#26144;&#23556;&#21040;&#35821;&#20041;&#30456;&#20284;&#30340;&#28304;&#35821;&#35328;&#26631;&#35760;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#35757;&#32451;&#21333;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#21021;&#22987;&#21270;&#36807;&#31243;&#65292;&#24182;&#22312;&#33655;&#20848;&#35821;&#21644;&#24343;&#37324;&#26031;&#20848;&#35821;&#31561;&#22810;&#31181;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36716;&#25442;&#31574;&#30053;&#65292;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#35757;&#32451;&#21333;&#35821;&#35328;&#27169;&#22411;&#26102;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#21644;&#36890;&#24120;&#19981;&#36275;&#22815;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#26469;&#28304;&#35821;&#35328;&#26631;&#35760;&#22120;&#21644;&#30446;&#26631;&#35821;&#35328;&#26631;&#35760;&#22120;&#20043;&#38388;&#24314;&#31435;&#19968;&#20010;&#21253;&#21547;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#21333;&#35789;&#32763;&#35793;&#23383;&#20856;&#30340;&#27867;&#21270;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#30446;&#26631;&#26631;&#35760;&#26144;&#23556;&#21040;&#35821;&#20041;&#30456;&#20284;&#30340;&#28304;&#35821;&#35328;&#26631;&#35760;&#12290;&#36825;&#31181;&#19968;&#23545;&#22810;&#30340;&#26631;&#35760;&#26144;&#23556;&#26497;&#22823;&#22320;&#25913;&#21892;&#20102;&#30446;&#26631;&#35821;&#35328;&#30340;&#23884;&#20837;&#34920;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#23545;&#39640;&#36164;&#28304;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#20013;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#20998;&#21035;&#26159;&#33655;&#20848;&#35821;&#21644;&#24343;&#37324;&#26031;&#20848;&#35821;&#12290;&#36825;&#20123;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#22312;&#36825;&#20123;&#35821;&#35328;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#36890;&#36807;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26368;&#26032;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#21644;&#26102;&#38388;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#27169;&#22411;&#36716;&#25442;&#31574;&#30053;&#22823;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training monolingual language models for low and mid-resource languages is made challenging by limited and often inadequate pretraining data. In this study, we propose a novel model conversion strategy to address this issue, adapting high-resources monolingual language models to a new target language. By generalizing over a word translation dictionary encompassing both the source and target languages, we map tokens from the target tokenizer to semantically similar tokens from the source language tokenizer. This one-to-many token mapping improves tremendously the initialization of the embedding table for the target language. We conduct experiments to convert high-resource models to midand low-resource languages, namely Dutch and Frisian. These converted models achieve a new state-of-the-art performance on these languages across all sorts of downstream tasks. By reducing significantly the amount of data and time required for training state-of-the-art models, our novel model conversion 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#30340;&#36890;&#29992;&#21487;&#25511;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#25991;&#26723;&#25688;&#35201;&#12290;&#36890;&#36807;&#35757;&#32451;&#21487;&#25511;&#30340;&#20869;&#23481;&#25552;&#21462;&#26041;&#26696;&#24182;&#21033;&#29992;&#35206;&#30422;&#21644;&#36830;&#36143;&#24615;&#30452;&#35266;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#28508;&#22312;&#30340;&#22522;&#32447;&#22312;&#36830;&#36143;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#20351;&#29992;ROUGE&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#26102;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03473</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35206;&#30422;&#21644;&#36830;&#36143;&#24615;&#30452;&#35266;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Controllable Multi-document Summarization: Coverage &amp; Coherence Intuitive Policy with Large Language Model Based Rewards. (arXiv:2310.03473v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#30340;&#36890;&#29992;&#21487;&#25511;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#25991;&#26723;&#25688;&#35201;&#12290;&#36890;&#36807;&#35757;&#32451;&#21487;&#25511;&#30340;&#20869;&#23481;&#25552;&#21462;&#26041;&#26696;&#24182;&#21033;&#29992;&#35206;&#30422;&#21644;&#36830;&#36143;&#24615;&#30452;&#35266;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#28508;&#22312;&#30340;&#22522;&#32447;&#22312;&#36830;&#36143;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#20351;&#29992;ROUGE&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#26102;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#39640;&#25991;&#26412;&#21487;&#35835;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38271;&#36755;&#20837;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65288;&#22914;&#22810;&#25991;&#26723;&#25688;&#35201;&#65289;&#65292;&#21487;&#25511;&#24615;&#26159;&#19968;&#20010;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21487;&#25511;&#26041;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#25991;&#26412;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#21487;&#25511;&#30340;&#20869;&#23481;&#25552;&#21462;&#26041;&#26696;&#65292;&#23427;&#25552;&#21462;&#23558;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#30340;&#25991;&#26412;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35206;&#30422;&#21644;&#36830;&#36143;&#24615;&#30452;&#35266;&#31574;&#30053;&#65292;&#24182;&#30001;&#19968;&#20010;&#34987;&#21160;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;ROUGE&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#26102;&#20135;&#29983;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#36830;&#36143;&#24615;&#26041;&#38754;&#20248;&#20110;&#28508;&#22312;&#30340;&#22522;&#32447;&#65292;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient large language models are good at refining text input for better readability. However, controllability is a matter of concern when it comes to text generation tasks with long inputs, such as multi-document summarization. In this work, we investigate for a generic controllable approach for multi-document summarization that leverages the capabilities of LLMs to refine the text. In particular, we train a controllable content extraction scheme to extract the text that will be refined by an LLM. The scheme is designed with a novel coverage and coherence intuitive policy, which is duly rewarded by a passively trained LLM. Our approach yields competitive results in the evaluation using ROUGE metrics and outperforms potential baselines in coherence, as per human evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#22577;&#21578;&#25552;&#20379;&#20102;&#21271;&#31995;&#32113;&#30340;&#31777;&#35201;&#27010;&#36848;&#65292;&#35442;&#31995;&#32113;&#26088;&#22312;&#23526;&#29694;&#23565;&#21488;&#28771;&#23458;&#23478;&#35486;&#65288;&#22235;&#32291;&#33108;&#65289;&#30340;&#33258;&#21205;&#35422;/&#38899;&#31680;&#35672;&#21029;&#12290;&#38364;&#37749;&#37096;&#20998;&#21253;&#25324;&#35347;&#32244;&#25976;&#25818;&#30340;&#29554;&#21462;&#12289;&#32068;&#25104;&#21644;&#21033;&#29992;&#65292;&#27169;&#22411;&#30340;&#26550;&#27083;&#65292;&#20197;&#21450;&#30828;&#20214;&#35215;&#26684;&#21644;&#36939;&#34892;&#32113;&#35336;&#12290;</title><link>http://arxiv.org/abs/2310.03443</link><description>&lt;p&gt;
2023&#24180;&#31119;&#29246;&#25705;&#27801;&#21475;&#38899;&#36776;&#35672;&#25361;&#25136;&#20013;&#30340;&#21271;&#31995;&#32113;
&lt;/p&gt;
&lt;p&gt;
The North System for Formosa Speech Recognition Challenge 2023. (arXiv:2310.03443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#22577;&#21578;&#25552;&#20379;&#20102;&#21271;&#31995;&#32113;&#30340;&#31777;&#35201;&#27010;&#36848;&#65292;&#35442;&#31995;&#32113;&#26088;&#22312;&#23526;&#29694;&#23565;&#21488;&#28771;&#23458;&#23478;&#35486;&#65288;&#22235;&#32291;&#33108;&#65289;&#30340;&#33258;&#21205;&#35422;/&#38899;&#31680;&#35672;&#21029;&#12290;&#38364;&#37749;&#37096;&#20998;&#21253;&#25324;&#35347;&#32244;&#25976;&#25818;&#30340;&#29554;&#21462;&#12289;&#32068;&#25104;&#21644;&#21033;&#29992;&#65292;&#27169;&#22411;&#30340;&#26550;&#27083;&#65292;&#20197;&#21450;&#30828;&#20214;&#35215;&#26684;&#21644;&#36939;&#34892;&#32113;&#35336;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#22577;&#21578;&#25552;&#20379;&#20102;&#21271;&#31995;&#32113;&#30340;&#31777;&#35201;&#27010;&#36848;&#65292;&#26088;&#22312;&#23526;&#29694;&#23565;&#21488;&#28771;&#23458;&#23478;&#35486;&#65288;&#22235;&#32291;&#33108;&#65289;&#30340;&#33258;&#21205;&#35422;/&#38899;&#31680;&#35672;&#21029;&#12290;&#35442;&#22577;&#21578;&#27010;&#36848;&#20102;&#31995;&#32113;&#30340;&#19977;&#20491;&#38364;&#37749;&#37096;&#20998;&#65306;&#35347;&#32244;&#25976;&#25818;&#30340;&#29554;&#21462;&#12289;&#32068;&#25104;&#21644;&#21033;&#29992;&#65307;&#27169;&#22411;&#30340;&#26550;&#27083;&#65307;&#20197;&#21450;&#30828;&#20214;&#35215;&#26684;&#21644;&#36939;&#34892;&#32113;&#35336;&#12290;&#31995;&#32113;&#30340;&#28436;&#31034;&#21487;&#20197;&#22312;https://asrvm.iis.sinica.edu.tw/hakka_sixian&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report provides a concise overview of the proposed North system, which aims to achieve automatic word/syllable recognition for Taiwanese Hakka (Sixian). The report outlines three key components of the system: the acquisition, composition, and utilization of the training data; the architecture of the model; and the hardware specifications and operational statistics. The demonstration of the system can be found at https://asrvm.iis.sinica.edu.tw/hakka_sixian.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20462;&#21098;&#26694;&#26550;&#30340;&#20934;&#21017;&#12289;&#26041;&#27861;&#21644;&#35843;&#24230;&#22120;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#20462;&#21098;&#22312;&#22810;&#20010;&#22330;&#26223;&#20013;&#20248;&#20110;&#24133;&#24230;&#39537;&#21160;&#30340;&#20462;&#21098;&#65292;&#36880;&#27493;&#20462;&#21098;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#19968;&#27425;&#24615;&#20462;&#21098;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#36880;&#27493;&#21387;&#32553;&#27169;&#22411;&#30340;&#20302;&#31209;&#36924;&#36817;&#26041;&#27861;&#65292;&#20026;&#20013;&#31561;&#21387;&#32553;&#31243;&#24230;&#19979;&#30340;&#20307;&#31215;&#20943;&#23567;&#21644;&#25512;&#29702;&#21152;&#36895;&#25552;&#20379;&#20102;&#26368;&#20339;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.03424</link><description>&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20462;&#21098;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Neural Language Model Pruning for Automatic Speech Recognition. (arXiv:2310.03424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20462;&#21098;&#26694;&#26550;&#30340;&#20934;&#21017;&#12289;&#26041;&#27861;&#21644;&#35843;&#24230;&#22120;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#20462;&#21098;&#22312;&#22810;&#20010;&#22330;&#26223;&#20013;&#20248;&#20110;&#24133;&#24230;&#39537;&#21160;&#30340;&#20462;&#21098;&#65292;&#36880;&#27493;&#20462;&#21098;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#19968;&#27425;&#24615;&#20462;&#21098;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#36880;&#27493;&#21387;&#32553;&#27169;&#22411;&#30340;&#20302;&#31209;&#36924;&#36817;&#26041;&#27861;&#65292;&#20026;&#20013;&#31561;&#21387;&#32553;&#31243;&#24230;&#19979;&#30340;&#20307;&#31215;&#20943;&#23567;&#21644;&#25512;&#29702;&#21152;&#36895;&#25552;&#20379;&#20102;&#26368;&#20339;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20462;&#21098;&#26694;&#26550;&#30340;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;&#20934;&#21017;&#12289;&#26041;&#27861;&#21644;&#35843;&#24230;&#22120;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#20934;&#30830;&#24230;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20851;&#20110;&#22823;&#35268;&#27169;&#35782;&#21035;&#31995;&#32479;&#30340;&#36825;&#31181;&#28145;&#20837;&#20998;&#26512;&#22312;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#25253;&#36947;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36880;&#27493;&#21387;&#32553;&#27169;&#22411;&#30340;&#20302;&#31209;&#36924;&#36817;&#30340;&#21464;&#20307;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#22823;&#23567;&#30340;&#27169;&#22411;&#12290;&#22312;&#20854;&#20182;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;a) &#25968;&#25454;&#39537;&#21160;&#30340;&#20462;&#21098;&#22312;&#20960;&#20010;&#22330;&#26223;&#19979;&#20248;&#20110;&#24133;&#24230;&#39537;&#21160;&#30340;&#20462;&#21098;&#65307;b) &#36880;&#27493;&#20462;&#21098;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#19968;&#27425;&#24615;&#20462;&#21098;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#36739;&#23567;&#30340;&#22823;&#23567;&#65307;c) &#20302;&#31209;&#36924;&#36817;&#22312;&#20013;&#31561;&#21387;&#32553;&#31243;&#24230;&#19979;&#65292;&#20307;&#31215;&#20943;&#23567;&#21644;&#25512;&#29702;&#21152;&#36895;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study model pruning methods applied to Transformer-based neural network language models for automatic speech recognition. We explore three aspects of the pruning frame work, namely criterion, method and scheduler, analyzing their contribution in terms of accuracy and inference speed. To the best of our knowledge, such in-depth analyses on large-scale recognition systems has not been reported in the literature. In addition, we propose a variant of low-rank approximation suitable for incrementally compressing models, and delivering multiple models with varied target sizes. Among other results, we show that a) data-driven pruning outperforms magnitude-driven in several scenarios; b) incremental pruning achieves higher accuracy compared to one-shot pruning, especially when targeting smaller sizes; and c) low-rank approximation presents the best trade-off between size reduction and inference speed-up for moderate compression.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#20107;&#20214;&#20559;&#21521;&#30340;&#21333;&#35843;&#27425;&#27169;&#20869;&#23481;&#25552;&#21462;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#21462;-&#37325;&#20889;&#26041;&#27861;&#21644;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#30830;&#20445;&#25688;&#35201;&#23458;&#35266;&#24615;&#21644;&#20449;&#24687;&#20016;&#23500;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03414</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#22522;&#20110;&#20027;&#20107;&#20214;&#20559;&#21521;&#30340;&#21333;&#35843;&#27425;&#27169;&#20869;&#23481;&#25552;&#21462;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
LLM Based Multi-Document Summarization Exploiting Main-Event Biased Monotone Submodular Content Extraction. (arXiv:2310.03414v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03414
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#20107;&#20214;&#20559;&#21521;&#30340;&#21333;&#35843;&#27425;&#27169;&#20869;&#23481;&#25552;&#21462;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#21462;-&#37325;&#20889;&#26041;&#27861;&#21644;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#30830;&#20445;&#25688;&#35201;&#23458;&#35266;&#24615;&#21644;&#20449;&#24687;&#20016;&#23500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25991;&#26723;&#25688;&#35201;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#20027;&#35266;&#20559;&#35265;&#65292;DUC-2004&#21442;&#32771;&#25688;&#35201;&#30340;&#20114;&#35780;ROUGE-1&#20998;&#25968;&#20165;&#20026;0.4&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#32858;&#28966;&#30456;&#20851;&#26032;&#38395;&#25991;&#26723;&#30340;&#20027;&#35201;&#20107;&#20214;&#24182;&#20197;&#20805;&#20998;&#30340;&#19978;&#19979;&#25991;&#19968;&#33268;&#22320;&#21576;&#29616;&#23427;&#26469;&#22686;&#24378;&#26032;&#38395;&#25688;&#35201;&#30340;&#23458;&#35266;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#31616;&#27905;&#22320;&#25253;&#36947;&#20027;&#35201;&#20107;&#20214;&#65292;&#20197;&#30830;&#20445;&#25688;&#35201;&#20445;&#25345;&#23458;&#35266;&#19988;&#20449;&#24687;&#20016;&#23500;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#25552;&#21462;-&#37325;&#20889;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#20027;&#20107;&#20214;&#20559;&#21521;&#30340;&#21333;&#35843;&#27425;&#27169;&#20989;&#25968;&#36827;&#34892;&#20869;&#23481;&#36873;&#25321;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#25991;&#26723;&#32676;&#20013;&#25552;&#21462;&#19982;&#20027;&#35201;&#20107;&#20214;&#30456;&#20851;&#30340;&#26368;&#20851;&#38190;&#20449;&#24687;&#12290;&#20026;&#30830;&#20445;&#36830;&#36143;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#25552;&#21462;&#30340;&#20869;&#23481;&#37325;&#20889;&#20026;&#36830;&#36143;&#30340;&#25991;&#26412;&#12290;&#20351;&#29992;&#23458;&#35266;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#32773;&#30340;&#35780;&#20272;&#30830;&#35748;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#36229;&#36234;&#20102;&#28508;&#22312;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-document summarization is a challenging task due to its inherent subjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4 among DUC-2004 reference summaries. In this work, we aim to enhance the objectivity of news summarization by focusing on the main event of a group of related news documents and presenting it coherently with sufficient context. Our primary objective is to succinctly report the main event, ensuring that the summary remains objective and informative. To achieve this, we employ an extract-rewrite approach that incorporates a main-event biased monotone-submodular function for content selection. This enables us to extract the most crucial information related to the main event from the document cluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for rewriting the extracted content into a coherent text. The evaluation using objective metrics and human evaluators confirms the effectiveness of our approach, as it surpasses pote
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31243;&#24207;&#24615;&#25991;&#26412;&#25366;&#25496;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-4&#27169;&#22411;&#21644;&#33258;&#23450;&#20041;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;PDF&#25991;&#26412;&#20013;&#25552;&#21462;&#31243;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.03376</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31243;&#24207;&#24615;&#25991;&#26412;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Procedural Text Mining with Large Language Models. (arXiv:2310.03376v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31243;&#24207;&#24615;&#25991;&#26412;&#25366;&#25496;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-4&#27169;&#22411;&#21644;&#33258;&#23450;&#20041;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;PDF&#25991;&#26412;&#20013;&#25552;&#21462;&#31243;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#35757;&#32451;&#20102;&#22823;&#37327;&#30693;&#35782;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20026;&#30693;&#35782;&#24037;&#31243;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#26426;&#36935;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35299;&#20915;&#20174;&#38750;&#32467;&#26500;&#21270;PDF&#25991;&#26412;&#20013;&#20197;&#22686;&#37327;&#38382;&#31572;&#26041;&#24335;&#25552;&#21462;&#31243;&#24207;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;GPT-4 (Generative Pre-trained Transformer 4)&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21464;&#20307;&#65292;&#21253;&#25324;&#24102;&#26377;&#31243;&#24207;&#21644;&#27493;&#39588;&#23450;&#20041;&#30340;&#26412;&#20307;&#21644;&#26377;&#38480;&#25968;&#37327;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#23450;&#21046;&#21270;&#30340;&#20215;&#20540;&#12290;&#36825;&#20123;&#20462;&#25913;&#26377;&#26395;&#26174;&#33879;&#35299;&#20915;&#33719;&#21462;&#36275;&#22815;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#24314;&#31435;HalluQA&#22522;&#20934;&#27979;&#35797;&#21644;&#20351;&#29992;GPT-4&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;18&#20010;&#27169;&#22411;&#30340;&#38750;&#24187;&#35273;&#29575;&#20302;&#20110;50%&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#31867;&#22411;&#21644;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2310.03368</link><description>&lt;p&gt;
&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Evaluating Hallucinations in Chinese Large Language Models. (arXiv:2310.03368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#24314;&#31435;HalluQA&#22522;&#20934;&#27979;&#35797;&#21644;&#20351;&#29992;GPT-4&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;18&#20010;&#27169;&#22411;&#30340;&#38750;&#24187;&#35273;&#29575;&#20302;&#20110;50%&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#31867;&#22411;&#21644;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;HalluQA&#65288;&#20013;&#25991;&#24187;&#35273;&#38382;&#31572;&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;HalluQA&#21253;&#21547;450&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#24615;&#38382;&#39064;&#65292;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#65292;&#24182;&#32771;&#34385;&#20102;&#20013;&#22269;&#21382;&#21490;&#25991;&#21270;&#12289;&#39118;&#20439;&#21644;&#31038;&#20250;&#29616;&#35937;&#12290;&#22312;&#26500;&#24314;HalluQA&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24187;&#35273;&#31867;&#22411;&#65306;&#27169;&#20223;&#24615;&#34394;&#20551;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#22522;&#20110;GLM-130B&#21644;ChatGPT&#26500;&#24314;&#23545;&#25239;&#26679;&#26412;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-4&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#26469;&#21028;&#26029;&#27169;&#22411;&#36755;&#20986;&#26159;&#21542;&#26159;&#24187;&#35273;&#12290;&#25105;&#20204;&#23545;24&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;ERNIE-Bot&#12289;Baichuan2&#12289;ChatGLM&#12289;Qwen&#12289;SparkDesk&#31561;&#12290;&#22312;&#36825;24&#20010;&#27169;&#22411;&#20013;&#65292;&#26377;18&#20010;&#30340;&#38750;&#24187;&#35273;&#29575;&#20302;&#20110;50%&#12290;&#36825;&#34920;&#26126;HalluQA&#20855;&#26377;&#24456;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#27169;&#22411;&#20013;&#20027;&#35201;&#30340;&#24187;&#35273;&#31867;&#22411;&#21450;&#20854;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we establish a benchmark named HalluQA (Chinese Hallucination Question-Answering) to measure the hallucination phenomenon in Chinese large language models. HalluQA contains 450 meticulously designed adversarial questions, spanning multiple domains, and takes into account Chinese historical culture, customs, and social phenomena. During the construction of HalluQA, we consider two types of hallucinations: imitative falsehoods and factual errors, and we construct adversarial samples based on GLM-130B and ChatGPT. For evaluation, we design an automated evaluation method using GPT-4 to judge whether a model output is hallucinated. We conduct extensive experiments on 24 large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than 50%. This indicates that HalluQA is highly challenging. We analyze the primary types of hallucinations in different types of models and their causes. Add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;GPT-4&#39046;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#36807;&#31243;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#8220;&#36866;&#24212;-&#26816;&#32034;-&#20462;&#35746;&#8221;&#30340;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#29983;&#25104;&#20869;&#23481;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03328</link><description>&lt;p&gt;
&#25226;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#36866;&#24212;&#37325;&#26032;&#34920;&#36848;&#20026;&#36866;&#24212;-&#26816;&#32034;-&#20462;&#35746;
&lt;/p&gt;
&lt;p&gt;
Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise. (arXiv:2310.03328v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;GPT-4&#39046;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#36807;&#31243;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#8220;&#36866;&#24212;-&#26816;&#32034;-&#20462;&#35746;&#8221;&#30340;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#29983;&#25104;&#20869;&#23481;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#22312;&#19968;&#33324;&#39046;&#22495;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#22312;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;&#20013;&#22269;&#27861;&#24459;&#65289;&#29983;&#25104;&#38169;&#35823;&#30340;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#36825;&#36890;&#24120;&#26159;&#30001;&#20110;&#27809;&#26377;&#21253;&#21547;&#36825;&#26679;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#24471;GPT-4&#26080;&#27861;&#33719;&#21462;&#39046;&#22495;&#20869;&#30340;&#30693;&#35782;&#12290;&#19968;&#20010;&#32039;&#36843;&#30340;&#25361;&#25112;&#26159;&#22312;&#39046;&#22495;&#20869;&#25968;&#25454;&#19978;&#32487;&#32493;&#35757;&#32451;&#22914;&#27492;&#22823;&#35268;&#27169;&#30340;LLM&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#29983;&#25104;&#36807;&#31243;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#8220;&#36866;&#24212;-&#26816;&#32034;-&#20462;&#35746;&#8221;&#30340;&#36807;&#31243;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;GPT-4&#39046;&#22495;&#36866;&#24212;&#26694;&#26550;&#12290;&#21021;&#22987;&#27493;&#39588;&#26159;&#36890;&#36807;&#22312;&#39046;&#22495;&#20869;&#25968;&#25454;&#19978;&#32487;&#32493;&#23398;&#20064;&#65292;&#23558;&#19968;&#20010;&#32463;&#27982;&#23454;&#24800;&#30340;7B LLM&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#35299;&#20915;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;&#36866;&#24212;&#30340;LLM&#26681;&#25454;&#20219;&#21153;&#26597;&#35810;&#29983;&#25104;&#19968;&#20010;&#21021;&#31295;&#31572;&#26696;&#12290;&#28982;&#21518;&#65292;&#21021;&#31295;&#31572;&#26696;&#23558;&#29992;&#20110;&#20174;&#22806;&#37096;&#26816;&#32034;&#25903;&#25345;&#35777;&#25454;&#30340;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) like GPT-4 have recently demonstrated astonishing zero-shot capabilities in general domain tasks, they often generate content with hallucinations in specific domains such as Chinese law, hindering their application in these areas. This is typically due to the absence of training data that encompasses such a specific domain, preventing GPT-4 from acquiring in-domain knowledge. A pressing challenge is that it's not plausible to continue training LLMs of such scale on in-domain data.  This paper introduces a simple and effective domain adaptation framework for GPT-4 by reformulating generation as an \textbf{adapt-retrieve-revise} process. The initial step is to \textbf{adapt} an affordable 7B LLM to the target domain by continuing learning on in-domain data. When solving a task, we leverage the adapted LLM to generate a draft answer given a task query. Then, the draft answer will be used to \textbf{retrieve} supporting evidence candidates from an externa
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03309</link><description>&lt;p&gt;
&#31616;&#26126;&#26377;&#24207;&#30340;&#24863;&#30693;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03309
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#28436;&#32462;&#25512;&#29702;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#22797;&#26434;&#30340;&#28436;&#32462;&#38382;&#39064;&#20013;&#20173;&#28982;&#24456;&#38590;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#36825;&#31867;&#38382;&#39064;&#20855;&#26377;&#22823;&#37327;&#21069;&#25552;&#65288;&#21363;&#20107;&#23454;&#25110;&#35268;&#21017;&#65289;&#65292;&#20854;&#20013;&#28041;&#21450;&#23454;&#20307;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#21407;&#22987;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#20197;&#21069;&#21521;&#65288;&#20363;&#22914;&#36873;&#25321;-&#25512;&#29702;&#65289;&#25110;&#21453;&#21521;&#65288;&#20363;&#22914;LAMBADA&#65289;&#26041;&#24335;&#23558;&#22810;&#20010;&#22240;&#26524;&#25512;&#29702;&#27493;&#39588;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#22823;&#37327;&#30340;&#24635;&#20307;&#38454;&#27573;&#65292;&#23548;&#33268;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#24182;&#19988;&#26377;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#27493;&#39588;&#12290;&#38500;&#20102;&#36880;&#38454;&#27573;&#20998;&#35299;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20154;&#31867;&#38382;&#39064;&#35299;&#20915;&#30340;&#21478;&#19968;&#20010;&#26041;&#38754;&#33719;&#24471;&#20102;&#21551;&#21457;&#12290;&#20154;&#31867;&#20542;&#21521;&#20110;&#25552;&#28860;&#20986;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#24182;&#26377;&#24207;&#22320;&#32452;&#32455;&#24605;&#32500;&#65288;&#20363;&#22914;&#21019;&#24314;&#24605;&#32500;&#23548;&#22270;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#20182;&#20204;&#23545;&#38382;&#39064;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#35770;&#25991;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#35780;&#23457;&#20154;&#21592;&#30340;&#31034;&#20363;&#35780;&#20215;&#36827;&#34892;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.03304</link><description>&lt;p&gt;
&#23398;&#20064;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Learning Personalized Story Evaluation. (arXiv:2310.03304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#35770;&#25991;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#35780;&#23457;&#20154;&#21592;&#30340;&#31034;&#20363;&#35780;&#20215;&#36827;&#34892;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35832;&#22914;&#38382;&#31572;&#21644;&#26816;&#32034;&#31561;&#26356;&#23458;&#35266;&#30340;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#35780;&#20272;&#23427;&#20204;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#34920;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#21253;&#25324;&#65288;1&#65289;&#25968;&#25454;&#27745;&#26579;&#65307;&#65288;2&#65289;&#22810;&#32500;&#35780;&#20272;&#26631;&#20934;&#65307;&#20197;&#21450;&#65288;3&#65289;&#26469;&#33258;&#35780;&#23457;&#20154;&#21592;&#20010;&#20154;&#20559;&#22909;&#30340;&#20027;&#35266;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#19968;&#20010;&#26080;&#27745;&#26579;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#35780;&#20272;&#20013;&#24314;&#27169;&#20010;&#24615;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#36866;&#24403;&#30340;&#21311;&#21517;&#21270;&#21644;&#26032;&#30340;&#20010;&#24615;&#21270;&#26631;&#31614;&#65292;&#37325;&#26032;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Per-MPST&#21644;Per-DOC&#29992;&#20110;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#27169;&#22411;PERSE&#26469;&#25512;&#27979;&#35780;&#23457;&#20154;&#21592;&#30340;&#20559;&#22909;&#65292;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#26576;&#20010;&#35780;&#23457;&#20154;&#21592;&#30340;&#19968;&#20123;&#31034;&#20363;&#35780;&#20215;&#65292;PERSE&#21487;&#20197;&#39044;&#27979;&#35813;&#35780;&#23457;&#20154;&#21592;&#22312;&#26032;&#30340;&#24773;&#33410;&#19978;&#30340;&#35814;&#32454;&#35780;&#23457;&#25110;&#32454;&#31890;&#24230;&#27604;&#36739;&#65288;&#22914;&#36259;&#21619;&#24615;&#21644;&#24778;&#21916;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have shown impressive results for more objective tasks such as QA and retrieval, it remains nontrivial to evaluate their performance on open-ended text generation for reasons including (1) data contamination; (2) multi-dimensional evaluation criteria; and (3) subjectiveness stemming from reviewers' personal preferences. To address such issues, we propose to model personalization in an uncontaminated open-ended generation assessment. We create two new datasets Per-MPST and Per-DOC for personalized story evaluation, by re-purposing existing datasets with proper anonymization and new personalized labels. We further develop a personalized story evaluation model PERSE to infer reviewer preferences and provide a personalized evaluation. Specifically, given a few exemplary reviews from a particular reviewer, PERSE predicts either a detailed review or fine-grained comparison in several aspects (such as interestingness and surprise) for that reviewer on a new 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38382;&#38382;&#39064;&#26816;&#27979;&#29992;&#25143;&#30340;&#38544;&#21547;&#24847;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65288;EDIT&#65289;&#26469;&#22686;&#24378;&#23545;&#35805;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#12290;&#35813;&#26694;&#26550;&#29983;&#25104;&#19982;&#29992;&#25143;&#24847;&#22270;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#19982;LLMs&#20132;&#20114;&#21644;&#39046;&#22495;&#20869;&#25628;&#32034;&#30340;&#26041;&#24335;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03293</link><description>&lt;p&gt;
&#36890;&#36807;&#38382;&#38382;&#39064;&#26816;&#27979;&#29992;&#25143;&#24847;&#22270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#29983;&#25104;&#20195;&#29702;&#30340;&#26032;&#22411;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions. (arXiv:2310.03293v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03293
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38382;&#38382;&#39064;&#26816;&#27979;&#29992;&#25143;&#30340;&#38544;&#21547;&#24847;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65288;EDIT&#65289;&#26469;&#22686;&#24378;&#23545;&#35805;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#12290;&#35813;&#26694;&#26550;&#29983;&#25104;&#19982;&#29992;&#25143;&#24847;&#22270;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#19982;LLMs&#20132;&#20114;&#21644;&#39046;&#22495;&#20869;&#25628;&#32034;&#30340;&#26041;&#24335;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#30001;&#20110;&#20854;&#24320;&#25918;&#22495;&#29983;&#25104;&#33021;&#21147;&#65292;&#24050;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23558;LLMs&#24212;&#29992;&#20110;&#23545;&#35805;&#20219;&#21153;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;1.&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#26377;&#38544;&#21547;&#30340;&#24847;&#22270;&#65292;&#36825;&#21487;&#33021;&#20250;&#34987;LLMs&#24573;&#35270;&#65292;&#22240;&#27492;&#29983;&#25104;&#30340;&#22238;&#22797;&#21487;&#33021;&#19982;&#29992;&#25143;&#30340;&#24847;&#22270;&#19981;&#19968;&#33268;&#12290;2.&#23545;&#20110;LLMs&#26469;&#35828;&#65292;&#24456;&#38590;&#20840;&#38754;&#28085;&#30422;&#25152;&#26377;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#22312;&#26576;&#20123;&#29305;&#23450;&#39046;&#22495;&#20013;&#65292;&#23427;&#20204;&#30340;&#30693;&#35782;&#21487;&#33021;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#24182;&#19988;LLMs&#26080;&#27861;&#23454;&#26102;&#26356;&#26032;&#26368;&#26032;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21363;&#36890;&#36807;&#38382;&#38382;&#39064;&#26816;&#27979;&#29992;&#25143;&#38544;&#21547;&#24847;&#22270;&#26469;&#22686;&#24378;&#23545;&#35805;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#65288;EDIT&#65289;&#12290;&#39318;&#20808;&#65292;EDIT&#26681;&#25454;&#23545;&#35805;&#19978;&#19979;&#25991;&#29983;&#25104;&#19982;&#29992;&#25143;&#24847;&#22270;&#30456;&#20851;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#65307;&#28982;&#21518;&#65292;EDIT&#36890;&#36807;&#19982;LLMs&#20132;&#20114;&#21644;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#25628;&#32034;&#30340;&#26041;&#24335;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, have recently been applied to various NLP tasks due to its open-domain generation capabilities. However, there are two issues with applying LLMs to dialogue tasks. 1. During the dialogue process, users may have implicit intentions that might be overlooked by LLMs. Consequently, generated responses couldn't align with the user's intentions. 2. It is unlikely for LLMs to encompass all fields comprehensively. In certain specific domains, their knowledge may be incomplete, and LLMs cannot update the latest knowledge in real-time. To tackle these issues, we propose a framework~\emph{using LLM to \textbf{E}nhance dialogue response generation by asking questions to \textbf{D}etect user's \textbf{I}mplicit in\textbf{T}entions} (\textbf{EDIT}). Firstly, EDIT generates open questions related to the dialogue context as the potential user's intention; Then, EDIT answers those questions by interacting with LLMs and searching in domain-specific knowledg
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23450;&#20041;&#20102;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#39118;&#38505;&#65288;&#20915;&#31574;&#39118;&#38505;&#21644;&#32508;&#21512;&#39118;&#38505;&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#39118;&#38505;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#22235;&#20010;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#24110;&#21161;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DwD&#30340;&#39118;&#38505;&#35843;&#25972;&#26657;&#20934;&#26041;&#27861;&#65292;&#22312;&#25972;&#20307;NLI&#26550;&#26500;&#20013;&#20943;&#23569;&#20915;&#31574;&#39118;&#38505;&#21644;&#32508;&#21512;&#39118;&#38505;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35780;&#20272;&#26694;&#26550;&#21644;DwD&#26041;&#27861;&#20855;&#26377;&#23454;&#38469;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.03283</link><description>&lt;p&gt;
&#19968;&#31181;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24418;&#24335;&#21270;&#21644;&#26041;&#27861;&#65306;&#20351;&#29992;&#39118;&#38505;&#35843;&#25972;&#30340;&#32622;&#20449;&#24230;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores. (arXiv:2310.03283v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23450;&#20041;&#20102;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#39118;&#38505;&#65288;&#20915;&#31574;&#39118;&#38505;&#21644;&#32508;&#21512;&#39118;&#38505;&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#39118;&#38505;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#22235;&#20010;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#24110;&#21161;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DwD&#30340;&#39118;&#38505;&#35843;&#25972;&#26657;&#20934;&#26041;&#27861;&#65292;&#22312;&#25972;&#20307;NLI&#26550;&#26500;&#20013;&#20943;&#23569;&#20915;&#31574;&#39118;&#38505;&#21644;&#32508;&#21512;&#39118;&#38505;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35780;&#20272;&#26694;&#26550;&#21644;DwD&#26041;&#27861;&#20855;&#26377;&#23454;&#38469;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#37324;&#31243;&#30865;&#12290;&#23613;&#31649;&#23427;&#20204;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20063;&#23384;&#22312;&#37325;&#35201;&#30340;&#39118;&#38505;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#31561;&#20219;&#21153;&#20013;&#25152;&#24102;&#26469;&#30340;&#19981;&#21516;&#39118;&#38505;&#30340;&#31995;&#32479;&#29702;&#35299;&#38750;&#24120;&#24517;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#21644;&#24418;&#24335;&#21270;&#20102;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#39118;&#38505;&#65306;&#20915;&#31574;&#39118;&#38505;&#21644;&#32508;&#21512;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#39118;&#38505;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#20010;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#29615;&#22659;&#20013;&#30340;&#36825;&#20123;&#39118;&#38505;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DwD&#30340;&#39118;&#38505;&#35843;&#25972;&#26657;&#20934;&#26041;&#27861;&#65292;&#24110;&#21161;LLMs&#22312;&#25972;&#20307;NLI&#26550;&#26500;&#20013;&#26368;&#23567;&#21270;&#36825;&#20123;&#39118;&#38505;&#12290;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;NLI&#22522;&#20934;&#27979;&#35797;&#12289;&#19977;&#20010;&#22522;&#20934;&#32447;&#21644;&#20004;&#20010;LLMs&#65288;&#21253;&#25324;ChatGPT&#65289;&#36827;&#34892;&#35814;&#32454;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35780;&#20272;&#26694;&#26550;&#30340;&#23454;&#38469;&#25928;&#29992;&#20197;&#21450;DwD&#22312;&#20943;&#23569;&#20915;&#31574;&#39118;&#38505;&#21644;&#32508;&#21512;&#39118;&#38505;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, have achieved impressive milestones in natural language processing (NLP). Despite their impressive performance, the models are known to pose important risks. As these models are deployed in real-world applications, a systematic understanding of different risks posed by these models on tasks such as natural language inference (NLI), is much needed. In this paper, we define and formalize two distinct types of risk: decision risk and composite risk. We also propose a risk-centric evaluation framework, and four novel metrics, for assessing LLMs on these risks in both in-domain and out-of-domain settings. Finally, we propose a risk-adjusted calibration method called DwD for helping LLMs minimize these risks in an overall NLI architecture. Detailed experiments, using four NLI benchmarks, three baselines and two LLMs, including ChatGPT, show both the practical utility of the evaluation framework, and the efficacy of DwD in reducing decision and c
&lt;/p&gt;</description></item><item><title>InstructProtein&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#30693;&#35782;&#25351;&#23548;&#23454;&#29616;&#20102;&#20154;&#31867;&#21644;&#34507;&#30333;&#36136;&#35821;&#35328;&#30340;&#23545;&#40784;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#39044;&#27979;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#25991;&#26412;&#21151;&#33021;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25351;&#23548;&#29983;&#25104;&#26694;&#26550;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#34507;&#30333;&#36136;-&#25991;&#26412;&#35821;&#26009;&#20013;&#30340;&#26631;&#27880;&#19981;&#24179;&#34913;&#21644;&#25351;&#23548;&#32570;&#22833;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03269</link><description>&lt;p&gt;
InstructProtein: &#36890;&#36807;&#30693;&#35782;&#25351;&#23548;&#23454;&#29616;&#20154;&#31867;&#21644;&#34507;&#30333;&#36136;&#35821;&#35328;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
InstructProtein: Aligning Human and Protein Language via Knowledge Instruction. (arXiv:2310.03269v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03269
&lt;/p&gt;
&lt;p&gt;
InstructProtein&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#30693;&#35782;&#25351;&#23548;&#23454;&#29616;&#20102;&#20154;&#31867;&#21644;&#34507;&#30333;&#36136;&#35821;&#35328;&#30340;&#23545;&#40784;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#39044;&#27979;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#25991;&#26412;&#21151;&#33021;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25351;&#23548;&#29983;&#25104;&#26694;&#26550;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#34507;&#30333;&#36136;-&#25991;&#26412;&#35821;&#26009;&#20013;&#30340;&#26631;&#27880;&#19981;&#24179;&#34913;&#21644;&#25351;&#23548;&#32570;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#29702;&#35299;&#34507;&#30333;&#36136;&#31561;&#29983;&#29289;&#24207;&#21015;&#26041;&#38754;&#20173;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructProtein&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;LLM&#65292;&#20855;&#22791;&#20154;&#31867;&#21644;&#34507;&#30333;&#36136;&#35821;&#35328;&#30340;&#21452;&#21521;&#29983;&#25104;&#33021;&#21147;&#65306;&#65288;i&#65289;&#20197;&#34507;&#30333;&#36136;&#24207;&#21015;&#20026;&#36755;&#20837;&#65292;&#39044;&#27979;&#20854;&#25991;&#26412;&#21151;&#33021;&#25551;&#36848;&#65307;&#65288;ii&#65289;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#34507;&#30333;&#36136;&#21644;&#33258;&#28982;&#35821;&#35328;&#35821;&#26009;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#36825;&#20004;&#31181;&#35821;&#35328;&#12290;&#28982;&#21518;&#37319;&#29992;&#30417;&#30563;&#25351;&#23548;&#35843;&#25972;&#30340;&#26041;&#24335;&#65292;&#20419;&#36827;&#36825;&#20004;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#23545;&#40784;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25351;&#23548;&#29983;&#25104;&#26694;&#26550;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#34507;&#30333;&#36136;-&#25991;&#26412;&#35821;&#26009;&#20013;&#30340;&#26631;&#27880;&#19981;&#24179;&#34913;&#21644;&#25351;&#23548;&#32570;&#22833;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#25351;&#23548;&#32487;&#25215;&#20102;&#34507;&#30333;&#36136;&#30340;&#32467;&#26500;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized the field of natural language processing, but they fall short in comprehending biological sequences such as proteins. To address this challenge, we propose InstructProtein, an innovative LLM that possesses bidirectional generation capabilities in both human and protein languages: (i) taking a protein sequence as input to predict its textual function description and (ii) using natural language to prompt protein sequence generation. To achieve this, we first pre-train an LLM on both protein and natural language corpora, enabling it to comprehend individual languages. Then supervised instruction tuning is employed to facilitate the alignment of these two distinct languages. Herein, we introduce a knowledge graph-based instruction generation framework to construct a high-quality instruction dataset, addressing annotation imbalance and instruction deficits in existing protein-text corpus. In particular, the instructions inherit the structural
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#20294;&#23427;&#20204;&#23637;&#31034;&#20102;&#20851;&#38190;&#32780;&#19968;&#33268;&#30340;&#20219;&#21153;&#24615;&#33021;&#25913;&#36827;&#65292;&#36825;&#19968;&#25913;&#36827;&#26080;&#27861;&#36890;&#36807;&#20256;&#32479;&#30340;&#35780;&#20272;&#31574;&#30053;&#26469;&#25429;&#25417;&#65292;&#22240;&#20026;&#35780;&#20272;&#30340;&#31934;&#24230;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.03262</link><description>&lt;p&gt;
&#35299;&#38145;&#20174;&#26032;&#20852;&#33021;&#21147;&#20013;&#21487;&#39044;&#27979;&#30340;&#25193;&#23637;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlock Predictable Scaling from Emergent Abilities. (arXiv:2310.03262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#20294;&#23427;&#20204;&#23637;&#31034;&#20102;&#20851;&#38190;&#32780;&#19968;&#33268;&#30340;&#20219;&#21153;&#24615;&#33021;&#25913;&#36827;&#65292;&#36825;&#19968;&#25913;&#36827;&#26080;&#27861;&#36890;&#36807;&#20256;&#32479;&#30340;&#35780;&#20272;&#31574;&#30053;&#26469;&#25429;&#25417;&#65292;&#22240;&#20026;&#35780;&#20272;&#30340;&#31934;&#24230;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31185;&#23398;&#25193;&#23637;&#65292;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#23427;&#20204;&#30340;&#25193;&#23637;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20851;&#20110;&#25193;&#23637;&#29305;&#24615;&#30340;&#30740;&#31350;&#21482;&#33021;&#24471;&#20986;&#19968;&#20010;&#19981;&#23436;&#25972;&#30340;&#31572;&#26696;&#65306;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#20248;&#21270;&#25439;&#22833;&#21487;&#39044;&#27979;&#22320;&#20943;&#23569;&#65292;&#31526;&#21512;&#24050;&#24314;&#31435;&#30340;&#32553;&#25918;&#23450;&#24459;&#65307;&#28982;&#32780;&#65292;&#20219;&#21153;&#30340;&#32553;&#25918;&#23450;&#24459;&#23578;&#26410;&#24314;&#31435;&#65292;&#20219;&#21153;&#34920;&#29616;&#22312;&#25193;&#23637;&#36807;&#31243;&#20013;&#36828;&#38750;&#21487;&#39044;&#27979;&#12290;&#20219;&#21153;&#34920;&#29616;&#36890;&#24120;&#22312;&#23567;&#27169;&#22411;&#19978;&#26174;&#31034;&#20986;&#36731;&#24494;&#22686;&#30410;&#65292;&#30452;&#21040;&#27169;&#22411;&#36229;&#36807;&#26576;&#20010;&#22823;&#23567;&#38408;&#20540;&#21518;&#25165;&#20986;&#29616;&#26174;&#33879;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#8220;&#26032;&#20852;&#33021;&#21147;&#8221;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;&#23567;&#27169;&#22411;&#34920;&#29616;&#20986;&#36731;&#24494;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#23637;&#29616;&#20102;&#20851;&#38190;&#32780;&#19968;&#33268;&#30340;&#20219;&#21153;&#24615;&#33021;&#25913;&#36827;&#65292;&#36825;&#20123;&#25913;&#36827;&#26080;&#27861;&#34987;&#20256;&#32479;&#35780;&#20272;&#31574;&#30053;&#25429;&#25417;&#21040;&#65292;&#22240;&#20026;&#27979;&#37327;&#20998;&#36776;&#29575;&#19981;&#36275;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#25913;&#36827;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PassUntil&#65292;&#22312;&#35299;&#30721;&#38454;&#27573;&#36890;&#36807;&#22823;&#35268;&#27169;&#25277;&#26679;&#36827;&#34892;&#35780;&#20272;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scientific scale-up of large language models (LLMs) necessitates a comprehensive understanding of their scaling properties. However, the existing literature on the scaling properties only yields an incomplete answer: optimization loss decreases predictably as the model size increases, in line with established scaling law; yet no scaling law for task has been established and the task performances are far from predictable during scaling. Task performances typically show minor gains on small models until they improve dramatically once models exceed a size threshold, exemplifying the ``emergent abilities''. In this study, we discover that small models, although they exhibit minor performance, demonstrate critical and consistent task performance improvements that are not captured by conventional evaluation strategies due to insufficient measurement resolution. To measure such improvements, we introduce PassUntil, an evaluation strategy through massive sampling in the decoding phase. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;PPNL&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;-&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23569;&#26679;&#26412;&#30340;GPT-4&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20173;&#26377;&#24453;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.03249</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#25104;&#20026;&#22909;&#30340;&#36335;&#24452;&#35268;&#21010;&#22120;&#21527;&#65311;&#23545;&#31354;&#38388;-&#26102;&#38388;&#25512;&#29702;&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning. (arXiv:2310.03249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;PPNL&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;-&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23569;&#26679;&#26412;&#30340;GPT-4&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20173;&#26377;&#24453;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#21644;&#31354;&#38388;&#25512;&#29702;&#30340;&#22330;&#26223;&#20013;&#20173;&#28982;&#38754;&#20020;&#38480;&#21046;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31216;&#20026;&#33258;&#28982;&#35821;&#35328;&#36335;&#24452;&#35268;&#21010;&#65288;PPNL&#65289;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#36890;&#36807;&#21046;&#23450;&#38656;&#35201;LLM&#23548;&#33322;&#21040;&#30446;&#26631;&#20301;&#32622;&#24182;&#36991;&#24320;&#38556;&#30861;&#29289;&#21644;&#36981;&#23432;&#32422;&#26463;&#26465;&#20214;&#30340;&#8220;&#36335;&#24452;&#35268;&#21010;&#8221;&#20219;&#21153;&#65292;&#35780;&#20272;LLM&#30340;&#31354;&#38388;-&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#21033;&#29992;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#21253;&#25324;GPT-4&#22312;&#20869;&#30340;LLM&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#21644;&#21508;&#31181;&#35268;&#27169;&#30340;BART&#21644;T5&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#31034;LLM&#36827;&#34892;&#25512;&#29702;&#21644;&#20132;&#20114;&#34892;&#21160;&#26102;&#65292;&#23569;&#26679;&#26412;&#30340;GPT-4&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26080;&#27861;&#36827;&#34892;&#38271;&#26399;&#26102;&#38388;&#25512;&#29702;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LLM&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success across a wide spectrum of tasks; however, they still face limitations in scenarios that demand long-term planning and spatial reasoning. To facilitate this line of research, in this work, we propose a new benchmark, termed $\textbf{P}$ath $\textbf{P}$lanning from $\textbf{N}$atural $\textbf{L}$anguage ($\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by formulating ''path planning'' tasks that require an LLM to navigate to target locations while avoiding obstacles and adhering to constraints. Leveraging this benchmark, we systematically investigate LLMs including GPT-4 via different few-shot prompting methodologies and BART and T5 of various sizes via fine-tuning. Our experimental results show the promise of few-shot GPT-4 in spatial reasoning, when it is prompted to reason and act interleavedly, although it still fails to make long-term temporal reasoning. In contrast, while fine-tuned LLMs achieve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#30340;&#35821;&#35328;&#34920;&#24449;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#23884;&#20837;&#65292;&#20998;&#26512;&#25233;&#37057;&#30151;&#29366;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#26631;&#20934;&#20998;&#31867;&#20196;&#29260;&#23884;&#20837;&#21644;&#22522;&#20110;&#39057;&#29575;&#30340;&#20195;&#35789;&#20998;&#26512;&#65292;&#19978;&#19979;&#25991;&#21270;&#30340;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#23884;&#20837;&#22312;&#39044;&#27979;&#25233;&#37057;&#30151;&#29366;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.03232</link><description>&lt;p&gt;
&#28145;&#24230;&#34920;&#24449;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#20197;&#39044;&#27979;&#25233;&#37057;&#30151;&#29366;&#30340;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Deep Representations of First-person Pronouns for Prediction of Depression Symptom Severity. (arXiv:2310.03232v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#30340;&#35821;&#35328;&#34920;&#24449;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#23884;&#20837;&#65292;&#20998;&#26512;&#25233;&#37057;&#30151;&#29366;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#26631;&#20934;&#20998;&#31867;&#20196;&#29260;&#23884;&#20837;&#21644;&#22522;&#20110;&#39057;&#29575;&#30340;&#20195;&#35789;&#20998;&#26512;&#65292;&#19978;&#19979;&#25991;&#21270;&#30340;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#23884;&#20837;&#22312;&#39044;&#27979;&#25233;&#37057;&#30151;&#29366;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20998;&#26512;&#20351;&#29992;&#31532;&#19968;&#20154;&#31216;&#21333;&#25968;&#20195;&#35789;&#21487;&#20197;&#25581;&#31034;&#20010;&#20307;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#29305;&#21035;&#26159;&#25233;&#37057;&#30151;&#29366;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#36825;&#20123;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#25991;&#26412;&#25968;&#25454;&#20013;&#31532;&#19968;&#20154;&#31216;&#21333;&#25968;&#20195;&#35789;&#30340;&#39057;&#29575;&#26469;&#24471;&#20986;&#32467;&#35770;&#12290;&#28982;&#32780;&#65292;&#35745;&#25968;&#19981;&#33021;&#25429;&#25417;&#21040;&#36825;&#20123;&#20195;&#35789;&#30340;&#20351;&#29992;&#26041;&#24335;&#12290;&#36817;&#26399;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#30340;&#36827;&#23637;&#21033;&#29992;&#20102;&#29983;&#25104;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#21033;&#29992;&#20174;&#19978;&#19979;&#25991;&#35821;&#35328;&#34920;&#24449;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#23884;&#20837;&#26469;&#25429;&#25417;&#36825;&#20123;&#20195;&#35789;&#30340;&#20351;&#29992;&#26041;&#24335;&#65292;&#20197;&#20998;&#26512;&#24515;&#29702;&#29366;&#24577;&#12290;&#35780;&#20272;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#22312;&#22312;&#32447;&#24515;&#29702;&#27835;&#30103;&#26399;&#38388;&#21457;&#36865;&#30340;&#21435;&#36523;&#20221;&#21270;&#30340;&#25991;&#26412;&#28040;&#24687;&#65292;&#20854;&#20013;&#27599;&#21608;&#35780;&#20272;&#20102;&#25233;&#37057;&#30151;&#29366;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#20998;&#31867;&#20196;&#29260;&#23884;&#20837;&#21644;&#22522;&#20110;&#39057;&#29575;&#30340;&#20195;&#35789;&#20998;&#26512;&#30456;&#27604;&#65292;&#19978;&#19979;&#25991;&#21270;&#30340;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#23884;&#20837;&#22312;&#39044;&#27979;&#25233;&#37057;&#30151;&#29366;&#30340;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has shown that analyzing the use of first-person singular pronouns can provide insight into individuals' mental status, especially depression symptom severity. These findings were generated by counting frequencies of first-person singular pronouns in text data. However, counting doesn't capture how these pronouns are used. Recent advances in neural language modeling have leveraged methods generating contextual embeddings. In this study, we sought to utilize the embeddings of first-person pronouns obtained from contextualized language representation models to capture ways these pronouns are used, to analyze mental status. De-identified text messages sent during online psychotherapy with weekly assessment of depression severity were used for evaluation. Results indicate the advantage of contextualized first-person pronoun embeddings over standard classification token embeddings and frequency-based pronoun analysis results in predicting depression symptom severity. This suggest
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#21047;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#30340;&#20107;&#23454;&#24615;&#65292;&#24341;&#20837;&#20102;FreshQA&#36825;&#19968;&#21160;&#24577;&#38382;&#31572;&#22522;&#20934;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#24182;&#34920;&#26126;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.03214</link><description>&lt;p&gt;
FreshLLMs: &#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#22686;&#24378;&#30340;&#26041;&#27861;&#21047;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation. (arXiv:2310.03214v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#21047;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#30340;&#20107;&#23454;&#24615;&#65292;&#24341;&#20837;&#20102;FreshQA&#36825;&#19968;&#21160;&#24577;&#38382;&#31572;&#22522;&#20934;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#24182;&#34920;&#26126;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#19981;&#36827;&#34892;&#26356;&#26032;&#65292;&#22240;&#27492;&#32570;&#20047;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#21160;&#24577;&#36866;&#24212;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#22312;&#27979;&#35797;&#24403;&#21069;&#19990;&#30028;&#30693;&#35782;&#30340;&#38382;&#39064;&#22238;&#31572;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FreshQA&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#21160;&#24577;&#38382;&#31572;&#22522;&#20934;&#65292;&#21253;&#25324;&#24191;&#27867;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#31867;&#22411;&#65292;&#21253;&#25324;&#38656;&#35201;&#24555;&#36895;&#21464;&#21270;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#38656;&#35201;&#25581;&#31034;&#38169;&#35823;&#21069;&#25552;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21452;&#27169;&#24335;&#35780;&#20272;&#36807;&#31243;&#20013;&#23545;&#22810;&#31181;&#38381;&#28304;&#21644;&#24320;&#28304;LLM&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#21516;&#26102;&#27979;&#37327;&#27491;&#30830;&#24615;&#21644;&#34394;&#26500;&#24615;&#12290;&#36890;&#36807;&#28041;&#21450;&#36229;&#36807;50K&#20010;&#35780;&#21028;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#25913;&#36827;&#30340;&#26174;&#33879;&#31354;&#38388;&#65306;&#20363;&#22914;&#65292;&#25152;&#26377;&#27169;&#22411;&#65288;&#26080;&#35770;&#27169;&#22411;&#22823;&#23567;&#22914;&#20309;&#65289;&#22312;&#28041;&#21450;&#24555;&#36895;&#21464;&#21270;&#30340;&#30693;&#35782;&#21644;&#38169;&#35823;&#21069;&#25552;&#30340;&#38382;&#39064;&#19978;&#37117;&#38754;&#20020;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22797;&#26434;&#25512;&#29702;&#12289;&#23545;&#35805;&#12289;&#22270;&#20687;&#25551;&#36848;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#23454;&#39564;&#65292;&#20026;&#23558;&#22810;&#27169;&#24577;&#33021;&#21147;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.03211</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Performance of Multimodal Language Models. (arXiv:2310.03211v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22797;&#26434;&#25512;&#29702;&#12289;&#23545;&#35805;&#12289;&#22270;&#20687;&#25551;&#36848;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#23454;&#39564;&#65292;&#20026;&#23558;&#22810;&#27169;&#24577;&#33021;&#21147;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29420;&#31435;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#36890;&#36807;&#27169;&#22411;&#23233;&#25509;&#30340;&#26041;&#24335;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21518;&#65292;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#26377;&#26395;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#19981;&#21516;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#22797;&#26434;&#25512;&#29702;&#12289;&#23545;&#35805;&#12289;&#22270;&#20687;&#25551;&#36848;&#12289;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#20108;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#23558;&#22810;&#27169;&#24577;&#33021;&#21147;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#25351;&#23548;&#26550;&#26500;&#36873;&#25321;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#27809;&#26377;&#36275;&#22815;&#22320;&#35299;&#20915;&#22810;&#26679;&#21270;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) have demonstrated promising zero-shot generalization capabilities across various downstream tasks. Recent research has introduced multimodal capabilities to LLMs by integrating independently pretrained vision encoders through model grafting. These multimodal variants undergo instruction tuning, similar to LLMs, enabling effective zero-shot generalization for multimodal tasks. This study conducts a comparative analysis of different multimodal instruction tuning approaches and evaluates their performance across a range of tasks, including complex reasoning, conversation, image captioning, multiple-choice questions (MCQs), and binary classification. Through rigorous benchmarking and ablation experiments, we reveal key insights for guiding architectural choices when incorporating multimodal capabilities into LLMs. However, current approaches have limitations; they do not sufficiently address the need for a diverse multimodal instruction datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#24037;&#21019;&#24314;&#30340;&#22810;&#36718;&#33487;&#26684;&#25289;&#24213;&#24314;&#35758;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24110;&#21161;&#21021;&#23398;&#32773;&#31243;&#24207;&#21592;&#20462;&#22797;&#31616;&#21333;&#35745;&#31639;&#38382;&#39064;&#30340;&#38169;&#35823;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23545;&#19968;&#31995;&#21015;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#33487;&#26684;&#25289;&#24213;&#35843;&#35797;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03210</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#65311;&#20851;&#20110;&#20195;&#30721;&#35843;&#35797;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Employ the Socratic Method? Experiments with Code Debugging. (arXiv:2310.03210v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#24037;&#21019;&#24314;&#30340;&#22810;&#36718;&#33487;&#26684;&#25289;&#24213;&#24314;&#35758;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24110;&#21161;&#21021;&#23398;&#32773;&#31243;&#24207;&#21592;&#20462;&#22797;&#31616;&#21333;&#35745;&#31639;&#38382;&#39064;&#30340;&#38169;&#35823;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23545;&#19968;&#31995;&#21015;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#33487;&#26684;&#25289;&#24213;&#35843;&#35797;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#37319;&#29992;&#33487;&#26684;&#25289;&#24213;&#25945;&#23398;&#27861;&#26102;&#65292;&#25351;&#23548;&#21592;&#20250;&#24341;&#23548;&#23398;&#29983;&#33258;&#24049;&#35299;&#20915;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#21892;&#23398;&#20064;&#32467;&#26524;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;&#33258;&#21160;&#21270;&#30340;&#33487;&#26684;&#25289;&#24213;&#23545;&#35805;&#20195;&#29702;&#21487;&#20197;&#22686;&#24378;&#20154;&#31867;&#25945;&#23398;&#65292;&#24182;&#25552;&#20379;&#25152;&#38656;&#30340;&#35268;&#27169;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#21457;&#23637;&#21463;&#21040;&#32570;&#20047;&#36866;&#21512;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25163;&#24037;&#21019;&#24314;&#30340;&#22810;&#36718;&#33487;&#26684;&#25289;&#24213;&#24314;&#35758;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24110;&#21161;&#21021;&#23398;&#32773;&#31243;&#24207;&#21592;&#20462;&#22797;&#31616;&#21333;&#35745;&#31639;&#38382;&#39064;&#30340;&#38169;&#35823;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#21518;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#23545;&#19968;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#33487;&#26684;&#25289;&#24213;&#35843;&#35797;&#33021;&#21147;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174;&#24494;&#35843;&#20197;&#25351;&#20196;&#20026;&#22522;&#30784;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#22120;Flan-T5&#65292;&#21040;&#38646;-shot&#21644;&#26356;&#22823;&#30340;GPT-4&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#24050;&#22312;&#19979;&#26041;&#38142;&#25509;&#20013;&#20813;&#36153;&#25552;&#20379;&#32473;&#30740;&#31350;&#32773;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
When employing the Socratic method of teaching, instructors guide students toward solving a problem on their own rather than providing the solution directly. While this strategy can substantially improve learning outcomes, it is usually time-consuming and cognitively demanding. Automated Socratic conversational agents can augment human instruction and provide the necessary scale, however their development is hampered by the lack of suitable data for training and evaluation. In this paper, we introduce a manually created dataset of multi-turn Socratic advice that is aimed at helping a novice programmer fix buggy solutions to simple computational problems. The dataset is then used for benchmarking the Socratic debugging abilities of a number of language models, ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5 to zero-shot and chain of thought prompting of the much larger GPT-4. The code and datasets are made freely available for research at the link below. 
&lt;/p&gt;</description></item><item><title>&#36861;&#36394;&#24320;&#25918;&#31185;&#23398;&#23454;&#36341;&#65292;&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20998;&#26512;&#20102;&#25968;&#25454;&#21644;&#26041;&#27861;&#20849;&#20139;&#30340;&#37319;&#29992;&#24773;&#20917;&#21644;&#23545;&#25991;&#31456;&#25509;&#21463;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#23454;&#36341;&#27491;&#22312;&#25193;&#25955;&#65292;&#23588;&#20854;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.03193</link><description>&lt;p&gt;
&#24320;&#25918;&#31185;&#23398;&#30340;&#23835;&#36215;&#65306;&#36861;&#36394;&#25968;&#25454;&#21644;&#26041;&#27861;&#20849;&#20139;&#23454;&#36341;&#30340;&#28436;&#21464;&#21644;&#24863;&#30693;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
The Rise of Open Science: Tracking the Evolution and Perceived Value of Data and Methods Link-Sharing Practices. (arXiv:2310.03193v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03193
&lt;/p&gt;
&lt;p&gt;
&#36861;&#36394;&#24320;&#25918;&#31185;&#23398;&#23454;&#36341;&#65292;&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20998;&#26512;&#20102;&#25968;&#25454;&#21644;&#26041;&#27861;&#20849;&#20139;&#30340;&#37319;&#29992;&#24773;&#20917;&#21644;&#23545;&#25991;&#31456;&#25509;&#21463;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#23454;&#36341;&#27491;&#22312;&#25193;&#25955;&#65292;&#23588;&#20854;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36164;&#21161;&#26426;&#26500;&#21644;&#26399;&#21002;&#36234;&#26469;&#36234;&#20513;&#23548;&#24320;&#25918;&#31185;&#23398;&#23454;&#36341;&#65288;&#20363;&#22914;&#25968;&#25454;&#21644;&#26041;&#27861;&#20849;&#20139;&#65289;&#65292;&#20197;&#25552;&#39640;&#31185;&#23398;&#30340;&#36879;&#26126;&#24230;&#12289;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#37327;&#21270;&#36825;&#20123;&#23454;&#36341;&#30340;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;arXiv&#19978;114&#19975;&#31687;&#35770;&#25991;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20195;&#34920;&#20102;&#29289;&#29702;&#23398;&#12289;&#25968;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#65292;&#20998;&#26512;&#20102;&#25968;&#25454;&#21644;&#26041;&#27861;&#20849;&#20139;&#23454;&#36341;&#30340;&#37319;&#29992;&#24773;&#20917;&#20197;&#21450;&#23545;&#25991;&#31456;&#25509;&#21463;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35782;&#21035;&#25968;&#25454;&#21644;&#26041;&#27861;&#30340;&#38142;&#25509;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#26681;&#25454;&#35770;&#25991;&#20013;&#30340;&#19978;&#19979;&#25991;&#25552;&#21450;&#33258;&#21160;&#20998;&#31867;URL&#31867;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35770;&#25991;&#21253;&#21547;&#27492;&#31867;&#38142;&#25509;&#65292;&#34920;&#26126;&#38142;&#25509;&#21040;&#26041;&#27861;&#21644;&#25968;&#25454;&#30340;&#20849;&#20139;&#23454;&#36341;&#27491;&#22312;&#25193;&#25955;&#12290;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#65292;&#21487;&#37325;&#22797;&#24615;&#30340;&#21162;&#21147;&#20063;&#22312;&#25193;&#25955;&#65292;&#22240;&#20026;&#30456;&#21516;&#30340;&#38142;&#25509;&#36234;&#26469;&#36234;&#22810;&#22320;&#22312;&#35770;&#25991;&#20013;&#34987;&#37325;&#22797;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, funding agencies and journals increasingly advocate for open science practices (e.g. data and method sharing) to improve the transparency, access, and reproducibility of science. However, quantifying these practices at scale has proven difficult. In this work, we leverage a large-scale dataset of 1.1M papers from arXiv that are representative of the fields of physics, math, and computer science to analyze the adoption of data and method link-sharing practices over time and their impact on article reception. To identify links to data and methods, we train a neural text classification model to automatically classify URL types based on contextual mentions in papers. We find evidence that the practice of link-sharing to methods and data is spreading as more papers include such URLs over time. Reproducibility efforts may also be spreading because the same links are being increasingly reused across papers (especially in computer science); and these links are increasingly con
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#25913;&#36827;&#25968;&#23398;&#38382;&#31572;&#65292;&#22312;&#21487;&#38752;&#24615;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;</title><link>http://arxiv.org/abs/2310.03184</link><description>&lt;p&gt;
&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#25913;&#36827;&#25968;&#23398;&#38382;&#31572;&#65306;&#22312;&#21487;&#38752;&#24615;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference. (arXiv:2310.03184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03184
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#25913;&#36827;&#25968;&#23398;&#38382;&#31572;&#65292;&#22312;&#21487;&#38752;&#24615;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20013;&#23398;&#25968;&#23398;&#23398;&#29983;&#26469;&#35828;&#65292;&#19982;&#23548;&#24072;&#36827;&#34892;&#20114;&#21160;&#38382;&#31572;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#23398;&#20064;&#26041;&#24335;&#12290;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#26032;&#20852;&#33021;&#21147;&#23548;&#33268;&#20154;&#20204;&#23545;&#33258;&#21160;&#21270;&#37096;&#20998;&#36741;&#23548;&#36807;&#31243;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#21253;&#25324;&#25903;&#25345;&#25968;&#23398;&#27010;&#24565;&#30340;&#27010;&#24565;&#35752;&#35770;&#30340;&#20114;&#21160;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#27169;&#22411;&#23545;&#25968;&#23398;&#38382;&#39064;&#30340;&#22238;&#31572;&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#65292;&#25110;&#32773;&#19982;&#25945;&#32946;&#32972;&#26223;&#19981;&#21305;&#37197;&#65292;&#20363;&#22914;&#19982;&#23398;&#26657;&#30340;&#35838;&#31243;&#19981;&#19968;&#33268;&#12290;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#20854;&#20013;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#22312;&#29983;&#25104;&#27169;&#22411;&#25552;&#31034;&#20013;&#21152;&#20837;&#32463;&#39564;&#35777;&#30340;&#22806;&#37096;&#30693;&#35782;&#36164;&#28304;&#26469;&#25552;&#39640;&#22238;&#31572;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25552;&#31034;&#26469;&#26816;&#32034;&#24182;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#25968;&#23398;&#25945;&#31185;&#20070;&#20013;&#30340;&#20869;&#23481;&#65292;&#20197;&#22238;&#31572;&#30495;&#23454;&#23398;&#29983;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#19968;&#39033;&#22810;&#26465;&#20214;&#35843;&#26597;&#26469;&#35780;&#20272;&#36825;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#20013;&#23398;&#20195;&#25968;&#21644;&#20960;&#20309;&#38382;&#31572;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
For middle-school math students, interactive question-answering (QA) with tutors is an effective way to learn. The flexibility and emergent capabilities of generative large language models (LLMs) has led to a surge of interest in automating portions of the tutoring process - including interactive QA to support conceptual discussion of mathematical concepts. However, LLM responses to math questions can be incorrect or mismatched to the educational context such as being misaligned with a school's curriculum. One potential solution is retrieval-augmented generation (RAG), which involves incorporating a vetted external knowledge source in the LLM prompt to increase response quality. In this paper, we designed prompts that retrieve and use content from a high-quality open-source math textbook to generate responses to real student questions. We evaluate the efficacy of this RAG system for middle-school algebra and geometry QA by administering a multi-condition survey, finding that humans p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#20174;GPT-4&#20013;&#26597;&#35810;&#20020;&#24202;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23558;&#28508;&#22312;&#30340;&#22270;&#20687;&#29305;&#24449;&#36716;&#21270;&#20026;&#26126;&#30830;&#30340;&#27010;&#24565;&#65292;&#20197;&#35299;&#20915;&#22312;&#30495;&#23454;&#19990;&#30028;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#23398;&#20064;&#19981;&#24819;&#20851;&#30340;&#30456;&#20851;&#24615;&#21644;&#32570;&#20047;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03182</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Robust and Interpretable Medical Image Classifiers via Concept Bottleneck Models. (arXiv:2310.03182v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#20174;GPT-4&#20013;&#26597;&#35810;&#20020;&#24202;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23558;&#28508;&#22312;&#30340;&#22270;&#20687;&#29305;&#24449;&#36716;&#21270;&#20026;&#26126;&#30830;&#30340;&#27010;&#24565;&#65292;&#20197;&#35299;&#20915;&#22312;&#30495;&#23454;&#19990;&#30028;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#23398;&#20064;&#19981;&#24819;&#20851;&#30340;&#30456;&#20851;&#24615;&#21644;&#32570;&#20047;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#26159;&#21307;&#30103;&#20445;&#20581;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#26377;&#26395;&#20943;&#36731;&#21307;&#29983;&#30340;&#24037;&#20316;&#36127;&#25285;&#24182;&#20419;&#36827;&#23545;&#24739;&#32773;&#30340;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#26102;&#65292;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#31070;&#32463;&#27169;&#22411;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#19981;&#30456;&#20851;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#26159;&#25152;&#26399;&#26395;&#30340;&#29305;&#24449;&#65292;&#36825;&#22312;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#26102;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#36275;&#65288;&#20363;&#22914;&#65292;&#20855;&#26377;&#19981;&#21516;&#24180;&#40836;&#30340;&#24739;&#32773;&#65289;&#12290;&#20854;&#27425;&#65292;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#32570;&#20047;&#35299;&#37322;&#24615;&#12290;&#22312;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#26102;&#65292;&#20102;&#35299;&#27169;&#22411;&#20026;&#20309;&#20570;&#20986;&#20915;&#31574;&#23545;&#20110;&#21487;&#20449;&#24230;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#29992;&#33258;&#28982;&#35821;&#35328;&#27010;&#24565;&#26500;&#24314;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;GPT-4&#20013;&#26597;&#35810;&#20020;&#24202;&#27010;&#24565;&#65292;&#28982;&#21518;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23558;&#28508;&#22312;&#30340;&#22270;&#20687;&#29305;&#24449;&#36716;&#21270;&#20026;&#26126;&#30830;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20843;&#20010;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Medical image classification is a critical problem for healthcare, with the potential to alleviate the workload of doctors and facilitate diagnoses of patients. However, two challenges arise when deploying deep learning models to real-world healthcare applications. First, neural models tend to learn spurious correlations instead of desired features, which could fall short when generalizing to new domains (e.g., patients with different ages). Second, these black-box models lack interpretability. When making diagnostic predictions, it is important to understand why a model makes a decision for trustworthy and safety considerations. In this paper, to address these two limitations, we propose a new paradigm to build robust and interpretable medical image classifiers with natural language concepts. Specifically, we first query clinical concepts from GPT-4, then transform latent image features into explicit concepts with a vision-language model. We systematically evaluate our method on eight
&lt;/p&gt;</description></item><item><title>$\mathcal{B}$-Coder&#26159;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#31243;&#24207;&#21512;&#25104;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03173</link><description>&lt;p&gt;
$\mathcal{B}$-Coder: &#22522;&#20110;&#20215;&#20540;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
$\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis. (arXiv:2310.03173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03173
&lt;/p&gt;
&lt;p&gt;
$\mathcal{B}$-Coder&#26159;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#31243;&#24207;&#21512;&#25104;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#25191;&#34892;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#21512;&#25104;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#21019;&#24314;&#20934;&#30830;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#12290;&#35813;&#39046;&#22495;&#32467;&#21512;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21147;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#35813;&#38598;&#25104;&#20027;&#35201;&#20851;&#27880;&#30452;&#25509;&#20248;&#21270;&#21151;&#33021;&#27491;&#30830;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#25439;&#22833;&#12290;&#23613;&#31649;&#24403;&#21069;&#25991;&#29486;&#20027;&#35201;&#25903;&#25345;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#20294;&#31243;&#24207;&#21512;&#25104;&#30340;&#23646;&#24615;&#34920;&#26126;&#19982;&#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#33258;&#28982;&#20860;&#23481;&#12290;&#36825;&#28304;&#20110;&#20154;&#31867;&#31243;&#24207;&#21592;&#24320;&#21457;&#30340;&#20016;&#23500;&#31163;&#32447;&#31243;&#24207;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#21333;&#20803;&#27979;&#35797;&#23545;&#29983;&#25104;&#30340;&#31243;&#24207;&#36827;&#34892;&#30452;&#35266;&#39564;&#35777;&#65288;&#21363;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#23481;&#26131;&#33719;&#24471;&#22870;&#21169;&#30340;&#35821;&#35328;&#34920;&#36798;&#65289;&#12290;&#19982;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#25105;&#20204;&#30340;$\mathcal{B}$-Coder&#65288;&#21457;&#38899;&#20026;Bellman coder&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program synthesis aims to create accurate, executable code from natural language descriptions. This field has leveraged the power of reinforcement learning (RL) in conjunction with large language models (LLMs), significantly enhancing code generation capabilities. This integration focuses on directly optimizing functional correctness, transcending conventional supervised losses. While current literature predominantly favors policy-based algorithms, attributes of program synthesis suggest a natural compatibility with value-based methods. This stems from rich collection of off-policy programs developed by human programmers, and the straightforward verification of generated programs through automated unit testing (i.e. easily obtainable rewards in RL language). Diverging from the predominant use of policy-based algorithms, our work explores the applicability of value-based approaches, leading to the development of our $\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MetaTool&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#12290;&#22522;&#20934;&#20013;&#21253;&#21547;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#29992;&#20110;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2310.03128</link><description>&lt;p&gt;
MetaTool&#22522;&#20934;&#65306;&#20915;&#23450;&#26159;&#21542;&#20351;&#29992;&#24037;&#20855;&#21644;&#36873;&#25321;&#20351;&#29992;&#21738;&#20010;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use. (arXiv:2310.03128v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MetaTool&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#12290;&#22522;&#20934;&#20013;&#21253;&#21547;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#29992;&#20110;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;LLMs&#30340;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#12290;&#23427;&#20204;&#20027;&#35201;&#30740;&#31350;&#20102;LLMs&#22914;&#20309;&#26377;&#25928;&#22320;&#19982;&#32473;&#23450;&#30340;&#29305;&#23450;&#24037;&#20855;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;LLMs&#20805;&#24403;&#26234;&#33021;&#20307;&#30340;&#22330;&#26223;&#20013;&#65292;&#20363;&#22914;AutoGPT&#21644;MetaGPT&#24212;&#29992;&#20013;&#65292;LLMs&#34987;&#26399;&#26395;&#21442;&#19982;&#28041;&#21450;&#26159;&#21542;&#20351;&#29992;&#24037;&#20855;&#20197;&#21450;&#20174;&#21487;&#29992;&#24037;&#20855;&#38598;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#24037;&#20855;&#26469;&#28385;&#36275;&#29992;&#25143;&#35831;&#27714;&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MetaTool&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#30340;&#22522;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#35813;&#22522;&#20934;&#20013;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20197;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#30340;&#25552;&#31034;&#24418;&#24335;&#20986;&#29616;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#21253;&#25324;&#21333;&#19968;&#24037;&#20855;&#21644;&#22810;&#31181;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-too
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.03094</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#31181;&#24378;&#22823;&#30340;&#24615;&#33021;&#36890;&#24120;&#20276;&#38543;&#30528;&#20351;&#29992;&#20184;&#36153;API&#26381;&#21153;&#30340;&#39640;&#26114;&#36153;&#29992;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#21160;&#26426;&#26159;&#20026;&#20102;&#30740;&#31350;&#26500;&#24314;LLM&#32423;&#32852;&#20197;&#33410;&#32422;&#20351;&#29992;LLM&#30340;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#36827;&#34892;&#25512;&#29702;&#65288;&#20363;&#22914;&#25968;&#23398;&#12289;&#22240;&#26524;&#25512;&#29702;&#65289;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#32423;&#32852;&#31649;&#36947;&#36981;&#24490;&#19968;&#20010;&#30452;&#35266;&#30340;&#24605;&#24819;&#65292;&#21363;&#31616;&#21333;&#30340;&#38382;&#39064;&#21487;&#20197;&#30001;&#19968;&#20010;&#26356;&#24369;&#20294;&#26356;&#23454;&#24800;&#30340;LLM&#26469;&#35299;&#20915;&#65292;&#32780;&#21482;&#26377;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#25165;&#38656;&#35201;&#26356;&#24378;&#22823;&#12289;&#26356;&#26114;&#36149;&#30340;LLM&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#20915;&#31574;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#26356;&#24369;&#30340;LLM&#30340;&#8220;&#31572;&#26696;&#19968;&#33268;&#24615;&#8221;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31572;&#26696;&#37319;&#26679;&#21644;&#19968;&#33268;&#24615;&#26816;&#26597;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#24605;&#32500;&#34920;&#31034;&#65288;&#21363;&#36830;&#32493;&#24605;&#32500;&#21644;&#31243;&#24207;&#24605;&#32500;&#65289;&#30340;&#28151;&#21512;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5-turbo&#21644;GPT-4&#20316;&#20026;&#36739;&#24369;&#30340;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65292;&#21363;&#36127;&#36131;&#32534;&#30721;&#29305;&#23450;&#30693;&#35782;&#30340;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#22320;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21448;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.03084</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Discovering Knowledge-Critical Subnetworks in Pretrained Language Models. (arXiv:2310.03084v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65292;&#21363;&#36127;&#36131;&#32534;&#30721;&#29305;&#23450;&#30693;&#35782;&#30340;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#22320;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21448;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#38544;&#21547;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#23450;&#20301;&#36825;&#20123;&#34920;&#31034;&#24182;&#23558;&#20854;&#35299;&#31163;&#20986;&#26469;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21253;&#21547;&#20102;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65306;&#36127;&#36131;&#32534;&#30721;&#27169;&#22411;&#25152;&#35760;&#24518;&#30340;&#29305;&#23450;&#30693;&#35782;&#30340;&#29305;&#23450;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#26469;&#21457;&#29616;&#36825;&#20123;&#23376;&#32593;&#32476;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#26469;&#31934;&#30830;&#22320;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;GPT2&#21464;&#20307;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#39640;&#24230;&#31232;&#30095;&#23376;&#32593;&#32476;&#65288;98%+&#65289;&#65292;&#23427;&#20204;&#20165;&#36127;&#36131;&#29305;&#23450;&#30340;&#20851;&#31995;&#30693;&#35782;&#38598;&#21512;&#12290;&#24403;&#21024;&#38500;&#36825;&#20123;&#23376;&#32593;&#32476;&#26102;&#65292;&#21097;&#20313;&#30340;&#32593;&#32476;&#20173;&#20445;&#25345;&#20102;&#22823;&#37096;&#20998;&#20854;&#21021;&#22987;&#23481;&#37327;&#65288;&#23545;&#35821;&#35328;&#21644;&#20854;&#20182;&#35760;&#24518;&#20851;&#31995;&#30340;&#24314;&#27169;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various knowledge-critical subnetworks: particular sparse computational subgraphs responsible for encoding specific knowledge the model has memorized. We propose a multi-objective differentiable weight masking scheme to discover these subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original language model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+) that are solely responsible for specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial capacity (modeling language and other memorized rel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#33539;&#24335;&#65306;&#24605;&#32771;&#20026;&#20102;&#34892;&#21160;&#65288;T4D&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#25797;&#38271;&#36319;&#36394;&#35282;&#33394;&#30340;&#20449;&#24565;&#65292;&#20294;&#22312;&#23558;&#36825;&#20123;&#25512;&#26029;&#36716;&#21270;&#20026;&#25112;&#30053;&#34892;&#21160;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#26680;&#24515;&#25361;&#25112;&#22312;&#20110;&#35782;&#21035;&#38544;&#21547;&#30340;&#20851;&#20110;&#24515;&#29702;&#29366;&#24577;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#26159;&#26126;&#30830;&#35810;&#38382;&#65292;&#36825;&#20250;&#23548;&#33268;&#27491;&#30830;&#30340;&#34892;&#21160;&#36873;&#25321;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.03051</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20855;&#26377;&#24515;&#28789;&#29702;&#35770;&#30340;&#26234;&#33021;&#20307;&#20043;&#38388;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
How FaR Are Large Language Models From Agents with Theory-of-Mind?. (arXiv:2310.03051v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#33539;&#24335;&#65306;&#24605;&#32771;&#20026;&#20102;&#34892;&#21160;&#65288;T4D&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#25797;&#38271;&#36319;&#36394;&#35282;&#33394;&#30340;&#20449;&#24565;&#65292;&#20294;&#22312;&#23558;&#36825;&#20123;&#25512;&#26029;&#36716;&#21270;&#20026;&#25112;&#30053;&#34892;&#21160;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#26680;&#24515;&#25361;&#25112;&#22312;&#20110;&#35782;&#21035;&#38544;&#21547;&#30340;&#20851;&#20110;&#24515;&#29702;&#29366;&#24577;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#26159;&#26126;&#30830;&#35810;&#38382;&#65292;&#36825;&#20250;&#23548;&#33268;&#27491;&#30830;&#30340;&#34892;&#21160;&#36873;&#25321;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#24605;&#32771;&#26159;&#20026;&#20102;&#34892;&#21160;&#12290;&#8221;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#26469;&#25512;&#26029;&#20182;&#20154;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#36825;&#31181;&#33021;&#21147;&#34987;&#31216;&#20026;&#24515;&#28789;&#29702;&#35770;&#65288;Theory-of-Mind&#65292;ToM&#65289;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#34892;&#20107;&#12290;&#29616;&#26377;&#30340;&#38382;&#31572;&#22522;&#20934;&#65288;&#22914;ToMi&#65289;&#35201;&#27714;&#27169;&#22411;&#26681;&#25454;&#25925;&#20107;&#20013;&#35282;&#33394;&#30340;&#20449;&#24565;&#36827;&#34892;&#25512;&#26029;&#65292;&#20294;&#24182;&#19981;&#27979;&#35797;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21033;&#29992;&#36825;&#20123;&#25512;&#26029;&#26469;&#25351;&#23548;&#33258;&#24049;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#30340;&#35780;&#20272;&#33539;&#24335;&#65306;&#24605;&#32771;&#20026;&#20102;&#34892;&#21160;&#65288;T4D&#65289;&#65292;&#36825;&#35201;&#27714;&#27169;&#22411;&#23558;&#20851;&#20110;&#20182;&#20154;&#24515;&#29702;&#29366;&#24577;&#30340;&#25512;&#26029;&#19982;&#31038;&#20132;&#22330;&#26223;&#20013;&#30340;&#34892;&#21160;&#32852;&#31995;&#36215;&#26469;&#12290;T4D&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;GPT-4&#21644;PaLM 2&#31561;LLMs&#20284;&#20046;&#25797;&#38271;&#36319;&#36394;&#25925;&#20107;&#20013;&#35282;&#33394;&#30340;&#20449;&#24565;&#65292;&#20294;&#20182;&#20204;&#22312;&#23558;&#36825;&#31181;&#33021;&#21147;&#36716;&#21270;&#20026;&#25112;&#30053;&#34892;&#21160;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;LLMs&#38754;&#20020;&#30340;&#26680;&#24515;&#25361;&#25112;&#22312;&#20110;&#35782;&#21035;&#38544;&#21547;&#30340;&#20851;&#20110;&#24515;&#29702;&#29366;&#24577;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#26159;&#20687;ToMi&#20013;&#37027;&#26679;&#26126;&#30830;&#35810;&#38382;&#65292;&#36825;&#20250;&#23548;&#33268;&#27491;&#30830;&#30340;&#34892;&#21160;&#36873;&#25321;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
"Thinking is for Doing." Humans can infer other people's mental states from observations--an ability called Theory-of-Mind (ToM)--and subsequently act pragmatically on those inferences. Existing question answering benchmarks such as ToMi ask models questions to make inferences about beliefs of characters in a story, but do not test whether models can then use these inferences to guide their actions. We propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others' mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action. Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct acti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#24182;&#25506;&#32034;&#20102;&#24503;&#35821;&#21644;&#33521;&#35821;&#30340;ChatGPT&#22238;&#24212;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#31995;&#32479;&#22810;&#27425;&#25552;&#20379;&#30456;&#21516;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#24212;&#23384;&#22312;&#24046;&#24322;&#12290;&#20351;&#29992;ChatGPT&#26469;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#24037;&#20316;&#25991;&#26412;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#31995;&#32479;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.03031</link><description>&lt;p&gt;
ChatGPT&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#26377;&#22810;&#26222;&#36941;&#65311;&#8212;&#8212; &#25506;&#32034;&#24503;&#35821;&#21644;&#33521;&#35821;ChatGPT&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses. (arXiv:2310.03031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#24182;&#25506;&#32034;&#20102;&#24503;&#35821;&#21644;&#33521;&#35821;&#30340;ChatGPT&#22238;&#24212;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#31995;&#32479;&#22810;&#27425;&#25552;&#20379;&#30456;&#21516;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#24212;&#23384;&#22312;&#24046;&#24322;&#12290;&#20351;&#29992;ChatGPT&#26469;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#24037;&#20316;&#25991;&#26412;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#31995;&#32479;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#30340;&#25512;&#20986;&#65292;OpenAI&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20379;&#20855;&#26377;&#26377;&#38480;IT&#19987;&#19994;&#30693;&#35782;&#30340;&#29992;&#25143;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#32972;&#26223;&#30340;&#29992;&#25143;&#21487;&#33021;&#32570;&#20047;&#23545;LLM&#30340;&#36866;&#24403;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#65292;&#32570;&#20047;&#23545;&#20854;&#22266;&#26377;&#38480;&#21046;&#30340;&#24847;&#35782;&#65292;&#23558;&#25509;&#21463;&#31995;&#32479;&#36755;&#20986;&#30340;&#34920;&#38754;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#36755;&#20837;&#25552;&#31034;&#21644;&#29983;&#25104;&#30340;&#22238;&#24212;&#65292;&#20197;&#35782;&#21035;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#65292;&#29992;&#25143;&#22312;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#38656;&#35201;&#24847;&#35782;&#21040;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;ChatGPT&#22312;&#33521;&#35821;&#21644;&#24503;&#35821;&#20013;&#30340;&#21453;&#24212;&#65292;&#24182;&#25552;&#20379;&#20102;&#22899;&#24615;&#12289;&#30007;&#24615;&#25110;&#20013;&#31435;&#35282;&#24230;&#30340;&#25351;&#20196;&#26102;&#65292;&#22238;&#22797;&#30340;&#26159;&#21542;&#26377;&#24046;&#24322;&#12290;&#36890;&#36807;&#28145;&#20837;&#35843;&#26597;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#36873;&#25321;&#30340;&#25552;&#31034;&#65292;&#24182;&#20998;&#26512;&#20102;&#31995;&#32479;&#22312;&#30456;&#21516;&#26041;&#24335;&#19979;&#22810;&#27425;&#25552;&#20379;&#25351;&#20196;&#26102;&#22238;&#24212;&#30340;&#24046;&#24322;&#31243;&#24230;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#26085;&#24120;&#24037;&#20316;&#25991;&#26412;&#65292;ChatGPT&#30830;&#23454;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#28982;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#35201;&#24847;&#35782;&#21040;&#65292;&#24403;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#65292;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#21040;&#20854;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the introduction of ChatGPT, OpenAI made large language models (LLM) accessible to users with limited IT expertise. However, users with no background in natural language processing (NLP) might lack a proper understanding of LLMs. Thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. We explore how ChatGPT reacts in English and German if prompted to answer from a female, male, or neutral perspective. In an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. On this basis, we show that ChatGPT is indeed useful for helping non-IT users draft texts for their daily work. However, it is absolutely crucial to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DQ-LoRe&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02954</link><description>&lt;p&gt;
DQ-LoRe: &#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20302;&#31209;&#36817;&#20284;&#21452;&#37325;&#26597;&#35810;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning. (arXiv:2310.02954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DQ-LoRe&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#30340;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#24341;&#23548;LLMs&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#26159;&#21033;&#29992;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#33539;&#24335;&#20013;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#26368;&#20027;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#26377;&#25928;&#22320;&#36873;&#25321;&#31034;&#20363;&#26469;&#20419;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#65288;DQ-LoRe&#65289;&#26469;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#12290;&#21452;&#37325;&#26597;&#35810;&#39318;&#20808;&#26597;&#35810;LLM&#20197;&#33719;&#21462;LLM&#29983;&#25104;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;CoT&#65292;&#28982;&#21518;&#36890;&#36807;&#38382;&#39064;&#21644;&#30693;&#35782;&#26597;&#35810;&#26816;&#32034;&#22120;&#20197;&#33719;&#21462;&#26368;&#32456;&#30340;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#31532;&#20108;&#20010;&#26597;&#35810;&#65292;LoRe&#21033;&#29992;&#38477;&#32500;&#25216;&#26415;&#26469;&#25913;&#36827;&#31034;&#20363;&#36873;&#25321;&#65292;&#30830;&#20445;&#19982;&#36755;&#20837;&#38382;&#39064;&#30340;&#30693;&#35782;&#23494;&#20999;&#23545;&#40784;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;DQ-LoRe&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in natural language processing, primarily propelled by Large Language Models (LLMs), have showcased their remarkable capabilities grounded in in-context learning. A promising avenue for guiding LLMs in intricate reasoning tasks involves the utilization of intermediate reasoning steps within the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies in the effective selection of exemplars for facilitating in-context learning. In this study, we introduce a framework that leverages Dual Queries and Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars for in-context learning. Dual Queries first query LLM to obtain LLM-generated knowledge such as CoT, then query the retriever to obtain the final exemplars via both question and the knowledge. Moreover, for the second query, LoRe employs dimensionality reduction techniques to refine exemplar selection, ensuring close alignment with the input question's knowledge. Through extensive ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#35780;&#20272;&#25991;&#26412;&#29702;&#35299;&#22256;&#38590;&#24230;&#30340;&#31616;&#21333;&#26041;&#27861;LC-Score&#65292;&#21487;&#20197;&#29992;&#20110;&#27861;&#35821;&#25991;&#26412;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#22312;0&#21040;100&#33539;&#22260;&#20869;&#30340;&#26131;&#35835;&#31243;&#24230;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20316;&#23478;&#20204;&#21019;&#20316;&#26131;&#20110;&#29702;&#35299;&#30340;&#20869;&#23481;&#65292;&#24182;&#25552;&#20379;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#33258;&#21160;&#21270;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.02754</link><description>&lt;p&gt;
LC-Score&#65306;&#26080;&#21442;&#32771;&#35780;&#20272;&#25991;&#26412;&#29702;&#35299;&#22256;&#38590;&#24230;
&lt;/p&gt;
&lt;p&gt;
LC-Score: Reference-less estimation of Text Comprehension Difficulty. (arXiv:2310.02754v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#35780;&#20272;&#25991;&#26412;&#29702;&#35299;&#22256;&#38590;&#24230;&#30340;&#31616;&#21333;&#26041;&#27861;LC-Score&#65292;&#21487;&#20197;&#29992;&#20110;&#27861;&#35821;&#25991;&#26412;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#22312;0&#21040;100&#33539;&#22260;&#20869;&#30340;&#26131;&#35835;&#31243;&#24230;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20316;&#23478;&#20204;&#21019;&#20316;&#26131;&#20110;&#29702;&#35299;&#30340;&#20869;&#23481;&#65292;&#24182;&#25552;&#20379;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#33258;&#21160;&#21270;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#33021;&#22815;&#38405;&#35835;&#21644;&#29702;&#35299;&#20070;&#38754;&#25991;&#26412;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#37096;&#20998;&#20154;&#21475;&#37117;&#23384;&#22312;&#29702;&#35299;&#38556;&#30861;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#26080;&#38556;&#30861;&#20513;&#35758;&#26469;&#25552;&#39640;&#21463;&#20247;&#30340;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20316;&#23478;&#20204;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#25903;&#25345;&#25110;&#40723;&#21169;&#20197;&#21019;&#20316;&#26131;&#20110;&#29702;&#35299;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#33258;&#21160;&#25991;&#26412;&#31616;&#21270;&#65288;ATS&#65289;&#27169;&#22411;&#30340;&#24320;&#21457;&#32570;&#20047;&#20934;&#30830;&#20272;&#35745;&#29702;&#35299;&#38590;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;\textsc{LC-Score}&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#25991;&#26412;&#29702;&#35299;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#27861;&#35821;&#25991;&#26412;&#32780;&#26080;&#38656;&#21442;&#32771;&#65292;&#21363;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#22312;$[0, 100]$&#33539;&#22260;&#20869;&#30340;&#26131;&#35835;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23450;&#37327;&#25429;&#25417;&#25991;&#26412;&#31526;&#21512;\textit{Langage Clair}&#65288;LC&#65292;&#21363;\textit{Clear Language}&#65289;&#25351;&#21335;&#30340;&#31243;&#24230;&#65292;&#35813;&#25351;&#21335;&#19982;&#33521;&#35821;&#31616;&#26126;&#35821;&#35328;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#65288;i&#65289;&#20351;&#29992;&#35821;&#35328;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Being able to read and understand written text is critical in a digital era. However, studies shows that a large fraction of the population experiences comprehension issues. In this context, further initiatives in accessibility are required to improve the audience text comprehension. However, writers are hardly assisted nor encouraged to produce easy-to-understand content. Moreover, Automatic Text Simplification (ATS) model development suffers from the lack of metric to accurately estimate comprehension difficulty We present \textsc{LC-Score}, a simple approach for training text comprehension metric for any French text without reference \ie predicting how easy to understand a given text is on a $[0, 100]$ scale. Our objective with this scale is to quantitatively capture the extend to which a text suits to the \textit{Langage Clair} (LC, \textit{Clear Language}) guidelines, a French initiative closely related to English Plain Language. We explore two approaches: (i) using linguistically
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02357</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#27602;&#24615;&#23450;&#20041;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02357
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27602;&#24615;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26681;&#26412;&#38382;&#39064;&#22312;&#20110;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#19981;&#28165;&#12290;&#35895;&#27468;&#26071;&#19979;&#30340;&#22242;&#38431;Jigsaw&#26159;&#35813;&#39046;&#22495;&#30340;&#39046;&#23548;&#32773;&#20043;&#19968;&#65292;&#20182;&#20204;&#20351;&#29992;Dixon&#31561;&#20154;&#32473;&#20986;&#30340;&#27602;&#24615;&#23450;&#20041;&#65306;&#8220;&#31895;&#40065;&#12289;&#19981;&#23562;&#37325;&#25110;&#19981;&#21512;&#29702;&#30340;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#35753;&#26576;&#20154;&#31163;&#24320;&#35752;&#35770;&#8221;&#12290;&#20154;&#20204;&#21487;&#20197;&#31435;&#21363;&#30475;&#21040;&#36825;&#20010;&#23450;&#20041;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32473;&#20986;&#27602;&#24615;&#30340;&#23450;&#37327;&#24230;&#37327;&#65292;&#32780;&#19988;&#28041;&#21450;&#39640;&#24230;&#20027;&#35266;&#30340;&#25991;&#21270;&#26415;&#35821;&#12290;&#23613;&#31649;&#23384;&#22312;&#27169;&#31946;&#21644;&#32570;&#38519;&#65292;&#20294;&#36825;&#20010;&#23450;&#20041;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#32773;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in the field, uses a definition of toxicity given by Dixon et al. - 'rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion'. One can instantly see the issue with this definition, as it gives no quantitative measure of the toxicity and operates with highly subjective cultural terms. Despite all vagueness and flaws, this definition is de-facto widely used by many researchers. In this work we suggest quantative stress-based defenition for the toxicity that overcomes existing shortcomings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#22359;&#35745;&#31639;&#21644;&#36890;&#20449;&#37325;&#21472;&#30340;&#26041;&#24335;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#21333;&#20010;&#35774;&#22791;&#23545;&#20869;&#23384;&#30340;&#32422;&#26463;&#65292;&#20351;&#24471;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#24207;&#21015;&#38271;&#24230;&#33021;&#22815;&#26356;&#38271;&#12290;</title><link>http://arxiv.org/abs/2310.01889</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#22359;Transformer&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#35299;&#20915;&#36817;&#26080;&#38480;&#19978;&#19979;&#25991;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ring Attention with Blockwise Transformers for Near-Infinite Context. (arXiv:2310.01889v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#22359;&#35745;&#31639;&#21644;&#36890;&#20449;&#37325;&#21472;&#30340;&#26041;&#24335;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#21333;&#20010;&#35774;&#22791;&#23545;&#20869;&#23384;&#30340;&#32422;&#26463;&#65292;&#20351;&#24471;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#24207;&#21015;&#38271;&#24230;&#33021;&#22815;&#26356;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#39318;&#36873;&#26550;&#26500;&#65292;&#22312;&#24191;&#27867;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#23545;&#20869;&#23384;&#30340;&#38656;&#27714;&#38480;&#21046;&#20102;&#23427;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#23545;&#20110;&#28041;&#21450;&#25193;&#23637;&#24207;&#21015;&#25110;&#38271;&#26399;&#20381;&#36182;&#30340;&#20219;&#21153;&#32780;&#35328;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#21363;&#29615;&#24418;&#27880;&#24847;&#21147;(Ring Attention)&#65292;&#23427;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#22359;&#35745;&#31639;&#23558;&#38271;&#24207;&#21015;&#20998;&#24067;&#21040;&#22810;&#20010;&#35774;&#22791;&#19978;&#65292;&#21516;&#26102;&#23558;&#20851;&#38190;-&#20540;&#22359;&#30340;&#36890;&#20449;&#19982;&#20998;&#22359;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#37325;&#21472;&#12290;&#36890;&#36807;&#22788;&#29702;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#21516;&#26102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#65292;&#29615;&#24418;&#27880;&#24847;&#21147;&#20351;&#24471;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#24207;&#21015;&#27604;&#20043;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;Transformer&#33021;&#22815;&#22810;&#20986;&#35774;&#22791;&#25968;&#37327;&#20493;&#65292;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#21333;&#20010;&#35774;&#22791;&#23545;&#20869;&#23384;&#30340;&#32422;&#26463;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving extended sequences or long-term dependencies. We present a distinct approach, Ring Attention, which leverages blockwise computation of self-attention to distribute long sequences across multiple devices while concurrently overlapping the communication of key-value blocks with the computation of blockwise attention. By processing longer input sequences while maintaining memory efficiency, Ring Attention enables training and inference of sequences that are device count times longer than those of prior memory-efficient Transformers, effectively eliminating the memory constraints imposed by individual devices. Extensive experiments on language modeling tasks demonstrate the effecti
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20027;&#24352;&#36890;&#36807;&#25506;&#32034;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.01425</link><description>&lt;p&gt;
Borges&#19982;AI
&lt;/p&gt;
&lt;p&gt;
Borges and AI. (arXiv:2310.01425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01425
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20027;&#24352;&#36890;&#36807;&#25506;&#32034;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21551;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26102;&#20195;&#12290;&#19968;&#20123;&#20154;&#30475;&#21040;&#20102;&#26426;&#36935;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#30475;&#21040;&#20102;&#21361;&#38505;&#12290;&#28982;&#32780;&#65292;&#25903;&#25345;&#32773;&#21644;&#21453;&#23545;&#32773;&#37117;&#36890;&#36807;&#31185;&#24187;&#23567;&#35828;&#20013;&#27969;&#34892;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;AI&#12290;&#26426;&#22120;&#26159;&#21542;&#20250;&#21464;&#24471;&#26377;&#24863;&#30693;&#33021;&#21147;&#24182;&#21453;&#25239;&#20854;&#21019;&#36896;&#32773;&#65311;&#25105;&#20204;&#26159;&#21542;&#20250;&#32463;&#21382;&#32440;&#22841;&#22841;&#23376;&#21551;&#31034;&#65311;&#22312;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#20043;&#21069;&#65292;&#25105;&#20204;&#39318;&#20808;&#24212;&#35813;&#38382;&#19968;&#19979;&#65292;&#36825;&#31181;&#24515;&#29702;&#24847;&#35937;&#26159;&#21542;&#23545;&#25163;&#22836;&#30340;&#29616;&#35937;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#25551;&#36848;&#12290;&#20165;&#36890;&#36807;&#31070;&#28789;&#30340;&#24773;&#32490;&#26469;&#29702;&#35299;&#22825;&#27668;&#27169;&#24335;&#30340;&#26041;&#27861;&#26159;&#26377;&#38480;&#30340;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#20027;&#24352;&#36890;&#36807;&#20052;&#27835;&#183;&#36335;&#26131;&#26031;&#183;&#21338;&#23572;&#36203;&#26031;&#30340;&#24847;&#35937;&#26469;&#29702;&#35299;LLMs&#21450;&#20854;&#19982;AI&#30340;&#20851;&#31995;&#65292;&#21338;&#23572;&#36203;&#26031;&#26159;20&#19990;&#32426;&#25991;&#23398;&#22823;&#24072;&#65292;&#39764;&#24187;&#29616;&#23454;&#20027;&#20041;&#20808;&#39537;&#21644;&#21518;&#29616;&#20195;&#25991;&#23398;&#30340;&#21069;&#22863;&#12290;&#36825;&#31181;&#25506;&#32034;&#26041;&#24335;&#24102;&#26469;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#38416;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#22312;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01423</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of AI Generated Text Detection Tools. (arXiv:2310.01423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#22312;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;ChatGPT&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#27169;&#22411;&#20197;&#26469;&#65292;&#23427;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65288;&#21253;&#25324;&#36719;&#20214;&#24320;&#21457;&#21644;&#32500;&#25252;&#65289;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22238;&#24212;&#65292;&#21560;&#24341;&#20102;&#35768;&#22810;&#20154;&#30340;&#20852;&#36259;&#12290;ChatGPT&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#35823;&#29992;&#21487;&#33021;&#20250;&#24102;&#26469;&#20005;&#37325;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#25945;&#32946;&#21644;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#12290;&#30446;&#21069;&#24050;&#32463;&#26377;&#20960;&#31181;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#21487;&#20379;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#37117;&#26159;&#22312;&#30495;&#23454;&#25991;&#26412;&#19978;&#36827;&#34892;&#27979;&#35797;&#30340;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#23427;&#20204;&#23545;&#22810;&#39046;&#22495;ChatGPT&#26448;&#26009;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#29992;&#20110;&#26816;&#27979;&#22823;&#23398;&#21644;&#20854;&#20182;&#30740;&#31350;&#26426;&#26500;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;API&#21644;&#24037;&#20855;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#25991;&#31456;&#12289;&#25688;&#35201;&#12289;&#25925;&#20107;&#12289;&#26032;&#38395;&#21644;&#20135;&#21697;&#35780;&#35770;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#27493;&#26159;&#20351;&#29992;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#20845;&#31181;&#24037;&#20855;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since ChatGPT has emerged as a major AIGC model, providing high-quality responses across a wide range of applications (including software development and maintenance), it has attracted much interest from many individuals. ChatGPT has great promise, but there are serious problems that might arise from its misuse, especially in the realms of education and public safety. Several AIGC detectors are available, and they have all been tested on genuine text. However, more study is needed to see how effective they are for multi-domain ChatGPT material. This study aims to fill this need by creating a multi-domain dataset for testing the state-of-the-art APIs and tools for detecting artificially generated information used by universities and other research institutions. A large dataset consisting of articles, abstracts, stories, news, and product reviews was created for this study. The second step is to use the newly created dataset to put six tools through their paces. Six different artificial 
&lt;/p&gt;</description></item><item><title>TADIS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#24341;&#23548;LLMs&#28145;&#20837;&#24605;&#32771;&#31034;&#33539;&#20363;&#23376;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#33258;&#20449;&#30340;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#36755;&#20986;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.00901</link><description>&lt;p&gt;
TADIS: &#28145;&#20837;&#24605;&#32771;&#31034;&#33539;&#20363;&#23376;&#30340;&#27169;&#22411;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
TADIS: Steering Models for Deep-Thinking about Demonstration Examples. (arXiv:2310.00901v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00901
&lt;/p&gt;
&lt;p&gt;
TADIS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#24341;&#23548;LLMs&#28145;&#20837;&#24605;&#32771;&#31034;&#33539;&#20363;&#23376;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#33258;&#20449;&#30340;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#20219;&#21153;&#23450;&#20041;&#12289;&#31034;&#20363;&#65289;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25351;&#31034;&#35843;&#25972;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#36739;&#20197;&#21069;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25253;&#21578;&#31216;&#65292;&#34394;&#20551;&#30340;&#20219;&#21153;&#31034;&#20363;&#21487;&#20197;&#23454;&#29616;&#19982;&#27491;&#30830;&#30340;&#31034;&#20363;&#20960;&#20046;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#36755;&#20837;-&#26631;&#31614;&#23545;&#24212;&#20851;&#31995;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#37325;&#35201;&#24615;&#36739;&#20302;&#12290;&#21463;&#21040;&#36825;&#19968;&#36829;&#21453;&#30452;&#35273;&#30340;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24576;&#30097;&#27169;&#22411;&#21644;&#20154;&#31867;&#19968;&#26679;&#23384;&#22312;&#33258;&#20449;&#30340;&#24187;&#35273;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TADIS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#19981;&#20165;&#20165;&#26159;&#30475;&#21040;&#31034;&#33539;&#20363;&#23376;&#65292;&#32780;&#26159;&#24341;&#23548;LLM&#36827;&#34892;&#8220;&#28145;&#20837;&#24605;&#32771;&#8221;&#12290;&#20026;&#20102;&#20943;&#36731;&#27169;&#22411;&#33258;&#20449;&#30340;&#24187;&#35273;&#65292;&#25105;&#20204;&#39318;&#20808;&#35201;&#27714;&#27169;&#22411;&#39564;&#35777;&#31034;&#20363;&#30340;&#27491;&#30830;&#24615;&#65292;&#28982;&#21518;&#26681;&#25454;&#39564;&#35777;&#32467;&#26524;&#20316;&#20026;&#26465;&#20214;&#26469;&#24341;&#23548;&#27169;&#22411;&#20135;&#29983;&#26356;&#22909;&#30340;&#31572;&#26696;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#31181;&#24605;&#32771;&#36807;&#31243;&#65292;&#25105;&#20204;&#24076;&#26395;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has been demonstrated that could significantly improve the zero-shot generalization capability to unseen tasks by an apparent margin. By incorporating additional context (e.g., task definition, examples) during the fine-tuning process, Large Language Models (LLMs) achieved much higher performance than before. However, recent work reported that delusive task examples can achieve almost the same performance as correct task examples, indicating the input-label correspondence is less important than previously thought. Intrigued by this counter-intuitive observation, we suspect models have the same illusion of competence as humans. Therefore, we propose a novel method called TADIS that steers LLMs for "Deep-Thinking'' about demonstration examples instead of merely seeing. To alleviate the illusion of competence of models, we first ask the model to verify the correctness of shown examples. Then, using the verification results as conditions to elicit models for a better ans
&lt;/p&gt;</description></item><item><title>DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.17167</link><description>&lt;p&gt;
DyVal: &#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17167
&lt;/p&gt;
&lt;p&gt;
DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35780;&#20272;&#22522;&#20934;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#20986;&#20102;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20204;&#24222;&#22823;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#22522;&#20934;&#30340;&#38745;&#24577;&#24615;&#36136;&#21644;&#22266;&#23450;&#22797;&#26434;&#24615;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#34913;&#37327;LLM&#30340;&#36827;&#27493;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DyVal&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#28789;&#27963;&#30340;&#21160;&#24577;&#35780;&#20272;LLM&#30340;&#21327;&#35758;&#12290;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#21160;&#24577;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21033;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#32467;&#26500;&#20248;&#21183;&#26500;&#24314;&#20102;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;DyVal&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#12290;DyVal&#29983;&#25104;&#20102;&#21253;&#25324;&#25968;&#23398;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#31639;&#27861;&#38382;&#39064;&#22312;&#20869;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#35780;&#20272;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20174;Flan-T5-large&#21040;ChatGPT&#21644;GPT4&#30340;&#21508;&#31181;LLM&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#22312;DyVal&#29983;&#25104;&#30340;&#35780;&#20272;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#23450;&#24615;&#20998;&#26512;&#36827;&#34892;&#27880;&#37322;&#21487;&#33021;&#24341;&#20837;&#20559;&#35265;&#21644;&#27979;&#37327;&#35823;&#24046;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#21644;&#28789;&#27963;&#32534;&#30721;&#23545;&#31616;&#21333;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26356;&#22909;&#22320;&#20943;&#23569;&#20559;&#35265;&#21644;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.17147</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#21487;&#33021;&#24341;&#20837;&#20005;&#37325;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models for Qualitative Analysis can Introduce Serious Bias. (arXiv:2309.17147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17147
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#23450;&#24615;&#20998;&#26512;&#36827;&#34892;&#27880;&#37322;&#21487;&#33021;&#24341;&#20837;&#20559;&#35265;&#21644;&#27979;&#37327;&#35823;&#24046;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#21644;&#28789;&#27963;&#32534;&#30721;&#23545;&#31616;&#21333;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26356;&#22909;&#22320;&#20943;&#23569;&#20559;&#35265;&#21644;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#36805;&#36895;&#26222;&#21450;&#65292;&#20294;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#24456;&#22909;&#29702;&#35299;&#12290;&#26412;&#25991;&#25506;&#35752;LLMs&#26159;&#21542;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#20998;&#26512;&#22823;&#37327;&#24320;&#25918;&#24615;&#38754;&#35848;&#25968;&#25454;&#65292;&#20197;&#32599;&#20852;&#20122;&#38590;&#27665;&#22312;&#23391;&#21152;&#25289;&#22269;&#31185;&#20811;&#26031;&#24052;&#25166;&#30340;&#35775;&#35848;&#35760;&#24405;&#20026;&#24212;&#29992;&#26696;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;LLMs&#23545;&#25991;&#26412;&#36827;&#34892;&#27880;&#37322;&#26102;&#38656;&#35201;&#38750;&#24120;&#35880;&#24910;&#65292;&#22240;&#20026;&#23384;&#22312;&#24341;&#20837;&#20559;&#35265;&#30340;&#39118;&#38505;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#25512;&#26029;&#12290;&#25105;&#20204;&#36825;&#37324;&#25152;&#25351;&#30340;&#20559;&#35265;&#26159;&#25216;&#26415;&#24847;&#20041;&#19978;&#30340;&#65292;&#21363;LLMs&#22312;&#27880;&#37322;&#35775;&#35848;&#35760;&#24405;&#26102;&#30340;&#38169;&#35823;&#19981;&#26159;&#19982;&#35775;&#35848;&#23545;&#35937;&#30340;&#29305;&#24449;&#26080;&#20851;&#30340;&#38543;&#26426;&#35823;&#24046;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#21644;&#28789;&#27963;&#32534;&#30721;&#23545;&#31616;&#21333;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#20943;&#23569;&#27979;&#37327;&#35823;&#24046;&#21644;&#20559;&#35265;&#65292;&#20248;&#20110;LLMs&#30340;&#27880;&#37322;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#21040;&#24517;&#39035;&#26377;&#19968;&#20123;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#20197;&#35780;&#20272;LLM&#26159;&#21542;&#24341;&#20837;&#20559;&#35265;&#65292;&#25105;&#20204;&#35748;&#20026;&#26368;&#22909;&#30340;&#36873;&#25321;&#21487;&#33021;&#26159;&#20351;&#29992;&#36739;&#31616;&#21333;&#30340;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are quickly becoming ubiquitous, but the implications for social science research are not yet well understood. This paper asks whether LLMs can help us analyse large-N qualitative data from open-ended interviews, with an application to transcripts of interviews with Rohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of caution is needed in using LLMs to annotate text as there is a risk of introducing biases that can lead to misleading inferences. We here mean bias in the technical sense, that the errors that LLMs make in annotating interview transcripts are not random with respect to the characteristics of the interview subjects. Training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than LLM annotations. Therefore, given that some high quality annotations are necessary in order to asses whether an LLM introduces bias, we argue that it is probably preferable to
&lt;/p&gt;</description></item><item><title>NLPBench&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20316;&#32773;&#25910;&#38598;&#20102;&#26469;&#33258;&#32822;&#40065;&#22823;&#23398;&#26399;&#26411;&#32771;&#35797;&#30340;378&#20010;&#28085;&#30422;&#22810;&#20010;NLP&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#20351;&#29992;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#19981;&#31283;&#23450;&#65292;&#24182;&#21487;&#33021;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.15630</link><description>&lt;p&gt;
NLPBench&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
NLPBench: Evaluating Large Language Models on Solving NLP Problems. (arXiv:2309.15630v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15630
&lt;/p&gt;
&lt;p&gt;
NLPBench&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20316;&#32773;&#25910;&#38598;&#20102;&#26469;&#33258;&#32822;&#40065;&#22823;&#23398;&#26399;&#26411;&#32771;&#35797;&#30340;378&#20010;&#28085;&#30422;&#22810;&#20010;NLP&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#20351;&#29992;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#19981;&#31283;&#23450;&#65292;&#24182;&#21487;&#33021;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#26174;&#31034;&#20986;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#22312;LLMs&#30340;NLP&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#20173;&#28982;&#32570;&#20047;&#19987;&#38376;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;NLPBench&#65292;&#21253;&#25324;378&#20010;&#28085;&#30422;&#21508;&#31181;NLP&#20027;&#39064;&#30340;&#22823;&#23398;&#27700;&#24179;NLP&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#28304;&#33258;&#32822;&#40065;&#22823;&#23398;&#20197;&#21069;&#30340;&#26399;&#26411;&#32771;&#35797;&#12290;NLPBench&#21253;&#25324;&#20855;&#26377;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#23376;&#38382;&#39064;&#20998;&#20139;&#30456;&#21516;&#30340;&#20844;&#20849;&#20449;&#24687;&#65292;&#24182;&#19988;&#21253;&#25324;&#22810;&#36873;&#39064;&#12289;&#31616;&#31572;&#39064;&#21644;&#25968;&#23398;&#39064;&#31561;&#22810;&#31181;&#38382;&#39064;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20197;GPT-3.5/4&#12289;PaLM-2&#21644;LLAMA-2&#31561;LLMs&#20026;&#20013;&#24515;&#65292;&#37319;&#29992;&#20102;&#35832;&#22914;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#31561;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#21487;&#33021;&#19981;&#19968;&#33268;&#65292;&#26377;&#26102;&#20250;&#25439;&#23475;LLMs&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;LLA&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models (LLMs) have shown promise in enhancing the capabilities of natural language processing (NLP). Despite these successes, there remains a dearth of research dedicated to the NLP problem-solving abilities of LLMs. To fill the gap in this area, we present a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams. NLPBench includes questions with context, in which multiple sub-questions share the same public information, and diverse question types, including multiple choice, short answer, and math. Our evaluation, centered on LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#20511;&#37492;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.11981</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65306;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26410;&#26469;&#24230;&#37327;&#30340;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11981
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#20511;&#37492;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#65292;&#36825;&#20026;&#37325;&#26032;&#23457;&#35270;&#20256;&#32479;&#30340;&#26426;&#22120;&#26234;&#33021;&#24230;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20174;&#20256;&#32479;&#30340;&#22270;&#28789;&#27979;&#35797;&#36716;&#21521;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#24182;&#20511;&#37492;&#20102;&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#28145;&#21463;&#22810;&#20010;&#23398;&#31185;&#30340;&#21331;&#36234;&#24037;&#20316;&#30340;&#24433;&#21709;&#65292;&#25351;&#20986;&#20102;&#20445;&#25345;&#36328;&#23398;&#31185;&#26725;&#26753;&#24320;&#25918;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#21246;&#21202;&#20102;&#19968;&#20010;&#26356;&#21152;&#31283;&#20581;&#21644;&#21487;&#25345;&#32493;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the burgeoning field of artificial intelligence (AI), the unprecedented progress of large language models (LLMs) in natural language processing (NLP) offers an opportunity to revisit the entire approach of traditional metrics of machine intelligence, both in form and content. As the realm of machine cognitive evaluation has already reached Imitation, the next step is an efficient Language Acquisition and Understanding. Our paper proposes a paradigm shift from the established Turing Test towards an all-embracing framework that hinges on language acquisition, taking inspiration from the recent advancements in LLMs. The present contribution is deeply tributary of the excellent work from various disciplines, point out the need to keep interdisciplinary bridges open, and delineates a more robust and sustainable approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#21644;&#22522;&#20110;&#35780;&#20272;&#30340;&#24773;&#32490;&#20998;&#26512;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#22312;&#24773;&#32490;&#20998;&#31867;&#30340;&#22522;&#30784;&#19978;&#28155;&#21152;&#20102;&#23545;&#25552;&#21450;&#23454;&#20307;&#30340;&#35270;&#35282;&#65292;&#25552;&#21462;&#20102;&#23545;&#24212;&#24773;&#32490;&#21407;&#22240;&#30340;&#25991;&#26412;&#33539;&#22260;&#12290;&#24773;&#32490;&#21644;&#20107;&#20214;&#20855;&#26377;&#20004;&#31181;&#20851;&#31995;&#65306;&#24773;&#32490;&#26412;&#36523;&#23601;&#26159;&#19968;&#31181;&#20107;&#20214;&#65292;&#24182;&#19988;&#24773;&#32490;&#26159;&#30001;&#20107;&#20214;&#24341;&#36215;&#30340;&#12290;&#36825;&#19968;&#27010;&#24565;&#23545;&#20110;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.02092</link><description>&lt;p&gt;
&#26550;&#26725;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#21644;&#22522;&#20110;&#35780;&#20272;&#30340;&#24773;&#32490;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Bridging Emotion Role Labeling and Appraisal-based Emotion Analysis. (arXiv:2309.02092v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#21644;&#22522;&#20110;&#35780;&#20272;&#30340;&#24773;&#32490;&#20998;&#26512;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#22312;&#24773;&#32490;&#20998;&#31867;&#30340;&#22522;&#30784;&#19978;&#28155;&#21152;&#20102;&#23545;&#25552;&#21450;&#23454;&#20307;&#30340;&#35270;&#35282;&#65292;&#25552;&#21462;&#20102;&#23545;&#24212;&#24773;&#32490;&#21407;&#22240;&#30340;&#25991;&#26412;&#33539;&#22260;&#12290;&#24773;&#32490;&#21644;&#20107;&#20214;&#20855;&#26377;&#20004;&#31181;&#20851;&#31995;&#65306;&#24773;&#32490;&#26412;&#36523;&#23601;&#26159;&#19968;&#31181;&#20107;&#20214;&#65292;&#24182;&#19988;&#24773;&#32490;&#26159;&#30001;&#20107;&#20214;&#24341;&#36215;&#30340;&#12290;&#36825;&#19968;&#27010;&#24565;&#23545;&#20110;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#24773;&#32490;&#20998;&#26512;&#28085;&#30422;&#20102;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20854;&#20849;&#21516;&#30446;&#26631;&#26159;&#35753;&#35745;&#31639;&#26426;&#29702;&#35299;&#24773;&#32490;&#12290;&#20854;&#20013;&#26368;&#27969;&#34892;&#30340;&#26159;&#24773;&#32490;&#20998;&#31867;&#65292;&#20854;&#20013;&#23558;&#19968;&#20010;&#25110;&#22810;&#20010;&#24773;&#32490;&#20998;&#37197;&#32473;&#39044;&#23450;&#20041;&#30340;&#25991;&#26412;&#21333;&#20301;&#12290;&#32780;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#21017;&#28155;&#21152;&#20102;&#25552;&#21450;&#23454;&#20307;&#30340;&#35270;&#35282;&#65292;&#24182;&#25552;&#21462;&#19982;&#24773;&#32490;&#21407;&#22240;&#30456;&#23545;&#24212;&#30340;&#25991;&#26412;&#33539;&#22260;&#12290;&#30456;&#20851;&#30340;&#24773;&#32490;&#29702;&#35770;&#36798;&#25104;&#19968;&#20010;&#37325;&#35201;&#35266;&#28857;&#65306;&#24773;&#32490;&#26159;&#30001;&#26576;&#20123;&#20869;&#37096;&#25110;&#22806;&#37096;&#20107;&#20214;&#24341;&#36215;&#24182;&#21253;&#21547;&#20102;&#22810;&#20010;&#23376;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#20027;&#35266;&#24863;&#21463;&#21644;&#35748;&#30693;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#24773;&#32490;&#21644;&#20107;&#20214;&#26377;&#20004;&#31181;&#20851;&#31995;&#12290; &#65288;1&#65289;&#24773;&#32490;&#26412;&#36523;&#23601;&#26159;&#20107;&#20214;&#65307;&#36825;&#20010;&#35270;&#35282;&#26159;&#24773;&#32490;&#35282;&#33394;&#26631;&#27880;&#20013;&#30340;&#22522;&#30784;&#12290; &#65288;2&#65289;&#24773;&#32490;&#26159;&#30001;&#20107;&#20214;&#24341;&#36215;&#30340;&#65307;&#36825;&#20010;&#35270;&#35282;&#21017;&#38656;&#35201;&#30740;&#31350;&#22914;&#20309;&#23558;&#24515;&#29702;&#35780;&#20272;&#32435;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term emotion analysis in text subsumes various natural language processing tasks which have in common the goal to enable computers to understand emotions. Most popular is emotion classification in which one or multiple emotions are assigned to a predefined textual unit. While such setting is appropriate to identify the reader's or author's emotion, emotion role labeling adds the perspective of mentioned entities and extracts text spans that correspond to the emotion cause. The underlying emotion theories agree on one important point; that an emotion is caused by some internal or external event and comprises several subcomponents, including the subjective feeling and a cognitive evaluation. We therefore argue that emotions and events are related in two ways. (1) Emotions are events; and this perspective is the fundament in NLP for emotion role labeling. (2) Emotions are caused by events; a perspective that is made explicit with research how to incorporate psychological appraisal the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00436</link><description>&lt;p&gt;
SelfCheck: &#20351;&#29992;LLMs&#33258;&#26816;&#20854;&#36880;&#27493;&#25512;&#29702;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#30340;&#21457;&#26126;&#65292;&#20351;&#24471;&#35299;&#20915;&#25512;&#29702;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#24378;&#22823;&#30340;LLMs&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#38750;&#32447;&#24615;&#24605;&#32500;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#20855;&#26377;&#35782;&#21035;&#33258;&#24049;&#38169;&#35823;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#36880;&#27493;&#25512;&#29702;&#20013;&#30340;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#20197;&#35782;&#21035;&#27492;&#31867;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#39564;&#35777;&#26041;&#26696;&#26469;&#25913;&#36827;&#38382;&#31572;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#29983;&#25104;&#30340;&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#25237;&#31080;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;-GSM8K&#65292;MathQA&#21644;MATH&#19978;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#32780;&#25552;&#39640;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#20851;&#20110;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#30340;&#25552;&#26696;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10025</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#65306;&#29983;&#32946;&#25919;&#31574;&#25552;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Fertility Proposals Using Multi-Grined Topic Analysis Methods. (arXiv:2307.10025v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#20851;&#20110;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#30340;&#25552;&#26696;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#32946;&#38382;&#39064;&#19982;&#20154;&#21475;&#23433;&#20840;&#23494;&#20999;&#30456;&#20851;&#65292;&#20013;&#22269;60&#24180;&#26469;&#39318;&#27425;&#20986;&#29616;&#20154;&#21475;&#36127;&#22686;&#38271;&#36235;&#21183;&#65292;&#29983;&#32946;&#25919;&#31574;&#30340;&#21464;&#21270;&#24341;&#36215;&#20102;&#31038;&#20250;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#37319;&#29992;&#20849;&#29616;&#35821;&#20041;&#20998;&#26512;&#12289;&#20027;&#39064;&#20998;&#26512;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#22810;&#31890;&#24230;&#30340;&#35821;&#20041;&#20998;&#26512;&#12290;&#21457;&#29616;&#20851;&#20110;&#8220;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#8221;&#30340;&#25552;&#26696;&#35752;&#35770;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#24182;&#35814;&#32454;&#25506;&#35752;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fertility issues are closely related to population security, in 60 years China's population for the first time in a negative growth trend, the change of fertility policy is of great concern to the community. 2023 ``two sessions" proposal ``suggests that the country in the form of legislation, the birth of the registration of the cancellation of the marriage restriction" This topic was once a hot topic on the Internet, and ``unbundling" the relationship between birth registration and marriage has become the focus of social debate. In this paper, we adopt co-occurrence semantic analysis, topic analysis and sentiment analysis to conduct multi-granularity semantic analysis of microblog comments. It is found that the discussion on the proposal of ``removing marriage restrictions from birth registration" involves the individual, society and the state at three dimensions, and is detailed into social issues such as personal behaviour, social ethics and law, and national policy, with people's s
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;LLM&#30340;&#29305;&#28857;&#21644;&#21151;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21457;&#29616;&#21644;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.06435</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06435
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;LLM&#30340;&#29305;&#28857;&#21644;&#21151;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#21457;&#29616;&#21644;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;&#20102;&#20247;&#22810;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#21508;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#35843;&#25972;&#29616;&#26377;&#30340;&#26550;&#26500;&#65292;&#22686;&#21152;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#22686;&#21152;&#35757;&#32451;&#26102;&#38388;&#20197;&#36229;&#36234;&#22522;&#32447;&#12290;&#20998;&#26512;&#26032;&#30340;&#21457;&#23637;&#23545;&#20110;&#35782;&#21035;&#22686;&#24378;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#25913;&#36827;LLM&#27867;&#21270;&#33021;&#21147;&#30340;&#21464;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;LLM&#30340;&#26550;&#26500;&#21450;&#20854;&#20998;&#31867;&#12289;&#35757;&#32451;&#31574;&#30053;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;LLM&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#21644;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;LLM&#30340;&#23436;&#25972;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#37325;&#35201;&#29305;&#28857;&#21644;&#21151;&#33021;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;LLM&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#24182;&#25972;&#21512;&#20102;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown excellent generalization capabilities that have led to the development of numerous models. These models propose various new architectures, tweaking existing architectures with refined training strategies, increasing context length, using high-quality training data, and increasing training time to outperform baselines. Analyzing new developments is crucial for identifying changes that enhance training stability and improve generalization in LLMs. This survey paper comprehensively analyses the LLMs architectures and their categorization, training strategies, training datasets, and performance evaluations and discusses future research directions. Moreover, the paper also discusses the basic building blocks and concepts behind LLMs, followed by a complete overview of LLMs, including their important features and functions. Finally, the paper summarizes significant findings from LLM research and consolidates essential architectural and training strateg
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;REFLECT&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22833;&#36133;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.15724</link><description>&lt;p&gt;
REFLECT:&#23545;&#26426;&#22120;&#20154;&#32463;&#21382;&#36827;&#34892;&#24635;&#32467;&#65292;&#20197;&#29992;&#20110;&#22833;&#36133;&#35299;&#37322;&#21644;&#32416;&#27491;
&lt;/p&gt;
&lt;p&gt;
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;REFLECT&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22833;&#36133;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#22833;&#36133;&#25191;&#34892;&#26159;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#31283;&#20581;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#36755;&#20837;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#21033;&#29992;LLM&#30340;&#21147;&#37327;&#36827;&#34892;&#26426;&#22120;&#20154;&#22833;&#36133;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;REFLECT&#65292;&#23558;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#36807;&#21435;&#32463;&#39564;&#30340;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#36880;&#27493;&#22833;&#36133;&#35299;&#37322;&#31639;&#27861;&#26597;&#35810;LLM&#12290;&#22522;&#20110;&#35299;&#37322;&#65292;&#22833;&#36133;&#32416;&#27491;&#35268;&#21010;&#22120;&#29983;&#25104;&#19968;&#20010;&#21487;&#25191;&#34892;&#35745;&#21010;&#65292;&#20197;&#32416;&#27491;&#22833;&#36133;&#24182;&#23436;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;RoboFail&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#20174;&#32780;&#24110;&#21161;&#25104;&#21151;&#30340;&#32416;&#27491;&#35268;&#21010;&#12290;&#39033;&#30446;&#32593;&#31449;&#65306;https://roboreflect.github.io/
&lt;/p&gt;
&lt;p&gt;
The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong common sense reasoning skills on textual inputs. To leverage the power of LLM for robot failure explanation, we propose a framework REFLECT, which converts multi-sensory data into a hierarchical summary of robot past experiences and queries LLM with a progressive failure explanation algorithm. Conditioned on the explanation, a failure correction planner generates an executable plan for the robot to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset and show that our LLM-based framework is able to generate informative failure explanations that assist successful correction planning. Project website: https://roboreflect.github.io/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#8212;&#8212;SqueezeLLM&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07629</link><description>&lt;p&gt;
SqueezeLLM&#65306;&#23494;&#38598;&#31232;&#30095;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
SqueezeLLM: Dense-and-Sparse Quantization. (arXiv:2306.07629v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#8212;&#8212;SqueezeLLM&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#35777;&#26126;&#22312;&#24191;&#27867;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#38750;&#20961;&#30340;&#25104;&#26524;&#12290;&#20294;&#26159;&#30001;&#20110;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#36164;&#28304;&#38656;&#27714;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#25512;&#29702;&#19968;&#30452;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#29616;&#26377;&#30340;&#37096;&#32626;&#26694;&#26550;&#38656;&#35201;&#20351;&#29992;&#22810;GPU&#25512;&#29702;&#31649;&#36947;&#65292;&#36825;&#36890;&#24120;&#26159;&#22797;&#26434;&#21644;&#26114;&#36149;&#30340;&#65292;&#25110;&#32773;&#20351;&#29992;&#26356;&#23567;&#19988;&#24615;&#33021;&#26356;&#20302;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#20110;LLMs&#29983;&#25104;&#25512;&#26029;&#30340;&#20027;&#35201;&#29942;&#39048;&#26159;&#20869;&#23384;&#24102;&#23485;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#65292;&#23588;&#20854;&#26159;&#21333;&#20010;&#25209;&#27425;&#25512;&#29702;&#12290;&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#20943;&#23569;&#31934;&#24230;&#26469;&#34920;&#31034;&#27169;&#22411;&#26435;&#37325;&#65292;&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#20197;&#21069;&#30340;&#21162;&#21147;&#36890;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;SqueezeLLM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing model weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;VisualGPTScore&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01879</link><description>&lt;p&gt;
VisualGPTScore: &#22810;&#27169;&#24577;&#29983;&#25104;&#39044;&#35757;&#32451;&#20998;&#25968;&#30340;&#35270;&#35273;&#35821;&#20041;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores. (arXiv:2306.01879v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01879
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VisualGPTScore&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; VisualGPTScore &#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#26469;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#19978;&#36816;&#31639;&#12290;&#19982;&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#30340;VLM&#21482;&#26159;&#26080;&#24847;&#20041;&#30340;&#21333;&#35789;&#34955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340; VisualGPTScore &#22312; ARO &#21644; Crepe &#31561;&#26368;&#36817;&#25552;&#20986;&#30340;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#39030;&#23574;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) discriminatively pre-trained with contrastive image-text matching losses such as $P(\text{match}|\text{text}, \text{image})$ have been criticized for lacking compositional understanding. This means they might output similar scores even if the original caption is rearranged into a different semantic statement. To address this, we propose to use the ${\bf V}$isual ${\bf G}$enerative ${\bf P}$re-${\bf T}$raining Score (${\bf VisualGPTScore}$) of $P(\text{text}|\text{image})$, a $\textit{multimodal generative}$ score that captures the likelihood of a text caption conditioned on an image using an image-conditioned language model. Contrary to the belief that VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore demonstrates top-tier performance on recently proposed image-text retrieval benchmarks like ARO and Crepe that assess compositional reasoning. Furthermore, we factorize VisualGPTScore into a product of the $\textit{marginal}$ P(text) and the
&lt;/p&gt;</description></item><item><title>CrossGET&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#23454;&#26102;&#30340;&#36328;&#27169;&#24577;&#23548;&#24341;&#65292;&#33258;&#36866;&#24212;&#22320;&#32467;&#21512;&#20196;&#29260;&#65292;&#23454;&#29616;&#20102;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#30340;&#22823;&#24133;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2305.17455</link><description>&lt;p&gt;
CrossGET: &#36328;&#23548;&#24341;&#30340;&#20196;&#29260;&#38598;&#21512;&#29992;&#20110;&#21152;&#36895;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers. (arXiv:2305.17455v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17455
&lt;/p&gt;
&lt;p&gt;
CrossGET&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#23454;&#26102;&#30340;&#36328;&#27169;&#24577;&#23548;&#24341;&#65292;&#33258;&#36866;&#24212;&#22320;&#32467;&#21512;&#20196;&#29260;&#65292;&#23454;&#29616;&#20102;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#30340;&#22823;&#24133;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#36828;&#36828;&#36229;&#20986;&#20102;&#25105;&#20204;&#30340;&#39044;&#26399;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#38543;&#30528;&#24555;&#36895;&#21457;&#23637;&#20063;&#22312;&#22823;&#24133;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#32780;&#35328;&#12290;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#21152;&#36895;&#21464;&#24471;&#26497;&#20854;&#20851;&#38190;&#12290;&#23613;&#31649;&#23545;&#20110;&#21333;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#30340;&#21152;&#36895;&#20173;&#28982;&#30456;&#23545;&#19981;&#36275;&#12290;&#20026;&#20102;&#36861;&#27714;&#26356;&#39640;&#25928;&#21644;&#21487;&#35775;&#38382;&#30340;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;CrossGET&#30340;&#36328;&#23548;&#24341;&#20196;&#29260;&#38598;&#21512;&#30340;&#36890;&#29992;&#21152;&#36895;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23454;&#26102;&#30340;&#36328;&#27169;&#24577;&#23548;&#24341;&#65292;&#33258;&#36866;&#24212;&#22320;&#32467;&#21512;&#20196;&#29260;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#24133;&#21152;&#36895;&#32780;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;CrossGET&#30340;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#28857;&#26159;&#65306;1) &#36328;&#23548;&#24341;&#21305;&#37197;&#21644;&#38598;&#21512;&#12290;CrossGET&#23558;&#36328;&#23548;&#24341;&#30340;&#21305;&#37197;&#21644;&#38598;&#21512;&#24212;&#29992;&#21040;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored. To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22635;&#34917;&#26041;&#27861;&#20026;&#25152;&#26377;&#27880;&#37322;&#32773;&#29983;&#25104;&#25152;&#26377;&#31034;&#20363;&#30340;&#24847;&#35265;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#19981;&#25490;&#26021;&#20219;&#20309;&#27880;&#37322;&#32773;&#35266;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#21457;&#29616;&#22635;&#34917;&#26041;&#27861;&#30340;&#36873;&#25321;&#23545;&#36719;&#26631;&#31614;&#21464;&#21270;&#21644;&#20998;&#24067;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.15070</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#39044;&#27979;&#30340;&#27880;&#37322;&#22635;&#34917;&#65306;&#20851;&#20110;&#20998;&#24067;&#21160;&#24577;&#21644;&#27169;&#22411;&#39044;&#27979;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Annotation Imputation to Individualize Predictions: Initial Studies on Distribution Dynamics and Model Predictions. (arXiv:2305.15070v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22635;&#34917;&#26041;&#27861;&#20026;&#25152;&#26377;&#27880;&#37322;&#32773;&#29983;&#25104;&#25152;&#26377;&#31034;&#20363;&#30340;&#24847;&#35265;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#19981;&#25490;&#26021;&#20219;&#20309;&#27880;&#37322;&#32773;&#35266;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#21457;&#29616;&#22635;&#34917;&#26041;&#27861;&#30340;&#36873;&#25321;&#23545;&#36719;&#26631;&#31614;&#21464;&#21270;&#21644;&#20998;&#24067;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20247;&#21253;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#38750;&#24120;&#36153;&#26102;&#36153;&#38065;&#12290;&#30001;&#20110;&#36825;&#20123;&#25104;&#26412;&#65292;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#32773;&#36890;&#24120;&#35753;&#27599;&#20010;&#27880;&#37322;&#32773;&#21482;&#23545;&#19968;&#23567;&#37096;&#20998;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;&#36825;&#23548;&#33268;&#20102;&#31232;&#30095;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#30340;&#31034;&#20363;&#21482;&#34987;&#23569;&#25968;&#27880;&#37322;&#32773;&#26631;&#35760;&#12290;&#36825;&#20010;&#36807;&#31243;&#30340;&#32570;&#28857;&#22312;&#20110;&#65292;&#22914;&#26524;&#19968;&#20010;&#27880;&#37322;&#32773;&#27809;&#26377;&#26631;&#27880;&#19968;&#20010;&#29305;&#23450;&#30340;&#31034;&#20363;&#65292;&#20182;&#20204;&#23545;&#23427;&#30340;&#30475;&#27861;&#23601;&#20250;&#34987;&#24573;&#35270;&#12290;&#36825;&#22312;&#20027;&#35266;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#20013;&#23588;&#20026;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#27809;&#26377;&#19968;&#20010;&#27491;&#30830;&#30340;&#26631;&#31614;&#65306;&#20154;&#20204;&#21487;&#33021;&#20250;&#26377;&#19981;&#21516;&#30340;&#26377;&#25928;&#35266;&#28857;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22635;&#34917;&#26041;&#27861;&#20026;&#25152;&#26377;&#31034;&#20363;&#29983;&#25104;&#25152;&#26377;&#27880;&#37322;&#32773;&#30340;&#24847;&#35265;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#19981;&#25490;&#26021;&#20219;&#20309;&#27880;&#37322;&#32773;&#35266;&#28857;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22635;&#34917;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#35757;&#32451;&#21644;&#25552;&#31034;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#21709;&#24212;&#21644;&#20010;&#21035;&#27880;&#37322;&#30340;&#20998;&#24067;&#12290;&#22312;&#25105;&#20204;&#23545;&#32467;&#26524;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22635;&#34917;&#26041;&#27861;&#30340;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#36719;&#26631;&#31614;&#30340;&#21464;&#21270;&#21644;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotating data via crowdsourcing is time-consuming and expensive. Due to these costs, dataset creators often have each annotator label only a small subset of the data. This leads to sparse datasets with examples that are marked by few annotators. The downside of this process is that if an annotator doesn't get to label a particular example, their perspective on it is missed. This is especially concerning for subjective NLP datasets where there is no single correct label: people may have different valid opinions. Thus, we propose using imputation methods to generate the opinions of all annotators for all examples, creating a dataset that does not leave out any annotator's view. We then train and prompt models, using data from the imputed dataset, to make predictions about the distribution of responses and individual annotations.  In our analysis of the results, we found that the choice of imputation method significantly impacts soft label changes and distribution. While the imputation 
&lt;/p&gt;</description></item><item><title>BA-SOT&#26159;&#19968;&#31181;&#38754;&#21521;&#22810;&#35828;&#35805;&#20154;ASR&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36793;&#30028;&#24863;&#30693;&#21644;&#36830;&#25509;&#26102;&#38388;&#20998;&#31867;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13716</link><description>&lt;p&gt;
BA-SOT: &#38754;&#21521;&#22810;&#35828;&#35805;&#20154;ASR&#30340;&#36793;&#30028;&#24863;&#30693;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR. (arXiv:2305.13716v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13716
&lt;/p&gt;
&lt;p&gt;
BA-SOT&#26159;&#19968;&#31181;&#38754;&#21521;&#22810;&#35828;&#35805;&#20154;ASR&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36793;&#30028;&#24863;&#30693;&#21644;&#36830;&#25509;&#26102;&#38388;&#20998;&#31867;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#65288;SOT&#65289;&#36890;&#36807;&#29983;&#25104;&#30001;&#29305;&#27530;&#26631;&#35760;&#20998;&#38548;&#30340;&#35828;&#35805;&#32773;&#36716;&#24405;&#31616;&#21270;&#20102;&#22810;&#35828;&#35805;&#32773;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#20294;&#26159;&#65292;&#39057;&#32321;&#30340;&#35828;&#35805;&#32773;&#26356;&#25913;&#21487;&#33021;&#20250;&#20351;&#35828;&#35805;&#32773;&#26356;&#25913;&#39044;&#27979;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36793;&#30028;&#24863;&#30693;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#65288;BA-SOT&#65289;&#65292;&#23427;&#36890;&#36807;&#35828;&#35805;&#32773;&#26356;&#25913;&#26816;&#27979;&#20219;&#21153;&#21644;&#36793;&#30028;&#32422;&#26463;&#25439;&#22833;&#23558;&#36793;&#30028;&#30693;&#35782;&#26126;&#30830;&#22320;&#32435;&#20837;&#35299;&#30721;&#22120;&#20013;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#36830;&#25509;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#31574;&#30053;&#65292;&#23427;&#23558;&#22522;&#20110;&#26631;&#35760;&#30340;SOT CTC&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24674;&#22797;&#26102;&#38388;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#38500;&#20102;&#20856;&#22411;&#30340;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;CER&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35805;&#35821;&#30340;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;UD-CER&#65289;&#65292;&#20197;&#36827;&#19968;&#27493;&#34913;&#37327;&#35828;&#35805;&#32773;&#26356;&#25913;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#19982;&#21407;&#22987;&#30340;SOT&#30456;&#27604;&#65292;BA-SOT&#23558;CER / UD-CER&#38477;&#20302;&#20102;5.1&#65285;/ 14.0&#65285;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#36827;&#34892;BA-SOT&#27169;&#22411;&#21021;&#22987;&#21270;&#36827;&#19968;&#27493;&#23558;CER / UD-CER&#38477;&#20302;&#20102;8.4&#65285;/ 19.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently proposed serialized output training (SOT) simplifies multi-talker automatic speech recognition (ASR) by generating speaker transcriptions separated by a special token. However, frequent speaker changes can make speaker change prediction difficult. To address this, we propose boundary-aware serialized output training (BA-SOT), which explicitly incorporates boundary knowledge into the decoder via a speaker change detection task and boundary constraint loss. We also introduce a two-stage connectionist temporal classification (CTC) strategy that incorporates token-level SOT CTC to restore temporal context information. Besides typical character error rate (CER), we introduce utterance-dependent character error rate (UD-CER) to further measure the precision of speaker change prediction. Compared to original SOT, BA-SOT reduces CER/UD-CER by 5.1%/14.0%, and leveraging a pre-trained ASR model for BA-SOT model initialization further reduces CER/UD-CER by 8.4%/19.9%.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.13673</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;&#19968;&#37096;&#20998;&#65292;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;-&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#31995;&#32479;&#65292;&#21487;&#25429;&#25417;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#65292;&#31243;&#24207;&#21644;&#20154;&#31867;&#36923;&#36753;&#30340;&#26041;&#38754;&#12290;CFG&#19982;&#19979;&#25512;&#33258;&#21160;&#26426;&#19968;&#26679;&#22256;&#38590;&#65292;&#21487;&#33021;&#26159;&#27169;&#26865;&#20004;&#21487;&#30340;&#65292;&#22240;&#27492;&#39564;&#35777;&#23383;&#31526;&#20018;&#26159;&#21542;&#28385;&#36275;&#35268;&#21017;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#20154;&#36896;&#25968;&#25454;&#65292;&#24182;&#35777;&#26126;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CFG&#65292;&#39044;&#35757;&#32451;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;transformers&#23398;&#20064;CFG&#32972;&#21518;&#30340;&#29289;&#29702;&#21407;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65288;&#22914;&#22312;&#23376;&#26641;&#36793;&#30028;&#19978;&#31934;&#30830;&#23450;&#20301;&#26641;&#33410;&#28857;&#20449;&#24687;&#65289;&#65292;&#24182;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#19968;&#20123;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#23637;&#31034;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#35748;&#20026;LLMs&#22312;&#38754;&#23545;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#12290;</title><link>http://arxiv.org/abs/2305.12766</link><description>&lt;p&gt;
&#23558; Emergent In-Context Learning &#35299;&#37322;&#20026;&#26680;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Explaining Emergent In-Context Learning as Kernel Regression. (arXiv:2305.12766v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#35748;&#20026;LLMs&#22312;&#38754;&#23545;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#19968;&#22330;&#33539;&#24335;&#36716;&#21464;&#12290;&#19982;&#32463;&#20856;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#36807;&#31243;&#30456;&#27604;&#65292;&#20026;&#20102;&#23558;LLMs&#29992;&#20110;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#65292;&#21482;&#38656;&#35201;&#25552;&#20379;&#19968;&#20123;&#31034;&#20363;&#65292;&#21363;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#32780;&#26080;&#38656;&#28155;&#21152;&#25110;&#26356;&#26032;&#29616;&#26377;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;LLMs&#30340;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#38750;&#24120;&#26377;&#24847;&#24605;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#23436;&#20840;&#20102;&#35299;&#39044;&#35757;&#32451;LLMs&#22914;&#20309;&#33719;&#24471;&#36825;&#31181;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#24403;&#38754;&#20020;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#65292;LLMs&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#65292;&#26469;&#30740;&#31350;&#20026;&#20309;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#26009;&#24211;&#20043;&#21518;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#26680;&#22238;&#24402; $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have initiated a paradigm shift in transfer learning. In contrast to the classic pretraining-then-finetuning procedure, in order to use LLMs for downstream prediction tasks, one only needs to provide a few demonstrations, known as in-context examples, without adding more or updating existing model parameters. This in-context learning (ICL) capability of LLMs is intriguing, and it is not yet fully understood how pretrained LLMs acquire such capabilities. In this paper, we investigate the reason why a transformer-based language model can accomplish in-context learning after pre-training on a general language corpus by proposing one hypothesis that LLMs can simulate kernel regression with internal representations when faced with in-context examples. More concretely, we first prove that Bayesian inference on in-context prompts can be asymptotically understood as kernel regression $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$ as the number of in-context demon
&lt;/p&gt;</description></item><item><title>MedAlpaca&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#21307;&#30103;&#20250;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#21512;&#65292;&#26088;&#22312;&#36890;&#36807;&#32454;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#21892;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#21644;&#21307;&#29983;&#35748;&#35777;&#32771;&#35797;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.08247</link><description>&lt;p&gt;
MedAlpaca -- &#19968;&#20010;&#24320;&#28304;&#30340;&#21307;&#30103;&#20250;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data. (arXiv:2304.08247v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08247
&lt;/p&gt;
&lt;p&gt;
MedAlpaca&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#21307;&#30103;&#20250;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#21512;&#65292;&#26088;&#22312;&#36890;&#36807;&#32454;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#21892;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#21644;&#21307;&#29983;&#35748;&#35777;&#32771;&#35797;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;OpenAI&#30340;GPT&#31995;&#21015;&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#25105;&#20204;&#35265;&#35777;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#20986;&#29616;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#22312;&#25913;&#21892;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#12289;&#35786;&#26029;&#12289;&#24739;&#32773;&#25252;&#29702;&#21644;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#28304;&#27169;&#22411;&#65292;&#20197;&#22312;&#26412;&#22320;&#37096;&#32626;&#20197;&#20445;&#25252;&#24739;&#32773;&#38544;&#31169;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;16&#19975;&#26465;&#25968;&#25454;&#65292;&#19987;&#38376;&#20026;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#21270;&#35843;&#25972;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#21307;&#30103;&#24212;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19978;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#21270;&#35843;&#25972;&#30340;&#24433;&#21709;&#65292;&#24182;&#38543;&#21518;&#36890;&#36807;&#27604;&#36739;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#32454;&#21270;&#35843;&#25972;&#27169;&#22411;&#22312;&#26410;&#26469;&#21307;&#29983;&#24517;&#39035;&#36890;&#36807;&#30340;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#26469;&#23637;&#31034;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) like OpenAI's GPT series continue to make strides, we witness the emergence of artificial intelligence applications in an ever-expanding range of fields. In medicine, these LLMs hold considerable promise for improving medical workflows, diagnostics, patient care, and education. Yet, there is an urgent need for open-source models that can be deployed on-premises to safeguard patient privacy. In our work, we present an innovative dataset consisting of over 160,000 entries, specifically crafted to fine-tune LLMs for effective medical applications. We investigate the impact of fine-tuning these datasets on publicly accessible pre-trained LLMs, and subsequently, we juxtapose the performance of pre-trained-only models against the fine-tuned models concerning the examinations that future medical doctors must pass to achieve certification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35843;&#35797;&#20854;&#39044;&#27979;&#30340;&#31243;&#24207;&#65292;&#22312;&#22810;&#39033;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05128</link><description>&lt;p&gt;
&#33258;&#25105;&#35843;&#35797;&#65306;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35843;&#35797;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Teaching Large Language Models to Self-Debug. (arXiv:2304.05128v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35843;&#35797;&#20854;&#39044;&#27979;&#30340;&#31243;&#24207;&#65292;&#22312;&#22810;&#39033;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#32534;&#31243;&#20219;&#21153;&#65292;&#22312;&#19968;&#27425;&#24615;&#29983;&#25104;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#26469;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#35843;&#35797;(Self-Debugging)&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#35797;&#20854;&#39044;&#27979;&#30340;&#31243;&#24207;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#35843;&#35797;&#21487;&#20197;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27233;&#30382;&#40493;&#23376;&#35843;&#35797;(Rubber Duck Debugging)&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#20195;&#30721;&#27491;&#30830;&#24615;&#25110;&#38169;&#35823;&#20449;&#24687;&#30340;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#29983;&#25104;&#30340;&#20195;&#30721;&#26469;&#35782;&#21035;&#23427;&#30340;&#38169;&#35823;&#12290;&#33258;&#25105;&#35843;&#35797;&#22312;&#22810;&#39033;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#30340;Spider&#25968;&#25454;&#38598;&#65292;C++&#21040;Python&#32763;&#35793;&#30340;TransCoder&#21644;&#25991;&#26412;&#21040;Python&#29983;&#25104;&#30340;MBPP&#12290;&#22312;&#27809;&#26377;&#21333;&#20803;&#27979;&#35797;&#30340;Spider&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#33258;&#25105;&#35843;&#35797;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit te
&lt;/p&gt;</description></item><item><title>GOAL&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#22914;&#20309;&#22522;&#20110;&#32972;&#26223;&#30693;&#35782;&#29983;&#25104;&#31934;&#32454;&#30340;&#35270;&#39057;&#25551;&#36848;&#30340;&#38382;&#39064;&#65292;&#35813;&#22522;&#20934;&#21253;&#21547;&#36229;&#36807;8.9k&#20010;&#36275;&#29699;&#35270;&#39057;&#21644;&#30456;&#20851;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#30340;&#38590;&#24230;&#21644;&#28508;&#22312;&#26041;&#21521;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.14655</link><description>&lt;p&gt;
GOAL:&#20026;&#23454;&#26102;&#36275;&#29699;&#35299;&#35828;&#29983;&#25104;&#25552;&#20379;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#39057;&#23383;&#24149;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GOAL: A Challenging Knowledge-grounded Video Captioning Benchmark for Real-time Soccer Commentary Generation. (arXiv:2303.14655v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14655
&lt;/p&gt;
&lt;p&gt;
GOAL&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#22914;&#20309;&#22522;&#20110;&#32972;&#26223;&#30693;&#35782;&#29983;&#25104;&#31934;&#32454;&#30340;&#35270;&#39057;&#25551;&#36848;&#30340;&#38382;&#39064;&#65292;&#35813;&#22522;&#20934;&#21253;&#21547;&#36229;&#36807;8.9k&#20010;&#36275;&#29699;&#35270;&#39057;&#21644;&#30456;&#20851;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#30340;&#38590;&#24230;&#21644;&#28508;&#22312;&#26041;&#21521;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#20986;&#29616;&#20102;&#35768;&#22810;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#22914;&#20309;&#22522;&#20110;&#32972;&#26223;&#30693;&#35782;&#29983;&#25104;&#29983;&#21160;&#12289;&#31934;&#32454;&#30340;&#35270;&#39057;&#25551;&#36848;&#65288;&#21363;&#20851;&#20110;&#29305;&#23450;&#39046;&#22495;&#22330;&#26223;&#30340;&#35814;&#32454;&#35780;&#35770;&#65292;&#24182;&#20855;&#26377;&#36866;&#24403;&#30340;&#25512;&#29702;&#65289;&#65292;&#20173;&#28982;&#23384;&#22312;&#24456;&#22823;&#30340;&#38590;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GOAL&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;8.9k&#20010;&#36275;&#29699;&#35270;&#39057;&#29255;&#27573;&#12289;22k&#20010;&#21477;&#23376;&#21644;42k&#20010;&#30693;&#35782;&#19977;&#20803;&#32452;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#25552;&#20986;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#20316;&#20026;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#65288;KGVC&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#24615;&#30340;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#19968;&#23453;&#36149;&#32780;&#23454;&#29992;&#20219;&#21153;&#30340;&#38590;&#24230;&#21644;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#22312;https://github.com/THU-KEG/goal &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent emergence of video captioning models, how to generate vivid, fine-grained video descriptions based on the background knowledge (i.e., long and informative commentary about the domain-specific scenes with appropriate reasoning) is still far from being solved, which however has great applications such as automatic sports narrative. In this paper, we present GOAL, a benchmark of over 8.9k soccer video clips, 22k sentences, and 42k knowledge triples for proposing a challenging new task setting as Knowledge-grounded Video Captioning (KGVC). Moreover, we conduct experimental adaption of existing methods to show the difficulty and potential directions for solving this valuable and applicable task. Our data and code are available at https://github.com/THU-KEG/goal.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.04054</link><description>&lt;p&gt;
&#36861;&#27714;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#25512;&#29702;&#22797;&#29616;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#8212;&#8212;&#21363;&#22312;&#22797;&#21046;&#30340;&#27169;&#22411;&#35757;&#32451;&#36816;&#34892;&#20013;&#35266;&#23519;&#21040;&#30340;&#35780;&#20272;&#20998;&#25968;&#30340;&#19968;&#33268;&#24615;&#8212;&#8212;&#21463;&#21040;&#20960;&#31181;&#38750;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#27979;&#37327;&#22122;&#22768;&#12290;&#30446;&#21069;&#30340;&#36235;&#21183;&#26159;&#21435;&#38500;&#22122;&#22768;&#65292;&#20197;&#24378;&#21046;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#24573;&#30053;&#20102;&#23454;&#29616;&#23618;&#38754;&#22266;&#26377;&#30340;&#38750;&#30830;&#23450;&#24615;&#20197;&#21450;&#31639;&#27861;&#22122;&#22768;&#22240;&#32032;&#21644;&#25968;&#25454;&#29305;&#24615;&#20043;&#38388;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;&#36825;&#38480;&#21046;&#20102;&#20174;&#36825;&#20123;&#23454;&#39564;&#20013;&#21487;&#20197;&#24471;&#20986;&#30340;&#32467;&#35770;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23558;&#20960;&#20010;&#26041;&#24046;&#26469;&#28304;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982;&#25968;&#25454;&#29305;&#24615;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#26174;&#33879;&#24615;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#20013;&#65292;&#20197;&#26399;&#20174;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#23450;&#23454;&#20363;&#24471;&#20986;&#25512;&#29702;&#32467;&#35770;, &#32780;&#38750;&#21435;&#38500;&#22122;&#22768;&#12290;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#29992;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#24335;&#26469;&#32771;&#34385;&#31639;&#27861;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#28304;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#21508;&#20010;&#26041;&#24046;&#26469;&#28304;&#23545;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#21644;&#22522;&#20110;&#23383;&#20856;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#23545;MeSH&#27010;&#24565;&#36827;&#34892;&#33258;&#21160;&#32454;&#21270;&#30340;&#20027;&#39064;&#27880;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.09350</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35843;&#26597;&#24369;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#32034;&#24341;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large-scale investigation of weakly-supervised deep learning for the fine-grained semantic indexing of biomedical literature. (arXiv:2301.09350v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#21644;&#22522;&#20110;&#23383;&#20856;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#23545;MeSH&#27010;&#24565;&#36827;&#34892;&#33258;&#21160;&#32454;&#21270;&#30340;&#20027;&#39064;&#27880;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#35821;&#20041;&#32034;&#24341;&#36890;&#24120;&#22312;MeSH&#25551;&#36848;&#31526;&#30340;&#32423;&#21035;&#19978;&#36827;&#34892;&#65292;&#23558;&#20960;&#20010;&#30456;&#20851;&#20294;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#32452;&#21512;&#22312;&#19968;&#36215;&#24182;&#35270;&#20026;&#21333;&#20010;&#20027;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;MeSH&#27010;&#24565;&#30340;&#32423;&#21035;&#19978;&#33258;&#21160;&#32454;&#21270;&#20027;&#39064;&#27880;&#37322;&#12290;&#26041;&#27861;&#65306;&#30001;&#20110;&#32570;&#23569;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#22522;&#20110;&#27010;&#24565;&#20986;&#29616;&#22312;&#25991;&#31456;&#25688;&#35201;&#20013;&#30340;&#24369;&#30417;&#30563;&#65292;&#36825;&#20063;&#36890;&#36807;&#22522;&#20110;&#23383;&#20856;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36873;&#25321;&#35774;&#35745;&#31574;&#30053;&#20197;&#24212;&#23545;&#36825;&#19968;&#20219;&#21153;&#30340;&#29305;&#27530;&#25361;&#25112;&#12290;&#26032;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#30340;&#22238;&#39038;&#24615;&#22330;&#26223;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#22522;&#20110;&#24050;&#32463;&#25552;&#21319;&#20026;&#25551;&#36848;&#31526;&#30340;&#27010;&#24565;&#12290;&#32467;&#26524;&#65306;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#27010;&#24565;&#20986;&#29616;&#26159;&#26368;&#24378;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#23439;F1&#20998;&#25968;&#32422;&#20026;0.63&#65292;&#36328;&#22810;&#20010;&#26631;&#31614;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#36229;&#36807;4&#20010;&#30334;&#20998;&#28857;&#12290;&#32467;&#35770;&#65306;&#32467;&#26524;&#34920;&#26126;&#65292;&#27010;&#24565;&#20986;&#29616;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#32034;&#24341;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Semantic indexing of biomedical literature is usually done at the level of MeSH descriptors with several related but distinct biomedical concepts often grouped together and treated as a single topic. This study proposes a new method for the automated refinement of subject annotations at the level of MeSH concepts. Methods: Lacking labelled data, we rely on weak supervision based on concept occurrence in the abstract of an article, which is also enhanced by dictionary-based heuristics. In addition, we investigate deep learning approaches, making design choices to tackle the particular challenges of this task. The new method is evaluated on a large-scale retrospective scenario, based on concepts that have been promoted to descriptors. Results: In our experiments concept occurrence was the strongest heuristic achieving a macro-F1 score of about 0.63 across several labels. The proposed method improved it further by more than 4pp. Conclusion: The results suggest that concept occu
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31934;&#35843;&#21487;&#20197;&#25552;&#39640;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#31934;&#35843;&#36890;&#24120;&#20250;&#20351;&#27169;&#22411;&#36807;&#24230;&#19987;&#38376;&#21270;&#65292;&#38477;&#20302;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#31934;&#35843;&#26694;&#26550;ProMoT&#21487;&#20197;&#20943;&#23569;&#36825;&#31181;&#26684;&#24335;&#29305;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.00635</link><description>&lt;p&gt;
&#20004;&#38454;&#27573;LLM&#31934;&#35843;&#26041;&#27861;&#65306;&#26356;&#23569;&#29305;&#21270;&#12289;&#26356;&#22810;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Two-stage LLM Fine-tuning with Less Specialization and More Generalization. (arXiv:2211.00635v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00635
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31934;&#35843;&#21487;&#20197;&#25552;&#39640;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#31934;&#35843;&#36890;&#24120;&#20250;&#20351;&#27169;&#22411;&#36807;&#24230;&#19987;&#38376;&#21270;&#65292;&#38477;&#20302;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#31934;&#35843;&#26694;&#26550;ProMoT&#21487;&#20197;&#20943;&#23569;&#36825;&#31181;&#26684;&#24335;&#29305;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#21644;&#25552;&#31034;&#30340;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31934;&#35843;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#20854;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#31934;&#35843;&#36890;&#24120;&#20351;&#27169;&#22411;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36807;&#20110;&#19987;&#38376;&#21270;&#65292;&#24182;&#38477;&#20302;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#65292;&#36825;&#22312;&#38656;&#35201;&#22788;&#29702;&#27809;&#26377;&#31934;&#35843;&#25968;&#25454;&#30340;&#20854;&#20182;&#20219;&#21153;&#26102;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#21333;&#20219;&#21153;&#31934;&#35843;&#30830;&#23454;&#20250;&#38477;&#20302;LLM&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#36951;&#24536;&#30340;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;&#26684;&#24335;&#29305;&#21270;&#65292;&#21363;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#20110;&#31934;&#35843;&#20219;&#21153;&#30340;&#26684;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#26684;&#24335;&#29305;&#21270;&#21457;&#29983;&#22312;&#31934;&#35843;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt Tuning with MOdel Tuning (ProMoT)&#36825;&#19968;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#31934;&#35843;&#26694;&#26550;&#65292;&#21487;&#20197;&#20943;&#23569;&#26684;&#24335;&#29305;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. In this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task. We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format special
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#25193;&#25955;&#33021;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#21464;&#20998;&#23398;&#20064;&#26694;&#26550;&#20013;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#21644;&#28508;&#22312;&#31354;&#38388;EBMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#28508;&#22312;&#31354;&#38388;EBMs&#22312;&#37319;&#26679;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.05895</link><description>&lt;p&gt;
&#28145;&#24230;&#25193;&#25955;&#33021;&#37327;&#27169;&#22411;&#29992;&#20110;&#21487;&#35299;&#37322;&#25991;&#26412;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Energy-Based Model for Interpretable Text Modeling. (arXiv:2206.05895v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05895
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#25193;&#25955;&#33021;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#21464;&#20998;&#23398;&#20064;&#26694;&#26550;&#20013;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#21644;&#28508;&#22312;&#31354;&#38388;EBMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#28508;&#22312;&#31354;&#38388;EBMs&#22312;&#37319;&#26679;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#31354;&#38388;&#33021;&#37327;&#27169;&#22411;&#65288;EBMs&#65289;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#22312;&#20854;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#26377;&#36259;&#30340;&#23581;&#35797;&#65292;&#26088;&#22312;&#23454;&#29616;&#25991;&#26412;&#24314;&#27169;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#28508;&#22312;&#31354;&#38388;EBMs&#20063;&#32487;&#25215;&#20102;&#25968;&#25454;&#31354;&#38388;EBMs&#30340;&#19968;&#20123;&#32570;&#38519;&#65307;&#23454;&#36341;&#20013;&#36864;&#21270;&#30340;MCMC&#37319;&#26679;&#36136;&#37327;&#21487;&#33021;&#23548;&#33268;&#29983;&#25104;&#36136;&#37327;&#24046;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#22797;&#26434;&#28508;&#22312;&#32467;&#26500;&#30340;&#25968;&#25454;&#19978;&#12290;&#21463;&#21040;&#26368;&#36817;&#21033;&#29992;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#37319;&#26679;&#38382;&#39064;&#30340;&#21162;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#21464;&#20998;&#23398;&#20064;&#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#25193;&#25955;&#27169;&#22411;&#21644;&#28508;&#22312;&#31354;&#38388;EBMs&#20043;&#38388;&#30340;&#26032;&#22411;&#20849;&#29983;&#20851;&#31995;&#65292;&#31216;&#20026;&#28508;&#22312;&#25193;&#25955;&#33021;&#37327;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#20960;&#20309;&#32858;&#31867;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21450;&#20449;&#24687;&#29942;&#39048;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#23398;&#20064;&#28508;&#31354;&#38388;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in generative modeling. Fueled by its flexibility in the formulation and strong modeling power of the latent space, recent works built upon it have made interesting attempts aiming at the interpretability of text modeling. However, latent space EBMs also inherit some flaws from EBMs in data space; the degenerate MCMC sampling quality in practice can lead to poor generation quality and instability in training, especially on data with complex latent structures. Inspired by the recent efforts that leverage diffusion recovery likelihood learning as a cure for the sampling issue, we introduce a novel symbiosis between the diffusion models and latent space EBMs in a variational learning framework, coined as the latent diffusion energy-based model. We develop a geometric clustering-based regularization jointly with the information bottleneck to further improve the quality of the learned la
&lt;/p&gt;</description></item><item><title>Colossal-AI&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#24182;&#34892;&#35757;&#32451;&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#33021;&#22815;&#20197;&#39640;&#36798;2.76&#20493;&#30340;&#36895;&#24230;&#21152;&#24555;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25903;&#25345;&#22810;&#31181;&#24182;&#34892;&#35757;&#32451;&#26041;&#27861;&#21644;&#38646;&#20887;&#20313;&#20248;&#21270;&#22120;&#38598;&#25104;&#30340;&#24322;&#26500;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.14883</link><description>&lt;p&gt;
Colossal-AI: &#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#24182;&#34892;&#35757;&#32451;&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training. (arXiv:2110.14883v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.14883
&lt;/p&gt;
&lt;p&gt;
Colossal-AI&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#24182;&#34892;&#35757;&#32451;&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#33021;&#22815;&#20197;&#39640;&#36798;2.76&#20493;&#30340;&#36895;&#24230;&#21152;&#24555;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25903;&#25345;&#22810;&#31181;&#24182;&#34892;&#35757;&#32451;&#26041;&#27861;&#21644;&#38646;&#20887;&#20313;&#20248;&#21270;&#22120;&#38598;&#25104;&#30340;&#24322;&#26500;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#30340;&#25104;&#21151;&#25512;&#21160;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#36798;&#21040;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21333;&#20010;GPU&#30340;&#26377;&#38480;&#20869;&#23384;&#36164;&#28304;&#65292;&#36873;&#25321;&#26368;&#20339;&#24182;&#34892;&#31574;&#30053;&#30340;&#26368;&#20339;&#23454;&#36341;&#20173;&#28982;&#32570;&#20047;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#28145;&#24230;&#23398;&#20064;&#21644;&#24182;&#34892;&#35745;&#31639;&#26041;&#38754;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;Colossal-AI&#31995;&#32479;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32479;&#19968;&#30340;&#25509;&#21475;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#23558;&#27169;&#22411;&#35757;&#32451;&#30340;&#39034;&#24207;&#20195;&#30721;&#25193;&#23637;&#21040;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#12290;&#23427;&#25903;&#25345;&#25968;&#25454;&#24182;&#34892;&#12289;&#27969;&#27700;&#32447;&#24182;&#34892;&#12289;&#24352;&#37327;&#24182;&#34892;&#21644;&#24207;&#21015;&#24182;&#34892;&#31561;&#24182;&#34892;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#21450;&#19982;&#38646;&#20887;&#20313;&#20248;&#21270;&#22120;&#38598;&#25104;&#30340;&#24322;&#26500;&#35757;&#32451;&#26041;&#27861;&#12290;&#19982;&#22522;&#20934;&#31995;&#32479;&#30456;&#27604;&#65292;Colossal-AI&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#21487;&#20197;&#36798;&#21040;&#39640;&#36798;2.76&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of Transformer models has pushed the deep learning model scale to billions of parameters. Due to the limited memory resource of a single GPU, However, the best practice for choosing the optimal parallel strategy is still lacking, since it requires domain expertise in both deep learning and parallel computing.  The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism, as well as heterogeneous training methods integrated with zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#32467;&#26500;&#19982;&#24038;&#35282;&#26550;&#26500;&#26159;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#26356;&#20855;&#35748;&#30693;&#21487;&#20449;&#24230;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2109.04939</link><description>&lt;p&gt;
&#21033;&#29992;&#24038;&#35282;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#35821;&#27861;&#27169;&#25311;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars. (arXiv:2109.04939v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04939
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#32467;&#26500;&#19982;&#24038;&#35282;&#26550;&#26500;&#26159;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#26356;&#20855;&#35748;&#30693;&#21487;&#20449;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#65292;&#24050;&#32463;&#34920;&#26126;&#20998;&#32423;&#32467;&#26500;&#20351;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26356;&#20687;&#20154;&#31867;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#25991;&#29486;&#23545;&#20998;&#23618;&#27169;&#22411;&#30340;&#20998;&#26512;&#31574;&#30053;&#25345;&#26080;&#30693;&#24577;&#24230;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#23618;&#32467;&#26500;&#26159;&#21542;&#20351;LM&#26356;&#20687;&#20154;&#31867;&#65292;&#22914;&#26524;&#26159;&#65292;&#26368;&#20855;&#35748;&#30693;&#21487;&#20449;&#24230;&#30340;&#35299;&#26512;&#31574;&#30053;&#26159;&#20160;&#20040;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#22836;&#32456;&#24038;&#21521;&#32467;&#26500;&#30340;&#26085;&#35821;&#38405;&#35835;&#26102;&#38388;&#19982;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#20316;&#20026;&#24207;&#21015;&#27169;&#22411;&#21644;&#20174;&#19978;&#21040;&#19979;&#21644;&#20174;&#24038;&#35282;RNNG&#20316;&#20026;&#20998;&#23618;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#20102;&#19977;&#20010;LM&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#24314;&#27169;&#34920;&#26126;&#65292;&#24038;&#19978;&#26041;RNNG&#20248;&#20110;&#20174;&#19978;&#21040;&#19979;&#30340;RNNG&#21644;LSTM&#65292;&#36825;&#34920;&#26126;&#20998;&#23618;&#21644;&#24038;&#35282;&#26550;&#26500;&#27604;&#20174;&#19978;&#21040;&#19979;&#25110;&#24207;&#21015;&#26550;&#26500;&#26356;&#20855;&#35748;&#30693;&#21487;&#20449;&#24230;&#12290;&#27492;&#22806;&#65292;&#35748;&#30693;&#21487;&#20449;&#24230;&#19982;&#65288;i&#65289;&#21477;&#23376;&#38271;&#24230;&#65292;&#65288;ii&#65289;&#21477;&#23376;&#32467;&#26500;&#21644;&#65288;iii&#65289;&#35835;&#32773;&#21475;&#35821;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#20063;&#24471;&#21040;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computational linguistics, it has been shown that hierarchical structures make language models (LMs) more human-like. However, the previous literature has been agnostic about a parsing strategy of the hierarchical models. In this paper, we investigated whether hierarchical structures make LMs more human-like, and if so, which parsing strategy is most cognitively plausible. In order to address this question, we evaluated three LMs against human reading times in Japanese with head-final left-branching structures: Long Short-Term Memory (LSTM) as a sequential model and Recurrent Neural Network Grammars (RNNGs) with top-down and left-corner parsing strategies as hierarchical models. Our computational modeling demonstrated that left-corner RNNGs outperformed top-down RNNGs and LSTM, suggesting that hierarchical and left-corner architectures are more cognitively plausible than top-down or sequential architectures. In addition, the relationships between the cognitive plausibility and (i) p
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;&#20027;&#21160;&#25512;&#29702;&#25216;&#26415;&#21644;&#21322;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/1912.05957</link><description>&lt;p&gt;
&#25991;&#26412;&#20316;&#20026;&#29615;&#22659;:&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;&#35780;&#20272;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Text as Environment: A Deep Reinforcement Learning Text Readability Assessment Model. (arXiv:1912.05957v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.05957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;&#20027;&#21160;&#25512;&#29702;&#25216;&#26415;&#21644;&#21322;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#30340;&#21487;&#35835;&#24615;&#21487;&#20197;&#26174;&#33879;&#20419;&#36827;&#20449;&#24687;&#30340;&#20934;&#30830;&#34920;&#36798;&#12290;&#25991;&#26412;&#21487;&#35835;&#24615;&#35780;&#20272;&#30340;&#21046;&#23450;&#28041;&#21450;&#23545;&#25991;&#26412;&#30340;&#26377;&#24847;&#20041;&#30340;&#23646;&#24615;&#36827;&#34892;&#35782;&#21035;&#65292;&#32780;&#19981;&#35770;&#20854;&#38271;&#24230;&#12290;&#20026;&#20102;&#20934;&#30830;&#35780;&#20272;&#25991;&#26412;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#20351;&#29992;&#20102;&#22797;&#26434;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#39640;&#25928;&#35780;&#20272;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#38382;&#39064;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#12290;&#36890;&#36807;&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;&#20027;&#21160;&#25512;&#29702;&#25216;&#26415;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#36755;&#20837;&#25991;&#26412;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#36890;&#36807;&#20351;&#29992;&#21322;&#30417;&#30563;&#20449;&#21495;&#65292;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#26368;&#23569;&#30340;&#25991;&#26412;&#26469;&#30830;&#23450;&#25991;&#26412;&#30340;&#21487;&#35835;&#24615;&#12290;&#23558;&#35813;&#27169;&#22411;&#19982;Weebit&#21644;&#21073;&#26725;&#32771;&#35797;&#31561;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the readability of a text can significantly facilitate the precise expression of information in written form. The formulation of text readability assessment involves the identification of meaningful properties of the text regardless of its length. Sophisticated features and models are used to evaluate the comprehensibility of texts accurately. Despite this, the problem of assessing texts' readability efficiently remains relatively untouched. The efficiency of state-of-the-art text readability assessment models can be further improved using deep reinforcement learning models. Using a hard attention-based active inference technique, the proposed approach makes efficient use of input text and computational resources. Through the use of semi-supervised signals, the reinforcement learning model uses the minimum amount of text in order to determine text's readability. A comparison of the model on Weebit and Cambridge Exams with state-of-the-art models, such as the BERT text readab
&lt;/p&gt;</description></item></channel></rss>