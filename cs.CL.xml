<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>SSLCL&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#20559;&#21521;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#12290;&#23427;&#35299;&#20915;&#20102;&#22522;&#20110;SCL&#30340;&#26041;&#27861;&#20013;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#38480;&#21046;&#21644;&#19982;&#29616;&#26377;ERC&#27169;&#22411;&#19981;&#20860;&#23481;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25237;&#24433;&#31163;&#25955;&#26631;&#31614;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#26469;&#25913;&#21892;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16676</link><description>&lt;p&gt;
SSLCL: &#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#20559;&#21521;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
SSLCL: An Efficient Model-Agnostic Supervised Contrastive Learning Framework for Emotion Recognition in Conversations. (arXiv:2310.16676v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16676
&lt;/p&gt;
&lt;p&gt;
SSLCL&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#20559;&#21521;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#12290;&#23427;&#35299;&#20915;&#20102;&#22522;&#20110;SCL&#30340;&#26041;&#27861;&#20013;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#38480;&#21046;&#21644;&#19982;&#29616;&#26377;ERC&#27169;&#22411;&#19981;&#20860;&#23481;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25237;&#24433;&#31163;&#25955;&#26631;&#31614;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#26469;&#25913;&#21892;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035; (ERC) &#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#23545;&#35805;&#20013;&#21457;&#35328;&#32773;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;ERC&#26041;&#27861;&#19987;&#27880;&#20110;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064; (SCL) &#26469;&#22686;&#24378;&#23398;&#21040;&#30340;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;ERC&#20013;&#22522;&#20110;SCL&#30340;&#26041;&#27861;&#21463;&#21040;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;ERC&#27169;&#22411;&#19981;&#20860;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26080;&#27169;&#22411;&#20559;&#21521;&#30340;SCL&#26694;&#26550;&#65292;&#21517;&#20026;Supervised Sample-Label Contrastive Learning with Soft-HGR Maximal Correlation (SSLCL)&#65292;&#23427;&#28040;&#38500;&#20102;&#23545;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;ERC&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#32780;&#19981;&#24341;&#20837;&#20219;&#20309;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#20551;&#35774;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26631;&#31614;&#34920;&#31034;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#23558;&#31163;&#25955;&#26631;&#31614;&#25237;&#24433;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition in conversations (ERC) is a rapidly evolving task within the natural language processing community, which aims to detect the emotions expressed by speakers during a conversation. Recently, a growing number of ERC methods have focused on leveraging supervised contrastive learning (SCL) to enhance the robustness and generalizability of learned features. However, current SCL-based approaches in ERC are impeded by the constraint of large batch sizes and the lack of compatibility with most existing ERC models. To address these challenges, we propose an efficient and model-agnostic SCL framework named Supervised Sample-Label Contrastive Learning with Soft-HGR Maximal Correlation (SSLCL), which eliminates the need for a large batch size and can be seamlessly integrated with existing ERC models without introducing any model-specific assumptions. Specifically, we introduce a novel perspective on utilizing label representations by projecting discrete labels into dense embeddi
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#19968;&#20010;&#26377;&#28508;&#21147;&#30340;&#38646;-shot&#20381;&#23384;&#35299;&#26512;&#22120;&#65292;&#23454;&#39564;&#32467;&#26524;&#21644;&#35821;&#35328;&#20998;&#26512;&#37117;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.16654</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#38646;-shot&#20381;&#23384;&#35299;&#26512;&#22120;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a Potential Zero-Shot Dependency Parser. (arXiv:2310.16654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16654
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#26377;&#28508;&#21147;&#30340;&#38646;-shot&#20381;&#23384;&#35299;&#26512;&#22120;&#65292;&#23454;&#39564;&#32467;&#26524;&#21644;&#35821;&#35328;&#20998;&#26512;&#37117;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#20381;&#23384;&#35299;&#26512;&#20219;&#21153;&#65292;&#24182;&#22312;&#35299;&#26512;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#22312;&#38646;-shot&#24773;&#26223;&#20013;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#19981;&#24341;&#20837;&#39069;&#22806;&#35299;&#26512;&#22120;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#33258;&#21457;&#22320;&#23637;&#31034;&#20986;&#20381;&#23384;&#35299;&#26512;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#26469;&#25506;&#32034;&#20381;&#23384;&#35299;&#26512;&#33021;&#21147;&#24182;&#36827;&#34892;&#35821;&#35328;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#38646;-shot&#20381;&#23384;&#35299;&#26512;&#22120;&#65292;&#24182;&#19988;&#35821;&#35328;&#20998;&#26512;&#36824;&#23637;&#31034;&#20102;&#19968;&#20123;&#29420;&#29305;&#30340;&#35299;&#26512;&#36755;&#20986;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have been widely used in dependency parsing task and have achieved significant improvements in parser performance. However, it remains an understudied question whether pre-trained language models can spontaneously exhibit the ability of dependency parsing without introducing additional parser structure in the zero-shot scenario. In this paper, we propose to explore the dependency parsing ability of large language models such as ChatGPT and conduct linguistic analysis. The experimental results demonstrate that ChatGPT is a potential zero-shot dependency parser, and the linguistic analysis also shows some unique preferences in parsing outputs.
&lt;/p&gt;</description></item><item><title>ArTST&#26159;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;&#38463;&#25289;&#20271;&#35821;&#24320;&#28304;&#35821;&#38899;&#25216;&#26415;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23427;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21644;&#21475;&#35821;&#26041;&#35328;&#35782;&#21035;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#29978;&#33267;&#36229;&#36807;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16621</link><description>&lt;p&gt;
ArTST: &#38463;&#25289;&#20271;&#25991;&#26412;&#21644;&#35821;&#38899;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
ArTST: Arabic Text and Speech Transformer. (arXiv:2310.16621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16621
&lt;/p&gt;
&lt;p&gt;
ArTST&#26159;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;&#38463;&#25289;&#20271;&#35821;&#24320;&#28304;&#35821;&#38899;&#25216;&#26415;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23427;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21644;&#21475;&#35821;&#26041;&#35328;&#35782;&#21035;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#29978;&#33267;&#36229;&#36807;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;&#38463;&#25289;&#20271;&#35821;&#24320;&#28304;&#35821;&#38899;&#25216;&#26415;&#30340;&#39044;&#35757;&#32451;&#38463;&#25289;&#20271;&#25991;&#26412;&#21644;&#35821;&#38899;&#21464;&#25442;&#22120;ArTST&#12290;&#35813;&#27169;&#22411;&#26550;&#26500;&#36981;&#24490;&#26368;&#36817;&#21457;&#24067;&#30340;&#33521;&#25991;&#32479;&#19968;&#27169;&#24577;&#26694;&#26550;SpeechT5&#65292;&#24182;&#19988;&#19987;&#27880;&#20110;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#65288;MSA&#65289;&#65292;&#35745;&#21010;&#23558;&#27169;&#22411;&#25193;&#23637;&#21040;&#26410;&#26469;&#29256;&#26412;&#30340;&#26041;&#35328;&#21644;&#28151;&#21512;&#38463;&#25289;&#20271;&#35821;&#12290;&#25105;&#20204;&#20174;&#22836;&#24320;&#22987;&#22312;MSA&#35821;&#38899;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#23545;&#20197;&#19979;&#20219;&#21153;&#36827;&#34892;&#20102;&#24494;&#35843;&#65306;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65288;TTS&#65289;&#21644;&#21475;&#35821;&#26041;&#35328;&#35782;&#21035;&#12290;&#36890;&#36807;&#19982;SpeechT5&#20197;&#21450;&#20808;&#21069;&#25253;&#36947;&#30340;&#36825;&#20123;&#20219;&#21153;&#30340;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#65292;ArTST&#22312;&#25152;&#26377;&#19977;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#25345;&#24179;&#25110;&#36229;&#36807;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;&#27867;&#21270;&#65292;&#36825;&#22312;&#20302;&#36164;&#28304;TTS&#20219;&#21153;&#20013;&#29305;&#21035;&#26126;&#26174;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#21450;&#24494;&#35843;&#30340;ASR&#21644;TTS&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
We present ArTST, a pre-trained Arabic text and speech transformer for supporting open-source speech technologies for the Arabic language. The model architecture follows the unified-modal framework, SpeechT5, that was recently released for English, and is focused on Modern Standard Arabic (MSA), with plans to extend the model for dialectal and code-switched Arabic in future editions. We pre-trained the model from scratch on MSA speech and text data, and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR), Text-To-Speech synthesis (TTS), and spoken dialect identification. In our experiments comparing ArTST with SpeechT5, as well as with previously reported results in these tasks, ArTST performs on a par with or exceeding the current state-of-the-art in all three tasks. Moreover, we find that our pre-training is conducive for generalization, which is particularly evident in the low-resource TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRMN&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#20197;&#34701;&#20837;&#19981;&#21516;&#23610;&#24230;&#20687;&#32032;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#20840;&#26223;&#21465;&#20107;&#23545;&#40784;&#20013;&#30701;&#35821;&#19982;&#20687;&#32032;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16616</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#30830;&#23454;&#24456;&#37325;&#35201;&#65306;&#20855;&#26377;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#31934;&#32454;&#21305;&#37197;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#20840;&#26223;&#21465;&#20107;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Context Does Matter: End-to-end Panoptic Narrative Grounding with Deformable Attention Refined Matching Network. (arXiv:2310.16616v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRMN&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#20197;&#34701;&#20837;&#19981;&#21516;&#23610;&#24230;&#20687;&#32032;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#20840;&#26223;&#21465;&#20107;&#23545;&#40784;&#20013;&#30701;&#35821;&#19982;&#20687;&#32032;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;&#21465;&#20107;&#23545;&#40784;&#65288;PNG&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#35270;&#35273;&#23545;&#40784;&#20219;&#21153;&#65292;&#26088;&#22312;&#22522;&#20110;&#23494;&#38598;&#21465;&#20107;&#26631;&#39064;&#22312;&#22270;&#20687;&#20013;&#20998;&#21106;&#35270;&#35273;&#23545;&#35937;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#32858;&#21512;&#26368;&#30456;&#20284;&#30340;$k$&#20010;&#22270;&#20687;&#20687;&#32032;&#26469;&#25913;&#36827;&#30701;&#35821;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#32463;&#36807;&#25913;&#36827;&#30340;&#25991;&#26412;&#34920;&#31034;&#19982;&#22270;&#20687;&#29305;&#24449;&#22270;&#30340;&#20687;&#32032;&#36827;&#34892;&#21305;&#37197;&#65292;&#20197;&#29983;&#25104;&#20998;&#21106;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#32858;&#21512;&#37319;&#26679;&#30340;&#22270;&#20687;&#29305;&#24449;&#24573;&#30053;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#30701;&#35821;&#19982;&#20687;&#32032;&#30340;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#31934;&#32454;&#21305;&#37197;&#32593;&#32476;&#65288;DRMN&#65289;&#65292;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#22312;&#29305;&#24449;&#23398;&#20064;&#30340;&#36845;&#20195;&#36807;&#31243;&#20013;&#24341;&#20837;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#65292;&#20197;&#34701;&#20837;&#19981;&#21516;&#23610;&#24230;&#20687;&#32032;&#30340;&#37325;&#35201;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;DRMN&#22312;&#26356;&#26032;&#20102;&#26368;&#30456;&#20284;&#30340;$k$&#20010;&#20687;&#32032;&#30340;&#29305;&#24449;&#34920;&#31034;&#21518;&#65292;&#20351;&#29992;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#32593;&#32476;&#23545;&#20687;&#32032;&#36827;&#34892;&#36845;&#20195;&#37325;&#26032;&#32534;&#30721;&#12290;&#22240;&#27492;&#65292;DRMN&#21487;&#20197;&#23454;&#29616;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Panoramic Narrative Grounding (PNG) is an emerging visual grounding task that aims to segment visual objects in images based on dense narrative captions. The current state-of-the-art methods first refine the representation of phrase by aggregating the most similar $k$ image pixels, and then match the refined text representations with the pixels of the image feature map to generate segmentation results. However, simply aggregating sampled image features ignores the contextual information, which can lead to phrase-to-pixel mis-match. In this paper, we propose a novel learning framework called Deformable Attention Refined Matching Network (DRMN), whose main idea is to bring deformable attention in the iterative process of feature learning to incorporate essential context information of different scales of pixels. DRMN iteratively re-encodes pixels with the deformable attention network after updating the feature representation of the top-$k$ most similar pixels. As such, DRMN can lead to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#36716;&#24405;&#27861;&#21644;&#38169;&#35823;&#20998;&#31867;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#23545;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#21512;&#25104;&#35821;&#38899;&#19982;&#38899;&#39057;&#24405;&#21046;&#23545;&#20110;&#35813;&#26041;&#27861;&#30340;&#32467;&#26524;&#27809;&#26377;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.16609</link><description>&lt;p&gt;
&#20316;&#20026;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#23545;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#30340;&#21453;&#36716;&#24405;&#27861;
&lt;/p&gt;
&lt;p&gt;
Back Transcription as a Method for Evaluating Robustness of Natural Language Understanding Models to Speech Recognition Errors. (arXiv:2310.16609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#36716;&#24405;&#27861;&#21644;&#38169;&#35823;&#20998;&#31867;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#23545;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#21512;&#25104;&#35821;&#38899;&#19982;&#38899;&#39057;&#24405;&#21046;&#23545;&#20110;&#35813;&#26041;&#27861;&#30340;&#32467;&#26524;&#27809;&#26377;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#20043;&#21069;&#26159;&#19968;&#20010;&#21487;&#33021;&#24433;&#21709;&#20854;&#24615;&#33021;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#23545;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#24615;&#33021;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#21453;&#36716;&#24405;&#31243;&#24207;&#19982;&#32454;&#31890;&#24230;&#30340;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#23545;&#24433;&#21709;NLU&#27169;&#22411;&#24615;&#33021;&#30340;&#38169;&#35823;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#21512;&#25104;&#35821;&#38899;&#36827;&#34892;NLU&#35780;&#20272;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#37325;&#35201;&#31243;&#24230;&#19978;&#65292;&#23558;&#21512;&#25104;&#35821;&#38899;&#29992;&#20110;&#38899;&#39057;&#24405;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#19981;&#20250;&#26174;&#33879;&#25913;&#21464;&#25152;&#25552;&#20986;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a spoken dialogue system, an NLU model is preceded by a speech recognition system that can deteriorate the performance of natural language understanding. This paper proposes a method for investigating the impact of speech recognition errors on the performance of natural language understanding models. The proposed method combines the back transcription procedure with a fine-grained technique for categorizing the errors that affect the performance of NLU models. The method relies on the usage of synthesized speech for NLU evaluation. We show that the use of synthesized speech in place of audio recording does not change the outcomes of the presented technique in a significant way.
&lt;/p&gt;</description></item><item><title>&#20844;&#24179;&#30340;NLP&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#20381;&#36182;&#20110;&#26356;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20559;&#35265;&#32531;&#35299;&#31639;&#27861;&#24182;&#19981;&#24635;&#26159;&#23548;&#33268;&#26356;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16607</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#19982;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Interplay between Fairness and Explainability. (arXiv:2310.16607v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16607
&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#30340;NLP&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#20381;&#36182;&#20110;&#26356;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20559;&#35265;&#32531;&#35299;&#31639;&#27861;&#24182;&#19981;&#24635;&#26159;&#23548;&#33268;&#26356;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#21487;&#38752;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#65292;&#27169;&#22411;&#38656;&#35201;&#22312;&#19981;&#21516;&#30340;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20013;&#26082;&#20855;&#26377;&#20844;&#24179;&#24615;&#21448;&#21487;&#35299;&#37322;&#12290;&#36890;&#24120;&#65292;&#36825;&#20004;&#20010;&#30446;&#26631;&#65292;&#21363;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20250;&#34987;&#29420;&#31435;&#22320;&#36827;&#34892;&#20248;&#21270;&#21644;/&#25110;&#30740;&#31350;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35748;&#20026;&#26410;&#26469;&#21487;&#20449;&#30340;NLP&#31995;&#32479;&#24212;&#35813;&#21516;&#26102;&#32771;&#34385;&#20004;&#32773;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#65306;&#26356;&#20844;&#24179;&#30340;&#27169;&#22411;&#26159;&#21542;&#20381;&#36182;&#20110;&#26356;&#21512;&#29702;&#30340;&#35299;&#37322;&#65311;&#21453;&#20043;&#20134;&#28982;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#33521;&#35821;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;BIOS&#21644;ECtHR&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20998;&#21035;&#25552;&#20379;&#20102;&#26377;&#20851;&#24615;&#21035;&#21644;&#22269;&#31821;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#20154;&#24037;&#26631;&#27880;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21253;&#25324;(i)&#20559;&#35265;&#32531;&#35299;&#65292;&#26088;&#22312;&#25552;&#39640;&#20844;&#24179;&#24615;&#65307;(ii)&#35299;&#37322;&#25552;&#21462;&#65292;&#26088;&#22312;&#20135;&#29983;&#21512;&#29702;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20559;&#35265;&#32531;&#35299;&#31639;&#27861;&#24182;&#19981;&#24635;&#26159;&#23548;&#33268;&#26356;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
In order to build reliable and trustworthy NLP applications, models need to be both fair across different demographics and explainable. Usually these two objectives, fairness and explainability, are optimized and/or examined independently of each other. Instead, we argue that forthcoming, trustworthy NLP systems should consider both. In this work, we perform a first study to understand how they influence each other: do fair(er) models rely on more plausible rationales? and vice versa. To this end, we conduct experiments on two English multi-class text classification datasets, BIOS and ECtHR, that provide information on gender and nationality, respectively, as well as human-annotated rationales. We fine-tune pre-trained language models with several methods for (i) bias mitigation, which aims to improve fairness; (ii) rationale extraction, which aims to produce plausible explanations. We find that bias mitigation algorithms do not always lead to fairer models. Moreover, we discover that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#24314;&#31435;&#20010;&#24615;&#21270;&#35789;&#20856;&#65292;&#26469;&#23450;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20197;&#21487;&#25554;&#25300;&#30340;&#26041;&#24335;&#32467;&#21512;&#20116;&#20010;&#22823;&#31867;&#22240;&#32032;&#65292;&#23454;&#29616;&#23545;&#20010;&#24615;&#29305;&#24449;&#30340;&#31934;&#30830;&#25805;&#32437;&#12290;</title><link>http://arxiv.org/abs/2310.16582</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#24314;&#31435;&#20010;&#24615;&#21270;&#35789;&#20856;&#26469;&#23450;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons. (arXiv:2310.16582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#24314;&#31435;&#20010;&#24615;&#21270;&#35789;&#20856;&#65292;&#26469;&#23450;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20197;&#21487;&#25554;&#25300;&#30340;&#26041;&#24335;&#32467;&#21512;&#20116;&#20010;&#22823;&#31867;&#22240;&#32032;&#65292;&#23454;&#29616;&#23545;&#20010;&#24615;&#29305;&#24449;&#30340;&#31934;&#30830;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#22312;&#22609;&#36896;&#20154;&#31867;&#34920;&#36798;&#27169;&#24335;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36171;&#20104;&#21644;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20010;&#24615;&#29305;&#24449;&#22312;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#22312;&#23500;&#21547;&#20010;&#24615;&#34920;&#36798;&#30340;&#35821;&#26009;&#24211;&#19978;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#35201;&#20040;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#25552;&#31034;&#26469;&#35825;&#23548;LLM&#20135;&#29983;&#20010;&#24615;&#21270;&#22238;&#24212;&#12290;&#21069;&#32773;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#36164;&#28304;&#26469;&#25910;&#38598;&#36275;&#22815;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#21518;&#32773;&#21487;&#33021;&#26080;&#27861;&#31934;&#30830;&#25805;&#32437;&#20010;&#24615;&#29305;&#24449;&#20197;&#36798;&#21040;&#32454;&#31890;&#24230;&#30340;&#27700;&#24179;&#65288;&#20363;&#22914;&#65292;&#22312;&#20943;&#23569;&#24320;&#25918;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#23452;&#20154;&#24615;&#65289;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23450;&#21046;LLM&#20013;&#30340;&#20010;&#24615;&#29305;&#24449;&#65292;&#20801;&#35768;&#20197;&#21487;&#25554;&#25300;&#30340;&#26041;&#24335;&#32467;&#21512;&#20116;&#20010;&#22823;&#31867;&#22240;&#32032;&#65288;&#21363;&#24320;&#25918;&#24615;&#12289;&#36131;&#20219;&#24515;&#12289;&#22806;&#21521;&#24615;&#12289;&#23452;&#20154;&#24615;&#21644;&#31070;&#32463;&#36136;&#65289;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personality plays a pivotal role in shaping human expression patterns, and empowering and manipulating large language models (LLMs) with personality traits holds significant promise in enhancing the user experience of LLMs. However, prior approaches either rely on fine-tuning LLMs on a corpus enriched with personalized expressions or necessitate the manual crafting of prompts to induce LLMs to produce personalized responses. The former approaches demand substantial time and resources for collecting sufficient training examples while the latter might fail in enabling the precise manipulation of the personality traits at a fine-grained level (e.g., achieving high agreeableness while reducing openness). In this study, we introduce a novel approach for tailoring personality traits within LLMs, allowing for the incorporation of any combination of the Big Five factors (i.e., openness, conscientiousness, extraversion, agreeableness, and neuroticism) in a pluggable manner. This is achieved by 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20551;&#26032;&#38395;&#25581;&#31359;&#39046;&#22495;&#20013;&#19968;&#39033;&#26032;&#20219;&#21153;&#8212;&#8212;&#26816;&#27979;&#21477;&#23376;&#32423;&#21035;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#20855;&#26377;&#21477;&#23376;&#32423;&#21035;&#30495;&#23454;&#24615;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Weakly Supervised Detection of Misinforming Se&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16579</link><description>&lt;p&gt;
WSDMS: &#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#31038;&#20132;&#26234;&#24935;&#23454;&#29616;&#24369;&#30417;&#30563;&#26816;&#27979;&#35823;&#23548;&#24615;&#21477;&#23376;&#26469;&#25581;&#31359;&#20551;&#26032;&#38395;
&lt;/p&gt;
&lt;p&gt;
WSDMS: Debunk Fake News via Weakly Supervised Detection of Misinforming Sentences with Contextualized Social Wisdom. (arXiv:2310.16579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20551;&#26032;&#38395;&#25581;&#31359;&#39046;&#22495;&#20013;&#19968;&#39033;&#26032;&#20219;&#21153;&#8212;&#8212;&#26816;&#27979;&#21477;&#23376;&#32423;&#21035;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#20855;&#26377;&#21477;&#23376;&#32423;&#21035;&#30495;&#23454;&#24615;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Weakly Supervised Detection of Misinforming Se&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25105;&#20204;&#30446;&#30585;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36805;&#36895;&#20256;&#25773;&#24182;&#20196;&#20844;&#20247;&#38663;&#24778;&#30340;&#34394;&#20551;&#21644;&#26410;&#32463;&#35777;&#23454;&#30340;&#20449;&#24687;&#65288;&#21363;&#35875;&#35328;&#65289;&#30340;&#29190;&#21457;&#12290;&#35875;&#35328;&#21487;&#20197;&#24341;&#21457;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#20043;&#38388;&#22810;&#26679;&#21270;&#20294;&#22823;&#22810;&#20855;&#26377;&#20105;&#35758;&#24615;&#30340;&#31435;&#22330;&#34920;&#36798;&#12290;&#35875;&#35328;&#39564;&#35777;&#21644;&#31435;&#22330;&#26816;&#27979;&#26159;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#20551;&#26032;&#38395;&#25581;&#31359;&#20027;&#35201;&#20851;&#27880;&#30830;&#23450;&#26032;&#38395;&#25991;&#31456;&#30340;&#30495;&#23454;&#24615;&#65292;&#36825;&#36807;&#20110;&#31616;&#21270;&#20102;&#38382;&#39064;&#65292;&#22240;&#20026;&#20551;&#26032;&#38395;&#32463;&#24120;&#32467;&#21512;&#20102;&#30495;&#23454;&#24615;&#21644;&#34394;&#20551;&#24615;&#30340;&#20803;&#32032;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#25991;&#31456;&#20013;&#29305;&#23450;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#23454;&#20363;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20551;&#26032;&#38395;&#25581;&#31359;&#39046;&#22495;&#20013;&#30340;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#26816;&#27979;&#21477;&#23376;&#32423;&#21035;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#12290;&#36825;&#39033;&#20219;&#21153;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#20855;&#26377;&#21477;&#23376;&#32423;&#21035;&#30495;&#23454;&#24615;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#21463;&#21040;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Weakly Supervised Detection of Misinforming Se&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, we witness the explosion of false and unconfirmed information (i.e., rumors) that went viral on social media and shocked the public. Rumors can trigger versatile, mostly controversial stance expressions among social media users. Rumor verification and stance detection are different yet relevant tasks. Fake news debunking primarily focuses on determining the truthfulness of news articles, which oversimplifies the issue as fake news often combines elements of both truth and falsehood. Thus, it becomes crucial to identify specific instances of misinformation within the articles. In this research, we investigate a novel task in the field of fake news debunking, which involves detecting sentence-level misinformation. One of the major challenges in this task is the absence of a training dataset with sentence-level annotations regarding veracity. Inspired by the Multiple Instance Learning (MIL) approach, we propose a model called Weakly Supervised Detection of Misinforming Se
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#25506;&#27979;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#31867;&#26041;&#26696;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.16570</link><description>&lt;p&gt;
&#32473;&#25105;&#20107;&#23454;&#65281;&#20851;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#25506;&#27979;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models. (arXiv:2310.16570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16570
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#25506;&#27979;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#31867;&#26041;&#26696;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#22312;&#19990;&#30028;&#30693;&#35782;&#20016;&#23500;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#19968;&#20107;&#23454;&#24341;&#36215;&#20102;&#31038;&#21306;&#23545;&#20110;&#37327;&#21270;PLMs&#20013;&#23384;&#22312;&#30340;&#20107;&#23454;&#30693;&#35782;&#37327;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#36825;&#35299;&#37322;&#20102;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#33021;&#35777;&#26126;&#23427;&#20204;&#20316;&#20026;&#30693;&#35782;&#24211;&#20351;&#29992;&#30340;&#21512;&#29702;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29992;&#20110;&#25506;&#27979;PLMs&#20107;&#23454;&#30693;&#35782;&#30340;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#65306;(1) &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36755;&#20837;&#12289;&#36755;&#20986;&#21644;&#34987;&#25506;&#27979;&#30340;PLMs&#22914;&#20309;&#36866;&#24212;&#30340;&#20998;&#31867;&#26041;&#26696;&#65307;(2) &#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#20107;&#23454;&#25506;&#27979;&#30340;&#25968;&#25454;&#38598;&#27010;&#36848;&#65307;(3) &#25105;&#20204;&#32508;&#21512;&#20102;&#20851;&#20110;PLMs&#20013;&#30693;&#35782;&#20445;&#30041;&#21644;&#25552;&#31034;&#20248;&#21270;&#30340;&#35266;&#28857;&#65292;&#20998;&#26512;&#20102;&#23558;PLMs&#20316;&#20026;&#30693;&#35782;&#24211;&#24212;&#29992;&#30340;&#38556;&#30861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models (PLMs) are trained on vast unlabeled data, rich in world knowledge. This fact has sparked the interest of the community in quantifying the amount of factual knowledge present in PLMs, as this explains their performance on downstream tasks, and potentially justifies their use as knowledge bases. In this work, we survey methods and datasets that are used to probe PLMs for factual knowledge. Our contributions are: (1) We propose a categorization scheme for factual probing methods that is based on how their inputs, outputs and the probed PLMs are adapted; (2) We provide an overview of the datasets used for factual probing; (3) We synthesize insights about knowledge retention and prompt optimization in PLMs, analyze obstacles to adopting PLMs as knowledge bases and outline directions for future work.
&lt;/p&gt;</description></item><item><title>1-PAGER&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#21644;&#35299;&#30721;&#36807;&#31243;&#21516;&#26102;&#22238;&#31572;&#38382;&#39064;&#21644;&#26816;&#32034;&#35777;&#25454;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32422;&#26463;&#35299;&#30721;&#21644;&#35777;&#25454;&#35821;&#26009;&#24211;&#30340;&#21033;&#29992;&#65292;&#36798;&#21040;&#20102;&#19982;&#20256;&#32479;&#30340;&#26816;&#32034;&#21644;&#38405;&#35835;&#26367;&#20195;&#26041;&#27861;&#30456;&#27604;&#26377;&#31454;&#20105;&#21147;&#30340;&#26816;&#32034;&#21644;&#22238;&#31572;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#20063;&#20026;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#26816;&#32034;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#35835;&#30340;&#25628;&#32034;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.16568</link><description>&lt;p&gt;
1-PAGER: &#19968;&#27425;&#36890;&#34892;&#30340;&#22238;&#31572;&#29983;&#25104;&#21644;&#35777;&#25454;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
1-PAGER: One Pass Answer Generation and Evidence Retrieval. (arXiv:2310.16568v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16568
&lt;/p&gt;
&lt;p&gt;
1-PAGER&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#21644;&#35299;&#30721;&#36807;&#31243;&#21516;&#26102;&#22238;&#31572;&#38382;&#39064;&#21644;&#26816;&#32034;&#35777;&#25454;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32422;&#26463;&#35299;&#30721;&#21644;&#35777;&#25454;&#35821;&#26009;&#24211;&#30340;&#21033;&#29992;&#65292;&#36798;&#21040;&#20102;&#19982;&#20256;&#32479;&#30340;&#26816;&#32034;&#21644;&#38405;&#35835;&#26367;&#20195;&#26041;&#27861;&#30456;&#27604;&#26377;&#31454;&#20105;&#21147;&#30340;&#26816;&#32034;&#21644;&#22238;&#31572;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#20063;&#20026;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#26816;&#32034;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#35835;&#30340;&#25628;&#32034;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;1-Pager&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#21333;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21644;&#35299;&#30721;&#36807;&#31243;&#21516;&#26102;&#22238;&#31572;&#38382;&#39064;&#21644;&#26816;&#32034;&#35777;&#25454;&#30340;&#31995;&#32479;&#12290;1-Pager&#20351;&#29992;&#32422;&#26463;&#35299;&#30721;&#36880;&#27493;&#20998;&#21106;&#26816;&#32034;&#35821;&#26009;&#24211;&#65292;&#36873;&#25321;&#25991;&#26723;&#21644;&#31572;&#26696;&#23383;&#31526;&#20018;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20986;&#22312;&#26816;&#32034;&#21644;&#22238;&#31572;&#20934;&#30830;&#24230;&#24230;&#37327;&#26041;&#38754;&#65292;&#36825;&#19982;&#21487;&#27604;&#36739;&#30340;&#26816;&#32034;&#21644;&#38405;&#35835;&#26367;&#20195;&#26041;&#27861;&#30456;&#27604;&#26159;&#26377;&#31454;&#20105;&#21147;&#30340;&#12290;1-Pager&#36824;&#36890;&#36807;&#23558;&#39044;&#27979;&#32467;&#26524;&#22522;&#20110;&#35777;&#25454;&#35821;&#26009;&#24211;&#65292;&#32988;&#36807;&#20102;&#31561;&#25928;&#30340;&#38381;&#20070;&#38382;&#31572;&#27169;&#22411;&#12290;&#34429;&#28982;1-Pager&#30340;&#34920;&#29616;&#36824;&#19981;&#21450;&#38405;&#35835;&#22810;&#20010;&#25991;&#26723;&#21518;&#29983;&#25104;&#31572;&#26696;&#30340;&#26356;&#26114;&#36149;&#31995;&#32479;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23427;&#26159;&#21521;&#24402;&#22240;&#29983;&#25104;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#65292;&#23558;&#26816;&#32034;&#34701;&#20837;&#21040;&#24403;&#21069;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#33539;&#24335;&#20013;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#29992;&#20110;&#20998;&#21106;&#35821;&#26009;&#24211;&#30340;&#25628;&#32034;&#36335;&#24452;&#26131;&#20110;&#38405;&#35835;&#21644;&#29702;&#35299;&#65292;&#20026;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#26816;&#32034;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present 1-Pager the first system that answers a question and retrieves evidence using a single Transformer-based model and decoding process. 1-Pager incrementally partitions the retrieval corpus using constrained decoding to select a document and answer string, and we show that this is competitive with comparable retrieve-and-read alternatives according to both retrieval and answer accuracy metrics. 1-Pager also outperforms the equivalent closed-book question answering model, by grounding predictions in an evidence corpus. While 1-Pager is not yet on-par with more expensive systems that read many more documents before generating an answer, we argue that it provides an important step toward attributed generation by folding retrieval into the sequence-to-sequence paradigm that is currently dominant in NLP. We also show that the search paths used to partition the corpus are easy to read and understand, paving a way forward for interpretable neural retrieval.
&lt;/p&gt;</description></item><item><title>FedTherapist&#26159;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#36827;&#34892;&#29992;&#25143;&#29983;&#25104;&#30340;&#35821;&#35328;&#34920;&#36798;&#30340;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#30340;&#31995;&#32479;&#12290;&#23427;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#25345;&#32493;&#35821;&#38899;&#21644;&#38190;&#30424;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#35821;&#35328;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#27604;&#36739;&#20102;&#38750;&#35821;&#35328;&#29305;&#24449;&#65292;&#32467;&#26524;&#26174;&#31034;FedTherapist&#22312;&#39044;&#27979;&#25233;&#37057;&#12289;&#21387;&#21147;&#12289;&#28966;&#34385;&#21644;&#24515;&#24773;&#26041;&#38754;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.16538</link><description>&lt;p&gt;
FedTherapist&#65306;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#29992;&#25143;&#29983;&#25104;&#30340;&#35821;&#35328;&#34920;&#36798;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning. (arXiv:2310.16538v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16538
&lt;/p&gt;
&lt;p&gt;
FedTherapist&#26159;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#36827;&#34892;&#29992;&#25143;&#29983;&#25104;&#30340;&#35821;&#35328;&#34920;&#36798;&#30340;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#30340;&#31995;&#32479;&#12290;&#23427;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#25345;&#32493;&#35821;&#38899;&#21644;&#38190;&#30424;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#35821;&#35328;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#27604;&#36739;&#20102;&#38750;&#35821;&#35328;&#29305;&#24449;&#65292;&#32467;&#26524;&#26174;&#31034;FedTherapist&#22312;&#39044;&#27979;&#25233;&#37057;&#12289;&#21387;&#21147;&#12289;&#28966;&#34385;&#21644;&#24515;&#24773;&#26041;&#38754;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#31070;&#31185;&#21307;&#29983;&#36890;&#36807;&#24739;&#32773;&#30340;&#35821;&#35328;&#20351;&#29992;&#26469;&#35786;&#26029;&#31934;&#31070;&#30142;&#30149;&#12290;&#20294;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#34987;&#21160;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20351;&#29992;&#25163;&#26426;&#35774;&#22791;&#30340;&#27963;&#21160;&#12289;&#24212;&#29992;&#20351;&#29992;&#21644;&#20301;&#32622;&#31561;&#26367;&#20195;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FedTherapist&#65292;&#19968;&#31181;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20197;&#38544;&#31169;&#20445;&#25252;&#26041;&#24335;&#20351;&#29992;&#25345;&#32493;&#35821;&#38899;&#21644;&#38190;&#30424;&#36755;&#20837;&#30340;&#31227;&#21160;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#35774;&#35745;&#30340;&#24615;&#33021;&#21644;&#24320;&#38144;&#26469;&#20811;&#26381;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#36827;&#34892;&#35774;&#22791;&#20869;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#35821;&#35328;&#23398;&#20064;&#65288;CALL&#65289;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#26234;&#33021;&#25163;&#26426;&#30340;&#22823;&#35268;&#27169;&#21644;&#22024;&#26434;&#25991;&#26412;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#20449;&#21495;&#24863;&#30693;&#12290;&#25105;&#20204;&#22312;46&#21517;&#21442;&#19982;&#32773;&#20013;&#36827;&#34892;&#20102;&#32463;IRB&#25209;&#20934;&#30340;&#33258;&#25105;&#25253;&#21578;&#25233;&#37057;&#12289;&#21387;&#21147;&#12289;&#28966;&#34385;&#21644;&#24515;&#24773;&#30340;&#39044;&#27979;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;FedTherapist&#30456;&#27604;&#20110;&#38750;&#35821;&#35328;&#29305;&#24449;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;0.15 AUROC&#12290;
&lt;/p&gt;
&lt;p&gt;
Psychiatrists diagnose mental disorders via the linguistic use of patients. Still, due to data privacy, existing passive mental health monitoring systems use alternative features such as activity, app usage, and location via mobile devices. We propose FedTherapist, a mobile mental health monitoring system that utilizes continuous speech and keyboard input in a privacy-preserving way via federated learning. We explore multiple model designs by comparing their performance and overhead for FedTherapist to overcome the complex nature of on-device language model training on smartphones. We further propose a Context-Aware Language Learning (CALL) methodology to effectively utilize smartphones' large and noisy text for mental health signal sensing. Our IRB-approved evaluation of the prediction of self-reported depression, stress, anxiety, and mood from 46 participants shows higher accuracy of FedTherapist compared with the performance with non-language features, achieving 0.15 AUROC improveme
&lt;/p&gt;</description></item><item><title>R$^3$&#25552;&#31034;&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#22122;&#22768;&#32972;&#26223;&#19979;&#36827;&#34892;CoT&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22797;&#23457;&#12289;&#25913;&#20889;&#21644;&#35299;&#20915;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#20197;&#36827;&#34892;&#20851;&#38190;&#21477;&#25552;&#21462;&#12289;&#21464;&#37327;&#22768;&#26126;&#21644;&#31572;&#26696;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.16535</link><description>&lt;p&gt;
R$^3$ Prompting&#65306;&#26080;&#22122;&#22768;&#32972;&#26223;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#30340;&#35780;&#23457;&#12289;&#25913;&#20889;&#21644;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context. (arXiv:2310.16535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16535
&lt;/p&gt;
&lt;p&gt;
R$^3$&#25552;&#31034;&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#22122;&#22768;&#32972;&#26223;&#19979;&#36827;&#34892;CoT&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22797;&#23457;&#12289;&#25913;&#20889;&#21644;&#35299;&#20915;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#20197;&#36827;&#34892;&#20851;&#38190;&#21477;&#25552;&#21462;&#12289;&#21464;&#37327;&#22768;&#26126;&#21644;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#25552;&#31034;&#30340;&#24110;&#21161;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#22312;&#26080;&#22122;&#22768;&#32972;&#26223;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;LLMs&#22312;&#22122;&#22768;&#32972;&#26223;&#19979;&#20135;&#29983;&#19981;&#20934;&#30830;&#32467;&#26524;&#30340;&#22256;&#22659;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35843;&#26597;&#12290;&#29616;&#26377;&#30740;&#31350;&#21033;&#29992;&#35302;&#21457;&#21477;&#23376;&#40723;&#21169;LLMs&#38598;&#20013;&#20110;&#30456;&#20851;&#20449;&#24687;&#65292;&#20294;&#35302;&#21457;&#23545;&#26368;&#32456;&#31572;&#26696;&#39044;&#27979;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;&#21463;&#20132;&#20114;&#24335;CoT&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#21644;LLMs&#20043;&#38388;&#22810;&#36718;&#20114;&#21160;&#20419;&#36827;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21363;R$^3$&#25552;&#31034;&#65292;&#29992;&#20110;&#22312;&#22122;&#22768;&#32972;&#26223;&#19979;&#36827;&#34892;CoT&#24605;&#32500;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;R$^3$&#25552;&#31034;&#19982;LLMs&#36827;&#34892;&#20851;&#38190;&#21477;&#25552;&#21462;&#12289;&#21464;&#37327;&#22768;&#26126;&#21644;&#31572;&#26696;&#39044;&#27979;&#30340;&#20132;&#20114;&#65292;&#23545;&#24212;&#20110;&#22797;&#23457;&#12289;&#25913;&#20889;&#21644;&#35299;&#20915;&#30340;&#24605;&#32771;&#36807;&#31243;&#12290;&#26368;&#21518;&#19968;&#27425;&#20114;&#21160;&#20013;&#29983;&#25104;&#30340;&#21709;&#24212;&#23558;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
With the help of Chain-of-Thought (CoT) prompting, Large Language Models (LLMs) have achieved remarkable performance on various reasoning tasks. However, most of them have been evaluated under noise-free context and the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated. Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction. Inspired by interactive CoT method, where intermediate reasoning steps are promoted by multiple rounds of interaction between users and LLMs, we propose a novel prompting method, namely R$^3$ prompting, for CoT reasoning under noisy context. Specifically, R$^3$ prompting interacts with LLMs to perform key sentence extraction, variable declaration and answer prediction, which corresponds to a thought process of reviewing, rephrasing and resolving. The responses generated at the last interaction will perfo
&lt;/p&gt;</description></item><item><title>GPT-4V&#22312;&#33521;&#25991;&#35270;&#35273;&#20026;&#20013;&#24515;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35782;&#21035;&#20013;&#25991;&#25991;&#26412;&#12289;&#22788;&#29702;&#25935;&#24863;&#29305;&#24449;&#30456;&#20851;&#38382;&#39064;&#21644;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.16534</link><description>&lt;p&gt;
GPT-4V&#30340;&#26089;&#26399;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Early Evaluation of GPT-4V(ision). (arXiv:2310.16534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16534
&lt;/p&gt;
&lt;p&gt;
GPT-4V&#22312;&#33521;&#25991;&#35270;&#35273;&#20026;&#20013;&#24515;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35782;&#21035;&#20013;&#25991;&#25991;&#26412;&#12289;&#22788;&#29702;&#25935;&#24863;&#29305;&#24449;&#30456;&#20851;&#38382;&#39064;&#21644;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;GPT-4V&#30340;&#22810;&#20010;&#33021;&#21147;&#65292;&#21253;&#25324;&#35270;&#35273;&#29702;&#35299;&#12289;&#35821;&#35328;&#29702;&#35299;&#12289;&#35270;&#35273;&#25340;&#22270;&#35299;&#20915;&#20197;&#21450;&#23545;&#28145;&#24230;&#12289;&#28909;&#37327;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#31561;&#20854;&#20182;&#27169;&#24577;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35780;&#20272;GPT-4V&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#25163;&#21160;&#26500;&#24314;&#20102;656&#20010;&#27979;&#35797;&#23454;&#20363;&#65292;&#24182;&#23545;GPT-4V&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#20180;&#32454;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#22914;&#19979;&#20142;&#28857;&#65306;(1) GPT-4V&#22312;&#33521;&#35821;&#35270;&#35273;&#20026;&#20013;&#24515;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22270;&#20687;&#20013;&#26080;&#27861;&#35782;&#21035;&#31616;&#21333;&#30340;&#20013;&#25991;&#25991;&#26412;&#65307;(2) &#24403;&#22238;&#31572;&#19982;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#25935;&#24863;&#29305;&#24449;&#30456;&#20851;&#30340;&#38382;&#39064;&#26102;&#65292;GPT-4V&#34920;&#29616;&#20986;&#19981;&#19968;&#33268;&#30340;&#25298;&#32477;&#34892;&#20026;&#65307;(3) GPT-4V&#22312;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#27604;GPT-4 (API)&#24046;&#65292;&#21253;&#25324;&#19968;&#33324;&#30340;&#35821;&#35328;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#21644;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#65307;(4) &#23569;&#26679;&#26412;&#21551;&#31034;&#27861;&#21487;&#20197;&#25552;&#39640;GPT-4V&#22312;&#35270;&#35273;&#29702;&#35299;&#21644;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#34920;&#29616;&#65307;(5) GPT-4V&#22312;&#23547;&#25214;&#32454;&#24494;&#24046;&#21035;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we evaluate different abilities of GPT-4V including visual understanding, language understanding, visual puzzle solving, and understanding of other modalities such as depth, thermal, video, and audio. To estimate GPT-4V's performance, we manually construct 656 test instances and carefully evaluate the results of GPT-4V. The highlights of our findings are as follows: (1) GPT-4V exhibits impressive performance on English visual-centric benchmarks but fails to recognize simple Chinese texts in the images; (2) GPT-4V shows inconsistent refusal behavior when answering questions related to sensitive traits such as gender, race, and age; (3) GPT-4V obtains worse results than GPT-4 (API) on language understanding tasks including general language understanding benchmarks and visual commonsense knowledge evaluation benchmarks; (4) Few-shot prompting can improve GPT-4V's performance on both visual understanding and language understanding; (5) GPT-4V struggles to find the nuances be
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#26597;&#29702;&#22823;&#23398;&#22312;MRL 2023&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#20449;&#24687;&#26816;&#32034;&#20849;&#20139;&#20219;&#21153;&#20013;&#37319;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#32763;&#35793;&#27979;&#35797;&#26041;&#27861;&#23454;&#29616;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#38382;&#39064;&#22238;&#31572;&#30340;&#22810;&#35821;&#35328;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#31614;&#25935;&#24863;&#30340;&#32763;&#35793;&#27169;&#22411;&#35780;&#20998;&#20505;&#36873;&#20301;&#32622;&#65292;&#20445;&#25345;&#20102;&#25512;&#26029;&#26631;&#31614;&#22312;&#21407;&#22987;&#35821;&#35328;&#20013;&#30340;&#27491;&#30830;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2310.16528</link><description>&lt;p&gt;
&#12298;CUNI&#25237;&#31295;MRL 2023&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#20449;&#24687;&#26816;&#32034;&#20849;&#20139;&#20219;&#21153;&#12299;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
CUNI Submission to MRL 2023 Shared Task on Multi-lingual Multi-task Information Retrieval. (arXiv:2310.16528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16528
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#26597;&#29702;&#22823;&#23398;&#22312;MRL 2023&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#20449;&#24687;&#26816;&#32034;&#20849;&#20139;&#20219;&#21153;&#20013;&#37319;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#32763;&#35793;&#27979;&#35797;&#26041;&#27861;&#23454;&#29616;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#38382;&#39064;&#22238;&#31572;&#30340;&#22810;&#35821;&#35328;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#31614;&#25935;&#24863;&#30340;&#32763;&#35793;&#27169;&#22411;&#35780;&#20998;&#20505;&#36873;&#20301;&#32622;&#65292;&#20445;&#25345;&#20102;&#25512;&#26029;&#26631;&#31614;&#22312;&#21407;&#22987;&#35821;&#35328;&#20013;&#30340;&#27491;&#30830;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#26597;&#29702;&#22823;&#23398;&#22312;MRL 2023&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#20449;&#24687;&#26816;&#32034;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#31995;&#32479;&#12290;&#35813;&#20849;&#20139;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#38024;&#23545;&#22810;&#20010;&#35821;&#35328;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#38382;&#39064;&#22238;&#31572;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#37117;&#20381;&#36182;&#20110;&#8220;&#32763;&#35793;&#27979;&#35797;&#8221;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#23558;&#26410;&#26631;&#35760;&#30340;&#31034;&#20363;&#32763;&#35793;&#25104;&#33521;&#35821;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#23545;&#32763;&#35793;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#25512;&#26029;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#26631;&#35760;&#30340;&#25968;&#25454;&#25237;&#24433;&#22238;&#21407;&#22987;&#35821;&#35328;&#12290;&#20026;&#20102;&#20445;&#25345;&#21407;&#22987;&#35821;&#35328;&#20013;&#30340;&#25512;&#26029;&#26631;&#31614;&#27491;&#30830;&#30340;&#20301;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20351;&#29992;&#26631;&#31614;&#25935;&#24863;&#32763;&#35793;&#27169;&#22411;&#35780;&#20998;&#20505;&#36873;&#20301;&#32622;&#30340;&#26041;&#27861;&#12290;&#22312;&#20004;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#22312;&#32763;&#35793;&#21518;&#30340;&#25968;&#25454;&#19978;&#23581;&#35797;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24320;&#21457;&#25968;&#25454;&#21644;&#20849;&#20139;&#20219;&#21153;&#39564;&#35777;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#23384;&#22312;&#39046;&#22495;&#19981;&#21305;&#37197;&#65292;&#24494;&#35843;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#24471;&#21040;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Charles University system for the MRL~2023 Shared Task on Multi-lingual Multi-task Information Retrieval. The goal of the shared task was to develop systems for named entity recognition and question answering in several under-represented languages. Our solutions to both subtasks rely on the translate-test approach. We first translate the unlabeled examples into English using a multilingual machine translation model. Then, we run inference on the translated data using a strong task-specific model. Finally, we project the labeled data back into the original language. To keep the inferred tags on the correct positions in the original language, we propose a method based on scoring the candidate positions using a label-sensitive translation model. In both settings, we experiment with finetuning the classification models on the translated data. However, due to a domain mismatch between the development data and the shared task validation and test sets, the finetuned models coul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#21475;&#22810;&#26679;&#24615;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;CCSV&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#25512;&#29702;&#33021;&#21147;&#26469;&#25913;&#21892;&#20154;&#21475;&#22810;&#26679;&#24615;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#30340;&#31034;&#20363;&#25110;&#25552;&#31034;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2310.16523</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#20307;&#25209;&#35780;&#21644;&#33258;&#25105;&#25237;&#31080;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#21475;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting. (arXiv:2310.16523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#21475;&#22810;&#26679;&#24615;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;CCSV&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#25512;&#29702;&#33021;&#21147;&#26469;&#25913;&#21892;&#20154;&#21475;&#22810;&#26679;&#24615;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#30340;&#31034;&#20363;&#25110;&#25552;&#31034;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35828;&#65292;&#22810;&#26679;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#24403;&#29992;&#25143;&#30340;&#25552;&#31034;&#19981;&#26126;&#30830;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#22312;&#29983;&#25104;&#21709;&#24212;&#26102;&#36981;&#24490;&#38544;&#21547;&#20551;&#35774;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#21709;&#24212;&#30340;&#21516;&#36136;&#21270;&#65292;&#20197;&#21450;&#26576;&#20123;&#20154;&#21475;&#32676;&#20307;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#29978;&#33267;&#28040;&#22833;&#22312;&#29983;&#25104;&#30340;&#21709;&#24212;&#20013;&#12290;&#26412;&#25991;&#35268;&#33539;&#20102;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#22810;&#26679;&#24615;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#34913;&#37327;&#22312;&#20154;&#21644;&#25991;&#21270;&#26041;&#21521;&#19978;&#29983;&#25104;&#21709;&#24212;&#22810;&#26679;&#24615;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#29702;&#35299;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#19988;&#23427;&#20204;&#21487;&#20197;&#23545;&#33258;&#24049;&#30340;&#21709;&#24212;&#36827;&#34892;&#25512;&#29702;&#21644;&#25209;&#35780;&#20197;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;&#36825;&#19968;&#21457;&#29616;&#28608;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#38598;&#20307;&#25209;&#35780;&#21644;&#33258;&#25105;&#25237;&#31080;(CCSC)&#30340;&#26032;&#25552;&#31034;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23427;&#30340;&#22810;&#26679;&#24615;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#39640;LLMs&#30340;&#20154;&#21475;&#22810;&#26679;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#31034;&#20363;&#25110;&#25552;&#31034;&#35843;&#25972;&#12290;&#36890;&#36807;&#20154;&#31867;&#21644;&#33258;&#21160;&#21270;&#35780;&#20272;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial challenge for generative large language models (LLMs) is diversity: when a user's prompt is under-specified, models may follow implicit assumptions while generating a response, which may result in homogenization of the responses, as well as certain demographic groups being under-represented or even erased from the generated responses. In this paper, we formalize diversity of representation in generative LLMs. We present evaluation datasets and propose metrics to measure diversity in generated responses along people and culture axes. We find that LLMs understand the notion of diversity, and that they can reason and critique their own responses for that goal. This finding motivated a new prompting technique called collective-critique and self-voting (CCSV) to self-improve people diversity of LLMs by tapping into its diversity reasoning capabilities, without relying on handcrafted examples or prompt tuning. Extensive empirical experiments with both human and automated evaluation
&lt;/p&gt;</description></item><item><title>OccuQuest&#26159;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32844;&#19994;&#20559;&#35265;&#30340;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#23427;&#21253;&#21547;&#20102;1,000&#22810;&#20010;&#32844;&#19994;&#30340;110,000+&#20010;&#25552;&#31034;&#23436;&#25104;&#23545;&#21644;30,000+&#20010;&#23545;&#35805;&#65292;&#24182;&#34920;&#29616;&#20986;&#26356;&#21152;&#24179;&#34913;&#30340;&#32844;&#19994;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2310.16517</link><description>&lt;p&gt;
OccuQuest&#65306;&#32531;&#35299;&#38024;&#23545;&#20855;&#26377;&#21253;&#23481;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32844;&#19994;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models. (arXiv:2310.16517v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16517
&lt;/p&gt;
&lt;p&gt;
OccuQuest&#26159;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32844;&#19994;&#20559;&#35265;&#30340;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#23427;&#21253;&#21547;&#20102;1,000&#22810;&#20010;&#32844;&#19994;&#30340;110,000+&#20010;&#25552;&#31034;&#23436;&#25104;&#23545;&#21644;30,000+&#20010;&#23545;&#35805;&#65292;&#24182;&#34920;&#29616;&#20986;&#26356;&#21152;&#24179;&#34913;&#30340;&#32844;&#19994;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#38598;&#23384;&#22312;&#32844;&#19994;&#20559;&#35265;&#65306;&#22823;&#22810;&#25968;&#25968;&#25454;&#21482;&#28041;&#21450;&#23569;&#25968;&#20960;&#20010;&#32844;&#19994;&#65292;&#36825;&#20351;&#24471;&#25351;&#23548;&#35843;&#20248;&#30340;LLMs&#38590;&#20197;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#20174;&#19994;&#20154;&#21592;&#30340;&#19987;&#19994;&#38382;&#39064;&#29983;&#25104;&#26377;&#29992;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#24182;&#25512;&#24191;&#21253;&#23481;&#24615;&#30340;LLMs&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;OccuQuest&#8221;&#30340;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;110,000&#22810;&#20010;&#25552;&#31034;&#23436;&#25104;&#23545;&#21644;30,000&#22810;&#20010;&#23545;&#35805;&#65292;&#28085;&#30422;&#20102;26&#20010;&#32844;&#19994;&#31867;&#21035;&#20013;&#30340;1,000&#22810;&#20010;&#32844;&#19994;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#19977;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#65288;Dolly&#12289;ShareGPT&#21644;WizardLM&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;OccuQuest&#22312;&#32844;&#19994;&#20998;&#24067;&#19978;&#26356;&#21152;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large language models (LLMs) has revolutionized natural language processing tasks. However, existing instruction-tuning datasets suffer from occupational bias: the majority of data relates to only a few occupations, which hampers the instruction-tuned LLMs to generate helpful responses to professional queries from practitioners in specific fields. To mitigate this issue and promote occupation-inclusive LLMs, we create an instruction-tuning dataset named \emph{OccuQuest}, which contains 110,000+ prompt-completion pairs and 30,000+ dialogues covering over 1,000 occupations in 26 occupational categories. We systematically request ChatGPT, organizing queries hierarchically based on Occupation, Responsibility, Topic, and Question, to ensure a comprehensive coverage of occupational specialty inquiries. By comparing with three commonly used datasets (Dolly, ShareGPT, and WizardLM), we observe that OccuQuest exhibits a more balanced distribution across occupations. Furthermore
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34920;&#31034;&#31354;&#38388;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#20061;&#20010;&#20219;&#21153;&#20013;&#30340;&#35821;&#35328;&#20449;&#24687;&#22312;&#19981;&#21516;&#38454;&#27573;&#21644;&#26102;&#38388;&#28857;&#36880;&#28176;&#20986;&#29616;&#12289;&#20849;&#20139;&#21644;&#20998;&#21270;&#12290;&#21477;&#27861;&#30693;&#35782;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#36805;&#36895;&#20064;&#24471;&#65292;&#32780;&#21518;&#26399;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#26469;&#33258;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#21644;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#29702;&#35299;&#30340;&#25552;&#21319;&#12290;&#35821;&#20041;&#21644;&#25512;&#29702;&#20219;&#21153;&#21017;&#21463;&#30410;&#20110;&#26356;&#39640;&#23618;&#27425;&#30340;&#29305;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.16484</link><description>&lt;p&gt;
&#32500;&#24230;&#31354;&#38388;&#32534;&#24180;&#21490;: &#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35821;&#35328;&#20449;&#24687;&#22914;&#20309;&#20986;&#29616;&#12289;&#21464;&#21270;&#21644;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training. (arXiv:2310.16484v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16484
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34920;&#31034;&#31354;&#38388;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#20061;&#20010;&#20219;&#21153;&#20013;&#30340;&#35821;&#35328;&#20449;&#24687;&#22312;&#19981;&#21516;&#38454;&#27573;&#21644;&#26102;&#38388;&#28857;&#36880;&#28176;&#20986;&#29616;&#12289;&#20849;&#20139;&#21644;&#20998;&#21270;&#12290;&#21477;&#27861;&#30693;&#35782;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#36805;&#36895;&#20064;&#24471;&#65292;&#32780;&#21518;&#26399;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#26469;&#33258;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#21644;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#29702;&#35299;&#30340;&#25552;&#21319;&#12290;&#35821;&#20041;&#21644;&#25512;&#29702;&#20219;&#21153;&#21017;&#21463;&#30410;&#20110;&#26356;&#39640;&#23618;&#27425;&#30340;&#29305;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#31354;&#38388;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#22522;&#30784;&#65292;&#28982;&#32780;&#20851;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#20449;&#24687;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#20197;&#21450;&#20309;&#26102;&#20986;&#29616;&#21644;&#20132;&#20114;&#30340;&#29702;&#35299;&#36824;&#26377;&#38480;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#35770;&#25506;&#27979;&#22871;&#20214;&#65292;&#25105;&#20204;&#19981;&#20165;&#33021;&#30452;&#25509;&#27604;&#36739;&#20219;&#21153;&#24615;&#33021;&#65292;&#36824;&#33021;&#27604;&#36739;&#23427;&#20204;&#30340;&#34920;&#31034;&#23376;&#31354;&#38388;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21253;&#25324;&#21477;&#27861;&#12289;&#35821;&#20041;&#21644;&#25512;&#29702;&#22312;&#20869;&#30340;&#20061;&#20010;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;2M&#20010;&#39044;&#35757;&#32451;&#27493;&#39588;&#21644;&#20116;&#20010;&#31181;&#23376;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20219;&#21153;&#21644;&#26102;&#38388;&#19978;&#30340;&#20851;&#38190;&#23398;&#20064;&#38454;&#27573;&#65292;&#22312;&#36825;&#20123;&#38454;&#27573;&#65292;&#23376;&#31354;&#38388;&#20250;&#20986;&#29616;&#12289;&#20849;&#20139;&#20449;&#24687;&#65292;&#28982;&#21518;&#20998;&#21270;&#25104;&#29305;&#23450;&#30340;&#23376;&#31354;&#38388;&#12290;&#22312;&#36825;&#20123;&#38454;&#27573;&#20013;&#65292;&#21477;&#27861;&#30693;&#35782;&#22312;&#23436;&#25104;&#20840;&#37096;&#35757;&#32451;&#30340;0.5%&#20043;&#21518;&#24456;&#24555;&#34987;&#33719;&#24471;&#12290;&#25345;&#32493;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#26469;&#28304;&#20110;&#23545;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#30340;&#20064;&#24471;&#65292;&#32780;&#35821;&#20041;&#21644;&#25512;&#29702;&#20219;&#21153;&#21017;&#21463;&#30410;&#20110;&#26356;&#39640;&#32423;&#21035;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#21644;&#26356;&#39640;&#23618;&#27425;&#30340;&#29305;&#21270;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#36328;&#20219;&#21153;&#30340;&#30456;&#20284;&#24230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Representational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic probing suite, which enables direct comparisons of not just task performance, but their representational subspaces, we analyze nine tasks covering syntax, semantics and reasoning, across 2M pre-training steps and five seeds. We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization. Measuring cross-task simil
&lt;/p&gt;</description></item><item><title>CLEX&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#38271;&#24230;&#22806;&#25512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20301;&#32622;&#23884;&#20837;&#32553;&#25918;&#26041;&#27861;&#25512;&#24191;&#21040;&#36830;&#32493;&#21160;&#24577;&#24314;&#27169;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#29305;&#23450;&#38271;&#24230;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16450</link><description>&lt;p&gt;
CLEX: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#38271;&#24230;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
CLEX: Continuous Length Extrapolation for Large Language Models. (arXiv:2310.16450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16450
&lt;/p&gt;
&lt;p&gt;
CLEX&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#38271;&#24230;&#22806;&#25512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20301;&#32622;&#23884;&#20837;&#32553;&#25918;&#26041;&#27861;&#25512;&#24191;&#21040;&#36830;&#32493;&#21160;&#24577;&#24314;&#27169;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#29305;&#23450;&#38271;&#24230;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21331;&#36234;&#33021;&#21147;&#21463;&#38480;&#20110;Transformer&#30340;&#39044;&#35774;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#20301;&#32622;&#23884;&#20837;&#65288;PE&#65289;&#32553;&#25918;&#26041;&#27861;&#34429;&#28982;&#33021;&#22815;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#29305;&#23450;&#38271;&#24230;&#65292;&#20294;&#22312;&#22806;&#25512;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#65292;&#25110;&#32773;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#29306;&#29298;&#37096;&#20998;&#24615;&#33021;&#12290;&#34429;&#28982;&#38271;&#24230;&#22806;&#25512;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#33021;&#22815;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#24310;&#38271;&#33267;&#35757;&#32451;&#24207;&#21015;&#38271;&#24230;&#20043;&#22806;&#65292;&#20294;&#22312;&#23454;&#38469;&#30340;&#38271;&#19978;&#19979;&#25991;&#24212;&#29992;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;LLMs&#30340;&#25345;&#32493;&#38271;&#24230;&#22806;&#25512;&#65288;CLEX&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;PE&#32553;&#25918;&#26041;&#27861;&#25512;&#24191;&#21040;&#36890;&#36807;&#24120;&#24494;&#20998;&#26041;&#31243;&#23545;&#38271;&#24230;&#32553;&#25918;&#22240;&#23376;&#24314;&#27169;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#24403;&#21069;&#20026;&#29305;&#23450;&#38271;&#24230;&#35774;&#35745;&#30340;PE&#32553;&#25918;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. Length extrapolation methods, although theoretically capable of extending the context window beyond the training sequence length, often underperform in practical long-context applications. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. Moreover, by extending 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#65288;mQG&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20851;&#27880;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;FairytaleQA&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20197;&#21450;&#22312;TellMeWhy&#21644;SQuAD1.1&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#38646;-shot&#36866;&#24212;&#65292;mQG&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#38382;&#39064;&#29983;&#25104;&#30340;&#22810;&#26679;&#24615;&#24341;&#20837;&#25925;&#20107;&#20070;&#39046;&#22495;&#65292;&#20026;&#25552;&#39640;&#29702;&#35299;&#21644;&#21442;&#19982;&#24230;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16446</link><description>&lt;p&gt;
&#25925;&#20107;&#20070;&#30340;&#22810;&#26679;&#24615;&#22686;&#24378;&#21465;&#20107;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diversity Enhanced Narrative Question Generation for Storybooks. (arXiv:2310.16446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#65288;mQG&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20851;&#27880;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;FairytaleQA&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20197;&#21450;&#22312;TellMeWhy&#21644;SQuAD1.1&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#38646;-shot&#36866;&#24212;&#65292;mQG&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#38382;&#39064;&#29983;&#25104;&#30340;&#22810;&#26679;&#24615;&#24341;&#20837;&#25925;&#20107;&#20070;&#39046;&#22495;&#65292;&#20026;&#25552;&#39640;&#29702;&#35299;&#21644;&#21442;&#19982;&#24230;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#29983;&#25104;&#38382;&#39064;&#21487;&#20197;&#22686;&#24378;&#29702;&#35299;&#12289;&#21442;&#19982;&#24230;&#12289;&#35780;&#20272;&#21644;&#23398;&#20064;&#25110;&#23545;&#35805;&#29615;&#22659;&#30340;&#25972;&#20307;&#25928;&#21147;&#12290;&#23613;&#31649;&#38382;&#39064;&#29983;&#25104;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#25552;&#39640;&#25110;&#34913;&#37327;&#29983;&#25104;&#38382;&#39064;&#30340;&#22810;&#26679;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#65288;mQG&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20851;&#27880;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#39564;&#35777;&#29983;&#25104;&#38382;&#39064;&#30340;&#21487;&#22238;&#31572;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#32463;&#36807;SQuAD2.0&#24494;&#35843;&#30340;&#38382;&#31572;&#27169;&#22411;&#65292;&#23558;&#38382;&#39064;&#20998;&#31867;&#20026;&#21487;&#22238;&#31572;&#25110;&#19981;&#21487;&#22238;&#31572;&#12290;&#25105;&#20204;&#22312;FairytaleQA&#25968;&#25454;&#38598;&#19978;&#23545;mQG&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#22522;&#20110;&#25925;&#20107;&#20070;&#30340;&#32467;&#26500;&#21270;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#21465;&#20107;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;TellMeWhy&#21644;SQuAD1.1&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#38646;-shot&#36866;&#24212;&#12290;mQG&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question generation (QG) from a given context can enhance comprehension, engagement, assessment, and overall efficacy in learning or conversational environments. Despite recent advancements in QG, the challenge of enhancing or measuring the diversity of generated questions often remains unaddressed. In this paper, we introduce a multi-question generation model (mQG), which is capable of generating multiple, diverse, and answerable questions by focusing on context and questions. To validate the answerability of the generated questions, we employ a SQuAD2.0 fine-tuned question answering model, classifying the questions as answerable or not. We train and evaluate mQG on the FairytaleQA dataset, a well-structured QA dataset based on storybooks, with narrative questions. We further apply a zero-shot adaptation on the TellMeWhy and SQuAD1.1 datasets. mQG shows promising results across various evaluation metrics, among strong baselines.
&lt;/p&gt;</description></item><item><title>DDCoT&#26159;&#19968;&#31181;&#32844;&#36131;&#20998;&#26126;&#30340;&#24605;&#36335;&#38142;&#21050;&#28608;&#26041;&#27861;&#65292;&#36890;&#36807;&#36127;&#31354;&#38388;&#21050;&#28608;&#20445;&#25345;&#25209;&#21028;&#24577;&#24230;&#65292;&#24182;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#32467;&#21512;&#20102;&#20851;&#38190;&#27934;&#35265;&#8220;&#20445;&#25345;&#25209;&#21028;&#24615;&#24605;&#32771;&#8221;&#21644;&#8220;&#35753;&#27599;&#20010;&#20154;&#21457;&#25381;&#33258;&#24049;&#30340;&#20316;&#29992;&#8221;&#12290;</title><link>http://arxiv.org/abs/2310.16436</link><description>&lt;p&gt;
DDCoT: &#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32844;&#36131;&#20998;&#26126;&#30340;&#24605;&#36335;&#38142;&#21050;&#28608;
&lt;/p&gt;
&lt;p&gt;
DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models. (arXiv:2310.16436v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16436
&lt;/p&gt;
&lt;p&gt;
DDCoT&#26159;&#19968;&#31181;&#32844;&#36131;&#20998;&#26126;&#30340;&#24605;&#36335;&#38142;&#21050;&#28608;&#26041;&#27861;&#65292;&#36890;&#36807;&#36127;&#31354;&#38388;&#21050;&#28608;&#20445;&#25345;&#25209;&#21028;&#24577;&#24230;&#65292;&#24182;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#32467;&#21512;&#20102;&#20851;&#38190;&#27934;&#35265;&#8220;&#20445;&#25345;&#25209;&#21028;&#24615;&#24605;&#32771;&#8221;&#21644;&#8220;&#35753;&#27599;&#20010;&#20154;&#21457;&#25381;&#33258;&#24049;&#30340;&#20316;&#29992;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#19968;&#20010;&#38271;&#36828;&#30446;&#26631;&#26159;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#30340;&#22797;&#26434;&#22810;&#27169;&#24577;&#25512;&#29702;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#27169;&#24577;&#19978;&#21033;&#29992;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#27169;&#25311;&#20154;&#31867;&#24605;&#32500;&#65292;&#22312;&#22810;&#27493;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#36827;&#23637;&#36716;&#31227;&#21040;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#24341;&#20837;&#20102;&#26356;&#39640;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#23545;&#21171;&#21160;&#23494;&#38598;&#22411;&#27880;&#37322;&#30340;&#38656;&#27714;&#20197;&#21450;&#28789;&#27963;&#24615;&#12289;&#19968;&#33324;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#21796;&#36215;CoT&#25512;&#29702;&#65292;&#26412;&#30740;&#31350;&#39318;&#20808;&#23545;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#27934;&#35265;&#65306;&#8220;&#20445;&#25345;&#25209;&#21028;&#24615;&#24605;&#32771;&#8221;&#21644;&#8220;&#35753;&#27599;&#20010;&#20154;&#21457;&#25381;&#33258;&#24049;&#30340;&#20316;&#29992;&#8221;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DDCoT&#21050;&#28608;&#26041;&#27861;&#65292;&#36890;&#36807;&#36127;&#31354;&#38388;&#21050;&#28608;&#20445;&#25345;&#25209;&#21028;&#24577;&#24230;&#65292;&#24182;&#36890;&#36807;&#39318;&#20808;&#21010;&#20998;&#36131;&#20219;&#26126;&#30830;&#22269;&#22810;&#27169;&#24577;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights: "keeping critical thinking" and "letting everyone do their jobs" in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the r
&lt;/p&gt;</description></item><item><title>PromptAgent&#26159;&#19968;&#20010;&#36890;&#36807;&#25112;&#30053;&#35268;&#21010;&#23558;&#30446;&#26631;&#20219;&#21153;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#23454;&#29616;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#20027;&#35774;&#35745;&#19982;&#19987;&#23478;&#25163;&#24037;&#25171;&#36896;&#30340;&#25552;&#31034;&#30456;&#24403;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16427</link><description>&lt;p&gt;
PromptAgent: &#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#25112;&#30053;&#35268;&#21010;&#23454;&#29616;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization. (arXiv:2310.16427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16427
&lt;/p&gt;
&lt;p&gt;
PromptAgent&#26159;&#19968;&#20010;&#36890;&#36807;&#25112;&#30053;&#35268;&#21010;&#23558;&#30446;&#26631;&#20219;&#21153;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#23454;&#29616;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#20027;&#35774;&#35745;&#19982;&#19987;&#23478;&#25163;&#24037;&#25171;&#36896;&#30340;&#25552;&#31034;&#30456;&#24403;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#36890;&#24120;&#38656;&#35201;&#32463;&#36807;&#19987;&#23478;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#38656;&#35201;&#32467;&#21512;&#35814;&#32454;&#30340;&#25351;&#23548;&#21644;&#39046;&#22495;&#35265;&#35299;&#65292;&#22522;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30446;&#26631;&#20219;&#21153;&#32454;&#33410;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#29983;&#25104;&#19987;&#23478;&#32423;&#25552;&#31034;&#30340;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#29616;&#26377;&#30340;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#39046;&#22495;&#30693;&#35782;&#30340;&#28145;&#24230;&#65292;&#24182;&#19988;&#38590;&#20197;&#39640;&#25928;&#22320;&#25506;&#32034;&#19987;&#23478;&#32423;&#25552;&#31034;&#30340;&#24191;&#38420;&#31354;&#38388;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptAgent&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#20027;&#35774;&#35745;&#19982;&#19987;&#23478;&#25163;&#24037;&#25171;&#36896;&#30340;&#25552;&#31034;&#30456;&#24403;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;PromptAgent&#23558;&#25552;&#31034;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#25112;&#30053;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#26681;&#25454;Monte Carlo&#26641;&#25628;&#32034;&#30340;&#35268;&#21017;&#35745;&#31639;&#31639;&#27861;&#22312;&#19987;&#23478;&#32423;&#25552;&#31034;&#31354;&#38388;&#20013;&#36827;&#34892;&#26377;&#25928;&#23548;&#33322;&#12290;PromptAgent&#36890;&#36807;&#31867;&#20154;&#31867;&#30340;&#21453;&#22797;&#35797;&#38169;&#25506;&#32034;&#65292;&#24341;&#21457;&#31934;&#30830;&#30340;&#19987;&#23478;&#32423;&#35265;&#35299;&#21644;&#28145;&#20837;&#30340;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Highly effective, task-specific prompts are often heavily engineered by experts to integrate detailed instructions and domain insights based on a deep understanding of both instincts of large language models (LLMs) and the intricacies of the target task. However, automating the generation of such expert-level prompts remains elusive. Existing prompt optimization methods tend to overlook the depth of domain knowledge and struggle to efficiently explore the vast space of expert-level prompts. Addressing this, we present PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to strategically navigate the expert-level prompt space. Inspired by human-like trial-and-error exploration, PromptAgent induces precise expert-level insights and in-depth instructions by r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35789;&#32423;&#31574;&#30053;&#25552;&#21319;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35789;&#32423;&#31574;&#30053;&#20248;&#20110;&#23376;&#35789;&#32423;&#31574;&#30053;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16417</link><description>&lt;p&gt;
&#36890;&#36807;&#35789;&#32423;&#31574;&#30053;&#25552;&#21319;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Enhanced Simultaneous Machine Translation with Word-level Policies. (arXiv:2310.16417v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35789;&#32423;&#31574;&#30053;&#25552;&#21319;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35789;&#32423;&#31574;&#30053;&#20248;&#20110;&#23376;&#35789;&#32423;&#31574;&#30053;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22312;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#65288;SiMT&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#24341;&#20837;&#20102;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#20915;&#23450;&#20102;&#22312;&#32763;&#35793;&#36807;&#31243;&#30340;&#27599;&#19968;&#27493;&#20013;&#26159;&#35835;&#21462;&#36824;&#26159;&#20889;&#20837;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#24120;&#35265;&#20551;&#35774;&#26159;&#65292;&#25805;&#20316;&#26159;&#22312;&#23376;&#35789;&#32423;&#21035;&#36827;&#34892;&#30340;&#65292;&#23613;&#31649;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26631;&#20934;&#21333;&#20301;&#36890;&#24120;&#26159;&#20197;&#35789;&#20026;&#21333;&#20301;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#23376;&#35789;&#32423;&#21035;&#21046;&#23450;&#21644;&#39564;&#35777;&#30340;&#31574;&#30053;&#34987;&#22312;&#20197;&#35789;&#20026;&#21333;&#20301;&#36827;&#34892;&#25805;&#20316;&#30340;&#31574;&#30053;&#36229;&#36234;&#65292;&#24182;&#19988;&#36825;&#20123;&#31574;&#30053;&#19968;&#27425;&#22788;&#29702;&#22810;&#20010;&#23376;&#35789;&#20197;&#24418;&#25104;&#19968;&#20010;&#23436;&#25972;&#30340;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#25552;&#21319;SiMT&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25152;&#25552;&#20986;&#30340;&#35789;&#32423;&#31574;&#30053;&#22312;&#35299;&#20915;LMs&#21644;SiMT&#27169;&#22411;&#20043;&#38388;&#30340;&#23376;&#35789;&#24046;&#24322;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/xl8-ai/WordSiMT&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen remarkable advances in the field of Simultaneous Machine Translation (SiMT) due to the introduction of innovative policies that dictate whether to READ or WRITE at each step of the translation process. However, a common assumption in many existing studies is that operations are carried out at the subword level, even though the standard unit for input and output in most practical scenarios is typically at the word level. This paper demonstrates that policies devised and validated at the subword level are surpassed by those operating at the word level, which process multiple subwords to form a complete word in a single step. Additionally, we suggest a method to boost SiMT models using language models (LMs), wherein the proposed word-level policy plays a vital role in addressing the subword disparity between LMs and SiMT models. Code is available at https://github.com/xl8-ai/WordSiMT.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#22312;&#38590;&#39064;&#35299;&#20915;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#26032;&#19968;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#38590;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#20154;&#31867;&#65292;&#20294;&#20154;&#31867;&#22312;&#39564;&#35777;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.16411</link><description>&lt;p&gt;
&#35299;&#23494;&#38590;&#39064;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#38382;&#39064;&#27714;&#35299;&#32773;&#30340;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
Decoding Stumpers: Large Language Models vs. Human Problem-Solvers. (arXiv:2310.16411v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16411
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#22312;&#38590;&#39064;&#35299;&#20915;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#26032;&#19968;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#38590;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#20154;&#31867;&#65292;&#20294;&#20154;&#31867;&#22312;&#39564;&#35777;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38590;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#25506;&#31350;&#20102;&#23427;&#20204;&#30340;&#38382;&#39064;&#27714;&#35299;&#33021;&#21147;&#12290;&#38590;&#39064;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#21333;&#27493;&#30452;&#35273;&#38382;&#39064;&#65292;&#23545;&#20154;&#31867;&#27714;&#35299;&#32773;&#26500;&#25104;&#25361;&#25112;&#65292;&#20294;&#24456;&#23481;&#26131;&#39564;&#35777;&#12290;&#25105;&#20204;&#23558;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;Davinci-2&#12289;Davinci-3&#12289;GPT-3.5-Turbo&#12289;GPT-4&#65289;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#34920;&#29616;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#26032;&#19968;&#20195;LLMs&#22312;&#35299;&#20915;&#38590;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#22312;&#39564;&#35777;&#30456;&#21516;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#25216;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#22686;&#36827;&#20102;&#25105;&#20204;&#23545;LLMs&#35748;&#30693;&#33021;&#21147;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#25552;&#21319;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#38382;&#39064;&#27714;&#35299;&#28508;&#21147;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the problem-solving capabilities of Large Language Models (LLMs) by evaluating their performance on stumpers, unique single-step intuition problems that pose challenges for human solvers but are easily verifiable. We compare the performance of four state-of-the-art LLMs (Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) to human participants. Our findings reveal that the new-generation LLMs excel in solving stumpers and surpass human performance. However, humans exhibit superior skills in verifying solutions to the same problems. This research enhances our understanding of LLMs' cognitive abilities and provides insights for enhancing their problem-solving potential across various domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#26465;&#20214;&#26597;&#35810;&#30340;Transformer&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#39057;&#25351;&#20195;&#34920;&#36798;&#29702;&#35299;&#12290;&#36890;&#36807;&#21019;&#24314;&#21160;&#24577;&#26597;&#35810;&#65292;&#32771;&#34385;&#36755;&#20837;&#35270;&#39057;&#21644;&#35821;&#35328;&#30340;&#24433;&#21709;&#65292;&#26469;&#24314;&#27169;&#22810;&#26679;&#21270;&#30340;&#25351;&#20195;&#23545;&#35937;&#12290;&#21516;&#26102;&#65292;&#23545;&#21477;&#23376;&#20013;&#30340;&#29305;&#23450;&#30701;&#35821;&#36827;&#34892;&#23545;&#40784;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26597;&#35810;&#29305;&#24449;&#24573;&#35270;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16402</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#26465;&#20214;&#26597;&#35810;&#30340;Transformer&#35270;&#39057;&#25351;&#20195;&#34920;&#36798;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Video Referring Expression Comprehension via Transformer with Content-conditioned Query. (arXiv:2310.16402v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#26465;&#20214;&#26597;&#35810;&#30340;Transformer&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#39057;&#25351;&#20195;&#34920;&#36798;&#29702;&#35299;&#12290;&#36890;&#36807;&#21019;&#24314;&#21160;&#24577;&#26597;&#35810;&#65292;&#32771;&#34385;&#36755;&#20837;&#35270;&#39057;&#21644;&#35821;&#35328;&#30340;&#24433;&#21709;&#65292;&#26469;&#24314;&#27169;&#22810;&#26679;&#21270;&#30340;&#25351;&#20195;&#23545;&#35937;&#12290;&#21516;&#26102;&#65292;&#23545;&#21477;&#23376;&#20013;&#30340;&#29305;&#23450;&#30701;&#35821;&#36827;&#34892;&#23545;&#40784;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26597;&#35810;&#29305;&#24449;&#24573;&#35270;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25351;&#20195;&#34920;&#36798;&#29702;&#35299;&#65288;REC&#65289;&#26088;&#22312;&#26681;&#25454;&#26597;&#35810;&#30340;&#33258;&#28982;&#35821;&#35328;&#22312;&#35270;&#39057;&#20013;&#23450;&#20301;&#30446;&#26631;&#23545;&#35937;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#20855;&#26377;&#21487;&#23398;&#20064;&#26597;&#35810;&#30340;Transformer&#26041;&#27861;&#22312;&#35270;&#39057;REC&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#32771;&#34385;&#21040;&#25991;&#26412;&#30417;&#30563;&#24102;&#26469;&#30340;&#35270;&#39057;REC&#30340;&#24320;&#25918;&#19990;&#30028;&#24615;&#36136;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#26597;&#35810;&#35774;&#35745;&#24182;&#19981;&#29702;&#24819;&#12290;&#30001;&#20110;&#23384;&#22312;&#20247;&#22810;&#28508;&#22312;&#30340;&#35821;&#20041;&#31867;&#21035;&#65292;&#20165;&#20381;&#38752;&#19968;&#20123;&#26356;&#26032;&#32531;&#24930;&#30340;&#26597;&#35810;&#26080;&#27861;&#20805;&#20998;&#25551;&#36848;&#23427;&#20204;&#12290;&#25105;&#20204;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#21019;&#24314;&#21160;&#24577;&#26597;&#35810;&#65292;&#36825;&#20123;&#26597;&#35810;&#21463;&#36755;&#20837;&#35270;&#39057;&#21644;&#35821;&#35328;&#30340;&#24433;&#21709;&#65292;&#20197;&#24314;&#27169;&#25152;&#25351;&#30340;&#22810;&#26679;&#21270;&#23545;&#35937;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#24103;&#20013;&#25918;&#32622;&#20102;&#22266;&#23450;&#25968;&#37327;&#30340;&#21487;&#23398;&#20064;&#36793;&#30028;&#26694;&#65292;&#24182;&#20351;&#29992;&#30456;&#24212;&#30340;&#21306;&#22495;&#29305;&#24449;&#25552;&#20379;&#20808;&#39564;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#24403;&#21069;&#30340;&#26597;&#35810;&#29305;&#24449;&#24573;&#35270;&#20102;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#21477;&#23376;&#20013;&#30340;&#29305;&#23450;&#30701;&#35821;&#19982;&#35821;&#20041;&#30456;&#20851;&#30340;&#35270;&#35273;&#21306;&#22495;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Referring Expression Comprehension (REC) aims to localize a target object in videos based on the queried natural language. Recent improvements in video REC have been made using Transformer-based methods with learnable queries. However, we contend that this naive query design is not ideal given the open-world nature of video REC brought by text supervision. With numerous potential semantic categories, relying on only a few slow-updated queries is insufficient to characterize them. Our solution to this problem is to create dynamic queries that are conditioned on both the input video and language to model the diverse objects referred to. Specifically, we place a fixed number of learnable bounding boxes throughout the frame and use corresponding region features to provide prior information. Also, we noticed that current query features overlook the importance of cross-modal alignment. To address this, we align specific phrases in the sentence with semantically relevant visual areas, a
&lt;/p&gt;</description></item><item><title>ZGUL&#26159;&#19968;&#20010;&#20351;&#29992;&#22810;&#28304;&#21512;&#24182;&#30340;&#35821;&#35328;&#36866;&#37197;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#31227;&#38382;&#39064;&#12290;&#22312;15&#20010;&#26410;&#35265;&#30446;&#26631;&#35821;&#35328;&#30340;&#23454;&#39564;&#20013;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;&#24494;&#35843;&#21644;&#21333;&#19968;&#28304;&#36866;&#37197;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;F1&#24471;&#20998;&#25552;&#39640;&#20102;3.2&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.16393</link><description>&lt;p&gt;
ZGUL: &#20351;&#29992;&#22810;&#28304;&#21512;&#24182;&#30340;&#35821;&#35328;&#36866;&#37197;&#22120;&#23454;&#29616;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters. (arXiv:2310.16393v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16393
&lt;/p&gt;
&lt;p&gt;
ZGUL&#26159;&#19968;&#20010;&#20351;&#29992;&#22810;&#28304;&#21512;&#24182;&#30340;&#35821;&#35328;&#36866;&#37197;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#31227;&#38382;&#39064;&#12290;&#22312;15&#20010;&#26410;&#35265;&#30446;&#26631;&#35821;&#35328;&#30340;&#23454;&#39564;&#20013;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;&#24494;&#35843;&#21644;&#21333;&#19968;&#28304;&#36866;&#37197;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;F1&#24471;&#20998;&#25552;&#39640;&#20102;3.2&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#36866;&#37197;&#22120;&#65288;LAs&#65289;&#26469;&#35299;&#20915;NLP&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#31227;&#38382;&#39064;&#12290;&#20043;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#26159;&#25506;&#32034;&#35757;&#32451;&#20351;&#29992;&#21333;&#19968;&#28304;&#65288;&#36890;&#24120;&#26159;&#33521;&#25991;&#65289;&#30340;&#36866;&#37197;&#22120;&#65292;&#28982;&#21518;&#20351;&#29992;&#30446;&#26631;&#36866;&#37197;&#22120;&#25110;&#20854;&#20182;&#30456;&#20851;&#35821;&#35328;&#30340;&#36866;&#37197;&#22120;&#36827;&#34892;&#27979;&#35797;&#12290;&#35757;&#32451;&#30446;&#26631;&#36866;&#37197;&#22120;&#38656;&#35201;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#23545;&#20110;&#20302;&#36164;&#28304;&#26410;&#35265;&#35821;&#35328;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#19981;&#23481;&#26131;&#33719;&#24471;&#65306;&#36825;&#20123;&#35821;&#35328;&#26082;&#19981;&#34987;&#22522;&#30784;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;mBERT&#65289;&#25152;&#35265;&#65292;&#20063;&#27809;&#26377;&#20219;&#20309;&#26631;&#35760;&#25110;&#26080;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#25105;&#20204;&#38656;&#35201;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#21033;&#29992;&#22810;&#20010;&#65288;&#22312;&#35821;&#35328;&#25110;&#22320;&#29702;&#19978;&#30456;&#20851;&#30340;&#65289;&#28304;&#35821;&#35328;&#30340;&#36866;&#37197;&#22120;&#65292;&#25105;&#20204;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;ZGUL&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#28085;&#30422;15&#31181;&#26410;&#35265;&#30446;&#26631;&#35821;&#35328;&#30340;&#22235;&#20010;&#35821;&#35328;&#32452;&#20043;&#38388;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#26631;&#20934;&#24494;&#35843;&#21644;&#21482;&#20351;&#29992;&#19968;&#20010;&#28304;&#36866;&#37197;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;F1&#24471;&#20998;&#25552;&#39640;&#20102;&#39640;&#36798;3.2&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via the use of language adapters (LAs). Most of the earlier works have explored training with adapter of a single source (often English), and testing either using the target LA or LA of another related language. Training target LA requires unlabeled data, which may not be readily available for low resource unseen languages: those that are neither seen by the underlying multilingual language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data for them. We posit that for more effective cross-lingual transfer, instead of just one source LA, we need to leverage LAs of multiple (linguistically or geographically related) source languages, both at train and test-time - which we investigate via our novel neural architecture, ZGUL. Extensive experimentation across four language groups, covering 15 unseen target languages, demonstrates improvements of up to 3.2 average F1 points over standard fine-tuning and o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#20174;&#24494;&#21338;&#24086;&#23376;&#20013;&#29983;&#25104;&#36275;&#29699;&#27604;&#36187;&#30340;&#23454;&#26102;&#26356;&#26032;&#65292;&#36890;&#36807;&#25511;&#21046;&#26356;&#26032;&#25968;&#37327;&#21644;&#20943;&#23569;&#20887;&#20313;&#26356;&#26032;&#65292;&#29992;&#25143;&#21487;&#20197;&#31435;&#21363;&#20102;&#35299;&#27604;&#36187;&#36827;&#23637;&#24182;&#20139;&#21463;&#27604;&#36187;&#30340;&#28608;&#21160;&#12290;</title><link>http://arxiv.org/abs/2310.16368</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#24494;&#21338;&#24086;&#23376;&#29983;&#25104;&#36275;&#29699;&#27604;&#36187;&#30340;&#23454;&#26102;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Live Update Generation for Soccer Matches from Microblog Posts. (arXiv:2310.16368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#20174;&#24494;&#21338;&#24086;&#23376;&#20013;&#29983;&#25104;&#36275;&#29699;&#27604;&#36187;&#30340;&#23454;&#26102;&#26356;&#26032;&#65292;&#36890;&#36807;&#25511;&#21046;&#26356;&#26032;&#25968;&#37327;&#21644;&#20943;&#23569;&#20887;&#20313;&#26356;&#26032;&#65292;&#29992;&#25143;&#21487;&#20197;&#31435;&#21363;&#20102;&#35299;&#27604;&#36187;&#36827;&#23637;&#24182;&#20139;&#21463;&#27604;&#36187;&#30340;&#28608;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20351;&#29992;&#24494;&#21338;&#24086;&#23376;&#35266;&#30475;&#20307;&#32946;&#27604;&#36187;&#30452;&#25773;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#20174;&#24222;&#22823;&#32780;&#22810;&#26679;&#30340;&#24494;&#21338;&#24086;&#23376;&#24207;&#21015;&#20013;&#29983;&#25104;&#36275;&#22815;&#30340;&#20307;&#32946;&#27604;&#36187;&#26356;&#26032;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#36275;&#29699;&#27604;&#36187;&#65292;&#33268;&#21147;&#20110;&#26500;&#24314;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#24494;&#21338;&#24086;&#23376;&#29983;&#25104;&#36275;&#29699;&#27604;&#36187;&#30340;&#23454;&#26102;&#26356;&#26032;&#65292;&#35753;&#29992;&#25143;&#21487;&#20197;&#31435;&#21363;&#20102;&#35299;&#27604;&#36187;&#30340;&#36827;&#23637;&#65292;&#24182;&#20174;&#21407;&#22987;&#24494;&#21338;&#20013;&#20307;&#39564;&#27604;&#36187;&#30340;&#28608;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#22522;&#20110;&#19968;&#20010;&#22823;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#34701;&#20837;&#20102;&#25511;&#21046;&#26356;&#26032;&#25968;&#37327;&#30340;&#26426;&#21046;&#20197;&#21450;&#20943;&#23569;&#37325;&#22797;&#21644;&#30456;&#20284;&#26356;&#26032;&#20887;&#20313;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been known to be difficult to generate adequate sports updates from a sequence of vast amounts of diverse live tweets, although the live sports viewing experience with tweets is gaining the popularity. In this paper, we focus on soccer matches and work on building a system to generate live updates for soccer matches from tweets so that users can instantly grasp a match's progress and enjoy the excitement of the match from raw tweets. Our proposed system is based on a large pre-trained language model and incorporates a mechanism to control the number of updates and a mechanism to reduce the redundancy of duplicate and similar updates.
&lt;/p&gt;</description></item><item><title>InstructPTS&#26159;&#19968;&#31181;&#29992;&#20110;&#20135;&#21697;&#26631;&#39064;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#33410;LLMs&#21487;&#26681;&#25454;&#22810;&#31181;&#26631;&#20934;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#25552;&#39640;&#20102;&#20135;&#21697;&#21517;&#31216;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.16361</link><description>&lt;p&gt;
InstructPTS: &#29992;&#20110;&#20135;&#21697;&#26631;&#39064;&#25688;&#35201;&#30340;&#25351;&#23548;&#35843;&#33410;LLMs&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InstructPTS: Instruction-Tuning LLMs for Product Title Summarization. (arXiv:2310.16361v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16361
&lt;/p&gt;
&lt;p&gt;
InstructPTS&#26159;&#19968;&#31181;&#29992;&#20110;&#20135;&#21697;&#26631;&#39064;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#33410;LLMs&#21487;&#26681;&#25454;&#22810;&#31181;&#26631;&#20934;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#25552;&#39640;&#20102;&#20135;&#21697;&#21517;&#31216;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#30446;&#24405;&#20013;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#21830;&#21697;&#12290;&#22823;&#22810;&#25968;&#20135;&#21697;&#26631;&#39064;&#24456;&#38271;&#65292;&#21334;&#23478;&#20351;&#29992;&#20135;&#21697;&#23646;&#24615;&#26469;&#25913;&#36827;&#21830;&#21697;&#26816;&#32034;&#24182;&#31361;&#20986;&#20851;&#38190;&#20135;&#21697;&#26041;&#38754;&#12290;&#36825;&#23548;&#33268;&#20102;&#19981;&#33258;&#28982;&#30340;&#20135;&#21697;&#26631;&#39064;&#19982;&#23458;&#25143;&#23545;&#20854;&#30340;&#31216;&#21628;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#20063;&#38480;&#21046;&#20102;&#30005;&#23376;&#21830;&#21153;&#21830;&#24215;&#20351;&#29992;&#36825;&#20123;&#21334;&#23478;&#25552;&#20379;&#30340;&#26631;&#39064;&#36827;&#34892;&#25512;&#33616;&#12289;&#38382;&#31572;&#25110;&#35780;&#35770;&#25688;&#35201;&#30340;&#33021;&#21147;&#12290;&#21463;&#26368;&#36817;&#26377;&#20851;&#25351;&#23548;&#35843;&#33410;LLMs&#30340;&#30740;&#31350;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructPTS&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20135;&#21697;&#26631;&#39064;&#25688;&#35201;&#20219;&#21153;&#30340;&#21487;&#25511;&#26041;&#27861;&#12290;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25351;&#23548;&#24494;&#35843;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#21508;&#31181;&#26631;&#20934;&#65288;&#20363;&#22914;&#25688;&#35201;&#20013;&#30340;&#21333;&#35789;&#25968;&#37327;&#12289;&#21253;&#21547;&#29305;&#23450;&#30701;&#35821;&#31561;&#65289;&#24635;&#32467;&#20135;&#21697;&#26631;&#39064;&#12290;&#23545;&#23454;&#38469;&#30005;&#23376;&#21830;&#21153;&#30446;&#24405;&#36827;&#34892;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#31616;&#21333;&#24494;&#35843;LLMs&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#20135;&#21697;&#21517;&#31216;&#25688;&#35201;&#65292;BLEU&#21644;ROUG&#25552;&#39640;&#20102;&#36229;&#36807;14&#21644;8&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce product catalogs contain billions of items. Most products have lengthy titles, as sellers pack them with product attributes to improve retrieval, and highlight key product aspects. This results in a gap between such unnatural products titles, and how customers refer to them. It also limits how e-commerce stores can use these seller-provided titles for recommendation, QA, or review summarization.  Inspired by recent work on instruction-tuned LLMs, we present InstructPTS, a controllable approach for the task of Product Title Summarization (PTS). Trained using a novel instruction fine-tuning strategy, our approach is able to summarize product titles according to various criteria (e.g. number of words in a summary, inclusion of specific phrases, etc.). Extensive evaluation on a real-world e-commerce catalog shows that compared to simple fine-tuning of LLMs, our proposed approach can generate more accurate product name summaries, with an improvement of over 14 and 8 BLEU and ROUG
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#36880;&#27493;&#26694;&#26550;&#29992;&#20110;&#25991;&#26723;&#32423;&#20449;&#24687;&#24615;&#35770;&#35777;&#25552;&#21462;&#65292;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#20107;&#20214;&#30340;&#38590;&#24230;&#24182;&#25353;&#29031;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#39034;&#24207;&#36827;&#34892;&#25552;&#21462;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#35760;&#24518;&#65292;&#36991;&#20813;&#22312;&#20381;&#36182;&#20808;&#21069;&#20107;&#20214;&#30340;&#19981;&#27491;&#30830;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#24341;&#20837;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2310.16358</link><description>&lt;p&gt;
&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#65306;&#19968;&#31181;&#36880;&#27493;&#26694;&#26550;&#29992;&#20110;&#25991;&#26723;&#32423;&#20449;&#24687;&#24615;&#35770;&#35777;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
From Simple to Complex: A Progressive Framework for Document-level Informative Argument Extraction. (arXiv:2310.16358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#36880;&#27493;&#26694;&#26550;&#29992;&#20110;&#25991;&#26723;&#32423;&#20449;&#24687;&#24615;&#35770;&#35777;&#25552;&#21462;&#65292;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#20107;&#20214;&#30340;&#38590;&#24230;&#24182;&#25353;&#29031;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#39034;&#24207;&#36827;&#34892;&#25552;&#21462;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#35760;&#24518;&#65292;&#36991;&#20813;&#22312;&#20381;&#36182;&#20808;&#21069;&#20107;&#20214;&#30340;&#19981;&#27491;&#30830;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#24341;&#20837;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65288;EAE&#65289;&#35201;&#27714;&#27169;&#22411;&#20174;&#21333;&#20010;&#25991;&#26723;&#20013;&#25552;&#21462;&#22810;&#20010;&#20107;&#20214;&#30340;&#35770;&#35777;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#26368;&#36817;&#30340;&#21162;&#21147;&#21033;&#29992;&#20102;"&#35760;&#24518;"&#30340;&#24819;&#27861;&#65292;&#24050;&#32463;&#39044;&#27979;&#30340;&#20107;&#20214;&#30340;&#32467;&#26524;&#34987;&#32531;&#23384;&#65292;&#21487;&#20197;&#29992;&#26469;&#24110;&#21161;&#39044;&#27979;&#21363;&#23558;&#21457;&#29983;&#30340;&#20107;&#20214;&#12290;&#36825;&#20123;&#26041;&#27861;&#26681;&#25454;&#20107;&#20214;&#22312;&#25991;&#26723;&#20013;&#30340;&#20986;&#29616;&#39034;&#24207;&#36827;&#34892;&#25552;&#21462;&#65292;&#28982;&#32780;&#65292;&#20986;&#29616;&#22312;&#31532;&#19968;&#21477;&#20013;&#30340;&#20107;&#20214;&#24182;&#19981;&#24847;&#21619;&#30528;&#23427;&#26159;&#26368;&#23481;&#26131;&#25552;&#21462;&#30340;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#22312;&#20381;&#36182;&#20110;&#20808;&#21069;&#20107;&#20214;&#30340;&#19981;&#27491;&#30830;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#24341;&#20837;&#22122;&#22768;&#21040;&#21363;&#23558;&#21457;&#29983;&#30340;&#20107;&#20214;&#30340;&#25552;&#21462;&#20013;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#35760;&#24518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#36880;&#27493;&#26694;&#26550;&#29992;&#20110;&#25991;&#26723;&#32423;EAE&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35745;&#31639;&#27599;&#20010;&#20107;&#20214;&#30340;&#38590;&#24230;&#65292;&#28982;&#21518;&#25353;&#29031;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#39034;&#24207;&#36827;&#34892;&#25552;&#21462;&#12290;&#36825;&#26679;&#65292;&#35760;&#24518;&#23558;&#23384;&#20648;&#26368;&#30830;&#23450;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20351;&#24471;&#21363;&#23558;&#21457;&#29983;&#30340;&#20107;&#20214;&#30340;&#25552;&#21462;&#26356;&#21152;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level Event Argument Extraction (EAE) requires the model to extract arguments of multiple events from a single document. Considering the underlying dependencies between these events, recent efforts leverage the idea of "memory", where the results of already predicted events are cached and can be retrieved to help the prediction of upcoming events. These methods extract events according to their appearance order in the document, however, the event that appears in the first sentence does not mean that it is the easiest to extract. Existing methods might introduce noise to the extraction of upcoming events if they rely on an incorrect prediction of previous events. In order to provide more reliable memory, we propose a simple-to-complex progressive framework for document-level EAE. Specifically, we first calculate the difficulty of each event and then, we conduct the extraction following a simple-to-complex order. In this way, the memory will store the most certain results, and t
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;(WIKI-DOC&#21644;MULTIEURLEX-DOC)&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#24182;&#25506;&#32034;&#20102;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#29702;&#35299;&#21644;&#25991;&#26723;AI&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#21644;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#25913;&#36827;&#25991;&#26723;AI&#27169;&#22411;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2310.16356</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#25991;&#26723;&#22270;&#20687;&#20998;&#31867;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Multi-Modal Multilingual Benchmark for Document Image Classification. (arXiv:2310.16356v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16356
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;(WIKI-DOC&#21644;MULTIEURLEX-DOC)&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#24182;&#25506;&#32034;&#20102;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#29702;&#35299;&#21644;&#25991;&#26723;AI&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#21644;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#25913;&#36827;&#25991;&#26723;AI&#27169;&#22411;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#22270;&#20687;&#20998;&#31867;&#19982;&#32431;&#25991;&#26412;&#25991;&#26723;&#20998;&#31867;&#19981;&#21516;&#65292;&#23427;&#28041;&#21450;&#23545;&#34920;&#21333;&#12289;&#30005;&#23376;&#37038;&#20214;&#31561;&#25991;&#26723;&#30340;&#20869;&#23481;&#21644;&#32467;&#26500;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#21807;&#19968;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#8212;&#8212;WIKI-DOC&#21644;MULTIEURLEX-DOC&#65292;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20197;&#21069;&#26410;&#32463;&#27979;&#35797;&#30340;&#35774;&#32622;&#19979;&#23545;&#24120;&#35265;&#30340;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#29702;&#35299;&#25110;&#25991;&#26723;AI&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;&#22914;1&#65289;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#21644;2&#65289;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#35774;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#36317;&#31163;&#36739;&#36828;&#30340;&#35821;&#35328;&#19978;&#22810;&#35821;&#35328;&#25991;&#26723;AI&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#21457;&#29616;&#20026;&#26410;&#26469;&#25913;&#36827;&#25991;&#26723;AI&#27169;&#22411;&#30340;&#30740;&#31350;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document image classification is different from plain-text document classification and consists of classifying a document by understanding the content and structure of documents such as forms, emails, and other such documents. We show that the only existing dataset for this task (Lewis et al., 2006) has several limitations and we introduce two newly curated multilingual datasets WIKI-DOC and MULTIEURLEX-DOC that overcome these limitations. We further undertake a comprehensive study of popular visually-rich document understanding or Document AI models in previously untested setting in document image classification such as 1) multi-label classification, and 2) zero-shot cross-lingual transfer setup. Experimental results show limitations of multilingual Document AI models on cross-lingual transfer across typologically distant languages. Our datasets and findings open the door for future research into improving Document AI models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#26426;&#21046;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#20013;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#32479;&#35745;&#29305;&#24449;&#65292;&#24182;&#25581;&#31034;&#20102;&#28608;&#27963;&#20989;&#25968;&#36873;&#25321;&#23545;&#29305;&#24449;&#25552;&#21462;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.16350</link><description>&lt;p&gt;
&#25581;&#31034;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Unraveling Feature Extraction Mechanisms in Neural Networks. (arXiv:2310.16350v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#26426;&#21046;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#20013;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#32479;&#35745;&#29305;&#24449;&#65292;&#24182;&#25581;&#31034;&#20102;&#28608;&#27963;&#20989;&#25968;&#36873;&#25321;&#23545;&#29305;&#24449;&#25552;&#21462;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#25429;&#25417;&#31934;&#30830;&#30693;&#35782;&#26041;&#38754;&#30340;&#22522;&#26412;&#26426;&#21046;&#19968;&#30452;&#26159;&#25345;&#32493;&#30740;&#31350;&#30340;&#23545;&#35937;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32771;&#34385;&#26080;&#38480;&#32593;&#32476;&#23485;&#24230;&#65292;&#25105;&#20204;&#29468;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#21487;&#33021;&#30452;&#35266;&#22320;&#25581;&#31034;&#20854;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#21462;&#30340;&#29305;&#24449;&#65292;&#36827;&#19968;&#27493;&#21152;&#28145;&#25105;&#20204;&#23545;&#20854;&#20869;&#37096;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#20013;&#22914;&#20309;&#21033;&#29992;&#32479;&#35745;&#29305;&#24449;&#12289;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#34701;&#20837;&#26368;&#32456;&#30340;&#20915;&#31574;&#20013;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#20250;&#24433;&#21709;&#29305;&#24449;&#25552;&#21462;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#21487;&#33021;&#20250;&#24341;&#20837;&#29305;&#24449;&#20559;&#24046;&#65292;&#36825;&#21487;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20102;&#26367;&#20195;&#20989;&#25968;&#26469;&#20195;&#26367;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
The underlying mechanism of neural networks in capturing precise knowledge has been the subject of consistent research efforts. In this work, we propose a theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such mechanisms. Specifically, considering the infinite network width, we hypothesize the learning dynamics of target models may intuitively unravel the features they acquire from training data, deepening our insights into their internal mechanisms. We apply our approach to several fundamental models and reveal how these models leverage statistical features during gradient descent and how they are integrated into final decisions. We also discovered that the choice of activation function can affect feature extraction. For instance, the use of the \textit{ReLU} activation function could potentially introduce a bias in features, providing a plausible explanation for its replacement with alternative functions in recent pre-trained language models. Additionally, we
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#20102;LLM&#30340;&#33021;&#21147;&#21644;&#19981;&#36275;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.16343</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Constrained Text Generation for Large Language Models. (arXiv:2310.16343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16343
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#20102;LLM&#30340;&#33021;&#21147;&#21644;&#19981;&#36275;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104; (NLG) &#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36827;&#27493;&#20351;&#24471;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33021;&#22815;&#29983;&#25104;&#29087;&#32451;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#23558;&#22797;&#26434;&#30340;&#32422;&#26463;&#38598;&#25104;&#21040;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;LLM&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#65292;&#20854;&#20013;&#22312;LLM&#30340;&#29983;&#25104;&#36807;&#31243;&#20013;&#24212;&#29992;&#20102;&#39044;&#23450;&#20041;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32771;&#23519;&#20102;&#22810;&#20010;LLM&#65292;&#21253;&#25324;ChatGPT&#21644;GPT-4&#65292;&#24182;&#23558;&#32422;&#26463;&#20998;&#20026;&#35789;&#27719;&#12289;&#32467;&#26500;&#21644;&#20851;&#31995;&#31867;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20934;&#26469;&#20419;&#36827;&#20844;&#24179;&#35780;&#20272;&#12290;&#30740;&#31350;&#35299;&#20915;&#20102;&#19968;&#20123;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65292;&#21253;&#25324;LLM&#19982;&#32422;&#26463;&#30340;&#36981;&#23432;&#31243;&#24230;&#12290;&#32467;&#26524;&#25581;&#31034;&#20102;LLM&#38598;&#25104;&#32422;&#26463;&#30340;&#33021;&#21147;&#21644;&#19981;&#36275;&#65292;&#24182;&#20026;&#26410;&#26469;&#21457;&#23637;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#23558;&#22312;&#34987;&#25509;&#21463;&#21518;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in natural language generation (NLG) and large language models (LLMs) have led to proficient text generation in various tasks. However, integrating intricate constraints into neural text generation, due to LLMs' opacity, remains challenging. This study investigates constrained text generation for LLMs, where predefined constraints are applied during LLM's generation process. Our research examines multiple LLMs, including ChatGPT and GPT-4, categorizing constraints into lexical, structural, and relation-based types. We also present various benchmarks to facilitate fair evaluation. The study addresses some key research questions, including the extent of LLMs' compliance with constraints. Results illuminate LLMs' capacity and deficiency to incorporate constraints and provide insights for future developments in constrained text generation. Codes and datasets will be released upon acceptance.
&lt;/p&gt;</description></item><item><title>RCAgent&#26159;&#19968;&#20010;&#24037;&#20855;&#22686;&#24378;&#30340;LLM&#33258;&#20027;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20113;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#33021;&#22815;&#23454;&#29616;&#33258;&#30001;&#26684;&#24335;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#24182;&#22312;&#21508;&#20010;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16340</link><description>&lt;p&gt;
RCAgent&#65306;&#22522;&#20110;&#33258;&#20027;&#20195;&#29702;&#21644;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20113;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models. (arXiv:2310.16340v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16340
&lt;/p&gt;
&lt;p&gt;
RCAgent&#26159;&#19968;&#20010;&#24037;&#20855;&#22686;&#24378;&#30340;LLM&#33258;&#20027;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20113;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#33021;&#22815;&#23454;&#29616;&#33258;&#30001;&#26684;&#24335;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#24182;&#22312;&#21508;&#20010;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20113;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#21463;&#21040;&#20102;&#31215;&#26497;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#25163;&#21160;&#24037;&#20316;&#27969;&#35774;&#32622;&#65292;&#24182;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;LLMs&#30340;&#20915;&#31574;&#21644;&#29615;&#22659;&#20132;&#20114;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RCAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#29992;&#21644;&#27880;&#37325;&#38544;&#31169;&#30340;&#24037;&#20855;&#22686;&#24378;LLM&#33258;&#20027;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#38469;&#30340;&#24037;&#19994;RCA&#20351;&#29992;&#12290;RCAgent&#22312;&#20869;&#37096;&#37096;&#32626;&#30340;&#27169;&#22411;&#19978;&#36816;&#34892;&#65292;&#32780;&#19981;&#26159;GPT&#31995;&#21015;&#65292;&#33021;&#22815;&#36827;&#34892;&#33258;&#30001;&#26684;&#24335;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#24182;&#32467;&#21512;&#21508;&#31181;&#22686;&#24378;&#21151;&#33021;&#65292;&#21253;&#25324;&#29420;&#29305;&#30340;&#34892;&#21160;&#36712;&#36857;&#33258;&#19968;&#33268;&#24615;&#21644;&#19968;&#22871;&#29992;&#20110;&#19978;&#19979;&#25991;&#31649;&#29702;&#12289;&#31283;&#23450;&#21270;&#21644;&#23548;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;RCAgent&#22312;RCA&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#39044;&#27979;&#26681;&#26412;&#21407;&#22240;&#12289;&#35299;&#20915;&#26041;&#26696;&#12289;&#35777;&#25454;&#21644;&#36131;&#20219;&#65289;&#20197;&#21450;&#24403;&#21069;&#35268;&#21017;&#26410;&#28085;&#30422;&#30340;&#20219;&#21153;&#19978;&#37117;&#26126;&#26174;&#20248;&#20110;ReAct&#65292;&#24471;&#21040;&#20102;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#39564;&#35777;&#30340;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM) applications in cloud root cause analysis (RCA) have been actively explored recently. However, current methods are still reliant on manual workflow settings and do not unleash LLMs' decision-making and environment interaction capabilities. We present RCAgent, a tool-augmented LLM autonomous agent framework for practical and privacy-aware industrial RCA usage. Running on an internally deployed model rather than GPT families, RCAgent is capable of free-form data collection and comprehensive analysis with tools. Our framework combines a variety of enhancements, including a unique Self-Consistency for action trajectories, and a suite of methods for context management, stabilization, and importing domain knowledge. Our experiments show RCAgent's evident and consistent superiority over ReAct across all aspects of RCA -- predicting root causes, solutions, evidence, and responsibilities -- and tasks covered or uncovered by current rules, as validated by both automate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#20351;&#29992;&#27969;&#21305;&#37197;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#24182;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;60k&#23567;&#26102;&#30340;&#26410;&#36716;&#24405;&#35821;&#38899;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#19987;&#23478;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#12289;&#20998;&#31163;&#21644;&#21512;&#25104;&#26041;&#38754;&#36827;&#34892;&#21305;&#37197;&#25110;&#36229;&#36234;&#12290;</title><link>http://arxiv.org/abs/2310.16338</link><description>&lt;p&gt;
&#24102;&#26377;&#27969;&#21305;&#37197;&#30340;&#35821;&#38899;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-training for Speech with Flow Matching. (arXiv:2310.16338v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#20351;&#29992;&#27969;&#21305;&#37197;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#24182;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;60k&#23567;&#26102;&#30340;&#26410;&#36716;&#24405;&#35821;&#38899;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#19987;&#23478;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#12289;&#20998;&#31163;&#21644;&#21512;&#25104;&#26041;&#38754;&#36827;&#34892;&#21305;&#37197;&#25110;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#38656;&#35201;&#20272;&#35745;&#21644;&#25277;&#26679;&#25968;&#25454;&#20998;&#24067;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#21512;&#25104;&#25968;&#25454;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#35821;&#38899;&#39046;&#22495;&#65292;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21644;&#31070;&#32463;&#22768;&#30721;&#22120;&#26159;&#29983;&#25104;&#27169;&#22411;&#25104;&#21151;&#24212;&#29992;&#30340;&#20856;&#22411;&#20363;&#23376;&#12290;&#23613;&#31649;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#35821;&#38899;&#30340;&#19981;&#21516;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#20294;&#36824;&#27809;&#26377;&#19968;&#20010;&#36890;&#29992;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24314;&#27169;&#35821;&#38899;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#21333;&#19968;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#24182;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36808;&#20986;&#20102;&#36825;&#20010;&#26041;&#21521;&#30340;&#19968;&#27493;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#27969;&#21305;&#37197;&#21644;&#33945;&#29256;&#26465;&#20214;&#22312;60k&#23567;&#26102;&#30340;&#26410;&#36716;&#24405;&#35821;&#38899;&#19978;&#39044;&#35757;&#32451;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechFlow&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22312;&#35821;&#38899;&#22686;&#24378;&#12289;&#20998;&#31163;&#21644;&#21512;&#25104;&#26041;&#38754;&#36798;&#21040;&#25110;&#36229;&#36807;&#29616;&#26377;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples where generative models have shined. While generative models have been applied to different applications in speech, there exists no general-purpose generative model that models speech directly. In this work, we take a step toward this direction by showing a single pre-trained generative model can be adapted to different downstream tasks with strong performance. Specifically, we pre-trained a generative model, named SpeechFlow, on 60k hours of untranscribed speech with Flow Matching and masked conditions. Experiment results show the pre-trained generative model can be fine-tuned with task-specific data to match or surpass existing expert models on speech enhancement, separation, and synthesis. Our work suggested 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoheSentia&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#30340;&#20154;&#31867;&#24863;&#30693;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#30340;&#27880;&#37322;&#21327;&#35758;&#21253;&#25324;&#25972;&#20307;&#35780;&#20998;&#21644;&#36880;&#21477;&#35780;&#20998;&#20004;&#20010;&#35282;&#24230;&#12290;&#36890;&#36807;&#27492;&#22522;&#20934;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#36830;&#36143;&#24615;&#24182;&#20998;&#26512;&#20854;&#30456;&#20851;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.16329</link><description>&lt;p&gt;
CoheSentia: &#19968;&#20010;&#23545;&#29983;&#25104;&#25991;&#26412;&#36830;&#36143;&#24615;&#36827;&#34892;&#22686;&#37327;&#19982;&#25972;&#20307;&#35780;&#20272;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#65288;arXiv:2310.16329v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts. (arXiv:2310.16329v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoheSentia&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#30340;&#20154;&#31867;&#24863;&#30693;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#30340;&#27880;&#37322;&#21327;&#35758;&#21253;&#25324;&#25972;&#20307;&#35780;&#20998;&#21644;&#36880;&#21477;&#35780;&#20998;&#20004;&#20010;&#35282;&#24230;&#12290;&#36890;&#36807;&#27492;&#22522;&#20934;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#36830;&#36143;&#24615;&#24182;&#20998;&#26512;&#20854;&#30456;&#20851;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#36143;&#24615;&#26159;&#19968;&#20010;&#35821;&#35328;&#23398;&#26415;&#35821;&#65292;&#25351;&#30340;&#26159;&#25991;&#26412;&#21333;&#20803;&#65288;&#21477;&#23376;&#12289;&#21629;&#39064;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20351;&#25991;&#26412;&#22312;&#36923;&#36753;&#19978;&#19968;&#33268;&#24182;&#23545;&#35835;&#32773;&#26377;&#24847;&#20041;&#12290;&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#36843;&#20999;&#38656;&#35201;&#33258;&#21160;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#30340;&#20154;&#31867;&#24863;&#30693;&#36830;&#36143;&#24615;&#12290;&#30446;&#21069;&#20026;&#27490;&#65292;&#20851;&#20110;&#26126;&#30830;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#36830;&#36143;&#24615;&#24182;&#20998;&#26512;&#65288;&#19981;&#65289;&#36830;&#36143;&#24615;&#22240;&#32032;&#30340;&#24037;&#20316;&#24456;&#23569;&#12290;&#20197;&#24448;&#20851;&#20110;&#35813;&#20027;&#39064;&#30340;&#30740;&#31350;&#20351;&#29992;&#20854;&#20182;&#20219;&#21153;&#65288;&#22914;&#21477;&#23376;&#37325;&#25490;&#65289;&#20316;&#20026;&#36830;&#36143;&#24615;&#30340;&#26367;&#20195;&#25351;&#26631;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#36827;&#34892;&#36830;&#36143;&#24615;&#26816;&#27979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CoheSentia&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#30340;&#20154;&#31867;&#24863;&#30693;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#30340;&#27880;&#37322;&#21327;&#35758;&#21453;&#26144;&#20102;&#20004;&#20010;&#35282;&#24230;&#65306;&#19968;&#20010;&#26159;&#20840;&#23616;&#35282;&#24230;&#65292;&#32473;&#20986;&#19968;&#20010;&#24635;&#20307;&#36830;&#36143;&#24615;&#20998;&#25968;&#65307;&#21478;&#19968;&#20010;&#26159;&#36880;&#21477;&#35780;&#20998;&#30340;&#22686;&#37327;&#35282;&#24230;&#12290;&#22686;&#37327;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#20010;&#65288;&#19981;&#65289;&#36830;&#36143;&#24615;&#35780;&#20272;&#20197;&#21450;&#30456;&#24212;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coherence is a linguistic term that refers to the relations between small textual units (sentences, propositions), which make the text logically consistent and meaningful to the reader. With the advances of generative foundational models in NLP, there is a pressing need to automatically assess the human-perceived coherence of automatically generated texts. Up until now, little work has been done on explicitly assessing the coherence of generated texts and analyzing the factors contributing to (in)coherence. Previous work on the topic used other tasks, e.g., sentence reordering, as proxies of coherence, rather than approaching coherence detection heads on. In this paper, we introduce {\sc CoheSentia}, a novel benchmark of human-perceived coherence of automatically generated texts. Our annotation protocol reflects two perspectives; one is global, assigning a single coherence score, and the other is incremental, scoring sentence by sentence. The incremental method produces an (in)coherenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19977;&#26143;&#33778;&#24459;&#23486;&#30740;&#31350;&#19982;&#24320;&#21457;&#20013;&#24515;&#22312;WMT 2023&#20013;&#25552;&#20986;&#30340;&#32422;&#26463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#22312;&#32508;&#21512;&#25968;&#25454;&#22788;&#29702;&#12289;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#21644;&#22312;&#32447;&#35299;&#30721;&#20013;&#20351;&#29992;&#22024;&#26434;&#36890;&#36947;&#37325;&#25490;&#30340;&#36807;&#31243;&#20013;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#34920;&#29616;&#20986;&#19982;&#24378;&#22522;&#32447;&#26080;&#32422;&#26463;&#31995;&#32479;&#30456;&#24403;&#29978;&#33267;&#36229;&#36807;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16322</link><description>&lt;p&gt;
&#19977;&#26143;&#33778;&#24459;&#23486;&#30740;&#31350;&#19982;&#24320;&#21457;&#20013;&#24515;&#22312;WMT 2023&#19978;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Samsung R&amp;D Institute Philippines at WMT 2023. (arXiv:2310.16322v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19977;&#26143;&#33778;&#24459;&#23486;&#30740;&#31350;&#19982;&#24320;&#21457;&#20013;&#24515;&#22312;WMT 2023&#20013;&#25552;&#20986;&#30340;&#32422;&#26463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#22312;&#32508;&#21512;&#25968;&#25454;&#22788;&#29702;&#12289;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#21644;&#22312;&#32447;&#35299;&#30721;&#20013;&#20351;&#29992;&#22024;&#26434;&#36890;&#36947;&#37325;&#25490;&#30340;&#36807;&#31243;&#20013;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#34920;&#29616;&#20986;&#19982;&#24378;&#22522;&#32447;&#26080;&#32422;&#26463;&#31995;&#32479;&#30456;&#24403;&#29978;&#33267;&#36229;&#36807;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19977;&#26143;&#33778;&#24459;&#23486;&#30740;&#31350;&#19982;&#24320;&#21457;&#20013;&#24515;&#22312;WMT 2023&#30340;&#19968;&#33324;&#32763;&#35793;&#20219;&#21153;&#20013;&#25552;&#20132;&#30340;&#32422;&#26463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26159;&#22522;&#20110;Transformer&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#32463;&#36807;&#32508;&#21512;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#27969;&#31243;&#12289;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#22312;&#32447;&#35299;&#30721;&#36807;&#31243;&#20013;&#20351;&#29992;&#22024;&#26434;&#36890;&#36947;&#37325;&#25490;&#12290;&#23613;&#31649;&#21442;&#25968;&#36739;&#23569;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;FLORES-200&#21644;NTREX-128&#20004;&#20010;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#24378;&#22522;&#32447;&#26080;&#32422;&#26463;&#31995;&#32479;&#65288;&#22914;mBART50 M2M&#21644;NLLB 200 MoE&#65289;&#30456;&#24403;&#29978;&#33267;&#26377;&#26102;&#36229;&#36807;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe the constrained MT systems submitted by Samsung R&amp;D Institute Philippines to the WMT 2023 General Translation Task for two directions: en$\rightarrow$he and he$\rightarrow$en. Our systems comprise of Transformer-based sequence-to-sequence models that are trained with a mix of best practices: comprehensive data preprocessing pipelines, synthetic backtranslated data, and the use of noisy channel reranking during online decoding. Our models perform comparably to, and sometimes outperform, strong baseline unconstrained systems such as mBART50 M2M and NLLB 200 MoE despite having significantly fewer parameters on two public benchmarks: FLORES-200 and NTREX-128.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23545;&#35805;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#38598;&#65288;DiQAD&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#24320;&#25918;&#22495;&#23545;&#35805;&#36136;&#37327;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#31471;&#21040;&#31471;&#21644;&#20154;&#31867;&#35748;&#30693;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#32422;100,000&#20010;&#23545;&#35805;&#12290;&#23545;&#35805;&#36136;&#37327;&#30340;&#35780;&#20272;&#26631;&#20934;&#26159;&#26681;&#25454;&#31526;&#21512;&#20154;&#31867;&#23545;&#35805;&#36136;&#37327;&#21028;&#26029;&#30340;&#32500;&#24230;&#24314;&#31435;&#30340;&#12290;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2310.16319</link><description>&lt;p&gt;
DiQAD&#65306;&#19968;&#31181;&#29992;&#20110;&#31471;&#21040;&#31471;&#24320;&#25918;&#22495;&#23545;&#35805;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DiQAD: A Benchmark Dataset for End-to-End Open-domain Dialogue Assessment. (arXiv:2310.16319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23545;&#35805;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#38598;&#65288;DiQAD&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#24320;&#25918;&#22495;&#23545;&#35805;&#36136;&#37327;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#31471;&#21040;&#31471;&#21644;&#20154;&#31867;&#35748;&#30693;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#32422;100,000&#20010;&#23545;&#35805;&#12290;&#23545;&#35805;&#36136;&#37327;&#30340;&#35780;&#20272;&#26631;&#20934;&#26159;&#26681;&#25454;&#31526;&#21512;&#20154;&#31867;&#23545;&#35805;&#36136;&#37327;&#21028;&#26029;&#30340;&#32500;&#24230;&#24314;&#31435;&#30340;&#12290;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#35780;&#20272;&#22312;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#24320;&#21457;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#30340;&#35282;&#33394;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#26080;&#27861;&#25552;&#20379;&#31471;&#21040;&#31471;&#21644;&#20154;&#31867;&#35748;&#30693;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#23427;&#20204;&#21482;&#25552;&#20379;&#19968;&#20123;&#23376;&#25351;&#26631;&#65292;&#22914;&#36830;&#36143;&#24615;&#25110;&#23545;&#35805;&#22312;&#36828;&#31163;&#23454;&#38469;&#29992;&#25143;&#29615;&#22659;&#30340;&#27880;&#37322;&#32773;&#20043;&#38388;&#36827;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23545;&#35805;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#38598;&#65288;DiQAD&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#24320;&#25918;&#22495;&#23545;&#35805;&#36136;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#65288;1&#65289;&#26681;&#25454;&#31526;&#21512;&#20154;&#31867;&#23545;&#35805;&#36136;&#37327;&#21028;&#26029;&#30340;&#32500;&#24230;&#24314;&#31435;&#35780;&#20272;&#26631;&#20934;&#65292;&#24182;&#65288;2&#65289;&#26681;&#25454;&#36825;&#20123;&#26631;&#20934;&#23545;&#23454;&#38469;&#29992;&#25143;&#20043;&#38388;&#23545;&#35805;&#36827;&#34892;&#22823;&#35268;&#27169;&#27880;&#37322;&#65292;&#20854;&#20013;&#21253;&#21547;&#32422;100,000&#20010;&#23545;&#35805;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#20010;&#23454;&#39564;&#65292;&#24182;&#25253;&#21578;&#20102;DiQAD&#19978;&#22522;&#20934;&#30340;&#22522;&#32447;&#24615;&#33021;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue assessment plays a critical role in the development of open-domain dialogue systems. Existing work are uncapable of providing an end-to-end and human-epistemic assessment dataset, while they only provide sub-metrics like coherence or the dialogues are conversed between annotators far from real user settings. In this paper, we release a large-scale dialogue quality assessment dataset (DiQAD), for automatically assessing open-domain dialogue quality. Specifically, we (1) establish the assessment criteria based on the dimensions conforming to human judgements on dialogue qualities, and (2) annotate large-scale dialogues that conversed between real users based on these annotation criteria, which contains around 100,000 dialogues. We conduct several experiments and report the performances of the baselines as the benchmark on DiQAD. The dataset is openly accessible at https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation.
&lt;/p&gt;</description></item><item><title>URL-BERT&#26159;&#19968;&#31181;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#20114;&#21160;&#35757;&#32451;&#32593;&#39029;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#23545;&#27604;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23545;URL&#21644;&#32593;&#39029;&#30340;&#26356;&#22909;&#29702;&#35299;&#21644;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.16303</link><description>&lt;p&gt;
URL-BERT: &#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#20114;&#21160;&#35757;&#32451;&#32593;&#39029;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
URL-BERT: Training Webpage Representations via Social Media Engagements. (arXiv:2310.16303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16303
&lt;/p&gt;
&lt;p&gt;
URL-BERT&#26159;&#19968;&#31181;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#20114;&#21160;&#35757;&#32451;&#32593;&#39029;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#23545;&#27604;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23545;URL&#21644;&#32593;&#39029;&#30340;&#26356;&#22909;&#29702;&#35299;&#21644;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#34920;&#31034;&#32593;&#39029;&#23545;&#20110;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#33267;&#20851;&#37325;&#35201;&#65292;&#29992;&#25143;&#21487;&#20197;&#20998;&#20139;&#21644;&#21442;&#19982;URL&#12290;&#24120;&#35265;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#32534;&#30721;&#22120;&#22914;BERT&#21487;&#20197;&#29992;&#20110;&#29702;&#35299;&#21644;&#34920;&#31034;&#32593;&#39029;&#30340;&#25991;&#26412;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#33021;&#26080;&#27861;&#24314;&#27169;&#32593;&#22495;&#21644;URL&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#20063;&#26080;&#27861;&#20934;&#30830;&#22320;&#25429;&#25417;&#23427;&#20204;&#23545;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#21560;&#24341;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;&#20351;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;URL&#21644;&#32593;&#39029;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#29992;&#25143;&#20114;&#21160;&#23398;&#20064;URL&#30340;&#27973;&#23618;&#34920;&#31034;&#30340;&#21487;&#25193;&#23637;&#22270;&#23884;&#20837;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23558;LM&#34920;&#31034;&#19982;&#21069;&#36848;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#30340;&#23545;&#27604;&#30446;&#26631;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#21040;BERT&#30340;&#22810;&#35821;&#35328;&#29256;&#26412;&#19978;&#65292;&#24471;&#21040;&#20102;&#27169;&#22411;URL-BERT&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#25913;&#21892;&#20102;&#21508;&#31181;&#20219;&#21153;&#30340;&#32593;&#39029;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and representing webpages is crucial to online social networks where users may share and engage with URLs. Common language model (LM) encoders such as BERT can be used to understand and represent the textual content of webpages. However, these representations may not model thematic information of web domains and URLs or accurately capture their appeal to social media users. In this work, we introduce a new pre-training objective that can be used to adapt LMs to understand URLs and webpages. Our proposed framework consists of two steps: (1) scalable graph embeddings to learn shallow representations of URLs based on user engagement on social media and (2) a contrastive objective that aligns LM representations with the aforementioned graph-based representation. We apply our framework to the multilingual version of BERT to obtain the model URL-BERT. We experimentally demonstrate that our continued pre-training approach improves webpage understanding on a variety of tasks and 
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#22810;&#26041;&#23545;&#35805;&#65288;MPC&#65289;&#65292;&#30446;&#21069;ChatGPT&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20196;&#20154;&#26399;&#24453;&#65292;&#32780;GPT-4&#30340;&#32467;&#26524;&#39044;&#31034;&#30528;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24341;&#20837;MPC&#32467;&#26500;&#65292;&#21253;&#25324;&#21457;&#35328;&#20154;&#21644;&#21463;&#35805;&#20154;&#26550;&#26500;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16301</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22810;&#26041;&#23545;&#35805;&#35299;&#20915;&#26041;&#26696;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Multi-Party Conversation Solver?. (arXiv:2310.16301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16301
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#26041;&#23545;&#35805;&#65288;MPC&#65289;&#65292;&#30446;&#21069;ChatGPT&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20196;&#20154;&#26399;&#24453;&#65292;&#32780;GPT-4&#30340;&#32467;&#26524;&#39044;&#31034;&#30528;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24341;&#20837;MPC&#32467;&#26500;&#65292;&#21253;&#25324;&#21457;&#35328;&#20154;&#21644;&#21463;&#35805;&#20154;&#26550;&#26500;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#24037;&#20855;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#22788;&#29702;&#22810;&#26041;&#23545;&#35805;&#65288;MPC&#65289;&#30340;&#33021;&#21147;&#8212;&#8212;&#19968;&#20010;&#30001;&#22810;&#20010;&#21442;&#19982;&#32773;&#21442;&#19982;&#22797;&#26434;&#20449;&#24687;&#20132;&#27969;&#30340;&#22330;&#26223;&#8212;&#8212;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;ChatGPT&#21644;GPT-4&#31561;&#29983;&#25104;&#22411;LLM&#22312;MPC&#29615;&#22659;&#19979;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#23545;&#21253;&#21547;&#20116;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#30340;&#19977;&#20010;MPC&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#36827;&#34892;&#20102;&#19968;&#20010;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;ChatGPT&#21644;GPT-4&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#19968;&#20123;&#35780;&#20272;&#30340;MPC&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20196;&#20154;&#26399;&#24453;&#65292;&#32780;GPT-4&#30340;&#32467;&#26524;&#39044;&#31034;&#30528;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21253;&#25324;&#21457;&#35328;&#20154;&#21644;&#21463;&#35805;&#20154;&#32467;&#26500;&#22312;&#20869;&#30340;MPC&#32467;&#26500;&#65292;&#21162;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35814;&#23613;&#30340;&#35780;&#20272;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as influential instruments within the realm of natural language processing; nevertheless, their capacity to handle multi-party conversations (MPCs) -- a scenario marked by the presence of multiple interlocutors involved in intricate information exchanges -- remains uncharted. In this paper, we delve into the potential of generative LLMs such as ChatGPT and GPT-4 within the context of MPCs. An empirical analysis is conducted to assess the zero-shot learning capabilities of ChatGPT and GPT-4 by subjecting them to evaluation across three MPC datasets that encompass five representative tasks. The findings reveal that ChatGPT's performance on a number of evaluated MPC tasks leaves much to be desired, whilst GPT-4's results portend a promising future. Additionally, we endeavor to bolster performance through the incorporation of MPC structures, encompassing both speaker and addressee architecture. This study provides an exhaustive evaluation and analy
&lt;/p&gt;</description></item><item><title>XFEVER &#25968;&#25454;&#38598;&#26159;&#20026;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#23545;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#32780;&#35774;&#35745;&#30340;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#26500;&#24314;&#19981;&#21516;&#35821;&#35328;&#30340;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16278</link><description>&lt;p&gt;
XFEVER: &#36328;&#35821;&#35328;&#20107;&#23454;&#39564;&#35777;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
XFEVER: Exploring Fact Verification across Languages. (arXiv:2310.16278v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16278
&lt;/p&gt;
&lt;p&gt;
XFEVER &#25968;&#25454;&#38598;&#26159;&#20026;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#23545;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#32780;&#35774;&#35745;&#30340;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#26500;&#24314;&#19981;&#21516;&#35821;&#35328;&#30340;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#36328;&#35821;&#35328;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#30340;Cross-lingual Fact Extraction and VERification (XFEVER)&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;Fact Extraction and VERification (FEVER)&#25968;&#25454;&#38598;&#30340;&#20027;&#24352;&#21644;&#35777;&#25454;&#25991;&#26412;&#32763;&#35793;&#25104;&#20845;&#31181;&#35821;&#35328;&#26469;&#26500;&#24314;&#23427;&#12290;&#35757;&#32451;&#38598;&#21644;&#24320;&#21457;&#38598;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#36827;&#34892;&#32763;&#35793;&#65292;&#32780;&#27979;&#35797;&#38598;&#21253;&#25324;&#30001;&#19987;&#19994;&#32763;&#35793;&#20154;&#21592;&#21644;&#26426;&#22120;&#32763;&#35793;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#20351;&#29992;XFEVER&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#23450;&#20041;&#20102;&#20004;&#31181;&#36328;&#35821;&#35328;&#20107;&#23454;&#39564;&#35777;&#22330;&#26223;&#65292;&#21363;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#32763;&#35793;&#35757;&#32451;&#23398;&#20064;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#22330;&#26223;&#25552;&#20986;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#26500;&#24314;&#19981;&#21516;&#35821;&#35328;&#30340;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#26377;&#24046;&#24322;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#24773;&#20917;&#19979;&#31245;&#36874;&#19968;&#31609;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;&#27169;&#22411;&#30340;&#35823;&#24046;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Cross-lingual Fact Extraction and VERification (XFEVER) dataset designed for benchmarking the fact verification models across different languages. We constructed it by translating the claim and evidence texts of the Fact Extraction and VERification (FEVER) dataset into six languages. The training and development sets were translated using machine translation, whereas the test set includes texts translated by professional translators and machine-translated texts. Using the XFEVER dataset, two cross-lingual fact verification scenarios, zero-shot learning and translate-train learning, are defined, and baseline models for each scenario are also proposed in this paper. Experimental results show that the multilingual language model can be used to build fact verification models in different languages efficiently. However, the performance varies by language and is somewhat inferior to the English case. We also found that we can effectively mitigate model miscalibratio
&lt;/p&gt;</description></item><item><title>CycleAlign&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#28860;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#25552;&#28860;&#23454;&#29616;&#23545;&#40657;&#30418;&#27169;&#22411;&#21040;&#30333;&#30418;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16271</link><description>&lt;p&gt;
CycleAlign: &#20174;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#21040;&#30333;&#30418;&#27169;&#22411;&#36827;&#34892;&#36845;&#20195;&#25552;&#28860;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment. (arXiv:2310.16271v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16271
&lt;/p&gt;
&lt;p&gt;
CycleAlign&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#28860;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#25552;&#28860;&#23454;&#29616;&#23545;&#40657;&#30418;&#27169;&#22411;&#21040;&#30333;&#30418;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#29983;&#25104;&#26377;&#23475;&#12289;&#26377;&#27602;&#25110;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#24726;&#30340;&#20869;&#23481;&#65292;&#20351;&#24471;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#23545;&#40784;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#23545;&#40784;&#30340;&#26041;&#27861;&#65288;&#22914;PPO&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#20294;&#24448;&#24448;&#22797;&#26434;&#12289;&#19981;&#31283;&#23450;&#19988;&#36164;&#28304;&#23494;&#38598;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25490;&#21517;&#30340;&#23545;&#40784;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#65292;&#36890;&#36807;&#29992;&#30417;&#30563;&#24494;&#35843;&#26367;&#25442;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20379;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#23427;&#20204;&#30340;&#25104;&#26412;&#36739;&#39640;&#12290;&#32771;&#34385;&#21040;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#24050;&#32463;&#30456;&#23545;&#36739;&#22909;&#22320;&#23545;&#40784;&#24182;&#19988;&#25104;&#26412;&#36739;&#20302;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#22987;&#20174;AI&#21453;&#39304;&#20013;&#23545;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#12290;&#29616;&#26377;&#30340;&#24120;&#35268;&#23454;&#36341;&#20165;&#20165;&#20174;LLMs&#25552;&#28860;&#20986;&#36981;&#24490;&#25351;&#20196;&#30340;&#21709;&#24212;&#65292;&#21463;&#21040;&#20102;&#29942;&#39048;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;CycleAlign&#26469;&#20174;&#21442;&#25968;&#38750;&#21487;&#35265;&#30340;&#27169;&#22411;&#20013;&#25552;&#28860;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models trained on large-scale corpus often generate content that is harmful, toxic, or contrary to human preferences, making their alignment with human values a critical concern. Reinforcement learning from human feedback (RLHF) with algorithms like PPO is a prevalent approach for alignment but is often complex, unstable, and resource-intensive. Recently, ranking-based alignment methods have emerged, offering stability and effectiveness by replacing the RL framework with supervised fine-tuning, but they are costly due to the need for annotated data. Considering that existing large language models (LLMs) like ChatGPT are already relatively well-aligned and cost-friendly, researchers have begun to align the language model with human preference from AI feedback. The common practices, which unidirectionally distill the instruction-following responses from LLMs, are constrained by their bottleneck. Thus we introduce CycleAlign to distill alignment capabilities from parameter-invisi
&lt;/p&gt;</description></item><item><title>Attention Lens&#26159;&#19968;&#31181;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#22836;&#29305;&#23450;&#36716;&#25442;&#23558;&#27880;&#24847;&#21147;&#22836;&#30340;&#36755;&#20986;&#32763;&#35793;&#20026;&#35789;&#27719;&#26631;&#35760;&#12290;&#20351;&#29992;Attention Lens&#65292;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27880;&#24847;&#21147;&#22836;&#22312;&#29983;&#25104;&#26368;&#32456;&#26631;&#35760;&#39044;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#27880;&#24847;&#21147;&#22836;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#25198;&#28436;&#30528;&#39640;&#24230;&#19987;&#38376;&#21270;&#30340;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.16270</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#38236;&#22836;&#65306;&#19968;&#31181;&#35299;&#37322;&#27880;&#24847;&#21147;&#22836;&#20449;&#24687;&#26816;&#32034;&#26426;&#21046;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism. (arXiv:2310.16270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16270
&lt;/p&gt;
&lt;p&gt;
Attention Lens&#26159;&#19968;&#31181;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#22836;&#29305;&#23450;&#36716;&#25442;&#23558;&#27880;&#24847;&#21147;&#22836;&#30340;&#36755;&#20986;&#32763;&#35793;&#20026;&#35789;&#27719;&#26631;&#35760;&#12290;&#20351;&#29992;Attention Lens&#65292;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27880;&#24847;&#21147;&#22836;&#22312;&#29983;&#25104;&#26368;&#32456;&#26631;&#35760;&#39044;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#27880;&#24847;&#21147;&#22836;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#25198;&#28436;&#30528;&#39640;&#24230;&#19987;&#38376;&#21270;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#32447;&#24615;&#23618;&#30340;&#20316;&#29992;&#65292;&#35299;&#30721;LLMs&#20026;&#25991;&#26412;&#23436;&#25104;&#20219;&#21153;&#20570;&#20986;&#26368;&#32456;&#39044;&#27979;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#27880;&#24847;&#21147;&#22836;&#22312;&#29983;&#25104;&#26368;&#32456;&#26631;&#35760;&#39044;&#27979;&#20013;&#30340;&#20855;&#20307;&#20316;&#29992;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Attention Lens&#65292;&#19968;&#20010;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#22836;&#29305;&#23450;&#36716;&#25442;(&#31216;&#20026;&#38236;&#22836;)&#23558;&#27880;&#24847;&#21147;&#22836;&#30340;&#36755;&#20986;&#32763;&#35793;&#20026;&#35789;&#27719;&#26631;&#35760;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#38236;&#22836;&#30340;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#65292;&#27880;&#24847;&#21147;&#22836;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#25198;&#28436;&#30528;&#39640;&#24230;&#19987;&#38376;&#21270;&#30340;&#35282;&#33394;&#12290;Attention Lens&#30340;&#20195;&#30721;&#21487;&#22312;github.com/msakarvadia/AttentionLens&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Large Language Models (LLMs) are the state-of-the-art for natural language tasks. Recent work has attempted to decode, by reverse engineering the role of linear layers, the internal mechanisms by which LLMs arrive at their final predictions for text completion tasks. Yet little is known about the specific role of attention heads in producing the final token prediction. We propose Attention Lens, a tool that enables researchers to translate the outputs of attention heads into vocabulary tokens via learned attention-head-specific transformations called lenses. Preliminary findings from our trained lenses indicate that attention heads play highly specialized roles in language models. The code for Attention Lens is available at github.com/msakarvadia/AttentionLens.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30495;&#23454;&#26032;&#38395;&#26426;&#26500;&#30340;&#35780;&#32423;&#26469;&#21019;&#24314;&#19968;&#20010;&#22810;&#35821;&#35328;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#31895;&#30053;&#30340;&#31435;&#22330;&#27880;&#37322;&#21644;&#33258;&#21160;&#25552;&#21462;&#30340;&#20027;&#39064;&#27880;&#37322;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27492;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#30830;&#23450;&#22823;&#22810;&#25968;&#26410;&#35265;&#25253;&#32440;&#30340;&#31038;&#35770;&#31435;&#22330;&#12290;</title><link>http://arxiv.org/abs/2310.16269</link><description>&lt;p&gt;
&#23186;&#20307;&#30340;&#22810;&#35821;&#35328;&#31895;&#30053;&#25919;&#27835;&#31435;&#22330;&#20998;&#31867;&#65306;ChatGPT&#21644;Bard&#25253;&#32440;&#30340;&#31038;&#35770;&#31435;&#22330;
&lt;/p&gt;
&lt;p&gt;
Multilingual Coarse Political Stance Classification of Media. The Editorial Line of a ChatGPT and Bard Newspaper. (arXiv:2310.16269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30495;&#23454;&#26032;&#38395;&#26426;&#26500;&#30340;&#35780;&#32423;&#26469;&#21019;&#24314;&#19968;&#20010;&#22810;&#35821;&#35328;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#31895;&#30053;&#30340;&#31435;&#22330;&#27880;&#37322;&#21644;&#33258;&#21160;&#25552;&#21462;&#30340;&#20027;&#39064;&#27880;&#37322;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27492;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#30830;&#23450;&#22823;&#22810;&#25968;&#26410;&#35265;&#25253;&#32440;&#30340;&#31038;&#35770;&#31435;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#31435;&#24615;&#22312;&#25919;&#27835;&#20013;&#24456;&#38590;&#23454;&#29616;&#65292;&#20063;&#26159;&#20027;&#35266;&#30340;&#12290;&#20256;&#32479;&#23186;&#20307;&#36890;&#24120;&#37319;&#21462;&#19968;&#31181;&#31038;&#35770;&#31435;&#22330;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#28508;&#22312;&#35835;&#32773;&#29992;&#26469;&#21028;&#26029;&#23186;&#20307;&#20559;&#35265;&#30340;&#25351;&#26631;&#12290;&#30446;&#21069;&#26377;&#20960;&#20010;&#24179;&#21488;&#26681;&#25454;&#25919;&#27835;&#20559;&#35265;&#23545;&#26032;&#38395;&#26426;&#26500;&#36827;&#34892;&#35780;&#32423;&#12290;&#31038;&#35770;&#31435;&#22330;&#21644;&#35780;&#32423;&#26377;&#21161;&#20110;&#35835;&#32773;&#33719;&#24471;&#19968;&#31181;&#24179;&#34913;&#30340;&#26032;&#38395;&#35266;&#28857;&#12290;&#20294;&#26159;&#38543;&#30528;&#25351;&#20196;&#36981;&#24490;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;&#20309;&#22312;&#19981;&#26045;&#21152;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#65292;&#20154;&#24037;&#26234;&#33021;&#26032;&#38395;&#26426;&#26500;&#20250;&#20301;&#20110;&#20309;&#22788;&#30340;&#20559;&#35265;&#35780;&#32423;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#26032;&#38395;&#26426;&#26500;&#30340;&#35780;&#32423;&#26469;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#31895;&#30053;&#31435;&#22330;&#27880;&#37322;&#65288;&#24038;&#32764;&#21644;&#21491;&#32764;&#65289;&#20197;&#21450;&#33258;&#21160;&#25552;&#21462;&#30340;&#20027;&#39064;&#27880;&#37322;&#30340;&#22810;&#35821;&#35328;&#26032;&#38395;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27492;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#35782;&#21035;&#20986;&#33521;&#35821;&#12289;&#24503;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#21152;&#27888;&#32599;&#23612;&#20122;&#35821;&#30340;&#22823;&#22810;&#25968;&#26410;&#35265;&#25253;&#32440;&#30340;&#31038;&#35770;&#31435;&#22330;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;101&#20221;&#31867;&#20284;&#25253;&#32440;&#30340;&#26032;&#38395;&#21002;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neutrality is difficult to achieve and, in politics, subjective. Traditional media typically adopt an editorial line that can be used by their potential readers as an indicator of the media bias. Several platforms currently rate news outlets according to their political bias. The editorial line and the ratings help readers in gathering a balanced view of news. But in the advent of instruction-following language models, tasks such as writing a newspaper article can be delegated to computers. Without imposing a biased persona, where would an AI-based news outlet lie within the bias ratings? In this work, we use the ratings of authentic news outlets to create a multilingual corpus of news with coarse stance annotations (Left and Right) along with automatically extracted topic annotations. We show that classifiers trained on this data are able to identify the editorial line of most unseen newspapers in English, German, Spanish and Catalan. We then apply the classifiers to 101 newspaper-lik
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#38024;&#23545;&#23433;&#20840;&#20195;&#30721;&#29983;&#25104;&#30340;&#32508;&#21512;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#21644;&#22686;&#24378;&#20195;&#30721;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20351;&#29992;&#26410;&#32463;&#28040;&#27602;&#30340;&#24320;&#28304;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#24341;&#20837;&#23433;&#20840;&#28431;&#27934;&#30340;&#39118;&#38505;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#29616;&#26377;&#27169;&#22411;&#22312;&#23433;&#20840;&#26041;&#38754;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;</title><link>http://arxiv.org/abs/2310.16263</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23433;&#20840;&#20195;&#30721;&#29983;&#25104;&#65306;&#22522;&#20110;&#25968;&#25454;&#38598;&#39537;&#21160;&#30340;&#28431;&#27934;&#32531;&#35299;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation. (arXiv:2310.16263v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#38024;&#23545;&#23433;&#20840;&#20195;&#30721;&#29983;&#25104;&#30340;&#32508;&#21512;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#21644;&#22686;&#24378;&#20195;&#30721;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20351;&#29992;&#26410;&#32463;&#28040;&#27602;&#30340;&#24320;&#28304;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#24341;&#20837;&#23433;&#20840;&#28431;&#27934;&#30340;&#39118;&#38505;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#29616;&#26377;&#27169;&#22411;&#22312;&#23433;&#20840;&#26041;&#38754;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20195;&#30721;&#29983;&#25104;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#26082;&#26377;&#21033;&#20110;&#26032;&#25163;&#24320;&#21457;&#20154;&#21592;&#65292;&#20063;&#26377;&#21033;&#20110;&#32463;&#39564;&#20016;&#23500;&#30340;&#24320;&#21457;&#20154;&#21592;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20351;&#29992;&#26469;&#33258;&#24320;&#28304;&#20179;&#24211;&#65288;&#22914;GitHub&#65289;&#30340;&#26410;&#32463;&#28040;&#27602;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20250;&#24341;&#20837;&#24847;&#22806;&#20256;&#25773;&#23433;&#20840;&#28431;&#27934;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#26377;&#25928;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#36719;&#20214;&#23433;&#20840;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#19968;&#39033;&#32508;&#21512;&#30740;&#31350;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#22686;&#24378;&#20195;&#30721;LLMs&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SecuCoGen&#65292;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;21&#31181;&#20851;&#38190;&#28431;&#27934;&#31867;&#22411;&#12290;SecuCoGen&#21253;&#21547;180&#20010;&#26679;&#26412;&#65292;&#24182;&#20316;&#20026;&#36827;&#34892;&#19977;&#20010;&#20851;&#38190;&#30340;&#19982;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#23454;&#39564;&#30340;&#22522;&#30784;&#65306;&#20195;&#30721;&#29983;&#25104;&#12289;&#20195;&#30721;&#20462;&#22797;&#21644;&#28431;&#27934;&#20998;&#31867;&#65292;&#20854;&#20013;&#23433;&#20840;&#24615;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#24378;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#22788;&#29702;&#23433;&#20840;&#38382;&#39064;&#26102;&#32463;&#24120;&#34987;&#24573;&#35270;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have brought significant advancements to code generation, benefiting both novice and experienced developers. However, their training using unsanitized data from open-source repositories, like GitHub, introduces the risk of inadvertently propagating security vulnerabilities. To effectively mitigate this concern, this paper presents a comprehensive study focused on evaluating and enhancing code LLMs from a software security perspective. We introduce SecuCoGen\footnote{SecuCoGen has been uploaded as supplemental material and will be made publicly available after publication.}, a meticulously curated dataset targeting 21 critical vulnerability types. SecuCoGen comprises 180 samples and serves as the foundation for conducting experiments on three crucial code-related tasks: code generation, code repair and vulnerability classification, with a strong emphasis on security. Our experimental results reveal that existing models often overlook security concerns during
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#33945;&#38754;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#29305;&#24615;&#23545;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#20998;&#24067;&#29305;&#24615;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#19981;&#33021;&#23436;&#20840;&#35299;&#37322;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#20998;&#26512;&#21457;&#29616;&#20998;&#24067;&#29305;&#24615;&#20063;&#26080;&#27861;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16261</link><description>&lt;p&gt;
&#20998;&#24067;&#20551;&#35774;&#19981;&#33021;&#23436;&#20840;&#35299;&#37322;&#33945;&#38754;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining. (arXiv:2310.16261v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#33945;&#38754;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#29305;&#24615;&#23545;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#20998;&#24067;&#29305;&#24615;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#19981;&#33021;&#23436;&#20840;&#35299;&#37322;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#20998;&#26512;&#21457;&#29616;&#20998;&#24067;&#29305;&#24615;&#20063;&#26080;&#27861;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#20998;&#24067;&#20551;&#35774;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#33945;&#38754;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#33945;&#38754;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#24402;&#22240;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#25152;&#32534;&#30721;&#30340;&#20998;&#24067;&#29305;&#24615;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20998;&#24067;&#29305;&#24615;&#30830;&#23454;&#23548;&#33268;&#20102;&#39044;&#35757;&#32451;&#33945;&#38754;&#35821;&#35328;&#27169;&#22411;&#30340;&#26356;&#22909;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#19981;&#33021;&#23436;&#20840;&#35299;&#37322;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20998;&#24067;&#29305;&#24615;&#20063;&#19981;&#33021;&#35299;&#37322;&#39044;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#25105;&#20204;&#23545;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#29702;&#35299;&#26377;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the masked language modeling pretraining objective function from the perspective of the distributional hypothesis. We investigate whether better sample efficiency and the better generalization capability of models pretrained with masked language modeling can be attributed to the semantic similarity encoded in the pretraining data's distributional property. Via a synthetic dataset, our analysis suggests that distributional property indeed leads to the better sample efficiency of pretrained masked language models, but does not fully explain the generalization capability. We also conduct analyses over two real-world datasets and demonstrate that the distributional property does not explain the generalization ability of pretrained natural language models either. Our results illustrate our limited understanding of model pretraining and provide future research directions.
&lt;/p&gt;</description></item><item><title>Speakerly&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#25991;&#26412;&#21019;&#20316;&#36741;&#21161;&#24037;&#20855;&#65292;&#29992;&#25143;&#21487;&#36890;&#36807;&#25351;&#20196;&#25110;&#21475;&#36848;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#31995;&#32479;&#29983;&#25104;&#26684;&#24335;&#33391;&#22909;&#12289;&#36830;&#36143;&#30340;&#25991;&#26723;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#24555;&#36895;&#26377;&#25928;&#30340;&#25991;&#26412;&#21019;&#20316;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#36755;&#20837;&#27169;&#24335;&#20197;&#25552;&#39640;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16251</link><description>&lt;p&gt;
Speakerly&#65306;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#25991;&#26412;&#21019;&#20316;&#36741;&#21161;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Speakerly: A Voice-based Writing Assistant for Text Composition. (arXiv:2310.16251v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16251
&lt;/p&gt;
&lt;p&gt;
Speakerly&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#25991;&#26412;&#21019;&#20316;&#36741;&#21161;&#24037;&#20855;&#65292;&#29992;&#25143;&#21487;&#36890;&#36807;&#25351;&#20196;&#25110;&#21475;&#36848;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#31995;&#32479;&#29983;&#25104;&#26684;&#24335;&#33391;&#22909;&#12289;&#36830;&#36143;&#30340;&#25991;&#26723;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#24555;&#36895;&#26377;&#25928;&#30340;&#25991;&#26412;&#21019;&#20316;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#36755;&#20837;&#27169;&#24335;&#20197;&#25552;&#39640;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Speakerly&#65292;&#19968;&#31181;&#26032;&#30340;&#23454;&#26102;&#35821;&#38899;&#25991;&#26412;&#21019;&#20316;&#36741;&#21161;&#31995;&#32479;&#65292;&#21487;&#24110;&#21161;&#29992;&#25143;&#22312;&#21508;&#31181;&#29992;&#20363;&#20013;&#36827;&#34892;&#25991;&#26412;&#21019;&#20316;&#65292;&#22914;&#30005;&#23376;&#37038;&#20214;&#12289;&#21363;&#26102;&#36890;&#35759;&#21644;&#31508;&#35760;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#25351;&#20196;&#25110;&#21475;&#36848;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#31995;&#32479;&#29983;&#25104;&#26684;&#24335;&#33391;&#22909;&#12289;&#36830;&#36143;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#31995;&#32479;&#26550;&#26500;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#22312;&#26500;&#24314;&#21644;&#37096;&#32626;&#22914;&#27492;&#35268;&#27169;&#30340;&#31995;&#32479;&#26102;&#22914;&#20309;&#35299;&#20915;&#21508;&#31181;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20351;&#29992;&#20102;&#19968;&#32452;&#23567;&#22411;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#24555;&#36895;&#26377;&#25928;&#30340;&#25991;&#26412;&#21019;&#20316;&#65292;&#21516;&#26102;&#25903;&#25345;&#21508;&#31181;&#36755;&#20837;&#27169;&#24335;&#20197;&#25552;&#39640;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Speakerly, a new real-time voice-based writing assistance system that helps users with text composition across various use cases such as emails, instant messages, and notes. The user can interact with the system through instructions or dictation, and the system generates a well-formatted and coherent document. We describe the system architecture and detail how we address the various challenges while building and deploying such a system at scale. More specifically, our system uses a combination of small, task-specific models as well as pre-trained language models for fast and effective text composition while supporting a variety of input modes for better usability.
&lt;/p&gt;</description></item><item><title>GlotLID-M&#26159;&#19968;&#20010;&#28385;&#36275;&#24191;&#27867;&#35206;&#30422;&#12289;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#35201;&#27714;&#30340;&#35821;&#35328;&#35782;&#21035;&#27169;&#22411;&#65292;&#20855;&#26377;1665&#20010;&#21487;&#35782;&#21035;&#35821;&#35328;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#35299;&#20915;&#20102;&#20302;&#36164;&#28304;LID&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#22686;&#24378;&#35775;&#38382;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16248</link><description>&lt;p&gt;
GlotLID: &#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35821;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
GlotLID: Language Identification for Low-Resource Languages. (arXiv:2310.16248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16248
&lt;/p&gt;
&lt;p&gt;
GlotLID-M&#26159;&#19968;&#20010;&#28385;&#36275;&#24191;&#27867;&#35206;&#30422;&#12289;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#35201;&#27714;&#30340;&#35821;&#35328;&#35782;&#21035;&#27169;&#22411;&#65292;&#20855;&#26377;1665&#20010;&#21487;&#35782;&#21035;&#35821;&#35328;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#35299;&#20915;&#20102;&#20302;&#36164;&#28304;LID&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#22686;&#24378;&#35775;&#38382;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#20960;&#31687;&#35770;&#25991;&#21457;&#34920;&#20102;&#38024;&#23545;&#32422;300&#31181;&#39640;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#35821;&#35328;&#35782;&#21035;&#65288;LID&#65289;&#30340;&#33391;&#22909;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21487;&#29992;&#30340;LID&#28385;&#36275;&#20197;&#19979;&#35201;&#27714;&#65306;&#65288;i&#65289;&#28085;&#30422;&#24191;&#27867;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#65288;ii&#65289;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#19988;&#21487;&#38752;&#65292;&#65288;iii&#65289;&#39640;&#25928;&#26131;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;GlotLID-M&#65292;&#19968;&#20010;&#28385;&#36275;&#24191;&#27867;&#35206;&#30422;&#12289;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#35201;&#27714;&#30340;LID&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#35782;&#21035;1665&#31181;&#35821;&#35328;&#65292;&#22312;&#35206;&#30422;&#33539;&#22260;&#19978;&#30456;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#26377;&#20102;&#22823;&#24133;&#22686;&#21152;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;GlotLID-M&#22312;&#24179;&#34913;F1&#20998;&#25968;&#21644;&#20551;&#38451;&#24615;&#29575;&#65288;FPR&#65289;&#26041;&#38754;&#20248;&#20110;&#22235;&#20010;&#22522;&#20934;&#27169;&#22411;&#65288;CLD3&#65292;FT176&#65292;OpenLID&#21644;NLLB&#65289;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20302;&#36164;&#28304;LID&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65306;&#19981;&#27491;&#30830;&#30340;&#35821;&#26009;&#24211;&#20803;&#25968;&#25454;&#65292;&#26469;&#33258;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#27844;&#28431;&#65292;&#38590;&#20197;&#21306;&#20998;&#23494;&#20999;&#30456;&#20851;&#30340;&#35821;&#35328;&#65292;&#22788;&#29702;&#23439;&#35821;&#35328;&#19982;&#26041;&#35328;&#65292;&#20197;&#21450;&#19968;&#33324;&#30340;&#22122;&#22768;&#25968;&#25454;&#12290;&#25105;&#20204;&#24076;&#26395;&#23558;GlotLID-M&#38598;&#25104;&#21040;&#25968;&#25454;&#38598;&#21019;&#24314;&#27969;&#31243;&#20013;&#65292;&#20197;&#25552;&#39640;&#36136;&#37327;&#21644;&#22686;&#24378;&#35775;&#38382;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent papers have published good solutions for language identification (LID) for about 300 high-resource and medium-resource languages. However, there is no LID available that (i) covers a wide range of low-resource languages, (ii) is rigorously evaluated and reliable and (iii) efficient and easy to use. Here, we publish GlotLID-M, an LID model that satisfies the desiderata of wide coverage, reliability and efficiency. It identifies 1665 languages, a large increase in coverage compared to prior work. In our experiments, GlotLID-M outperforms four baselines (CLD3, FT176, OpenLID and NLLB) when balancing F1 and false positive rate (FPR). We analyze the unique challenges that low-resource LID poses: incorrect corpus metadata, leakage from high-resource languages, difficulty separating closely related languages, handling of macrolanguage vs varieties and in general noisy data. We hope that integrating GlotLID-M into dataset creation pipelines will improve quality and enhance acces
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ZzzGPT&#30340;&#20132;&#20114;&#24335;GPT&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21644;&#21453;&#39304;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#29992;&#25143;&#23548;&#21521;&#35774;&#35745;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#21644;&#26377;&#20215;&#20540;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.16242</link><description>&lt;p&gt;
ZzzGPT: &#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#30340;&#20132;&#20114;&#24335;GPT&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality. (arXiv:2310.16242v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ZzzGPT&#30340;&#20132;&#20114;&#24335;GPT&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21644;&#21453;&#39304;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#29992;&#25143;&#23548;&#21521;&#35774;&#35745;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#21644;&#26377;&#20215;&#20540;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#19990;&#30028;&#20013;&#65292;&#30561;&#30496;&#36136;&#37327;&#23545;&#24635;&#20307;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25552;&#20379;&#23454;&#26102;&#30417;&#27979;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#35265;&#35299;&#65292;&#23548;&#33268;&#29992;&#25143;&#25918;&#24323;&#20351;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25216;&#26415;&#22312;&#29702;&#35299;&#30561;&#30496;&#27169;&#24335;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs)&#65292;&#26088;&#22312;&#25552;&#20379;&#20934;&#30830;&#30340;&#30561;&#30496;&#39044;&#27979;&#21644;&#26377;&#20215;&#20540;&#30340;&#21453;&#39304;&#12290;&#21033;&#29992;GLOBEM&#25968;&#25454;&#38598;&#21644;LLMs&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;XGBoost&#31561;&#27169;&#22411;&#30456;&#27604;&#30340;&#22686;&#24378;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#19982;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35774;&#35745;&#30456;&#32467;&#21512;&#65292;&#23558;&#31185;&#23398;&#20934;&#30830;&#24615;&#19982;&#23454;&#29992;&#24615;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's world, sleep quality is pivotal for overall well-being. While wearable sensors offer real-time monitoring, they often lack actionable insights, leading to user abandonment. This paper delves into the role of technology in understanding sleep patterns. We introduce a two-stage framework, utilizing Large Language Models (LLMs), aiming to provide accurate sleep predictions with actionable feedback. Leveraging the GLOBEM dataset and synthetic data from LLMs, we highlight enhanced results with models like XGBoost. Our approach merges advanced machine learning with user-centric design, blending scientific accuracy with practicality.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#35821;&#35328;&#32467;&#26500;&#65292;&#36890;&#36807;&#28151;&#21512;&#35821;&#35328;&#19987;&#23478;&#26550;&#26500;&#26469;&#25913;&#36827;&#21644;&#35299;&#37322;&#20854;&#24615;&#33021;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#27604;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.16240</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#35821;&#35328;&#19987;&#23478;&#36866;&#37197;&#22120;&#25913;&#36827;&#21644;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting Pre-trained Language Models. (arXiv:2310.16240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#35821;&#35328;&#32467;&#26500;&#65292;&#36890;&#36807;&#28151;&#21512;&#35821;&#35328;&#19987;&#23478;&#26550;&#26500;&#26469;&#25913;&#36827;&#21644;&#35299;&#37322;&#20854;&#24615;&#33021;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#27604;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#32467;&#26500;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#35774;&#32622;&#19979;&#65292;&#23558;&#20004;&#20010;&#28909;&#38376;&#30740;&#31350;&#39046;&#22495;&#30456;&#32467;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#35821;&#35328;&#19987;&#23478;&#26550;&#26500;&#32452;&#21512;&#32534;&#30721;&#19981;&#21516;&#35821;&#35328;&#32467;&#26500;&#30340;&#24182;&#34892;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#36890;&#36807;Gumbel-Softmax&#38376;&#30830;&#23450;&#27169;&#22411;&#27599;&#19968;&#23618;&#20013;&#36825;&#20123;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#65292;&#22312;&#20462;&#21098;&#19987;&#23478;&#30340;&#37325;&#35201;&#24615;&#35780;&#20998;&#21518;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#27169;&#22411;&#36827;&#34892;&#19968;&#23450;&#25968;&#37327;&#30340;&#22266;&#23450;&#27493;&#39588;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#21487;&#27604;&#36739;&#21442;&#25968;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#20998;&#26512;&#26469;&#26816;&#26597;&#27599;&#20010;&#27169;&#22411;&#22312;&#27599;&#23618;&#36873;&#25321;&#30340;&#19987;&#23478;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a method that combines two popular research areas by injecting linguistic structures into pre-trained language models in the parameter-efficient fine-tuning (PEFT) setting. In our approach, parallel adapter modules encoding different linguistic structures are combined using a novel Mixture-of-Linguistic-Experts architecture, where Gumbel-Softmax gates are used to determine the importance of these modules at each layer of the model. To reduce the number of parameters, we first train the model for a fixed small number of steps before pruning the experts based on their importance scores. Our experiment results with three different pre-trained models show that our approach can outperform state-of-the-art PEFT methods with a comparable number of parameters. In addition, we provide additional analysis to examine the experts selected by each model at each layer to provide insights for future studies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#36830;&#32493; (TiC) &#22522;&#20934;&#65292;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#25490;&#32451;&#26041;&#27861;&#26469;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16226</link><description>&lt;p&gt;
TiC-CLIP: CLIP&#27169;&#22411;&#30340;&#25345;&#32493;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TiC-CLIP: Continual Training of CLIP Models. (arXiv:2310.16226v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16226
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#36830;&#32493; (TiC) &#22522;&#20934;&#65292;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#25490;&#32451;&#26041;&#27861;&#26469;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25345;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#19982;&#26368;&#26032;&#25968;&#25454;&#20445;&#25345;&#21516;&#27493;&#26412;&#36523;&#23601;&#26159;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#26029;&#37325;&#26032;&#35757;&#32451;&#30340;&#39640;&#25104;&#26412;&#65292;&#25345;&#32493;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#32570;&#20047;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#25110;&#22522;&#32447;&#25152;&#21152;&#21095;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#31532;&#19968;&#25209; Web &#35268;&#27169;&#26102;&#38388;&#36830;&#32493;&#65288;TiC&#65289;&#22522;&#20934;&#65306;TiC-DataCompt&#12289;TiC-YFCC &#21644; TiC-RedCaps&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807; 127 &#20159;&#20010;&#26102;&#38388;&#25139;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#36328;&#36234;&#20102; 9 &#24180;&#30340;&#26102;&#38388;&#65288;2014-2022&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#36825;&#20123;&#22522;&#20934;&#26469;&#31574;&#21010;&#21508;&#31181;&#21160;&#24577;&#35780;&#20272;&#65292;&#20197;&#34913;&#37327;&#29616;&#26377;&#27169;&#22411;&#30340;&#26102;&#38388;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; OpenAI &#30340; CLIP &#27169;&#22411;&#65288;&#20351;&#29992; 2020 &#24180;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65289;&#22312;&#25105;&#20204;&#31574;&#21010;&#30340;&#20174; 2021 &#24180;&#21040; 2022 &#24180;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;&#22833;&#21435;&#20102;&#32422; 8% &#30340;&#38646;-shot&#20934;&#30830;&#29575;&#65292;&#32780;&#19982; OpenCLIP &#23384;&#20648;&#24211;&#20013;&#26368;&#36817;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#23545;&#26102;&#38388;&#36830;&#32493;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25490;&#32451;&#26041;&#27861;&#65292;&#20174;&#19978;&#27425;&#30340;&#35757;&#32451;&#20013;&#32487;&#32493;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with over 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses $\approx 8\%$ zero-shot accuracy on our curated retrieval task from 2021--2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the l
&lt;/p&gt;</description></item><item><title>CleanCoNLL&#26159;&#19968;&#31181;&#20960;&#20046;&#26080;&#22122;&#22768;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20840;&#38754;&#37325;&#26631;&#35760;&#21644;&#33258;&#21160;&#19968;&#33268;&#24615;&#26816;&#26597;&#26469;&#32416;&#27491;CoNLL-03&#20013;&#30340;&#27880;&#37322;&#38169;&#35823;&#65292;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;F1&#20998;&#25968;&#65292;&#24182;&#20943;&#23569;&#20102;&#22240;&#27880;&#37322;&#32570;&#22833;&#32780;&#35823;&#21028;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.16225</link><description>&lt;p&gt;
CleanCoNLL: &#19968;&#31181;&#20960;&#20046;&#26080;&#22122;&#22768;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset. (arXiv:2310.16225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16225
&lt;/p&gt;
&lt;p&gt;
CleanCoNLL&#26159;&#19968;&#31181;&#20960;&#20046;&#26080;&#22122;&#22768;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20840;&#38754;&#37325;&#26631;&#35760;&#21644;&#33258;&#21160;&#19968;&#33268;&#24615;&#26816;&#26597;&#26469;&#32416;&#27491;CoNLL-03&#20013;&#30340;&#27880;&#37322;&#38169;&#35823;&#65292;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;F1&#20998;&#25968;&#65292;&#24182;&#20943;&#23569;&#20102;&#22240;&#27880;&#37322;&#32570;&#22833;&#32780;&#35823;&#21028;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CoNLL-03&#35821;&#26009;&#24211;&#34987;&#35748;&#20026;&#26159;&#26368;&#33879;&#21517;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#22823;&#37327;&#30340;&#27880;&#37322;&#38169;&#35823;&#12289;&#19981;&#23436;&#25972;&#24615;&#21644;&#25968;&#25454;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#32473;&#23458;&#35266;&#27604;&#36739;NER&#26041;&#27861;&#21644;&#20998;&#26512;&#20854;&#38169;&#35823;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;CoNLL-03&#20013;&#36798;&#21040;&#30340;F1&#20998;&#25968;&#19982;&#20272;&#35745;&#30340;&#22122;&#22768;&#27700;&#24179;&#30456;&#24403;&#29978;&#33267;&#26356;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#19968;&#33268;&#24615;&#26816;&#26597;&#36741;&#21161;&#30340;&#20840;&#38754;&#37325;&#26631;&#35760;&#24037;&#20316;&#26469;&#32416;&#27491;&#33521;&#25991;CoNLL-03&#20013;&#25152;&#26377;&#26631;&#31614;&#30340;7.0&#65285;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#37322;NER&#26631;&#31614;&#21644;&#20316;&#20026;&#38468;&#21152;&#20445;&#35777;&#27880;&#37322;&#36136;&#37327;&#32780;&#28155;&#21152;&#20102;&#19968;&#20010;&#23454;&#20307;&#38142;&#25509;&#27880;&#37322;&#23618;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#19978;&#36798;&#21040;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;F1&#20998;&#25968;&#65288;97.1&#65285;&#65289;&#65292;&#32780;&#19988;&#20851;&#38190;&#26159;&#27491;&#30830;&#39044;&#27979;&#34987;&#38169;&#35823;&#22320;&#35745;&#31639;&#20026;&#38169;&#35823;&#30340;&#27604;&#20363;&#30001;&#20110;&#27880;&#37322;&#30340;&#32570;&#22833;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CoNLL-03 corpus is arguably the most well-known and utilized benchmark dataset for named entity recognition (NER). However, prior works found significant numbers of annotation errors, incompleteness, and inconsistencies in the data. This poses challenges to objectively comparing NER approaches and analyzing their errors, as current state-of-the-art models achieve F1-scores that are comparable to or even exceed the estimated noise level in CoNLL-03. To address this issue, we present a comprehensive relabeling effort assisted by automatic consistency checking that corrects 7.0% of all labels in the English CoNLL-03. Our effort adds a layer of entity linking annotation both for better explainability of NER labels and as additional safeguard of annotation quality. Our experimental evaluation finds not only that state-of-the-art approaches reach significantly higher F1-scores (97.1%) on our data, but crucially that the share of correct predictions falsely counted as errors due to annota
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#20197;&#32435;&#20837;&#26032;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16218</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Editing for Large Language Models: A Survey. (arXiv:2310.16218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16218
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#20197;&#32435;&#20837;&#26032;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36817;&#26399;&#20197;&#20854;&#20986;&#33394;&#30340;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26681;&#25454;&#20854;&#24191;&#21338;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#25913;&#21464;&#20102;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#30340;&#26684;&#23616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#26102;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#20026;&#20854;&#21442;&#25968;&#25968;&#37327;&#21069;&#25152;&#26410;&#26377;&#12290;&#24403;&#38656;&#35201;&#39057;&#32321;&#24341;&#20837;&#26032;&#30693;&#35782;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26102;&#65292;&#36825;&#20010;&#32570;&#28857;&#26356;&#21152;&#26174;&#33879;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20256;&#32479;&#26041;&#27861;&#26159;&#36890;&#36807;&#30452;&#25509;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#32534;&#30721;&#21040;&#39044;&#35757;&#32451;LLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#37325;&#26032;&#35757;&#32451;LLMs&#21487;&#33021;&#35745;&#31639;&#36164;&#28304;&#23494;&#38598;&#65292;&#24182;&#19988;&#23384;&#22312;&#23558;&#19982;&#27169;&#22411;&#26356;&#26032;&#26080;&#20851;&#30340;&#26377;&#20215;&#20540;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#36864;&#21270;&#30340;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#32534;&#36753;(KME)&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#31934;&#30830;&#20462;&#25913;LLMs&#20197;&#32435;&#20837;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20219;&#21153;&#65292;&#21363;&#29983;&#25104;&#26032;&#38395;&#20107;&#20214;&#30340;&#32972;&#26223;&#25688;&#35201;&#65292;&#29992;&#20110;&#34917;&#20805;&#27599;&#20010;&#26102;&#38388;&#32447;&#26356;&#26032;&#30340;&#30456;&#20851;&#21069;&#32622;&#20107;&#20214;&#65292;&#36890;&#36807;&#21512;&#24182;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#31995;&#32479;&#24314;&#31435;&#22522;&#20934;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26597;&#35810;&#20026;&#37325;&#28857;&#30340;&#21464;&#20307;&#26469;&#29983;&#25104;&#32972;&#26223;&#25688;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#25688;&#35201;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.16197</link><description>&lt;p&gt;
&#20107;&#20214;&#26102;&#38388;&#32447;&#30340;&#32972;&#26223;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Background Summarization of Event Timelines. (arXiv:2310.16197v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20219;&#21153;&#65292;&#21363;&#29983;&#25104;&#26032;&#38395;&#20107;&#20214;&#30340;&#32972;&#26223;&#25688;&#35201;&#65292;&#29992;&#20110;&#34917;&#20805;&#27599;&#20010;&#26102;&#38388;&#32447;&#26356;&#26032;&#30340;&#30456;&#20851;&#21069;&#32622;&#20107;&#20214;&#65292;&#36890;&#36807;&#21512;&#24182;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#31995;&#32479;&#24314;&#31435;&#22522;&#20934;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26597;&#35810;&#20026;&#37325;&#28857;&#30340;&#21464;&#20307;&#26469;&#29983;&#25104;&#32972;&#26223;&#25688;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#25688;&#35201;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#26032;&#38395;&#20107;&#20214;&#30340;&#31616;&#27905;&#25688;&#35201;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#34429;&#28982;&#35760;&#32773;&#36890;&#24120;&#20250;&#25972;&#29702;&#26102;&#38388;&#32447;&#26469;&#31361;&#20986;&#20851;&#38190;&#23376;&#20107;&#20214;&#65292;&#20294;&#26159;&#23545;&#20110;&#26032;&#38395;&#20107;&#20214;&#30340;&#26032;&#20154;&#26469;&#35828;&#65292;&#20102;&#35299;&#20854;&#21382;&#21490;&#32972;&#26223;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#32972;&#26223;&#26032;&#38395;&#25688;&#35201;&#30340;&#20219;&#21153;&#26469;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#65292;&#35813;&#20219;&#21153;&#20026;&#27599;&#20010;&#26102;&#38388;&#32447;&#26356;&#26032;&#37197;&#19978;&#30456;&#20851;&#21069;&#32622;&#20107;&#20214;&#30340;&#32972;&#26223;&#25688;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#24182;&#29616;&#26377;&#30340;&#26102;&#38388;&#32447;&#25968;&#25454;&#38598;&#24182;&#35201;&#27714;&#20154;&#24037;&#27880;&#37322;&#32773;&#20026;&#27599;&#20010;&#26032;&#38395;&#20107;&#20214;&#30340;&#27599;&#19968;&#26102;&#38388;&#27493;&#39588;&#32534;&#20889;&#32972;&#26223;&#25688;&#35201;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#31995;&#32479;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#20934;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#26597;&#35810;&#20026;&#37325;&#28857;&#30340;&#21464;&#20307;&#26469;&#29983;&#25104;&#32972;&#26223;&#25688;&#35201;&#12290;&#20026;&#20102;&#35780;&#20272;&#32972;&#26223;&#25688;&#35201;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#32972;&#26223;&#25928;&#29992;&#20998;&#25968;&#65288;BUS&#65289;&#65292;&#23427;&#34913;&#37327;&#20102;&#32972;&#26223;&#25688;&#35201;&#22238;&#31572;&#24403;&#21069;&#20107;&#20214;&#26102;&#38388;&#27493;&#39588;&#38382;&#39064;&#30340;&#30334;&#20998;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating concise summaries of news events is a challenging natural language processing task. While journalists often curate timelines to highlight key sub-events, newcomers to a news event face challenges in catching up on its historical context. In this paper, we address this need by introducing the task of background news summarization, which complements each timeline update with a background summary of relevant preceding events. We construct a dataset by merging existing timeline datasets and asking human annotators to write a background summary for each timestep of each news event. We establish strong baseline performance using state-of-the-art summarization systems and propose a query-focused variant to generate background summaries. To evaluate background summary quality, we present a question-answering-based evaluation metric, Background Utility Score (BUS), which measures the percentage of questions about a current event timestep that a background summary answers. Our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#38271;&#24230;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20165;&#20381;&#36182;&#20110;&#25991;&#26723;&#38271;&#24230;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24310;&#38271;&#25991;&#26723;&#30340;&#38271;&#24230;&#20250;&#21152; intensify &#36798;&#21040;&#30340;&#39640;&#20869;&#37096;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#31561;&#21521;&#24615;&#30340;&#34920;&#29616;&#39640;&#24230;&#20381;&#36182;&#20110;&#25991;&#26412;&#38271;&#24230;&#33539;&#22260;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#25991;&#26723;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#20041;&#40065;&#26834;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.16193</link><description>&lt;p&gt;
&#38271;&#24230;&#23545;&#20110;&#25991;&#26723;&#32423;&#35821;&#20041;&#32780;&#35328;&#26082;&#26159;&#35781;&#21650;&#20063;&#26159;&#31119;&#38899;
&lt;/p&gt;
&lt;p&gt;
Length is a Curse and a Blessing for Document-level Semantics. (arXiv:2310.16193v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#38271;&#24230;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20165;&#20381;&#36182;&#20110;&#25991;&#26723;&#38271;&#24230;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24310;&#38271;&#25991;&#26723;&#30340;&#38271;&#24230;&#20250;&#21152; intensify &#36798;&#21040;&#30340;&#39640;&#20869;&#37096;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#31561;&#21521;&#24615;&#30340;&#34920;&#29616;&#39640;&#24230;&#20381;&#36182;&#20110;&#25991;&#26412;&#38271;&#24230;&#33539;&#22260;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#25991;&#26723;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#20041;&#40065;&#26834;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#20174;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#24674;&#22797;&#21477;&#23376;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#22522;&#20110;CL&#30340;&#27169;&#22411;&#30340;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#65292;&#21363;&#23427;&#20204;&#23545;&#20110;&#38271;&#24230;&#24341;&#36215;&#30340;&#35821;&#20041;&#21464;&#21270;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#38271;&#24230;&#26131;&#21463;&#25915;&#20987;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#34987;&#24573;&#35270;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#19988;&#25105;&#20204;&#21487;&#20197;&#35774;&#35745;&#20165;&#20381;&#36182;&#20110;&#25991;&#26723;&#38271;&#24230;&#25552;&#20379;&#30340;&#35821;&#20041;&#20449;&#21495;&#30340;&#26080;&#30417;&#30563;CL&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;&#38271;&#24230;&#25915;&#20987;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#34920;&#26126;&#24310;&#38271;&#25991;&#26723;&#20250;&#21152; intensify &#24050;&#32463;&#30001;CL&#24102;&#26469;&#30340;&#39640;&#20869;&#37096;&#25991;&#26723;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;CL&#25215;&#35834;&#30340;&#31561;&#21521;&#24615;&#39640;&#24230;&#20381;&#36182;&#20110;&#35757;&#32451;&#20013;&#26292;&#38706;&#30340;&#25991;&#26412;&#38271;&#24230;&#33539;&#22260;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#25991;&#26723;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;LA(SER)$^{3}$: &#38271;&#24230;&#19981;&#21463;&#38480;&#30340;&#33258;&#25105;&#21442;&#29031;&#29992;&#20110;&#35821;&#20041;&#40065;&#26834;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In recent years, contrastive learning (CL) has been extensively utilized to recover sentence and document-level encoding capability from pre-trained language models. In this work, we question the length generalizability of CL-based models, i.e., their vulnerability towards length-induced semantic shift. We verify not only that length vulnerability is a significant yet overlooked research gap, but we can devise unsupervised CL methods solely depending on the semantic signal provided by document length. We first derive the theoretical foundations underlying length attacks, showing that elongating a document would intensify the high intra-document similarity that is already brought by CL. Moreover, we found that isotropy promised by CL is highly dependent on the length range of text exposed in training. Inspired by these findings, we introduce a simple yet universal document representation learning framework, LA(SER)$^{3}$: length-agnostic self-reference for semantically robust sentence r
&lt;/p&gt;</description></item><item><title>BLP 2023&#20219;&#21153;2&#26159;&#20851;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#21560;&#24341;&#20102;71&#20010;&#21442;&#19982;&#32773;&#12290;&#21442;&#19982;&#32773;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20132;&#20102;597&#20010;&#36816;&#34892;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20219;&#21153;&#30340;&#35814;&#32454;&#35774;&#32622;&#21644;&#21442;&#19982;&#32773;&#25552;&#20132;&#31995;&#32479;&#30340;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.16183</link><description>&lt;p&gt;
BLP 2023&#20219;&#21153;2&#65306;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
BLP 2023 Task 2: Sentiment Analysis. (arXiv:2310.16183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16183
&lt;/p&gt;
&lt;p&gt;
BLP 2023&#20219;&#21153;2&#26159;&#20851;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#21560;&#24341;&#20102;71&#20010;&#21442;&#19982;&#32773;&#12290;&#21442;&#19982;&#32773;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20132;&#20102;597&#20010;&#36816;&#34892;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20219;&#21153;&#30340;&#35814;&#32454;&#35774;&#32622;&#21644;&#21442;&#19982;&#32773;&#25552;&#20132;&#31995;&#32479;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24635;&#32467;&#20102;&#20316;&#20026;BLP 2023&#21019;&#26032;&#24037;&#20316;&#22346;&#30340;&#19968;&#37096;&#20998;&#20030;&#21150;&#30340;BLP&#24773;&#24863;&#20849;&#20139;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#30340;&#23450;&#20041;&#26159;&#22312;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#12290;&#35813;&#20219;&#21153;&#21560;&#24341;&#20102;71&#20010;&#21442;&#19982;&#32773;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#38454;&#27573;&#20998;&#21035;&#26377;29&#20010;&#21644;30&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#12290;&#24635;&#20849;&#65292;&#21442;&#19982;&#32773;&#25552;&#20132;&#20102;597&#20010;&#36816;&#34892;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24635;&#20849;&#26377;15&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#25551;&#36848;&#35770;&#25991;&#12290;&#25552;&#20132;&#30340;&#31995;&#32479;&#28085;&#30422;&#20102;&#20174;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21040;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#35813;&#20219;&#21153;&#30340;&#35774;&#32622;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;&#21442;&#19982;&#32773;&#25552;&#20132;&#30340;&#31995;&#32479;&#12290;&#20849;&#20139;&#20219;&#21153;&#30340;&#25152;&#26377;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#33050;&#26412;&#24050;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an overview of the BLP Sentiment Shared Task, organized as part of the inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is defined as the detection of sentiment in a given piece of social media text. This task attracted interest from 71 participants, among whom 29 and 30 teams submitted systems during the development and evaluation phases, respectively. In total, participants submitted 597 runs. However, a total of 15 teams submitted system description papers. The range of approaches in the submitted systems spans from classical machine learning models, fine-tuning pre-trained models, to leveraging Large Language Model (LLMs) in zero- and few-shot settings. In this paper, we provide a detailed account of the task setup, including dataset development and evaluation setup. Additionally, we provide a brief overview of the systems submitted by the participants. All datasets and evaluation scripts from the shared task have been made publicly available for the res
&lt;/p&gt;</description></item><item><title>&#38544;&#34255;&#24341;&#29992;&#29616;&#35937;&#22312;&#31185;&#23398;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#27491;&#24335;&#24341;&#29992;&#30340;&#25968;&#37327;&#65292;&#34920;&#26126;&#20256;&#32479;&#30340;&#24341;&#25991;&#20998;&#26512;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#31185;&#23398;&#21457;&#29616;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16181</link><description>&lt;p&gt;
&#31185;&#23398;&#20013;&#30340;&#38544;&#34255;&#24341;&#29992;&#27169;&#31946;&#20102;&#30495;&#27491;&#30340;&#24433;&#21709;&#21147;
&lt;/p&gt;
&lt;p&gt;
Hidden Citations Obscure True Impact in Science. (arXiv:2310.16181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16181
&lt;/p&gt;
&lt;p&gt;
&#38544;&#34255;&#24341;&#29992;&#29616;&#35937;&#22312;&#31185;&#23398;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#27491;&#24335;&#24341;&#29992;&#30340;&#25968;&#37327;&#65292;&#34920;&#26126;&#20256;&#32479;&#30340;&#24341;&#25991;&#20998;&#26512;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#31185;&#23398;&#21457;&#29616;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#29992;&#26159;&#31185;&#23398;&#23478;&#20204;&#29992;&#26469;&#34920;&#31034;&#20043;&#21069;&#30693;&#35782;&#30340;&#26426;&#21046;&#65292;&#20294;&#26368;&#36817;&#24050;&#32463;&#21464;&#25104;&#20102;&#24191;&#27867;&#20351;&#29992;&#21644;&#28389;&#29992;&#30340;&#31185;&#23398;&#24433;&#21709;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#24403;&#19968;&#20010;&#21457;&#29616;&#21464;&#25104;&#20849;&#35782;&#26102;&#65292;&#24341;&#29992;&#20250;&#22240;&#20026;&#34987;&#24573;&#35270;&#32780;&#34987;&#21512;&#24182;&#12290;&#36825;&#23548;&#33268;&#20102;&#38544;&#34255;&#24341;&#29992;&#30340;&#27010;&#24565;&#65292;&#23427;&#34920;&#31034;&#23545;&#19968;&#20010;&#21457;&#29616;&#30340;&#26126;&#30830;&#25991;&#26412;&#35748;&#21487;&#65292;&#20294;&#27809;&#26377;&#24341;&#29992;&#35813;&#21457;&#29616;&#30340;&#20986;&#29256;&#29289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#27599;&#31687;&#35770;&#25991;&#30340;&#20840;&#25991;&#65292;&#20197;&#31995;&#32479;&#22320;&#35782;&#21035;&#38544;&#34255;&#24341;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#26377;&#24433;&#21709;&#21147;&#30340;&#21457;&#29616;&#65292;&#38544;&#34255;&#24341;&#29992;&#25968;&#37327;&#36229;&#36807;&#20102;&#24341;&#29992;&#35745;&#25968;&#65292;&#32780;&#19988;&#19981;&#21463;&#20986;&#29256;&#22330;&#25152;&#21644;&#23398;&#31185;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38544;&#34255;&#24341;&#29992;&#30340;&#26222;&#36941;&#24615;&#19981;&#26159;&#30001;&#24341;&#29992;&#35745;&#25968;&#39537;&#21160;&#30340;&#65292;&#32780;&#26159;&#30001;&#20110;&#25163;&#31295;&#25991;&#26412;&#20013;&#23545;&#35805;&#39064;&#30340;&#35752;&#35770;&#31243;&#24230;&#20915;&#23450;&#30340;&#65292;&#36825;&#34920;&#26126;&#19968;&#20010;&#21457;&#29616;&#34987;&#35752;&#35770;&#24471;&#36234;&#22810;&#65292;&#23427;&#22312;&#26631;&#20934;&#30340;&#24341;&#25991;&#20998;&#26512;&#20013;&#23601;&#36234;&#19981;&#21487;&#35265;&#12290;&#38544;&#34255;&#24341;&#29992;&#25351;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
References, the mechanism scientists rely on to signal previous knowledge, lately have turned into widely used and misused measures of scientific impact. Yet, when a discovery becomes common knowledge, citations suffer from obliteration by incorporation. This leads to the concept of hidden citation, representing a clear textual credit to a discovery without a reference to the publication embodying it. Here, we rely on unsupervised interpretable machine learning applied to the full text of each paper to systematically identify hidden citations. We find that for influential discoveries hidden citations outnumber citation counts, emerging regardless of publishing venue and discipline. We show that the prevalence of hidden citations is not driven by citation counts, but rather by the degree of the discourse on the topic within the text of the manuscripts, indicating that the more discussed is a discovery, the less visible it is to standard bibliometric analysis. Hidden citations indicate t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27979;&#37327;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#36827;&#34892;&#20943;&#36731;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.16176</link><description>&lt;p&gt;
&#36890;&#36807;&#22238;&#28335;&#27861;&#32416;&#27491;&#65292;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Correction with Backtracking Reduces Hallucination in Summarization. (arXiv:2310.16176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27979;&#37327;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#36827;&#34892;&#20943;&#36731;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#28304;&#25991;&#20214;&#30340;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#65292;&#26082;&#31616;&#27905;&#21448;&#20445;&#30041;&#37325;&#35201;&#20803;&#32032;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#31070;&#32463;&#25991;&#26412;&#25688;&#35201;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65288;&#25110;&#26356;&#20934;&#30830;&#22320;&#35828;&#26159;&#28151;&#28102;&#65289;&#65292;&#21363;&#29983;&#25104;&#30340;&#25688;&#35201;&#21253;&#21547;&#28304;&#25991;&#20214;&#20013;&#27809;&#26377;&#26681;&#25454;&#30340;&#32454;&#33410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#27493;&#39588;&#65306;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#36731;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#27979;&#37327;&#26377;&#20851;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#31616;&#21333;&#32479;&#35745;&#20449;&#24687;&#21487;&#20197;&#23454;&#29616;&#21069;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#22312;&#20943;&#36731;&#24187;&#35273;&#26041;&#38754;&#30340;&#24778;&#20154;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25991;&#26412;&#25688;&#35201;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstractive summarization aims at generating natural language summaries of a source document that are succinct while preserving the important elements. Despite recent advances, neural text summarization models are known to be susceptible to hallucinating (or more correctly confabulating), that is to produce summaries with details that are not grounded in the source document. In this paper, we introduce a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization. The approach is based on two steps: hallucination detection and mitigation. We show that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words. Further, we demonstrate that straight-forward backtracking is surprisingly effective at mitigation. We thoroughly evaluate the proposed method with prior art on three benchmark datasets for text summarization. The results show that CoBa is effective and efficient in reducing hall
&lt;/p&gt;</description></item><item><title>WojoodNER-2023&#26159;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20849;&#20139;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#21644;&#23376;&#20219;&#21153;&#65292;&#20197;&#20419;&#36827;NER&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#26368;&#32456;&#33719;&#32988;&#30340;&#22242;&#38431;&#22312;FlatNER&#21644;NestedNER&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16153</link><description>&lt;p&gt;
WojoodNER 2023: &#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20849;&#20139;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
WojoodNER 2023: The First Arabic Named Entity Recognition Shared Task. (arXiv:2310.16153v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16153
&lt;/p&gt;
&lt;p&gt;
WojoodNER-2023&#26159;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20849;&#20139;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#21644;&#23376;&#20219;&#21153;&#65292;&#20197;&#20419;&#36827;NER&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#26368;&#32456;&#33719;&#32988;&#30340;&#22242;&#38431;&#22312;FlatNER&#21644;NestedNER&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;WojoodNER-2023&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20849;&#20139;&#20219;&#21153;&#12290;WojoodNER-2023&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#38463;&#25289;&#20271;NER&#65292;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;NER&#25968;&#25454;&#38598;&#65288;&#21363;Wojood&#65289;&#21644;&#26088;&#22312;&#20419;&#36827;&#19981;&#21516;NER&#26041;&#27861;&#20043;&#38388;&#26377;&#24847;&#20041;&#27604;&#36739;&#30340;&#23376;&#20219;&#21153;&#23450;&#20041;&#12290;WojoodNER-2023&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;FlatNER&#21644;NestedNER&#12290;&#20849;&#26377;45&#20010;&#29420;&#31435;&#22242;&#38431;&#27880;&#20876;&#21442;&#21152;&#27492;&#20849;&#20139;&#20219;&#21153;&#65292;&#20854;&#20013;&#26377;11&#20010;&#22242;&#38431;&#22312;&#27979;&#35797;&#38454;&#27573;&#31215;&#26497;&#21442;&#19982;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;11&#20010;&#22242;&#38431;&#21442;&#19982;&#20102;FlatNER&#65292;&#32780;8&#20010;&#22242;&#38431;&#22788;&#29702;&#20102;NestedNER&#12290;&#33719;&#32988;&#30340;&#22242;&#38431;&#22312;FlatNER&#21644;NestedNER&#20013;&#20998;&#21035;&#36798;&#21040;&#20102;91.96&#21644;93.73&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present WojoodNER-2023, the first Arabic Named Entity Recognition (NER) Shared Task. The primary focus of WojoodNER-2023 is on Arabic NER, offering novel NER datasets (i.e., Wojood) and the definition of subtasks designed to facilitate meaningful comparisons between different NER approaches. WojoodNER-2023 encompassed two Subtasks: FlatNER and NestedNER. A total of 45 unique teams registered for this shared task, with 11 of them actively participating in the test phase. Specifically, 11 teams participated in FlatNER, while $8$ teams tackled NestedNER. The winning teams achieved F1 scores of 91.96 and 93.73 in FlatNER and NestedNER, respectively.
&lt;/p&gt;</description></item><item><title>PreWoMe&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#38382;&#31572;&#20013;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#38382;&#39064;&#20013;&#30340;&#39044;&#35774;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#24037;&#20316;&#35760;&#24518;&#26469;&#29983;&#25104;&#21453;&#39304;&#21644;&#34892;&#21160;&#65292;&#19981;&#20165;&#33021;&#26377;&#25928;&#35299;&#20915;&#35823;&#23548;&#24615;&#38382;&#39064;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#22788;&#29702;&#27491;&#24120;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#38382;&#31572;&#22330;&#26223;&#20013;&#21033;&#29992;&#39044;&#35774;&#12289;&#21453;&#39304;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16147</link><description>&lt;p&gt;
PreWoMe:&#21033;&#29992;&#39044;&#35774;&#20026;&#38271;&#31687;&#38382;&#31572;&#20013;&#30340;&#24037;&#20316;&#35760;&#24518;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
PreWoMe: Exploiting Presuppositions as Working Memory for Long Form Question Answering. (arXiv:2310.16147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16147
&lt;/p&gt;
&lt;p&gt;
PreWoMe&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#38382;&#31572;&#20013;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#38382;&#39064;&#20013;&#30340;&#39044;&#35774;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#24037;&#20316;&#35760;&#24518;&#26469;&#29983;&#25104;&#21453;&#39304;&#21644;&#34892;&#21160;&#65292;&#19981;&#20165;&#33021;&#26377;&#25928;&#35299;&#20915;&#35823;&#23548;&#24615;&#38382;&#39064;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#22788;&#29702;&#27491;&#24120;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#38382;&#31572;&#22330;&#26223;&#20013;&#21033;&#29992;&#39044;&#35774;&#12289;&#21453;&#39304;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#31687;&#38382;&#31572;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#24120;&#24120;&#30001;&#20110;&#38382;&#39064;&#20013;&#30340;&#27169;&#31946;&#25110;&#38169;&#35823;&#39044;&#35774;&#32780;&#35823;&#23548;&#12290;&#34429;&#28982;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#35823;&#23548;&#24615;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#38024;&#23545;&#30340;&#26159;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24456;&#38590;&#36866;&#24212;&#23454;&#38469;&#24773;&#20917;&#20013;&#19981;&#21487;&#39044;&#27979;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PreWoMe&#65292;&#36825;&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#20309;&#31867;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#12290;PreWoMe&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#25552;&#21462;&#38382;&#39064;&#20013;&#30340;&#39044;&#35774;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#24037;&#20316;&#35760;&#24518;&#26469;&#29983;&#25104;&#23545;&#38382;&#39064;&#30340;&#21453;&#39304;&#21644;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PreWoMe&#19981;&#20165;&#22312;&#35299;&#20915;&#35823;&#23548;&#24615;&#38382;&#39064;&#26041;&#38754;&#26377;&#25928;&#65292;&#32780;&#19988;&#22312;&#22788;&#29702;&#27491;&#24120;&#38382;&#39064;&#26041;&#38754;&#20063;&#24456;&#26377;&#25928;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#38382;&#31572;&#22330;&#26223;&#20013;&#21033;&#29992;&#39044;&#35774;&#12289;&#21453;&#39304;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information-seeking questions in long-form question answering (LFQA) often prove misleading due to ambiguity or false presupposition in the question. While many existing approaches handle misleading questions, they are tailored to limited questions, which are insufficient in a real-world setting with unpredictable input characteristics. In this work, we propose PreWoMe, a unified approach capable of handling any type of information-seeking question. The key idea of PreWoMe involves extracting presuppositions in the question and exploiting them as working memory to generate feedback and action about the question. Our experiment shows that PreWoMe is effective not only in tackling misleading questions but also in handling normal ones, thereby demonstrating the effectiveness of leveraging presuppositions, feedback, and action for real-world QA settings.
&lt;/p&gt;</description></item><item><title>Clinfo.ai&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#31185;&#23398;&#25991;&#29486;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#21644;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#65292;&#21457;&#24067;&#20102;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.16146</link><description>&lt;p&gt;
Clinfo.ai:&#29992;&#31185;&#23398;&#25991;&#29486;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#30340;&#24320;&#28304;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature. (arXiv:2310.16146v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16146
&lt;/p&gt;
&lt;p&gt;
Clinfo.ai&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#31185;&#23398;&#25991;&#29486;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#21644;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#65292;&#21457;&#24067;&#20102;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#23398;&#25991;&#29486;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#24456;&#38590;&#21450;&#26102;&#36319;&#19978;&#24182;&#24635;&#32467;&#26368;&#36817;&#30340;&#30456;&#20851;&#21457;&#29616;&#12290;&#34429;&#28982;&#29616;&#22312;&#23384;&#22312;&#20960;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38381;&#28304;&#25688;&#35201;&#24037;&#20855;&#65292;&#20294;&#20854;&#36755;&#20986;&#32467;&#26524;&#32570;&#20047;&#20005;&#26684;&#21644;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#21644;&#36866;&#24403;&#30340;&#22522;&#20934;&#20219;&#21153;&#26469;&#35780;&#20272;&#36825;&#20123;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#36129;&#29486;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#25105;&#20204;&#21457;&#24067;&#20102;&#21517;&#20026;Clinfo.ai&#30340;&#24320;&#28304;WebApp&#65292;&#23427;&#22522;&#20110;&#21160;&#24577;&#26816;&#32034;&#30340;&#31185;&#23398;&#25991;&#29486;&#22238;&#31572;&#20020;&#24202;&#38382;&#39064;&#65307;&#25105;&#20204;&#25351;&#23450;&#20102;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#21644;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#65292;&#20197;&#35780;&#20272;&#36825;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;LLM&#31995;&#32479;&#30340;&#24615;&#33021;&#65307;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;200&#20010;&#38382;&#39064;&#21450;&#20854;&#23545;&#24212;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;PubMed&#26816;&#32034;&#21644;&#32508;&#36848;&#65288;PubMedRS-200&#65289;&#65307;&#24182;&#25253;&#21578;&#20102;Cli&#30340;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quickly-expanding nature of published medical literature makes it challenging for clinicians and researchers to keep up with and summarize recent, relevant findings in a timely manner. While several closed-source summarization tools based on large language models (LLMs) now exist, rigorous and systematic evaluations of their outputs are lacking. Furthermore, there is a paucity of high-quality datasets and appropriate benchmark tasks with which to evaluate these tools. We address these issues with four contributions: we release Clinfo.ai, an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature; we specify an information retrieval and abstractive summarization task to evaluate the performance of such retrieval-augmented LLM systems; we release a dataset of 200 questions and corresponding answers derived from published systematic reviews, which we name PubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for Cli
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#33258;&#25105;&#27880;&#24847;&#22836;&#32039;&#23494;&#27169;&#25311;&#20102;&#35748;&#30693;&#29702;&#35770;&#20013;&#20551;&#35774;&#30340;&#35760;&#24518;&#31995;&#32479;&#65292;&#24182;&#25429;&#25417;&#21040;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#20013;&#30340;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2310.16142</link><description>&lt;p&gt;
&#26377;&#38480;&#35760;&#24518;&#23481;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#20013;&#30340;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing. (arXiv:2310.16142v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16142
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#33258;&#25105;&#27880;&#24847;&#22836;&#32039;&#23494;&#27169;&#25311;&#20102;&#35748;&#30693;&#29702;&#35770;&#20013;&#20551;&#35774;&#30340;&#35760;&#24518;&#31995;&#32479;&#65292;&#24182;&#25429;&#25417;&#21040;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#20013;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#22256;&#38590;&#30340;&#20004;&#20010;&#26680;&#24515;&#22240;&#32032;&#34987;&#35748;&#20026;&#26159;&#26399;&#26395;&#21644;&#26469;&#33258;&#24037;&#20316;&#35760;&#24518;&#30340;&#26816;&#32034;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#23581;&#35797;&#65292;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#32508;&#21512;&#35748;&#30693;&#27169;&#22411;&#65292;&#23558;&#36825;&#20004;&#20010;&#22240;&#32032;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#20381;&#36182;&#20110;transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#20013;&#22522;&#20110;&#26263;&#31034;&#30340;&#24037;&#20316;&#35760;&#24518;&#26816;&#32034;&#29702;&#35770;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#65288;Ryu and Lewis, 2021&#65289;.&#34429;&#28982;Ryu&#21644;Lewis&#23637;&#31034;&#20102;GPT-2&#30340;&#29305;&#27530;&#33258;&#27880;&#24847;&#22836;&#20013;&#30340;&#27880;&#24847;&#27169;&#24335;&#19982;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#24178;&#25200;&#30340;&#20851;&#38190;&#39044;&#27979;&#19968;&#33268;&#65292;&#36825;&#26159;&#22522;&#20110;&#26263;&#31034;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#20294;&#20182;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#35782;&#21035;&#20986;&#21477;&#27861;&#29305;&#21270;&#30340;&#33258;&#27880;&#24847;&#22836;&#65292;&#24182;&#20570;&#20986;&#35748;&#30693;&#19978;&#19981;&#21512;&#29702;&#30340;&#20551;&#35774;&#65292;&#21363;&#25968;&#30334;&#27425;&#30340;&#20869;&#23384;&#26816;&#32034;&#25805;&#20316;&#26159;&#24182;&#34892;&#36827;&#34892;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#21333;&#20010;&#33258;&#25105;&#27880;&#24847;&#22836;&#30340;&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#26356;&#36148;&#36817;&#35748;&#30693;&#29702;&#35770;&#25152;&#20551;&#35774;&#30340;&#35760;&#24518;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Two of the central factors believed to underpin human sentence processing difficulty are expectations and retrieval from working memory. A recent attempt to create a unified cognitive model integrating these two factors relied on the parallels between the self-attention mechanism of transformer language models and cue-based retrieval theories of working memory in human sentence processing (Ryu and Lewis 2021). While Ryu and Lewis show that attention patterns in specialized attention heads of GPT-2 are consistent with similarity-based interference, a key prediction of cue-based retrieval models, their method requires identifying syntactically specialized attention heads, and makes the cognitively implausible assumption that hundreds of memory retrieval operations take place in parallel. In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories. We show that our model's
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21512;&#25104;&#29615;&#22659;&#29992;&#20110;&#27979;&#35797;&#32842;&#22825;&#27169;&#22411;&#30340;&#24773;&#26223;&#29702;&#35299;&#33021;&#21147;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36861;&#36394;&#21644;&#21015;&#20030;&#29615;&#22659;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#24615;&#33021;&#27169;&#24335;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2310.16135</link><description>&lt;p&gt;
&#33021;&#36319;&#19978;&#25105;&#21527;&#65311;&#22312;ChatGPT&#20013;&#27979;&#35797;&#24773;&#26223;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Can You Follow Me? Testing Situational Understanding in ChatGPT. (arXiv:2310.16135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16135
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21512;&#25104;&#29615;&#22659;&#29992;&#20110;&#27979;&#35797;&#32842;&#22825;&#27169;&#22411;&#30340;&#24773;&#26223;&#29702;&#35299;&#33021;&#21147;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36861;&#36394;&#21644;&#21015;&#20030;&#29615;&#22659;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#24615;&#33021;&#27169;&#24335;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21477;&#23376;&#30340;&#21547;&#20041;&#24182;&#36866;&#26102;&#26356;&#26032;&#20449;&#24687;&#29366;&#24577;&#65292;&#21363;&#25105;&#20204;&#25152;&#31216;&#30340;"&#24773;&#26223;&#29702;&#35299;"&#65288;SU&#65289;&#65292;&#23545;&#20110;&#31867;&#20284;ChatGPT&#30340;&#32842;&#22825;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#12290;SU&#23545;&#20110;&#32842;&#22825;&#27169;&#22411;&#29305;&#21035;&#37325;&#35201;&#65292;&#21487;&#20197;&#23454;&#29616;&#20154;&#26426;&#20043;&#38388;&#30340;&#19968;&#33268;&#12289;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#23545;&#35805;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;&#38750;&#32842;&#22825;&#26426;&#22120;&#20154;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23384;&#22312;&#26576;&#20123;SU&#38480;&#21046;&#65292;&#20294;&#23545;&#36825;&#20123;&#38480;&#21046;&#30340;&#31243;&#24230;&#21644;&#21407;&#22240;&#23578;&#19981;&#20102;&#35299;&#65292;&#24182;&#19988;&#24403;&#21069;&#32842;&#22825;&#27169;&#22411;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27979;&#35797;&#32842;&#22825;&#23548;&#21521;&#27169;&#22411;SU&#30340;&#21512;&#25104;&#29615;&#22659;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36319;&#36394;&#21644;&#21015;&#20030;&#29615;&#22659;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;SU&#30340;&#21463;&#25511;&#21644;&#31995;&#32479;&#21270;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#29615;&#22659;&#36824;&#20801;&#35768;&#23545;&#27169;&#22411;&#24615;&#33021;&#21160;&#24577;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#24615;&#33021;&#27169;&#24335;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding sentence meanings and updating information states appropriately across time -- what we call "situational understanding" (SU) -- is a critical ability for human-like AI agents. SU is essential in particular for chat models, such as ChatGPT, to enable consistent, coherent, and effective dialogue between humans and AI. Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs), but the extent and causes of these limitations are not well understood, and capabilities of current chat-based models in this domain have not been explored. In this work we tackle these questions, proposing a novel synthetic environment for SU testing which allows us to do controlled and systematic testing of SU in chat-oriented models, through assessment of models' ability to track and enumerate environment states. Our environment also allows for close analysis of dynamics of model performance, to better understand underlying causes for performance patterns. We 
&lt;/p&gt;</description></item><item><title>GenKIE&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#25991;&#26723;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#24369;&#30417;&#30563;&#20449;&#21495;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#22312;&#19981;&#38656;&#35201;&#35760;&#21495;&#32423;&#26631;&#27880;&#21644;&#21487;&#33258;&#21160;&#32416;&#27491;OCR&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2310.16131</link><description>&lt;p&gt;
GenKIE: &#24378;&#22823;&#30340;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#25991;&#26723;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
GenKIE: Robust Generative Multimodal Document Key Information Extraction. (arXiv:2310.16131v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16131
&lt;/p&gt;
&lt;p&gt;
GenKIE&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#25991;&#26723;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#24369;&#30417;&#30563;&#20449;&#21495;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#22312;&#19981;&#38656;&#35201;&#35760;&#21495;&#32423;&#26631;&#27880;&#21644;&#21487;&#33258;&#21160;&#32416;&#27491;OCR&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#20174;&#25195;&#25551;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65288;KIE&#65289;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#19968;&#20123;&#26368;&#36817;&#30340;KIE&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26159;&#22522;&#20110;&#21028;&#21035;&#27169;&#22411;&#26500;&#24314;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#22788;&#29702;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#38169;&#35823;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#36153;&#21147;&#30340;&#35760;&#21495;&#32423;&#26631;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#21517;&#20026;GenKIE&#65292;&#26469;&#35299;&#20915;KIE&#20219;&#21153;&#12290;GenKIE&#26159;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#23884;&#20837;&#35270;&#35273;&#12289;&#24067;&#23616;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#35299;&#30721;&#22120;&#29983;&#25104;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#35774;&#35745;&#33391;&#22909;&#30340;&#25552;&#31034;&#34987;&#21033;&#29992;&#26469;&#23558;&#26631;&#31614;&#35821;&#20041;&#20316;&#20026;&#24369;&#30417;&#30563;&#20449;&#21495;&#24182;&#28608;&#21457;&#20851;&#38190;&#20449;&#24687;&#30340;&#29983;&#25104;&#12290;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20010;&#26174;&#30528;&#20248;&#21183;&#26159;&#23427;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#19981;&#38656;&#35201;&#35760;&#21495;&#32423;&#30340;&#31890;&#24230;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Key information extraction (KIE) from scanned documents has gained increasing attention because of its applications in various domains. Although promising results have been achieved by some recent KIE approaches, they are usually built based on discriminative models, which lack the ability to handle optical character recognition (OCR) errors and require laborious token-level labelling. In this paper, we propose a novel generative end-to-end model, named GenKIE, to address the KIE task. GenKIE is a sequence-to-sequence multimodal generative model that utilizes multimodal encoders to embed visual, layout and textual features and a decoder to generate the desired output. Well-designed prompts are leveraged to incorporate the label semantics as the weakly supervised signals and entice the generation of the key information. One notable advantage of the generative model is that it enables automatic correction of OCR errors. Besides, token-level granular annotation is not required. Extensive 
&lt;/p&gt;</description></item><item><title>Octopus&#26159;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#21644;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;AraT5v2&#27169;&#22411;&#21644;&#31995;&#32479;&#35757;&#32451;&#65292;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#19979;&#23454;&#29616;&#20102;&#20248;&#20110;&#31454;&#20105;&#22522;&#20934;&#32447;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16127</link><description>&lt;p&gt;
Octopus: &#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#21644;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Octopus: A Multitask Model and Toolkit for Arabic Natural Language Generation. (arXiv:2310.16127v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16127
&lt;/p&gt;
&lt;p&gt;
Octopus&#26159;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#21644;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;AraT5v2&#27169;&#22411;&#21644;&#31995;&#32479;&#35757;&#32451;&#65292;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#19979;&#23454;&#29616;&#20102;&#20248;&#20110;&#31454;&#20105;&#22522;&#20934;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#38463;&#25289;&#20271;&#25991;&#26412;&#24182;&#20135;&#29983;&#20154;&#31867;&#21270;&#30340;&#22238;&#22797;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#27169;&#22411;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#24037;&#20855;&#21253;&#65292;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;&#21040;&#25991;&#26412;Transformer&#27169;&#22411;&#65292;&#21517;&#20026;AraT5v2&#12290;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#22312;&#24191;&#27867;&#32780;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#35757;&#32451;&#65292;&#21033;&#29992;&#20102;&#25193;&#23637;&#30340;&#24207;&#21015;&#38271;&#24230;&#36798;&#21040;2,048&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#26080;&#30417;&#30563;&#12289;&#26377;&#30417;&#30563;&#21644;&#32852;&#21512;&#35757;&#32451;&#65292;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#31454;&#20105;&#22522;&#20934;&#32447;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;Octopus&#65292;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#22871;&#20214;&#21644;&#21629;&#20196;&#34892;&#24037;&#20855;&#65292;&#19987;&#38376;&#38024;&#23545;&#20843;&#39033;&#38463;&#25289;&#20271;&#35821;&#29983;&#25104;&#20219;&#21153;&#65292;&#20840;&#37096;&#21033;&#29992;&#21333;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#20844;&#20849;&#20195;&#30721;&#24211;&#19978;&#21457;&#24067;&#20102;&#27169;&#22411;&#21644;&#24037;&#20855;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding Arabic text and generating human-like responses is a challenging endeavor. While many researchers have proposed models and solutions for individual problems, there is an acute shortage of a comprehensive Arabic natural language generation toolkit that is capable of handling a wide range of tasks. In this work, we present a novel Arabic text-to-text Transformer model, namely AraT5v2. Our new model is methodically trained on extensive and diverse data, utilizing an extended sequence length of 2,048 tokens. We explore various pretraining strategies including unsupervised, supervised, and joint pertaining, under both single and multitask settings. Our models outperform competitive baselines with large margins. We take our work one step further by developing and publicly releasing Octopus, a Python-based package and command-line toolkit tailored for eight Arabic generation tasks all exploiting a single model. We release the models and the toolkit on our public repository.
&lt;/p&gt;</description></item><item><title>NADI 2023&#26159;&#31532;&#22235;&#23626;Nuanced&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#20849;&#20139;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#25512;&#36827;&#38463;&#25289;&#20271;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#20849;&#26377;58&#20010;&#22242;&#38431;&#21442;&#19982;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#26041;&#35328;&#35782;&#21035;&#21644;&#26041;&#35328;&#21040;MSA&#26426;&#22120;&#32763;&#35793;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.16117</link><description>&lt;p&gt;
NADI 2023&#65306;&#31532;&#22235;&#23626;Nuanced&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#20849;&#20139;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
NADI 2023: The Fourth Nuanced Arabic Dialect Identification Shared Task. (arXiv:2310.16117v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16117
&lt;/p&gt;
&lt;p&gt;
NADI 2023&#26159;&#31532;&#22235;&#23626;Nuanced&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#20849;&#20139;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#25512;&#36827;&#38463;&#25289;&#20271;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#20849;&#26377;58&#20010;&#22242;&#38431;&#21442;&#19982;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#26041;&#35328;&#35782;&#21035;&#21644;&#26041;&#35328;&#21040;MSA&#26426;&#22120;&#32763;&#35793;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#31532;&#22235;&#23626;Nuanced&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#20849;&#20139;&#20219;&#21153;&#65288;NADI 2023&#65289;&#30340;&#21457;&#29616;&#12290;NADI&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#22312;&#26631;&#20934;&#26465;&#20214;&#19979;&#21019;&#24314;&#21512;&#20316;&#31454;&#20105;&#30340;&#26426;&#20250;&#65292;&#24110;&#21161;&#25512;&#36827;&#26368;&#20808;&#36827;&#30340;&#38463;&#25289;&#20271;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#23427;&#19987;&#27880;&#20110;&#38463;&#25289;&#20271;&#26041;&#35328;&#65292;&#25552;&#20379;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23450;&#20041;&#20102;&#20801;&#35768;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#26377;&#24847;&#20041;&#27604;&#36739;&#30340;&#23376;&#20219;&#21153;&#12290;NADI 2023&#38024;&#23545;&#26041;&#35328;&#35782;&#21035;&#65288;&#23376;&#20219;&#21153;1&#65289;&#21644;&#26041;&#35328;&#21040;MSA&#26426;&#22120;&#32763;&#35793;&#65288;&#23376;&#20219;&#21153;2&#21644;&#23376;&#20219;&#21153;3&#65289;&#12290;&#20849;&#26377;58&#20010;&#29420;&#29305;&#30340;&#22242;&#38431;&#27880;&#20876;&#21442;&#21152;&#20102;&#20849;&#20139;&#20219;&#21153;&#65292;&#20854;&#20013;&#26377;18&#20010;&#22242;&#38431;&#21442;&#21152;&#20102;&#65288;&#27979;&#35797;&#38454;&#27573;&#26377;76&#20010;&#26377;&#25928;&#30340;&#25552;&#20132;&#65289;&#12290;&#20854;&#20013;&#65292;16&#20010;&#22242;&#38431;&#21442;&#21152;&#20102;&#23376;&#20219;&#21153;1&#65292;5&#20010;&#22242;&#38431;&#21442;&#21152;&#20102;&#23376;&#20219;&#21153;2&#65292;3&#20010;&#22242;&#38431;&#21442;&#21152;&#20102;&#23376;&#20219;&#21153;3&#12290;&#33719;&#32988;&#30340;&#22242;&#38431;&#22312;&#23376;&#20219;&#21153;1&#19978;&#23454;&#29616;&#20102;87.27&#30340;F1&#24471;&#20998;&#65292;&#22312;&#23376;&#20219;&#21153;2&#19978;&#23454;&#29616;&#20102;14.76&#30340;Bleu&#24471;&#20998;&#65292;&#22312;&#23376;&#20219;&#21153;3&#19978;&#23454;&#29616;&#20102;21.10&#30340;Bleu&#24471;&#20998;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19977;&#20010;&#23376;&#20219;&#21153;&#37117;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe the findings of the fourth Nuanced Arabic Dialect Identification Shared Task (NADI 2023). The objective of NADI is to help advance state-of-the-art Arabic NLP by creating opportunities for teams of researchers to collaboratively compete under standardized conditions. It does so with a focus on Arabic dialects, offering novel datasets and defining subtasks that allow for meaningful comparisons between different approaches. NADI 2023 targeted both dialect identification (Subtask 1) and dialect-to-MSA machine translation (Subtask 2 and Subtask 3). A total of 58 unique teams registered for the shared task, of whom 18 teams have participated (with 76 valid submissions during test phase). Among these, 16 teams participated in Subtask 1, 5 participated in Subtask 2, and 3 participated in Subtask 3. The winning teams achieved 87.27  F1 on Subtask 1, 14.76 Bleu in Subtask 2, and 21.10 Bleu in Subtask 3, respectively. Results show that all three subtasks remain challenging, thereby m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#25991;&#26723;&#29983;&#25104;&#26426;&#21046;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#38454;&#25552;&#31034;&#23545;&#25239;&#20316;&#32773;&#21435;&#21311;&#21517;&#25915;&#20987;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#19979;&#28216;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26426;&#21046;&#22312;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#21516;&#26102;&#33021;&#22815;&#23436;&#20840;&#24674;&#22797;&#28165;&#27905;&#30340;&#24773;&#24863;&#20998;&#25968;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.16111</link><description>&lt;p&gt;
&#20351;&#29992;&#38646;&#38454;&#25552;&#31034;&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#25991;&#26723;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Locally Differentially Private Document Generation Using Zero Shot Prompting. (arXiv:2310.16111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#25991;&#26723;&#29983;&#25104;&#26426;&#21046;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#38454;&#25552;&#31034;&#23545;&#25239;&#20316;&#32773;&#21435;&#21311;&#21517;&#25915;&#20987;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#19979;&#28216;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26426;&#21046;&#22312;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#21516;&#26102;&#33021;&#22815;&#23436;&#20840;&#24674;&#22797;&#28165;&#27905;&#30340;&#24773;&#24863;&#20998;&#25968;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30740;&#31350;&#24050;&#32463;&#24378;&#35843;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20026;&#38544;&#31169;&#20445;&#25252;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DP-Prompt&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#38454;&#25552;&#31034;&#26469;&#23545;&#25239;&#20316;&#32773;&#21435;&#21311;&#21517;&#25915;&#20987;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#19979;&#28216;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#24403;DP-Prompt&#19982;&#20687;ChatGPT&#65288;gpt-3.5&#65289;&#36825;&#26679;&#30340;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21435;&#21311;&#21517;&#25915;&#20987;&#25104;&#21151;&#29575;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#19988;&#23613;&#31649;&#20854;&#35774;&#35745;&#26356;&#31616;&#21333;&#65292;&#20294;&#23427;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24456;&#22823;&#31243;&#24230;&#12290;&#20363;&#22914;&#65292;&#22312;IMDB&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;DP-Prompt&#65288;&#20351;&#29992;ChatGPT&#65289;&#23436;&#20840;&#24674;&#22797;&#20102;&#28165;&#27905;&#30340;&#24773;&#24863;F1&#20998;&#25968;&#65292;&#24182;&#22312;&#38745;&#24577;&#25915;&#20987;&#32773;&#30340;&#20316;&#32773;&#35782;&#21035;F1&#20998;&#25968;&#19978;&#23454;&#29616;&#20102;46&#65285;&#30340;&#38477;&#20302;&#21644;26&#65285;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\% reduction in author identification F1 score against static attackers and a 26\% reduc
&lt;/p&gt;</description></item><item><title>CR-COPEC&#26159;&#19968;&#20010;&#20174;&#36130;&#21153;&#25253;&#21578;&#20013;&#23398;&#20064;&#20225;&#19994;&#32489;&#25928;&#21464;&#21270;&#30340;&#22240;&#26524;&#35299;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#20026;&#20010;&#20154;&#25237;&#36164;&#32773;&#21644;&#20998;&#26512;&#24072;&#25552;&#20379;&#37325;&#35201;&#30340;&#20449;&#24687;&#36164;&#28304;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#19981;&#21516;&#34892;&#19994;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.16095</link><description>&lt;p&gt;
CR-COPEC: &#20174;&#36130;&#21153;&#25253;&#21578;&#23398;&#20064;&#20225;&#19994;&#32489;&#25928;&#21464;&#21270;&#30340;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
CR-COPEC: Causal Rationale of Corporate Performance Changes to Learn from Financial Reports. (arXiv:2310.16095v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16095
&lt;/p&gt;
&lt;p&gt;
CR-COPEC&#26159;&#19968;&#20010;&#20174;&#36130;&#21153;&#25253;&#21578;&#20013;&#23398;&#20064;&#20225;&#19994;&#32489;&#25928;&#21464;&#21270;&#30340;&#22240;&#26524;&#35299;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#20026;&#20010;&#20154;&#25237;&#36164;&#32773;&#21644;&#20998;&#26512;&#24072;&#25552;&#20379;&#37325;&#35201;&#30340;&#20449;&#24687;&#36164;&#28304;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#19981;&#21516;&#34892;&#19994;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CR-COPEC&#65292;&#21363;&#20174;&#36130;&#21153;&#25253;&#21578;&#20013;&#23398;&#20064;&#20225;&#19994;&#32489;&#25928;&#21464;&#21270;&#30340;&#22240;&#26524;&#35299;&#37322;&#12290;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#22823;&#35268;&#27169;&#39046;&#22495;&#33258;&#36866;&#24212;&#22240;&#26524;&#21477;&#23376;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;&#20225;&#19994;&#30340;&#36130;&#21153;&#32489;&#25928;&#21464;&#21270;&#12290;CR-COPEC&#30340;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#39318;&#20808;&#65292;&#23427;&#20174;&#32654;&#22269;&#20844;&#21496;&#30340;10-K&#24180;&#24230;&#25253;&#21578;&#20013;&#26816;&#27979;&#22240;&#26524;&#35299;&#37322;&#65292;&#36825;&#20123;&#25253;&#21578;&#25353;&#29031;&#27491;&#24335;&#30340;&#20250;&#35745;&#20934;&#21017;&#30001;&#19987;&#23478;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#20010;&#20154;&#25237;&#36164;&#32773;&#21644;&#20998;&#26512;&#24072;&#65292;&#20316;&#20026;&#25237;&#36164;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#37325;&#35201;&#20449;&#24687;&#36164;&#28304;&#65292;&#26080;&#38656;&#22823;&#37327;&#38405;&#35835;&#25152;&#26377;&#25991;&#20214;&#12290;&#20854;&#27425;&#65292;&#23427;&#20180;&#32454;&#32771;&#34385;&#20102;&#24433;&#21709;&#21313;&#20108;&#20010;&#34892;&#19994;&#20844;&#21496;&#36130;&#21153;&#32489;&#25928;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;CR-COPEC&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#34892;&#19994;&#30340;&#29420;&#29305;&#21465;&#36848;&#65292;&#21306;&#20998;&#21508;&#20010;&#34892;&#19994;&#20013;&#30340;&#22240;&#26524;&#21477;&#23376;&#12290;&#25105;&#20204;&#36824;&#23545;CR-COPEC&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce CR-COPEC called Causal Rationale of Corporate Performance Changes from financial reports. This is a comprehensive large-scale domain-adaptation causal sentence dataset to detect financial performance changes of corporate. CR-COPEC contributes to two major achievements. First, it detects causal rationale from 10-K annual reports of the U.S. companies, which contain experts' causal analysis following accounting standards in a formal manner. This dataset can be widely used by both individual investors and analysts as material information resources for investing and decision making without tremendous effort to read through all the documents. Second, it carefully considers different characteristics which affect the financial performance of companies in twelve industries. As a result, CR-COPEC can distinguish causal sentences in various industries by taking unique narratives in each industry into consideration. We also provide an extensive analysis of how well CR-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#25191;&#34892;Web&#36719;&#20214;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27493;&#39588;&#24615;&#29983;&#25104;&#23567;&#22411;&#31243;&#24207;&#26469;&#23454;&#29616;&#23545;&#28857;&#20987;&#12289;&#28378;&#21160;&#21644;&#25991;&#26412;&#36755;&#20837;&#25805;&#20316;&#30340;&#25511;&#21046;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;MiniWob++&#22522;&#20934;&#27979;&#35797;&#20013;&#36890;&#36807;&#19968;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#23601;&#33021;&#36798;&#21040;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16042</link><description>&lt;p&gt;
WebWISE&#65306;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Web&#30028;&#38754;&#25511;&#21046;&#21644;&#39034;&#24207;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
WebWISE: Web Interface Control and Sequential Exploration with Large Language Models. (arXiv:2310.16042v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#25191;&#34892;Web&#36719;&#20214;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27493;&#39588;&#24615;&#29983;&#25104;&#23567;&#22411;&#31243;&#24207;&#26469;&#23454;&#29616;&#23545;&#28857;&#20987;&#12289;&#28378;&#21160;&#21644;&#25991;&#26412;&#36755;&#20837;&#25805;&#20316;&#30340;&#25511;&#21046;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;MiniWob++&#22522;&#20934;&#27979;&#35797;&#20013;&#36890;&#36807;&#19968;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#23601;&#33021;&#36798;&#21040;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#25191;&#34892;&#28857;&#20987;&#12289;&#28378;&#21160;&#21644;&#25991;&#26412;&#36755;&#20837;&#25805;&#20316;&#30340;Web&#36719;&#20214;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#22914;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25110;&#27169;&#20223;&#23398;&#20064;&#65292;&#35757;&#32451;&#25928;&#29575;&#20302;&#19988;&#29305;&#23450;&#20110;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#31579;&#36873;&#21518;&#30340;&#25991;&#26723;&#23545;&#35937;&#27169;&#22411;&#65288;DOM&#65289;&#20803;&#32032;&#20316;&#20026;&#35266;&#23519;&#65292;&#36880;&#27493;&#25191;&#34892;&#20219;&#21153;&#65292;&#26681;&#25454;&#24403;&#21069;&#35266;&#23519;&#29983;&#25104;&#23567;&#22411;&#31243;&#24207;&#12290;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#25163;&#21160;&#25552;&#20379;&#30340;&#31034;&#20363;&#20013;&#33719;&#30410;&#65292;&#25110;&#32773;&#20174;&#25104;&#21151;&#30340;&#38646;&#26679;&#20363;&#35797;&#39564;&#20013;&#29983;&#25104;&#33258;&#21160;&#31034;&#20363;&#12290;&#25105;&#20204;&#22312;MiniWob++&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#21482;&#26377;&#19968;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#30340;WebWISE&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#20854;&#20182;&#38656;&#35201;&#35768;&#22810;&#28436;&#31034;&#25110;&#35797;&#39564;&#30340;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper investigates using a Large Language Model (LLM) to automatically perform web software tasks using click, scroll, and text input operations. Previous approaches, such as reinforcement learning (RL) or imitation learning, are inefficient to train and task-specific. Our method uses filtered Document Object Model (DOM) elements as observations and performs tasks step-by-step, sequentially generating small programs based on the current observations. We use in-context learning, either benefiting from a single manually provided example, or an automatically generated example based on a successful zero-shot trial. We evaluate the proposed method on the MiniWob++ benchmark. With only one in-context example, our WebWISE method achieves similar or better performance than other methods that require many demonstrations or trials.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24050;&#35265;&#21644;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#37117;&#33021;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.15970</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Accented Speech Recognition With Accent-specific Codebooks. (arXiv:2310.15970v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24050;&#35265;&#21644;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#37117;&#33021;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21475;&#38899;&#23545;&#20110;&#29616;&#26377;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#21475;&#38899;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#20005;&#37325;&#38459;&#30861;&#20102;ASR&#30340;&#26222;&#21450;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#36825;&#20123;&#21487;&#23398;&#20064;&#30340;&#20195;&#30721;&#26412;&#25429;&#25417;&#20102;&#21475;&#38899;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#34987;&#25972;&#21512;&#21040;ASR&#32534;&#30721;&#22120;&#23618;&#20013;&#12290;&#27169;&#22411;&#22312;&#24102;&#21475;&#38899;&#30340;&#33521;&#35821;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#27979;&#35797;&#25968;&#25454;&#20013;&#20063;&#21253;&#21547;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#21475;&#38899;&#12290;&#22312;Mozilla Common Voice&#22810;&#21475;&#38899;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#20165;&#22312;&#24050;&#35265;&#30340;&#33521;&#35821;&#21475;&#38899;&#20013;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;&#21333;&#35789;&#38169;&#35823;&#29575;&#30456;&#23545;&#25552;&#21319;&#39640;&#36798;37%&#65289;&#65292;&#32780;&#19988;&#22312;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#20063;&#33719;&#24471;&#20102;5%&#30340;&#30456;&#23545;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;L2Artic&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#35774;&#32622;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech accents pose a significant challenge to state-of-the-art automatic speech recognition (ASR) systems. Degradation in performance across underrepresented accents is a severe deterrent to the inclusive adoption of ASR. In this work, we propose a novel accent adaptation approach for end-to-end ASR systems using cross-attention with a trainable set of codebooks. These learnable codebooks capture accent-specific information and are integrated within the ASR encoder layers. The model is trained on accented English speech, while the test data also contained accents which were not seen during training. On the Mozilla Common Voice multi-accented dataset, we show that our proposed approach yields significant performance gains not only on the seen English accents (up to $37\%$ relative improvement in word error rate) but also on the unseen accents (up to $5\%$ relative improvement in WER). Further, we illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We also compare
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.15694</link><description>&lt;p&gt;
COPF: &#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15694
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#25216;&#26415;&#26159;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;RLHF&#30340;LM&#22312;&#24341;&#20837;&#26032;&#30340;&#26597;&#35810;&#25110;&#21453;&#39304;&#26102;&#38656;&#35201;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20154;&#31867;&#20559;&#22909;&#22312;&#19981;&#21516;&#39046;&#22495;&#25110;&#20219;&#21153;&#20043;&#38388;&#21487;&#33021;&#20250;&#26377;&#25152;&#21464;&#21270;&#12290;&#30001;&#20110;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#19982;&#25968;&#25454;&#38544;&#31169;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#37325;&#26032;&#35757;&#32451;LM&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#23454;&#38469;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25345;&#32493;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#65288;COPF&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#27861;&#20272;&#35745;&#19968;&#31995;&#21015;&#26368;&#20248;&#31574;&#30053;&#65292;&#28982;&#21518;&#36890;&#36807;&#20989;&#25968;&#27491;&#21017;&#21270;&#19981;&#26029;&#25311;&#21512;&#31574;&#30053;&#24207;&#21015;&#12290;COPF&#21253;&#21547;&#19968;&#20010;&#21333;&#19968;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability 
&lt;/p&gt;</description></item><item><title>TCRA-LLM&#26159;&#36890;&#36807;&#27010;&#36848;&#21387;&#32553;&#21644;&#35821;&#20041;&#21387;&#32553;&#20004;&#31181;&#26041;&#27861;&#26469;&#20943;&#23569;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25104;&#26412;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.15556</link><description>&lt;p&gt;
TCRA-LLM: &#29992;&#20110;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#30340;&#20196;&#29260;&#21387;&#32553;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction. (arXiv:2310.15556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15556
&lt;/p&gt;
&lt;p&gt;
TCRA-LLM&#26159;&#36890;&#36807;&#27010;&#36848;&#21387;&#32553;&#21644;&#35821;&#20041;&#21387;&#32553;&#20004;&#31181;&#26041;&#27861;&#26469;&#20943;&#23569;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25104;&#26412;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;ChatGPT&#21457;&#24067;&#20102;API&#20379;&#20844;&#20247;&#20351;&#29992;&#20197;&#26469;&#65292;&#26500;&#24314;&#22312;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20043;&#19978;&#30340;&#24212;&#29992;&#31243;&#24207;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#19968;&#20010;&#27969;&#34892;&#29992;&#27861;&#26159;&#21033;&#29992;&#20854;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#24182;&#29983;&#25104;&#21709;&#24212;&#20197;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#65292;&#24182;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#37096;&#32626;&#21830;&#19994;&#26816;&#32034;&#22686;&#24378;&#22411;LLM&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#25104;&#26412;&#65292;&#22240;&#20026;&#39069;&#22806;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#22823;&#22823;&#22686;&#21152;&#20102;LLM&#30340;&#36755;&#20837;&#26631;&#35760;&#37327;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#29260;&#21387;&#32553;&#26041;&#26696;&#65292;&#21253;&#25324;&#20004;&#31181;&#26041;&#27861;&#65306;&#27010;&#36848;&#21387;&#32553;&#21644;&#35821;&#20041;&#21387;&#32553;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;T5&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#26679;&#26412;&#30340;&#33258;&#25351;&#31034;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#36890;&#36807;&#27010;&#36848;&#26469;&#20943;&#23569;&#20196;&#29260;&#22823;&#23567;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#36890;&#36807;&#31227;&#38500;&#23545;&#35821;&#20041;&#24433;&#21709;&#36739;&#23567;&#30340;&#35789;&#26469;&#36827;&#19968;&#27493;&#21387;&#32553;&#20196;&#29260;&#22823;&#23567;&#12290;&#20026;&#20102;&#20805;&#20998;&#35780;&#20272;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
Since ChatGPT released its API for public use, the number of applications built on top of commercial large language models (LLMs) increase exponentially. One popular usage of such models is leveraging its in-context learning ability and generating responses given user queries leveraging knowledge obtained by retrieval augmentation. One problem of deploying commercial retrieval-augmented LLMs is the cost due to the additionally retrieved context that largely increases the input token size of the LLMs. To mitigate this, we propose a token compression scheme that includes two methods: summarization compression and semantic compression. The first method applies a T5-based model that is fine-tuned by datasets generated using self-instruct containing samples with varying lengths and reduce token size by doing summarization. The second method further compresses the token size by removing words with lower impact on the semantic. In order to adequately evaluate the effectiveness of the proposed
&lt;/p&gt;</description></item><item><title>FANToM&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#21387;&#21147;&#27979;&#35797;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;&#27169;&#22411;&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#24471;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.15421</link><description>&lt;p&gt;
FANToM: &#22312;&#20132;&#20114;&#20013;&#23545;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions. (arXiv:2310.15421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15421
&lt;/p&gt;
&lt;p&gt;
FANToM&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#21387;&#21147;&#27979;&#35797;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;&#27169;&#22411;&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#24471;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#30340;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#32570;&#20047;&#20114;&#21160;&#24615;&#30340;&#34987;&#21160;&#25925;&#20107;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FANToM&#65292;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#24515;&#26234;&#29702;&#35770;&#30340;&#21387;&#21147;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#32467;&#21512;&#20102;&#24515;&#29702;&#23398;&#20013;&#30340;&#37325;&#35201;&#29702;&#35770;&#35201;&#27714;&#21644;&#23545;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#24517;&#35201;&#30340;&#32463;&#39564;&#32771;&#34385;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#22810;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#35201;&#27714;&#30456;&#21516;&#30340;&#22522;&#26412;&#25512;&#29702;&#26469;&#35782;&#21035;LLM&#20013;&#19981;&#23384;&#22312;&#25110;&#34394;&#20551;&#30340;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FANToM&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;LLM&#20063;&#34920;&#29616;&#27604;&#20154;&#31867;&#24046;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.
&lt;/p&gt;</description></item><item><title>TaskDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#23545;&#35805;&#32452;&#25104;&#37096;&#20998;&#26469;&#35745;&#31639;&#30456;&#20284;&#24230;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15298</link><description>&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TaskDiff: A Similarity Metric for Task-Oriented Conversations. (arXiv:2310.15298v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15298
&lt;/p&gt;
&lt;p&gt;
TaskDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#23545;&#35805;&#32452;&#25104;&#37096;&#20998;&#26469;&#35745;&#31639;&#30456;&#20284;&#24230;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#25968;&#23383;&#21161;&#25163;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#22823;&#37327;&#20250;&#35805;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#21644;&#20010;&#24615;&#21270;&#21709;&#24212;&#29983;&#25104;&#12290;&#20351;&#29992;&#20687;ChatGPT&#36825;&#26679;&#30340;&#27969;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#36825;&#20123;&#21161;&#25163;&#36824;&#38656;&#35201;&#39069;&#22806;&#24378;&#35843;&#25552;&#31034;&#24037;&#31243;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#25991;&#26412;&#30456;&#20284;&#24230;&#24230;&#37327;&#26159;&#36825;&#31181;&#20998;&#26512;&#21644;&#35780;&#20272;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#26041;&#38754;&#24182;&#19981;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#29420;&#29305;&#30340;&#23545;&#35805;&#29305;&#24449;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;TaskDiff&#65292;&#23427;&#21033;&#29992;&#23545;&#35805;&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#65288;&#35805;&#35821;&#12289;&#24847;&#22270;&#21644;&#27133;&#65289;&#21450;&#20854;&#20998;&#24067;&#26469;&#35745;&#31639;&#30456;&#20284;&#24230;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;TaskDiff&#22312;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#30456;&#20851;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of conversational digital assistants has resulted in the availability of large amounts of conversational data which can be utilized for improved user experience and personalized response generation. Building these assistants using popular large language models like ChatGPT also require additional emphasis on prompt engineering and evaluation methods. Textual similarity metrics are a key ingredient for such analysis and evaluations. While many similarity metrics have been proposed in the literature, they have not proven effective for task-oriented conversations as they do not take advantage of unique conversational features. To address this gap, we present TaskDiff, a novel conversational similarity metric that utilizes different dialogue components (utterances, intents, and slots) and their distributions to compute similarity. Extensive experimental evaluation of TaskDiff on a benchmark dataset demonstrates its superior performance and improved robustness over other rela
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#19987;&#23478;&#24494;&#35843;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;DISC-FinLLM&#65292;&#36890;&#36807;&#36171;&#20104;&#27169;&#22411;&#22810;&#36718;&#38382;&#31572;&#12289;&#39046;&#22495;&#25991;&#26412;&#22788;&#29702;&#12289;&#25968;&#23398;&#35745;&#31639;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#37329;&#34701;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15205</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#19987;&#23478;&#24494;&#35843;&#30340;&#20013;&#22269;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;DISC-FinLLM
&lt;/p&gt;
&lt;p&gt;
DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning. (arXiv:2310.15205v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15205
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#19987;&#23478;&#24494;&#35843;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;DISC-FinLLM&#65292;&#36890;&#36807;&#36171;&#20104;&#27169;&#22411;&#22810;&#36718;&#38382;&#31572;&#12289;&#39046;&#22495;&#25991;&#26412;&#22788;&#29702;&#12289;&#25968;&#23398;&#35745;&#31639;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#37329;&#34701;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#19987;&#23478;&#24494;&#35843;&#26694;&#26550;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;DISC-FinLLM&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36171;&#20104;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#22810;&#36718;&#38382;&#31572;&#33021;&#21147;&#12289;&#39046;&#22495;&#25991;&#26412;&#22788;&#29702;&#33021;&#21147;&#12289;&#25968;&#23398;&#35745;&#31639;&#25216;&#33021;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#33021;&#21147;&#26469;&#25913;&#36827;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#37329;&#34701;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;DISC-FIN-SFT&#65292;&#21253;&#25324;&#22235;&#20010;&#20998;&#31867;&#30340;&#25351;&#20196;&#26679;&#26412;&#65288;&#21672;&#35810;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#35745;&#31639;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#37329;&#34701;&#22330;&#26223;&#20013;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#26356;&#22810;&#36164;&#28304;&#21487;&#20197;&#22312;https://github.com/FudanDISC/DISC-FinLLM&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Multiple Experts Fine-tuning Framework to build a financial large language model (LLM), DISC-FinLLM. Our methodology improves general LLMs by endowing them with multi-turn question answering abilities, domain text processing capabilities, mathematical computation skills, and retrieval-enhanced generation capabilities. We build a financial instruction-tuning dataset named DISC-FIN-SFT, including instruction samples of four categories (consulting, NLP tasks, computing and retrieval-augmented generation). Evaluations conducted on multiple benchmarks demonstrate that our model performs better than baseline models in various financial scenarios. Further resources can be found at https://github.com/FudanDISC/DISC-FinLLM.
&lt;/p&gt;</description></item><item><title>NormDial&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21452;&#35821;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27169;&#25311;&#31038;&#20250;&#35268;&#33539;&#30340;&#36981;&#23432;&#21644;&#36829;&#21453;&#12290;&#36890;&#36807;&#24341;&#20837;&#31038;&#20250;&#35268;&#33539;&#36981;&#23432;&#26816;&#27979;&#20219;&#21153;&#65292;&#35813;&#25968;&#25454;&#38598;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#36328;&#35821;&#35328;&#21644;&#25991;&#21270;&#32972;&#26223;&#19979;&#23545;&#35805;&#29615;&#22659;&#20013;&#31038;&#20250;&#35268;&#33539;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.14563</link><description>&lt;p&gt;
NormDial: &#29992;&#20110;&#27169;&#25311;&#31038;&#20250;&#35268;&#33539;&#36981;&#23432;&#21644;&#36829;&#21453;&#30340;&#21487;&#27604;&#36739;&#21452;&#35821;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling Social Norm Adherence and Violation. (arXiv:2310.14563v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14563
&lt;/p&gt;
&lt;p&gt;
NormDial&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21452;&#35821;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27169;&#25311;&#31038;&#20250;&#35268;&#33539;&#30340;&#36981;&#23432;&#21644;&#36829;&#21453;&#12290;&#36890;&#36807;&#24341;&#20837;&#31038;&#20250;&#35268;&#33539;&#36981;&#23432;&#26816;&#27979;&#20219;&#21153;&#65292;&#35813;&#25968;&#25454;&#38598;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#36328;&#35821;&#35328;&#21644;&#25991;&#21270;&#32972;&#26223;&#19979;&#23545;&#35805;&#29615;&#22659;&#20013;&#31038;&#20250;&#35268;&#33539;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#35268;&#33539;&#20174;&#26681;&#26412;&#19978;&#22609;&#36896;&#20102;&#20154;&#38469;&#20132;&#27969;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;NormDial&#65292;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21452;&#20154;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20013;&#32654;&#20004;&#31181;&#25991;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#36981;&#23432;&#21644;&#36829;&#21453;&#30340;&#36880;&#36718;&#27880;&#37322;&#12290;&#36890;&#36807;&#24341;&#20837;&#31038;&#20250;&#35268;&#33539;&#36981;&#23432;&#26816;&#27979;&#20219;&#21153;&#65292;&#25105;&#20204;&#20351;&#29992;&#20154;&#26426;&#21327;&#20316;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#19987;&#23478;&#27880;&#37322;&#30340;&#31038;&#20250;&#35268;&#33539;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#33521;&#25991;&#20013;&#29983;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#29983;&#25104;&#30340;&#23545;&#35805;&#36136;&#37327;&#24456;&#39640;&#65292;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25351;&#21521;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#20197;&#20102;&#35299;&#36328;&#35821;&#35328;&#21644;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#31038;&#20250;&#35268;&#33539;&#30340;&#24494;&#22937;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social norms fundamentally shape interpersonal communication. We present NormDial, a high-quality dyadic dialogue dataset with turn-by-turn annotations of social norm adherences and violations for Chinese and American cultures. Introducing the task of social norm observance detection, our dataset is synthetically generated in both Chinese and English using a human-in-the-loop pipeline by prompting large language models with a small collection of expert-annotated social norms. We show that our generated dialogues are of high quality through human evaluation and further evaluate the performance of existing large language models on this task. Our findings point towards new directions for understanding the nuances of social norms as they manifest in conversational contexts that span across languages and cultures.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#24037;&#20855;&#30340;&#39640;&#32423;&#33521;&#35793;&#38463;&#25289;&#20271;&#35821;&#32763;&#35793;&#22120;&#65292;&#20351;&#29992;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#21644;&#32431;&#25991;&#23398;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#65292;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#35821;&#22659;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.13613</link><description>&lt;p&gt;
Hunayn&#65306;&#36229;&#36234;&#23383;&#38754;&#24847;&#20041;&#30340;&#32763;&#35793;&#36827;&#27493;
&lt;/p&gt;
&lt;p&gt;
Hunayn: Elevating Translation Beyond the Literal. (arXiv:2310.13613v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#24037;&#20855;&#30340;&#39640;&#32423;&#33521;&#35793;&#38463;&#25289;&#20271;&#35821;&#32763;&#35793;&#22120;&#65292;&#20351;&#29992;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#21644;&#32431;&#25991;&#23398;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#65292;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#35821;&#22659;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#24037;&#20855;&#30340;&#39640;&#32423;&#33521;&#35793;&#38463;&#25289;&#20271;&#35821;&#32763;&#35793;&#22120;&#12290;&#21033;&#29992;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#65288;MarianMT&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#22312;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#12289;&#32431;&#25991;&#23398;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#19982;&#35895;&#27468;&#32763;&#35793;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#23450;&#24615;&#35780;&#20272;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#22312;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#35821;&#22659;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#22312;&#20351;&#29992;Fusha&#25968;&#25454;&#38598;&#30340;&#33521;&#38463;&#32763;&#35793;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This project introduces an advanced English-to-Arabic translator surpassing conventional tools. Leveraging the Helsinki transformer (MarianMT), our approach involves fine-tuning on a self-scraped, purely literary Arabic dataset. Evaluations against Google Translate show consistent outperformance in qualitative assessments. Notably, it excels in cultural sensitivity and context accuracy. This research underscores the Helsinki transformer's superiority for English-to-Arabic translation using a Fusha dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;&#65292;&#36890;&#36807;&#32858;&#31867;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#65292;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#65292;&#24182;&#25366;&#25496;&#20102;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.13447</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Multiscale Superpixel Structured Difference Graph Convolutional Network for VL Representation. (arXiv:2310.13447v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;&#65292;&#36890;&#36807;&#32858;&#31867;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#65292;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#65292;&#24182;&#25366;&#25496;&#20102;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#20013;&#65292;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#20851;&#38190;&#22312;&#20110;&#24314;&#31435;&#19968;&#20010;&#33391;&#22909;&#30340;&#23545;&#40784;&#31574;&#30053;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#34920;&#24449;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#35821;&#20041;&#34920;&#24449;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#24403;&#21069;&#22522;&#20110;&#20687;&#32032;&#25110;&#22359;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#25552;&#21462;&#22797;&#26434;&#22330;&#26223;&#36793;&#30028;&#26041;&#38754;&#23384;&#22312;&#31354;&#38388;&#35821;&#20041;&#36830;&#36143;&#24615;&#19981;&#36275;&#21644;&#23545;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#23558;&#36229;&#20687;&#32032;&#20316;&#20026;&#21487;&#23398;&#20064;&#22270;&#20687;&#25968;&#25454;&#30340;&#32508;&#21512;&#32039;&#20945;&#34920;&#24449;&#65292;&#36890;&#36807;&#23545;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#36827;&#34892;&#32858;&#31867;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#12290;&#20026;&#20102;&#25366;&#25496;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#12290;&#23427;&#23558;&#25972;&#20010;&#22270;&#20687;&#35299;&#26512;&#20026;&#32454;&#21040;&#31895;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25972;&#20010;&#22270;&#20687;&#30340;&#35299;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the multimodal field, the key to integrating vision and language lies in establishing a good alignment strategy. Recently, benefiting from the success of self-supervised learning, significant progress has been made in multimodal semantic representation based on pre-trained models for vision and language. However, there is still room for improvement in visual semantic representation. The lack of spatial semantic coherence and vulnerability to noise makes it challenging for current pixel or patch-based methods to accurately extract complex scene boundaries. To this end, this paper develops superpixel as a comprehensive compact representation of learnable image data, which effectively reduces the number of visual primitives for subsequent processing by clustering perceptually similar pixels. To mine more precise topological relations, we propose a Multiscale Difference Graph Convolutional Network (MDGCN). It parses the entire image as a fine-to-coarse hierarchical structure of cons
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#21521;&#22270;&#21367;&#31215;&#36827;&#34892;&#30340;&#40065;&#26834;&#36328;&#27169;&#24577;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#26377;&#25928;&#20998;&#31163;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.13276</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#22270;&#21367;&#31215;&#23454;&#29616;&#40065;&#26834;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution. (arXiv:2310.13276v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13276
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#22270;&#21367;&#31215;&#36827;&#34892;&#30340;&#40065;&#26834;&#36328;&#27169;&#24577;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#26377;&#25928;&#20998;&#31163;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#37325;&#22823;&#36827;&#23637;&#20027;&#35201;&#26159;&#36890;&#36807;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#30340;&#31361;&#30772;&#25512;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#25968;&#25454;&#34920;&#31034;&#24448;&#24448;&#22312;&#26377;&#38480;&#30340;&#20984;&#38181;&#20869;&#32858;&#38598;&#65288;&#20316;&#20026;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#65289;&#65292;&#36825;&#30001;&#20110;&#36825;&#20123;&#34920;&#31034;&#30340;&#19981;&#21487;&#20998;&#31163;&#24615;&#32780;&#38459;&#30861;&#20102;&#26816;&#32034;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22810;&#20010;&#36328;&#27169;&#24577;&#22522;&#20934;&#21644;&#26041;&#27861;&#32463;&#39564;&#35777;&#23454;&#20102;&#34920;&#31034;&#36864;&#21270;&#38382;&#39064;&#30340;&#23384;&#22312;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;InvGC&#65292;&#23427;&#26159;&#19968;&#31181;&#21463;&#22270;&#21367;&#31215;&#21644;&#24179;&#22343;&#27744;&#21270;&#21551;&#21457;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;InvGC&#22312;&#25968;&#25454;&#38598;&#20013;&#23450;&#20041;&#22270;&#25299;&#25169;&#65292;&#28982;&#21518;&#24212;&#29992;&#22270;&#21367;&#31215;&#20197;&#19968;&#31181;&#20943;&#27861;&#30340;&#26041;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#26377;&#25928;&#22320;&#20998;&#31163;&#34920;&#31034;&#12290;&#20026;&#20102;&#25552;&#39640;InvGC&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32423;&#22270;&#25299;&#25169;&#65292;Lo
&lt;/p&gt;
&lt;p&gt;
Over recent decades, significant advancements in cross-modal retrieval are mainly driven by breakthroughs in visual and linguistic modeling. However, a recent study shows that multi-modal data representations tend to cluster within a limited convex cone (as representation degeneration problem), which hinders retrieval performance due to the inseparability of these representations. In our study, we first empirically validate the presence of the representation degeneration problem across multiple cross-modal benchmarks and methods. Next, to address it, we introduce a novel method, called InvGC, a post-processing technique inspired by graph convolution and average pooling. Specifically, InvGC defines the graph topology within the datasets and then applies graph convolution in a subtractive manner. This method effectively separates representations by increasing the distances between data points. To improve the efficiency and effectiveness of InvGC, we propose an advanced graph topology, Lo
&lt;/p&gt;</description></item><item><title>CLAIR&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#30340;&#22270;&#20687;&#26631;&#39064;&#12290;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;CLAIR&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#38024;&#23545;&#20855;&#20307;&#25968;&#25454;&#38598;&#21462;&#24471;&#20102;&#36739;&#22823;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.12971</link><description>&lt;p&gt;
CLAIR: &#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22270;&#20687;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
CLAIR: Evaluating Image Captions with Large Language Models. (arXiv:2310.12971v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12971
&lt;/p&gt;
&lt;p&gt;
CLAIR&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#30340;&#22270;&#20687;&#26631;&#39064;&#12290;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;CLAIR&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#38024;&#23545;&#20855;&#20307;&#25968;&#25454;&#38598;&#21462;&#24471;&#20102;&#36739;&#22823;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#29983;&#25104;&#22270;&#20687;&#26631;&#39064;&#30340;&#35780;&#20272;&#26159;&#19968;&#20010;&#26377;&#36259;&#20294;&#25345;&#20037;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26377;&#25928;&#30340;&#35780;&#20272;&#25351;&#26631;&#24517;&#39035;&#32771;&#34385;&#22810;&#20010;&#30456;&#20284;&#24615;&#32500;&#24230;&#65292;&#21253;&#25324;&#35821;&#20041;&#30456;&#20851;&#24615;&#12289;&#35270;&#35273;&#32467;&#26500;&#12289;&#29289;&#20307;&#20132;&#20114;&#12289;&#26631;&#39064;&#22810;&#26679;&#24615;&#21644;&#29305;&#23450;&#24615;&#12290;&#29616;&#26377;&#30340;&#39640;&#24230;&#24037;&#31243;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#35797;&#22270;&#25429;&#25417;&#29305;&#23450;&#26041;&#38754;&#65292;&#20294;&#22312;&#25552;&#20379;&#19982;&#20154;&#31867;&#21028;&#26029;&#23494;&#20999;&#19968;&#33268;&#30340;&#25972;&#20307;&#35780;&#20998;&#26041;&#38754;&#20173;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLAIR&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38646;&#23556;&#35821;&#35328;&#24314;&#27169;&#33021;&#21147;&#26469;&#35780;&#20272;&#20505;&#36873;&#26631;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;CLAIR&#30456;&#23545;&#20110;&#29616;&#26377;&#25351;&#26631;&#26356;&#33021;&#19982;&#20154;&#31867;&#23545;&#26631;&#39064;&#36136;&#37327;&#30340;&#21028;&#26029;&#30456;&#20851;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;Flickr8K-Expert&#19978;&#65292;CLAIR&#22312;&#19982;SPICE&#30456;&#27604;&#30340;&#30456;&#20851;&#25913;&#36827;&#26041;&#38754;&#25552;&#39640;&#20102;39.6&#65285;&#65292;&#22312;&#19982;RefCLIP-S&#31561;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#30456;&#27604;&#30340;&#30456;&#20851;&#25913;&#36827;&#26041;&#38754;&#25552;&#39640;&#20102;18.3&#65285;&#12290;&#27492;&#22806;&#65292;CLAIR&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#32467;&#26524;&#65292;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;u
&lt;/p&gt;
&lt;p&gt;
The evaluation of machine-generated image captions poses an interesting yet persistent challenge. Effective evaluation measures must consider numerous dimensions of similarity, including semantic relevance, visual structure, object interactions, caption diversity, and specificity. Existing highly-engineered measures attempt to capture specific aspects, but fall short in providing a holistic score that aligns closely with human judgments. Here, we propose CLAIR, a novel method that leverages the zero-shot language modeling capabilities of large language models (LLMs) to evaluate candidate captions. In our evaluations, CLAIR demonstrates a stronger correlation with human judgments of caption quality compared to existing measures. Notably, on Flickr8K-Expert, CLAIR achieves relative correlation improvements over SPICE of 39.6% and over image-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides noisily interpretable results by allowing the language model to identify the u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25104;&#21151;&#23558;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#26041;&#24335;&#20316;&#20026;MOEA/D&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#65292;&#24182;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#12290;</title><link>http://arxiv.org/abs/2310.12541</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22810;&#30446;&#26631;&#36827;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Multi-objective Evolutionary Optimization. (arXiv:2310.12541v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25104;&#21151;&#23558;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#26041;&#24335;&#20316;&#20026;MOEA/D&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#65292;&#24182;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEAs&#65289;&#26159;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65288;MOPs&#65289;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;MOEAs&#65292;&#20854;&#25805;&#20316;&#31526;&#38656;&#35201;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#23581;&#35797;&#23558;MOEAs&#20013;&#25163;&#21160;&#35774;&#35745;&#30340;&#25805;&#20316;&#31526;&#26367;&#25442;&#20026;&#22522;&#20110;&#23398;&#20064;&#30340;&#25805;&#20316;&#31526;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#21644;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#23398;&#20064;&#21040;&#30340;&#25805;&#20316;&#31526;&#21487;&#33021;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#35299;&#20915;&#26032;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35753;&#19968;&#20010;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#20316;&#20026;&#20998;&#35299;&#22411;MOEA&#65288;MOEA/D&#65289;&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#65292;&#24182;&#25552;&#20986;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Multiobjective evolutionary algorithms (MOEAs) are major methods for solving multiobjective optimization problems (MOPs). Many MOEAs have been proposed in the past decades, of which the operators need carefully handcrafted design with domain knowledge. Recently, some attempts have been made to replace the manually designed operators in MOEAs with learning-based operators (e.g., neural network models). However, much effort is still required for designing and training such models, and the learned operators might not generalize well to solve new problems. To tackle the above challenges, this work investigates a novel approach that leverages the powerful large language model (LLM) to design MOEA operators. With proper prompt engineering, we successfully let a general LLM serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a zero-shot manner. In addition, by learning from the LLM behavior, we further design an explicit white-box operator with randomness and propose
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#20197;&#21450;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#30740;&#31350;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#22312;&#40664;&#35748;&#20026;&#30007;&#24615;&#32763;&#35793;&#19978;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65292;&#21516;&#26102;&#24573;&#35270;&#20102;&#25351;&#31034;&#32844;&#19994;&#24615;&#21035;&#30340;&#20195;&#35789;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#34892;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.12127</link><description>&lt;p&gt;
&#20195;&#35789;&#25925;&#20107;&#65306;&#21487;&#35299;&#37322;&#24615;&#25351;&#23548;&#19979;&#30340;&#20844;&#24179;&#25351;&#23548;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation. (arXiv:2310.12127v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#20197;&#21450;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#30740;&#31350;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#22312;&#40664;&#35748;&#20026;&#30007;&#24615;&#32763;&#35793;&#19978;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65292;&#21516;&#26102;&#24573;&#35270;&#20102;&#25351;&#31034;&#32844;&#19994;&#24615;&#21035;&#30340;&#20195;&#35789;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#34892;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#21487;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#35299;&#20915;&#38382;&#39064;&#65292;&#20854;&#20013;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#29992;&#20363;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#22312;&#26631;&#20934;&#24615;&#33021;&#22522;&#20934;&#19978;&#65292;&#24573;&#35270;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#20844;&#24179;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;&#22312;MT&#20013;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24615;&#21035;&#38169;&#35823;&#30340;&#32763;&#35793;&#65292;&#20174;&#32780;&#23548;&#33268;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#30340;&#25345;&#32493;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#36825;&#20123;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#26159;&#21542;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#20197;&#21450;&#22914;&#20309;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#20174;&#33521;&#25991;&#21040;&#24503;&#25991;&#21644;&#35199;&#29677;&#29273;&#25991;&#30340;WinoMT&#35821;&#26009;&#24211;&#19978;&#35745;&#31639;&#24050;&#24314;&#31435;&#30340;&#24615;&#21035;&#20559;&#35265;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#40664;&#35748;&#20026;&#30007;&#24615;&#23624;&#20174;&#32763;&#35793;&#65292;&#29978;&#33267;&#24573;&#35270;&#22899;&#24615;&#32844;&#19994;&#21051;&#26495;&#21360;&#35937;&#12290;&#25509;&#19979;&#26469;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27169;&#22411;&#31995;&#32479;&#24615;&#22320;&#24573;&#35270;&#25351;&#31034;&#30446;&#26631;&#32844;&#19994;&#24615;&#21035;&#30340;&#20195;&#35789;&#22312;&#21516;&#26102;&#24615;&#21035;&#38169;&#35823;&#30340;&#32763;&#35793;&#20013;&#12290;&#26368;&#21518;&#65292;&#26681;&#25454;&#21487;&#35299;&#37322;&#24615;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24615;&#21035;&#20559;&#35265;&#32531;&#35299;&#30340;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;MT&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. In this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it. Concretely, we compute established gender bias metrics on the WinoMT corpus from English to German and Spanish. We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes. Next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. Finally, based on 
&lt;/p&gt;</description></item><item><title>MusicAgent&#26159;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#20195;&#29702;&#65292;&#36890;&#36807;&#38598;&#25104;&#38899;&#20048;&#30456;&#20851;&#24037;&#20855;&#21644;&#33258;&#20027;&#24037;&#20316;&#27969;&#31243;&#65292;&#24110;&#21161;&#29992;&#25143;&#33258;&#21160;&#20998;&#26512;&#38656;&#27714;&#24182;&#35843;&#29992;&#21512;&#36866;&#30340;&#24037;&#20855;&#36827;&#34892;&#38899;&#20048;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.11954</link><description>&lt;p&gt;
MusicAgent&#65306;&#19968;&#31181;&#29992;&#20110;&#38899;&#20048;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models. (arXiv:2310.11954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11954
&lt;/p&gt;
&lt;p&gt;
MusicAgent&#26159;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#20195;&#29702;&#65292;&#36890;&#36807;&#38598;&#25104;&#38899;&#20048;&#30456;&#20851;&#24037;&#20855;&#21644;&#33258;&#20027;&#24037;&#20316;&#27969;&#31243;&#65292;&#24110;&#21161;&#29992;&#25143;&#33258;&#21160;&#20998;&#26512;&#38656;&#27714;&#24182;&#35843;&#29992;&#21512;&#36866;&#30340;&#24037;&#20855;&#36827;&#34892;&#38899;&#20048;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI-&#21152;&#24378;&#30340;&#38899;&#20048;&#22788;&#29702;&#26159;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#39046;&#22495;&#65292;&#28085;&#30422;&#20102;&#35768;&#22810;&#20219;&#21153;&#65292;&#20174;&#29983;&#25104;&#20219;&#21153;&#65288;&#20363;&#22914;&#38899;&#33394;&#21512;&#25104;&#65289;&#21040;&#29702;&#35299;&#20219;&#21153;&#65288;&#20363;&#22914;&#38899;&#20048;&#20998;&#31867;&#65289;&#12290;&#23545;&#20110;&#24320;&#21457;&#20154;&#21592;&#21644;&#19994;&#20313;&#29233;&#22909;&#32773;&#26469;&#35828;&#65292;&#24456;&#38590;&#25484;&#25569;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#65292;&#20197;&#28385;&#36275;&#20182;&#20204;&#22312;&#38899;&#20048;&#22788;&#29702;&#26041;&#38754;&#30340;&#38656;&#27714;&#65292;&#23588;&#20854;&#32771;&#34385;&#21040;&#38899;&#20048;&#25968;&#25454;&#30340;&#34920;&#31034;&#21644;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#27169;&#22411;&#36866;&#29992;&#24615;&#22312;&#21508;&#20010;&#24179;&#21488;&#19978;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#19968;&#20010;&#31995;&#32479;&#26469;&#32452;&#32455;&#21644;&#38598;&#25104;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#24110;&#21161;&#20174;&#19994;&#32773;&#33258;&#21160;&#20998;&#26512;&#20182;&#20204;&#30340;&#38656;&#27714;&#24182;&#35843;&#29992;&#36866;&#24403;&#30340;&#24037;&#20855;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#26469;&#28385;&#36275;&#20182;&#20204;&#30340;&#35201;&#27714;&#26159;&#24517;&#35201;&#30340;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20219;&#21153;&#33258;&#21160;&#21270;&#26041;&#38754;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#31034;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;MusicAgent&#30340;&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#20247;&#22810;&#19982;&#38899;&#20048;&#30456;&#20851;&#30340;&#24037;&#20855;&#21644;&#33258;&#20027;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-empowered music processing is a diverse field that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classification). For developers and amateurs, it is very difficult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, includi
&lt;/p&gt;</description></item><item><title>VoxArabica&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#35328;&#24863;&#30693;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#24320;&#21457;&#21644;&#28436;&#31034;&#65292;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35821;&#26041;&#35328;&#35782;&#21035;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#35813;&#31995;&#32479;&#35757;&#32451;&#20102;&#21508;&#31181;&#27169;&#22411;&#29992;&#20110;&#19981;&#21516;&#26041;&#35328;&#30340;&#35782;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#21151;&#33021;&#30340;&#32593;&#32476;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.11069</link><description>&lt;p&gt;
VoxArabica&#65306;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#35328;&#24863;&#30693;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System. (arXiv:2310.11069v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11069
&lt;/p&gt;
&lt;p&gt;
VoxArabica&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#35328;&#24863;&#30693;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#24320;&#21457;&#21644;&#28436;&#31034;&#65292;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35821;&#26041;&#35328;&#35782;&#21035;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#35813;&#31995;&#32479;&#35757;&#32451;&#20102;&#21508;&#31181;&#27169;&#22411;&#29992;&#20110;&#19981;&#21516;&#26041;&#35328;&#30340;&#35782;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#21151;&#33021;&#30340;&#32593;&#32476;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#25289;&#20271;&#35821;&#26159;&#19968;&#31181;&#22797;&#26434;&#30340;&#35821;&#35328;&#65292;&#20840;&#29699;&#26377;&#36229;&#36807;4.5&#20159;&#20154;&#21475;&#20351;&#29992;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#35328;&#21644;&#21475;&#38899;&#12290;&#30001;&#20110;&#35821;&#35328;&#30340;&#22810;&#26679;&#24615;&#21644;&#21464;&#21270;&#65292;&#20026;&#38463;&#25289;&#20271;&#35821;&#26500;&#24314;&#19968;&#20010;&#31283;&#20581;&#19988;&#36890;&#29992;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#21644;&#28436;&#31034;&#19968;&#20010;&#21517;&#20026;VoxArabica&#30340;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#29992;&#20110;&#26041;&#35328;&#35782;&#21035;(DID)&#21644;&#38463;&#25289;&#20271;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#12290;&#25105;&#20204;&#22312;&#30417;&#30563;&#29615;&#22659;&#19979;&#35757;&#32451;&#20102;&#21508;&#31181;&#27169;&#22411;&#65292;&#20363;&#22914;HuBERT(DID)&#12289;Whisper&#21644;XLS-R(ASR)&#65292;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#30340;DID&#21644;ASR&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;DID&#27169;&#22411;&#34987;&#35757;&#32451;&#29992;&#20110;&#35782;&#21035;&#38500;&#20102;&#26631;&#20934;&#38463;&#25289;&#20271;&#20043;&#22806;&#30340;17&#31181;&#19981;&#21516;&#30340;&#26041;&#35328;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;(MSA)&#12289;&#22467;&#21450;&#35821;&#12289;&#25705;&#27931;&#21733;&#35821;&#21644;&#28151;&#21512;&#25968;&#25454;&#19978;&#24494;&#35843;&#25105;&#20204;&#30340;ASR&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;ASR&#20013;&#30340;&#20854;&#20182;&#26041;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Whisper&#21644;MMS&#31561;&#19981;&#21516;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#38598;&#25104;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#32593;&#32476;&#30028;&#38754;&#20013;&#65292;&#20855;&#26377;&#22810;&#26679;&#30340;&#21151;&#33021;&#65292;&#22914;&#38899;&#39057;&#24405;&#21046;&#12289;&#19978;&#20256;&#25991;&#20214;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#20986;&#38382;&#39064;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Arabic is a complex language with many varieties and dialects spoken by over 450 millions all around the world. Due to the linguistic diversity and variations, it is challenging to build a robust and generalized ASR system for Arabic. In this work, we address this gap by developing and demoing a system, dubbed VoxArabica, for dialect identification (DID) as well as automatic speech recognition (ASR) of Arabic. We train a wide range of models such as HuBERT (DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR tasks. Our DID models are trained to identify 17 different dialects in addition to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data. Additionally, for the remaining dialects in ASR, we provide the option to choose various models such as Whisper and MMS in a zero-shot setting. We integrate these models into a single web interface with diverse features such as audio recording, file upload, model selection, and the option to raise fl
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#21152;&#23494;&#36164;&#20135;&#30417;&#31649;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#35843;&#26597;&#20102;&#23545;&#26410;&#21463;&#30417;&#31649;&#30340;&#21152;&#23494;&#36164;&#20135;&#30333;&#30382;&#20070;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#30340;&#29616;&#26377;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#20102;&#30740;&#31350;&#31354;&#30333;&#12290;&#28982;&#21518;&#65292;&#20998;&#26512;&#20102;&#27431;&#30431;&#30340;&#21152;&#23494;&#36164;&#20135;&#24066;&#22330;&#27861;&#35268;&#24341;&#20837;&#30340;&#21464;&#21270;&#65292;&#25506;&#35752;&#20102;&#22312;&#26032;&#30340;&#30417;&#31649;&#26694;&#26550;&#20869;&#25972;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#26377;&#28508;&#21147;&#20351;&#30417;&#31649;&#26426;&#26500;&#12289;&#21152;&#23494;&#36164;&#20135;&#21457;&#34892;&#32773;&#21644;&#25237;&#36164;&#32773;&#21463;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.10333</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#36164;&#20135;&#30417;&#31649;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65306;&#19968;&#20221;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
NLP for Crypto-Asset Regulation: A Roadmap. (arXiv:2310.10333v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10333
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#21152;&#23494;&#36164;&#20135;&#30417;&#31649;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#35843;&#26597;&#20102;&#23545;&#26410;&#21463;&#30417;&#31649;&#30340;&#21152;&#23494;&#36164;&#20135;&#30333;&#30382;&#20070;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#30340;&#29616;&#26377;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#20102;&#30740;&#31350;&#31354;&#30333;&#12290;&#28982;&#21518;&#65292;&#20998;&#26512;&#20102;&#27431;&#30431;&#30340;&#21152;&#23494;&#36164;&#20135;&#24066;&#22330;&#27861;&#35268;&#24341;&#20837;&#30340;&#21464;&#21270;&#65292;&#25506;&#35752;&#20102;&#22312;&#26032;&#30340;&#30417;&#31649;&#26694;&#26550;&#20869;&#25972;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#26377;&#28508;&#21147;&#20351;&#30417;&#31649;&#26426;&#26500;&#12289;&#21152;&#23494;&#36164;&#20135;&#21457;&#34892;&#32773;&#21644;&#25237;&#36164;&#32773;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#21306;&#22359;&#38142;&#36164;&#20135;&#39046;&#22495;&#65292;&#30333;&#30382;&#20070;&#26159;&#25237;&#36164;&#32773;&#25351;&#23548;&#30340;&#37325;&#35201;&#25991;&#20214;&#65292;&#22312;&#27431;&#30431;&#30340;&#21152;&#23494;&#36164;&#20135;&#24066;&#22330;&#27861;&#35268;&#65288;MiCAR&#65289;&#19979;&#65292;&#23427;&#20204;&#29616;&#22312;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#20869;&#23481;&#35201;&#27714;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#20316;&#20026;&#20998;&#26512;&#36825;&#20123;&#25991;&#20214;&#21644;&#21327;&#21161;&#30417;&#31649;&#21512;&#35268;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26412;&#25991;&#20026;&#35813;&#20027;&#39064;&#25552;&#20379;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23545;&#26410;&#21463;&#30417;&#31649;&#30340;&#21306;&#22359;&#38142;&#36164;&#20135;&#30333;&#30382;&#20070;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#30340;&#29616;&#26377;&#24212;&#29992;&#65292;&#21457;&#29616;&#20102;&#21487;&#20197;&#36890;&#36807;&#36328;&#23398;&#31185;&#21512;&#20316;&#26469;&#22635;&#34917;&#30740;&#31350;&#31354;&#30333;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;MiCAR&#24341;&#20837;&#30340;&#21464;&#21270;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#22312;&#26032;&#30340;&#30417;&#31649;&#26694;&#26550;&#20869;&#25972;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#26377;&#28508;&#21147;&#20351;&#30417;&#31649;&#26426;&#26500;&#12289;&#21306;&#22359;&#38142;&#36164;&#20135;&#21457;&#34892;&#32773;&#21644;&#25237;&#36164;&#32773;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of crypto-assets, white papers are essential documents for investor guidance, and are now subject to unprecedented content requirements under the EU's Markets in Crypto-Assets Regulation (MiCAR). Natural Language Processing can serve as a powerful tool for both analyzing these documents and assisting in regulatory compliance. This paper delivers two contributions to the topic. First, we survey existing applications of textual analysis to unregulated crypto-asset white papers, uncovering a research gap that could be bridged with interdisciplinary collaboration. We then conduct an analysis of the changes introduced by MiCAR, highlighting the opportunities and challenges of integrating NLP within the new regulatory framework. Our findings set the stage for further research, with the potential to benefit regulators, crypto-asset issuers, and investors.
&lt;/p&gt;</description></item><item><title>KGQuiz&#26159;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#28085;&#30422;&#19977;&#20010;&#30693;&#35782;&#39046;&#22495;&#21644;&#20116;&#20010;&#20219;&#21153;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#30693;&#35782;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09725</link><description>&lt;p&gt;
KGQuiz: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30693;&#35782;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models. (arXiv:2310.09725v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09725
&lt;/p&gt;
&lt;p&gt;
KGQuiz&#26159;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#28085;&#30422;&#19977;&#20010;&#30693;&#35782;&#39046;&#22495;&#21644;&#20116;&#20010;&#20219;&#21153;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#30693;&#35782;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#34920;&#26126;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#34987;&#32534;&#30721;&#22312;&#23427;&#20204;&#30340;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#22312;&#26377;&#38480;&#30340;&#30693;&#35782;&#39046;&#22495;&#19978;&#36827;&#34892;&#19968;&#20123;&#25506;&#32034;&#24615;&#20219;&#21153;&#20043;&#22806;&#65292;&#25105;&#20204;&#23545;&#20110;&#22914;&#20309;&#31995;&#32479;&#35780;&#20272;LLMs&#30340;&#30693;&#35782;&#33021;&#21147;&#20197;&#21450;&#23427;&#20204;&#30340;&#30693;&#35782;&#33021;&#21147;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#36880;&#28176;&#22797;&#26434;&#30340;&#20219;&#21153;&#26684;&#24335;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#24182;&#19981;&#20102;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KGQuiz&#65292;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#20840;&#38754;&#35843;&#26597;LLMs&#30340;&#30693;&#35782;&#27867;&#21270;&#33021;&#21147;&#12290;KGQuiz&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#30001;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#30693;&#35782;&#26500;&#24314;&#65292;&#28085;&#30422;&#20102;&#19977;&#20010;&#30693;&#35782;&#39046;&#22495;&#65292;&#24182;&#21253;&#25324;&#20116;&#20010;&#20219;&#21153;&#65292;&#38590;&#24230;&#36882;&#22686;&#65306;&#30495;&#20551;&#21028;&#26029;&#12289;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#12289;&#22635;&#31354;&#12289;&#20107;&#23454;&#32534;&#36753;&#21644;&#24320;&#25918;&#24335;&#30693;&#35782;&#29983;&#25104;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#30340;&#30693;&#35782;&#33021;&#21147;&#21644;&#23427;&#20204;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;10&#31181;&#24320;&#28304;&#21644;&#40657;&#30418;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate remarkable performance on knowledge-intensive tasks, suggesting that real-world knowledge is encoded in their model parameters. However, besides explorations on a few probing tasks in limited knowledge domains, it is not well understood how to evaluate LLMs' knowledge systematically and how well their knowledge abilities generalize, across a spectrum of knowledge domains and progressively complex task formats. To this end, we propose KGQuiz, a knowledge-intensive benchmark to comprehensively investigate the knowledge generalization abilities of LLMs. KGQuiz is a scalable framework constructed from triplet-based knowledge, which covers three knowledge domains and consists of five tasks with increasing complexity: true-or-false, multiple-choice QA, blank filling, factual editing, and open-ended knowledge generation. To gain a better understanding of LLMs' knowledge abilities and their generalization, we evaluate 10 open-source and black-box LLMs o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#30340;&#21453;&#39304;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#25216;&#26415;&#23558;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#19982;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05857</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Summarization with Human Edits. (arXiv:2310.05857v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#30340;&#21453;&#39304;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#25216;&#26415;&#23558;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#19982;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#33539;&#24335;&#23398;&#20064;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#22312;&#36890;&#29992;&#39046;&#22495;&#25277;&#35937;&#21270;&#25688;&#35201;&#29983;&#25104;&#20013;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24182;&#33719;&#24471;&#20102;&#36229;&#36234;&#20256;&#32479;&#20284;&#28982;&#35757;&#32451;&#30340;&#25688;&#35201;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#36739;&#23569;&#25506;&#32034;&#30340;&#20154;&#31867;&#21453;&#39304;&#24418;&#24335;&#8212;&#8212;&#20154;&#24037;&#32534;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#8212;&#8212;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#65292;&#22312;&#35757;&#32451;&#24490;&#29615;&#20013;&#21516;&#26102;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#29616;&#26377;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22522;&#20934;&#25688;&#35201;&#26469;&#27169;&#25311;&#20154;&#24037;&#32534;&#36753;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#21518;&#33719;&#21462;&#30340;&#27169;&#22411;&#29983;&#25104;&#25688;&#35201;&#65292;&#20197;&#20943;&#23569;&#23545;&#26114;&#36149;&#30340;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#21453;&#39304;&#30340;&#25506;&#32034;&#20174;&#36890;&#29992;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#25193;&#23637;&#21040;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;SALT&#22312;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training. In this paper, we focus on a less explored form of human feedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training (SALT), a novel technique to use both the human-edited and model-generated data together in the training loop. In addition, we demonstrate simulating Human Edits with ground truth summaries coming from existing training data -Imitation edits, along with the model-generated summaries obtained after the training, to reduce the need for expensive human-edit data. In our experiments, we extend human feedback exploration from general domain summarization to medical domain summarization. Our results demonstrate the effectiveness of SALT in improv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#29702;&#35299;&#21453;&#21521;&#20851;&#31995;&#26041;&#38754;&#30340;&#26080;&#25928;&#24615;&#12290;&#20316;&#32773;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;ConvRe&#30340;&#26032;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#36870;&#21521;&#20851;&#31995;&#12290;&#36890;&#36807;&#20004;&#20010;&#20219;&#21153;Re2Text&#21644;Text2Re&#65292;&#20316;&#32773;&#35780;&#20272;&#20102;LLMs&#30830;&#23450;&#20851;&#31995;&#21644;&#30456;&#20851;&#25991;&#26412;&#20043;&#38388;&#21305;&#37197;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#22312;&#27492;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.05163</link><description>&lt;p&gt;
LLMs&#22312;&#29702;&#35299;&#21453;&#21521;&#20851;&#31995;&#20013;&#30340;&#26080;&#25928;&#24615;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Investigation of LLMs' Inefficacy in Understanding Converse Relations. (arXiv:2310.05163v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#29702;&#35299;&#21453;&#21521;&#20851;&#31995;&#26041;&#38754;&#30340;&#26080;&#25928;&#24615;&#12290;&#20316;&#32773;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;ConvRe&#30340;&#26032;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#36870;&#21521;&#20851;&#31995;&#12290;&#36890;&#36807;&#20004;&#20010;&#20219;&#21153;Re2Text&#21644;Text2Re&#65292;&#20316;&#32773;&#35780;&#20272;&#20102;LLMs&#30830;&#23450;&#20851;&#31995;&#21644;&#30456;&#20851;&#25991;&#26412;&#20043;&#38388;&#21305;&#37197;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#22312;&#27492;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#24418;&#24335;&#35821;&#35328;&#23548;&#21521;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#65292;&#22914;&#32467;&#26500;&#21270;&#25968;&#25454;&#21040;&#25991;&#26412;&#21644;&#35821;&#20041;&#35299;&#26512;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#22823;&#22810;&#36981;&#24490;LLMs&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;LLMs&#30495;&#27491;&#29702;&#35299;&#24418;&#24335;&#35821;&#35328;&#30340;&#32467;&#26500;&#21270;&#35821;&#20041;&#21527;&#65311;&#26412;&#25991;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#21363;&#36870;&#21521;&#20108;&#36827;&#21046;&#20851;&#31995;&#19978;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;ConvRe&#30340;&#26032;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#36870;&#21521;&#20851;&#31995;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#25968;&#25454;&#38598;&#30340;17&#20010;&#20851;&#31995;&#21644;1240&#20010;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;ConvRE&#21253;&#25324;&#20004;&#20010;&#20219;&#21153;&#65292;Re2Text&#21644;Text2Re&#65292;&#36825;&#20123;&#20219;&#21153;&#34987;&#21046;&#23450;&#20026;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30830;&#23450;&#20851;&#31995;&#21644;&#30456;&#20851;&#25991;&#26412;&#20043;&#38388;&#21305;&#37197;&#33021;&#21147;&#12290;&#22312;&#35780;&#20272;&#21327;&#35758;&#26041;&#38754;&#65292;&#38500;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#27979;&#35797;&#25991;&#26412;&#21644;&#23569;&#26679;&#26412;&#31034;&#20363;&#25991;&#26412;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#23454;&#39564;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages. In this paper, we investigate this problem on a special case, converse binary relation. We introduce a new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are formulated as multi-choice question answering to evaluate LLMs' ability to determine the matching between relations and associated text. For the evaluation protocol, apart from different prompting methods, we further introduce variants to the test text and few-shot example text. We conduct experiments on three
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#24314;&#31435;HalluQA&#22522;&#20934;&#27979;&#35797;&#21644;&#20351;&#29992;GPT-4&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;18&#20010;&#27169;&#22411;&#30340;&#38750;&#24187;&#35273;&#29575;&#20302;&#20110;50%&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#31867;&#22411;&#21644;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2310.03368</link><description>&lt;p&gt;
&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Evaluating Hallucinations in Chinese Large Language Models. (arXiv:2310.03368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#24314;&#31435;HalluQA&#22522;&#20934;&#27979;&#35797;&#21644;&#20351;&#29992;GPT-4&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;18&#20010;&#27169;&#22411;&#30340;&#38750;&#24187;&#35273;&#29575;&#20302;&#20110;50%&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#31867;&#22411;&#21644;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;HalluQA&#65288;&#20013;&#25991;&#24187;&#35273;&#38382;&#31572;&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;HalluQA&#21253;&#21547;450&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#24615;&#38382;&#39064;&#65292;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#65292;&#24182;&#32771;&#34385;&#20102;&#20013;&#22269;&#21382;&#21490;&#25991;&#21270;&#12289;&#39118;&#20439;&#21644;&#31038;&#20250;&#29616;&#35937;&#12290;&#22312;&#26500;&#24314;HalluQA&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24187;&#35273;&#31867;&#22411;&#65306;&#27169;&#20223;&#24615;&#34394;&#20551;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#22522;&#20110;GLM-130B&#21644;ChatGPT&#26500;&#24314;&#23545;&#25239;&#26679;&#26412;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-4&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#26469;&#21028;&#26029;&#27169;&#22411;&#36755;&#20986;&#26159;&#21542;&#26159;&#24187;&#35273;&#12290;&#25105;&#20204;&#23545;24&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;ERNIE-Bot&#12289;Baichuan2&#12289;ChatGLM&#12289;Qwen&#12289;SparkDesk&#31561;&#12290;&#22312;&#36825;24&#20010;&#27169;&#22411;&#20013;&#65292;&#26377;18&#20010;&#30340;&#38750;&#24187;&#35273;&#29575;&#20302;&#20110;50%&#12290;&#36825;&#34920;&#26126;HalluQA&#20855;&#26377;&#24456;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#27169;&#22411;&#20013;&#20027;&#35201;&#30340;&#24187;&#35273;&#31867;&#22411;&#21450;&#20854;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we establish a benchmark named HalluQA (Chinese Hallucination Question-Answering) to measure the hallucination phenomenon in Chinese large language models. HalluQA contains 450 meticulously designed adversarial questions, spanning multiple domains, and takes into account Chinese historical culture, customs, and social phenomena. During the construction of HalluQA, we consider two types of hallucinations: imitative falsehoods and factual errors, and we construct adversarial samples based on GLM-130B and ChatGPT. For evaluation, we design an automated evaluation method using GPT-4 to judge whether a model output is hallucinated. We conduct extensive experiments on 24 large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than 50%. This indicates that HalluQA is highly challenging. We analyze the primary types of hallucinations in different types of models and their causes. Add
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25253;&#36947;&#20102;BioLaySumm 2023&#20849;&#20139;&#20219;&#21153;&#30340;&#32467;&#26524;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#24320;&#21457;&#25277;&#35937;&#21270;&#25688;&#35201;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21487;&#25511;&#25110;&#19981;&#21487;&#25511;&#30340;&#29615;&#22659;&#20013;&#29983;&#25104;&#24120;&#35268;&#25688;&#35201;&#65292;&#26377;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#24120;&#35268;&#25688;&#35201;&#21644;&#21487;&#35835;&#24615;&#25511;&#21046;&#30340;&#25688;&#35201;&#12290;&#20849;&#26377;20&#20010;&#22242;&#38431;&#21442;&#19982;&#20102;&#35813;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.17332</link><description>&lt;p&gt;
&#20851;&#20110;BioLaySumm 2023&#20849;&#20139;&#20219;&#21153;&#30340;&#32508;&#36848;&#65306;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#30340;&#31616;&#21270;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of Biomedical Research Articles. (arXiv:2309.17332v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17332
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25253;&#36947;&#20102;BioLaySumm 2023&#20849;&#20139;&#20219;&#21153;&#30340;&#32467;&#26524;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#24320;&#21457;&#25277;&#35937;&#21270;&#25688;&#35201;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21487;&#25511;&#25110;&#19981;&#21487;&#25511;&#30340;&#29615;&#22659;&#20013;&#29983;&#25104;&#24120;&#35268;&#25688;&#35201;&#65292;&#26377;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#24120;&#35268;&#25688;&#35201;&#21644;&#21487;&#35835;&#24615;&#25511;&#21046;&#30340;&#25688;&#35201;&#12290;&#20849;&#26377;20&#20010;&#22242;&#38431;&#21442;&#19982;&#20102;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;ACL 2023&#30340;BioNLP&#30740;&#35752;&#20250;&#19978;&#20030;&#21150;&#30340;&#20851;&#20110;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#31616;&#21270;&#25688;&#35201;&#30340;&#20849;&#20139;&#20219;&#21153;&#65288;BioLaySumm&#65289;&#30340;&#32467;&#26524;&#12290;&#35813;&#20849;&#20139;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#33021;&#22815;&#29983;&#25104;&#8220;&#24120;&#35268;&#25688;&#35201;&#8221;&#65288;&#21363;&#21487;&#29702;&#35299;&#32473;&#38750;&#25216;&#26415;&#20154;&#21592;&#30340;&#25688;&#35201;&#65289;&#30340;&#25277;&#35937;&#21270;&#25688;&#35201;&#27169;&#22411;&#65292;&#26080;&#35770;&#22312;&#21487;&#25511;&#25110;&#19981;&#21487;&#25511;&#30340;&#29615;&#22659;&#20013;&#12290;&#20849;&#20139;&#20219;&#21153;&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;1&#65289;&#24120;&#35268;&#25688;&#35201;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#26681;&#25454;&#25552;&#20379;&#30340;&#23436;&#25972;&#25991;&#31456;&#25991;&#26412;&#21644;&#23545;&#24212;&#30340;&#25688;&#35201;&#20316;&#20026;&#36755;&#20837;&#65292;&#26500;&#24314;&#20135;&#29983;&#24120;&#35268;&#25688;&#35201;&#30340;&#27169;&#22411;&#65307;2&#65289;&#21487;&#35835;&#24615;&#25511;&#21046;&#30340;&#25688;&#35201;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#26681;&#25454;&#25991;&#31456;&#30340;&#20027;&#35201;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#35757;&#32451;&#27169;&#22411;&#26469;&#29983;&#25104;&#25216;&#26415;&#25688;&#35201;&#21644;&#24120;&#35268;&#25688;&#35201;&#12290;&#38500;&#20102;&#24635;&#20307;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;BioLaySumm&#20849;&#20139;&#20219;&#21153;&#30340;&#35774;&#32622;&#21644;&#35265;&#35299;&#65292;&#20849;&#26377;20&#20010;&#21442;&#19982;&#22242;&#38431;&#21442;&#19982;&#20102;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of the shared task on Lay Summarisation of Biomedical Research Articles (BioLaySumm), hosted at the BioNLP Workshop at ACL 2023. The goal of this shared task is to develop abstractive summarisation models capable of generating "lay summaries" (i.e., summaries that are comprehensible to non-technical audiences) in both a controllable and non-controllable setting. There are two subtasks: 1) Lay Summarisation, where the goal is for participants to build models for lay summary generation only, given the full article text and the corresponding abstract as input; and 2) Readability-controlled Summarisation, where the goal is for participants to train models to generate both the technical abstract and the lay summary, given an article's main text as input. In addition to overall results, we report on the setup and insights from the BioLaySumm shared task, which attracted a total of 20 participating teams across both subtasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22797;&#29616;&#20102;Whisper&#39118;&#26684;&#30340;&#35757;&#32451;&#65292;&#20351;&#29992;&#24320;&#28304;&#24037;&#20855;&#21644;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OWSM&#30340;&#27169;&#22411;&#65292;&#25903;&#25345;&#26356;&#22810;&#30340;&#32763;&#35793;&#26041;&#21521;&#24182;&#19988;&#26356;&#39640;&#25928;&#22320;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.13876</link><description>&lt;p&gt;
&#20351;&#29992;&#24320;&#28304;&#24037;&#20855;&#21644;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#22797;&#29616;Whisper&#39118;&#26684;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data. (arXiv:2309.13876v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22797;&#29616;&#20102;Whisper&#39118;&#26684;&#30340;&#35757;&#32451;&#65292;&#20351;&#29992;&#24320;&#28304;&#24037;&#20855;&#21644;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OWSM&#30340;&#27169;&#22411;&#65292;&#25903;&#25345;&#26356;&#22810;&#30340;&#32763;&#35793;&#26041;&#21521;&#24182;&#19988;&#26356;&#39640;&#25928;&#22320;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;OpenAI&#30340;Whisper&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#32463;&#36807;&#20102;680k&#23567;&#26102;&#30340;&#30417;&#30563;&#24335;&#35821;&#38899;&#25968;&#25454;&#35757;&#32451;&#12290;&#23427;&#22312;&#21508;&#31181;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#20063;&#33021;&#22815;&#21457;&#25381;&#33391;&#22909;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#31181;&#27169;&#22411;&#30340;&#23436;&#25972;&#27969;&#31243;&#65288;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#35757;&#32451;&#65289;&#24182;&#19981;&#20844;&#24320;&#21487;&#35775;&#38382;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#38590;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#20854;&#24615;&#33021;&#24182;&#35299;&#20915;&#35757;&#32451;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#22914;&#25928;&#29575;&#12289;&#20581;&#22766;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Open Whisper-style Speech Model&#65288;OWSM&#65289;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#24320;&#28304;&#24037;&#20855;&#21644;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#22797;&#29616;&#20102;Whisper&#39118;&#26684;&#30340;&#35757;&#32451;&#12290;OWSM&#29978;&#33267;&#25903;&#25345;&#26356;&#22810;&#30340;&#32763;&#35793;&#26041;&#21521;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#21457;&#24067;&#29992;&#20110;&#25968;&#25454;&#20934;&#22791;&#12289;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#35780;&#20998;&#30340;&#25152;&#26377;&#33050;&#26412;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35757;&#32451;&#26085;&#24535;&#65292;&#20197;&#20419;&#36827;&#24320;&#25918;&#31185;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper-style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre-trained models and training logs to promote open science.
&lt;/p&gt;</description></item><item><title>NJUNLP&#22242;&#38431;&#23545;WMT2023&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#36827;&#34892;&#20102;&#25237;&#31295;&#65292;&#36890;&#36807;&#20351;&#29992;&#20266;&#25968;&#25454;&#26041;&#27861;&#21644;&#26680;&#24515;&#36229;&#21442;&#25968;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20182;&#20204;&#30340;&#27169;&#22411;&#22312;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#36136;&#37327;&#39044;&#27979;&#21644;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13230</link><description>&lt;p&gt;
NJUNLP&#23545;WMT2023&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#30340;&#21442;&#19982;
&lt;/p&gt;
&lt;p&gt;
NJUNLP's Participation for the WMT2023 Quality Estimation Shared Task. (arXiv:2309.13230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13230
&lt;/p&gt;
&lt;p&gt;
NJUNLP&#22242;&#38431;&#23545;WMT2023&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#36827;&#34892;&#20102;&#25237;&#31295;&#65292;&#36890;&#36807;&#20351;&#29992;&#20266;&#25968;&#25454;&#26041;&#27861;&#21644;&#26680;&#24515;&#36229;&#21442;&#25968;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20182;&#20204;&#30340;&#27169;&#22411;&#22312;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#36136;&#37327;&#39044;&#27979;&#21644;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;NJUNLP&#22242;&#38431;&#22312;WMT 2023&#36136;&#37327;&#20272;&#35745;&#65288;QE&#65289;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#25237;&#31295;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#25552;&#20132;&#20102;&#23545;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#25152;&#26377;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#39044;&#27979;&#65306;&#65288;i&#65289;&#21477;&#23376;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#36136;&#37327;&#39044;&#27979;&#65307;&#65288;ii&#65289;&#32454;&#31890;&#24230;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#12290;&#20170;&#24180;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#22522;&#20110;NJUQE&#26694;&#26550;&#65288;https://github.com/NJUNLP/njuqe&#65289;&#30340;&#20266;&#25968;&#25454;&#26041;&#27861;&#36827;&#34892;QE&#12290;&#25105;&#20204;&#20351;&#29992;WMT&#32763;&#35793;&#20219;&#21153;&#30340;&#24182;&#34892;&#25968;&#25454;&#29983;&#25104;&#20266;MQM&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20266;QE&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;XLMR&#22823;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;QE&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#20004;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20849;&#21516;&#23398;&#20064;&#21477;&#23376;&#32423;&#20998;&#25968;&#21644;&#21333;&#35789;&#32423;&#26631;&#31614;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#26469;&#23547;&#25214;&#25913;&#21892;&#24615;&#33021;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#12290;&#22312;&#25216;&#26415;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#35789;&#32423;&#36755;&#20986;&#36716;&#25442;&#20026;&#32454;&#31890;&#24230;&#38169;&#35823;&#36328;&#24230;&#32467;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#21333;&#35789;&#32423;&#21035;&#21644;&#32454;&#31890;&#24230;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#23376;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the submissions of the NJUNLP team to the WMT 2023 Quality Estimation (QE) shared task. Our team submitted predictions for the English-German language pair on all two sub-tasks: (i) sentence- and word-level quality prediction; and (ii) fine-grained error span detection. This year, we further explore pseudo data methods for QE based on NJUQE framework (https://github.com/NJUNLP/njuqe). We generate pseudo MQM data using parallel data from the WMT translation task. We pre-train the XLMR large model on pseudo QE data, then fine-tune it on real QE data. At both stages, we jointly learn sentence-level scores and word-level tags. Empirically, we conduct experiments to find the key hyper-parameters that improve the performance. Technically, we propose a simple method that covert the word-level outputs to fine-grained error span results. Overall, our models achieved the best results in English-German for both word-level and fine-grained error span detection sub-tasks by a considera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.09357</link><description>&lt;p&gt;
Talk2Care: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20419;&#36827;&#24322;&#27493;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#36828;&#31243;&#21307;&#30103;&#24212;&#29992;&#31243;&#24207;&#26469;&#24110;&#21161;&#23478;&#24237;&#20013;&#30340;&#32769;&#24180;&#20154;&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;&#65292;&#20294;&#22522;&#26412;&#30340;&#28040;&#24687;&#21644;&#30005;&#35805;&#20173;&#28982;&#26159;&#26368;&#24120;&#35265;&#30340;&#36890;&#20449;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#26377;&#38480;&#30340;&#21487;&#29992;&#24615;&#12289;&#20449;&#24687;&#20002;&#22833;&#21644;&#27969;&#31243;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#20419;&#36827;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21450;&#20854;&#24378;&#22823;&#30340;&#33258;&#28982;&#23545;&#35805;&#21644;&#25688;&#35201;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLMs&#22312;&#36890;&#20449;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#36824;&#23384;&#22312;&#26377;&#38480;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#20004;&#39033;&#35775;&#35848;&#30740;&#31350;&#65292;&#20998;&#21035;&#19982;&#32769;&#24180;&#20154;(N=10)&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;(N=9)&#36827;&#34892;&#20102;&#20132;&#27969;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#22312;&#24739;&#32773;-&#21307;&#29983;&#24322;&#27493;&#36890;&#20449;&#20013;&#23545;LLMs&#30340;&#38656;&#27714;&#21644;&#26426;&#20250;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;Talk2Care&#65292;&#24182;&#20026;&#20004;&#20010;&#32676;&#20307;&#35774;&#35745;&#20102;&#20132;&#20114;&#32452;&#20214;: (1) &#23545;&#20110;&#32769;&#24180;&#20154;&#65292;&#25105;&#20204;&#21033;&#29992;&#35821;&#38899;&#21161;&#25163;&#30340;&#20415;&#21033;&#24615;&#21644;&#26131;&#20110;&#33719;&#21462;&#24615;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#35821;&#38899;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered VA i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26469;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#31038;&#21306;&#39537;&#21160;&#30340;&#27880;&#37322;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#22522;&#20934;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2309.06578</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#36776;&#21035;&#31185;&#23398;&#20551;&#35774;&#30340;&#35777;&#25454;&#65311;&#31038;&#20250;&#31185;&#23398;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences. (arXiv:2309.06578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26469;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#31038;&#21306;&#39537;&#21160;&#30340;&#27880;&#37322;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#22522;&#20934;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#30340;&#21046;&#23450;&#21644;&#27979;&#35797;&#26159;&#32463;&#39564;&#24615;&#30740;&#31350;&#30340;&#26680;&#24515;&#12290;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#20551;&#35774;&#26159;&#22522;&#20110;&#29616;&#26377;&#35777;&#25454;&#30340;&#26368;&#20339;&#29468;&#27979;&#65292;&#24182;&#19988;&#26159;&#22522;&#20110;&#30456;&#20851;&#25991;&#29486;&#30340;&#20840;&#38754;&#35270;&#22270;&#36827;&#34892;&#21551;&#21457;&#30340;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27599;&#24180;&#31185;&#23398;&#25991;&#31456;&#25968;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#23545;&#20110;&#32473;&#23450;&#20551;&#35774;&#30456;&#20851;&#35777;&#25454;&#30340;&#25163;&#21160;&#27719;&#24635;&#21644;&#32508;&#21512;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#20013;&#30340;&#35777;&#25454;&#65292;&#33021;&#21542;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20849;&#20139;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#20013;&#20351;&#29992;&#31038;&#21306;&#39537;&#21160;&#30340;&#30740;&#31350;&#27880;&#37322;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#24615;&#33021;&#19982;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#25351;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/Sai90000/ScientificHypothesisEvidencing.git&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypothesis formulation and testing are central to empirical research. A strong hypothesis is a best guess based on existing evidence and informed by a comprehensive view of relevant literature. However, with exponential increase in the number of scientific articles published annually, manual aggregation and synthesis of evidence related to a given hypothesis is a challenge. Our work explores the ability of current large language models (LLMs) to discern evidence in support or refute of specific hypotheses based on the text of scientific abstracts. We share a novel dataset for the task of scientific hypothesis evidencing using community-driven annotations of studies in the social sciences. We compare the performance of LLMs to several state-of-the-art benchmarks and highlight opportunities for future research in this area. The dataset is available at https://github.com/Sai90000/ScientificHypothesisEvidencing.git
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65292;&#23558;&#20219;&#21153;&#20449;&#24687;&#19982;MoE&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#26032;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.15772</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#29992;&#20110;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Task-Based MoE for Multitask Multilingual Machine Translation. (arXiv:2308.15772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65292;&#23558;&#20219;&#21153;&#20449;&#24687;&#19982;MoE&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#26032;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#30340;&#22810;&#31181;&#24212;&#29992;&#20013;&#65292;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;MoE&#23454;&#29616;&#26159;&#20219;&#21153;&#26080;&#20851;&#30340;&#65292;&#23558;&#19981;&#21516;&#20219;&#21153;&#30340;&#25152;&#26377;&#26631;&#35760;&#20197;&#30456;&#21516;&#26041;&#24335;&#22788;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#21160;&#24577;&#22522;&#20110;&#20219;&#21153;&#30340;&#36866;&#37197;&#22120;&#65292;&#22312;MoE&#27169;&#22411;&#30340;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#23558;&#20219;&#21153;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#19978;&#30340;&#20248;&#21183;&#12290;&#20511;&#21161;&#20219;&#21153;&#29305;&#23450;&#30340;&#36866;&#37197;&#22120;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#39640;&#25928;&#22320;&#25512;&#24191;&#21040;&#26032;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-experts (MoE) architecture has been proven a powerful method for diverse tasks in training deep models in many applications. However, current MoE implementations are task agnostic, treating all tokens from different tasks in the same manner. In this work, we instead design a novel method that incorporates task information into MoE models at different granular levels with shared dynamic task-based adapters. Our experiments and analysis show the advantages of our approaches over the dense and canonical MoE models on multi-task multilingual machine translations. With task-specific adapters, our models can additionally generalize to new tasks efficiently.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SeamlessM4T&#65292;&#36825;&#26159;&#19968;&#20010;&#25903;&#25345;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#35821;&#38899;&#25968;&#25454;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#32763;&#35793;&#65292;&#20197;&#21450;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11596</link><description>&lt;p&gt;
SeamlessM4T-&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation. (arXiv:2308.11596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SeamlessM4T&#65292;&#36825;&#26159;&#19968;&#20010;&#25903;&#25345;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#35821;&#38899;&#25968;&#25454;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#32763;&#35793;&#65292;&#20197;&#21450;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#19968;&#31181;&#31867;&#20284;&#20110;&#24052;&#21035;&#40060;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#20154;&#22312;&#20219;&#24847;&#20004;&#31181;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#35821;&#38899;&#32763;&#35793;&#65292;&#38656;&#35201;&#20184;&#20986;&#20160;&#20040;&#26679;&#30340;&#21162;&#21147;&#65311;&#34429;&#28982;&#26368;&#36817;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#20351;&#26426;&#22120;&#32763;&#35793;&#30340;&#35206;&#30422;&#33539;&#22260;&#36229;&#36807;&#20102;200&#31181;&#35821;&#35328;&#65292;&#20294;&#32479;&#19968;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#36824;&#27809;&#26377;&#21462;&#24471;&#31867;&#20284;&#30340;&#36827;&#23637;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20256;&#32479;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#20381;&#36182;&#20110;&#28176;&#36827;&#24335;&#30340;&#32423;&#32852;&#31995;&#32479;&#36827;&#34892;&#32763;&#35793;&#65292;&#20351;&#39640;&#24615;&#33021;&#30340;&#32479;&#19968;&#31995;&#32479;&#38590;&#20197;&#23454;&#29616;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SeamlessM4T&#65292;&#19968;&#31181;&#25903;&#25345;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#25991;&#26412;&#32763;&#35793;&#20197;&#21450;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#25903;&#25345;&#22810;&#36798;100&#31181;&#35821;&#35328;&#12290;&#20026;&#20102;&#26500;&#24314;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;100&#19975;&#23567;&#26102;&#30340;&#24320;&#25918;&#24335;&#35821;&#38899;&#38899;&#39057;&#25968;&#25454;&#65292;&#20351;&#29992;&#20102;w2v-BERT 2.0&#26469;&#23398;&#20064;&#33258;&#30417;&#30563;&#30340;&#35821;&#38899;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#33258;&#21160;&#23545;&#40784;&#35821;&#38899;&#32763;&#35793;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages? While recent breakthroughs in text-based models have pushed machine translation coverage beyond 200 languages, unified speech-to-speech translation models have yet to achieve similar strides. More specifically, conventional speech-to-speech translation systems rely on cascaded systems that perform translation progressively, putting high-performing unified systems out of reach. To address these gaps, we introduce SeamlessM4T, a single model that supports speech-to-speech translation, speech-to-text translation, text-to-speech translation, text-to-text translation, and automatic speech recognition for up to 100 languages. To build this, we used 1 million hours of open speech audio data to learn self-supervised speech representations with w2v-BERT 2.0. Subsequently, we created a multimodal corpus of automatically aligned speech translations. Filtered and combined with h
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;KGSimple&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#25216;&#26415;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#25991;&#26412;&#31616;&#21270;&#65292;&#23454;&#29616;&#20174;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#29983;&#25104;&#31616;&#26126;&#25991;&#26412;&#65292;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#24182;&#36755;&#20986;&#27969;&#30021;&#19988;&#25551;&#36848;&#24615;&#30340;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2308.06975</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#33021;&#31616;&#21270;&#25991;&#26412;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Knowledge Graphs Simplify Text?. (arXiv:2308.06975v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06975
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;KGSimple&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#25216;&#26415;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#25991;&#26412;&#31616;&#21270;&#65292;&#23454;&#29616;&#20174;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#29983;&#25104;&#31616;&#26126;&#25991;&#26412;&#65292;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#24182;&#36755;&#20986;&#27969;&#30021;&#19988;&#25551;&#36848;&#24615;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#21040;&#25991;&#26412;&#29983;&#25104;&#22312;&#29983;&#25104;&#27969;&#30021;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#21477;&#23376;&#26041;&#38754;&#26377;&#20102;&#26368;&#26032;&#30340;&#25913;&#36827;&#65292;&#36825;&#20123;&#21477;&#23376;&#25551;&#36848;&#20102;&#32473;&#23450;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#30001;&#20110;&#30693;&#35782;&#22270;&#35889;&#22312;&#22810;&#20010;&#39046;&#22495;&#24191;&#27867;&#23384;&#22312;&#19988;&#21253;&#21547;&#37325;&#35201;&#30340;&#23454;&#20307;&#20851;&#31995;&#20449;&#24687;&#65292;&#24182;&#19988;&#25991;&#26412;&#31616;&#21270;&#26088;&#22312;&#20943;&#23569;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25991;&#26412;&#30340;&#24847;&#24605;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KGSimple&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25991;&#26412;&#31616;&#21270;&#26041;&#27861;&#65292;&#23427;&#34701;&#20837;&#20102;&#30693;&#35782;&#22270;&#35889;&#25216;&#26415;&#26469;&#26500;&#24314;&#31616;&#21270;&#30340;&#30693;&#35782;&#22270;&#35889;&#36335;&#24452;&#65292;&#24182;&#29983;&#25104;&#20445;&#30041;&#21407;&#22987;&#36755;&#20837;&#24847;&#20041;&#30340;&#31616;&#26126;&#25991;&#26412;&#12290;&#36890;&#36807;&#36845;&#20195;&#21644;&#37319;&#26679;&#30340;&#20197;&#30693;&#35782;&#22270;&#35889;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20174;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#31616;&#21270;&#25991;&#26412;&#65292;&#36890;&#36807;&#23398;&#20064;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#24182;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21040;&#25991;&#26412;&#29983;&#25104;&#36755;&#20986;&#27969;&#30021;&#19988;&#25551;&#36848;&#24615;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#22312;&#30446;&#21069;&#21487;&#29992;&#30340;&#30693;&#35782;&#22270;&#35889;&#21040;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;KGSimple&#27169;&#22411;&#30340;&#21508;&#31181;&#35774;&#32622;&#65292;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#26080;&#30417;&#30563;&#25991;&#26412;&#31616;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph (KG)-to-Text Generation has seen recent improvements in generating fluent and informative sentences which describe a given KG. As KGs are widespread across multiple domains and contain important entity-relation information, and as text simplification aims to reduce the complexity of a text while preserving the meaning of the original text, we propose KGSimple, a novel approach to unsupervised text simplification which infuses KG-established techniques in order to construct a simplified KG path and generate a concise text which preserves the original input's meaning. Through an iterative and sampling KG-first approach, our model is capable of simplifying text when starting from a KG by learning to keep important information while harnessing KG-to-text generation to output fluent and descriptive sentences. We evaluate various settings of the KGSimple model on currently-available KG-to-text datasets, demonstrating its effectiveness compared to unsupervised text simplificat
&lt;/p&gt;</description></item><item><title>AgentBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;&#30340;&#22810;&#32500;&#24230;&#22522;&#20934;&#65292;&#21457;&#29616;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#21830;&#19994;LLMs&#22312;&#20805;&#24403;&#20195;&#29702;&#20154;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#19982;&#24320;&#28304;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#38271;&#26399;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#19978;&#30340;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2308.03688</link><description>&lt;p&gt;
AgentBench: &#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
AgentBench: Evaluating LLMs as Agents. (arXiv:2308.03688v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03688
&lt;/p&gt;
&lt;p&gt;
AgentBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;&#30340;&#22810;&#32500;&#24230;&#22522;&#20934;&#65292;&#21457;&#29616;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#21830;&#19994;LLMs&#22312;&#20805;&#24403;&#20195;&#29702;&#20154;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#19982;&#24320;&#28304;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#38271;&#26399;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#19978;&#30340;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21464;&#24471;&#36234;&#26469;&#36234;&#26234;&#33021;&#21644;&#33258;&#20027;&#65292;&#38024;&#23545;&#20256;&#32479;&#30340;NLP&#20219;&#21153;&#20043;&#22806;&#30340;&#29616;&#23454;&#19990;&#30028;&#23454;&#38469;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#22312;&#20114;&#21160;&#29615;&#22659;&#20013;&#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AgentBench&#65292;&#19968;&#20010;&#22810;&#32500;&#24230;&#28436;&#21464;&#30340;&#22522;&#20934;&#65292;&#30446;&#21069;&#21253;&#25324;8&#20010;&#19981;&#21516;&#30340;&#29615;&#22659;&#65292;&#20197;&#35780;&#20272;LLM&#20316;&#20026;&#20195;&#29702;&#20154;&#22312;&#22810;&#36718;&#24320;&#25918;&#24335;&#29983;&#25104;&#35774;&#32622;&#20013;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;27&#20010;&#22522;&#20110;API&#21644;&#24320;&#28304;&#30340;LLM&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#39030;&#32423;&#21830;&#19994;LLM&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20195;&#29702;&#20154;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#19982;&#24320;&#28304;&#31454;&#20105;&#23545;&#25163;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#24456;&#22823;&#12290;&#25105;&#20204;&#25214;&#20986;&#20102;&#29615;&#22659;&#21644;LLM&#20013;&#22833;&#36133;&#30340;&#20856;&#22411;&#21407;&#22240;&#65292;&#34920;&#26126;&#38271;&#26399;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#36981;&#24490;&#25351;&#31034;&#33021;&#21147;&#19981;&#20339;&#26159;&#24320;&#21457;&#21487;&#29992;LLM&#20195;&#29702;&#20154;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#36890;&#36807;&#23545;&#20195;&#30721;&#21644;&#39640;&#36136;&#37327;&#36827;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Training on code and high quality 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#65288;KTGF&#65289;&#29992;&#20110;&#21307;&#23398;&#23545;&#35805;&#20449;&#24687;&#25552;&#21462;&#12290;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#29983;&#25104;&#65292;&#20998;&#21035;&#29983;&#25104;&#21307;&#23398;&#23545;&#35805;&#20013;&#30340;&#26415;&#35821;&#21644;&#27599;&#20010;&#26415;&#35821;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.16200</link><description>&lt;p&gt;
&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#29992;&#20110;&#21307;&#23398;&#23545;&#35805;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction. (arXiv:2307.16200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#65288;KTGF&#65289;&#29992;&#20110;&#21307;&#23398;&#23545;&#35805;&#20449;&#24687;&#25552;&#21462;&#12290;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#29983;&#25104;&#65292;&#20998;&#21035;&#29983;&#25104;&#21307;&#23398;&#23545;&#35805;&#20013;&#30340;&#26415;&#35821;&#21644;&#27599;&#20010;&#26415;&#35821;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#21307;&#23398;&#23545;&#35805;&#20013;&#30340;&#26415;&#35821;-&#29366;&#24577;&#23545;&#25552;&#21462;&#65288;MD-TSPE&#65289;&#65292;&#36825;&#22312;&#35786;&#26029;&#23545;&#35805;&#31995;&#32479;&#21644;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65288;EMR&#65289;&#30340;&#33258;&#21160;&#25220;&#20889;&#20013;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;MD-TSPE&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#20043;&#21518;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#26041;&#27861;&#22312;&#19968;&#38454;&#27573;&#36755;&#20986;&#25972;&#20010;&#30001;&#26415;&#35821;-&#29366;&#24577;&#23545;&#32452;&#25104;&#30340;&#24207;&#21015;&#26102;&#24573;&#30053;&#20102;&#38598;&#25104;&#20808;&#21069;&#30693;&#35782;&#30340;&#38656;&#27714;&#65292;&#36825;&#38656;&#35201;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#26469;&#24314;&#27169;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#25512;&#26029;&#27599;&#20010;&#26415;&#35821;&#30340;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#65288;KTGF&#65289;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#37319;&#29992;&#21333;&#19968;&#27169;&#22411;&#20197;&#32479;&#19968;&#30340;&#29983;&#25104;&#24418;&#24335;&#23436;&#25104;MD-TSPE&#30340;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#29983;&#25104;&#25152;&#26377;&#30340;&#26415;&#35821;&#65292;&#28982;&#21518;&#29983;&#25104;&#27599;&#20010;&#29983;&#25104;&#30340;&#26415;&#35821;&#30340;&#29366;&#24577;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on term-status pair extraction from medical dialogues (MD-TSPE), which is essential in diagnosis dialogue systems and the automatic scribe of electronic medical records (EMRs). In the past few years, works on MD-TSPE have attracted increasing research attention, especially after the remarkable progress made by generative methods. However, these generative methods output a whole sequence consisting of term-status pairs in one stage and ignore integrating prior knowledge, which demands a deeper understanding to model the relationship between terms and infer the status of each term. This paper presents a knowledge-enhanced two-stage generative framework (KTGF) to address the above challenges. Using task-specific prompts, we employ a single model to complete the MD-TSPE through two phases in a unified generative form: we generate all terms the first and then generate the status of each generated term. In this way, the relationship between terms can be learned more effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#21644;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#25991;&#26412;&#22686;&#24378;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#26694;&#26550;&#36873;&#25321;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#25110;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2307.15776</link><description>&lt;p&gt;
&#36873;&#25321;&#21644;&#22686;&#24378;&#65306;&#22686;&#24378;&#31264;&#23494;&#26816;&#32034;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Select and Augment: Enhanced Dense Retrieval Knowledge Graph Augmentation. (arXiv:2307.15776v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#21644;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#25991;&#26412;&#22686;&#24378;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#26694;&#26550;&#36873;&#25321;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#25110;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#20013;&#65292;&#23558;&#25991;&#26412;&#20449;&#24687;&#27880;&#20837;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#20307;&#34920;&#31034;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20540;&#24471;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#20197;&#25552;&#39640;KG&#30456;&#20851;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#24120;&#29992;&#30340;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;KG&#23884;&#20837;&#30340;&#26041;&#27861;&#21253;&#25324;&#35821;&#20041;&#20016;&#23500;&#30340;&#20381;&#36182;&#35299;&#26512;&#29305;&#24449;&#12289;&#19968;&#32452;&#30456;&#20851;&#20851;&#38190;&#35789;&#65292;&#20197;&#21450;&#26469;&#33258;&#22806;&#37096;&#35821;&#26009;&#24211;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#30340;&#23436;&#25972;&#25991;&#26412;&#25551;&#36848;&#12290;&#23613;&#31649;&#36825;&#31181;&#21019;&#26032;&#65288;&#25991;&#26412;&#22686;&#24378;&#30340;KG&#23884;&#20837;&#65289;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#25105;&#20204;&#19981;&#20877;&#20351;&#29992;&#21333;&#19968;&#25991;&#26412;&#25551;&#36848;&#65288;&#22240;&#20026;&#25991;&#26412;&#30340;&#22266;&#26377;&#35821;&#20041;&#27495;&#20041;&#26080;&#27861;&#20805;&#20998;&#34920;&#31034;&#19968;&#20010;&#23454;&#20307;&#65289;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#26082;&#33021;&#36873;&#25321;&#19982;KG&#23454;&#20307;&#30456;&#20851;&#30340;&#19968;&#32452;&#25991;&#26412;&#25551;&#36848;&#65292;&#21448;&#33021;&#23558;KG&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#23545;&#40784;&#25110;&#22686;&#24378;&#12290;&#19982;&#20043;&#21069;&#23558;&#24418;&#24335;&#21270;&#23454;&#20307;&#25551;&#36848;&#25554;&#20837;&#30693;&#35782;&#24211;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#19968;&#26041;&#27861;&#26159;&#25552;&#20379;&#20102;&#23545;KG&#23884;&#20837;&#36827;&#34892;&#22686;&#24378;&#21644;&#23545;&#40784;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Injecting textual information into knowledge graph (KG) entity representations has been a worthwhile expedition in terms of improving performance in KG oriented tasks within the NLP community. External knowledge often adopted to enhance KG embeddings ranges from semantically rich lexical dependency parsed features to a set of relevant key words to entire text descriptions supplied from an external corpus such as wikipedia and many more. Despite the gains this innovation (Text-enhanced KG embeddings) has made, the proposal in this work suggests that it can be improved even further. Instead of using a single text description (which would not sufficiently represent an entity because of the inherent lexical ambiguity of text), we propose a multi-task framework that jointly selects a set of text descriptions relevant to KG entities as well as align or augment KG embeddings with text descriptions. Different from prior work that plugs formal entity descriptions declared in knowledge bases, th
&lt;/p&gt;</description></item><item><title>WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13854</link><description>&lt;p&gt;
WebArena: &#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13854
&lt;/p&gt;
&lt;p&gt;
WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#28508;&#21147;&#36880;&#28176;&#26174;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#26159;&#22312;&#31616;&#21270;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#21644;&#27979;&#35797;&#30340;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#22797;&#29616;&#30340;&#26234;&#33021;&#20307;&#25351;&#20196;&#21644;&#25511;&#21046;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#24120;&#35265;&#39046;&#22495;&#30340;&#23436;&#20840;&#21151;&#33021;&#32593;&#31449;&#30340;&#29615;&#22659;&#65292;&#20998;&#21035;&#26159;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#35770;&#22363;&#35752;&#35770;&#12289;&#21327;&#21516;&#36719;&#20214;&#24320;&#21457;&#21644;&#20869;&#23481;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#29615;&#22659;&#20351;&#29992;&#24037;&#20855;&#65288;&#22914;&#22320;&#22270;&#65289;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;&#29992;&#25143;&#25163;&#20876;&#65289;&#26469;&#40723;&#21169;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#32452;&#37325;&#28857;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;&#25105;&#20204;&#22522;&#20934;&#20219;&#21153;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#38271;&#36828;&#30340;&#35270;&#37326;&#65292;&#24182;&#19988;&#34987;&#35774;&#35745;&#20026;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#28145;&#23618;&#27425;&#30340;&#20219;&#21153;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
&lt;/p&gt;</description></item><item><title>Video-LLaMA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21033;&#29992;&#24050;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#21644;&#21548;&#35273;&#30340;&#29702;&#35299;&#38382;&#39064;&#65292;&#20854;&#20013;Video Q-former&#21644;Audio Q-former&#29992;&#20110;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#19982;&#26102;&#38388;&#21464;&#21270;&#21644;&#38899;&#39057;&#20449;&#21495;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.02858</link><description>&lt;p&gt;
Video-LLaMA&#65306;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#30340;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#38899;-&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding. (arXiv:2306.02858v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02858
&lt;/p&gt;
&lt;p&gt;
Video-LLaMA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21033;&#29992;&#24050;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#21644;&#21548;&#35273;&#30340;&#29702;&#35299;&#38382;&#39064;&#65292;&#20854;&#20013;Video Q-former&#21644;Audio Q-former&#29992;&#20110;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#19982;&#26102;&#38388;&#21464;&#21270;&#21644;&#38899;&#39057;&#20449;&#21495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;Video-LLaMA&#65292;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#21644;&#21548;&#35273;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;Video-LLaMA&#20174;&#24050;&#32463;&#39044;&#35757;&#32451;&#22909;&#30340;&#35270;&#35273;&#21644;&#38899;&#39057;&#32534;&#30721;&#22120;&#20197;&#21450;&#24050;&#32463;&#20923;&#32467;&#30340;LLMs&#36827;&#34892;&#36328;&#27169;&#24577;&#35757;&#32451;&#12290;&#30456;&#27604;&#20110;&#20043;&#21069;&#19987;&#27880;&#20110;&#38745;&#24577;&#22270;&#20687;&#29702;&#35299;&#30340;&#35270;&#35273;-LLMs&#65292;&#22914;MiniGPT-4&#21644;LLaVA&#65292;Video-LLaMA&#20027;&#35201;&#35299;&#20915;&#20004;&#20010;&#35270;&#39057;&#29702;&#35299;&#26041;&#38754;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#25429;&#25417;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#26102;&#38388;&#21464;&#21270;&#65292;&#65288;2&#65289;&#38598;&#25104;&#38899;&#39057;&#35270;&#35273;&#20449;&#21495;&#12290;&#20026;&#20102;&#20811;&#26381;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Video Q-former&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#32452;&#35013;&#21040;&#25105;&#20204;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#20013;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#35270;&#39057;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#26469;&#23398;&#20064;&#35270;&#39057;-&#35821;&#35328;&#23545;&#24212;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;ImageBind&#65292;&#19968;&#20010;&#23558;&#22810;&#31181;&#27169;&#24577;&#23545;&#40784;&#30340;&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#65292;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;ImageBind&#20043;&#19978;&#24341;&#20837;&#19968;&#20010;Audio Q-former&#65292;&#23398;&#20064;&#21512;&#29702;&#30340;&#21548;&#35273;&#26597;&#35810;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual &amp; audio encoders and the frozen LLMs. Unlike previous vision-LLMs that focus on static image comprehensions such as MiniGPT-4 and LLaVA, Video-LLaMA mainly tackles two challenges in video understanding: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble the pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities as the pre-trained audio encoder, and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#20110;&#35757;&#32451;&#20808;&#39564;&#30340;&#20381;&#36182;&#31243;&#24230;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#39057;&#29575;&#26356;&#39640;&#30340;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#65292;&#20294;&#36825;&#20063;&#20250;&#38477;&#20302;&#20854;&#29983;&#25104;&#20197;&#32763;&#36716;&#19977;&#20803;&#32452;&#20026;&#22522;&#30784;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.01755</link><description>&lt;p&gt;
&#35757;&#32451;&#20808;&#39564;&#24433;&#21709;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Training Priors Predict Text-To-Image Model Performance. (arXiv:2306.01755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#20110;&#35757;&#32451;&#20808;&#39564;&#30340;&#20381;&#36182;&#31243;&#24230;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#39057;&#29575;&#26356;&#39640;&#30340;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#65292;&#20294;&#36825;&#20063;&#20250;&#38477;&#20302;&#20854;&#29983;&#25104;&#20197;&#32763;&#36716;&#19977;&#20803;&#32452;&#20026;&#22522;&#30784;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#19968;&#20123;&#20851;&#31995;&#65292;&#27604;&#22914;&#8220;&#23431;&#33322;&#21592;&#39569;&#39532;&#8221;&#65292;&#20294;&#21364;&#19981;&#33021;&#29983;&#25104;&#30001;&#30456;&#21516;&#22522;&#26412;&#37096;&#20998;&#32452;&#25104;&#30340;&#20854;&#20182;&#20851;&#31995;&#65292;&#27604;&#22914;&#8220;&#39532;&#39569;&#23431;&#33322;&#21592;&#8221;&#12290;&#36825;&#20123;&#22833;&#36133;&#36890;&#24120;&#34987;&#35270;&#20026;&#27169;&#22411;&#20381;&#36182;&#35757;&#32451;&#20808;&#39564;&#32780;&#19981;&#26159;&#26500;&#24314;&#26032;&#39062;&#30340;&#22270;&#20687;&#32452;&#21512;&#30340;&#35777;&#25454;&#12290;&#26412;&#25991;&#30452;&#25509;&#22312;&#31283;&#23450;&#25193;&#25955;2.1&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#36890;&#36807;&#35266;&#23519;&#32452;&#25104;&#36825;&#20123;&#25552;&#31034;&#30340;&#20027;&#35821;-&#35859;&#35821;-&#23486;&#35821; (SVO) &#19977;&#20803;&#32452;&#65288;&#20363;&#22914;&#65292;&#8220;&#23431;&#33322;&#21592;&#8221;&#65292;&#8220;&#39569;&#8221;&#65292;&#8220;&#39532;&#8221;&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;SVO&#19977;&#20803;&#32452;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#27425;&#25968;&#36234;&#22810;&#65292;&#35813;&#27169;&#22411;&#23601;&#33021;&#29983;&#25104;&#19982;&#35813;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#23601;&#36234;&#22909;&#12290;&#22312;&#36825;&#37324;&#65292;&#36890;&#36807;&#23545;&#40784;&#65292;&#25105;&#20204;&#30340;&#24847;&#24605;&#26159;&#27599;&#20010;&#26415;&#35821;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#20197;&#27491;&#30830;&#30340;&#20851;&#31995;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#21152;&#30340;&#39057;&#29575;&#20063;&#20250;&#20943;&#23569;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#19982;&#32763;&#36716;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#8220;&#23431;&#33322;&#21592;&#39569;&#39532;&#8221;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#39057;&#32321;&#20986;&#29616;&#65292;&#37027;&#20040;&#8220;&#39532;&#39569;&#23431;&#33322;&#21592;&#8221;&#30340;&#23545;&#40784;&#36136;&#37327;&#23601;&#20250;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models can often generate some relations, i.e., "astronaut riding horse", but fail to generate other relations composed of the same basic parts, i.e., "horse riding astronaut". These failures are often taken as evidence that the models rely on training priors rather than constructing novel images compositionally. This paper tests this intuition directly on the stablediffusion 2.1 text-to-image model. By looking at the subject-verb-object (SVO) triads that form the backbone of these prompts (e.g., "astronaut", "ride", "horse"), we find that the more often an SVO triad appears in the training data, the better the model can generate an image aligned with that triad. Here, by aligned we mean that each of the terms appears in the generated image in the proper relation to each other. However, this increased frequency also diminishes how well the model can generate an image aligned with the flipped triad. For example, if "astronaut riding horse" appears frequently in the trainin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NUDGE&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.01439</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#24341;&#23548;&#31526;&#21495;&#25277;&#35937;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#36923;&#36753;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction. (arXiv:2306.01439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01439
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NUDGE&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#35201;&#30340;&#26377;&#38480;&#20808;&#39564;&#20351;&#20854;&#25104;&#20026;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#32534;&#30721;&#21644;&#23398;&#20064;&#31574;&#30053;&#30340;&#20027;&#35201;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#26159;&#40657;&#21283;&#23376;&#65292;&#22312;&#24037;&#20316;&#22312;&#22270;&#20687;&#32423;&#21035;&#26102;&#38590;&#20197;&#29702;&#35299;&#20195;&#29702;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#31070;&#32463;&#31526;&#21495;RL&#26088;&#22312;&#39318;&#20808;&#21019;&#24314;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21487;&#35299;&#37322;&#24615;&#19981;&#24847;&#21619;&#30528;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#24341;&#23548;&#21487;&#24494;&#20998;&#36923;&#36753;&#31574;&#30053;&#65288;NUDGE&#65289;&#12290;NUDGE&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#20505;&#36873;&#21152;&#26435;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#36923;&#36753;&#26469;&#35757;&#32451;&#36923;&#36753;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;NUDGE&#20195;&#29702;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#32988;&#36807;&#32431;&#31070;&#32463;&#20195;&#29702;&#65292;&#24182;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#21021;&#22987;&#29366;&#24577;&#21644;&#38382;&#39064;&#22823;&#23567;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
The limited priors required by neural networks make them the dominating choice to encode and learn policies using reinforcement learning (RL). However, they are also black-boxes, making it hard to understand the agent's behaviour, especially when working on the image level. Therefore, neuro-symbolic RL aims at creating policies that are interpretable in the first place. Unfortunately, interpretability is not explainability. To achieve both, we introduce Neurally gUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural network-based agents to guide the search of candidate-weighted logic rules, then uses differentiable logic to train the logic agents. Our experimental evaluation demonstrates that NUDGE agents can induce interpretable and explainable policies while outperforming purely neural ones and showing good flexibility to environments of different initial states and problem sizes.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#37325;&#28857;&#25918;&#22312;&#38544;&#24335;&#20869;&#23481;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#29702;&#21644;&#20998;&#35299;&#26041;&#27861;&#38477;&#20302;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22797;&#26434;&#24230;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23884;&#20837;&#65292;&#35745;&#31639;&#25919;&#27835;&#23398;&#21644;&#26500;&#24314;&#21457;&#29616;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.14583</link><description>&lt;p&gt;
&#35753;&#38544;&#21547;&#30340;&#26174;&#24615;&#21270;&#65306;&#20197;NLP&#20013;&#30340;&#38544;&#24335;&#20869;&#23481;&#20026;&#31532;&#19968;&#20844;&#27665;
&lt;/p&gt;
&lt;p&gt;
Making the Implicit Explicit: Implicit Content as a First Class Citizen in NLP. (arXiv:2305.14583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14583
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#37325;&#28857;&#25918;&#22312;&#38544;&#24335;&#20869;&#23481;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#29702;&#21644;&#20998;&#35299;&#26041;&#27861;&#38477;&#20302;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22797;&#26434;&#24230;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23884;&#20837;&#65292;&#35745;&#31639;&#25919;&#27835;&#23398;&#21644;&#26500;&#24314;&#21457;&#29616;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26159;&#22810;&#20803;&#21270;&#30340;&#65292;&#19968;&#20010;&#34920;&#36848;&#21487;&#20197;&#29992;&#31561;&#20215;&#30340;&#24418;&#24335;&#37325;&#30003;&#65292;&#32780;&#20854;&#20013;&#30340;&#38544;&#21547;&#21644;&#26174;&#24615;&#20869;&#23481;&#25903;&#25345;&#21508;&#31181;&#36923;&#36753;&#21644;&#35821;&#29992;&#25512;&#29702;&#12290;&#22312;&#22788;&#29702;&#34920;&#36848;&#26102;&#65292;&#25105;&#20204;&#32771;&#34385;&#36825;&#20123;&#19981;&#21516;&#30340;&#26041;&#38754;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#29702;&#35299;&#8220;&#36825;&#37324;&#24456;&#40657;&#8221;&#21487;&#33021;&#26159;&#19968;&#20010;&#26263;&#31034;&#38656;&#35201;&#25171;&#24320;&#28783;&#12290;&#28982;&#32780;&#65292;NLP&#26041;&#27861;&#36890;&#24120;&#20165;&#20165;&#22522;&#20110;&#34920;&#38754;&#24418;&#24335;&#25805;&#20316;&#65292;&#30465;&#30053;&#20102;&#36825;&#31181;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29992;&#35821;&#35328;&#26469;&#34920;&#31034;&#35821;&#35328;&#65292;&#24182;&#24341;&#23548;LLM&#23558;&#34920;&#36848;&#20998;&#35299;&#20026;&#36923;&#36753;&#21644;&#21487;&#20449;&#30340;&#25512;&#29702;&#12290;&#20998;&#35299;&#30340;&#38477;&#20302;&#22797;&#26434;&#24615;&#65292;&#20351;&#23427;&#20204;&#26356;&#23481;&#26131;&#23884;&#20837;&#65292;&#24320;&#21551;&#20102;&#26032;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21464;&#21270;&#22312;&#21477;&#23376;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25913;&#36827;&#65292;&#22312;&#35745;&#31639;&#25919;&#27835;&#23398;&#20013;&#26377;&#23454;&#36136;&#24615;&#24212;&#29992;&#65292;&#24182;&#24341;&#20986;&#19968;&#31181;&#26032;&#30340;&#26500;&#24314;&#21457;&#29616;&#36807;&#31243;&#65292;&#25105;&#20204;&#29992;&#20154;&#24037;&#27880;&#37322;&#39564;&#35777;&#20102;&#36825;&#31181;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is multifaceted. A given utterance can be re-expressed in equivalent forms, and its implicit and explicit content support various logical and pragmatic inferences. When processing an utterance, we consider these different aspects, as mediated by our interpretive goals -- understanding that "it's dark in here" may be a veiled direction to turn on a light. Nonetheless, NLP methods typically operate over the surface form alone, eliding this nuance.  In this work, we represent language with language, and direct an LLM to decompose utterances into logical and plausible inferences. The reduced complexity of the decompositions makes them easier to embed, opening up novel applications. Variations on our technique lead to state-of-the-art improvements on sentence embedding benchmarks, a substantive application in computational political science, and to a novel construct-discovery process, which we validate with human annotations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#21477;&#23376;&#23884;&#20837;&#22914;&#20309;&#25429;&#25417;&#27431;&#27954;&#22269;&#23478;&#21644;&#32844;&#19994;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#20013;&#26368;&#31361;&#20986;&#30340;&#22269;&#23478;&#29305;&#24449;&#26159;&#20854;GPD&#32463;&#27982;&#23454;&#21147;&#12290;&#26412;&#30740;&#31350;&#20013;&#30340;&#22823;&#37096;&#20998;&#22269;&#23478;&#32500;&#24230;&#19982;&#32844;&#19994;&#32500;&#24230;&#19981;&#30456;&#20851;&#65292;&#20294;&#19968;&#31181;&#27169;&#22411;&#34920;&#29616;&#20986;&#32844;&#19994;&#22768;&#26395;&#21644;&#21407;&#31821;&#22269;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36825;&#26159;&#19968;&#31181;&#28508;&#22312;&#30340;&#22522;&#20110;&#22269;&#31821;&#30340;&#27495;&#35270;&#12290;</title><link>http://arxiv.org/abs/2305.14482</link><description>&lt;p&gt;
&#26159;&#21542;&#20855;&#26377;&#22768;&#26395;&#30340;&#24037;&#20316;&#19982;&#22768;&#26395;&#39640;&#30340;&#22269;&#23478;&#30456;&#21516;&#65311;&#22810;&#35821;&#21477;&#23376;&#23884;&#20837;&#21644;&#27431;&#27954;&#22269;&#23478;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is a Prestigious Job the same as a Prestigious Country? A Case Study on Multilingual Sentence Embeddings and European Countries. (arXiv:2305.14482v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#21477;&#23376;&#23884;&#20837;&#22914;&#20309;&#25429;&#25417;&#27431;&#27954;&#22269;&#23478;&#21644;&#32844;&#19994;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#20013;&#26368;&#31361;&#20986;&#30340;&#22269;&#23478;&#29305;&#24449;&#26159;&#20854;GPD&#32463;&#27982;&#23454;&#21147;&#12290;&#26412;&#30740;&#31350;&#20013;&#30340;&#22823;&#37096;&#20998;&#22269;&#23478;&#32500;&#24230;&#19982;&#32844;&#19994;&#32500;&#24230;&#19981;&#30456;&#20851;&#65292;&#20294;&#19968;&#31181;&#27169;&#22411;&#34920;&#29616;&#20986;&#32844;&#19994;&#22768;&#26395;&#21644;&#21407;&#31821;&#22269;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36825;&#26159;&#19968;&#31181;&#28508;&#22312;&#30340;&#22522;&#20110;&#22269;&#31821;&#30340;&#27495;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#35821;&#21477;&#23376;&#34920;&#31034;&#22914;&#20309;&#25429;&#25417;&#27431;&#27954;&#22269;&#23478;&#65292;&#20197;&#21450;&#36825;&#31181;&#24046;&#24322;&#22914;&#20309;&#22312;&#27431;&#27954;&#35821;&#35328;&#20043;&#38388;&#19981;&#21516;&#12290;&#25105;&#20204;&#29992;&#27169;&#26495;&#21477;&#23376;&#25552;&#31034;&#27169;&#22411;&#65292;&#23558;&#20854;&#26426;&#22120;&#32763;&#35793;&#25104;12&#31181;&#27431;&#27954;&#35821;&#35328;&#65292;&#24182;&#20998;&#26512;&#23884;&#20837;&#20013;&#26368;&#31361;&#20986;&#30340;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23884;&#20837;&#20013;&#26368;&#31361;&#20986;&#30340;&#22269;&#23478;&#29305;&#24449;&#26159;&#20854;GPD&#32463;&#27982;&#23454;&#21147;&#12290;&#24403;&#29305;&#21035;&#35810;&#38382;&#22768;&#26395;&#39640;&#20302;&#26102;&#65292;&#23884;&#20837;&#31354;&#38388;&#28165;&#26970;&#22320;&#21306;&#20998;&#20102;&#22768;&#26395;&#39640;&#20302;&#30340;&#24037;&#20316;&#12290;&#19977;&#20010;&#21463;&#30740;&#31350;&#30340;&#27169;&#22411;&#20013;&#30340;&#22823;&#37096;&#20998;&#22269;&#23478;&#32500;&#24230;&#19982;&#32844;&#19994;&#32500;&#24230;&#19981;&#30456;&#20851;&#65292;&#20294;Distilled Multilingual Universal Sentence Encoder&#27169;&#22411;&#34920;&#29616;&#20986;&#32844;&#19994;&#22768;&#26395;&#21644;&#21407;&#31821;&#22269;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36825;&#26159;&#19968;&#31181;&#28508;&#22312;&#30340;&#22522;&#20110;&#22269;&#31821;&#30340;&#27495;&#35270;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#19982;&#19978;&#36848;&#20363;&#22806;&#24773;&#20917;&#19979;&#30340;&#21463;&#30740;&#31350;&#30340;&#34920;&#31034;&#27169;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how multilingual sentence representations capture European countries and how this differs across European languages. We prompt the models with templated sentences that we machine-translate into 12 European languages and analyze the most prominent dimensions in the embeddings. Our analysis reveals that the most prominent country feature in the embedding is its economic strength in terms of GPD. When prompted specifically for job prestige, the embedding space clearly distinguishes high and low-prestige jobs. The occupational dimension is uncorrelated with the most dominant country dimensions for three out of four studied models. One model: Distilled Multilingual Universal Sentence Encoder, however, exhibited a connection between occupational prestige and country of origin, which is a potential source of nationality-based discrimination. Our findings are consistent across languages and, to some extent, with the exception mentioned above, across studied representation models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#23398;&#20064;&#30340;&#22270;&#20687;&#25805;&#20316;&#31995;&#32479;NeuroSIM&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22810;&#36339;&#25351;&#20196;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#65292;&#21482;&#38656;&#35201;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#25110;&#36229;&#36807;SOTA&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.14410</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#36339;&#25351;&#20196;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;&#8212;&#8212;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Image Manipulation via Multi-Hop Instructions -- A New Dataset and Weakly-Supervised Neuro-Symbolic Approach. (arXiv:2305.14410v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14410
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#23398;&#20064;&#30340;&#22270;&#20687;&#25805;&#20316;&#31995;&#32479;NeuroSIM&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22810;&#36339;&#25351;&#20196;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#65292;&#21482;&#38656;&#35201;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#25110;&#36229;&#36807;SOTA&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;&#24863;&#20852;&#36259;&#65292;&#36825;&#26159;&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#31243;&#24207;&#20013;&#26377;&#29992;&#30340;&#20219;&#21153;&#65292;&#20294;&#38656;&#35201;&#23545;&#22810;&#27169;&#24577;&#31354;&#38388;&#36827;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#23398;&#20064;(NSCL)&#65292;&#35813;&#26041;&#27861;&#22312;&#35270;&#35273;&#38382;&#31572;(VQA)&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#25928;&#65292;&#25193;&#23637;&#20854;&#29992;&#20110;&#22270;&#20687;&#25805;&#20316;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#31216;&#20026;NeuroSIM&#65292;&#21487;&#20197;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#19978;&#25191;&#34892;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#65292;&#21482;&#38656;&#35201;&#20197;VQA&#30340;&#27880;&#37322;&#25968;&#25454;&#24418;&#24335;&#25552;&#20379;&#24369;&#30417;&#30563;&#12290;NeuroSIM&#23558;&#25351;&#20196;&#35299;&#26512;&#25104;&#31526;&#21495;&#31243;&#24207;&#65292;&#22522;&#20110;&#30001;&#23545;&#35937;&#23646;&#24615;&#21644;&#25805;&#20316;&#32452;&#25104;&#30340;&#19987;&#19994;&#39046;&#22495;&#35821;&#35328;(DSL)&#65292;&#25351;&#23548;&#20854;&#25191;&#34892;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NeuroSIM&#19982;&#20351;&#29992;&#30417;&#30563;&#25968;&#25454;&#36827;&#34892;&#25805;&#20316;&#30340;SOTA&#22522;&#32447;&#30456;&#27604;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#25110;&#36229;&#36807;SOTA&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are interested in image manipulation via natural language text -- a task that is useful for multiple AI applications but requires complex reasoning over multi-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning (NSCL), which has been quite effective for the task of Visual Question Answering (VQA), for the task of image manipulation. Our system referred to as NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NeuroSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides its execution. We create a new dataset for the task, and extensive experiments demonstrate that NeuroSIM is highly competitive with or beats SOTA baselines that make use of supervised data for manipulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#25991;&#26412;&#20013;&#35299;&#26512;&#20986;&#30340;&#22330;&#26223;&#22270;&#35270;&#20026;&#22270;&#20687;&#22330;&#26223;&#22270;&#30340;&#20195;&#29702;&#65292;&#24182;&#23545;&#22270;&#36827;&#34892;&#20998;&#35299;&#21644;&#22686;&#24378;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#23545;&#27604;&#23398;&#20064;&#20197;&#23558;&#21508;&#31181;&#22797;&#26434;&#24230;&#30340;&#21477;&#23376;&#23545;&#40784;&#21040;&#21516;&#19968;&#24133;&#22270;&#20687;&#19978;&#65292;&#21516;&#26102;&#22312;&#22330;&#26223;&#22270;&#31354;&#38388;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#36127;&#26679;&#26412;&#25366;&#25496;&#25216;&#26415;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13812</link><description>&lt;p&gt;
&#20174;&#22270;&#20687;-&#25991;&#26412;-&#22270;&#35889;&#31354;&#38388;&#20013;&#36827;&#34892;&#31895;&#21040;&#32454;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#25552;&#39640;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality. (arXiv:2305.13812v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#25991;&#26412;&#20013;&#35299;&#26512;&#20986;&#30340;&#22330;&#26223;&#22270;&#35270;&#20026;&#22270;&#20687;&#22330;&#26223;&#22270;&#30340;&#20195;&#29702;&#65292;&#24182;&#23545;&#22270;&#36827;&#34892;&#20998;&#35299;&#21644;&#22686;&#24378;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#23545;&#27604;&#23398;&#20064;&#20197;&#23558;&#21508;&#31181;&#22797;&#26434;&#24230;&#30340;&#21477;&#23376;&#23545;&#40784;&#21040;&#21516;&#19968;&#24133;&#22270;&#20687;&#19978;&#65292;&#21516;&#26102;&#22312;&#22330;&#26223;&#22270;&#31354;&#38388;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#36127;&#26679;&#26412;&#25366;&#25496;&#25216;&#26415;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20174;&#32780;&#20026;&#21508;&#31181;&#19979;&#28216;&#22810;&#27169;&#24577;&#20219;&#21153;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20984;&#26174;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23545;&#35937;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#30340;&#32452;&#25104;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#20005;&#37325;&#38480;&#21046;&#12290;&#22330;&#26223;&#22270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#29702;&#35299;&#22270;&#20687;&#32452;&#25104;&#30340;&#26377;&#25928;&#26041;&#24335;&#12290;&#36825;&#20123;&#26159;&#22270;&#20687;&#30340;&#22270;&#24418;&#32467;&#26500;&#21270;&#35821;&#20041;&#34920;&#31034;&#65292;&#21253;&#25324;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#12289;&#23427;&#20204;&#30340;&#23646;&#24615;&#21644;&#19982;&#22330;&#26223;&#20013;&#20854;&#20182;&#23545;&#35937;&#30340;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20174;&#25991;&#26412;&#20013;&#35299;&#26512;&#20986;&#30340;&#22330;&#26223;&#22270;&#35270;&#20026;&#22270;&#20687;&#22330;&#26223;&#22270;&#30340;&#20195;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20998;&#35299;&#21644;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#21450;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#23558;&#21508;&#31181;&#22797;&#26434;&#24230;&#30340;&#21477;&#23376;&#23545;&#40784;&#21040;&#21516;&#19968;&#24133;&#22270;&#20687;&#19978;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#22312;&#22330;&#26223;&#22270;&#31354;&#38388;&#25552;&#20986;&#20102;&#26032;&#30340;&#36127;&#26679;&#26412;&#25366;&#25496;&#25216;&#26415;&#65292;&#29992;&#20110;&#25913;&#21892;&#23545;&#27604;&#23398;&#20064;&#12290;&#22312;&#19977;&#20010;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24378;&#22823;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#32447;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#35937;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#30340;&#32452;&#25104;&#25512;&#29702;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning, leading to state-of-the-art models for various downstream multimodal tasks. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations. Scene graphs have emerged as an effective way to understand images compositionally. These are graph-structured semantic representations of images that contain objects, their attributes, and relations with other objects in a scene. In this work, we consider the scene graph parsed from text as a proxy for the image scene graph and propose a graph decomposition and augmentation framework along with a coarse-to-fine contrastive learning objective between images and text that aligns sentences of various complexities to the same image. Along with this, we propose novel negative mining techniques in the scene graph space for im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24320;&#25918;&#22495;QA&#20013;&#27495;&#20041;&#38382;&#39064;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#25552;&#38382;&#28548;&#28165;&#26469;&#30830;&#23450;&#26368;&#31526;&#21512;&#29992;&#25143;&#24847;&#22270;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.13808</link><description>&lt;p&gt;
&#24320;&#25918;&#22495;QA&#20013;&#36890;&#36807;&#25552;&#38382;&#28548;&#28165;&#35299;&#20915;&#27495;&#20041;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Asking Clarification Questions to Handle Ambiguity in Open-Domain QA. (arXiv:2305.13808v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24320;&#25918;&#22495;QA&#20013;&#27495;&#20041;&#38382;&#39064;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#25552;&#38382;&#28548;&#28165;&#26469;&#30830;&#23450;&#26368;&#31526;&#21512;&#29992;&#25143;&#24847;&#22270;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#20013;&#23384;&#22312;&#27495;&#20041;&#38382;&#39064;&#65292;&#22914;&#20309;&#25552;&#20986;&#19968;&#20010;&#20934;&#30830;&#19988;&#29420;&#19968;&#26080;&#20108;&#30340;&#38382;&#39064;&#26159;&#24456;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#36807;&#21435;&#65292;Min et al. (2020) &#36890;&#36807;&#20026;&#25152;&#26377;&#21487;&#33021;&#30340;&#35299;&#37322;&#29983;&#25104;&#28040;&#38500;&#27495;&#20041;&#30340;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26377;&#25928;&#65292;&#20294;&#24182;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#25552;&#38382;&#28548;&#28165;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29992;&#25143;&#30340;&#22238;&#31572;&#23558;&#24110;&#21161;&#30830;&#23450;&#26368;&#31526;&#21512;&#29992;&#25143;&#24847;&#22270;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;CAMBIGNQ&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;5,654&#20010;&#21547;&#26377;&#30456;&#20851;&#27573;&#33853;&#12289;&#21487;&#33021;&#30340;&#31572;&#26696;&#21644;&#28548;&#28165;&#38382;&#39064;&#30340;&#27495;&#20041;&#38382;&#39064;&#32452;&#25104;&#12290;&#36825;&#20123;&#28548;&#28165;&#38382;&#39064;&#26159;&#36890;&#36807;&#20351;&#29992;InstructGPT&#29983;&#25104;&#28982;&#21518;&#36827;&#34892;&#24517;&#35201;&#30340;&#25163;&#21160;&#20462;&#35746;&#26469;&#39640;&#25928;&#21019;&#24314;&#30340;&#12290;&#28982;&#21518;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#27495;&#20041;&#26816;&#27979;&#19978;&#33719;&#24471;&#20102;61.3 F1&#65292;&#22312;&#28548;&#28165;&#22411;QA&#19978;&#33719;&#24471;&#20102;40.5 F1&#65292;&#20026;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#31572;&#26696;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ambiguous questions persist in open-domain question answering, because formulating a precise question with a unique answer is often challenging. Previously, Min et al. (2020) have tackled this issue by generating disambiguated questions for all possible interpretations of the ambiguous question. This can be effective, but not ideal for providing an answer to the user. Instead, we propose to ask a clarification question, where the user's response will help identify the interpretation that best aligns with the user's intention. We first present CAMBIGNQ, a dataset consisting of 5,654 ambiguous questions, each with relevant passages, possible answers, and a clarification question. The clarification questions were efficiently created by generating them using InstructGPT and manually revising them as necessary. We then define a pipeline of tasks and design appropriate evaluation metrics. Lastly, we achieve 61.3 F1 on ambiguity detection and 40.5 F1 on clarification-based QA, providing stron
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#26469;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30740;&#31350;&#20027;&#39064;&#30340;&#28436;&#21464;&#36235;&#21183;&#65292;&#25581;&#31034;&#20102;&#20219;&#21153;&#21644;&#26041;&#27861;&#26159;&#39537;&#21160;&#30740;&#31350;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#32780;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.12920</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#33539;&#24335;&#36716;&#21464;&#30340;&#21382;&#26102;&#20998;&#26512;&#65306;&#20309;&#26102;&#12289;&#22914;&#20309;&#21644;&#20026;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
A Diachronic Analysis of Paradigm Shifts in NLP Research: When, How, and Why?. (arXiv:2305.12920v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#26469;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30740;&#31350;&#20027;&#39064;&#30340;&#28436;&#21464;&#36235;&#21183;&#65292;&#25581;&#31034;&#20102;&#20219;&#21153;&#21644;&#26041;&#27861;&#26159;&#39537;&#21160;&#30740;&#31350;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#32780;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31185;&#23398;&#39046;&#22495;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#36235;&#21183;&#23545;&#20110;&#36319;&#19978;&#20854;&#25345;&#32493;&#21457;&#23637;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#21457;&#29616;&#21644;&#25512;&#29702;&#25216;&#26415;&#26469;&#20998;&#26512;&#31185;&#23398;&#39046;&#22495;&#20013;&#30740;&#31350;&#20027;&#39064;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19977;&#20010;&#21464;&#37327;&#65292;&#20197;&#28085;&#30422;NLP&#30740;&#31350;&#20027;&#39064;&#28436;&#21464;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#25581;&#31034;&#36825;&#20123;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#32467;&#26500;&#26469;&#27979;&#37327;&#36825;&#20123;&#20851;&#31995;&#30340;&#24378;&#24230;&#12290;&#36890;&#36807;&#22312;ACL Anthology&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#21457;&#29616;&#24191;&#27867;&#30340;NLP&#30740;&#31350;&#20027;&#39064;&#30340;&#28436;&#21464;&#36235;&#21183;&#21644;&#28508;&#22312;&#21407;&#22240;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#20219;&#21153;&#21644;&#26041;&#27861;&#26159;&#25512;&#21160;NLP&#30740;&#31350;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#32780;&#25968;&#25454;&#38598;&#32039;&#38543;&#20854;&#21518;&#65292;&#32780;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the fundamental concepts and trends in a scientific field is crucial for keeping abreast of its continuous advancement. In this study, we propose a systematic framework for analyzing the evolution of research topics in a scientific field using causal discovery and inference techniques. We define three variables to encompass diverse facets of the evolution of research topics within NLP and utilize a causal discovery algorithm to unveil the causal connections among these variables using observational data. Subsequently, we leverage this structure to measure the intensity of these relationships. By conducting extensive experiments on the ACL Anthology corpus, we demonstrate that our framework effectively uncovers evolutionary trends and the underlying causes for a wide range of NLP research topics. Specifically, we show that tasks and methods are primary drivers of research in NLP, with datasets following, while metrics have minimal impact.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bi-ACL&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#30446;&#26631;&#35821;&#21333;&#35821;&#25968;&#25454;&#21644;&#21452;&#35821;&#35789;&#20856;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#34920;&#31034;&#34928;&#20943;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#38271;&#23614;&#35821;&#31181;&#20013;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.12786</link><description>&lt;p&gt;
&#20943;&#36731;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#34920;&#31034;&#34928;&#20943;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Data Imbalance and Representation Degeneration in Multilingual Machine Translation. (arXiv:2305.12786v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bi-ACL&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#30446;&#26631;&#35821;&#21333;&#35821;&#25968;&#25454;&#21644;&#21452;&#35821;&#35789;&#20856;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#34920;&#31034;&#34928;&#20943;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#38271;&#23614;&#35821;&#31181;&#20013;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793; (MNMT) &#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#22312;&#36825;&#20010;&#39046;&#22495;&#20173;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#34920;&#31034;&#34928;&#20943;&#12290;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#25351;&#30340;&#26159;&#25152;&#26377;&#35821;&#35328;&#23545;&#30340;&#24179;&#34892;&#35821;&#26009;&#25968;&#37327;&#30340;&#19981;&#24179;&#34913;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38271;&#23614;&#35821;&#31181; (&#21363;&#36164;&#28304;&#26497;&#24230;&#21294;&#20047;&#30340;&#35821;&#31181;)&#12290;&#34920;&#31034;&#34928;&#20943;&#38382;&#39064;&#25351;&#30340;&#26159;&#32534;&#30721;&#26631;&#35760;&#36235;&#21521;&#20110;&#20165;&#22312; MNMT &#27169;&#22411;&#21487;&#29992;&#30340;&#20840;&#31354;&#38388;&#30340;&#19968;&#20010;&#23567;&#23376;&#31354;&#38388;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bi-ACL&#65292;&#19968;&#20010;&#21482;&#20351;&#29992;&#30446;&#26631;&#35821;&#21333;&#35821;&#25968;&#25454;&#21644;&#21452;&#35821;&#35789;&#20856;&#26469;&#25913;&#21892;MNMT&#27169;&#22411;&#24615;&#33021;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#20010;&#27169;&#22359;&#65292;&#21517;&#20026;&#21452;&#21521;&#33258;&#32534;&#30721;&#22120;&#21644;&#21452;&#21521;&#23545;&#27604;&#23398;&#20064;&#65292;&#23427;&#20204;&#19982;&#22312;&#32447;&#32422;&#26463;&#26463;&#25628;&#32034;&#21644;&#35838;&#31243;&#23398;&#20064;&#37319;&#26679;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38271;&#23614;&#35821;&#31181;&#20013;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in multilingual neural machine translation (MNMT), we argue that there are still two major challenges in this area: data imbalance and representation degeneration. The data imbalance problem refers to the imbalance in the amount of parallel corpora for all language pairs, especially for long-tail languages (i.e., very low-resource languages). The representation degeneration problem refers to the problem of encoded tokens tending to appear only in a small subspace of the full space available to the MNMT model. To solve these two issues, we propose Bi-ACL, a framework that uses only target-side monolingual data and a bilingual dictionary to improve the performance of the MNMT model. We define two modules, named bidirectional autoencoder and bidirectional contrastive learning, which we combine with an online constrained beam search and a curriculum learning sampling strategy. Extensive experiments show that our proposed method is more effective both in long-tail languages
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#26469;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#21464;&#24322;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11430</link><description>&lt;p&gt;
TELeR&#65306;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#22797;&#26434;&#20219;&#21153;&#30340;LLM&#25552;&#31034;&#30340;&#36890;&#29992;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks. (arXiv:2305.11430v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#26469;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#21464;&#24322;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLM&#22312;&#20256;&#32479;&#23545;&#35805;&#29615;&#22659;&#20013;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#26102;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#25191;&#34892;&#19981;&#26126;&#30830;&#30340;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#21463;&#21040;&#24456;&#23569;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#20197;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31867;&#22411;/&#39118;&#26684;&#21644;&#25552;&#31034;&#25552;&#20379;&#30340;&#19981;&#21516;&#35814;&#32454;&#31243;&#24230;&#26102;LLM&#24615;&#33021;&#21464;&#21270;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#23558;&#20351;&#26410;&#26469;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#33021;&#22815;&#25253;&#21578;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#29305;&#23450;&#25552;&#31034;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#19981;&#21516;&#30740;&#31350;&#30340;&#26377;&#24847;&#20041;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw mo
&lt;/p&gt;</description></item><item><title>DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10429</link><description>&lt;p&gt;
DoReMi: &#20248;&#21270;&#25968;&#25454;&#28151;&#21512;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10429
&lt;/p&gt;
&lt;p&gt;
DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#22495;&#30340;&#28151;&#21512;&#27604;&#20363;&#65288;&#20363;&#22914;&#65292;&#32500;&#22522;&#30334;&#31185;&#12289;&#22270;&#20070;&#12289;&#32593;&#39029;&#25991;&#26412;&#65289;&#26497;&#22823;&#22320;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DoReMi&#30340;Domain Reweighting with Minimax Optimization&#26041;&#27861;&#65292;&#23427;&#39318;&#20808;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65288;Group DRO&#65289;&#35757;&#32451;&#19968;&#20010;&#23567;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65288;&#28151;&#21512;&#27604;&#20363;&#65289;&#65292;&#32780;&#19981;&#38656;&#35201;&#30693;&#36947;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#22495;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#26356;&#22823;&#30340;&#65292;&#20840;&#23610;&#23544;&#30340;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;DoReMi&#22312;&#19968;&#20010;280M&#21442;&#25968;&#30340;&#20195;&#29702;&#27169;&#22411;&#19978;&#65292;&#26356;&#26377;&#25928;&#22320;&#25214;&#21040;&#35757;&#32451;&#19968;&#20010;8B&#21442;&#25968;&#27169;&#22411;&#65288;30&#20493;&#22823;&#65289;&#30340;&#22495;&#26435;&#37325;&#12290;&#22312;The Pile&#19978;&#65292;&#21363;&#20351;&#22312;&#20943;&#23567;&#19968;&#20123;&#22495;&#30340;&#27604;&#37325;&#26102;&#65292;DoReMi&#20063;&#33021;&#25552;&#39640;&#25152;&#26377;&#22495;&#30340;perplexity&#12290;&#30456;&#27604;&#20351;&#29992;The Pile&#30340;&#40664;&#35748;&#22495;&#26435;&#37325;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;DoReMi&#23558;&#24179;&#22343;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;6.5%&#65292;&#24182;&#20351;&#29992;2.6&#20493;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;&#22312;GLaM&#25968;&#25454;&#38598;&#19978;&#65292;DoReMi&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;4.7%&#65288;&#27425;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65289;&#30340;few-shot&#20934;&#30830;&#24230;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#19979;&#25552;&#39640;&#20102;9.0%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no know
&lt;/p&gt;</description></item><item><title>FACE&#26159;&#19968;&#32452;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20154;&#31867;&#21644;&#27169;&#22411;&#20043;&#38388;&#24046;&#36317;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#23427;&#22522;&#20110;&#20613;&#37324;&#21494;&#20998;&#26512;&#21644;&#20132;&#21449;&#29109;&#20272;&#35745;&#65292;&#21487;&#20197;&#21453;&#26144;&#27169;&#22411;&#22823;&#23567;&#12289;&#35299;&#30721;&#37319;&#26679;&#26041;&#27861;&#21644;&#20154;&#31867;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.10307</link><description>&lt;p&gt;
FACE: &#20351;&#29992;&#20132;&#21449;&#29109;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy. (arXiv:2305.10307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10307
&lt;/p&gt;
&lt;p&gt;
FACE&#26159;&#19968;&#32452;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20154;&#31867;&#21644;&#27169;&#22411;&#20043;&#38388;&#24046;&#36317;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#23427;&#22522;&#20110;&#20613;&#37324;&#21494;&#20998;&#26512;&#21644;&#20132;&#21449;&#29109;&#20272;&#35745;&#65292;&#21487;&#20197;&#21453;&#26144;&#27169;&#22411;&#22823;&#23567;&#12289;&#35299;&#30721;&#37319;&#26679;&#26041;&#27861;&#21644;&#20154;&#31867;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#30340;&#35821;&#35328;&#19982;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#36317;&#31163;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#21463;&#21040;&#35821;&#35328;&#23398;&#24515;&#29702;&#23398;&#20851;&#20110;&#35821;&#35328;&#29109;&#21608;&#26399;&#24615;&#23454;&#35777;&#21457;&#29616;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FACE&#8212;&#8212;&#19968;&#32452;&#22522;&#20110;&#35821;&#35328;&#20132;&#21449;&#29109;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#35821;&#35328;&#19982;&#20154;&#31867;&#20070;&#20889;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#24320;&#25918;&#24335;&#30340;&#29983;&#25104;&#20219;&#21153;&#21644;&#20197;&#21069;&#30740;&#31350;&#30340;&#23454;&#39564;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;FACE&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20154;&#31867;&#27169;&#22411;&#24046;&#36317;&#65292;&#22312;&#27169;&#22411;&#35268;&#27169;&#19978;&#26377;&#25152;&#32553;&#25918;&#65292;&#21453;&#26144;&#20102;&#19981;&#21516;&#35299;&#30721;&#37319;&#26679;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#19982;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#21028;&#26029;&#20998;&#25968;&#30456;&#20851;&#33391;&#22909;&#12290;FACE&#22312;&#35745;&#31639;&#19978;&#26159;&#39640;&#25928;&#30340;&#65292;&#24182;&#25552;&#20379;&#30452;&#35266;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring the distance between machine-produced and human language is acritical open problem. Inspired by empirical findings from psycholinguistics on theperiodicity of entropy in language, we propose FACE, a set of metrics based onFourier Analysis of the estimated Cross-Entropy of language, for measuring thesimilarity between model-generated and human-written languages. Based on anopen-ended generation task and the experimental data from previous studies, weind that FACE can effectively identify the human-model gap, scales with modelsize, reflects the outcomes of different sampling methods for decoding, correlateswell with other evaluation metrics and with human judgment scores. FACE iscomputationally efficient and provides intuitive interpretations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;StrAE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21477;&#23376;&#32467;&#26500;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#22810;&#32423;&#33410;&#28857;&#23884;&#20837;&#65292;&#24182;&#19988;&#21457;&#29616;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#23884;&#20837;&#34920;&#29616;&#65292;&#26032;&#30340;&#23545;&#27604;&#30446;&#26631;&#20248;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#23436;&#20840;&#24544;&#23454;&#20110;&#32467;&#26500;&#30830;&#23454;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#28040;&#38500;&#32467;&#26500;&#31867;&#22411;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.05588</link><description>&lt;p&gt;
StrAE&#65306;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#30340;&#33258;&#32534;&#30721;&#39044;&#35757;&#32451;&#23884;&#20837;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure. (arXiv:2305.05588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;StrAE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21477;&#23376;&#32467;&#26500;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#22810;&#32423;&#33410;&#28857;&#23884;&#20837;&#65292;&#24182;&#19988;&#21457;&#29616;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#23884;&#20837;&#34920;&#29616;&#65292;&#26032;&#30340;&#23545;&#27604;&#30446;&#26631;&#20248;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#23436;&#20840;&#24544;&#23454;&#20110;&#32467;&#26500;&#30830;&#23454;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#28040;&#38500;&#32467;&#26500;&#31867;&#22411;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;StrAE&#36825;&#19968;&#33258;&#32534;&#30721;&#26694;&#26550;&#65292;&#25506;&#31350;&#20102;&#22312;NLP&#20013;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#23454;&#29992;&#24615;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21477;&#23376;&#32467;&#26500;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#22810;&#32423;&#33410;&#28857;&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#22312;&#20869;&#30340;&#19981;&#21516;&#31867;&#22411;&#21477;&#23376;&#32467;&#26500;&#21644;&#30446;&#26631;&#19979;&#20351;&#29992;StrAE&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#20869;&#22312;&#21644;&#22806;&#22312;&#20219;&#21153;&#19978;&#35780;&#20272;&#25152;&#23398;&#23884;&#20837;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;StrAE&#21033;&#29992;&#26174;&#24335;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#23884;&#20837;&#65292;&#26032;&#30340;&#23545;&#27604;&#30446;&#26631;&#20248;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#19982;&#20197;&#24448;&#30340;&#20570;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#23436;&#20840;&#24544;&#23454;&#20110;&#32467;&#26500;&#30830;&#23454;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#28040;&#38500;&#32467;&#26500;&#31867;&#22411;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;&#20316;&#20026;StrAE&#23454;&#29992;&#24615;&#30340;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;p
&lt;/p&gt;
&lt;p&gt;
This work explores the utility of explicit structure for representation learning in NLP by developing StrAE -- an autoencoding framework that faithfully leverages sentence structure to learn multi-level node embeddings in an unsupervised fashion. We use StrAE to train models across different types of sentential structure and objectives, including a novel contrastive loss over structure, and evaluate the learnt embeddings on a series of both intrinsic and extrinsic tasks. Our experiments indicate that leveraging explicit structure through StrAE leads to improved embeddings over prior work, and that our novel contrastive objective over structure outperforms the standard cross-entropy objective. Moreover, in contrast to findings from prior work that weakly leverages structure, we find that being completely faithful to structure does enable disambiguation between types of structure based on the corresponding model's performance. As further evidence of StrAE's utility, we develop a simple p
&lt;/p&gt;</description></item><item><title>API-Bank&#26159;&#19968;&#20010;&#38024;&#23545;&#24037;&#20855;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#35299;&#20915;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#26469;&#35780;&#20272;LLMs&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;GPT-3.5&#30340;&#25913;&#36827;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.08244</link><description>&lt;p&gt;
API-Bank: &#19968;&#31181;&#38024;&#23545;&#24037;&#20855;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs. (arXiv:2304.08244v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08244
&lt;/p&gt;
&lt;p&gt;
API-Bank&#26159;&#19968;&#20010;&#38024;&#23545;&#24037;&#20855;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#35299;&#20915;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#26469;&#35780;&#20272;LLMs&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;GPT-3.5&#30340;&#25913;&#36827;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#26469;&#22686;&#24378;&#20854;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#23578;&#26410;&#35299;&#31572;&#65306;&#65288;1&#65289;&#30446;&#21069;&#30340;LLMs&#22312;&#21033;&#29992;&#24037;&#20855;&#26041;&#38754;&#30340;&#25928;&#26524;&#22914;&#20309;&#65311;&#65288;2&#65289;&#22914;&#20309;&#22686;&#24378;LLMs&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65311;&#65288;3&#65289;&#22914;&#20309;&#20811;&#26381;&#21033;&#29992;&#24037;&#20855;&#25152;&#38754;&#20020;&#30340;&#38556;&#30861;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;API-Bank&#65292;&#19968;&#20010;&#20855;&#26377;&#31361;&#30772;&#24615;&#24847;&#20041;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#19987;&#38376;&#20026;&#24037;&#20855;&#22686;&#24378;&#30340;LLMs&#35774;&#35745;&#12290;&#38024;&#23545;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#36816;&#34892;&#30340;&#35780;&#20272;&#31995;&#32479;&#65292;&#21253;&#21547;73&#20010;API&#24037;&#20855;&#12290;&#25105;&#20204;&#20351;&#29992;753&#20010;API&#35843;&#29992;&#27880;&#37322;&#20102;314&#20010;&#24037;&#20855;&#20351;&#29992;&#23545;&#35805;&#65292;&#20197;&#35780;&#20272;&#29616;&#26377;LLMs&#22312;&#35268;&#21010;&#12289;&#26816;&#32034;&#21644;&#35843;&#29992;API&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#38024;&#23545;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;1,000&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;2,138&#20010;API&#30340;1,888&#20010;&#24037;&#20855;&#20351;&#29992;&#23545;&#35805;&#30340;&#20840;&#38754;&#35757;&#32451;&#38598;&#12290;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;Lynx&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;Alpaca&#21021;&#22987;&#21270;&#30340;&#24037;&#20855;&#22686;&#24378;LLM&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#26174;&#31034;&#20986;&#20102;&#25913;&#36827;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs' capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;D5&#65292;&#36890;&#36807;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#24335;&#33258;&#21160;&#21457;&#29616;&#20004;&#20010;&#22823;&#22411;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;D5&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#35821;&#26009;&#24211;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2302.14233</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#30446;&#26631;&#30340;&#35821;&#35328;&#25551;&#36848;&#21457;&#29616;&#20998;&#24067;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Goal Driven Discovery of Distributional Differences via Language Descriptions. (arXiv:2302.14233v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;D5&#65292;&#36890;&#36807;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#24335;&#33258;&#21160;&#21457;&#29616;&#20004;&#20010;&#22823;&#22411;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;D5&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#35821;&#26009;&#24211;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25366;&#25496;&#22823;&#22411;&#35821;&#26009;&#24211;&#21487;&#20197;&#20135;&#29983;&#26377;&#29992;&#30340;&#21457;&#29616;&#65292;&#20294;&#23545;&#20154;&#31867;&#26469;&#35828;&#32791;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;D5&#65292;&#23427;&#20197;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#24335;&#33258;&#21160;&#21457;&#29616;&#20004;&#20010;&#22823;&#22411;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20219;&#21153;&#36755;&#20837;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#19968;&#20010;&#30740;&#31350;&#30446;&#26631;&#8220;&#27604;&#36739;&#33647;&#29289;A&#21644;&#33647;&#29289;B&#30340;&#21103;&#20316;&#29992;&#8221;&#65292;&#20197;&#21450;&#19968;&#20010;&#35821;&#26009;&#24211;&#23545;&#65288;&#20004;&#20010;&#22823;&#22411;&#24739;&#32773;&#33258;&#25253;&#21453;&#24212;&#30340;&#38598;&#21512;&#65289;&#12290;&#36755;&#20986;&#26159;&#23545;&#36825;&#20123;&#35821;&#26009;&#24211;&#24046;&#24322;&#30340;&#35821;&#35328;&#25551;&#36848;&#65288;&#21457;&#29616;&#65289;&#65288;&#20351;&#29992;&#33647;&#29289;A&#21518;&#65292;&#24739;&#32773;&#26356;&#32463;&#24120;&#25552;&#21040;&#8220;&#20559;&#25191;&#24863;&#8221;&#65289;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;D5&#31995;&#32479;&#65292;&#24182;&#20026;&#20102;&#23450;&#37327;&#34913;&#37327;&#20854;&#24615;&#33021;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#20803;&#25968;&#25454;&#38598;OpenD5&#65292;&#32858;&#21512;&#20102;675&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#21830;&#19994;&#12289;&#31038;&#20250;&#31185;&#23398;&#12289;&#20154;&#25991;&#23398;&#31185;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#20581;&#24247;&#31561;&#39046;&#22495;&#65292;&#21516;&#26102;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;&#26377;&#25928;&#24615;&#12289;&#30456;&#20851;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#26174;&#33879;&#24615;&#12290;&#36890;&#36807;&#25968;&#25454;&#38598;&#21644;&#32479;&#19968;&#25351;&#26631;&#65292;&#25105;&#20204;&#30830;&#35748;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#35821;&#26009;&#24211;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining large corpora can generate useful discoveries but is time-consuming for humans. We formulate a new task, D5, that automatically discovers differences between two large corpora in a goal-driven way. The task input is a problem comprising a research goal "$\textit{comparing the side effects of drug A and drug B}$" and a corpus pair (two large collections of patients' self-reported reactions after taking each drug). The output is a language description (discovery) of how these corpora differ (patients taking drug A "$\textit{mention feelings of paranoia}$" more often). We build a D5 system, and to quantitatively measure its performance, we 1) contribute a meta-dataset, OpenD5, aggregating 675 open-ended problems ranging across business, social sciences, humanities, machine learning, and health, and 2) propose a set of unified evaluation metrics: validity, relevance, novelty, and significance. With the dataset and the unified metrics, we confirm that language models can use the goal
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#33976;&#39311;&#21644;&#26631;&#31614;&#24179;&#28369;&#34987;&#35748;&#20026;&#26159;&#31561;&#20215;&#30340;&#26041;&#27861;&#65292;&#20294;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#23545;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#24433;&#21709;&#26041;&#21521;&#23436;&#20840;&#30456;&#21453;&#12290;&#30693;&#35782;&#33976;&#39311;&#19981;&#20165;&#20256;&#36882;&#30693;&#35782;&#65292;&#36824;&#20256;&#36882;&#20102;&#33258;&#20449;&#24515;&#12290;</title><link>http://arxiv.org/abs/2301.12609</link><description>&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#8776;&#26631;&#31614;&#24179;&#28369;&#65306;&#20107;&#23454;&#36824;&#26159;&#35884;&#35823;&#65311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation $\approx$ Label Smoothing: Fact or Fallacy?. (arXiv:2301.12609v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12609
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#21644;&#26631;&#31614;&#24179;&#28369;&#34987;&#35748;&#20026;&#26159;&#31561;&#20215;&#30340;&#26041;&#27861;&#65292;&#20294;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#23545;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#24433;&#21709;&#26041;&#21521;&#23436;&#20840;&#30456;&#21453;&#12290;&#30693;&#35782;&#33976;&#39311;&#19981;&#20165;&#20256;&#36882;&#30693;&#35782;&#65292;&#36824;&#20256;&#36882;&#20102;&#33258;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#21021;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20174;&#19968;&#20010;&#27169;&#22411;&#21521;&#21478;&#19968;&#20010;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#26041;&#27861;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#30693;&#35782;&#33976;&#39311;(KD)&#23454;&#38469;&#19978;&#26159;&#19968;&#31181;&#27491;&#21017;&#21270;&#30340;&#24418;&#24335;&#12290;&#26368;&#24378;&#26377;&#21147;&#30340;&#25903;&#25345;&#26469;&#33258;&#20110;&#23427;&#19982;&#26631;&#31614;&#24179;&#28369;(LS)&#26041;&#27861;&#30340;&#26126;&#26174;&#30456;&#20284;&#20043;&#22788;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#25152;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#65292;&#37325;&#26032;&#32771;&#23519;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#12290;&#22312;&#28041;&#21450;&#19981;&#21516;&#35268;&#27169;&#27169;&#22411;&#30340;&#22235;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#26174;&#31034;&#20986;&#65306;(a)&#22312;&#22823;&#22810;&#25968;&#35774;&#32622;&#20013;&#65292;KD&#21644;LS&#20250;&#23436;&#20840;&#30456;&#21453;&#22320;&#24433;&#21709;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#12290;(b) &#22312;KD&#20013;&#65292;&#23398;&#29983;&#19981;&#20165;&#32487;&#25215;&#30693;&#35782;&#65292;&#32780;&#19988;&#36824;&#20174;&#32769;&#24072;&#37027;&#37324;&#32487;&#25215;&#33258;&#20449;&#24515;&#65292;&#21152;&#24378;&#20102;&#20256;&#32479;&#30340;&#30693;&#35782;&#20256;&#36882;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originally proposed as a method for knowledge transfer from one model to another, some recent studies have suggested that knowledge distillation (KD) is in fact a form of regularization. Perhaps the strongest support of all for this new perspective comes from its apparent similarities with label smoothing (LS). Here we re-examine this stated equivalence between the two methods by comparing the predictive confidences of the models they train. Experiments on four text classification tasks involving models of different sizes show that: (a) In most settings, KD and LS drive model confidence in completely opposite directions, and (b) In KD, the student inherits not only its knowledge but also its confidence from the teacher, reinforcing the classical knowledge transfer view.
&lt;/p&gt;</description></item><item><title>JASMINE&#26159;&#19968;&#32452;&#21151;&#33021;&#24378;&#22823;&#30340;&#38463;&#25289;&#20271;&#33258;&#22238;&#24402;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20110;&#22823;&#35268;&#27169;&#22810;&#26679;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.10755</link><description>&lt;p&gt;
JASMINE&#65306;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#38463;&#25289;&#20271;GPT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
JASMINE: Arabic GPT Models for Few-Shot Learning. (arXiv:2212.10755v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10755
&lt;/p&gt;
&lt;p&gt;
JASMINE&#26159;&#19968;&#32452;&#21151;&#33021;&#24378;&#22823;&#30340;&#38463;&#25289;&#20271;&#33258;&#22238;&#24402;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20110;&#22823;&#35268;&#27169;&#22810;&#26679;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#39044;&#35757;&#32451;&#65288;GPT&#65289;&#30340;&#23398;&#26415;&#30740;&#31350;&#20173;&#28982;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#65292;&#23548;&#33268;&#25105;&#20204;&#23545;&#33258;&#22238;&#24402;&#27169;&#22411;&#25972;&#20010;&#31867;&#21035;&#30340;&#29702;&#35299;&#23384;&#22312;&#20005;&#37325;&#30340;&#31354;&#30333;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#25991;&#21270;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#21450;&#20854;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;JASMINE&#65292;&#23427;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#36825;&#20010;&#25317;&#26377;4&#20159;&#22810;&#20154;&#21475;&#12289;&#21253;&#21547;&#24191;&#27867;&#35821;&#35328;&#21644;&#26041;&#35328;&#21464;&#20307;&#30340;&#35821;&#35328;&#38598;&#21512;&#32780;&#35774;&#35745;&#12290;JASMINE&#26159;&#19968;&#32452;&#21151;&#33021;&#24378;&#22823;&#30340;&#38463;&#25289;&#20271;&#33258;&#22238;&#24402;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21442;&#25968;&#33539;&#22260;&#20026;3&#20159;-67&#20159;&#65292;&#39044;&#35757;&#32451;&#20110;&#19968;&#20010;&#22823;&#32780;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#65288;&#32422;235GB&#30340;&#25991;&#26412;&#65289;&#12290;&#25105;&#20204;&#36824;&#31934;&#24515;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#38463;&#25289;&#20271;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#28508;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#12289;&#21361;&#23475;&#21644;&#26377;&#23475;&#24615;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#22522;&#20934;&#65292;&#25105;&#20204;&#23545;JASMINE&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#20854;&#33258;&#36523;&#24378;&#22823;&#30340;&#24615;&#33021;&#20197;&#21450;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scholarship on generative pretraining (GPT) remains acutely Anglocentric, leaving serious gaps in our understanding of the whole class of autoregressive models. For example, we have little knowledge about the potential of these models and their societal impacts in diverse linguistic and cultural settings. We alleviate this issue for Arabic, a wide collection of languages and dialectal varieties with more than 400 million population, by introducing JASMINE. JASMINE is a suite of powerful Arabic autoregressive Transformer language models ranging in size between 300 million-6.7 billion parameters pretrained on a large and diverse dataset (~ 235 GB of text). We also carefully design and release a comprehensive benchmark for both automated and human evaluation of Arabic autoregressive models, with coverage of potential social biases, harms, and toxicity. Using our novel benchmark, we evaluate JASMINE extensively showing powerful performance intrinsically as well as in few-shot learning on a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;"&#24320;&#25918;&#39046;&#22495;"&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#22120;&#22312;&#27492;&#24773;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#20005;&#37325;&#65292;&#20294;&#36827;&#34892;&#39069;&#22806;&#30340;&#24320;&#25918;&#39046;&#22495;&#30340;&#35757;&#32451;&#21487;&#20197;&#38477;&#20302;&#23545;&#19981;&#23436;&#32654;&#26816;&#32034;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10526</link><description>&lt;p&gt;
&#38754;&#21521;&#24320;&#25918;&#39046;&#22495;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Towards multi-document summarization in the open-domain. (arXiv:2212.10526v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;"&#24320;&#25918;&#39046;&#22495;"&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#22120;&#22312;&#27492;&#24773;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#20005;&#37325;&#65292;&#20294;&#36827;&#34892;&#39069;&#22806;&#30340;&#24320;&#25918;&#39046;&#22495;&#30340;&#35757;&#32451;&#21487;&#20197;&#38477;&#20302;&#23545;&#19981;&#23436;&#32654;&#26816;&#32034;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#36890;&#24120;&#20551;&#35774;&#25552;&#20379;&#19968;&#32452;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#20294;&#26159;&#65292;&#36825;&#20010;&#25991;&#26723;&#38598;&#36890;&#24120;&#26159;&#25968;&#25454;&#38598;&#31574;&#21010;&#36807;&#31243;&#30340;&#20135;&#29289;&#65307;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#19981;&#19968;&#23450;&#21487;&#29992;&#65292;&#38656;&#35201;&#26681;&#25454;&#20449;&#24687;&#38656;&#27714;&#65292;&#21363;&#38382;&#39064;&#25110;&#20027;&#39064;&#38472;&#36848;&#36827;&#34892;&#26816;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#24418;&#24335;&#21270;&#20219;&#21153;&#24182;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#26816;&#32034;&#22120;&#21644;&#25688;&#35201;&#22120;&#26469;&#24341;&#23548;&#36825;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#8220;&#24320;&#25918;&#39046;&#22495;&#8221;&#35774;&#32622;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#65306;(1)&#21363;&#20351;&#26816;&#32034;&#24615;&#33021;&#36739;&#39640;&#65292;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#22120;&#22312;&#24212;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#26102;&#20063;&#20250;&#22823;&#24133;&#38477;&#20302;&#24615;&#33021;;(2)&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#21487;&#20197;&#38477;&#20302;&#23545;&#19981;&#23436;&#32654;&#26816;&#32034;&#30340;&#25935;&#24863;&#24615;&#65292;(3)&#25688;&#35201;&#22120;&#23545;&#26816;&#32034;&#37325;&#22797;&#25991;&#26723;&#21644;&#26816;&#32034;&#25991;&#26723;&#30340;&#39034;&#24207;&#19981;&#25935;&#24863;&#65292;&#20294;&#23545;&#20854;&#20182;&#38169;&#35823;&#65292;&#22914;&#26816;&#32034;&#26080;&#20851;&#25991;&#26723;&#30340;&#25935;&#24863;&#24615;&#36739;&#39640;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Multi-document summarization (MDS) traditionally assumes a set of topic-related documents are provided. However, this document set is often an artifact of the dataset curation process; in practice, it is not necessarily available and would need to be retrieved given an information need, i.e. a question or topic statement. We study this more challenging "open-domain" setting by formalizing the task and bootstrapping it using existing datasets, retrievers and summarizers. Via extensive experimentation, we determine that: (1) state-of-the-art summarizers suffer large reductions in performance when applied to the open-domain, even when retrieval performance is high, (2) additional training in the open-domain setting can reduce this sensitivity to imperfect retrieval, and (3) summarizers are insensitive to the retrieval of duplicate documents and the order of retrieved documents, but highly sensitive to other errors, like the retrieval of irrelevant documents. Based on our results, we provi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21457;&#29616;&#20102;&#22312;&#35757;&#32451;&#29983;&#25104;&#24335;&#27169;&#22411;&#26102;&#24120;&#34987;&#24573;&#35270;&#30340;&#26631;&#35760;&#19968;&#33268;&#24615;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#19968;&#33268;&#30340;&#26631;&#35760;&#21270;&#65292;&#27169;&#22411;&#22312;&#25277;&#21462;&#22411;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343; F2 &#22686;&#30410;&#20026; +1.7&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#19981;&#30456;&#20851;&#36755;&#20986;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2212.09912</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#27169;&#22411;&#22312;&#25277;&#21462;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26631;&#35760;&#19968;&#33268;&#24615;&#38382;&#39064;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks. (arXiv:2212.09912v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21457;&#29616;&#20102;&#22312;&#35757;&#32451;&#29983;&#25104;&#24335;&#27169;&#22411;&#26102;&#24120;&#34987;&#24573;&#35270;&#30340;&#26631;&#35760;&#19968;&#33268;&#24615;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#19968;&#33268;&#30340;&#26631;&#35760;&#21270;&#65292;&#27169;&#22411;&#22312;&#25277;&#21462;&#22411;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343; F2 &#22686;&#30410;&#20026; +1.7&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#19981;&#30456;&#20851;&#36755;&#20986;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#27169;&#22411;&#22312;&#35299;&#20915;&#25277;&#21462;&#22411;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#22312;&#25277;&#21462;&#24335;&#38382;&#31572;&#65288;QA&#65289;&#20013;&#65292;&#29983;&#25104;&#24335;&#27169;&#22411;&#19968;&#30452;&#20445;&#25345;&#30528;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#26102;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#26631;&#35760;&#21270;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#26631;&#35760;&#22120;&#23545;&#36755;&#20837;&#21644;&#36755;&#20986;&#36827;&#34892;&#19981;&#19968;&#33268;&#30340;&#26631;&#35760;&#21270;&#21518;&#20250;&#30772;&#22351;&#36825;&#20123;&#20219;&#21153;&#30340;&#25277;&#21462;&#24615;&#36136;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#21644;&#34394;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#25277;&#21462;&#24335;&#38382;&#31572;&#26041;&#38754;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#19968;&#33268;&#30340;&#26631;&#35760;&#21270;&#65292;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#22312;&#23558; BART &#27169;&#22411;&#35757;&#32451;&#20110; SQuAD &#24182;&#22312; 8 &#20010; QA &#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#26102;&#65292;&#24179;&#22343; F2 &#22686;&#30410;&#26174;&#33879;&#65292;&#20026; +1.7&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#65292;&#24182;&#19988;&#26356;&#19981;&#23481;&#26131;&#29983;&#25104;&#19981;&#30456;&#20851;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have been widely applied to solve extractive tasks, where parts of the input is extracted to form the desired output, and achieved significant success. For example, in extractive question answering (QA), generative models have constantly yielded state-of-the-art results. In this work, we identify the issue of tokenization inconsistency that is commonly neglected in training these models. This issue damages the extractive nature of these tasks after the input and output are tokenized inconsistently by the tokenizer, and thus leads to performance drop as well as hallucination. We propose a simple yet effective fix to this issue and conduct a case study on extractive QA. We show that, with consistent tokenization, the model performs better in both in-domain and out-of-domain datasets, with a notable average of +1.7 F2 gain when a BART model is trained on SQuAD and evaluated on 8 QA datasets. Further, the model converges faster, and becomes less likely to generate out-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27861;&#24459;&#21512;&#21516;&#30340;&#21508;&#26041;&#29305;&#23450;&#25688;&#35201;&#25552;&#21462;&#20219;&#21153;&#65292;&#26088;&#22312;&#24110;&#21161;&#26356;&#24555;&#22320;&#23457;&#26597;&#21644;&#29702;&#35299;&#37325;&#35201;&#30340;&#26435;&#21033;&#21644;&#20041;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#27861;&#24459;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#25688;&#35201;&#31995;&#32479;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25688;&#35201;&#20013;&#24341;&#20837;&#39046;&#22495;&#29305;&#23450;&#37325;&#35201;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.09825</link><description>&lt;p&gt;
&#12298;&#21512;&#21516;&#20013;&#35201;&#35835;&#20123;&#20160;&#20040;&#65311;&#23545;&#27861;&#24459;&#20041;&#21153;&#12289;&#26435;&#30410;&#21644;&#31105;&#27490;&#30340;&#21508;&#26041;&#29305;&#23450;&#25688;&#35201;&#12299;
&lt;/p&gt;
&lt;p&gt;
What to Read in a Contract? Party-Specific Summarization of Legal Obligations, Entitlements, and Prohibitions. (arXiv:2212.09825v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27861;&#24459;&#21512;&#21516;&#30340;&#21508;&#26041;&#29305;&#23450;&#25688;&#35201;&#25552;&#21462;&#20219;&#21153;&#65292;&#26088;&#22312;&#24110;&#21161;&#26356;&#24555;&#22320;&#23457;&#26597;&#21644;&#29702;&#35299;&#37325;&#35201;&#30340;&#26435;&#21033;&#21644;&#20041;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#27861;&#24459;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#25688;&#35201;&#31995;&#32479;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25688;&#35201;&#20013;&#24341;&#20837;&#39046;&#22495;&#29305;&#23450;&#37325;&#35201;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21512;&#21516;&#38271;&#24230;&#21644;&#39046;&#22495;&#29305;&#23450;&#24615;&#65292;&#23457;&#26597;&#21644;&#29702;&#35299;&#27861;&#24459;&#21512;&#21516;&#20013;&#30340;&#37325;&#35201;&#20041;&#21153;&#12289;&#26435;&#30410;&#21644;&#31105;&#27490;&#21487;&#33021;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#38656;&#35201;&#23457;&#26597;&#30340;&#20027;&#35201;&#26435;&#21033;&#21644;&#20041;&#21153;&#22312;&#27599;&#20010;&#21512;&#32422;&#26041;&#20043;&#38388;&#20063;&#23384;&#22312;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#20219;&#21153;&#65292;&#21363;&#29992;&#20110;&#27861;&#24459;&#21512;&#21516;&#30340;&#21508;&#26041;&#29305;&#23450;&#25688;&#35201;&#25552;&#21462;&#65292;&#20197;&#20419;&#36827;&#26356;&#24555;&#30340;&#23457;&#26597;&#21644;&#25552;&#39640;&#23545;&#26435;&#21033;&#21644;&#20041;&#21153;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#27861;&#24459;&#19987;&#23478;&#27880;&#37322;&#30340;&#21508;&#26041;&#29305;&#23450;&#37325;&#35201;&#24615;&#27604;&#36739;&#65292;&#28085;&#30422;&#20102;&#20174;&#31199;&#36161;&#21327;&#35758;&#20013;&#25552;&#21462;&#30340;&#22823;&#32422;293K&#20010;&#21477;&#23376;&#23545;&#65292;&#21253;&#25324;&#20041;&#21153;&#12289;&#26435;&#30410;&#21644;&#31105;&#27490;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#37325;&#35201;&#24615;&#25490;&#24207;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#25552;&#21462;&#24615;&#25688;&#35201;&#31995;&#32479;&#65292;&#29983;&#25104;&#20102;&#21508;&#26041;&#29305;&#23450;&#30340;&#21512;&#21516;&#25688;&#35201;&#12290;&#36890;&#36807;&#19982;&#21508;&#31181;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25688;&#35201;&#20013;&#24341;&#20837;&#39046;&#22495;&#29305;&#23450;&#37325;&#35201;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reviewing and comprehending key obligations, entitlements, and prohibitions in legal contracts can be a tedious task due to their length and domain-specificity. Furthermore, the key rights and duties requiring review vary for each contracting party. In this work, we propose a new task of party-specific extractive summarization for legal contracts to facilitate faster reviewing and improved comprehension of rights and duties. To facilitate this, we curate a dataset comprising of party-specific pairwise importance comparisons annotated by legal experts, covering ~293K sentence pairs that include obligations, entitlements, and prohibitions extracted from lease agreements. Using this dataset, we train a pairwise importance ranker and propose a pipeline-based extractive summarization system that generates a party-specific contract summary. We establish the need for incorporating domain-specific notion of importance during summarization by comparing our system against various baselines using
&lt;/p&gt;</description></item><item><title>GLM-130B&#26159;&#19968;&#20010;&#20855;&#26377;1300&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#21452;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#36229;&#36234;GPT-3&#21644;&#26368;&#22823;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;ERNIE TITAN 3.0 260B&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.02414</link><description>&lt;p&gt;
GLM-130B: &#19968;&#20010;&#24320;&#28304;&#30340;&#21452;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GLM-130B: An Open Bilingual Pre-trained Model. (arXiv:2210.02414v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02414
&lt;/p&gt;
&lt;p&gt;
GLM-130B&#26159;&#19968;&#20010;&#20855;&#26377;1300&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#21452;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#36229;&#36234;GPT-3&#21644;&#26368;&#22823;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;ERNIE TITAN 3.0 260B&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GLM-130B&#65292;&#19968;&#20010;&#20855;&#26377;1300&#20159;&#21442;&#25968;&#30340;&#21452;&#35821;&#65288;&#33521;&#35821;&#21644;&#20013;&#25991;&#65289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#26159;&#20026;&#20102;&#25171;&#24320;1000&#20159;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#33267;&#23569;&#19982;GPT-3&#65288;davinci&#65289;&#19968;&#26679;&#22909;&#65292;&#24182;&#25581;&#31034;&#22914;&#20309;&#25104;&#21151;&#22320;&#36827;&#34892;&#22914;&#27492;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#35768;&#22810;&#24847;&#22806;&#30340;&#25216;&#26415;&#21644;&#24037;&#31243;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#25439;&#22833;&#23792;&#21644;&#21457;&#25955;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GLM-130B&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#21253;&#25324;&#35774;&#35745;&#36873;&#25321;&#12289;&#25552;&#39640;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21450;&#24037;&#31243;&#21162;&#21147;&#12290;GLM-130B&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#33521;&#35821;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;GPT-3 175B&#65288;davinci&#65289;&#65292;&#20294;&#22312;OPT-175B&#21644;BLOOM-176B&#20013;&#27809;&#26377;&#35266;&#23519;&#21040;&#24615;&#33021;&#20248;&#21183;&#12290;&#23427;&#36824;&#22312;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#19988;&#26174;&#33879;&#20248;&#20110;&#26368;&#22823;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;ERNIE TITAN 3.0 260B&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;GLM-130B&#30340;&#29420;&#29305;&#32553;&#25918;&#24615;&#33021;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to rea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#29305;&#24449;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2110.03427</link><description>&lt;p&gt;
&#26159;&#21542;&#24635;&#26159;&#38656;&#35201;&#27880;&#24847;&#21147;&#65311;&#35821;&#35328;&#35782;&#21035;&#26696;&#20363;&#30740;&#31350;&#12290;(arXiv:2110.03427v3 [cs.LG] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Is Attention always needed? A Case Study on Language Identification from Speech. (arXiv:2110.03427v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#29305;&#24449;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35782;&#21035;&#65288;LID&#65289;&#26159;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#39044;&#22788;&#29702;&#36807;&#31243;&#65292;&#28041;&#21450;&#20174;&#38899;&#39057;&#26679;&#26412;&#20013;&#35782;&#21035;&#20986;&#35762;&#35805;&#35821;&#35328;&#12290;&#24403;&#21069;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#30340;&#31995;&#32479;&#35201;&#27714;&#29992;&#25143;&#22312;&#20351;&#29992;&#20043;&#21069;&#26126;&#30830;&#25351;&#23450;&#19968;&#31181;&#25110;&#22810;&#31181;&#35821;&#35328;&#12290;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;&#24403;ASR&#31995;&#32479;&#26080;&#27861;&#29702;&#35299;&#35762;&#35805;&#35821;&#35328;&#26102;&#65292;LID&#20219;&#21153;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#23548;&#33268;&#35821;&#38899;&#35782;&#21035;&#32467;&#26524;&#22833;&#36133;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;CRNN&#65289;&#30340;LID&#26041;&#27861;&#65292;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#38899;&#39057;&#26679;&#26412;&#30340;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65288;MFCC&#65289;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22797;&#21046;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;&#24102;&#26377;&#27880;&#24847;&#21147;&#30340;CRNN&#65289;&#65292;&#24182;&#19982;&#25105;&#20204;&#30340;&#22522;&#20110;CRNN&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Language Identification (LID) is a crucial preliminary process in the field of Automatic Speech Recognition (ASR) that involves the identification of a spoken language from audio samples. Contemporary systems that can process speech in multiple languages require users to expressly designate one or more languages prior to utilization. The LID task assumes a significant role in scenarios where ASR systems are unable to comprehend the spoken language in multilingual settings, leading to unsuccessful speech recognition outcomes. The present study introduces convolutional recurrent neural network (CRNN) based LID, designed to operate on the Mel-frequency Cepstral Coefficient (MFCC) characteristics of audio samples. Furthermore, we replicate certain state-of-the-art methodologies, specifically the Convolutional Neural Network (CNN) and Attention-based Convolutional Recurrent Neural Network (CRNN with attention), and conduct a comparative analysis with our CRNN-based approach. We conducted co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#20102;&#20154;&#31867;&#21644;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#36880;&#35789;&#21453;&#24212;&#26102;&#38388;&#19978;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;&#27169;&#22411;&#65292;&#37117;&#22312;&#35821;&#27861;&#38169;&#35823;&#21477;&#23376;&#21306;&#22495;&#34920;&#29616;&#20986;&#20102;&#22686;&#21152;&#30340;&#22788;&#29702;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#22312;&#39044;&#27979;&#35821;&#27861;&#21644;&#38750;&#35821;&#27861;&#21477;&#23376;&#20043;&#38388;&#22686;&#37327;&#22788;&#29702;&#22256;&#38590;&#24046;&#24322;&#30340;&#24133;&#24230;&#19978;&#23384;&#22312;&#31995;&#32479;&#24615;&#20302;&#20272;&#12290;</title><link>http://arxiv.org/abs/2106.03232</link><description>&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#30340;&#22686;&#37327;&#22788;&#29702;&#30340;&#26377;&#38024;&#23545;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Targeted Assessment of Incremental Processing in Neural LanguageModels and Humans. (arXiv:2106.03232v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#20102;&#20154;&#31867;&#21644;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#36880;&#35789;&#21453;&#24212;&#26102;&#38388;&#19978;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;&#27169;&#22411;&#65292;&#37117;&#22312;&#35821;&#27861;&#38169;&#35823;&#21477;&#23376;&#21306;&#22495;&#34920;&#29616;&#20986;&#20102;&#22686;&#21152;&#30340;&#22788;&#29702;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#22312;&#39044;&#27979;&#35821;&#27861;&#21644;&#38750;&#35821;&#27861;&#21477;&#23376;&#20043;&#38388;&#22686;&#37327;&#22788;&#29702;&#22256;&#38590;&#24046;&#24322;&#30340;&#24133;&#24230;&#19978;&#23384;&#22312;&#31995;&#32479;&#24615;&#20302;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#36328;&#36234;&#22810;&#31181;&#32467;&#26500;&#29616;&#35937;&#30340;&#21313;&#20845;&#20010;&#19981;&#21516;&#21477;&#27861;&#27979;&#35797;&#22871;&#20214;&#30340;&#36880;&#35789;&#21453;&#24212;&#26102;&#38388;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#22686;&#37327;&#22788;&#29702;&#30340;&#26377;&#38024;&#23545;&#24615;&#12289;&#35268;&#27169;&#21270;&#27604;&#36739;&#12290;&#20154;&#31867;&#21453;&#24212;&#26102;&#38388;&#25968;&#25454;&#26469;&#33258;&#19968;&#31181;&#31216;&#20026;"Interpolated Maze"&#20219;&#21153;&#30340;&#26032;&#22411;&#22312;&#32447;&#23454;&#39564;&#33539;&#20363;&#12290;&#25105;&#20204;&#23558;&#20154;&#31867;&#21453;&#24212;&#26102;&#38388;&#19982;&#22235;&#31181;&#19981;&#21516;&#26550;&#26500;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#33539;&#22260;&#30340;&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#35789;&#27010;&#29575;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#29616;&#35937;&#20013;&#65292;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#27861;&#38169;&#35823;&#30340;&#21477;&#23376;&#21306;&#22495;&#37117;&#26174;&#31034;&#20986;&#20102;&#22686;&#21152;&#30340;&#22788;&#29702;&#22256;&#38590;&#65292;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#8220;&#20934;&#30830;&#24230;&#8221;&#24471;&#20998;&#65288;&#31867;&#20284;&#20110;Marvin&#21644;Linzen(2018)&#65289;&#22823;&#33268;&#30456;&#31561;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#30340;&#26041;&#21521;&#30456;&#21305;&#37197;&#65292;&#25105;&#20204;&#34920;&#26126;&#27169;&#22411;&#22312;&#39044;&#27979;&#35821;&#27861;&#21644;&#38750;&#35821;&#27861;&#21477;&#23376;&#20043;&#38388;&#22686;&#37327;&#22788;&#29702;&#22256;&#38590;&#24046;&#24322;&#30340;&#24133;&#24230;&#19978;&#31995;&#32479;&#24615;&#22320;&#20302;&#20272;&#20102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#27169;&#22411;&#36935;&#21040;&#21477;&#27861;&#38169;&#35823;&#26102;&#65292;&#27169;&#22411;&#22312;&#22686;&#37327;&#22788;&#29702;&#22256;&#38590;&#30340;&#24046;&#24322;&#22823;&#23567;&#19978;&#20302;&#20272;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a targeted, scaled-up comparison of incremental processing in humans and neural language models by collecting by-word reaction time data for sixteen different syntactic test suites across a range of structural phenomena. Human reaction time data comes from a novel online experimental paradigm called the Interpolated Maze task. We compare human reaction times to by-word probabilities for four contemporary language models, with different architectures and trained on a range of data set sizes. We find that across many phenomena, both humans and language models show increased processing difficulty in ungrammatical sentence regions with human and model `accuracy' scores (a la Marvin and Linzen(2018)) about equal. However, although language model outputs match humans in direction, we show that models systematically under-predict the difference in magnitude of incremental processing difficulty between grammatical and ungrammatical sentences. Specifically, when models encounter synt
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;P-Tuning&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#23884;&#20837;&#21644;&#31163;&#25955;&#25552;&#31034;&#30340;&#25340;&#25509;&#65292;&#31283;&#23450;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2103.10385</link><description>&lt;p&gt;
GPT&#20063;&#29702;&#35299;&#20102;
&lt;/p&gt;
&lt;p&gt;
GPT Understands, Too. (arXiv:2103.10385v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.10385
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;P-Tuning&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#23884;&#20837;&#21644;&#31163;&#25955;&#25552;&#31034;&#30340;&#25340;&#25509;&#65292;&#31283;&#23450;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#27169;&#24335;&#26469;&#20419;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#20855;&#26377;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#26041;&#38754;&#30340;&#25928;&#26524;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#21457;&#29616;&#65292;&#25163;&#21160;&#31163;&#25955;&#30340;&#25552;&#31034;&#24448;&#24448;&#23548;&#33268;&#24615;&#33021;&#19981;&#31283;&#23450;&#8212;&#8212;&#20363;&#22914;&#65292;&#22312;&#25552;&#31034;&#20013;&#25913;&#21464;&#19968;&#20010;&#21333;&#35789;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;P-Tuning&#65292;&#23427;&#20351;&#29992;&#35757;&#32451;&#21487;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#23884;&#20837;&#20197;&#21450;&#31163;&#25955;&#25552;&#31034;&#30340;&#25340;&#25509;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;P-Tuning&#19981;&#20165;&#36890;&#36807;&#20943;&#23567;&#21508;&#31181;&#31163;&#25955;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#31283;&#23450;&#35757;&#32451;&#65292;&#36824;&#36890;&#36807;&#36739;&#22823;&#24133;&#24230;&#30340;&#25552;&#21319;&#22312;&#21253;&#25324;LAMA&#21644;SuperGLUE&#22312;&#20869;&#30340;&#21508;&#31181;NLU&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#26080;&#35770;&#26159;&#20923;&#32467;&#30340;&#36824;&#26159;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20840;&#30417;&#30563;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;P-Tuning&#36890;&#24120;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performance -- e.g., changing a single word in the prompt might result in substantial performance drop. We propose a novel method P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts. Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.
&lt;/p&gt;</description></item></channel></rss>