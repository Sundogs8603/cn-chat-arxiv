<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#24341;&#20837;&#20102;&#22238;&#28335;&#20219;&#21153;&#65292;&#36890;&#36807;&#26816;&#32034;&#25991;&#26412;&#27573;&#26469;&#30830;&#23450;&#24341;&#21457;&#29992;&#25143;&#26597;&#35810;&#30340;&#21407;&#22240;&#65292;&#28041;&#21450;&#21040;&#19981;&#21516;&#39046;&#22495;&#65292;&#21253;&#25324;&#35762;&#24231;&#12289;&#26032;&#38395;&#21644;&#23545;&#35805;&#65292;&#35780;&#20272;&#20102;&#38646;&#27425;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03956</link><description>&lt;p&gt;
&#22238;&#28335;&#65306;&#26816;&#32034;&#26597;&#35810;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Backtracing: Retrieving the Cause of the Query
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03956
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22238;&#28335;&#20219;&#21153;&#65292;&#36890;&#36807;&#26816;&#32034;&#25991;&#26412;&#27573;&#26469;&#30830;&#23450;&#24341;&#21457;&#29992;&#25143;&#26597;&#35810;&#30340;&#21407;&#22240;&#65292;&#28041;&#21450;&#21040;&#19981;&#21516;&#39046;&#22495;&#65292;&#21253;&#25324;&#35762;&#24231;&#12289;&#26032;&#38395;&#21644;&#23545;&#35805;&#65292;&#35780;&#20272;&#20102;&#38646;&#27425;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22312;&#32447;&#20869;&#23481;&#38376;&#25143;&#20801;&#35768;&#29992;&#25143;&#25552;&#20986;&#38382;&#39064;&#20197;&#34917;&#20805;&#20182;&#20204;&#30340;&#29702;&#35299;&#65288;&#20363;&#22914;&#65292;&#23545;&#35762;&#24231;&#30340;&#29702;&#35299;&#65289;&#12290;&#34429;&#28982;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#21487;&#20197;&#20026;&#36825;&#31867;&#29992;&#25143;&#26597;&#35810;&#25552;&#20379;&#31572;&#26696;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#30452;&#25509;&#24110;&#21161;&#20869;&#23481;&#21019;&#24314;&#32773;&#65288;&#22914;&#24076;&#26395;&#25913;&#36827;&#20869;&#23481;&#30340;&#35762;&#24072;&#65289;&#35782;&#21035;&#24341;&#21457;&#29992;&#25143;&#25552;&#20986;&#36825;&#20123;&#38382;&#39064;&#30340;&#27573;&#33853;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22238;&#28335;&#20219;&#21153;&#65292;&#31995;&#32479;&#26816;&#32034;&#20986;&#26368;&#26377;&#21487;&#33021;&#24341;&#21457;&#29992;&#25143;&#26597;&#35810;&#30340;&#25991;&#26412;&#27573;&#12290;&#25105;&#20204;&#23545;&#25552;&#39640;&#20869;&#23481;&#20256;&#36882;&#21644;&#27807;&#36890;&#20013;&#37325;&#35201;&#30340;&#22238;&#28335;&#20219;&#21153;&#36827;&#34892;&#20102;&#19977;&#20010;&#29616;&#23454;&#19990;&#30028;&#39046;&#22495;&#30340;&#24418;&#24335;&#21270;&#65306;&#65288;a&#65289;&#35762;&#24231;&#39046;&#22495;&#20013;&#23398;&#29983;&#22256;&#24785;&#30340;&#21407;&#22240;&#65292;&#65288;b&#65289;&#26032;&#38395;&#25991;&#31456;&#39046;&#22495;&#20013;&#35835;&#32773;&#22909;&#22855;&#24515;&#30340;&#21407;&#22240;&#65292;&#20197;&#21450;&#65288;c&#65289;&#23545;&#35805;&#39046;&#22495;&#20013;&#29992;&#25143;&#24773;&#32490;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#21644;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#30340;&#38646;&#27425;&#24615;&#33021;&#65292;&#21253;&#25324;&#21452;&#32534;&#30721;&#22120;&#12289;&#37325;&#26032;&#25490;&#24207;&#21644;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03956v1 Announce Type: cross  Abstract: Many online content portals allow users to ask questions to supplement their understanding (e.g., of lectures). While information retrieval (IR) systems may provide answers for such user queries, they do not directly assist content creators -- such as lecturers who want to improve their content -- identify segments that _caused_ a user to ask those questions. We introduce the task of backtracing, in which systems retrieve the text segment that most likely caused a user query. We formalize three real-world domains for which backtracing is important in improving content delivery and communication: understanding the cause of (a) student confusion in the Lecture domain, (b) reader curiosity in the News Article domain, and (c) user emotion in the Conversation domain. We evaluate the zero-shot performance of popular information retrieval methods and language modeling methods, including bi-encoder, re-ranking and likelihood-based methods and 
&lt;/p&gt;</description></item><item><title>&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#21457;&#29616;&#25152;&#26377;&#23376;&#32593;&#32476;&#37117;&#20849;&#20139;&#19968;&#20010;&#27880;&#24847;&#21147;&#22836;&#38598;&#21512;&#65292;&#34987;&#31216;&#20026;&#21551;&#21457;&#24335;&#26680;&#24515;&#65292;&#36825;&#21487;&#33021;&#26159;&#36896;&#25104;&#23376;&#32593;&#32476;&#27867;&#21270;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2403.03942</link><description>&lt;p&gt;
&#12298;&#21551;&#21457;&#24335;&#26680;&#24515;&#65306;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23376;&#32593;&#32476;&#27867;&#21270;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03942
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#21457;&#29616;&#25152;&#26377;&#23376;&#32593;&#32476;&#37117;&#20849;&#20139;&#19968;&#20010;&#27880;&#24847;&#21147;&#22836;&#38598;&#21512;&#65292;&#34987;&#31216;&#20026;&#21551;&#21457;&#24335;&#26680;&#24515;&#65292;&#36825;&#21487;&#33021;&#26159;&#36896;&#25104;&#23376;&#32593;&#32476;&#27867;&#21270;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#19981;&#21516;&#38543;&#26426;&#31181;&#23376;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#39046;&#22495;&#20869;&#24615;&#33021;&#65292;&#20294;&#22312;&#21477;&#27861;&#27867;&#21270;&#27979;&#35797;&#20013;&#27867;&#21270;&#19981;&#21516;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#21363;&#20351;&#22312;&#21333;&#19968;&#27169;&#22411;&#20869;&#37096;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#25214;&#21040;&#25191;&#34892;&#31867;&#20284;&#39046;&#22495;&#20869;&#25805;&#20316;&#20294;&#27867;&#21270;&#24046;&#24322;&#24040;&#22823;&#30340;&#22810;&#20010;&#23376;&#32593;&#32476;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#29616;&#35937;&#65292;&#25105;&#20204;&#35843;&#26597;&#26159;&#21542;&#21487;&#20197;&#29992;&#8220;&#31454;&#20105;&#24615;&#23376;&#32593;&#32476;&#8221;&#26469;&#29702;&#35299;&#23427;&#20204;&#65306;&#27169;&#22411;&#26368;&#21021;&#34920;&#31034;&#21508;&#31181;&#19981;&#21516;&#31639;&#27861;&#65292;&#23545;&#24212;&#20110;&#19981;&#21516;&#30340;&#23376;&#32593;&#32476;&#65292;&#24403;&#26368;&#32456;&#25910;&#25947;&#21040;&#20854;&#20013;&#19968;&#20010;&#26102;&#27867;&#21270;&#21457;&#29983;&#12290;&#36825;&#19968;&#35299;&#37322;&#24050;&#34987;&#29992;&#20110;&#35299;&#37322;&#31616;&#21333;&#31639;&#27861;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24182;&#38750;&#25214;&#21040;&#31454;&#20105;&#24615;&#23376;&#32593;&#32476;&#65292;&#32780;&#26159;&#25152;&#26377;&#23376;&#32593;&#32476; -- &#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#27867;&#21270; -- &#37117;&#20849;&#20139;&#19968;&#32452;&#27880;&#24847;&#21147;&#22836;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#21551;&#21457;&#24335;&#26680;&#24515;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#21487;&#33021;&#26159;&#36896;&#25104;&#19981;&#21516;&#23376;&#32593;&#32476;&#27867;&#21270;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03942v1 Announce Type: new  Abstract: Prior work has found that pretrained language models (LMs) fine-tuned with different random seeds can achieve similar in-domain performance but generalize differently on tests of syntactic generalization. In this work, we show that, even within a single model, we can find multiple subnetworks that perform similarly in-domain, but generalize vastly differently. To better understand these phenomena, we investigate if they can be understood in terms of "competing subnetworks": the model initially represents a variety of distinct algorithms, corresponding to different subnetworks, and generalization occurs when it ultimately converges to one. This explanation has been used to account for generalization in simple algorithmic tasks. Instead of finding competing subnetworks, we find that all subnetworks -- whether they generalize or not -- share a set of attention heads, which we refer to as the heuristic core. Further analysis suggests that th
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26032;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21508;&#31181;&#22122;&#22768;&#36755;&#20837;&#26102;&#27604;&#20808;&#21069;&#30340;&#27169;&#22411;&#26356;&#21152;&#31283;&#20581;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#21442;&#25968;&#26356;&#22810;&#12289;&#35757;&#32451;&#36807;&#31243;&#26356;&#22797;&#26434;&#65292;&#24182;&#19988;&#27809;&#26377;&#37319;&#29992;&#29305;&#23450;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#31283;&#20581;&#24615;&#30340;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.03923</link><description>&lt;p&gt;
&#32763;&#35793;&#27169;&#22411;&#26159;&#21542;&#22312;&#26080;&#20154;&#21457;&#35273;&#30340;&#24773;&#20917;&#19979;&#21464;&#24471;&#26356;&#21152;&#31283;&#20581;&#20102;&#65311;
&lt;/p&gt;
&lt;p&gt;
Did Translation Models Get More Robust Without Anyone Even Noticing?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03923
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26032;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21508;&#31181;&#22122;&#22768;&#36755;&#20837;&#26102;&#27604;&#20808;&#21069;&#30340;&#27169;&#22411;&#26356;&#21152;&#31283;&#20581;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#21442;&#25968;&#26356;&#22810;&#12289;&#35757;&#32451;&#36807;&#31243;&#26356;&#22797;&#26434;&#65292;&#24182;&#19988;&#27809;&#26377;&#37319;&#29992;&#29305;&#23450;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#31283;&#20581;&#24615;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#27169;&#22411;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#20294;&#26222;&#36941;&#35748;&#20026;&#23427;&#20204;&#23545;"&#22024;&#26434;"&#36755;&#20837;&#65288;&#22914;&#25340;&#20889;&#38169;&#35823;&#12289;&#32553;&#20889;&#21644;&#20854;&#20182;&#26684;&#24335;&#38382;&#39064;&#65289;&#38750;&#24120;&#25935;&#24863;&#12290;&#26412;&#25991;&#38024;&#23545;&#26368;&#36817;&#30340;&#22810;&#35821;&#35328;MT&#27169;&#22411;&#21644;&#24212;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#37325;&#26032;&#23457;&#35270;&#36825;&#19968;&#35266;&#28857;&#12290;&#26377;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#21463;&#25511;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#35768;&#22810;&#31181;&#22122;&#22768;&#27604;&#20808;&#21069;&#30340;&#27169;&#22411;&#26356;&#21152;&#31283;&#20581;&#65292;&#21363;&#20351;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#34920;&#29616;&#31867;&#20284;&#12290;&#36825;&#24456;&#24341;&#20154;&#27880;&#30446;&#65292;&#22240;&#20026;&#23613;&#31649;LLMs&#25317;&#26377;&#27604;&#36807;&#21435;&#27169;&#22411;&#26356;&#22810;&#30340;&#21442;&#25968;&#21644;&#26356;&#22797;&#26434;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#25105;&#20204;&#32771;&#34385;&#30340;&#24320;&#28304;&#27169;&#22411;&#20013;&#27809;&#26377;&#19968;&#20010;&#20351;&#29992;&#20219;&#20309;&#19987;&#38376;&#35774;&#35745;&#30340;&#40723;&#21169;&#31283;&#20581;&#24615;&#30340;&#25216;&#26415;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#31867;&#20284;&#30340;&#36235;&#21183;&#20063;&#36866;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#32763;&#35793;&#23454;&#39564;&#8212;&#8212;LLMs&#23545;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#26356;&#21152;&#31283;&#20581;&#12290;&#25105;&#20204;&#36824;&#21253;&#25324;&#20102;&#19968;&#39033;&#20851;&#20110;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03923v1 Announce Type: new  Abstract: Neural machine translation (MT) models achieve strong results across a variety of settings, but it is widely believed that they are highly sensitive to "noisy" inputs, such as spelling errors, abbreviations, and other formatting issues. In this paper, we revisit this insight in light of recent multilingual MT models and large language models (LLMs) applied to machine translation. Somewhat surprisingly, we show through controlled experiments that these models are far more robust to many kinds of noise than previous models, even when they perform similarly on clean data. This is notable because, even though LLMs have more parameters and more complex training processes than past models, none of the open ones we consider use any techniques specifically designed to encourage robustness. Next, we show that similar trends hold for social media translation experiments -- LLMs are more robust to social media text. We include an analysis of the ci
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#25991;&#26412;&#20998;&#26512;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#25945;&#23398;&#36136;&#37327;&#25552;&#21319;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#21644;&#21487;&#25805;&#20316;&#30340;&#21453;&#39304;&#12290;</title><link>https://arxiv.org/abs/2403.03920</link><description>&lt;p&gt;
&#25552;&#21319;&#25945;&#23398;&#36136;&#37327;&#65306;&#21033;&#29992;&#35745;&#31639;&#26426;&#36741;&#21161;&#25991;&#26412;&#20998;&#26512;&#20174;&#25945;&#32946;&#25991;&#29486;&#20013;&#29983;&#25104;&#28145;&#20837;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Enhancing Instructional Quality: Leveraging Computer-Assisted Textual Analysis to Generate In-Depth Insights from Educational Artifacts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03920
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#25991;&#26412;&#20998;&#26512;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#25945;&#23398;&#36136;&#37327;&#25552;&#21319;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#21644;&#21487;&#25805;&#20316;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35745;&#31639;&#26426;&#36741;&#21161;&#25991;&#26412;&#20998;&#26512;&#22312;&#36890;&#36807;&#25945;&#32946;&#25991;&#29486;&#25552;&#20379;&#28145;&#20837;&#35265;&#35299;&#20197;&#25552;&#21319;&#25945;&#23398;&#36136;&#37327;&#20013;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#29702;&#26597;&#24503;&#183;&#22467;&#23572;&#33707;&#23572;&#30340;&#25945;&#23398;&#26680;&#24515;&#26694;&#26550;&#26469;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#65292;&#21487;&#20197;&#20998;&#26512;&#25945;&#32946;&#20869;&#23481;&#12289;&#25945;&#24072;&#35805;&#35821;&#21644;&#23398;&#29983;&#22238;&#24212;&#20197;&#20419;&#36827;&#25945;&#23398;&#25913;&#36827;&#12290;&#36890;&#36807;&#23545;&#25945;&#23398;&#26680;&#24515;&#26694;&#26550;&#20869;&#30340;&#32508;&#21512;&#35780;&#35770;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;AI/ML&#25972;&#21512;&#25552;&#20379;&#26174;&#33879;&#20248;&#21183;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#21253;&#25324;&#25945;&#24072;&#36741;&#23548;&#12289;&#23398;&#29983;&#25903;&#25345;&#21644;&#20869;&#23481;&#24320;&#21457;&#12290;&#25105;&#20204;&#25581;&#31034;&#20986;&#30340;&#27169;&#24335;&#34920;&#26126;&#65292;AI/ML &#19981;&#20165;&#31616;&#21270;&#20102;&#34892;&#25919;&#20219;&#21153;&#65292;&#36824;&#20026;&#20010;&#24615;&#21270;&#23398;&#20064;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#65292;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#21453;&#39304;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#25945;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03920v1 Announce Type: new  Abstract: This paper explores the transformative potential of computer-assisted textual analysis in enhancing instructional quality through in-depth insights from educational artifacts. We integrate Richard Elmore's Instructional Core Framework to examine how artificial intelligence (AI) and machine learning (ML) methods, particularly natural language processing (NLP), can analyze educational content, teacher discourse, and student responses to foster instructional improvement. Through a comprehensive review and case studies within the Instructional Core Framework, we identify key areas where AI/ML integration offers significant advantages, including teacher coaching, student support, and content development. We unveil patterns that indicate AI/ML not only streamlines administrative tasks but also introduces novel pathways for personalized learning, providing actionable feedback for educators and contributing to a richer understanding of instructi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25968;&#25454;&#38598;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#21442;&#32771;&#35821;&#35328;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#65292;&#21033;&#29992;&#36866;&#21512;&#27604;&#36739;&#24230;&#37327;&#38598;&#21512;&#30340;Jaccard&#25351;&#25968;&#30340;&#29256;&#26412;&#26469;&#26368;&#22823;&#31243;&#24230;&#22320;&#20419;&#36827;&#38271;&#26399;&#20869;&#30340;&#22810;&#35821;&#35328;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03909</link><description>&lt;p&gt;
&#29992;&#20110;&#36879;&#26126;&#27604;&#36739;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Measure for Transparent Comparison of Linguistic Diversity in Multilingual NLP Data Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25968;&#25454;&#38598;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#21442;&#32771;&#35821;&#35328;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#65292;&#21033;&#29992;&#36866;&#21512;&#27604;&#36739;&#24230;&#37327;&#38598;&#21512;&#30340;Jaccard&#25351;&#25968;&#30340;&#29256;&#26412;&#26469;&#26368;&#22823;&#31243;&#24230;&#22320;&#20419;&#36827;&#38271;&#26399;&#20869;&#30340;&#22810;&#35821;&#35328;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22411;&#23398;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#21019;&#24314;&#26469;&#36861;&#36394;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#36890;&#24120;&#34987;&#27979;&#37327;&#20026;&#26679;&#26412;&#20013;&#21253;&#21547;&#30340;&#35821;&#35328;&#25110;&#35821;&#35328;&#23478;&#26063;&#30340;&#25968;&#37327;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#24182;&#26410;&#32771;&#34385;&#25152;&#21253;&#21547;&#35821;&#35328;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#19982;&#21442;&#32771;&#35821;&#35328;&#26679;&#26412;&#36827;&#34892;&#35780;&#20272;&#65292;&#20316;&#20026;&#26368;&#22823;&#31243;&#24230;&#22320;&#20419;&#36827;&#38271;&#26399;&#20869;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#25105;&#20204;&#23558;&#35821;&#35328;&#34920;&#31034;&#20026;&#29305;&#24449;&#38598;&#65292;&#24182;&#24212;&#29992;&#36866;&#29992;&#20110;&#27604;&#36739;&#24230;&#37327;&#38598;&#21512;&#30340;Jaccard&#25351;&#25968;&#30340;&#29256;&#26412;&#12290;&#38500;&#20102;&#20174;&#31867;&#22411;&#23398;&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#19968;&#31181;&#33258;&#21160;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#20811;&#26381;&#25163;&#21160;&#25910;&#38598;&#29305;&#24449;&#20013;&#24050;&#30693;&#30340;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#25105;&#20204;&#30340;&#22810;&#26679;&#24615;&#35780;&#20998;&#22312;&#35821;&#35328;&#29305;&#24449;&#26041;&#38754;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#35782;&#21035;&#35821;&#35328;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03909v1 Announce Type: new  Abstract: Typologically diverse benchmarks are increasingly created to track the progress achieved in multilingual NLP. Linguistic diversity of these data sets is typically measured as the number of languages or language families included in the sample, but such measures do not consider structural properties of the included languages. In this paper, we propose assessing linguistic diversity of a data set against a reference language sample as a means of maximising linguistic diversity in the long run. We represent languages as sets of features and apply a version of the Jaccard index suitable for comparing sets of measures. In addition to the features extracted from typological data bases, we propose an automatic text-based measure, which can be used as a means of overcoming the well-known problem of data sparsity in manually collected features. Our diversity score is interpretable in terms of linguistic features and can identify the types of lang
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.03894</link><description>&lt;p&gt;
IRCoder: &#20013;&#38388;&#34920;&#31034;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#31283;&#20581;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03894
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03894v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#24050;&#36805;&#36895;&#25104;&#20026;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26368;&#21463;&#27426;&#36814;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19982;&#33258;&#28982;&#35821;&#35328;LM&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#23545;&#20195;&#30721;-LMs&#65288;&#21363;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;LMs&#65289;&#30340;&#22810;&#35821;&#35328;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#22914;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#25968;&#25454;&#22686;&#24378;&#20197;&#21450;&#20107;&#21518;LM&#35843;&#25972;&#65292;&#20197;&#21450;&#21033;&#29992;&#21407;&#22987;&#25991;&#26412;&#20869;&#23481;&#20043;&#22806;&#30340;&#25968;&#25454;&#28304;&#65292;&#35201;&#31232;&#23569;&#24471;&#22810;&#12290;&#29305;&#21035;&#26159;&#65292;&#22823;&#22810;&#25968;&#20027;&#27969;&#20195;&#30721;-LMs&#20165;&#22312;&#28304;&#20195;&#30721;&#25991;&#20214;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#29616;&#25104;&#30340;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#65288;&#36328;&#32534;&#31243;&#35821;&#35328;&#20849;&#20139;&#65289;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#24182;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#21069;&#26223;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#32534;&#21046;&#20102;SLTrans&#65292;&#19968;&#20010;&#30001;&#36817;400&#19975;&#20010;&#33258;&#21253;&#21547;&#28304;&#20195;&#30721;&#25991;&#20214;&#32452;&#25104;&#30340;&#24182;&#34892;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03894v1 Announce Type: new  Abstract: Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.   To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled wi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25299;&#23637;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27602;&#24615;&#32531;&#35299;&#30340;&#33539;&#22260;&#65292;&#28085;&#30422;&#20102;&#22810;&#35821;&#35328;&#29615;&#22659;&#65292;&#36890;&#36807;&#32763;&#35793;&#25968;&#25454;&#35780;&#20272;&#21644;&#22686;&#24378;&#32531;&#35299;&#25216;&#26415;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#32531;&#35299;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#37327;&#23545;&#32531;&#35299;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03893</link><description>&lt;p&gt;
&#20174;&#21333;&#19968;&#21040;&#22810;&#26679;&#65306;&#25299;&#23637;&#35821;&#35328;&#27169;&#22411;&#20013;&#27602;&#24615;&#32531;&#35299;&#30340;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03893
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25299;&#23637;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27602;&#24615;&#32531;&#35299;&#30340;&#33539;&#22260;&#65292;&#28085;&#30422;&#20102;&#22810;&#35821;&#35328;&#29615;&#22659;&#65292;&#36890;&#36807;&#32763;&#35793;&#25968;&#25454;&#35780;&#20272;&#21644;&#22686;&#24378;&#32531;&#35299;&#25216;&#26415;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#32531;&#35299;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#37327;&#23545;&#32531;&#35299;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27602;&#24615;&#32531;&#35299;&#20960;&#20046;&#23436;&#20840;&#38598;&#20013;&#22312;&#21333;&#35821;&#35328;&#29615;&#22659;&#20013;&#12290;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#25317;&#25265;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#23433;&#20840;&#25514;&#26045;&#36319;&#19978;&#27493;&#20240;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#27602;&#24615;&#32531;&#35299;&#33539;&#22260;&#25193;&#23637;&#21040;&#24212;&#23545;&#22810;&#35821;&#35328;&#24102;&#26469;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#32570;&#20047;&#36328;&#35821;&#35328;&#30340;&#36275;&#22815;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#32763;&#35793;&#25968;&#25454;&#26469;&#35780;&#20272;&#21644;&#22686;&#24378;&#25105;&#20204;&#30340;&#32531;&#35299;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#22312;&#38745;&#24577;&#21644;&#25345;&#32493;&#27602;&#24615;&#32531;&#35299;&#22330;&#26223;&#19979;&#27604;&#36739;&#20102;&#24494;&#35843;&#32531;&#35299;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26816;&#39564;&#32763;&#35793;&#36136;&#37327;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#23545;&#27602;&#24615;&#32531;&#35299;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#25968;&#37327;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#32531;&#35299;&#24037;&#20316;&#30340;&#25104;&#21151;&#12290;&#28085;&#30422;&#20102;&#20061;&#31181;&#35821;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20195;&#34920;&#20102;&#24191;&#27867;&#30340;&#35821;&#35328;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03893v1 Announce Type: cross  Abstract: To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it's crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic f
&lt;/p&gt;</description></item><item><title>FaaF&#26159;&#19968;&#31181;&#26032;&#30340;&#20107;&#23454;&#39564;&#35777;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#20989;&#25968;&#35843;&#29992;&#33021;&#21147;&#21644;&#38754;&#21521;RAG&#20107;&#23454;&#22238;&#24518;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;LM&#35782;&#21035;&#19981;&#25903;&#25345;&#20107;&#23454;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#25928;&#29575;&#21644;&#25104;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.03888</link><description>&lt;p&gt;
FaaF&#65306;&#20316;&#20026;RAG&#31995;&#32479;&#35780;&#20272;&#30340;&#20107;&#23454;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
FaaF: Facts as a Function for the evaluation of RAG systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03888
&lt;/p&gt;
&lt;p&gt;
FaaF&#26159;&#19968;&#31181;&#26032;&#30340;&#20107;&#23454;&#39564;&#35777;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#20989;&#25968;&#35843;&#29992;&#33021;&#21147;&#21644;&#38754;&#21521;RAG&#20107;&#23454;&#22238;&#24518;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;LM&#35782;&#21035;&#19981;&#25903;&#25345;&#20107;&#23454;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#25928;&#29575;&#21644;&#25104;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21442;&#32771;&#36164;&#26009;&#20013;&#20934;&#30830;&#25552;&#21462;&#20107;&#23454;&#23545;&#20110;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#25506;&#26597;&#20102;&#26816;&#32034;&#21644;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#21487;&#38752;&#39640;&#25928;&#22320;&#25191;&#34892;&#36825;&#31181;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#36890;&#36807;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35780;&#20272;&#22120;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65292;&#28982;&#32780;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20449;&#24687;&#19981;&#23436;&#25972;&#25110;&#19981;&#20934;&#30830;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FaaF&#65288;Facts as a Function&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;LM&#30340;&#20989;&#25968;&#35843;&#29992;&#33021;&#21147;&#21644;&#38754;&#21521;RAG&#20107;&#23454;&#22238;&#24518;&#35780;&#20272;&#30340;&#26694;&#26550;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;FaaF&#26174;&#30528;&#25552;&#39640;&#20102;LM&#35782;&#21035;&#25991;&#26412;&#20013;&#19981;&#25903;&#25345;&#20107;&#23454;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#38477;&#20302;&#20102;&#25104;&#26412;&#20960;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03888v1 Announce Type: new  Abstract: Factual recall from a reference source is crucial for evaluating the performance of Retrieval Augmented Generation (RAG) systems, as it directly probes into the quality of both retrieval and generation. However, it still remains a challenge to perform this evaluation reliably and efficiently. Recent work has focused on fact verification via prompting language model (LM) evaluators, however we demonstrate that these methods are unreliable in the presence of incomplete or inaccurate information. We introduce Facts as a Function (FaaF), a new approach to fact verification that utilizes the function calling abilities of LMs and a framework for RAG factual recall evaluation. FaaF substantially improves the ability of LMs to identify unsupported facts in text with incomplete information whilst improving efficiency and lowering cost by several times, compared to prompt-based approaches.
&lt;/p&gt;</description></item><item><title>SaulLM-7B&#26159;&#39318;&#20010;&#19987;&#20026;&#27861;&#24459;&#25991;&#26412;&#35774;&#35745;&#30340;7B&#21442;&#25968;LLM&#65292;&#36890;&#36807;InnovativeFine-tuning&#26041;&#27861;&#65292;&#23637;&#29616;&#20102;&#39046;&#20808;&#30340;&#27861;&#24459;&#25991;&#20214;&#29702;&#35299;&#21644;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03883</link><description>&lt;p&gt;
SaulLM-7B: &#38754;&#21521;&#27861;&#24459;&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SaulLM-7B: A pioneering Large Language Model for Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03883
&lt;/p&gt;
&lt;p&gt;
SaulLM-7B&#26159;&#39318;&#20010;&#19987;&#20026;&#27861;&#24459;&#25991;&#26412;&#35774;&#35745;&#30340;7B&#21442;&#25968;LLM&#65292;&#36890;&#36807;InnovativeFine-tuning&#26041;&#27861;&#65292;&#23637;&#29616;&#20102;&#39046;&#20808;&#30340;&#27861;&#24459;&#25991;&#20214;&#29702;&#35299;&#21644;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SaulLM-7B&#65292;&#19968;&#20010;&#19987;&#20026;&#27861;&#24459;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#25317;&#26377;70&#20159;&#21442;&#25968;&#30340;SaulLM-7B&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#27861;&#24459;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#32780;&#35774;&#35745;&#30340;LLM&#12290;&#20197;Mistral 7B&#26550;&#26500;&#20026;&#22522;&#30784;&#65292;SaulLM-7B&#22312;&#36229;&#36807;300&#20159;&#26631;&#35760;&#30340;&#33521;&#25991;&#27861;&#24459;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;SaulLM-7B&#22312;&#29702;&#35299;&#21644;&#22788;&#29702;&#27861;&#24459;&#25991;&#20214;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25351;&#23548;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#27861;&#24459;&#25968;&#25454;&#38598;&#36827;&#19968;&#27493;&#22686;&#24378;SaulLM-7B&#22312;&#27861;&#24459;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;SaulLM-7B&#37319;&#29992;CC-BY-SA-4.0&#35768;&#21487;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03883v1 Announce Type: new  Abstract: In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is released under the CC-BY-SA-4.0 License.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;NLP&#39046;&#22495;&#20013;&#32570;&#20047;&#23545;&#31038;&#20250;&#38454;&#32423;&#22240;&#32032;&#30340;&#30740;&#31350;&#65292;&#21628;&#21505;&#30740;&#31350;&#32773;&#22312;NLP&#25216;&#26415;&#20013;&#32771;&#34385;&#21644;&#25805;&#20316;&#21270;&#38454;&#32423;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.03874</link><description>&lt;p&gt;
&#36139;&#22256;&#30340;&#35821;&#35328;&#25216;&#26415;&#65306;NLP&#20013;&#32570;&#20047;&#65288;&#31038;&#20250;&#65289;&#38454;&#32423;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Impoverished Language Technology: The Lack of (Social) Class in NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;NLP&#39046;&#22495;&#20013;&#32570;&#20047;&#23545;&#31038;&#20250;&#38454;&#32423;&#22240;&#32032;&#30340;&#30740;&#31350;&#65292;&#21628;&#21505;&#30740;&#31350;&#32773;&#22312;NLP&#25216;&#26415;&#20013;&#32771;&#34385;&#21644;&#25805;&#20316;&#21270;&#38454;&#32423;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;Labov&#65288;1964&#24180;&#65289;&#20851;&#20110;&#35821;&#35328;&#31038;&#20250;&#23618;&#32423;&#30340;&#22522;&#30784;&#24615;&#24037;&#20316;&#20197;&#26469;&#65292;&#35821;&#35328;&#23398;&#30028;&#33268;&#21147;&#20110;&#29702;&#35299;&#31038;&#20250;&#20154;&#21475;&#22240;&#32032;&#21644;&#35821;&#35328;&#29983;&#20135;&#21644;&#24863;&#30693;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#22823;&#37327;&#35777;&#25454;&#34920;&#26126;&#31038;&#20250;&#20154;&#21475;&#22240;&#32032;&#19982;&#35821;&#35328;&#29983;&#20135;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#20851;&#31995;&#65292;&#20294;&#30456;&#23545;&#36739;&#23569;&#30340;&#22240;&#32032;&#22312;NLP&#25216;&#26415;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#30740;&#31350;&#12290;&#34429;&#28982;&#24180;&#40836;&#21644;&#24615;&#21035;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#35206;&#30422;&#65292;Labov&#26368;&#21021;&#20851;&#27880;&#30340;&#31038;&#20250;&#32463;&#27982;&#38454;&#32423;&#21364;&#20960;&#20046;&#19981;&#34987;&#28041;&#21450;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25991;&#29486;&#65292;&#24182;&#21457;&#29616;&#21482;&#26377;20&#31687;&#35770;&#25991;&#25552;&#21040;&#20102;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35770;&#25991;&#20013;&#30340;&#22823;&#37096;&#20998;&#24182;&#27809;&#26377;&#25506;&#35752;&#36229;&#20986;&#25910;&#38598;&#27880;&#37322;&#32773;&#20154;&#21475;&#20449;&#24687;&#20043;&#22806;&#30340;&#38454;&#32423;&#12290;&#37492;&#20110;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#34987;NLP&#30740;&#31350;&#20154;&#21592;&#25805;&#20316;&#30340;&#38454;&#32423;&#23450;&#20041;&#65292;&#24182;&#20027;&#24352;&#21253;&#21547;&#35813;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03874v1 Announce Type: cross  Abstract: Since Labov's (1964) foundational work on the social stratification of language, linguistics has dedicated concerted efforts towards understanding the relationships between socio-demographic factors and language production and perception. Despite the large body of evidence identifying significant relationships between socio-demographic factors and language production, relatively few of these factors have been investigated in the context of NLP technology. While age and gender are well covered, Labov's initial target, socio-economic class, is largely absent. We survey the existing Natural Language Processing (NLP) literature and find that only 20 papers even mention socio-economic status. However, the majority of those papers do not engage with class beyond collecting information of annotator-demographics. Given this research lacuna, we provide a definition of class that can be operationalised by NLP researchers, and argue for including
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#20102;&#19968;&#31181;&#21327;&#20316;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26631;&#35760;&#32423;&#21035;&#20132;&#38169;&#29983;&#25104;&#26469;&#25945;&#25480;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#65292;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#65292;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#34701;&#21512;&#27599;&#20010;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#32852;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03870</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Decode Collaboratively with Multiple Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03870
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20102;&#19968;&#31181;&#21327;&#20316;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26631;&#35760;&#32423;&#21035;&#20132;&#38169;&#29983;&#25104;&#26469;&#25945;&#25480;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#65292;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#65292;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#34701;&#21512;&#27599;&#20010;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#32852;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26631;&#35760;&#32423;&#21035;&#20132;&#38169;&#23427;&#20204;&#30340;&#29983;&#25104;&#26469;&#25945;&#25480;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21327;&#20316;&#12290;&#25105;&#20204;&#23558;&#19979;&#19968;&#20010;&#26631;&#35760;&#30001;&#21738;&#20010;LLM&#29983;&#25104;&#30340;&#20915;&#31574;&#24314;&#27169;&#20026;&#28508;&#21464;&#37327;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#19979;&#20248;&#21270;&#35757;&#32451;&#38598;&#30340;&#36793;&#38469;&#20284;&#28982;&#65292;&#22522;&#30784;LLM&#33258;&#21160;&#23398;&#20064;&#20309;&#26102;&#29983;&#25104;&#33258;&#36523;&#20197;&#21450;&#20309;&#26102;&#35843;&#29992;&#20854;&#20013;&#19968;&#20010;&#8220;&#21161;&#25163;&#8221;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36827;&#34892;&#26631;&#35760;&#32423;&#21327;&#20316;&#20801;&#35768;&#34701;&#21512;&#27599;&#20010;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20197;&#31526;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#21327;&#20316;&#35299;&#30721;&#22312;&#36328;&#39046;&#22495;&#35774;&#32622;&#20013;&#29305;&#21035;&#26377;&#29992;&#65292;&#20854;&#20013;&#36890;&#29992;&#22522;&#30784;LLM&#23398;&#20250;&#35843;&#29992;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#25191;&#34892;&#25351;&#20196;&#12289;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#21644;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32852;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#20248;&#20110;&#21333;&#29420;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#23398;&#20064;&#21040;&#30340;&#28508;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03870v1 Announce Type: new  Abstract: We propose a method to teach multiple large language models (LLM) to collaborate by interleaving their generations at the token level. We model the decision of which LLM generates the next token as a latent variable. By optimizing the marginal likelihood of a training set under our latent variable model, the base LLM automatically learns when to generate itself and when to call on one of the ``assistant'' language models to generate, all without direct supervision. Token-level collaboration during decoding allows for a fusion of each model's expertise in a manner tailored to the specific task at hand. Our collaborative decoding is especially useful in cross-domain settings where a generalist base LLM learns to invoke domain expert models. On instruction-following, domain-specific QA, and reasoning tasks, we show that the performance of the joint system exceeds that of the individual models. Through qualitative analysis of the learned lat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32447;&#24615;&#34920;&#31034;&#30340;&#36215;&#28304;&#65292;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#24615;&#20559;&#24046;&#19982;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#20849;&#21516;&#20419;&#36827;&#20102;&#27010;&#24565;&#30340;&#32447;&#24615;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.03867</link><description>&lt;p&gt;
&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32447;&#24615;&#34920;&#31034;&#30340;&#36215;&#28304;
&lt;/p&gt;
&lt;p&gt;
On the Origins of Linear Representations in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32447;&#24615;&#34920;&#31034;&#30340;&#36215;&#28304;&#65292;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#24615;&#20559;&#24046;&#19982;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#20849;&#21516;&#20419;&#36827;&#20102;&#27010;&#24565;&#30340;&#32447;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#32534;&#30721;&#20102;&#39640;&#23618;&#35821;&#20041;&#27010;&#24565;&#26159;"&#32447;&#24615;"&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#32447;&#24615;&#34920;&#31034;&#30340;&#36215;&#28304;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#25277;&#35937;&#21644;&#24418;&#24335;&#21270;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#27010;&#24565;&#21160;&#24577;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#24418;&#24335;&#21270;&#23637;&#31034;&#20102;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#65288;&#20855;&#26377;&#20132;&#21449;&#29109;&#30340;softmax&#65289;&#21644;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#24615;&#20559;&#24046;&#20849;&#21516;&#20419;&#36827;&#20102;&#27010;&#24565;&#30340;&#32447;&#24615;&#34920;&#31034;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#23398;&#20064;&#19982;&#28508;&#21464;&#37327;&#27169;&#22411;&#21305;&#37197;&#30340;&#25968;&#25454;&#26102;&#65292;&#32447;&#24615;&#34920;&#31034;&#20250;&#20986;&#29616;&#65292;&#20174;&#32780;&#30830;&#35748;&#36825;&#31181;&#31616;&#21333;&#32467;&#26500;&#24050;&#36275;&#20197;&#20135;&#29983;&#32447;&#24615;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992; LLaMA-2 &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20102;&#36825;&#19968;&#29702;&#35770;&#30340;&#37096;&#20998;&#39044;&#27979;&#65292;&#35777;&#26126;&#20102;&#31616;&#21270;&#27169;&#22411;&#25552;&#20379;&#20102;&#21487;&#25512;&#24191;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03867v1 Announce Type: new  Abstract: Recent works have argued that high-level semantic concepts are encoded "linearly" in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a simple latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to show that the next token prediction objective (softmax with cross-entropy) and the implicit bias of gradient descent together promote the linear representation of concepts. Experiments show that linear representations emerge when learning from data matching the latent variable model, confirming that this simple structure already suffices to yield linear representations. We additionally confirm some predictions of the theory using the LLaMA-2 large language model, giving evidence that the simplified model yields generalizable insights.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;KIWI&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#31185;&#23398;&#20889;&#20316;&#24110;&#21161;&#26102;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#25972;&#21512;&#26032;&#20449;&#24687;&#21040;&#29616;&#26377;&#31572;&#26696;&#20013;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.03866</link><description>&lt;p&gt;
KIWI&#65306;&#29992;&#20110;&#22238;&#31572;&#30740;&#31350;&#38382;&#39064;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#20889;&#20316;&#25351;&#21335;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03866
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;KIWI&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#31185;&#23398;&#20889;&#20316;&#24110;&#21161;&#26102;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#25972;&#21512;&#26032;&#20449;&#24687;&#21040;&#29616;&#26377;&#31572;&#26696;&#20013;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03866v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#34987;&#35843;&#25972;&#20197;&#36981;&#24490;&#29992;&#25143;&#25351;&#20196;&#65292;&#24182;&#24191;&#27867;&#37096;&#32626;&#20026;&#20250;&#35805;&#20195;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#36234;&#26469;&#36234;&#24120;&#35265;&#30340;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#65306;&#25552;&#20379;&#20889;&#20316;&#24110;&#21161;&#20197;&#25776;&#20889;&#38271;&#31687;&#31572;&#26696;&#12290;&#20026;&#20102;&#35780;&#20272;&#24403;&#21069;LLMs&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22312;&#31185;&#23398;&#39046;&#22495;&#20013;&#21253;&#21547;&#30693;&#35782;&#23494;&#38598;&#22411;&#20889;&#20316;&#25351;&#21335;&#30340;&#25968;&#25454;&#38598;KIWI&#12290;&#32473;&#23450;&#19968;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#19968;&#20010;&#21021;&#22987;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#21644;&#19968;&#32452;&#30456;&#20851;&#35770;&#25991;&#65292;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#32773;&#20250;&#36845;&#20195;&#22320;&#21457;&#24067;&#25351;&#23548;&#27169;&#22411;&#20462;&#35746;&#21644;&#25913;&#36827;&#31572;&#26696;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#20174;&#19982;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#30340;234&#20010;&#20132;&#20114;&#20250;&#35805;&#20013;&#25910;&#38598;&#20102;1,260&#20010;&#20132;&#20114;&#36718;&#27425;&#12290;&#27599;&#20010;&#36718;&#27425;&#21253;&#25324;&#29992;&#25143;&#25351;&#20196;&#12289;&#27169;&#22411;&#22238;&#24212;&#21644;&#23545;&#27169;&#22411;&#22238;&#24212;&#30340;&#20154;&#24037;&#35780;&#20272;&#12290;&#36890;&#36807;&#23545;&#25910;&#38598;&#21040;&#30340;&#22238;&#24212;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#38590;&#20197;&#23558;&#26032;&#20449;&#24687;&#25972;&#21512;&#21040;&#29616;&#26377;&#31572;&#26696;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03866v1 Announce Type: new  Abstract: Large language models (LLMs) adapted to follow user instructions are now widely deployed as conversational agents. In this work, we examine one increasingly common instruction-following task: providing writing assistance to compose a long-form answer. To evaluate the capabilities of current LLMs on this task, we construct KIWI, a dataset of knowledge-intensive writing instructions in the scientific domain. Given a research question, an initial model-generated answer and a set of relevant papers, an expert annotator iteratively issues instructions for the model to revise and improve its answer. We collect 1,260 interaction turns from 234 interaction sessions with three state-of-the-art LLMs. Each turn includes a user instruction, a model response, and a human evaluation of the model response. Through a detailed analysis of the collected responses, we find that all models struggle to incorporate new information into an existing answer, and
&lt;/p&gt;</description></item><item><title>X-Shot &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#25361;&#25112;&#65292;&#26088;&#22312;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#21516;&#26102;&#22788;&#29702;&#39057;&#32321;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#26631;&#31614;&#65292;&#31995;&#32479;&#38656;&#35201;&#33021;&#22815;&#36866;&#24212;&#20219;&#20309;&#26631;&#31614;&#20986;&#29616;&#39057;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.03863</link><description>&lt;p&gt;
X-Shot: &#19968;&#20010;&#32479;&#19968;&#30340;&#31995;&#32479;&#65292;&#21516;&#26102;&#22788;&#29702;&#20998;&#31867;&#20013;&#30340;&#39057;&#32321;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
X-Shot: A Unified System to Handle Frequent, Few-shot and Zero-shot Learning Simultaneously in Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03863
&lt;/p&gt;
&lt;p&gt;
X-Shot &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#25361;&#25112;&#65292;&#26088;&#22312;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#21516;&#26102;&#22788;&#29702;&#39057;&#32321;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#26631;&#31614;&#65292;&#31995;&#32479;&#38656;&#35201;&#33021;&#22815;&#36866;&#24212;&#20219;&#20309;&#26631;&#31614;&#20986;&#29616;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#23558;&#39057;&#32321;&#20986;&#29616;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#35270;&#20026;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#20165;&#20026;&#20854;&#20013;&#19968;&#31181;&#24773;&#20917;&#20248;&#21270;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#20013;&#65292;&#26631;&#31614;&#20986;&#29616;&#30340;&#39057;&#29575;&#24046;&#24322;&#24456;&#22823;&#12290;&#20026;&#20102;&#23454;&#29616;&#23454;&#38469;&#37096;&#32626;&#65292;&#20851;&#38190;&#22312;&#20110;&#31995;&#32479;&#33021;&#22815;&#36866;&#24212;&#20219;&#20309;&#26631;&#31614;&#20986;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#25361;&#25112;&#65306;X-shot&#65292;&#21453;&#26144;&#20102;&#19968;&#20010;&#23454;&#38469;&#29615;&#22659;&#65292;&#20854;&#20013;&#39057;&#32321;&#20986;&#29616;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#26631;&#31614;&#22312;&#27809;&#26377;&#39044;&#23450;&#20041;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#20986;&#29616;&#12290;&#22312;&#36825;&#37324;&#65292;X&#21487;&#20197;&#20174;0&#21040;&#27491;&#26080;&#31351;&#22823;&#12290;X-shot&#30340;&#20851;&#38190;&#22312;&#20110;&#24320;&#25918;&#39046;&#22495;&#27867;&#21270;&#21644;&#35774;&#35745;&#19968;&#20010;&#36275;&#22815;&#28789;&#27963;&#30340;&#31995;&#32479;&#26469;&#31649;&#29702;&#21508;&#31181;&#26631;&#31614;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03863v1 Announce Type: new  Abstract: In recent years, few-shot and zero-shot learning, which learn to predict labels with limited annotated instances, have garnered significant attention. Traditional approaches often treat frequent-shot (freq-shot; labels with abundant instances), few-shot, and zero-shot learning as distinct challenges, optimizing systems for just one of these scenarios. Yet, in real-world settings, label occurrences vary greatly. Some of them might appear thousands of times, while others might only appear sporadically or not at all. For practical deployment, it is crucial that a system can adapt to any label occurrence. We introduce a novel classification challenge: X-shot, reflecting a real-world context where freq-shot, few-shot, and zero-shot labels co-occur without predefined limits. Here, X can span from 0 to positive infinity. The crux of X-shot centers on open-domain generalization and devising a system versatile enough to manage various label scena
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31034;&#20363;&#19982;&#27979;&#35797;&#21477;&#23376;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#23545;&#40784;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.03861</link><description>&lt;p&gt;
&#20026;&#23569;&#26679;&#26412;&#31034;&#20363;&#36873;&#25321;&#35774;&#35745;&#20449;&#24687;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Designing Informative Metrics for Few-Shot Example Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03861
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31034;&#20363;&#19982;&#27979;&#35797;&#21477;&#23376;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#23545;&#40784;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#25552;&#20379;&#36866;&#24403;&#26684;&#24335;&#30340;&#31034;&#20363;&#26102;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#8220;&#26368;&#20339;&#8221;&#31034;&#20363;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#35757;&#32451;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#36873;&#25321;&#31034;&#20363;&#30340;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#29305;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#23545;&#40784;&#27979;&#35797;&#21477;&#23376;&#21644;&#31034;&#20363;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#21477;&#23376;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23558;&#31034;&#20363;&#30340;&#22797;&#26434;&#24230;&#19982;&#32771;&#34385;&#20013;&#30340;&#65288;&#27979;&#35797;&#65289;&#21477;&#23376;&#36827;&#34892;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;PLMs&#20013;&#25552;&#21462;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65306;&#22312;&#23569;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;CoNLL2003&#25968;&#25454;&#38598;&#19978;&#23545;GPT-4&#30340;F1&#20998;&#25968;&#23454;&#29616;&#20102;5%&#30340;&#32477;&#23545;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#22312;&#20687;GPT-j-6B&#36825;&#26679;&#30340;&#36739;&#23567;&#27169;&#22411;&#20013;&#30475;&#21040;&#20102;&#39640;&#36798;28.85&#20010;&#28857;&#65288;F1/Acc.&#65289;&#30340;&#26174;&#33879;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03861v1 Announce Type: new  Abstract: Pretrained language models (PLMs) have shown remarkable few-shot learning capabilities when provided with properly formatted examples. However, selecting the "best" examples remains an open challenge. We propose a complexity-based prompt selection approach for sequence tagging tasks. This approach avoids the training of a dedicated model for selection of examples, and instead uses certain metrics to align the syntactico-semantic complexity of test sentences and examples. We use both sentence- and word-level metrics to match the complexity of examples to the (test) sentence being considered. Our results demonstrate that our approach extracts greater performance from PLMs: it achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.
&lt;/p&gt;</description></item><item><title>Emojinize &#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#36866;&#24403;&#30340;&#34920;&#24773;&#31526;&#21495;&#32763;&#35793;&#25991;&#26412;&#65292;&#25552;&#39640;&#29468;&#27979;&#24615;&#65292;&#20154;&#31867;&#29468;&#27979;&#21487;&#36798;55&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.03857</link><description>&lt;p&gt;
Emojinize&#65306;&#29992;&#34920;&#24773;&#31526;&#21495;&#20016;&#23500;&#20219;&#20309;&#25991;&#26412;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Emojinize : Enriching Any Text with Emoji Translations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03857
&lt;/p&gt;
&lt;p&gt;
Emojinize &#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#36866;&#24403;&#30340;&#34920;&#24773;&#31526;&#21495;&#32763;&#35793;&#25991;&#26412;&#65292;&#25552;&#39640;&#29468;&#27979;&#24615;&#65292;&#20154;&#31867;&#29468;&#27979;&#21487;&#36798;55&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Emoji&#24050;&#32463;&#25104;&#20026;&#20070;&#38754;&#20132;&#27969;&#20013;&#26080;&#22788;&#19981;&#22312;&#30340;&#23384;&#22312;&#65292;&#22312;&#32593;&#32476;&#19978;&#21644;&#26356;&#36828;&#30340;&#22320;&#26041;&#12290;&#23427;&#20204;&#21487;&#20197;&#24378;&#35843;&#25110;&#28548;&#28165;&#24773;&#32490;&#65292;&#20026;&#23545;&#35805;&#22686;&#28155;&#32454;&#33410;&#65292;&#25110;&#32773;&#31616;&#21333;&#22320;&#36215;&#35013;&#39280;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#38543;&#24847;&#20351;&#29992;&#20165;&#20165;&#26159;&#34920;&#24773;&#31526;&#21495;&#34920;&#36798;&#33021;&#21147;&#30340;&#20912;&#23665;&#19968;&#35282;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#37322;&#25918;&#36825;&#31181;&#21147;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Emojinize&#65292;&#19968;&#31181;&#23558;&#20219;&#24847;&#25991;&#26412;&#30701;&#35821;&#32763;&#35793;&#25104;&#19968;&#20010;&#25110;&#22810;&#20010;&#34920;&#24773;&#31526;&#21495;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#36755;&#20837;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;Emojinize&#21487;&#20197;&#26681;&#25454;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#26495;&#29699;-&#29699;&#26834;vs&#34649;&#34656;&#65289;&#28040;&#38500;&#27495;&#20041;&#22320;&#36873;&#25321;&#36866;&#24403;&#30340;&#34920;&#24773;&#31526;&#21495;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#34920;&#24773;&#31526;&#21495;&#65288;&#20363;&#22914;&#65292;&#8220;Emojinize&#8221;&#34987;&#32763;&#35793;&#25104;&#36755;&#20837;-&#25289;&#19969;&#25991;&#23383;&#27597;-&#21491;&#31661;&#22836;-&#31505;&#33080;&#65289;&#26469;&#32452;&#21512;&#34920;&#36798;&#22797;&#26434;&#27010;&#24565;&#12290;&#22312;&#22522;&#20110;&#22635;&#31354;&#27979;&#35797;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;Emojinize&#30340;&#34920;&#24773;&#31526;&#21495;&#32763;&#35793;&#21487;&#20197;&#20351;&#25513;&#30422;&#30340;&#21333;&#35789;&#30340;&#29468;&#27979;&#24615;&#22686;&#21152;55&#65285;&#65292;&#32780;&#20154;&#31867;&#36873;&#25321;&#30340;&#34920;&#24773;&#31526;&#21495;&#32763;&#35793;&#21482;&#33021;&#22686;&#21152;29&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03857v1 Announce Type: new  Abstract: Emoji have become ubiquitous in written communication, on the Web and beyond. They can emphasize or clarify emotions, add details to conversations, or simply serve decorative purposes. This casual use, however, barely scratches the surface of the expressive power of emoji. To further unleash this power, we present Emojinize, a method for translating arbitrary text phrases into sequences of one or more emoji without requiring human input. By leveraging the power of large language models, Emojinize can choose appropriate emoji by disambiguating based on context (eg, cricket-bat vs bat) and can express complex concepts compositionally by combining multiple emoji (eq, ''Emojinize'' is translated to input-latin-letters right-arrow grinning-face). In a cloze test--based user study, we show that Emojinize's emoji translations increase the human guessability of masked words by 55%, whereas human-picked emoji translations do so by only 29%. These
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23618;&#32423;&#23384;&#22312;&#36739;&#39640;&#30456;&#20284;&#24615;&#65292;&#26377;&#20123;&#23618;&#23545;&#32593;&#32476;&#21151;&#33021;&#20960;&#20046;&#26080;&#24433;&#21709;&#12290;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#21306;&#22359;&#24433;&#21709;&#30340;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#23618;&#21024;&#38500;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#30340;&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03853</link><description>&lt;p&gt;
ShortGPT: &#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23618;&#32423;&#27604;&#24744;&#24819;&#35937;&#30340;&#26356;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
ShortGPT: Layers in Large Language Models are More Redundant Than You Expect
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03853
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23618;&#32423;&#23384;&#22312;&#36739;&#39640;&#30456;&#20284;&#24615;&#65292;&#26377;&#20123;&#23618;&#23545;&#32593;&#32476;&#21151;&#33021;&#20960;&#20046;&#26080;&#24433;&#21709;&#12290;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#21306;&#22359;&#24433;&#21709;&#30340;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#23618;&#21024;&#38500;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#30340;&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24615;&#33021;&#19978;&#19981;&#26029;&#21462;&#24471;&#36827;&#23637;&#65292;&#20854;&#35268;&#27169;&#26174;&#33879;&#22686;&#21152;&#65292;&#24403;&#21069;&#30340;LLMs&#21253;&#21547;&#25968;&#21313;&#20159;&#29978;&#33267;&#25968;&#19975;&#20159;&#20010;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35768;&#22810;LLMs&#30340;&#23618;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#19968;&#20123;&#23618;&#22312;&#32593;&#32476;&#21151;&#33021;&#20013;&#36215;&#21040;&#20102;&#21487;&#24573;&#30053;&#30340;&#20316;&#29992;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#31216;&#20026;&#21306;&#22359;&#24433;&#21709;&#65288;BI&#65289;&#30340;&#24230;&#37327;&#34913;&#37327;LLMs&#20013;&#27599;&#20010;&#23618;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#21098;&#26041;&#27861;&#65306;&#23618;&#21024;&#38500;&#65292;&#21363;&#26681;&#25454;&#23427;&#20204;&#30340;BI&#24471;&#20998;&#30452;&#25509;&#21024;&#38500;LLMs&#20013;&#30340;&#20887;&#20313;&#23618;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;ShortGPT&#22312;&#27169;&#22411;&#20462;&#21098;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20197;&#24448;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;ShortGPT&#19982;&#37327;&#21270;&#31561;&#26041;&#27861;&#27491;&#20132;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#21442;&#25968;&#21644;&#35745;&#31639;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#23618;&#21024;&#38500;&#21363;&#21487;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#30340;&#33021;&#21147;&#65292;&#19982;&#20256;&#32479;&#30340;&#31934;&#30830;&#20462;&#21098;&#26041;&#27861;&#25130;&#28982;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03853v1 Announce Type: new  Abstract: As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as oppo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#65292;&#21253;&#25324;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#12289;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#12289;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#12289;&#24635;&#32467;&#23545;&#35805;&#20197;&#21450;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#30340;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#35780;&#20215;&#25351;&#26631;PREFS&#12290;</title><link>https://arxiv.org/abs/2403.03823</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Modular Approach for Multimodal Summarization of TV Shows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03823
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#65292;&#21253;&#25324;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#12289;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#12289;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#12289;&#24635;&#32467;&#23545;&#35805;&#20197;&#21450;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#30340;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#35780;&#20215;&#25351;&#26631;PREFS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#21040;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65306;&#22797;&#26434;&#25512;&#29702;&#12289;&#22810;&#27169;&#24577;&#21644;&#38271;&#31687;&#21465;&#20107;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#21508;&#20010;&#32452;&#20214;&#25191;&#34892;&#19987;&#38376;&#30340;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#35748;&#20026;&#19982;&#31471;&#21040;&#31471;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#28041;&#21450;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#65292;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#20197;&#23613;&#37327;&#20943;&#23569;&#19981;&#21516;&#20107;&#20214;&#20043;&#38388;&#30340;&#20999;&#25442;&#27425;&#25968;&#65292;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#24635;&#32467;&#27599;&#20010;&#22330;&#26223;&#20013;&#30340;&#23545;&#35805;&#65292;&#24182;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#25104;&#25972;&#38598;&#30340;&#26368;&#32456;&#25688;&#35201;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;PREFS&#65288;&#25688;&#35201;&#20107;&#23454;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35780;&#20272;&#65289;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#25688;&#35201;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#35299;&#20026;&#21407;&#23376;&#20107;&#23454;&#12290;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;SummScreen3D&#25968;&#25454;&#38598;Papalampidi&#21644;Lapata&#65288;2023&#65289;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03823v1 Announce Type: new  Abstract: In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, PREFS (\textbf{P}recision and \textbf{R}ecall \textbf{E}valuation of Summary \textbf{F}act\textbf{s}), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset Papalampidi and Lapata (2023), our method produces hi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MultiQ&#22522;&#20934;&#65292;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;LLMs&#22312;&#20854;&#39044;&#26399;&#20351;&#29992;&#33539;&#22260;&#20043;&#22806;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#33267;&#23569;&#26576;&#20123;&#35821;&#35328;&#33021;&#22815;&#24544;&#23454;&#21644;&#20934;&#30830;&#22320;&#36827;&#34892;&#22238;&#31572;&#12290;</title><link>https://arxiv.org/abs/2403.03814</link><description>&lt;p&gt;
&#29992;MultiQ&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MultiQ&#22522;&#20934;&#65292;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;LLMs&#22312;&#20854;&#39044;&#26399;&#20351;&#29992;&#33539;&#22260;&#20043;&#22806;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#33267;&#23569;&#26576;&#20123;&#35821;&#35328;&#33021;&#22815;&#24544;&#23454;&#21644;&#20934;&#30830;&#22320;&#36827;&#34892;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#20026;&#20840;&#29699;&#22823;&#22810;&#25968;&#38750;&#33521;&#35821;&#20351;&#29992;&#32773;&#25552;&#20379;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;LLMs&#20170;&#22825;&#65292;&#29305;&#21035;&#26159;&#24320;&#25918;&#30340;LLMs&#65292;&#36890;&#24120;&#20165;&#24847;&#20026;&#22312;&#33521;&#35821;&#65288;&#20363;&#22914;Llama2&#12289;Mistral&#65289;&#25110;&#23569;&#25968;&#20960;&#31181;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;&#20363;&#22914;Mixtral&#12289;Qwen&#65289;&#20013;&#20351;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#23384;&#22312;&#20351;&#29992;&#19978;&#30340;&#38480;&#21046;&#65292;&#20154;&#20204;&#20250;&#29992;&#35768;&#22810;&#19981;&#21516;&#30340;&#35821;&#35328;&#25552;&#31034;LLMs&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;LLMs&#22312;&#20854;&#39044;&#26399;&#20351;&#29992;&#33539;&#22260;&#20043;&#22806;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MultiQ&#65292;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#22522;&#26412;&#24320;&#25918;&#24335;&#38382;&#31572;&#30340;&#38134;&#26631;&#20934;&#22522;&#20934;&#65292;&#28085;&#30422;137&#31181;&#35821;&#35328;&#30340;27.4k&#20010;&#27979;&#35797;&#38382;&#39064;&#12290;&#36890;&#36807;MultiQ&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35821;&#35328;&#24544;&#23454;&#24230;&#65292;&#21363;&#27169;&#22411;&#26159;&#21542;&#20197;&#25552;&#31034;&#30340;&#35821;&#35328;&#22238;&#22797;&#65292;&#20197;&#21450;&#38382;&#39064;&#22238;&#31572;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#27979;&#35797;&#30340;&#25152;&#26377;LLMs&#23545;&#33267;&#23569;&#26576;&#20123;&#35821;&#35328;&#21709;&#24212;&#24471;&#24544;&#23454;&#21644;/&#25110;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03814v1 Announce Type: cross  Abstract: Large language models (LLMs) need to serve everyone, including a global majority of non-English speakers. However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent research shows that, despite limits in their intended use, people prompt LLMs in many different languages. Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open LLMs beyond their intended use. For this purpose, we introduce MultiQ, a new silver standard benchmark for basic open-ended question answering with 27.4k test questions across a typologically diverse set of 137 languages. With MultiQ, we evaluate language fidelity, i.e.\ whether models respond in the prompted language, and question answering accuracy. All LLMs we test respond faithfully and/or accurately for at least some languages be
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PowerPoint&#20219;&#21153;&#23436;&#25104;&#40065;&#26834;&#24615;&#22522;&#20934;&#65288;PPTC-R&#65289;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#29992;&#25143;PPT&#20219;&#21153;&#25351;&#20196;&#21644;&#36719;&#20214;&#29256;&#26412;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;GPT-4&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2403.03788</link><description>&lt;p&gt;
PPTC-R&#22522;&#20934;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;PowerPoint&#20219;&#21153;&#23436;&#25104;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03788
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PowerPoint&#20219;&#21153;&#23436;&#25104;&#40065;&#26834;&#24615;&#22522;&#20934;&#65288;PPTC-R&#65289;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#29992;&#25143;PPT&#20219;&#21153;&#25351;&#20196;&#21644;&#36719;&#20214;&#29256;&#26412;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;GPT-4&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#26041;&#38754;&#30340;&#26085;&#30410;&#20381;&#36182;&#65292;&#26377;&#24517;&#35201;&#20840;&#38754;&#20102;&#35299;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#20219;&#21153;&#23436;&#25104;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PowerPoint&#20219;&#21153;&#23436;&#25104;&#40065;&#26834;&#24615;&#22522;&#20934;&#65288;PPTC-R&#65289;&#65292;&#20197;&#34913;&#37327;LLMs&#23545;&#29992;&#25143;PPT&#20219;&#21153;&#25351;&#20196;&#21644;&#36719;&#20214;&#29256;&#26412;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#21477;&#23376;&#12289;&#35821;&#20041;&#21644;&#22810;&#35821;&#35328;&#32423;&#21035;&#25915;&#20987;&#29992;&#25143;&#25351;&#20196;&#26469;&#26500;&#24314;&#23545;&#25239;&#24615;&#29992;&#25143;&#25351;&#20196;&#12290;&#20026;&#20102;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#36719;&#20214;&#29256;&#26412;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25913;&#21464;&#25552;&#20379;&#30340;API&#25968;&#37327;&#20197;&#27169;&#25311;&#26368;&#26032;&#29256;&#26412;&#21644;&#26089;&#26399;&#29256;&#26412;&#35774;&#32622;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#36825;&#20123;&#40065;&#26834;&#24615;&#35774;&#32622;&#30340;&#22522;&#20934;&#27979;&#35797;3&#20010;&#38381;&#28304;&#21644;4&#20010;&#24320;&#28304;LLMs&#65292;&#26088;&#22312;&#35780;&#20272;&#20559;&#31163;&#22914;&#20309;&#24433;&#21709;LLMs&#30340;API&#35843;&#29992;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03788v1 Announce Type: new  Abstract: The growing dependence on Large Language Models (LLMs) for finishing user instructions necessitates a comprehensive understanding of their robustness to complex task completion in real-world situations. To address this critical need, we propose the PowerPoint Task Completion Robustness benchmark (PPTC-R) to measure LLMs' robustness to the user PPT task instruction and software version. Specifically, we construct adversarial user instructions by attacking user instructions at sentence, semantic, and multi-language levels. To assess the robustness of Language Models to software versions, we vary the number of provided APIs to simulate both the newest version and earlier version settings. Subsequently, we test 3 closed-source and 4 open-source LLMs using a benchmark that incorporates these robustness settings, aiming to evaluate how deviations impact LLMs' API calls for task completion. We find that GPT-4 exhibits the highest performance an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#26032;&#38395;&#25688;&#35201;&#20013;&#24187;&#35273;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;absinth&#65292;&#25506;&#35752;&#20102;LLMs&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.03750</link><description>&lt;p&gt;
&#24503;&#35821;&#20063;&#20135;&#29983;&#24187;&#35273;&#65281;&#20351;&#29992;Absinth&#25968;&#25454;&#38598;&#26816;&#27979;&#26032;&#38395;&#25688;&#35201;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#26032;&#38395;&#25688;&#35201;&#20013;&#24187;&#35273;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;absinth&#65292;&#25506;&#35752;&#20102;LLMs&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#20135;&#29983;&#20449;&#24687;&#24187;&#35273;&#30340;&#38382;&#39064;&#65292;&#36825;&#22312;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#22810;&#35821;&#35328;&#26041;&#27861;&#32570;&#20047;&#24503;&#35821;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;absinth&#65292;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#26032;&#38395;&#25688;&#35201;&#20013;&#24187;&#35273;&#26816;&#27979;&#30340;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#35752;&#20102;&#26032;&#22411;&#24320;&#28304;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#35774;&#32622;&#12290;&#25105;&#20204;&#24320;&#28304;&#24182;&#21457;&#24067;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03750v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has led to remarkable progress on a wide range of natural language processing tasks. Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document. Previous research addresses the challenging task of detecting hallucinations in the output (i.e. inconsistency detection) in order to evaluate the faithfulness of the generated summaries. However, these works primarily focus on English and recent multilingual approaches lack German data. This work presents absinth, a manually annotated dataset for hallucination detection in German news summarization and explores the capabilities of novel open-source LLMs on this task in both fine-tuning and in-context learning settings. We open-source and releas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Transformer-Representation&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;TNTM&#65289;&#65292;&#32467;&#21512;&#20102;transformer&#23884;&#20837;&#31354;&#38388;&#20013;&#20027;&#39064;&#34920;&#31034;&#30340;&#20248;&#21183;&#21644;&#27010;&#29575;&#24314;&#27169;&#20197;&#21450;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20027;&#39064;&#24314;&#27169;&#30340;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.03737</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#34920;&#31034;&#30340;&#27010;&#29575;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Topic Modelling with Transformer Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03737
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Transformer-Representation&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;TNTM&#65289;&#65292;&#32467;&#21512;&#20102;transformer&#23884;&#20837;&#31354;&#38388;&#20013;&#20027;&#39064;&#34920;&#31034;&#30340;&#20248;&#21183;&#21644;&#27010;&#29575;&#24314;&#27169;&#20197;&#21450;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20027;&#39064;&#24314;&#27169;&#30340;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#22823;&#22810;&#30001;&#36125;&#21494;&#26031;&#22270;&#27169;&#22411;&#20027;&#23548;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20852;&#36215;&#65292;&#19968;&#20123;&#20381;&#36182;&#20110;transformer&#23884;&#20837;&#31354;&#38388;&#20013;&#31616;&#21333;&#32858;&#31867;&#26041;&#27861;&#30340;&#25104;&#21151;&#27169;&#22411;&#24050;&#32463;&#20986;&#29616;&#24182;&#24041;&#22266;&#20102;&#20027;&#39064;&#20316;&#20026;&#23884;&#20837;&#21521;&#37327;&#32858;&#31867;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Transformer-Representation&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;TNTM&#65289;&#65292;&#32467;&#21512;&#20102;transformer&#23884;&#20837;&#31354;&#38388;&#20013;&#20027;&#39064;&#34920;&#31034;&#30340;&#20248;&#21183;&#21644;&#27010;&#29575;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#22522;&#20110;transformer&#23884;&#20837;&#30340;&#24378;&#22823;&#22810;&#21151;&#33021;&#20027;&#39064;&#27010;&#24565;&#19982;&#23436;&#20840;&#27010;&#29575;&#24314;&#27169;&#32479;&#19968;&#36215;&#26469;&#65292;&#22914;Latent Dirichlet Allocation&#65288;LDA&#65289;&#31561;&#27169;&#22411;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26694;&#26550;&#25913;&#36827;&#25512;&#29702;&#36895;&#24230;&#21644;&#24314;&#27169;&#28789;&#27963;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#21508;&#31181;&#27169;&#22411;&#36798;&#21040;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03737v1 Announce Type: cross  Abstract: Topic modelling was mostly dominated by Bayesian graphical models during the last decade. With the rise of transformers in Natural Language Processing, however, several successful models that rely on straightforward clustering approaches in transformer-based embedding spaces have emerged and consolidated the notion of topics as clusters of embedding vectors. We propose the Transformer-Representation Neural Topic Model (TNTM), which combines the benefits of topic representations in transformer-based embedding spaces and probabilistic modelling. Therefore, this approach unifies the powerful and versatile notion of topics based on transformer embeddings with fully probabilistic modelling, as in models such as Latent Dirichlet Allocation (LDA). We utilize the variational autoencoder (VAE) framework for improved inference speed and modelling flexibility. Experimental results show that our proposed model achieves results on par with various 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;GPT-4&#33258;&#25351;&#23548;&#26041;&#27861;&#65292;&#24555;&#36895;&#24320;&#21457;&#39640;&#36136;&#37327;&#30340;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#26080;&#38656;&#22823;&#37327;&#20154;&#21147;&#25237;&#20837;&#65292;&#24182;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#36164;&#28304;</title><link>https://arxiv.org/abs/2403.03690</link><description>&lt;p&gt;
&#24555;&#36895;&#24320;&#21457;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#20943;&#23569;&#20154;&#21147;&#25237;&#20837;&#65306;&#20197;&#26085;&#35821;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03690
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;GPT-4&#33258;&#25351;&#23548;&#26041;&#27861;&#65292;&#24555;&#36895;&#24320;&#21457;&#39640;&#36136;&#37327;&#30340;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#26080;&#38656;&#22823;&#37327;&#20154;&#21147;&#25237;&#20837;&#65292;&#24182;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#65292;&#21019;&#24314;&#25351;&#20196;&#25968;&#25454;&#21644;&#35780;&#20272;&#22522;&#20934;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#12290;&#24403;&#20026;&#26085;&#35821;&#31561;&#38750;&#33521;&#35821;&#35821;&#35328;&#24555;&#36895;&#24320;&#21457;&#36825;&#20123;&#36164;&#28304;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-4&#30340;&#39640;&#25928;&#33258;&#25351;&#23548;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#23558;&#29616;&#26377;&#30340;&#33521;&#35821;&#36164;&#28304;&#32763;&#35793;&#25104;&#26085;&#35821;&#65288;&#20363;&#22914;Japanese-Alpaca&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#23569;&#37327;&#33521;&#35821;&#25351;&#20196;&#32763;&#35793;&#25104;&#26085;&#35821;&#65292;&#24182;&#36827;&#34892;&#21518;&#26399;&#32534;&#36753;&#20197;&#33719;&#24471;native-level&#36136;&#37327;&#12290;&#28982;&#21518;&#65292;GPT-4&#21033;&#29992;&#36825;&#20123;&#25351;&#20196;&#20316;&#20026;&#31034;&#33539;&#65292;&#33258;&#21160;&#29983;&#25104;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;GPT-4&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;80&#20010;&#38382;&#39064;&#36328;8&#20010;&#31867;&#21035;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#20351;&#29992;GPT-4&#33258;&#21160;&#35780;&#20272;LLMs&#30340;&#21709;&#24212;&#36136;&#37327;&#65292;&#26080;&#38656;&#20154;&#24037;&#21442;&#32771;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#25105;&#20204;&#30340;GPT-4&#33258;&#25351;&#23548;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03690v1 Announce Type: cross  Abstract: The creation of instruction data and evaluation benchmarks for serving Large language models often involves enormous human annotation. This issue becomes particularly pronounced when rapidly developing such resources for a non-English language like Japanese. Instead of following the popular practice of directly translating existing English resources into Japanese (e.g., Japanese-Alpaca), we propose an efficient self-instruct method based on GPT-4. We first translate a small amount of English instructions into Japanese and post-edit them to obtain native-level quality. GPT-4 then utilizes them as demonstrations to automatically generate Japanese instruction data. We also construct an evaluation benchmark containing 80 questions across 8 categories, using GPT-4 to automatically assess the response quality of LLMs without human references. The empirical results suggest that the models fine-tuned on our GPT-4 self-instruct data significant
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;G2ST&#30340;&#20004;&#27493;&#24494;&#35843;&#33539;&#24335;&#65292;&#36890;&#36807;&#33258;&#23545;&#27604;&#35821;&#20041;&#22686;&#24378;&#23558;&#36890;&#29992;NMT&#27169;&#22411;&#36716;&#25442;&#20026;&#19987;&#38376;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;NMT&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03689</link><description>&lt;p&gt;
&#36890;&#29992;&#21040;&#19987;&#19994;&#30340;&#30005;&#23376;&#21830;&#21153;LLMs&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
General2Specialized LLMs Translation for E-commerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;G2ST&#30340;&#20004;&#27493;&#24494;&#35843;&#33539;&#24335;&#65292;&#36890;&#36807;&#33258;&#23545;&#27604;&#35821;&#20041;&#22686;&#24378;&#23558;&#36890;&#29992;NMT&#27169;&#22411;&#36716;&#25442;&#20026;&#19987;&#38376;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;NMT&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#20027;&#35201;&#22788;&#29702;&#36890;&#29992;&#39046;&#22495;&#30340;&#32763;&#35793;&#65292;&#24573;&#30053;&#20102;&#20855;&#26377;&#29305;&#27530;&#20889;&#20316;&#20844;&#24335;&#30340;&#39046;&#22495;&#65292;&#27604;&#22914;&#30005;&#23376;&#21830;&#21153;&#21644;&#27861;&#24459;&#25991;&#20214;&#12290;&#20197;&#30005;&#23376;&#21830;&#21153;&#20026;&#20363;&#65292;&#25991;&#26412;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#39046;&#22495;&#30456;&#20851;&#35789;&#27719;&#65292;&#24182;&#19988;&#23384;&#22312;&#26356;&#22810;&#30340;&#35821;&#27861;&#38382;&#39064;&#65292;&#36825;&#23548;&#33268;&#24403;&#21069;NMT&#26041;&#27861;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20004;&#20010;&#19982;&#39046;&#22495;&#30456;&#20851;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#19968;&#32452;&#26415;&#35821;&#23545;&#65288;&#23545;&#40784;&#30340;&#20013;&#33521;&#21452;&#35821;&#26415;&#35821;&#65289;&#21644;&#19968;&#20010;&#38024;&#23545;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#36827;&#34892;&#27880;&#37322;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#24494;&#35843;&#33539;&#24335;&#65288;&#21517;&#20026;G2ST&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#23545;&#27604;&#35821;&#20041;&#22686;&#24378;&#65292;&#20197;&#23558;&#19968;&#20010;&#36890;&#29992;NMT&#27169;&#22411;&#36716;&#25442;&#20026;&#19987;&#38376;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;NMT&#27169;&#22411;&#12290;&#35813;&#33539;&#24335;&#36866;&#29992;&#20110;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;NMT&#27169;&#22411;&#12290;&#23545;&#30495;&#23454;&#30005;&#23376;&#21830;&#21153;&#26631;&#39064;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#20102;&#21331;&#36234;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03689v1 Announce Type: cross  Abstract: Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents. Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods. To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain. Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce. The paradigm can be used for the NMT models based on Large language models (LLMs). Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and 
&lt;/p&gt;</description></item><item><title>Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03640</link><description>&lt;p&gt;
Apollo&#65306;&#36731;&#37327;&#32423;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65306;&#35753;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#26222;&#24800;60&#20159;&#20154;
&lt;/p&gt;
&lt;p&gt;
Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03640
&lt;/p&gt;
&lt;p&gt;
Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20840;&#29699;&#21307;&#23398;&#30693;&#35782;&#30340;&#24222;&#22823;&#23384;&#20648;&#24211;&#20027;&#35201;&#26159;&#20197;&#33521;&#35821;&#20026;&#20027;&#65292;&#20294;&#22312;&#20256;&#36882;&#37327;&#36523;&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#26041;&#38754;&#65292;&#26412;&#22320;&#35821;&#35328;&#23545;&#20110;&#22312;&#21307;&#30103;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#23558;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#20154;&#32676;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#28085;&#30422;&#20840;&#29699;61&#20159;&#20154;&#21475;&#30340;&#20845;&#31181;&#26368;&#24120;&#29992;&#35821;&#35328;&#30340;&#21307;&#23398;LLMs&#12290;&#36825;&#19968;&#21162;&#21147;&#26368;&#32456;&#20419;&#25104;&#20102;ApolloCorpora&#22810;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#38598;&#21644;XMedBench&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#22312;&#22810;&#35821;&#35328;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21457;&#24067;&#30340;Apollo&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#30456;&#23545;&#36739;&#23567;&#23610;&#23544;&#65288;&#21363;0.5B&#12289;1.8B&#12289;2B&#12289;6B&#21644;7B&#65289;&#19978;&#21462;&#24471;&#20102;&#19982;&#21516;&#31561;&#22823;&#23567;&#27169;&#22411;&#26368;&#20339;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;Apollo-7B&#26159;&#36804;&#20170;&#20026;&#27490;&#36798;&#21040;70B&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#36739;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03640v1 Announce Type: cross  Abstract: Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;GPTopic&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#21160;&#24577;&#12289;&#20132;&#20114;&#24335;&#20027;&#39064;&#34920;&#31034;&#30340;&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#30452;&#35266;&#30340;&#32842;&#22825;&#30028;&#38754;&#20351;&#20027;&#39064;&#24314;&#27169;&#26356;&#26131;&#29992;&#21644;&#20840;&#38754;&#12290;</title><link>https://arxiv.org/abs/2403.03628</link><description>&lt;p&gt;
&#21160;&#24577;&#20132;&#20114;&#24335;&#20027;&#39064;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
GPTopic: Dynamic and Interactive Topic Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03628
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;GPTopic&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#21160;&#24577;&#12289;&#20132;&#20114;&#24335;&#20027;&#39064;&#34920;&#31034;&#30340;&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#30452;&#35266;&#30340;&#32842;&#22825;&#30028;&#38754;&#20351;&#20027;&#39064;&#24314;&#27169;&#26356;&#26131;&#29992;&#21644;&#20840;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#20284;&#20046;&#20960;&#20046;&#31561;&#21516;&#20110;&#29983;&#25104;&#21015;&#20986;&#39030;&#37096;&#21333;&#35789;&#20197;&#34920;&#31034;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#20174;&#36825;&#26679;&#19968;&#21015;&#20010;&#21035;&#26415;&#35821;&#20013;&#25512;&#26029;&#20027;&#39064;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#32463;&#39564;&#65292;&#20351;&#24471;&#20027;&#39064;&#24314;&#27169;&#23545;&#20110;&#23545;&#29305;&#23450;&#39030;&#35789;&#35299;&#37322;&#30340;&#32454;&#24494;&#24046;&#21035;&#21644;&#32570;&#38519;&#19981;&#29087;&#24713;&#30340;&#20154;&#32676;&#19981;&#22815;&#26131;&#29992;&#12290;&#20165;&#38480;&#20110;&#39030;&#35789;&#30340;&#20027;&#39064;&#34920;&#31034;&#21487;&#33021;&#36827;&#19968;&#27493;&#26080;&#27861;&#25552;&#20379;&#20027;&#39064;&#21487;&#33021;&#20855;&#26377;&#30340;&#21508;&#20010;&#26041;&#38754;&#12289;&#26041;&#38754;&#21644;&#32454;&#24494;&#24046;&#21035;&#30340;&#20840;&#38754;&#19988;&#26131;&#20110;&#35775;&#38382;&#30340;&#34920;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GPTopic&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21019;&#24314;&#21160;&#24577;&#65292;&#20132;&#20114;&#24335;&#20027;&#39064;&#34920;&#31034;&#30340;&#36719;&#20214;&#21253;&#12290;GPTopic&#25552;&#20379;&#30452;&#35266;&#30340;&#32842;&#22825;&#30028;&#38754;&#65292;&#20379;&#29992;&#25143;&#20132;&#20114;&#22320;&#25506;&#32034;&#12289;&#20998;&#26512;&#21644;&#23436;&#21892;&#20027;&#39064;&#65292;&#20351;&#20027;&#39064;&#24314;&#27169;&#26356;&#26131;&#20351;&#29992;&#21644;&#20840;&#38754;&#12290;&#30456;&#24212;&#30340;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#25214;&#21040;&#65306;https://gith
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03628v1 Announce Type: new  Abstract: Topic modeling seems to be almost synonymous with generating lists of top words to represent topics within large text corpora. However, deducing a topic from such list of individual terms can require substantial expertise and experience, making topic modelling less accessible to people unfamiliar with the particularities and pitfalls of top-word interpretation. A topic representation limited to top-words might further fall short of offering a comprehensive and easily accessible characterization of the various aspects, facets and nuances a topic might have. To address these challenges, we introduce GPTopic, a software package that leverages Large Language Models (LLMs) to create dynamic, interactive topic representations. GPTopic provides an intuitive chat interface for users to explore, analyze, and refine topics interactively, making topic modeling more accessible and comprehensive. The corresponding code is available here: https://gith
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#35299;&#37322;&#24694;&#24847;&#21644;&#35823;&#23548;&#24615;&#22768;&#26126;&#30340;&#19981;&#21512;&#29702;&#20043;&#22788;&#21644;&#28508;&#22312;&#21160;&#26426;&#12290;</title><link>https://arxiv.org/abs/2403.03627</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models to Support Real-World Fact-Checking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03627
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#35299;&#37322;&#24694;&#24847;&#21644;&#35823;&#23548;&#24615;&#22768;&#26126;&#30340;&#19981;&#21512;&#29702;&#20043;&#22788;&#21644;&#28508;&#22312;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20855;&#26377;&#28508;&#21147;&#25903;&#25345;&#20154;&#31867;&#22788;&#29702;&#22823;&#37327;&#20449;&#24687;&#12290;&#34429;&#28982;MLLMs&#24050;&#32463;&#34987;&#29992;&#20316;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#65292;&#20294;&#23601;&#20854;&#22312;&#27492;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#32780;&#35328;&#65292;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#31995;&#32479;&#35780;&#20272;&#24403;&#21069;&#22810;&#27169;&#24577;&#27169;&#22411;&#20419;&#36827;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#26159;&#26080;&#38656;&#35777;&#25454;&#30340;&#65292;&#20165;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#22266;&#26377;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#35774;&#35745;&#33021;&#22815;&#25552;&#21462;&#27169;&#22411;&#39044;&#27979;&#12289;&#35299;&#37322;&#21644;&#32622;&#20449;&#27700;&#24179;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20851;&#20110;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#22833;&#36133;&#21407;&#22240;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#65292;(1) GPT-4V&#22312;&#35782;&#21035;&#24694;&#24847;&#21644;&#35823;&#23548;&#24615;&#22810;&#27169;&#24577;&#22768;&#26126;&#26041;&#38754;&#34920;&#29616;&#20986;&#36229;&#20961;&#24615;&#33021;&#65292;&#33021;&#22815;&#35299;&#37322;&#19981;&#21512;&#29702;&#30340;&#26041;&#38754;&#21644;&#28508;&#22312;&#21160;&#26426;&#65292;&#20197;&#21450;(2)&#29616;&#26377;&#30340;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03627v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) carry the potential to support humans in processing vast amounts of information. While MLLMs are already being used as a fact-checking tool, their abilities and limitations in this regard are understudied. Here is aim to bridge this gap. In particular, we propose a framework for systematically assessing the capacity of current multimodal models to facilitate real-world fact-checking. Our methodology is evidence-free, leveraging only these models' intrinsic knowledge and reasoning capabilities. By designing prompts that extract models' predictions, explanations, and confidence levels, we delve into research questions concerning model accuracy, robustness, and reasons for failure. We empirically find that (1) GPT-4V exhibits superior performance in identifying malicious and misleading multimodal claims, with the ability to explain the unreasonable aspects and underlying motives, and (2) existing o
&lt;/p&gt;</description></item><item><title>&#35813;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207; adaptNMT &#31616;&#21270;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#25552;&#20379;&#20102;&#22270;&#24418;&#21270;&#35757;&#32451;&#36827;&#23637;&#23637;&#31034;&#12289;&#23376;&#35789;&#20998;&#21106;&#27169;&#22411;&#21644;&#19968;&#38190;&#24335;&#27169;&#22411;&#24320;&#21457;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03582</link><description>&lt;p&gt;
&#20026;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#35774;&#35745;&#30340;&#24320;&#28304;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Design of an Open-Source Architecture for Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207; adaptNMT &#31616;&#21270;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#25552;&#20379;&#20102;&#22270;&#24418;&#21270;&#35757;&#32451;&#36827;&#23637;&#23637;&#31034;&#12289;&#23376;&#35789;&#20998;&#21106;&#27169;&#22411;&#21644;&#19968;&#38190;&#24335;&#27169;&#22411;&#24320;&#21457;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03582v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; adaptNMT &#26159;&#19968;&#20010;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#21644;&#37096;&#32626;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#26500;&#24314;&#22312;&#24191;&#27867;&#37319;&#29992;&#30340;OpenNMT&#29983;&#24577;&#31995;&#32479;&#20043;&#19978;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#35813;&#39046;&#22495;&#30340;&#26032;&#36827;&#20837;&#32773;&#65292;&#22240;&#20026;&#23427;&#31616;&#21270;&#20102;&#24320;&#21457;&#29615;&#22659;&#30340;&#35774;&#32622;&#20197;&#21450;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#19968;&#20010;&#22270;&#24418;&#21270;&#21151;&#33021;&#65292;&#29992;&#20110;&#23637;&#31034;&#27169;&#22411;&#35757;&#32451;&#30340;&#36827;&#23637;&#65292;&#24182;&#37319;&#29992;SentencePiece&#26469;&#21019;&#24314;&#23376;&#35789;&#20998;&#21106;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#31616;&#21270;&#20102;&#36229;&#21442;&#25968;&#30340;&#23450;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#24212;&#29992;&#23454;&#29616;&#20102;&#19968;&#38190;&#24335;&#27169;&#22411;&#24320;&#21457;&#26041;&#27861;&#65292;&#36890;&#36807;adaptNMT&#24320;&#21457;&#30340;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#40723;&#21169;&#29615;&#20445;&#30740;&#31350;&#65292;adaptNMT&#38598;&#25104;&#20102;&#19968;&#20010;&#29615;&#20445;&#25253;&#21578;&#65292;&#26631;&#24535;&#30528;&#33021;&#28304;&#28040;&#32791;&#21644;&#20108;&#27687;&#21270;&#30899;&#25490;&#25918;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03582v1 Announce Type: cross  Abstract: adaptNMT is an open-source application that offers a streamlined approach to the development and deployment of Recurrent Neural Networks and Transformer models. This application is built upon the widely-adopted OpenNMT ecosystem, and is particularly useful for new entrants to the field, as it simplifies the setup of the development environment and creation of train, validation, and test splits. The application offers a graphing feature that illustrates the progress of model training, and employs SentencePiece for creating subword segmentation models. Furthermore, the application provides an intuitive user interface that facilitates hyperparameter customization. Notably, a single-click model development approach has been implemented, and models developed by adaptNMT can be evaluated using a range of metrics. To encourage eco-friendly research, adaptNMT incorporates a green report that flags the power consumption and kgCO${_2}$ emissions
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;ASD&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#23588;&#20854;&#22312;&#20799;&#31461;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.03581</link><description>&lt;p&gt;
&#25552;&#39640;ASD&#26816;&#27979;&#20934;&#30830;&#24615;&#65306;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#32508;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing ASD detection accuracy: a combined approach of machine learning and deep learning models with natural language processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03581
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;ASD&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#23588;&#20854;&#22312;&#20799;&#31461;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03581v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#30446;&#30340;&#65306;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26469;&#35786;&#26029;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#12290;&#23427;&#19987;&#27880;&#20110;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#20174;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#25991;&#26412;&#36755;&#20837;&#20013;&#26816;&#27979;ASD&#65292;&#35299;&#20915;&#20256;&#32479;ASD&#35786;&#26029;&#20013;&#30340;&#25361;&#25112;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;ML&#21644;DL&#27169;&#22411;&#65288;&#21253;&#25324;&#20915;&#31574;&#26641;&#12289;XGB&#12289;KNN&#12289;RNN&#12289;LSTM&#12289;Bi-LSTM&#12289;BERT&#21644;BERTweet&#65289;&#20998;&#26512;&#20102;404,627&#26465;tweets&#65292;&#26681;&#25454;ASD&#25110;&#38750;ASD&#20316;&#32773;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#12290;&#20854;&#20013;90,000&#26465;tweets&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;AI&#27169;&#22411;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#25104;&#21151;&#29575;&#36798;&#21040;88&#65285;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#26469;&#33258;ASD&#24739;&#32773;&#30340;&#25991;&#26412;&#12290;&#32467;&#35770;&#65306;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;AI&#22312;&#25913;&#21892;ASD&#35786;&#26029;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#20799;&#31461;&#20013;&#65292;&#31361;&#26174;&#20102;&#26089;&#26399;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03581v1 Announce Type: new  Abstract: Purpose: Our study explored the use of artificial intelligence (AI) to diagnose autism spectrum disorder (ASD). It focused on machine learning (ML) and deep learning (DL) to detect ASD from text inputs on social media, addressing challenges in traditional ASD diagnosis.   Methods: We used natural language processing (NLP), ML, and DL models (including decision trees, XGB, KNN, RNN, LSTM, Bi-LSTM, BERT, and BERTweet) to analyze 404,627 tweets, classifying them based on ASD or non-ASD authors. A subset of 90,000 tweets was used for model training and testing.   Results: Our AI models showed high accuracy, with an 88% success rate in identifying texts from individuals with ASD.   Conclusion: The study demonstrates AI's potential in improving ASD diagnosis, especially in children, highlighting the importance of early detection.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20221;&#29992;&#20110;&#20302;&#36164;&#28304;&#33521;&#35821;&#21040;&#29233;&#23572;&#20848;&#35821;&#35328;&#23545;&#30340;&#29305;&#23450;&#20581;&#24247;&#39046;&#22495;&#25968;&#25454;&#38598;&#65292;&#23454;&#35777;&#35777;&#26126;&#20351;&#29992;&#39046;&#22495;&#25968;&#25454;&#23545;&#20581;&#24247;&#39046;&#22495;&#20855;&#26377;&#26126;&#26174;&#22909;&#22788;&#65292;&#24182;&#23637;&#31034;&#30340;&#26368;&#22823;BLEU&#20998;&#25968;&#25552;&#21319;&#20026;22.2&#28857;&#65288;40%&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.03575</link><description>&lt;p&gt;
gaHealth: &#19968;&#20221;&#33521;&#35821;-&#29233;&#23572;&#20848;&#29233;&#23572;&#20848;&#21452;&#35821;&#20581;&#24247;&#25968;&#25454;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
gaHealth: An English-Irish Bilingual Corpus of Health Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03575
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20221;&#29992;&#20110;&#20302;&#36164;&#28304;&#33521;&#35821;&#21040;&#29233;&#23572;&#20848;&#35821;&#35328;&#23545;&#30340;&#29305;&#23450;&#20581;&#24247;&#39046;&#22495;&#25968;&#25454;&#38598;&#65292;&#23454;&#35777;&#35777;&#26126;&#20351;&#29992;&#39046;&#22495;&#25968;&#25454;&#23545;&#20581;&#24247;&#39046;&#22495;&#20855;&#26377;&#26126;&#26174;&#22909;&#22788;&#65292;&#24182;&#23637;&#31034;&#30340;&#26368;&#22823;BLEU&#20998;&#25968;&#25552;&#21319;&#20026;22.2&#28857;&#65288;40%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03575v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#30028;  &#25688;&#35201;&#65306;&#26426;&#22120;&#32763;&#35793;&#26159;&#19968;&#31181;&#25104;&#29087;&#30340;&#25216;&#26415;&#65292;&#36866;&#29992;&#20110;&#35768;&#22810;&#39640;&#36164;&#28304;&#35821;&#35328;&#23545;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#29615;&#22659;&#20013;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#21487;&#29992;&#20110;&#24320;&#21457;&#32763;&#35793;&#27169;&#22411;&#30340;&#24179;&#34892;&#25968;&#25454;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#24320;&#21457;&#25968;&#25454;&#38598;&#30340;&#24037;&#20316;&#24448;&#24448;&#38598;&#20013;&#20110;&#31616;&#21333;&#22320;&#21019;&#24314;&#29992;&#20110;&#36890;&#29992;&#32763;&#35793;&#30340;&#23613;&#21487;&#33021;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#24456;&#23481;&#26131;&#24573;&#35270;&#20351;&#29992;&#23567;&#22411;&#39046;&#22495;&#25968;&#25454;&#38598;&#30340;&#22909;&#22788;&#21644;&#21457;&#23637;&#12290;&#20026;&#20102;&#35780;&#20272;&#20351;&#29992;&#39046;&#22495;&#25968;&#25454;&#30340;&#20248;&#28857;&#65292;&#20026;&#20302;&#36164;&#28304;&#30340;&#33521;&#35821;&#21040;&#29233;&#23572;&#20848;&#35821;&#35328;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#20581;&#24247;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#27010;&#36848;&#20102;&#24320;&#21457;&#35821;&#26009;&#24211;&#25152;&#20351;&#29992;&#30340;&#36807;&#31243;&#65292;&#24182;&#20174;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#20351;&#29992;&#20581;&#24247;&#39046;&#22495;&#30340;&#39046;&#22495;&#25968;&#25454;&#30340;&#22909;&#22788;&#12290;&#22312;&#32763;&#35793;&#19982;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#65292;&#20351;&#29992;gaHealth&#35821;&#26009;&#24211;&#24320;&#21457;&#30340;&#27169;&#22411;&#22312;&#27604;&#36739;&#26102;&#26174;&#31034;&#20986;&#26368;&#22823;BLEU&#20998;&#25968;&#25552;&#21319;22.2&#28857;&#65288;40%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03575v1 Announce Type: cross  Abstract: Machine Translation is a mature technology for many high-resource language pairs. However in the context of low-resource languages, there is a paucity of parallel data datasets available for developing translation models. Furthermore, the development of datasets for low-resource languages often focuses on simply creating the largest possible dataset for generic translation. The benefits and development of smaller in-domain datasets can easily be overlooked. To assess the merits of using in-domain data, a dataset for the specific domain of health was developed for the low-resource English to Irish language pair. Our study outlines the process used in developing the corpus and empirically demonstrates the benefits of using an in-domain dataset for the health domain. In the context of translating health-related data, models developed using the gaHealth corpus demonstrated a maximum BLEU score improvement of 22.2 points (40%) when compared
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#27861;&#22238;&#31572;&#30340;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#33021;&#21147;&#65292;&#36890;&#36807;&#24320;&#21457;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#30340;5200&#20010;&#38382;&#39064;&#30340;UMWP&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#25991;&#26412;&#30456;&#20284;&#24230;&#21644;&#25968;&#23398;&#34920;&#36798;&#24335;&#26816;&#27979;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#34920;&#26126;&#22312;-context&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;(RLHF)&#35757;&#32451;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#36991;&#20813;&#24187;&#35273;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03558</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#27861;&#22238;&#31572;&#30340;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#27861;&#22238;&#31572;&#30340;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#33021;&#21147;&#65292;&#36890;&#36807;&#24320;&#21457;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#30340;5200&#20010;&#38382;&#39064;&#30340;UMWP&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#25991;&#26412;&#30456;&#20284;&#24230;&#21644;&#25968;&#23398;&#34920;&#36798;&#24335;&#26816;&#27979;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#34920;&#26126;&#22312;-context&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;(RLHF)&#35757;&#32451;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#36991;&#20813;&#24187;&#35273;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#31946;&#35821;&#22659;&#20013;&#23481;&#26131;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#25512;&#27979;&#65292;&#31216;&#20026;&#24187;&#35273;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#26080;&#27861;&#22238;&#31572;&#30340;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;(QA)&#30340;LLM&#24187;&#35273;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;&#26080;&#27861;&#22238;&#31572;&#30340;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;(UMWP)&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20116;&#20010;&#31867;&#21035;&#30340;5200&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#26041;&#27861;&#65292;&#32467;&#21512;&#25991;&#26412;&#30456;&#20284;&#24230;&#21644;&#25968;&#23398;&#34920;&#36798;&#24335;&#26816;&#27979;&#65292;&#20197;&#30830;&#23450;LLM&#26159;&#21542;&#35748;&#20026;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#12290;&#23545;31&#20010;LLM&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;GPT-3&#12289;InstructGPT&#12289;LLaMA&#21644;Claude&#65292;&#22312;-context&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;(RLHF)&#35757;&#32451;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#36991;&#20813;&#24187;&#35273;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03558v1 Announce Type: new  Abstract: Large language models (LLMs) are highly effective in various natural language processing (NLP) tasks. However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination. This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP). To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 questions across five categories. We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable. The results of extensive experiments conducted on 31 LLMs, including GPT-3, InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and reinforcement learning with human feedback (RLHF) training significantly enhance the model's ability to avoid hallucinati
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20449;&#24687;&#29109;&#36827;&#34892;&#25552;&#31034;&#25366;&#25496;&#65292;&#25506;&#32034;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#65292;&#25552;&#39640;&#22522;&#20110;&#35821;&#35328;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;</title><link>https://arxiv.org/abs/2403.03544</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#30340;&#25552;&#31034;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Prompt Mining for Language-based Human Mobility Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03544
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20449;&#24687;&#29109;&#36827;&#34892;&#25552;&#31034;&#25366;&#25496;&#65292;&#25506;&#32034;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#65292;&#25552;&#39640;&#22522;&#20110;&#35821;&#35328;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;&#35821;&#35328;&#30340;&#39044;&#27979;&#26368;&#36817;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#20986;&#29616;&#65292;&#29992;&#20110;&#39044;&#27979;&#20154;&#31867;&#31227;&#21160;&#27169;&#24335;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#20351;&#29992;&#25552;&#31034;&#23558;&#20197;&#25968;&#23383;&#20540;&#32473;&#20986;&#30340;&#21407;&#22987;&#31227;&#21160;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26410;&#26469;&#35266;&#23519;&#30340;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20165;&#37319;&#29992;&#22266;&#23450;&#21644;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#26495;&#23558;&#25968;&#23383;&#20540;&#36716;&#21270;&#20026;&#21477;&#23376;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25552;&#31034;&#65292;&#20351;&#29992;&#22266;&#23450;&#27169;&#26495;&#36827;&#34892;&#25552;&#31034;&#21487;&#33021;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#31227;&#21160;&#39044;&#27979;&#25552;&#31034;&#25366;&#25496;&#26694;&#26550;&#65292;&#26088;&#22312;&#25506;&#32034;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#22522;&#20110;&#25552;&#31034;&#20449;&#24687;&#29109;&#30340;&#25552;&#31034;&#29983;&#25104;&#38454;&#27573;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03544v1 Announce Type: new  Abstract: With the advancement of large language models, language-based forecasting has recently emerged as an innovative approach for predicting human mobility patterns. The core idea is to use prompts to transform the raw mobility data given as numerical values into natural language sentences so that the language models can be leveraged to generate the description for future observations. However, previous studies have only employed fixed and manually designed templates to transform numerical values into sentences. Since the forecasting performance of language models heavily relies on prompts, using fixed templates for prompting may limit the forecasting capability of language models. In this paper, we propose a novel framework for prompt mining in language-based mobility forecasting, aiming to explore diverse prompt design strategies. Specifically, the framework includes a prompt generation stage based on the information entropy of prompts and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#33258;&#21160;&#24191;&#25773;&#24191;&#21578;&#26816;&#27979;&#25216;&#26415;RADIA&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#24191;&#25773;&#20013;&#30340;&#21363;&#20852;&#21644;&#26032;&#24191;&#21578;&#65292;&#20026;&#24191;&#25773;&#24191;&#21578;&#26816;&#27979;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1-macro&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.03538</link><description>&lt;p&gt;
RADIA -- &#20855;&#26377;&#26234;&#33021;&#20998;&#26512;&#30340;&#24191;&#25773;&#24191;&#21578;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
RADIA -- Radio Advertisement Detection with Intelligent Analytics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#33258;&#21160;&#24191;&#25773;&#24191;&#21578;&#26816;&#27979;&#25216;&#26415;RADIA&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#24191;&#25773;&#20013;&#30340;&#21363;&#20852;&#21644;&#26032;&#24191;&#21578;&#65292;&#20026;&#24191;&#25773;&#24191;&#21578;&#26816;&#27979;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1-macro&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#25773;&#24191;&#21578;&#20173;&#28982;&#26159;&#29616;&#20195;&#33829;&#38144;&#31574;&#30053;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#20854;&#21560;&#24341;&#21147;&#21644;&#38024;&#23545;&#24615;&#20256;&#25773;&#30340;&#28508;&#21147;&#19981;&#21487;&#21542;&#35748;&#12290;&#28982;&#32780;&#65292;&#24191;&#25773;&#25773;&#20986;&#26102;&#38388;&#30340;&#21160;&#24577;&#24615;&#21644;&#22810;&#20010;&#24191;&#25773;&#24191;&#21578;&#30340;&#26085;&#30410;&#22686;&#38271;&#36235;&#21183;&#20026;&#30417;&#27979;&#24191;&#21578;&#25773;&#20986;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#35201;&#27714;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19968;&#31181;&#32467;&#21512;&#20808;&#36827;&#35821;&#38899;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#30340;&#26032;&#22411;&#33258;&#21160;&#24191;&#25773;&#24191;&#21578;&#26816;&#27979;&#25216;&#26415;&#12290;RADIA&#30340;&#26041;&#27861;&#36890;&#36807;&#28040;&#38500;&#23545;&#24191;&#25773;&#20869;&#23481;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#38656;&#27714;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;&#36825;&#19968;&#36129;&#29486;&#20801;&#35768;&#26816;&#27979;&#21363;&#20852;&#21644;&#26032;&#24341;&#20837;&#30340;&#24191;&#21578;&#65292;&#20026;&#24191;&#25773;&#24191;&#21578;&#26816;&#27979;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#31934;&#24515;&#20998;&#21106;&#21644;&#26631;&#35760;&#30340;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;F1-macro&#24471;&#20998;87.76&#65292;&#29702;&#35770;&#26368;&#22823;&#20540;&#20026;89.33&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03538v1 Announce Type: cross  Abstract: Radio advertising remains an integral part of modern marketing strategies, with its appeal and potential for targeted reach undeniably effective. However, the dynamic nature of radio airtime and the rising trend of multiple radio spots necessitates an efficient system for monitoring advertisement broadcasts. This study investigates a novel automated radio advertisement detection technique incorporating advanced speech recognition and text classification algorithms. RadIA's approach surpasses traditional methods by eliminating the need for prior knowledge of the broadcast content. This contribution allows for detecting impromptu and newly introduced advertisements, providing a comprehensive solution for advertisement detection in radio broadcasting. Experimental results show that the resulting model, trained on carefully segmented and tagged text data, achieves an F1-macro score of 87.76 against a theoretical maximum of 89.33. This pape
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#21644;&#25216;&#26415;&#39564;&#35777;&#27010;&#24565;&#65292;&#29992;&#20110;&#23545;&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#65292;&#20174;&#32780;&#20026;&#25506;&#32034;&#34920;&#36798;&#23454;&#29616;&#22810;&#23618;&#38901;&#24459;&#20107;&#20214;&#30340;&#22823;&#22411;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03522</link><description>&lt;p&gt;
&#33258;&#21457;&#24615;&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#24687; - &#26397;&#30528;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Non-verbal information in spontaneous speech - towards a new framework of analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03522
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#21644;&#25216;&#26415;&#39564;&#35777;&#27010;&#24565;&#65292;&#29992;&#20110;&#23545;&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#65292;&#20174;&#32780;&#20026;&#25506;&#32034;&#34920;&#36798;&#23454;&#29616;&#22810;&#23618;&#38901;&#24459;&#20107;&#20214;&#30340;&#22823;&#22411;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#21495;&#26159;&#30001;&#38901;&#24459;&#32534;&#30721;&#30340;&#65292;&#25658;&#24102;&#30340;&#20449;&#24687;&#33539;&#22260;&#20174;&#23545;&#35805;&#34892;&#20026;&#21040;&#24577;&#24230;&#21644;&#24773;&#24863;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#25484;&#25569;&#25484;&#22768;&#32467;&#26500;&#30340;&#21407;&#21017;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#21644;&#25216;&#26415;&#39564;&#35777;&#27010;&#24565;&#65292;&#29992;&#20110;&#23545;&#38901;&#24459;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#12290;&#35813;&#26694;&#26550;&#35299;&#37322;&#20102;&#22810;&#23618;&#38901;&#24459;&#20107;&#20214;&#30340;&#34920;&#23618;&#34920;&#31034;&#12290;&#20316;&#20026;&#23454;&#26045;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#36807;&#31243;&#65292;&#21487;&#20197;&#35299;&#24320;&#19977;&#20010;&#32423;&#21035;&#30340;&#38901;&#24459;&#29616;&#35937;&#12290;&#23427;&#20381;&#36182;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#23454;&#29616;&#21516;&#26102;&#30340;&#22810;&#31867;&#21035;/&#22810;&#26631;&#31614;&#26816;&#27979;&#12290;&#23427;&#21487;&#20197;&#27010;&#25324;&#21508;&#31181;&#21508;&#26679;&#30340;&#33258;&#21457;&#25968;&#25454;&#65292;&#22312;&#19982;&#20154;&#31867;&#27880;&#37322;&#30456;&#24403;&#25110;&#20248;&#20110;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#12290;&#38500;&#20102;&#23545;&#38901;&#24459;&#30340;&#26631;&#20934;&#21270;&#24418;&#24335;&#21270;&#22806;&#65292;&#35299;&#24320;&#38901;&#24459;&#27169;&#24335;&#36824;&#21487;&#20197;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03522v1 Announce Type: cross  Abstract: Non-verbal signals in speech are encoded by prosody and carry information that ranges from conversation action to attitude and emotion. Despite its importance, the principles that govern prosodic structure are not yet adequately understood. This paper offers an analytical schema and a technological proof-of-concept for the categorization of prosodic signals and their association with meaning. The schema interprets surface-representations of multi-layered prosodic events. As a first step towards implementation, we present a classification process that disentangles prosodic phenomena of three orders. It relies on fine-tuning a pre-trained speech recognition model, enabling the simultaneous multi-class/multi-label detection. It generalizes over a large variety of spontaneous data, performing on a par with, or superior to, human annotation. In addition to a standardized formalization of prosody, disentangling prosodic patterns can direct a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22522;&#20110;&#35821;&#20041;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;BabelNet&#35789;&#20856;&#35745;&#31639;&#28304;&#25991;&#26412;&#21644;&#21453;&#21521;&#32763;&#35793;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#65292;&#23454;&#29616;&#20102;&#22312;&#30456;&#21516;&#35821;&#35328;&#27700;&#24179;&#19978;&#30340;&#21477;&#23376;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2403.03521</link><description>&lt;p&gt;
BiVert&#65306;&#20351;&#29992;&#20851;&#31995;&#36827;&#34892;&#21452;&#21521;&#35789;&#27719;&#35780;&#20272;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
BiVert: Bidirectional Vocabulary Evaluation using Relations for Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03521
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22522;&#20110;&#35821;&#20041;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;BabelNet&#35789;&#20856;&#35745;&#31639;&#28304;&#25991;&#26412;&#21644;&#21453;&#21521;&#32763;&#35793;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#65292;&#23454;&#29616;&#20102;&#22312;&#30456;&#21516;&#35821;&#35328;&#27700;&#24179;&#19978;&#30340;&#21477;&#23376;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.03521v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#22312;&#36807;&#21435;&#20960;&#24180;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20026;&#19981;&#21516;&#35821;&#35328;&#30340;&#25913;&#36827;&#21644;&#36136;&#37327;&#32763;&#35793;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#23545;&#36825;&#19968;&#20219;&#21153;&#30340;&#35780;&#20272;&#23545;&#20110;&#30830;&#23450;&#32763;&#35793;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#32763;&#35793;&#30340;&#23454;&#38469;&#21547;&#20041;&#26041;&#38754;&#27809;&#26377;&#36275;&#22815;&#30340;&#24378;&#35843;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22522;&#20110;&#35821;&#20041;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;&#32763;&#35793;&#19982;&#28304;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#32508;&#21512;&#30340;&#22810;&#35821;&#35328;&#30334;&#31185;&#20840;&#20070;&#35789;&#20856;BabelNet&#12290;&#36890;&#36807;&#35745;&#31639;&#28304;&#25991;&#26412;&#21450;&#20854;&#36755;&#20986;&#30340;&#21453;&#21521;&#32763;&#35793;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20351;&#21477;&#23376;&#22312;&#30456;&#21516;&#30340;&#35821;&#35328;&#27700;&#24179;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#20107;&#23454;&#20998;&#26512;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#24179;&#22343;&#35780;&#20272;&#20998;&#25968;&#19982;&#21508;&#31181;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03521v1 Announce Type: new  Abstract: Neural machine translation (NMT) has progressed rapidly in the past few years, promising improvements and quality translations for different languages. Evaluation of this task is crucial to determine the quality of the translation. Overall, insufficient emphasis is placed on the actual sense of the translation in traditional methods. We propose a bidirectional semantic-based evaluation method designed to assess the sense distance of the translation from the source text. This approach employs the comprehensive multilingual encyclopedic dictionary BabelNet. Through the calculation of the semantic distance between the source and its back translation of the output, our method introduces a quantifiable approach that empowers sentence comparison on the same linguistic level. Factual analysis shows a strong correlation between the average evaluation scores generated by our method and the human assessments across various machine translation syst
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#23454;&#29616;&#30340;&#26080;&#30417;&#30563;&#22810;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#20013;&#21462;&#24471;&#20248;&#24322;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#22810;&#35821;&#35328;&#26816;&#32034;&#22120;&#30340;&#23454;&#29992;&#24615;</title><link>https://arxiv.org/abs/2403.03516</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#23454;&#29616;&#30340;&#26080;&#30417;&#30563;&#22810;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03516
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#23454;&#29616;&#30340;&#26080;&#30417;&#30563;&#22810;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#20013;&#21462;&#24471;&#20248;&#24322;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#22810;&#35821;&#35328;&#26816;&#32034;&#22120;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#22312;&#22810;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#37197;&#23545;&#25968;&#25454;&#65292;&#36825;&#22312;&#22810;&#35821;&#35328;&#22330;&#26223;&#19979;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;UMR&#65292;&#19968;&#31181;&#26080;&#38656;&#20219;&#20309;&#37197;&#23545;&#25968;&#25454;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#22810;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#24207;&#21015;&#20284;&#28982;&#20272;&#35745;&#33021;&#21147;&#26469;&#33719;&#21462;&#29992;&#20110;&#35757;&#32451;&#31264;&#23494;&#26816;&#32034;&#22120;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#25913;&#21892;&#22810;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;&#22120;&#30340;&#24615;&#33021;&#12290;&#23545;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UMR&#30340;&#24615;&#33021;&#20248;&#20110;&#30417;&#30563;&#22522;&#32447;&#65292;&#23637;&#31034;&#20102;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#35757;&#32451;&#22810;&#35821;&#35328;&#26816;&#32034;&#22120;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#27169;&#22411;&#24050;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03516v1 Announce Type: new  Abstract: Dense retrieval methods have demonstrated promising performance in multilingual information retrieval, where queries and documents can be in different languages. However, dense retrievers typically require a substantial amount of paired data, which poses even greater challenges in multilingual scenarios. This paper introduces UMR, an Unsupervised Multilingual dense Retriever trained without any paired data. Our approach leverages the sequence likelihood estimation capabilities of multilingual language models to acquire pseudo labels for training dense retrievers. We propose a two-stage framework which iteratively improves the performance of multilingual dense retrievers. Experimental results on two benchmark datasets show that UMR outperforms supervised baselines, showcasing the potential of training multilingual retrievers without paired data, thereby enhancing their practicality. Our source code, data, and models are publicly available
&lt;/p&gt;</description></item><item><title>CLongEval&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20013;&#25991;&#22522;&#20934;&#65292;&#20855;&#26377;&#36275;&#22815;&#30340;&#25968;&#25454;&#37327;&#12289;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#39640;&#36136;&#37327;&#65292;&#21487;&#20197;&#23545;&#22810;&#20010;&#24320;&#28304;&#21644;&#21830;&#19994;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.03514</link><description>&lt;p&gt;
CLongEval: &#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03514
&lt;/p&gt;
&lt;p&gt;
CLongEval&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20013;&#25991;&#22522;&#20934;&#65292;&#20855;&#26377;&#36275;&#22815;&#30340;&#25968;&#25454;&#37327;&#12289;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#39640;&#36136;&#37327;&#65292;&#21487;&#20197;&#23545;&#22810;&#20010;&#24320;&#28304;&#21644;&#21830;&#19994;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03514v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#24320;&#21457;&#20855;&#26377;&#24378;&#22823;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19968;&#30452;&#26159;&#26368;&#36817;&#30340;&#30740;&#31350;&#37325;&#28857;&#65292;&#23548;&#33268;&#38271;&#19978;&#19979;&#25991;&#20013;&#25991;&#33021;&#21147;&#23092;&#29087;&#30340;LLMs&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35780;&#20272;&#20173;&#28982;&#19981;&#22815;&#23436;&#21892;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;CLongEval&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;LLMs&#30340;&#20840;&#38754;&#20013;&#25991;&#22522;&#20934;&#12290;CLongEval&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#65306;(1)&#36275;&#22815;&#30340;&#25968;&#25454;&#37327;&#65292;&#21253;&#25324;7&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;7,267&#20010;&#31034;&#20363;&#65307;(2)&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#36866;&#29992;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#20174;1K&#21040;100K&#30340;&#27169;&#22411;&#65307;(3)&#39640;&#36136;&#37327;&#65292;&#38500;&#20102;&#33258;&#21160;&#26500;&#24314;&#30340;&#26631;&#31614;&#22806;&#65292;&#36824;&#26377;&#36229;&#36807;2,000&#20010;&#25163;&#24037;&#27880;&#37322;&#30340;&#38382;&#31572;&#23545;&#12290;&#20511;&#21161;CLongEval&#65292;&#25105;&#20204;&#23545;6&#20010;&#24320;&#28304;&#38271;&#19978;&#19979;&#25991;LLMs&#21644;2&#20010;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#21644;&#20013;&#25991;&#29087;&#32451;&#24230;&#30340;&#39046;&#20808;&#21830;&#19994;&#31454;&#20105;&#23545;&#25163;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03514v1 Announce Type: new  Abstract: Developing Large Language Models (LLMs) with robust long-context capabilities has been the recent research focus, resulting in the emergence of long-context LLMs proficient in Chinese. However, the evaluation of these models remains underdeveloped due to a lack of benchmarks. To address this gap, we present CLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs. CLongEval is characterized by three key features: (1) Sufficient data volume, comprising 7 distinct tasks and 7,267 examples; (2) Broad applicability, accommodating to models with context windows size from 1K to 100K; (3) High quality, with over 2,000 manually annotated question-answer pairs in addition to the automatically constructed labels. With CLongEval, we undertake a comprehensive assessment of 6 open-source long-context LLMs and 2 leading commercial counterparts that feature both long-context abilities and proficiency in Chinese. We also provide in-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.03506</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#28041;&#21450;&#24102;&#26377;&#26377;&#38480;&#36793;&#30028;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26816;&#27979;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#30740;&#31350;&#24212;&#35206;&#30422;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#29983;&#25104;&#30340;&#19981;&#21516;&#31867;&#22411;&#28151;&#21512;&#25991;&#26412;&#65292;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;CoAuthor&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#36890;&#36807;&#20154;&#31867;&#20316;&#32773;&#21644;&#26234;&#33021;&#20889;&#20316;&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#29983;&#25104;&#30340;&#22810;&#36718;&#20132;&#20114;&#20013;&#20135;&#29983;&#30340;&#22810;&#26679;&#21270;&#12289;&#30495;&#23454;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#27493;&#20998;&#21106;&#20026;&#22522;&#30784;&#30340;&#27969;&#31243;&#65306;(i)&#26816;&#27979;&#32473;&#23450;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#21508;&#20010;&#27573;&#33853;&#65292;&#20854;&#20013;&#27599;&#20010;&#27573;&#33853;&#21253;&#21547;&#19968;&#33268;&#20316;&#32773;&#30340;&#21477;&#23376;&#65292;&#20197;&#21450;(ii)&#20998;&#31867;&#27599;&#20010;&#30830;&#23450;&#27573;&#33853;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03506v1 Announce Type: cross  Abstract: This study explores the challenge of sentence-level AI-generated text detection within human-AI collaborative hybrid texts. Existing studies of AI-generated text detection for hybrid texts often rely on synthetic datasets. These typically involve hybrid texts with a limited number of boundaries. We contend that studies of detecting AI-generated content within hybrid texts should cover different types of hybrid texts generated in realistic settings to better inform real-world applications. Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions. We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid text where each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment. Our empirical 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#36136;&#37327;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#25903;&#25345;&#30693;&#35782;&#26816;&#32034;&#65292;&#22312;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#29983;&#25104;&#20013;&#24341;&#20837;&#26032;&#30340;&#30693;&#35782;&#28304;</title><link>https://arxiv.org/abs/2403.03496</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#29983;&#25104;&#30340;&#30693;&#35782;&#21363;&#25554;&#21363;&#29992;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#36136;&#37327;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#25903;&#25345;&#30693;&#35782;&#26816;&#32034;&#65292;&#22312;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#29983;&#25104;&#20013;&#24341;&#20837;&#26032;&#30340;&#30693;&#35782;&#28304;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#29983;&#25104;&#26088;&#22312;&#26500;&#24314;&#21487;&#20197;&#21033;&#29992;&#25366;&#25496;&#30340;&#25903;&#25345;&#30693;&#35782;&#19982;&#20154;&#31867;&#20132;&#35848;&#30340;&#38386;&#32842;&#31995;&#32479;&#12290;&#20808;&#21069;&#24050;&#32463;&#23637;&#31034;&#20102;&#35768;&#22810;&#31181;&#31867;&#22411;&#21644;&#26469;&#28304;&#30340;&#30693;&#35782;&#20316;&#20026;&#25903;&#25345;&#30693;&#35782;&#26159;&#26377;&#29992;&#30340;&#12290;&#21363;&#20351;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#22522;&#20110;&#20174;&#39069;&#22806;&#26368;&#26032;&#28304;&#26816;&#32034;&#30340;&#30693;&#35782;&#36827;&#34892;&#22238;&#24212;&#29983;&#25104;&#20173;&#28982;&#26159;&#19968;&#31181;&#23454;&#38469;&#37325;&#35201;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03496v1 Announce Type: new  Abstract: Knowledge-based, open-domain dialogue generation aims to build chit-chat systems that talk to humans using mined support knowledge. Many types and sources of knowledge have previously been shown to be useful as support knowledge. Even in the era of large language models, response generation grounded in knowledge retrieved from additional up-to-date sources remains a practically important approach. While prior work using single-source knowledge has shown a clear positive correlation between the performances of knowledge selection and response generation, there are no existing multi-source datasets for evaluating support knowledge retrieval. Further, prior work has assumed that the knowledge sources available at test time are the same as during training. This unrealistic assumption unnecessarily handicaps models, as new knowledge sources can become available after a model is trained. In this paper, we present a high-quality benchmark named
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLM&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#20803;&#25968;&#25454;&#32465;&#23450;&#21040;&#25991;&#26412;&#23454;&#20307;&#65292;&#25193;&#23637;&#20102;&#25991;&#26723;&#27880;&#37322;&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#23545;&#20462;&#25913;&#21518;&#31243;&#24207;&#30340;&#33258;&#21160;&#37325;&#26032;&#26631;&#35760;&#65292;&#24182;&#25552;&#20379;&#20102;&#27491;&#24335;&#38382;&#39064;&#23450;&#20041;&#21644;&#22522;&#20934;&#22871;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.03481</link><description>&lt;p&gt;
&#39764;&#27861;&#26631;&#35760;&#65306;&#20351;&#29992;LLM&#32500;&#25252;&#25991;&#26723;&#22806;&#37096;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Magic Markup: Maintaining Document-External Markup with an LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03481
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLM&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#20803;&#25968;&#25454;&#32465;&#23450;&#21040;&#25991;&#26412;&#23454;&#20307;&#65292;&#25193;&#23637;&#20102;&#25991;&#26723;&#27880;&#37322;&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#23545;&#20462;&#25913;&#21518;&#31243;&#24207;&#30340;&#33258;&#21160;&#37325;&#26032;&#26631;&#35760;&#65292;&#24182;&#25552;&#20379;&#20102;&#27491;&#24335;&#38382;&#39064;&#23450;&#20041;&#21644;&#22522;&#20934;&#22871;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25991;&#26723;&#65292;&#21253;&#25324;&#31243;&#24207;&#65292;&#36890;&#24120;&#20855;&#26377;&#20154;&#31867;&#21487;&#35835;&#30340;&#35821;&#20041;&#32467;&#26500;&#12290;&#21382;&#21490;&#19978;&#65292;&#23545;&#36825;&#20123;&#35821;&#20041;&#30340;&#31243;&#24207;&#21270;&#35775;&#38382;&#38656;&#35201;&#26174;&#24335;&#30340;&#25991;&#26723;&#20869;&#26631;&#35760;&#12290;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#20855;&#26377;&#25191;&#34892;&#35821;&#20041;&#30340;&#31995;&#32479;&#20013;&#65292;&#36825;&#24847;&#21619;&#30528;&#36825;&#26159;&#19968;&#20010;&#38590;&#20197;&#27491;&#30830;&#25903;&#25345;&#30340;&#33258;&#24895;&#21151;&#33021;&#12290;&#29616;&#20170;&#65292;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#21487;&#20197;&#20351;&#29992;&#27169;&#22411;&#23545;&#35821;&#20041;&#30340;&#20154;&#31867;&#21270;&#29702;&#35299;&#23558;&#20803;&#25968;&#25454;&#32465;&#23450;&#21040;&#21464;&#21270;&#30340;&#25991;&#26412;&#23454;&#20307;&#65292;&#32780;&#19981;&#38656;&#35201;&#25991;&#26723;&#32467;&#26500;&#30340;&#35201;&#27714;&#12290;&#35813;&#26041;&#27861;&#25193;&#23637;&#20102;&#25991;&#26723;&#27880;&#37322;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#31243;&#24207;&#32534;&#20889;&#12289;&#35843;&#35797;&#12289;&#32500;&#25252;&#21644;&#23637;&#31034;&#20013;&#30340;&#22522;&#26412;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21033;&#29992;&#26234;&#33021;&#20195;&#29702;&#23545;&#20462;&#25913;&#21518;&#30340;&#31243;&#24207;&#37325;&#26032;&#36827;&#34892;&#26631;&#35760;&#65292;&#20351;&#24471;&#20016;&#23500;&#30340;&#27880;&#37322;&#21487;&#20197;&#38543;&#30528;&#20195;&#30721;&#28436;&#36827;&#32780;&#33258;&#21160;&#36319;&#38543;&#12290;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#38382;&#39064;&#23450;&#20041;&#65292;&#19968;&#20010;&#32463;&#39564;&#21512;&#25104;&#22522;&#20934;&#22871;&#20214;&#65292;&#20197;&#21450;&#25105;&#20204;&#30340;&#22522;&#20934;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03481v1 Announce Type: new  Abstract: Text documents, including programs, typically have human-readable semantic structure. Historically, programmatic access to these semantics has required explicit in-document tagging. Especially in systems where the text has an execution semantics, this means it is an opt-in feature that is hard to support properly. Today, language models offer a new method: metadata can be bound to entities in changing text using a model's human-like understanding of semantics, with no requirements on the document structure. This method expands the applications of document annotation, a fundamental operation in program writing, debugging, maintenance, and presentation. We contribute a system that employs an intelligent agent to re-tag modified programs, enabling rich annotations to automatically follow code as it evolves. We also contribute a formal problem definition, an empirical synthetic benchmark suite, and our benchmark generator. Our system achieve
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#36234;&#21335;&#35821;&#35328;&#39046;&#22495;&#30340;&#27861;&#24459;&#25991;&#26412;&#34164;&#28085;&#35782;&#21035;&#39318;&#27425;&#34987;&#24341;&#20837;&#65292;&#20998;&#26512;&#20102;&#21442;&#19982;&#32773;&#32467;&#26524;&#24182;&#35752;&#35770;&#20102;&#27861;&#24459;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#35821;&#35328;&#22240;&#32032;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03435</link><description>&lt;p&gt;
VLSP 2023 -- LTER: &#27861;&#24459;&#25991;&#26412;&#34164;&#28085;&#35782;&#21035;&#25361;&#25112;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
VLSP 2023 -- LTER: A Summary of the Challenge on Legal Textual Entailment Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03435
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#36234;&#21335;&#35821;&#35328;&#39046;&#22495;&#30340;&#27861;&#24459;&#25991;&#26412;&#34164;&#28085;&#35782;&#21035;&#39318;&#27425;&#34987;&#24341;&#20837;&#65292;&#20998;&#26512;&#20102;&#21442;&#19982;&#32773;&#32467;&#26524;&#24182;&#35752;&#35770;&#20102;&#27861;&#24459;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#35821;&#35328;&#22240;&#32032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#24555;&#36895;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#26032;&#26102;&#20195;&#65292;&#29305;&#21035;&#26159;&#22312;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#38656;&#27714;&#26085;&#30410;&#20851;&#38190;&#12290;&#22312;&#24050;&#32463;&#24314;&#31435;&#33391;&#22909;&#30340;&#33521;&#35821;&#12289;&#26085;&#35821;&#21644;&#27721;&#35821;&#31561;&#20854;&#20182;&#35821;&#35328;&#30740;&#31350;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#36234;&#21335;&#35821;&#35328;&#21644;&#35821;&#38899;&#22788;&#29702;&#30740;&#35752;&#20250;&#19978;&#38024;&#23545;&#36234;&#21335;&#35821;&#35328;&#39046;&#22495;&#30340;&#39318;&#20010;&#22522;&#30784;&#30740;&#31350;&#65306;&#27861;&#24459;&#25991;&#26412;&#34164;&#28085;&#35782;&#21035;&#12290;&#36890;&#36807;&#20998;&#26512;&#21442;&#19982;&#32773;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#27861;&#24459;&#39046;&#22495;&#20013;&#26576;&#20123;&#35821;&#35328;&#26041;&#38754;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#26500;&#25104;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03435v1 Announce Type: new  Abstract: In this new era of rapid AI development, especially in language processing, the demand for AI in the legal domain is increasingly critical. In the context where research in other languages such as English, Japanese, and Chinese has been well-established, we introduce the first fundamental research for the Vietnamese language in the legal domain: legal textual entailment recognition through the Vietnamese Language and Speech Processing workshop. In analyzing participants' results, we discuss certain linguistic aspects critical in the legal domain that pose challenges that need to be addressed.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#28151;&#21512;LoRAs&#65288;MoA&#65289;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#26631;&#31614;&#21644;&#26174;&#24335;&#36335;&#30001;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20219;&#21153;&#24178;&#25200;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03432</link><description>&lt;p&gt;
&#28151;&#21512;LoRAs&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03432
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#28151;&#21512;LoRAs&#65288;MoA&#65289;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#26631;&#31614;&#21644;&#26174;&#24335;&#36335;&#30001;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20219;&#21153;&#24178;&#25200;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#20248;&#26377;&#28508;&#21147;&#28608;&#21457;&#25110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29305;&#23450;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#25968;&#25454;&#30340;&#27491;&#30830;&#24179;&#34913;&#23545;&#20110;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20219;&#21153;&#20043;&#38388;&#30340;&#24178;&#25200;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#24182;&#22686;&#24378;&#35757;&#32451;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;LLMs&#30340;&#26032;&#39062;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35843;&#20248;&#26041;&#27861;&#8212;&#8212;&#28151;&#21512;LoRAs&#65288;MoA&#65289;&#26550;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30456;&#24212;&#30340;&#30417;&#30563;&#35821;&#26009;&#24211;&#25968;&#25454;&#21333;&#29420;&#35757;&#32451;&#22810;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;LoRA&#27169;&#22359;&#12290;&#36825;&#20123;LoRA&#27169;&#22359;&#21487;&#20197;&#19982;&#22312;&#19987;&#23478;&#35774;&#35745;&#21407;&#21017;&#20013;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#30456;&#19968;&#33268;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26174;&#24335;&#36335;&#30001;&#31574;&#30053;&#32452;&#21512;&#22810;&#20010;LoRAs&#65292;&#24182;&#24341;&#20837;&#39046;&#22495;&#26631;&#31614;&#20197;&#20419;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#36825;&#26377;&#21161;&#20110;&#38450;&#27490;&#20219;&#21153;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#24182;&#26368;&#32456;&#25552;&#21319;&#27599;&#20010;&#20010;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03432v1 Announce Type: cross  Abstract: Instruction Tuning has the potential to stimulate or enhance specific capabilities of large language models (LLMs). However, achieving the right balance of data is crucial to prevent catastrophic forgetting and interference between tasks. To address these limitations and enhance training flexibility, we propose the Mixture-of-LoRAs (MoA) architecture which is a novel and parameter-efficient tuning method designed for multi-task learning with LLMs. In this paper, we start by individually training multiple domain-specific LoRA modules using corresponding supervised corpus data. These LoRA modules can be aligned with the expert design principles observed in Mixture-of-Experts (MoE). Subsequently, we combine the multiple LoRAs using an explicit routing strategy and introduce domain labels to facilitate multi-task learning, which help prevent interference between tasks and ultimately enhances the performance of each individual task. Further
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;Distributional Dispreference Optimization (D$^2$O)&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#20154;&#31867;&#27491;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#40784;&#65292;&#20943;&#23569;&#20102;&#26377;&#23475;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;</title><link>https://arxiv.org/abs/2403.03419</link><description>&lt;p&gt;
&#21542;&#23450;&#21542;&#23450;&#65306;&#36890;&#36807;&#20998;&#24067;&#24335;&#21453;&#21916;&#22909;&#20248;&#21270;&#23454;&#29616;&#23545;&#40784;&#32780;&#26080;&#38656;&#20154;&#31867;&#27491;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03419
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;Distributional Dispreference Optimization (D$^2$O)&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#20154;&#31867;&#27491;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#40784;&#65292;&#20943;&#23569;&#20102;&#26377;&#23475;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#35282;&#33394;&#65292;&#20294;&#20063;&#21487;&#33021;&#23384;&#22312;&#20256;&#25773;&#19981;&#36947;&#24503;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#23545;&#40784;&#25216;&#26415;&#34987;&#24341;&#20837;&#20197;&#24341;&#23548;LLM&#26397;&#30528;&#20154;&#31867;&#20559;&#22909;&#26041;&#21521;&#21457;&#23637;&#65292;&#24182;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#27491;&#36127;&#35757;&#32451;&#23545;&#65292;&#21463;&#21040;&#22024;&#26434;&#26631;&#31614;&#21644;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#21709;&#24212;&#25968;&#25454;&#20043;&#38388;&#30340;&#36793;&#32536;&#21306;&#21035;&#30340;&#22256;&#25200;&#12290;&#37492;&#20110;&#26368;&#36817;LLM&#22312;&#29983;&#25104;&#26377;&#29992;&#21709;&#24212;&#26041;&#38754;&#30340;&#39640;&#27700;&#24179;&#65292;&#26412;&#25991;&#23558;&#30740;&#31350;&#37325;&#28857;&#36716;&#21521;&#19968;&#20010;&#26032;&#30340;&#26041;&#21521;&#65306;&#20165;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#36127;&#26679;&#26412;&#26469;&#23454;&#29616;&#23545;&#40784;&#65292;&#20445;&#30041;&#26377;&#29992;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#26377;&#23475;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#21453;&#21916;&#22909;&#20248;&#21270;&#65288;D$^2$O&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#30340;&#21709;&#24212;&#19982;&#38750;&#39318;&#36873;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26377;&#25928;&#22320;&#25490;&#38500;&#26377;&#23475;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03419v1 Announce Type: cross  Abstract: Large language models (LLMs) have revolutionized the role of AI, yet also pose potential risks of propagating unethical content. Alignment technologies have been introduced to steer LLMs towards human preference, gaining increasing attention. Despite notable breakthroughs in this direction, existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy labels and the marginal distinction between preferred and dispreferred response data. Given recent LLMs' proficiency in generating helpful responses, this work pivots towards a new research focus: achieving alignment using solely human-annotated negative samples, preserving helpfulness while reducing harmfulness. For this purpose, we propose Distributional Dispreference Optimization (D$^2$O), which maximizes the discrepancy between the generated responses and the dispreferred ones to effectively eschew harmful information. We theoretically demonstrat
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#19968;&#33268;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#25919;&#31574;&#21046;&#23450;&#32773;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#21548;&#20174;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#24212;&#35880;&#24910;&#23545;&#24453;&#12290;</title><link>https://arxiv.org/abs/2403.03407</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#25239;&#26426;&#22120;&#65306;&#35821;&#35328;&#27169;&#22411;&#19982;&#25112;&#20105;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Human vs. Machine: Language Models and Wargames
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03407
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#19968;&#33268;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#25919;&#31574;&#21046;&#23450;&#32773;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#21548;&#20174;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#24212;&#35880;&#24910;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#20105;&#28216;&#25103;&#22312;&#20891;&#20107;&#25112;&#30053;&#30340;&#21457;&#23637;&#21644;&#22269;&#23478;&#23545;&#23041;&#32961;&#25110;&#25915;&#20987;&#30340;&#21709;&#24212;&#20013;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20986;&#29616;&#25215;&#35834;&#20102;&#26356;&#22909;&#30340;&#20915;&#31574;&#21046;&#23450;&#21644;&#22686;&#24378;&#30340;&#20891;&#20107;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;AI&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#19982;&#20154;&#31867;&#30340;&#34892;&#20026;&#26377;&#20309;&#19981;&#21516;&#20173;&#23384;&#22312;&#20105;&#35758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25112;&#20105;&#28216;&#25103;&#23454;&#39564;&#65292;&#20849;&#26377;107&#20301;&#22269;&#23478;&#23433;&#20840;&#19987;&#23478;&#20154;&#31867;&#21442;&#19982;&#32773;&#21442;&#19982;&#65292;&#26088;&#22312;&#30740;&#31350;&#22312;&#19968;&#20010;&#34394;&#26500;&#30340;&#32654;&#20013;&#24773;&#26223;&#20013;&#30340;&#21361;&#26426;&#21319;&#32423;&#65292;&#24182;&#27604;&#36739;&#20154;&#31867;&#21442;&#19982;&#32773;&#19982;LLM&#27169;&#25311;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#21644;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#26174;&#33879;&#19968;&#33268;&#24615;&#65292;&#20294;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#27169;&#25311;&#21644;&#20154;&#31867;&#21442;&#19982;&#32773;&#20043;&#38388;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#24046;&#24322;&#65292;&#36825;&#20419;&#20351;&#20915;&#31574;&#32773;&#22312;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#36981;&#24490;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#35880;&#24910;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03407v1 Announce Type: cross  Abstract: Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#21477;&#23376;&#32763;&#35793;&#32451;&#20064;&#30340;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#26085;&#33521;STE&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24494;&#35843;&#30340;BERT&#27169;&#22411;&#33021;&#22815;&#20197;&#32422;90%&#30340;F1&#20540;&#20998;&#31867;&#27491;&#30830;&#22238;&#31572;</title><link>https://arxiv.org/abs/2403.03396</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#35780;&#20998;&#30340;&#26085;&#33521;&#21477;&#23376;&#32763;&#35793;&#32451;&#20064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Japanese-English Sentence Translation Exercises Dataset for Automatic Grading
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#21477;&#23376;&#32763;&#35793;&#32451;&#20064;&#30340;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#26085;&#33521;STE&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24494;&#35843;&#30340;BERT&#27169;&#22411;&#33021;&#22815;&#20197;&#32422;90%&#30340;F1&#20540;&#20998;&#31867;&#27491;&#30830;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#35780;&#20272;&#21477;&#23376;&#32763;&#35793;&#32451;&#20064;&#65288;STEs&#65289;&#30340;&#20219;&#21153;&#65292;&#36825;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#30340;&#26089;&#26399;&#38454;&#27573;&#34987;&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#36825;&#39033;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#20026;&#27599;&#20010;&#30001;&#25945;&#32946;&#24037;&#20316;&#32773;&#39044;&#20808;&#25351;&#23450;&#30340;&#35780;&#20998;&#26631;&#20934;&#23545;&#23398;&#29983;&#22238;&#31572;&#36827;&#34892;&#35780;&#20998;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;21&#20010;&#38382;&#39064;&#30340;&#26085;&#33521;STE&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#20849;3,498&#20010;&#23398;&#29983;&#22238;&#31572;&#65288;&#24179;&#22343;167&#20010;&#65289;&#12290;&#22238;&#31572;&#25968;&#25454;&#26469;&#33258;&#23398;&#29983;&#21644;&#20247;&#21253;&#24037;&#20316;&#32773;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21253;&#25324;&#24494;&#35843;&#30340;BERT&#21644;&#20855;&#26377;&#23569;&#37327;&#36828;&#31243;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;GPT&#27169;&#22411;&#22312;&#20869;&#30340;&#22522;&#32447;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#24494;&#35843;BERT&#30340;&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#20197;&#32422;90%&#30340;F1&#20540;&#20998;&#31867;&#27491;&#30830;&#22238;&#31572;&#65292;&#20294;&#23545;&#20110;&#38169;&#35823;&#22238;&#31572;&#20165;&#19981;&#21040;80%&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#23569;&#37327;&#36828;&#31243;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;GPT&#27169;&#22411;&#26174;&#31034;&#20986;&#27604;&#24494;&#35843;&#30340;BERT&#26356;&#24046;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#25105;&#20204;&#26032;&#25552;&#20986;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03396v1 Announce Type: new  Abstract: This paper proposes the task of automatic assessment of Sentence Translation Exercises (STEs), that have been used in the early stage of L2 language learning. We formalize the task as grading student responses for each rubric criterion pre-specified by the educators. We then create a dataset for STE between Japanese and English including 21 questions, along with a total of 3, 498 student responses (167 on average). The answer responses were collected from students and crowd workers. Using this dataset, we demonstrate the performance of baselines including finetuned BERT and GPT models with few-shot in-context learning. Experimental results show that the baseline model with finetuned BERT was able to classify correct responses with approximately 90% in F1, but only less than 80% for incorrect responses. Furthermore, the GPT models with few-shot learning show poorer results than finetuned BERT, indicating that our newly proposed task prese
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24605;&#32500;&#38142;&#33976;&#39311;&#20013;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#19982;&#30693;&#35782;&#38598;&#25104;&#19981;&#36275;&#38382;&#39064;&#30340;&#21464;&#20998;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03348</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#36827;&#34892;&#24605;&#32500;&#38142;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Learning to Maximize Mutual Information for Chain-of-Thought Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03348
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24605;&#32500;&#38142;&#33976;&#39311;&#20013;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#19982;&#30693;&#35782;&#38598;&#25104;&#19981;&#36275;&#38382;&#39064;&#30340;&#21464;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#23558;&#22823;&#22411;&#22797;&#26434;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#36739;&#23567;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#26159;&#23454;&#29616;&#39640;&#25928;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#36890;&#36807;&#21033;&#29992;&#24605;&#32500;&#38142; (CoT) &#33976;&#39311;&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#36880;&#27493;&#33976;&#39311; (DSS)&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#20026;&#36739;&#23567;&#27169;&#22411;&#36171;&#20104;&#20854;&#36739;&#22823;&#21516;&#34892;&#30340;&#20248;&#36234;&#25512;&#29702;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#22312;DSS&#20013;&#65292;&#33976;&#39311;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#21516;&#26102;&#33719;&#24471;&#29983;&#25104;&#29702;&#30001;&#21644;&#39044;&#27979;&#26631;&#31614;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;DSS&#24573;&#30053;&#20102;&#36825;&#20004;&#20010;&#35757;&#32451;&#20219;&#21153;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#65292;&#23548;&#33268;CoT&#30693;&#35782;&#19982;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#30340;&#26377;&#25928;&#25972;&#21512;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#29942;&#39048;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#34920;&#36848;&#20026;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03348v1 Announce Type: cross  Abstract: Knowledge distillation, the technique of transferring knowledge from large, complex models to smaller ones, marks a pivotal step towards efficient AI deployment. Distilling Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) distillation, has demonstrated promise by imbuing smaller models with the superior reasoning capabilities of their larger counterparts. In DSS, the distilled model acquires the ability to generate rationales and predict labels concurrently through a multi-task learning framework. However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT knowledge with the task of label prediction. To this end, we investigate the mutual relationship of the two tasks from Information Bottleneck perspective and formulate it as maximizing the mutual information of the representation features of the two tasks. We propose a variational approach to solve thi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#20013;&#36827;&#34892;&#26032;&#20852;&#35266;&#28857;&#25366;&#25496;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Claim&#35782;&#21035;&#26041;&#27861;&#21644;&#35266;&#28857;&#25366;&#25496;&#39537;&#21160;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#21457;&#24067;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#20013;&#23459;&#31216;&#35782;&#21035;&#21644;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#30340;&#26032;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.03336</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#20581;&#24247;&#35805;&#35821;&#20013;&#25366;&#25496;&#26032;&#20852;&#35266;&#28857;&#30340;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
Scope of Large Language Models for Mining Emerging Opinions in Online Health Discourse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#20013;&#36827;&#34892;&#26032;&#20852;&#35266;&#28857;&#25366;&#25496;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Claim&#35782;&#21035;&#26041;&#27861;&#21644;&#35266;&#28857;&#25366;&#25496;&#39537;&#21160;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#21457;&#24067;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#20013;&#23459;&#31216;&#35782;&#21035;&#21644;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#30340;&#26032;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#20013;&#31574;&#21010;&#21644;&#35780;&#20272;&#26032;&#20852;&#35266;&#28857;&#30340;&#25366;&#25496;&#12290;&#25105;&#20204;&#23558;&#26032;&#20852;&#35266;&#28857;&#25366;&#25496;&#23450;&#20041;&#20026;&#26469;&#33258;Reddit&#30340;&#65288;&#26631;&#39064;&#65292;&#35780;&#35770;&#65289;&#37197;&#23545;&#38388;&#30340;&#20004;&#20004;&#31435;&#22330;&#26816;&#27979;&#38382;&#39064;&#65292;&#20854;&#20013;&#24086;&#23376;&#26631;&#39064;&#21253;&#21547;&#26410;&#39044;&#23450;&#20041;&#35805;&#39064;&#30340;&#26032;&#20852;&#20581;&#24247;&#30456;&#20851;&#22768;&#26126;&#12290;&#23459;&#31216;&#21487;&#20197;&#30001;&#29992;&#25143;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#34920;&#36798;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#65288;i&#65289;&#23459;&#31216;&#35782;&#21035;&#30340;&#26041;&#27861;--&#35782;&#21035;&#24086;&#23376;&#26631;&#39064;&#26159;&#21542;&#21253;&#21547;&#23459;&#31216;&#30340;&#20219;&#21153;&#65307;&#20197;&#21450;&#65288;ii&#65289;&#20351;&#29992;LLMs&#36827;&#34892;&#31435;&#22330;&#26816;&#27979;&#30340;&#35266;&#28857;&#25366;&#25496;&#39537;&#21160;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#21457;&#24067;&#19968;&#20010;&#26032;&#39062;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;Long COVID-Stance&#65292;&#25110;LC-stance&#65292;&#26469;&#20419;&#36827;&#25105;&#20204;&#30340;&#25506;&#32034;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#20013;&#23459;&#31216;&#35782;&#21035;&#21644;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#38271;&#26399;COVID&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;COVID&#21518;&#30142;&#30149;&#65292;&#27835;&#30103;&#25351;&#21335;&#19981;&#30830;&#23450;&#19988;&#22797;&#26434;&#65292;&#22240;&#27492;&#26500;&#25104;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03336v1 Announce Type: new  Abstract: In this paper, we develop an LLM-powered framework for the curation and evaluation of emerging opinion mining in online health communities. We formulate emerging opinion mining as a pairwise stance detection problem between (title, comment) pairs sourced from Reddit, where post titles contain emerging health-related claims on a topic that is not predefined. The claims are either explicitly or implicitly expressed by the user. We detail (i) a method of claim identification -- the task of identifying if a post title contains a claim and (ii) an opinion mining-driven evaluation framework for stance detection using LLMs.   We facilitate our exploration by releasing a novel test dataset, Long COVID-Stance, or LC-stance, which can be used to evaluate LLMs on the tasks of claim identification and stance detection in online health communities. Long Covid is an emerging post-COVID disorder with uncertain and complex treatment guidelines, thus mak
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DIVERSE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;173,000&#26465;YouTube&#35270;&#39057;&#35780;&#35770;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#21477;&#23376;&#20013;&#30340;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.03334</link><description>&lt;p&gt;
DIVERSE&#65306;&#36890;&#36807;&#35270;&#39057;&#35780;&#35770;&#24577;&#24230;&#20998;&#26512;&#35299;&#35835;&#20114;&#32852;&#32593;&#23545;&#32654;&#22269;&#20891;&#20107;&#30340;&#30475;&#27861;&#65292;&#19968;&#20010;&#29992;&#20110;&#31435;&#22330;&#20998;&#31867;&#30340;&#26032;&#39062;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DIVERSE: Deciphering Internet Views on the U.S. Military Through Video Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DIVERSE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;173,000&#26465;YouTube&#35270;&#39057;&#35780;&#35770;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#21477;&#23376;&#20013;&#30340;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#31435;&#22330;&#26816;&#27979;&#26159;&#28041;&#21450;&#35782;&#21035;&#22312;&#26377;&#20105;&#35758;&#20027;&#39064;&#19978;&#25317;&#26377;&#30456;&#21453;&#35266;&#28857;&#30340;&#29992;&#25143;&#32676;&#32452;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#30123;&#33495;&#25509;&#31181;&#21644;&#20105;&#35770;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#31435;&#22330;&#25552;&#20379;&#20102;&#23545;&#23454;&#20307;&#31435;&#22330;&#30340;&#25351;&#31034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DIVERSE&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#23545;&#36229;&#36807;173,000&#20010;YouTube&#35270;&#39057;&#35780;&#35770;&#36827;&#34892;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#26631;&#27880;&#20102;&#36825;&#20123;&#35780;&#35770;&#23545;&#20110;&#32654;&#22269;&#20891;&#20107;&#35270;&#39057;&#30340;&#31435;&#22330;&#12290;&#36825;&#20123;&#31435;&#22330;&#36890;&#36807;&#19968;&#31181;&#30001;&#20154;&#31867;&#24341;&#23548;&#12289;&#26426;&#22120;&#36741;&#21161;&#30340;&#26631;&#27880;&#26041;&#27861;&#36827;&#34892;&#26631;&#27880;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#21477;&#23376;&#20013;&#34164;&#21547;&#30340;&#35821;&#27668;&#24369;&#20449;&#21495;&#20316;&#20026;&#25903;&#25345;&#25351;&#26631;&#65292;&#32780;&#38750;&#20351;&#29992;&#20154;&#31867;&#25163;&#21160;&#27880;&#37322;&#12290;&#36825;&#20123;&#24369;&#20449;&#21495;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#21644;&#35773;&#21050;&#30340;&#23384;&#22312;&#65292;&#29305;&#23450;&#20851;&#38190;&#35789;&#30340;&#23384;&#22312;&#65292;&#25991;&#26412;&#30340;&#24773;&#24863;&#20197;&#21450;&#20174;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#26029;&#30340;&#31435;&#22330;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#35780;&#35770;&#34987;&#27880;&#37322;&#20043;&#21069;&#65292;&#36825;&#20123;&#24369;&#20449;&#21495;&#20351;&#29992;&#25968;&#25454;&#32534;&#31243;&#27169;&#22411;&#36827;&#34892; consol
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03334v1 Announce Type: cross  Abstract: Stance detection of social media text is a key component of downstream tasks involving the identification of groups of users with opposing opinions on contested topics such as vaccination and within arguments. In particular, stance provides an indication of an opinion towards an entity. This paper introduces DIVERSE, a dataset of over 173,000 YouTube video comments annotated for their stance towards videos of the U.S. military. The stance is annotated through a human-guided, machine-assisted labeling methodology that makes use of weak signals of tone within the sentence as supporting indicators, as opposed to using manual annotations by humans. These weak signals consist of the presence of hate speech and sarcasm, the presence of specific keywords, the sentiment of the text, and the stance inference from two Large Language Models. The weak signals are then consolidated using a data programming model before each comment is annotated wit
&lt;/p&gt;</description></item><item><title>&#31616;&#21333;&#30340;&#22522;&#20110;guardrail&#30340;&#26041;&#27861;&#22914;&#25552;&#31034;&#21644;&#36807;&#28388;&#21487;&#20197;&#23454;&#29616;&#19982;fine-tuning&#30456;&#23218;&#32654;&#30340;unlearning&#32467;&#26524;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#22312;&#35780;&#20272;&#26356;&#28040;&#32791;&#35745;&#31639;&#36164;&#28304;&#30340;fine-tuning&#26041;&#27861;&#26102;&#32771;&#34385;&#36825;&#20123;&#36731;&#37327;&#32423;&#22522;&#32447;&#12290;</title><link>https://arxiv.org/abs/2403.03329</link><description>&lt;p&gt;
Guardrail Baselines for Unlearning in LLMs
&lt;/p&gt;
&lt;p&gt;
Guardrail Baselines for Unlearning in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03329
&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#30340;&#22522;&#20110;guardrail&#30340;&#26041;&#27861;&#22914;&#25552;&#31034;&#21644;&#36807;&#28388;&#21487;&#20197;&#23454;&#29616;&#19982;fine-tuning&#30456;&#23218;&#32654;&#30340;unlearning&#32467;&#26524;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#22312;&#35780;&#20272;&#26356;&#28040;&#32791;&#35745;&#31639;&#36164;&#28304;&#30340;fine-tuning&#26041;&#27861;&#26102;&#32771;&#34385;&#36825;&#20123;&#36731;&#37327;&#32423;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;fine-tuning&#26159;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#8220;unlearn&#8221;&#27010;&#24565;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;fine-tuning&#21487;&#33021;&#24456;&#26114;&#36149;&#65292;&#22240;&#20026;&#23427;&#26082;&#38656;&#35201;&#29983;&#25104;&#19968;&#32452;&#31034;&#20363;&#65292;&#21448;&#38656;&#35201;&#36816;&#34892;&#22810;&#27425;&#36845;&#20195;&#30340;fine-tuning&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#22522;&#20110;guardrail&#30340;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#21644;&#36807;&#28388;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;fine-tuning&#30456;&#23218;&#32654;&#30340;unlearning&#32467;&#26524;&#12290;&#25105;&#20204;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#22312;&#35780;&#20272;&#26356;&#28040;&#32791;&#35745;&#31639;&#36164;&#28304;&#30340;fine-tuning&#26041;&#27861;&#30340;&#24615;&#33021;&#26102;&#65292;&#35843;&#26597;&#36825;&#20123;&#36731;&#37327;&#32423;&#22522;&#32447;&#12290;&#34429;&#28982;&#25105;&#20204;&#24182;&#19981;&#22768;&#31216;&#25552;&#31034;&#25110;&#36807;&#28388;&#31561;&#26041;&#27861;&#26159;unlearning&#38382;&#39064;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#38656;&#35201;&#26356;&#22909;&#22320;&#21306;&#20998;guardrails&#19982;fine-tuning&#30340;&#24378;&#22823;&#20043;&#22788;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#24378;&#35843;guardrails&#26412;&#36523;&#21487;&#33021;&#20026;unlearning&#20855;&#26377;&#20248;&#21183;&#30340;&#22330;&#26223;&#65292;&#20363;&#22914;&#29983;&#25104;&#31034;&#20363;&#29992;&#20110;fine-tuning&#25110;u
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03329v1 Announce Type: new  Abstract: Recent work has demonstrated that fine-tuning is a promising approach to `unlearn' concepts from large language models. However, fine-tuning can be expensive, as it requires both generating a set of examples and running iterations of fine-tuning to update the model. In this work, we show that simple guardrail-based approaches such as prompting and filtering can achieve unlearning results comparable to fine-tuning. We recommend that researchers investigate these lightweight baselines when evaluating the performance of more computationally intensive fine-tuning methods. While we do not claim that methods such as prompting or filtering are universal solutions to the problem of unlearning, our work suggests the need for evaluation metrics that can better separate the power of guardrails vs. fine-tuning, and highlights scenarios where guardrails themselves may be advantageous for unlearning, such as in generating examples for fine-tuning or u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25945;&#31185;&#20070;&#29983;&#25104;&#21512;&#25104;&#25945;&#24072;-&#23398;&#29983;&#20114;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#21512;&#25104;&#23545;&#35805;&#35757;&#32451;&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22909;&#22788;&#65292;&#24182;&#27604;&#36739;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20294;&#26368;&#20339;&#25968;&#25454;&#32508;&#21512;&#26041;&#27861;&#20173;&#23384;&#22312;&#24187;&#35273;&#21644;&#37325;&#22797;&#20449;&#24687;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03307</link><description>&lt;p&gt;
Book2Dial:&#20174;&#25945;&#31185;&#20070;&#20013;&#29983;&#25104;&#25945;&#24072;-&#23398;&#29983;&#20114;&#21160;&#65292;&#20197;&#23454;&#29616;&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25104;&#26412;&#25928;&#30410;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Book2Dial: Generating Teacher-Student Interactions from Textbooks for Cost-Effective Development of Educational Chatbots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25945;&#31185;&#20070;&#29983;&#25104;&#21512;&#25104;&#25945;&#24072;-&#23398;&#29983;&#20114;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#21512;&#25104;&#23545;&#35805;&#35757;&#32451;&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22909;&#22788;&#65292;&#24182;&#27604;&#36739;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20294;&#26368;&#20339;&#25968;&#25454;&#32508;&#21512;&#26041;&#27861;&#20173;&#23384;&#22312;&#24187;&#35273;&#21644;&#37325;&#22797;&#20449;&#24687;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#36741;&#21161;&#23398;&#29983;&#23398;&#20064;&#30340;&#26377;&#21069;&#26223;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#25945;&#32946;&#39046;&#22495;&#65292;&#24320;&#21457;&#26377;&#25928;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#24456;&#23569;&#26377;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21487;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#19968;&#32452;&#25945;&#31185;&#20070;&#30340;&#21512;&#25104;&#25945;&#24072;-&#23398;&#29983;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25429;&#25417;&#20102;&#23398;&#20064;&#20114;&#21160;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#21363;&#23545;&#26448;&#26009;&#20135;&#29983;&#20852;&#36259;&#30340;&#37096;&#20998;&#30693;&#35782;&#30340;&#23398;&#29983;&#19982;&#32769;&#24072;&#20114;&#21160;&#22320;&#21521;&#32769;&#24072;&#25552;&#20986;&#20851;&#20110;&#25945;&#31185;&#20070;&#20013;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#36825;&#31181;&#23545;&#35805;&#24212;&#28385;&#36275;&#30340;&#21508;&#31181;&#36136;&#37327;&#26631;&#20934;&#65292;&#24182;&#27604;&#36739;&#20102;&#20960;&#31181;&#20381;&#36182;&#20110;&#25552;&#31034;&#25110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#23545;&#35805;&#26469;&#35757;&#32451;&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24182;&#23637;&#31034;&#20102;&#36827;&#19968;&#27493;&#22312;&#19981;&#21516;&#25945;&#32946;&#39046;&#22495;&#36827;&#34892;&#24494;&#35843;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#26368;&#20339;&#30340;&#25968;&#25454;&#32508;&#21512;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#65292;&#24182;&#19988;&#20542;&#21521;&#20110;&#37325;&#22797;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03307v1 Announce Type: new  Abstract: Educational chatbots are a promising tool for assisting student learning. However, the development of effective chatbots in education has been challenging, as high-quality data is seldom available in this domain. In this paper, we propose a framework for generating synthetic teacher-student interactions grounded in a set of textbooks. Our approaches capture one aspect of learning interactions where curious students with partial knowledge interactively ask a teacher questions about the material in the textbook. We highlight various quality criteria that such dialogues should fulfill and compare several approaches relying on either prompting or fine-tuning large language models. We use synthetic dialogues to train educational chatbots and show benefits of further fine-tuning in different educational domains. However, human evaluation shows that our best data synthesis method still suffers from hallucinations and tends to reiterate informat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#36890;&#36807;&#31070;&#32463;&#32452;&#20214;&#25552;&#21319;&#35268;&#21017;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#21183;&#65292;&#19988;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.03305</link><description>&lt;p&gt;
&#20004;&#20840;&#20854;&#32654;&#65306;&#19968;&#31181;&#28789;&#27963;&#19988;&#36890;&#29992;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#29992;&#20110;&#20851;&#31995;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach for Relation Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#36890;&#36807;&#31070;&#32463;&#32452;&#20214;&#25552;&#21319;&#35268;&#21017;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#21183;&#65292;&#19988;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#29992;&#20110;&#20851;&#31995;&#20998;&#31867;&#65288;RC&#65289;&#65292;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#19982;&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21457;&#25381;&#20102;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#21183;&#65306;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#30340;&#36866;&#24212;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#19968;&#20010;&#29992;&#20110;&#36879;&#26126;&#20998;&#31867;&#30340;&#22768;&#26126;&#24615;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#36890;&#36807;&#35821;&#20041;&#25991;&#26412;&#21305;&#37197;&#22686;&#24378;&#35268;&#21017;&#27867;&#21270;&#33021;&#21147;&#30340;&#31070;&#32463;&#32452;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#35821;&#20041;&#21305;&#37197;&#22120;&#20165;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#22312;&#26080;&#30417;&#30563;&#39046;&#22495;&#26080;&#20851;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32452;&#20214;&#26494;&#25955;&#32806;&#21512;&#65292;&#20801;&#35768;&#23545;&#35268;&#21017;&#36827;&#34892;&#20462;&#25913;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#35821;&#20041;&#21305;&#37197;&#22120;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#25968;&#25454;&#38598;&#65306;Few-Shot TACRED &#21644; Few-Shot &#29256;&#26412;&#30340; NYT29&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03305v1 Announce Type: cross  Abstract: This paper introduces a novel neuro-symbolic architecture for relation classification (RC) that combines rule-based methods with contemporary deep learning techniques. This approach capitalizes on the strengths of both paradigms: the adaptability of rule-based systems and the generalization power of neural networks. Our architecture consists of two components: a declarative rule-based model for transparent classification and a neural component to enhance rule generalizability through semantic text matching. Notably, our semantic matcher is trained in an unsupervised domain-agnostic way, solely with synthetic data. Further, these components are loosely coupled, allowing for rule modifications without retraining the semantic matcher. In our evaluation, we focused on two few-shot relation classification datasets: Few-Shot TACRED and a Few-Shot version of NYT29. We show that our proposed method outperforms previous state-of-the-art models 
&lt;/p&gt;</description></item><item><title>Mad Libs &#25552;&#20379;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861; MLA &#22312;&#36328;&#39046;&#22495;&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25968;&#25454;&#25552;&#21462;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102; 2.6 &#20010; F1 &#20998;&#25968;&#12290;&#21516;&#26102;&#65292;&#22312;&#38646;&#21644;&#23569;&#26679;&#26412;&#20107;&#20214;&#35282;&#33394;&#26041;&#38754;&#30456;&#27604;&#26080;&#22686;&#24378;&#22522;&#32447;&#65292;&#25552;&#39640;&#20102; 3.9 &#21644; 5.2 &#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.03304</link><description>&lt;p&gt;
&#25152;&#38656;&#21482;&#26159; Mad Libs: &#22686;&#24378;&#36328;&#39046;&#22495;&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03304
&lt;/p&gt;
&lt;p&gt;
Mad Libs &#25552;&#20379;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861; MLA &#22312;&#36328;&#39046;&#22495;&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25968;&#25454;&#25552;&#21462;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102; 2.6 &#20010; F1 &#20998;&#25968;&#12290;&#21516;&#26102;&#65292;&#22312;&#38646;&#21644;&#23569;&#26679;&#26412;&#20107;&#20214;&#35282;&#33394;&#26041;&#38754;&#30456;&#27604;&#26080;&#22686;&#24378;&#22522;&#32447;&#65292;&#25552;&#39640;&#20102; 3.9 &#21644; 5.2 &#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#21442;&#25968;&#25552;&#21462;&#65288;DocEAE&#65289;&#26159;&#19968;&#20010;&#26497;&#20854;&#22256;&#38590;&#30340;&#20449;&#24687;&#25552;&#21462;&#38382;&#39064;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#36328;&#39046;&#22495;&#35774;&#32622;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; Mad Lib Aug&#65288;MLA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335; DocEAE &#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102; Mad Libs &#30340;&#30452;&#35273;&#65292;&#21363;&#20316;&#20026;&#19968;&#31181;&#28909;&#38376;&#28216;&#25103;&#20013;&#20351;&#29992;&#30340;&#20998;&#31867;&#25513;&#30721;&#25991;&#26723;&#21487;&#20197;&#34987; LLMs &#29983;&#25104;&#24182;&#35299;&#31572;&#65292;&#20174;&#32780;&#20026; DocEAE &#29983;&#25104;&#25968;&#25454;&#12290;&#20351;&#29992; MLA&#65292;&#25105;&#20204;&#30340;&#25972;&#20307; F1 &#20998;&#25968;&#24179;&#22343;&#25913;&#36827;&#20102; 2.6 &#20010;&#30334;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#19982;&#26080;&#22686;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#21644;&#23569;&#26679;&#26412;&#20107;&#20214;&#35282;&#33394;&#26041;&#38754;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#24179;&#22343;&#25552;&#39640;&#20102; 3.9 &#21644; 5.2 &#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03304v1 Announce Type: new  Abstract: Document-Level Event Argument Extraction (DocEAE) is an extremely difficult information extraction problem -- with significant limitations in low-resource cross-domain settings. To address this problem, we introduce Mad Lib Aug (MLA), a novel generative DocEAE data augmentation framework. Our approach leverages the intuition that Mad Libs, which are categorically masked documents used as a part of a popular game, can be generated and solved by LLMs to produce data for DocEAE. Using MLA, we achieve a 2.6-point average improvement in overall F1 score. Moreover, this approach achieves a 3.9 and 5.2 point average increase in zero and few-shot event roles compared to augmentation-free baselines across all experiments.   To better facilitate analysis of cross-domain DocEAE, we additionally introduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to identify roles in the target domain which are semantic outliers with respect t
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;SQL&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#26368;&#20339;&#25552;&#31034;&#27169;&#26495;&#21644;&#35774;&#35745;&#26694;&#26550;&#20173;&#26080;&#20849;&#35782;&#65292;&#26032;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#20219;&#21153;&#26377;&#21161;&#20110;&#20840;&#38754;&#35780;&#20272;&#21508;&#31181;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.02951</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;SQL&#33021;&#21147;&#65306;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02951
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;SQL&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#26368;&#20339;&#25552;&#31034;&#27169;&#26495;&#21644;&#35774;&#35745;&#26694;&#26550;&#20173;&#26080;&#20849;&#35782;&#65292;&#26032;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#20219;&#21153;&#26377;&#21161;&#20110;&#20840;&#38754;&#35780;&#20272;&#21508;&#31181;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#25512;&#21160;&#25991;&#26412;&#29983;&#25104;SQL&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#20110;&#26368;&#20339;&#25552;&#31034;&#27169;&#26495;&#21644;&#35774;&#35745;&#26694;&#26550;&#20173;&#28982;&#27809;&#26377;&#36798;&#25104;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02951v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions.To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs. Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process.Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task. These findings offer
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;OffLanDat&#65292;&#20026;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#25552;&#20379;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.02472</link><description>&lt;p&gt;
OffLanDat&#65306;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02472
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;OffLanDat&#65292;&#20026;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#25552;&#20379;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#25915;&#20987;&#24615;&#35821;&#35328;&#30340;&#26222;&#36941;&#23384;&#22312;&#23545;&#31038;&#20250;&#31119;&#31049;&#20135;&#29983;&#20102;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#39640;&#24230;&#37325;&#35270;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#25915;&#20987;&#24615;&#35821;&#35328;&#26082;&#23384;&#22312;&#26126;&#30830;&#24418;&#24335;&#65292;&#20063;&#23384;&#22312;&#38544;&#24335;&#24418;&#24335;&#65292;&#21518;&#32773;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#24403;&#21069;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#36935;&#21040;&#20960;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20027;&#35201;&#20381;&#36182;&#20110;&#25910;&#38598;&#21253;&#21547;&#26126;&#30830;&#25915;&#20987;&#24615;&#20851;&#38190;&#35789;&#30340;&#25991;&#26412;&#65292;&#36825;&#20351;&#24471;&#25429;&#25417;&#19981;&#21253;&#21547;&#36825;&#20123;&#20851;&#38190;&#35789;&#19988;&#38544;&#21547;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#27425;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#35770;&#20542;&#21521;&#20110;&#20165;&#20851;&#27880;&#25991;&#26412;&#20998;&#26512;&#65292;&#24573;&#35270;&#31038;&#21306;&#20449;&#24687;&#21487;&#20197;&#25552;&#20379;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#22312;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OffLanDat&#65292;&#36825;&#26159;&#30001;ChatGPT&#29983;&#25104;&#30340;&#22522;&#20110;&#31038;&#21306;&#30340;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02472v1 Announce Type: new  Abstract: The widespread presence of offensive languages on social media has resulted in adverse effects on societal well-being. As a result, it has become very important to address this issue with high priority. Offensive languages exist in both explicit and implicit forms, with the latter being more challenging to detect. Current research in this domain encounters several challenges. Firstly, the existing datasets primarily rely on the collection of texts containing explicit offensive keywords, making it challenging to capture implicitly offensive contents that are devoid of these keywords. Secondly, usual methodologies tend to focus solely on textual analysis, neglecting the valuable insights that community information can provide. In this research paper, we introduce a novel dataset OffLanDat, a community based implicit offensive language dataset generated by ChatGPT containing data for 38 different target groups. Despite limitations in genera
&lt;/p&gt;</description></item><item><title>&#36825;&#19968;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35828;&#35805;&#32773;&#30340;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#38899;&#20219;&#21153;&#65292;&#36890;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#36716;&#24405;&#30830;&#20445;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02371</link><description>&lt;p&gt;
NeuroVoz&#65306;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#30340;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NeuroVoz: a Castillian Spanish corpus of parkinsonian speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02371
&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35828;&#35805;&#32773;&#30340;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#38899;&#20219;&#21153;&#65292;&#36890;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#36716;&#24405;&#30830;&#20445;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#38899;&#20998;&#26512;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#35786;&#26029;&#30340;&#36827;&#23637;&#21463;&#21040;&#20844;&#24320;&#21487;&#29992;&#12289;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#26174;&#33879;&#32570;&#20047;&#30340;&#38459;&#30861;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#20877;&#29616;&#24615;&#21644;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#26469;&#33258;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#30340;&#35828;&#35805;&#32773;&#65292;&#21253;&#25324;55&#21517;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;53&#21517;&#34987;&#35786;&#26029;&#24739;&#26377;PD&#30340;&#20010;&#20307;&#65292;&#25152;&#26377;&#36825;&#20123;&#20010;&#20307;&#37117;&#22312;&#33647;&#29289;&#27835;&#30103;&#19979;&#65292;&#24182;&#19988;&#22312;&#33647;&#29289;&#20248;&#21270;&#29366;&#24577;&#19979;&#36827;&#34892;&#35760;&#24405;&#12290; &#36825;&#19968;&#29420;&#29305;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#35821;&#38899;&#20219;&#21153;&#65292;&#21253;&#25324;&#25345;&#32493;&#21457;&#38899;&#20116;&#20010;&#35199;&#29677;&#29273;&#20803;&#38899;&#12289;&#21457;&#38899;&#27979;&#35797;&#12289;16&#20010;&#21548;&#21518;&#37325;&#22797;&#30340;&#35805;&#35821;&#20197;&#21450;&#33258;&#30001;&#29420;&#30333;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#19987;&#23478;&#25163;&#21160;&#36716;&#24405;&#21548;&#21518;&#37325;&#22797;&#20219;&#21153;&#24378;&#35843;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#21033;&#29992;Whisper&#36827;&#34892;&#33258;&#21160;&#29420;&#30333;&#36716;&#24405;&#65292;&#20351;&#20854;&#25104;&#20026;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#30340;&#26368;&#23436;&#25972;&#30340;&#20844;&#24320;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02371v1 Announce Type: cross  Abstract: The advancement of Parkinson's Disease (PD) diagnosis through speech analysis is hindered by a notable lack of publicly available, diverse language datasets, limiting the reproducibility and further exploration of existing research.   In response to this gap, we introduce a comprehensive corpus from 108 native Castilian Spanish speakers, comprising 55 healthy controls and 53 individuals diagnosed with PD, all of whom were under pharmacological treatment and recorded in their medication-optimized state. This unique dataset features a wide array of speech tasks, including sustained phonation of the five Spanish vowels, diadochokinetic tests, 16 listen-and-repeat utterances, and free monologues. The dataset emphasizes accuracy and reliability through specialist manual transcriptions of the listen-and-repeat tasks and utilizes Whisper for automated monologue transcriptions, making it the most complete public corpus of Parkinsonian speech, 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.19379</link><description>&lt;p&gt;
&#30789;&#35895;&#20154;&#32676;&#30340;&#26234;&#24935;&#65306;LLM&#38598;&#25104;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#20154;&#32676;&#20934;&#30830;&#29575;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#25928;&#24212;&#65292;&#21363;&#36890;&#36807;&#32858;&#21512;&#19968;&#32676;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#36807;&#21435;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#21069;&#27839;LLMs&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#20154;&#31867;&#32676;&#20307;&#39044;&#27979;&#27604;&#36187;&#30340;&#40644;&#37329;&#26631;&#20934;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30001;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;31&#20010;&#20108;&#20803;&#38382;&#39064;&#30340;&#32858;&#21512;LLM&#39044;&#27979;&#19982;&#19968;&#20010;&#26469;&#33258;&#19977;&#20010;&#26376;&#39044;&#27979;&#27604;&#36187;&#30340;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#32676;&#20307;&#30340;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#31181;&#39034;&#20174;&#25928;&#24212;&#65292;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26126;&#26174;&#39640;&#20110;50%&#65292;&#23613;&#31649;&#20960;&#20046;&#26159;&#24179;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18571</link><description>&lt;p&gt;
&#29992;&#20110;&#28385;&#36275;&#22810;&#26679;&#29992;&#25143;&#20559;&#22909;&#30340;&#31639;&#26415;&#25511;&#21046;LLMs&#65306;&#20855;&#26377;&#22810;&#30446;&#26631;&#22870;&#21169;&#30340;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31934;&#32454;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#36866;&#24212;&#21508;&#31181;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#24314;&#27169;&#26469;&#34920;&#31034;&#22810;&#26679;&#21270;&#30340;&#20559;&#22909;&#37197;&#32622;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20026;&#22870;&#21169;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65288;&#21363;&#21333;&#20301;&#21521;&#37327;&#65289;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18571v1 Announce Type: cross  Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;Clean-LaVe&#65292;&#26088;&#22312;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#26469;&#22686;&#24378;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18061</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#20449;&#24687;&#25552;&#21462;&#20013;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#36827;&#34892;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;Clean-LaVe&#65292;&#26088;&#22312;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#26469;&#22686;&#24378;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#39046;&#22495;&#65292;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#20854;&#20182;NLP&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#25991;&#26412;&#34164;&#28085;&#65289;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;NLP&#20219;&#21153;&#30340;&#29616;&#25104;&#27169;&#22411;&#30452;&#25509;&#23545;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#22823;&#37327;&#30340;IE&#27880;&#37322;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#28508;&#22312;&#26377;&#20215;&#20540;&#30340;&#21103;&#20135;&#21697;&#26159;&#22823;&#35268;&#27169;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#65292;&#21363;&#20854;&#20182;NLP&#20219;&#21153;&#30340;&#29616;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20266;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#21033;&#29992;&#24182;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;Clean-LaVe&#65292;&#26088;&#22312;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#26469;&#22686;&#24378;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;Clean-LaVe&#21253;&#25324;&#22235;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#33719;&#21462;&#38134;&#26631;&#20934;&#25968;&#25454;&#65307;&#65288;2&#65289;&#20174;&#38134;&#26631;&#20934;&#25968;&#25454;&#20013;&#35782;&#21035;&#30456;&#23545;&#24178;&#20928;&#30340;&#25968;&#25454;&#65307;&#65288;3&#65289;&#20351;&#29992;&#24178;&#20928;&#25968;&#25454;&#24494;&#35843;&#29616;&#25104;&#27169;&#22411;&#65307;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18061v1 Announce Type: cross  Abstract: The superior performance of supervised classification methods in the information extraction (IE) area heavily relies on a large amount of gold standard data. Recent zero-shot classification methods converted the task to other NLP tasks (e.g., textual entailment) and used off-the-shelf models of these NLP tasks to directly perform inference on the test data without using a large amount of IE annotation data. A potentially valuable by-product of these methods is the large-scale silver standard data, i.e., pseudo-labeled data by the off-the-shelf models of other NLP tasks. However, there is no further investigation into the use of these data. In this paper, we propose a new framework, Clean-LaVe, which aims to utilize silver standard data to enhance the zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining silver data; (2) Identifying relatively clean data from silver data; (3) Finetuning the off-the-shelf model using clea
&lt;/p&gt;</description></item><item><title>GraphWiz&#26159;&#19968;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#35299;&#20915;&#21508;&#31181;&#22270;&#38382;&#39064;&#31867;&#22411;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;65%&#65292;&#36229;&#36807;&#20102;GPT-4&#30340;43.8%&#12290;</title><link>https://arxiv.org/abs/2402.16029</link><description>&lt;p&gt;
GraphWiz&#65306;&#29992;&#20110;&#22270;&#38382;&#39064;&#30340;&#25351;&#20196;&#36319;&#38543;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraphWiz: An Instruction-Following Language Model for Graph Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16029
&lt;/p&gt;
&lt;p&gt;
GraphWiz&#26159;&#19968;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#35299;&#20915;&#21508;&#31181;&#22270;&#38382;&#39064;&#31867;&#22411;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;65%&#65292;&#36229;&#36807;&#20102;GPT-4&#30340;43.8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#29702;&#35299;&#21644;&#35299;&#20915;&#22797;&#26434;&#22270;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GraphInstruct&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#22788;&#29702;&#21508;&#31181;&#22270;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#26126;&#30830;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;&#21033;&#29992;GraphInstruct&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;GraphWiz&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#22270;&#38382;&#39064;&#31867;&#22411;&#24182;&#29983;&#25104;&#28165;&#26224;&#25512;&#29702;&#36807;&#31243;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26694;&#26550;&#32435;&#20837;&#22270;&#38382;&#39064;&#27714;&#35299;&#29615;&#22659;&#20013;&#12290;&#22686;&#24378;&#27169;&#22411;GraphWiz-DPO&#22312;&#20061;&#20010;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24615;&#27700;&#24179;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;65%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;43.8%&#30340;GPT-4&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16029v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes. To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Moreover, our research delves into the d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Noise-BERT&#26694;&#26550;&#65292;&#21253;&#21547;&#22122;&#22768;&#23545;&#40784;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#21644;&#23545;&#25239;&#25915;&#20987;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#30340;&#27133;&#22635;&#20805;&#20219;&#21153;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.14494</link><description>&lt;p&gt;
Noise-BERT: &#19968;&#31181;&#20855;&#26377;&#22122;&#22768;&#23545;&#40784;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#25200;&#21160;&#40065;&#26834;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22024;&#26434;&#30340;&#27133;&#22635;&#20805;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14494
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Noise-BERT&#26694;&#26550;&#65292;&#21253;&#21547;&#22122;&#22768;&#23545;&#40784;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#21644;&#23545;&#25239;&#25915;&#20987;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#30340;&#27133;&#22635;&#20805;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#36755;&#20837;&#20449;&#24687;&#32463;&#24120;&#36973;&#21463;&#21508;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#25200;&#21160;&#65292;&#36825;&#24433;&#21709;&#20102;&#27133;&#22635;&#20805;&#20219;&#21153;&#12290;&#23613;&#31649;&#22522;&#20110;&#35268;&#21017;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#38754;&#23545;&#26410;&#30693;&#22122;&#22768;&#24178;&#25200;&#26102;&#65292;&#23427;&#20204;&#26080;&#27861;&#23637;&#29616;&#20986;&#26399;&#26395;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;Noise-BERT&#26469;&#35299;&#20915;&#27133;&#22635;&#20805;&#20013;&#36755;&#20837;&#25200;&#21160;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#22122;&#22768;&#23545;&#40784;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#25200;&#21160;&#40065;&#26834;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;&#20004;&#20010;Noise Alignment&#39044;&#35757;&#32451;&#20219;&#21153;&#65306;&#27133;&#23631;&#34109;&#39044;&#27979;&#21644;&#21477;&#23376;&#22024;&#26434;&#24230;&#21028;&#21035;&#65292;&#26088;&#22312;&#24341;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#20934;&#30830;&#30340;&#27133;&#20449;&#24687;&#21644;&#22122;&#22768;&#20998;&#24067;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#26469;&#22686;&#24378;&#23454;&#20307;&#21644;&#26631;&#31614;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#25239;&#25915;&#20987;&#35757;&#32451;&#31574;&#30053;&#20197;&#25552;&#39640;&#35821;&#20041;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14494v1 Announce Type: new  Abstract: In a realistic dialogue system, the input information from users is often subject to various types of input perturbations, which affects the slot-filling task. Although rule-based data augmentation methods have achieved satisfactory results, they fail to exhibit the desired generalization when faced with unknown noise disturbances. In this study, we address the challenges posed by input perturbations in slot filling by proposing Noise-BERT, a unified Perturbation-Robust Framework with Noise Alignment Pre-training. Our framework incorporates two Noise Alignment Pre-training tasks: Slot Masked Prediction and Sentence Noisiness Discrimination, aiming to guide the pre-trained language model in capturing accurate slot information and noise distribution. During fine-tuning, we employ a contrastive learning loss to enhance the semantic representation of entities and labels. Additionally, we introduce an adversarial attack training strategy to i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$\texttt{Se}^2$&#65292;&#19968;&#31181;&#39034;&#24207;&#24863;&#30693;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24110;&#21161;&#25429;&#25417;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21644;&#24207;&#21015;&#20449;&#24687;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#31034;&#30340;&#30456;&#20851;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13874</link><description>&lt;p&gt;
$\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for In-Context Learning
&lt;/p&gt;
&lt;p&gt;
$\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\texttt{Se}^2$&#65292;&#19968;&#31181;&#39034;&#24207;&#24863;&#30693;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24110;&#21161;&#25429;&#25417;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21644;&#24207;&#21015;&#20449;&#24687;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#31034;&#30340;&#30456;&#20851;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#38656;&#35201;&#36890;&#36807;&#31034;&#20363;&#31034;&#33539;&#26469;&#28608;&#27963;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24191;&#27867;&#25506;&#35752;&#20102;&#29992;&#20110;ICL&#30340;&#31034;&#20363;&#36873;&#25321;&#65292;&#20027;&#35201;&#36981;&#24490;&#8220;&#20808;&#36873;&#25321;&#20877;&#32452;&#32455;&#8221;&#30340;&#33539;&#24335;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#31034;&#20363;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#65292;&#23384;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#20010;&#24207;&#36143;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;$\texttt{Se}^2$&#65292;&#36825;&#26159;&#19968;&#31181;&#39034;&#24207;&#24863;&#30693;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#21453;&#39304;&#65292;&#26377;&#21161;&#20110;&#25429;&#25417;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21644;&#24207;&#21015;&#20449;&#24687;&#65292;&#26174;&#33879;&#20016;&#23500;ICL&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;&#26463;&#25628;&#32034;&#26469;&#23547;&#25214;&#21644;&#26500;&#24314;&#31034;&#20363;&#24207;&#21015;&#65292;&#22686;&#24378;&#20102;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#22312;8&#20010;&#19981;&#21516;&#31867;&#21035;&#20013;&#30340;23&#20010;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13874v1 Announce Type: new  Abstract: The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the "select then organize" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\textit{se}$quential $\textit{se}$lection problem and introduce $\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03299</link><description>&lt;p&gt;
GUARD: &#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#21335;&#30340;&#21512;&#35268;&#24615;
&lt;/p&gt;
&lt;p&gt;
GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#21644;&#26377;&#23475;&#22238;&#24212;&#30340;"&#36234;&#29425;"&#24050;&#32463;&#40723;&#21169;&#31038;&#21306;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#23433;&#20840;&#25514;&#26045;&#26159;&#22312;&#21457;&#24067;&#20043;&#21069;&#29992;&#36234;&#29425;&#20027;&#21160;&#27979;&#35797;LLM&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#27979;&#35797;&#23558;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#22823;&#35268;&#27169;&#19988;&#39640;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#36861;&#38543;&#19968;&#31181;&#26032;&#39062;&#32780;&#30452;&#35266;&#30340;&#31574;&#30053;&#19979;&#65292;&#20197;&#20154;&#31867;&#29983;&#25104;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#36234;&#29425;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#65292;&#23558;&#22235;&#31181;&#19981;&#21516;&#35282;&#33394;&#20998;&#37197;&#32473;&#29992;&#25143;LLM&#65292;&#20197;&#20415;&#21327;&#20316;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25910;&#38598;&#29616;&#26377;&#30340;&#36234;&#29425;&#65292;&#24182;&#36890;&#36807;&#21477;&#23376;&#36880;&#21477;&#36827;&#34892;&#32858;&#31867;&#39057;&#29575;&#21644;&#35821;&#20041;&#27169;&#24335;&#30340;&#21010;&#20998;&#65292;&#23558;&#23427;&#20204;&#20998;&#25104;&#19981;&#21516;&#30340;&#29420;&#31435;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#29305;&#24449;&#32452;&#32455;&#25104;&#19968;&#20010;&#30693;&#35782;&#22270;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#35775;&#38382;&#21644;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#35282;&#33394;&#31995;&#32479;&#23558;&#21033;&#29992;&#36825;&#20010;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#19968;&#20010;&#30740;&#31350;&#31354;&#30333;&#65292;&#25506;&#35752;&#20102;&#22312;&#38750;&#33521;&#25991;&#25991;&#26412;&#20013;&#24212;&#35813;&#20351;&#29992;&#21738;&#31181;&#35821;&#35328;&#26469;&#25552;&#31034;&#24773;&#32490;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2402.03223</link><description>&lt;p&gt;
&#33521;&#25991;&#25552;&#31034;&#27604;&#30446;&#26631;&#35821;&#35328;&#25552;&#31034;&#26356;&#36866;&#29992;&#20110;&#22522;&#20110;NLI&#30340;&#38646;-shot&#24773;&#32490;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#19968;&#20010;&#30740;&#31350;&#31354;&#30333;&#65292;&#25506;&#35752;&#20102;&#22312;&#38750;&#33521;&#25991;&#25991;&#26412;&#20013;&#24212;&#35813;&#20351;&#29992;&#21738;&#31181;&#35821;&#35328;&#26469;&#25552;&#31034;&#24773;&#32490;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24773;&#32490;&#20998;&#31867;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20027;&#35266;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#38656;&#35201;&#36827;&#34892;&#35748;&#30693;&#25512;&#35770;&#36807;&#31243;&#26469;&#35299;&#37322;&#25991;&#23383;&#21050;&#28608;&#12290;&#27492;&#22806;&#65292;&#24773;&#32490;&#31867;&#21035;&#38598;&#21512;&#39640;&#24230;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#12290;&#20363;&#22914;&#65292;&#25991;&#23398;&#20998;&#26512;&#21487;&#33021;&#38656;&#35201;&#20351;&#29992;&#23457;&#32654;&#24773;&#24863;&#65288;&#20363;&#22914;&#65292;&#21457;&#29616;&#26576;&#29289;&#32654;&#20029;&#65289;&#65292;&#32780;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#21017;&#21487;&#20197;&#20174;&#32454;&#31890;&#24230;&#30340;&#38598;&#21512;&#20013;&#33719;&#21462;&#22909;&#22788;&#65288;&#20363;&#22914;&#65292;&#23558;&#24868;&#24594;&#19982;&#28902;&#24700;&#20998;&#24320;&#65289;&#65292;&#19982;&#22522;&#26412;&#24773;&#32490;&#31867;&#21035;&#30456;&#23545;&#24212;&#12290;&#36825;&#20351;&#24471;&#35813;&#20219;&#21153;&#25104;&#20026;&#20102;&#38646;-shot&#20998;&#31867;&#30340;&#19968;&#20010;&#26377;&#36259;&#39046;&#22495;&#65292;&#22312;&#36825;&#31181;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#24320;&#21457;&#26102;&#19981;&#30693;&#36947;&#26631;&#31614;&#38598;&#21512;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#24773;&#32490;&#20998;&#26512;&#36164;&#28304;&#37117;&#26159;&#33521;&#25991;&#30340;&#65292;&#22240;&#27492;&#65292;&#24773;&#32490;&#20998;&#26512;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#26159;&#29992;&#33521;&#25991;&#36827;&#34892;&#30340;&#65292;&#21253;&#25324;&#37027;&#20123;&#28041;&#21450;&#20351;&#29992;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;&#36825;&#32473;&#25105;&#20204;&#30041;&#19979;&#19968;&#20010;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25506;&#35752;&#65306;&#22312;&#38750;&#33521;&#25991;&#25991;&#26412;&#20013;&#65292;&#25105;&#20204;&#24212;&#35813;&#29992;&#21738;&#31181;&#35821;&#35328;&#25552;&#31034;&#24773;&#32490;&#26631;&#31614;&#65311;
&lt;/p&gt;
&lt;p&gt;
Emotion classification in text is a challenging and subjective task, due to the involved cognitive inference processes that are required to interpret a textual stimulus. In addition, the set of emotion categories is highly domain-specific. For instance, literature analysis might require the use of aesthetic emotions (e.g., finding something beautiful), and social media analysis could benefit from fine-grained sets (e.g., separating anger from annoyance) in contrast to basic emotion categories. This renders the task an interesting field for zero-shot classifications, in which the label set is not known at model development time. Unfortunately, most resources for emotion analysis are English, and therefore, most studies on emotion analysis have been performed in English, including those that involve prompting language models for text labels. This leaves us with a research gap that we address in this paper: In which language should we prompt for emotion labels on non-English texts? This i
&lt;/p&gt;</description></item><item><title>&#24314;&#35758;&#23558;&#35748;&#30693;&#20219;&#21153;&#25972;&#21512;&#21040;&#22823;&#22411;&#27169;&#22411;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#20013;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#27169;&#22411;&#30340;&#22810;&#32500;&#26234;&#33021;&#12290;&#36825;&#20010;&#26694;&#26550;&#32467;&#21512;&#20102;&#35748;&#30693;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21253;&#21547;&#20102;&#31283;&#24577;&#26234;&#21147;&#12289;&#27969;&#24577;&#26234;&#21147;&#21644;&#31038;&#20132;&#26234;&#33021;&#31561;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.02547</link><description>&lt;p&gt;
&#23558;&#35748;&#30693;&#20219;&#21153;&#25972;&#21512;&#21040;&#22823;&#22411;&#27169;&#22411;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#20013;
&lt;/p&gt;
&lt;p&gt;
Integration of cognitive tasks into artificial general intelligence test for large models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02547
&lt;/p&gt;
&lt;p&gt;
&#24314;&#35758;&#23558;&#35748;&#30693;&#20219;&#21153;&#25972;&#21512;&#21040;&#22823;&#22411;&#27169;&#22411;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#20013;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#27169;&#22411;&#30340;&#22810;&#32500;&#26234;&#33021;&#12290;&#36825;&#20010;&#26694;&#26550;&#32467;&#21512;&#20102;&#35748;&#30693;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21253;&#21547;&#20102;&#31283;&#24577;&#26234;&#21147;&#12289;&#27969;&#24577;&#26234;&#21147;&#21644;&#31038;&#20132;&#26234;&#33021;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#27169;&#22411;&#30340;&#21457;&#23637;&#36807;&#31243;&#20013;&#65292;&#24517;&#39035;&#23545;&#20013;&#38388;&#27169;&#22411;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#20197;&#35780;&#20272;&#20854;&#33021;&#21147;&#65292;&#24182;&#23545;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#24615;&#35780;&#20272;&#65292;&#20197;&#30830;&#20445;&#22312;&#23454;&#38469;&#24212;&#29992;&#20043;&#21069;&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#35780;&#20272;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#32570;&#20047;&#23545;&#22823;&#22411;&#27169;&#22411;&#30340;&#22810;&#32500;&#26234;&#33021;&#35780;&#20272;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#20013;&#65292;&#25105;&#20204;&#25552;&#20513;&#24314;&#31435;&#19968;&#20010;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#28385;&#36275;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#22823;&#22411;&#27169;&#22411;&#30340;&#27979;&#35797;&#38656;&#27714;&#65292;&#20197;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#35813;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27979;&#35797;&#26694;&#26550;&#23558;&#35748;&#30693;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32852;&#31995;&#36215;&#26469;&#65292;&#21253;&#25324;&#26234;&#21147;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#31283;&#24577;&#26234;&#21147;&#65292;&#21363;&#31215;&#32047;&#30340;&#30693;&#35782;&#21644;&#32463;&#39564;&#30340;&#21453;&#26144;; &#27969;&#24577;&#26234;&#21147;&#65292;&#29305;&#28857;&#26159;&#35299;&#20915;&#38382;&#39064;&#21644;&#36866;&#24212;&#24615;&#25512;&#29702;; &#31038;&#20132;&#26234;&#33021;&#65292;&#34920;&#31034;&#22312;&#22810;&#26041;&#38754;&#29702;&#35299;&#21644;&#36866;&#24212;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the evolution of large models, performance evaluation is necessarily performed on the intermediate models to assess their capabilities, and on the well-trained model to ensure safety before practical application. However, current model evaluations mainly rely on specific tasks and datasets, lacking a united framework for assessing the multidimensional intelligence of large models. In this perspective, we advocate for a comprehensive framework of artificial general intelligence (AGI) test, aimed at fulfilling the testing needs of large language models and multi-modal large models with enhanced capabilities. The AGI test framework bridges cognitive science and natural language processing to encompass the full spectrum of intelligence facets, including crystallized intelligence, a reflection of amassed knowledge and experience; fluid intelligence, characterized by problem-solving and adaptive reasoning; social intelligence, signifying comprehension and adaptation within multifacete
&lt;/p&gt;</description></item><item><title>LOCOST&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#12290;&#19982;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;LOCOST&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;LOCOST&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;93-96%&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2401.17919</link><description>&lt;p&gt;
LOCOST: &#38271;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#21270;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LOCOST: State-Space Models for Long Document Abstractive Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17919
&lt;/p&gt;
&lt;p&gt;
LOCOST&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#12290;&#19982;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;LOCOST&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;LOCOST&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;93-96%&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26159;&#32534;&#30721;&#38271;&#24207;&#21015;&#21644;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#30340;&#20302;&#22797;&#26434;&#24230;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LOCOST&#65306;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#36755;&#20837;&#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#31181;&#26550;&#26500;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;O&#65288;L log L&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#27604;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#38271;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#24615;&#33021;&#27700;&#24179;&#19978;&#36798;&#21040;&#20102;&#19982;&#30456;&#21516;&#22823;&#23567;&#30340;&#26368;&#20248;&#31232;&#30095;&#21464;&#21387;&#22120;&#30456;&#24403;&#30340;93-96%&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26399;&#38388;&#33410;&#30465;&#20102;&#39640;&#36798;50%&#30340;&#20869;&#23384;&#65292;&#22312;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#20102;&#39640;&#36798;87%&#30340;&#20869;&#23384;&#12290;&#27492;&#22806;&#65292;LOCOST&#26377;&#25928;&#22320;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#65292;&#20026;&#23436;&#25972;&#20070;&#25688;&#35201;&#21270;&#35774;&#23450;&#20102;&#26032;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#24182;&#20026;&#38271;&#36755;&#20837;&#22788;&#29702;&#24320;&#36767;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of $O(L \log L)$, this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#21307;&#30103;&#38382;&#31572;&#30340;&#36890;&#29992;&#21644;&#21307;&#23398;&#29305;&#23450;&#30340;&#31934;&#28860;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#20197;&#22635;&#34917;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#20013;&#36825;&#20123;&#27169;&#22411;&#24615;&#33021;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2401.11389</link><description>&lt;p&gt;
MedLM: &#25506;&#32034;&#29992;&#20110;&#21307;&#30103;&#38382;&#31572;&#31995;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MedLM: Exploring Language Models for Medical Question Answering Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#21307;&#30103;&#38382;&#31572;&#30340;&#36890;&#29992;&#21644;&#21307;&#23398;&#29305;&#23450;&#30340;&#31934;&#28860;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#20197;&#22635;&#34917;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#20013;&#36825;&#20123;&#27169;&#22411;&#24615;&#33021;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#36805;&#36895;&#25193;&#22823;&#30340;&#22312;&#32447;&#21307;&#23398;&#25991;&#29486;&#65292;&#33258;&#21160;&#21270;&#31995;&#32479;&#29992;&#20110;&#32858;&#21512;&#21644;&#24635;&#32467;&#20449;&#24687;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#21464;&#24471;&#26085;&#30410;&#20851;&#38190;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#20808;&#36827;&#30340;&#29983;&#25104;&#33021;&#21147;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#23427;&#20204;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#23553;&#38381;&#24335;&#29983;&#25104;&#38382;&#31572;&#26041;&#38754;&#65292;&#26159;&#26174;&#33879;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#31572;&#31561;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#36739;&#23569;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#27604;&#36739;&#36890;&#29992;&#21644;&#19987;&#38376;&#29992;&#20110;&#21307;&#23398;&#30340;&#31934;&#28860;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#38382;&#31572;&#20013;&#30340;&#34920;&#29616;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#24494;&#35843;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#23558;&#25506;&#35752;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12289;&#27604;&#36739;&#24615;&#33021;&#21644;&#22312;&#21307;&#30103;&#38382;&#31572;&#32972;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11389v2 Announce Type: replace-cross  Abstract: In the face of rapidly expanding online medical literature, automated systems for aggregating and summarizing information are becoming increasingly crucial for healthcare professionals and patients. Large Language Models (LLMs), with their advanced generative capabilities, have shown promise in various NLP tasks, and their potential in the healthcare domain, particularly for Closed-Book Generative QnA, is significant. However, the performance of these models in domain-specific tasks such as medical Q&amp;A remains largely unexplored. This study aims to fill this gap by comparing the performance of general and medical-specific distilled LMs for medical Q&amp;A. We aim to evaluate the effectiveness of fine-tuning domain-specific LMs and compare the performance of different families of Language Models. The study will address critical questions about these models' reliability, comparative performance, and effectiveness in the context of me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#20197;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#38754;&#20020;&#30340;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.09340</link><description>&lt;p&gt;
SceneVerse&#65306;&#20026;&#22522;&#20110;&#22330;&#26223;&#30340;&#22330;&#26223;&#29702;&#35299;&#25193;&#23637;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.09340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#20197;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#38754;&#20020;&#30340;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#65292;&#21363;&#23558;&#35821;&#35328;&#19982;3D&#29289;&#29702;&#29615;&#22659;&#23545;&#40784;&#65292;&#26159;&#21457;&#23637;&#20855;&#36523;&#20307;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#30340;&#22522;&#30707;&#12290;&#19982;2D&#39046;&#22495;&#26368;&#36817;&#30340;&#36827;&#23637;&#30456;&#27604;&#65292;&#23558;&#35821;&#35328;&#19982;3D&#22330;&#26223;&#23545;&#40784;&#38754;&#20020;&#30528;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;3D&#22330;&#26223;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#30001;&#20110;&#22810;&#26679;&#30340;&#29289;&#20307;&#37197;&#32622;&#12289;&#20016;&#23500;&#30340;&#23646;&#24615;&#21644;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65307;&#65288;ii&#65289;&#25903;&#25345;&#22522;&#20110;&#22330;&#26223;&#23398;&#20064;&#30340;&#37197;&#23545;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#32570;&#20047;&#20174;&#22522;&#20110;&#22330;&#26223;&#30340;3D&#25968;&#25454;&#20013;&#25552;&#28860;&#30693;&#35782;&#30340;&#32479;&#19968;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#20174;&#32780;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#36825;&#19977;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#21253;&#21547;&#32422;68K&#20010;3D&#23460;&#20869;&#22330;&#26223;&#65292;&#21253;&#25324;250&#19975;&#20010;&#35270;&#35273;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.09340v2 Announce Type: replace-cross  Abstract: 3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-langu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#27492;&#31867;&#25915;&#20987;&#26102;&#30340;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#25915;&#20987;&#25104;&#21151;&#30340;&#21407;&#22240;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.14197</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#27492;&#31867;&#25915;&#20987;&#26102;&#30340;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#25915;&#20987;&#25104;&#21151;&#30340;&#21407;&#22240;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#20869;&#23481;&#30340;&#25972;&#21512;&#24050;&#32463;&#23454;&#29616;&#20102;LLMs&#30340;&#26356;&#26032;&#21644;&#24191;&#27867;&#24212;&#29992;&#65292;&#27604;&#22914;&#24494;&#36719;Copilot&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25972;&#21512;&#20063;&#35753;LLMs&#38754;&#20020;&#20102;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#22806;&#37096;&#20869;&#23481;&#20013;&#23884;&#20837;&#24694;&#24847;&#25351;&#20196;&#65292;&#20174;&#32780;ompromising LLM&#36755;&#20986;&#24182;&#23548;&#33268;&#21709;&#24212;&#20559;&#31163;&#29992;&#25143;&#26399;&#26395;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#20197;&#35780;&#20272;&#36825;&#31867;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#22522;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#20998;&#26512;&#20102;&#35813;&#25915;&#20987;&#25104;&#21151;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21363;LLMs&#26080;&#27861;&#21306;&#20998;&#25351;&#20196;&#21644;&#22806;&#37096;&#20869;&#23481;&#20197;&#21450;&#32570;&#20047;&#24847;&#35782;&#19981;&#25191;&#34892;&#22806;&#37096;&#20869;&#23481;&#20869;&#30340;&#25351;&#20196;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#40657;&#30418;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14197v2 Announce Type: replace-cross  Abstract: The integration of large language models (LLMs) with external content has enabled more up-to-date and wide-ranging applications of LLMs, such as Microsoft Copilot. However, this integration has also exposed LLMs to the risk of indirect prompt injection attacks, where an attacker can embed malicious instructions within external content, compromising LLM output and causing responses to deviate from user expectations. To investigate this important but underexplored issue, we introduce the first benchmark for indirect prompt injection attacks, named BIPIA, to evaluate the risk of such attacks. Based on the evaluation, our work makes a key analysis of the underlying reason for the success of the attack, namely the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content. Building upon this analysis, we develop two black-box metho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#31639;&#26415;&#25512;&#26029;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#25110;&#20351;&#29992;&#39640;&#24230;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#26500;&#25104;&#21644;&#20559;&#32622;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#27604;&#30452;&#25509;&#25552;&#31034;&#21644;&#20808;&#21069;&#21463;&#25511;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#26356;&#31934;&#30830;&#30340;&#25991;&#26412;&#29983;&#25104;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2311.14479</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#31639;&#26415;&#23454;&#29616;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controlled Text Generation via Language Model Arithmetic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14479
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#31639;&#26415;&#25512;&#26029;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#25110;&#20351;&#29992;&#39640;&#24230;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#26500;&#25104;&#21644;&#20559;&#32622;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#27604;&#30452;&#25509;&#25552;&#31034;&#21644;&#20808;&#21069;&#21463;&#25511;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#26356;&#31934;&#30830;&#30340;&#25991;&#26412;&#29983;&#25104;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24191;&#27867;&#37096;&#32626;&#65292;&#23450;&#21046;&#35789;&#27719;&#12289;&#39118;&#26684;&#21644;&#23383;&#31526;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27169;&#22411;&#31639;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#26029;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#65288;&#37325;&#26032;&#65289;&#35757;&#32451;&#25110;&#39640;&#24230;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#26500;&#25104;&#21644;&#20559;&#32622;LLMs&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#27604;&#30452;&#25509;&#25552;&#31034;&#21644;&#20808;&#21069;&#30340;&#21463;&#25511;&#25991;&#26412;&#29983;&#25104;&#65288;CTG&#65289;&#25216;&#26415;&#26356;&#31934;&#30830;&#22320;&#25511;&#21046;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#20351;&#29992;&#27169;&#22411;&#31639;&#26415;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20808;&#21069;&#30340;CTG&#25216;&#26415;&#34920;&#31034;&#20026;&#31616;&#21333;&#30340;&#20844;&#24335;&#65292;&#24182;&#33258;&#28982;&#22320;&#23558;&#20854;&#25193;&#23637;&#21040;&#26032;&#30340;&#21644;&#26356;&#26377;&#25928;&#30340;&#20844;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#25512;&#27979;&#37319;&#26679;&#30340;&#39640;&#25928;LLM&#37319;&#26679;&#25216;&#26415;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#25105;&#20204;&#30340;&#35774;&#32622;&#12290;&#36825;&#20351;&#24471;&#20351;&#29992;&#22810;&#20010;&#32452;&#21512;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#25991;&#26412;&#29983;&#25104;&#20960;&#20046;&#27809;&#26377;&#36229;&#36807;&#21333;&#20010;&#27169;&#22411;&#30340;&#39069;&#22806;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#27169;&#22411;&#31639;&#26415;&#20801;&#35768;&#23545;&#29983;&#25104;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14479v2 Announce Type: replace  Abstract: As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style, and character becomes more important. In this work, we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition, the framework allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. Further, we show that speculative sampling, a technique for efficient LLM sampling, extends to our setting. This enables highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of genera
&lt;/p&gt;</description></item><item><title>AdaCCD&#26159;&#19968;&#31181;&#36328;&#35821;&#35328;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#21644;&#39044;&#35757;&#32451;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#35821;&#35328;&#26080;&#20851;&#30340;&#20195;&#30721;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#30446;&#26631;&#35821;&#35328;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#26032;&#35821;&#35328;&#20013;&#30340;&#20811;&#38534;&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2311.07277</link><description>&lt;p&gt;
AdaCCD:&#22522;&#20110;&#33258;&#36866;&#24212;&#35821;&#20041;&#23545;&#27604;&#21457;&#29616;&#30340;&#36328;&#35821;&#35328;&#36866;&#24212;&#26041;&#27861;&#29992;&#20110;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AdaCCD: Adaptive Semantic Contrasts Discovery Based Cross Lingual Adaptation for Code Clone Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07277
&lt;/p&gt;
&lt;p&gt;
AdaCCD&#26159;&#19968;&#31181;&#36328;&#35821;&#35328;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#21644;&#39044;&#35757;&#32451;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#35821;&#35328;&#26080;&#20851;&#30340;&#20195;&#30721;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#30446;&#26631;&#35821;&#35328;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#26032;&#35821;&#35328;&#20013;&#30340;&#20811;&#38534;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26088;&#22312;&#20174;&#22823;&#22411;&#20195;&#30721;&#24211;&#20013;&#26816;&#32034;&#21151;&#33021;&#30456;&#20284;&#30340;&#31243;&#24207;&#65292;&#19968;&#30452;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#20195;&#36719;&#20214;&#36890;&#24120;&#28041;&#21450;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#20197;&#21450;&#33258;&#36523;&#27169;&#22411;&#35774;&#35745;&#38480;&#21046;&#65292;&#24403;&#21069;&#30340;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#20165;&#38480;&#20110;&#20960;&#31181;&#27969;&#34892;&#30340;&#32534;&#31243;&#35821;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaCCD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#35821;&#35328;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#26032;&#35821;&#35328;&#20013;&#30340;&#20811;&#38534;&#20195;&#30721;&#12290;AdaCCD&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#26080;&#20851;&#20195;&#30721;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31934;&#21046;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#21521;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#20256;&#36882;&#30693;&#35782;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#22810;&#35821;&#35328;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#22522;&#20934;&#26469;&#35780;&#20272;AdaCCD&#30340;&#36328;&#35821;&#35328;&#36866;&#24212;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07277v2 Announce Type: replace-cross  Abstract: Code Clone Detection, which aims to retrieve functionally similar programs from large code bases, has been attracting increasing attention. Modern software often involves a diverse range of programming languages. However, current code clone detection methods are generally limited to only a few popular programming languages due to insufficient annotated data as well as their own model design constraints. To address these issues, we present AdaCCD, a novel cross-lingual adaptation method that can detect cloned codes in a new language without annotations in that language. AdaCCD leverages language-agnostic code representations from pre-trained programming language models and propose an Adaptively Refined Contrastive Learning framework to transfer knowledge from resource-rich languages to resource-poor languages. We evaluate the cross-lingual adaptation results of AdaCCD by constructing a multilingual code clone detection benchmark
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#24179;&#21488;&#21644;&#20998;&#26512;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#21644;&#27604;&#36739;&#20102;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#20869;&#23481;&#65292;&#21457;&#29616;&#20102;&#20851;&#20110;&#35821;&#26009;&#24211;&#20869;&#23481;&#30340;&#20960;&#20010;&#20196;&#20154;&#24778;&#35766;&#19988;&#20197;&#21069;&#26410;&#34987;&#35760;&#24405;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2310.20707</link><description>&lt;p&gt;
&#25105;&#30340;&#22823;&#25968;&#25454;&#20013;&#26377;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What's In My Big Data?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.20707
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#24179;&#21488;&#21644;&#20998;&#26512;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#21644;&#27604;&#36739;&#20102;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#20869;&#23481;&#65292;&#21457;&#29616;&#20102;&#20851;&#20110;&#35821;&#26009;&#24211;&#20869;&#23481;&#30340;&#20960;&#20010;&#20196;&#20154;&#24778;&#35766;&#19988;&#20197;&#21069;&#26410;&#34987;&#35760;&#24405;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#26159;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#35821;&#26009;&#24211;&#30340;&#20869;&#23481;&#65292;&#21253;&#25324;&#19968;&#33324;&#32479;&#35745;&#20449;&#24687;&#12289;&#36136;&#37327;&#12289;&#31038;&#20250;&#22240;&#32032;&#21644;&#35780;&#20272;&#25968;&#25454;&#65288;&#27745;&#26579;&#65289;&#30340;&#29702;&#35299;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;What's In My Big Data?&#8221;&#65288;WIMBD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24179;&#21488;&#21644;&#19968;&#32452;&#21313;&#20845;&#20010;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25581;&#31034;&#21644;&#27604;&#36739;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#20869;&#23481;&#12290;WIMBD&#22522;&#20110;&#20004;&#31181;&#22522;&#26412;&#33021;&#21147;&#8212;&#8212;&#35745;&#25968;&#21644;&#25628;&#32034;&#8212;&#8212;&#22312;&#35268;&#27169;&#19978;&#36827;&#34892;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26631;&#20934;&#35745;&#31639;&#33410;&#28857;&#19978;&#20998;&#26512;&#36229;&#36807;35TB&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;WIMBD&#24212;&#29992;&#20110;&#29992;&#20110;&#35757;&#32451;&#27969;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#21313;&#20010;&#19981;&#21516;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;C4&#12289;The Pile&#21644;RedPajama&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20851;&#20110;&#36825;&#20123;&#35821;&#26009;&#24211;&#30340;&#20960;&#20010;&#20196;&#20154;&#24778;&#35766;&#19988;&#20197;&#21069;&#26410;&#35760;&#24405;&#30340;&#21457;&#29616;&#65292;&#21253;&#25324;&#37325;&#22797;&#20869;&#23481;&#12289;&#21512;&#25104;&#20869;&#23481;&#12289;&#20302;&#36136;&#37327;&#20869;&#23481;&#12289;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#12289;&#26377;&#27602;&#35821;&#35328;&#21644;&#22522;&#20934;&#27745;&#26579;&#30340;&#39640;&#27969;&#34892;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.20707v2 Announce Type: replace  Abstract: Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#26080;&#32541;&#36866;&#24212;Avalon&#28216;&#25103;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#27807;&#36890;&#21644;&#20114;&#21160;&#65292;&#35780;&#20272;&#20102;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#21644;&#31038;&#20250;&#34892;&#20026;&#65292;&#23637;&#31034;&#20102;LLM&#26234;&#33021;&#20307;&#22312;&#28216;&#25103;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2310.14985</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20307;&#31038;&#20250;&#34892;&#20026;&#30740;&#31350;&#65306;Avalon&#28216;&#25103;&#20013;&#30340;&#21327;&#20316;&#19982;&#23545;&#25239;
&lt;/p&gt;
&lt;p&gt;
LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#26080;&#32541;&#36866;&#24212;Avalon&#28216;&#25103;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#27807;&#36890;&#21644;&#20114;&#21160;&#65292;&#35780;&#20272;&#20102;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#21644;&#31038;&#20250;&#34892;&#20026;&#65292;&#23637;&#31034;&#20102;LLM&#26234;&#33021;&#20307;&#22312;&#28216;&#25103;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#25581;&#31034;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20307;&#31038;&#20250;&#34892;&#20026;&#30340;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#36798;&#21040;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;Avalon&#20316;&#20026;&#20195;&#34920;&#24615;&#30340;&#27807;&#36890;&#28216;&#25103;&#29615;&#22659;&#65292;&#24182;&#20351;&#29992;&#31995;&#32479;&#25552;&#31034;&#24341;&#23548;LLM&#26234;&#33021;&#20307;&#36827;&#34892;&#28216;&#25103;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#36827;&#34892;&#20102;&#20851;&#20110;LLM&#26234;&#33021;&#20307;&#30340;&#28216;&#25103;&#29609;&#27861;&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;&#20294;&#26159;&#20182;&#20204;&#30340;&#31038;&#20250;&#34892;&#20026;&#20173;&#32570;&#20047;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#26080;&#32541;&#36866;&#24212;Avalon&#28216;&#25103;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#29616;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26377;&#25928;&#27807;&#36890;&#21644;&#20114;&#21160;&#12290;&#25105;&#20204;&#26681;&#25454;&#20004;&#20010;&#35282;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#35780;&#20272;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#65306;&#36194;&#24471;&#28216;&#25103;&#21644;&#26356;&#20998;&#26512;LLM&#26234;&#33021;&#20307;&#30340;&#31038;&#20250;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#29983;&#25104;&#33258;&#36866;&#24212;&#21644;&#26234;&#33021;&#26234;&#33021;&#20307;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;LLM&#26234;&#33021;&#20307;&#22312;&#24212;&#23545;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14985v2 Announce Type: replace  Abstract: This paper aims to investigate the open research problem of uncovering the social behaviors of LLM-based agents. To achieve this goal, we adopt Avalon, a representative communication game, as the environment and use system prompts to guide LLM agents to play the game. While previous studies have conducted preliminary investigations into gameplay with LLM agents, there lacks research on their social behaviors. In this paper, we present a novel framework designed to seamlessly adapt to Avalon gameplay. The core of our proposed framework is a multi-agent system that enables efficient communication and interaction among agents. We evaluate the performance of our framework based on metrics from two perspectives: winning the game and analyzing the social behaviors of LLM agents. Our results demonstrate the effectiveness of our framework in generating adaptive and intelligent agents and highlight the potential of LLM-based agents in address
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32842;&#22825;&#21521;&#37327;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#22411;&#31639;&#26415;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#22312;&#26032;&#35821;&#35328;&#20013;&#36981;&#24490;&#25351;&#20196;&#21644;&#23454;&#29616;&#27169;&#22411;&#23545;&#40784;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2310.04799</link><description>&lt;p&gt;
Chat Vector&#65306;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#26032;&#35821;&#35328;&#20013;&#20026;LLMs&#25552;&#20379;&#25351;&#20196;&#36981;&#24490;&#21644;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32842;&#22825;&#21521;&#37327;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#22411;&#31639;&#26415;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#22312;&#26032;&#35821;&#35328;&#20013;&#36981;&#24490;&#25351;&#20196;&#21644;&#23454;&#29616;&#27169;&#22411;&#23545;&#40784;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#36805;&#36895;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30001;&#20110;&#25968;&#25454;&#32422;&#26463;&#65292;&#22823;&#22810;&#25968;&#24320;&#28304;LLM&#30340;&#33021;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32842;&#22825;&#21521;&#37327;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#22411;&#31639;&#26415;&#20026;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25351;&#20196;&#36981;&#24490;&#21644;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#12290;&#32842;&#22825;&#21521;&#37327;&#26159;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65288;&#20363;&#22914;LLaMA2&#65289;&#30340;&#26435;&#37325;&#20943;&#21435;&#20854;&#23545;&#24212;&#30340;&#32842;&#22825;&#27169;&#22411;&#65288;&#20363;&#22914;LLaMA2-chat&#65289;&#30340;&#26435;&#37325;&#24471;&#20986;&#30340;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558;&#32842;&#22825;&#21521;&#37327;&#28155;&#21152;&#21040;&#25345;&#32493;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#26032;&#35821;&#35328;&#20013;&#20855;&#26377;&#32842;&#22825;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#23637;&#31034;&#20102;&#32842;&#22825;&#21521;&#37327;&#22312;&#19977;&#20010;&#19981;&#21516;&#26041;&#38754;&#30340;&#20248;&#36234;&#26377;&#25928;&#24615;&#65306;&#25351;&#20196;&#36981;&#24490;&#12289;&#27602;&#24615;&#32531;&#35299;&#21644;&#22810;&#36718;&#23545;&#35805;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04799v2 Announce Type: replace  Abstract: Recently, the development of open-source large language models (LLMs) has advanced rapidly. Nevertheless, due to data constraints, the capabilities of most open-source LLMs are primarily focused on English. To address this issue, we introduce the concept of chat vector to equip pre-trained language models with instruction following and human value alignment via simple model arithmetic. The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained model's weights, we can endow the model with chat capabilities in new languages without the need for further training. Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase the adaptability of our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#20197;&#20943;&#23569;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#22495;&#20869;&#30417;&#30563;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2309.13734</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31435;&#22330;&#20998;&#31867;&#30340;&#25552;&#31034;&#21644;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#20197;&#20943;&#23569;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#22495;&#20869;&#30417;&#30563;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#20998;&#31867;&#26159;&#19968;&#20010;&#38271;&#26399;&#20197;&#26469;&#30740;&#31350;&#37325;&#28857;&#39046;&#22495;&#65292;&#20174;&#31038;&#20250;&#31185;&#23398;&#21040;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36825;&#39033;&#20219;&#21153;&#28041;&#21450;&#39044;&#27979;&#20316;&#32773;&#23545;&#24863;&#20852;&#36259;&#20027;&#39064;&#30340;&#35266;&#28857;&#12290;&#24403;&#21069;&#30340;&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#21477;&#23376;&#65292;&#28982;&#21518;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25163;&#21160;&#27880;&#37322;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#24037;&#20316;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#23427;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#27867;&#21270;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#21487;&#20197;&#20943;&#23569;&#29978;&#33267;&#28040;&#38500;&#25163;&#21160;&#27880;&#37322;&#38656;&#27714;&#30340;&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;10&#20010;&#24320;&#28304;&#27169;&#22411;&#21644;7&#31181;&#25552;&#31034;&#26041;&#26696;&#65292;&#21457;&#29616;LLMs&#22312;&#19982;&#22495;&#20869;&#30417;&#30563;&#27169;&#22411;&#20855;&#31454;&#20105;&#21147;&#65292;&#20294;&#24615;&#33021;&#24182;&#19981;&#19968;&#23450;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;LLMs&#30340;&#24494;&#35843;&#65292;&#20294;&#21457;&#29616;&#24494;&#35843;&#36807;&#31243;&#19981;&#24517;&#28982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13734v2 Announce Type: replace-cross  Abstract: Stance classification, the task of predicting the viewpoint of an author on a subject of interest, has long been a focal point of research in domains ranging from social science to machine learning. Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model. However, this manual annotation process requires laborious annotation effort, and thus hampers its potential to generalize across different contexts. In this work, we investigate the use of Large Language Models (LLMs) as a stance detection methodology that can reduce or even eliminate the need for manual annotations. We investigate 10 open-source models and 7 prompting schemes, finding that LLMs are competitive with in-domain supervised models but are not necessarily consistent in their performance. We also fine-tuned the LLMs, but discovered that fine-tuning process does not necessarily l
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.16553</link><description>&lt;p&gt;
SelectLLM&#65306;LLMs&#33021;&#21542;&#36873;&#25321;&#37325;&#35201;&#30340;&#25351;&#20196;&#36827;&#34892;&#27880;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
SelectLLM: Can LLMs Select Important Instructions to Annotate?. (arXiv:2401.16553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#20351;&#27169;&#22411;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#19968;&#23567;&#32452;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#21487;&#20197;&#36229;&#36807;&#20351;&#29992;&#22823;&#37327;&#26356;&#22024;&#26434;&#30340;&#25351;&#20196;&#12290;&#30001;&#20110;&#25351;&#20196;&#26159;&#26080;&#26631;&#31614;&#30340;&#65292;&#19988;&#21709;&#24212;&#26159;&#33258;&#28982;&#25991;&#26412;&#65292;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36873;&#25321;&#26080;&#26631;&#31614;&#25351;&#20196;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#65292;&#31216;&#20026;SelectLLM&#65292;&#23427;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#39640;&#32423;&#24605;&#24819;&#26159;&#21033;&#29992;LLMs&#36890;&#36807;&#25552;&#31034;&#26469;&#20272;&#35745;&#27599;&#20010;&#25351;&#20196;&#22312;&#27809;&#26377;&#30456;&#24212;&#26631;&#31614;&#65288;&#21363;&#21709;&#24212;&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#12290;SelectLLM&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#65288;&#20363;&#22914;CoreSet&#65289;&#23558;&#26080;&#26631;&#31614;&#25351;&#20196;&#21010;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#65292;&#28982;&#21518;&#25552;&#31034;LLMs&#22312;&#20854;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models (LLMs) with a large and diverse instruction dataset aligns the models to comprehend and follow human instructions. Recent works have shown that using a small set of high-quality instructions can outperform using large yet more noisy ones. Because instructions are unlabeled and their responses are natural text, traditional active learning schemes with the model's confidence cannot be directly applied to the selection of unlabeled instructions. In this work, we propose a novel method for instruction selection, called SelectLLM, that leverages LLMs for the selection of high-quality instructions. Our high-level idea is to use LLMs to estimate the usefulness and impactfulness of each instruction without the corresponding labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to multiple clusters, and then prompting LLMs to choose high-quality instructions within e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10712</link><description>&lt;p&gt;
Q&amp;A&#25552;&#31034;&#65306;&#36890;&#36807;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#25552;&#31034;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#28385;&#36275;&#23545;&#22810;&#26679;&#19990;&#30028;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#65292;&#22238;&#31572;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;AI&#27169;&#22411;&#37197;&#22791;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#30340;&#35748;&#30693;&#26041;&#26696;&#23578;&#26410;&#31995;&#32479;&#22320;&#34987;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30456;&#20449;&#65292;&#22914;&#26524;&#25105;&#20204;&#33021;&#23613;&#21487;&#33021;&#25910;&#38598;&#32473;&#23450;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#25105;&#20204;&#23558;&#33021;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#22270;&#20687;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#65292;&#26356;&#23481;&#26131;&#22238;&#24518;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#26368;&#32456;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#36825;&#20123;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#25552;&#31034;&#21457;&#36865;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22270;&#20687;-&#31572;&#26696;&#23545;&#21644;&#30456;&#24212;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35757;&#32451;&#19968;&#20010;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&amp;A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06401</link><description>&lt;p&gt;
DevEval: &#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#25552;&#20986;&#65292;&#20294;&#26159;&#19982;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#19981;&#19968;&#33268;&#65292;&#20363;&#22914;&#34394;&#26500;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20381;&#36182;&#19981;&#36275;&#21644;&#23567;&#35268;&#27169;&#39033;&#30446;&#32972;&#26223;&#12290;&#22240;&#27492;&#65292;LLMs&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#33021;&#21147;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#24320;&#21457;&#20154;&#21592;&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#32463;&#39564;&#30456;&#21563;&#21512;&#12290;DevEval&#36890;&#36807;&#19968;&#20010;&#20005;&#26684;&#30340;&#27969;&#31243;&#25910;&#38598;&#21040;&#20102;&#26469;&#33258;119&#20010;&#23454;&#38469;&#39033;&#30446;&#30340;2690&#20010;&#26679;&#26412;&#65292;&#28085;&#30422;10&#20010;&#39046;&#22495;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#19982;&#23454;&#38469;&#39033;&#30446;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#30495;&#23454;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#12290;&#25105;&#20204;&#22312;DevEval&#19978;&#35780;&#20272;&#20102;&#20116;&#20010;&#27969;&#34892;&#30340;LLMs&#65288;&#20363;&#22914;gpt-4&#65292;gpt-3.5-turbo&#65292;CodeLLaMa&#21644;StarCoder&#65289;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;gpt-3.5-turbo&#30340;&#26368;&#39640;Pass@1&#21482;&#26377;42&#12290;
&lt;/p&gt;
&lt;p&gt;
How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experim
&lt;/p&gt;</description></item><item><title>GoLLIE &#26159;&#19968;&#20010;&#36981;&#24490;&#27880;&#37322;&#25351;&#21335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#20197;&#25913;&#36827;&#26410;&#35265;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03668</link><description>&lt;p&gt;
GoLLIE:&#27880;&#37322;&#25351;&#21335;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction. (arXiv:2310.03668v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03668
&lt;/p&gt;
&lt;p&gt;
GoLLIE &#26159;&#19968;&#20010;&#36981;&#24490;&#27880;&#37322;&#25351;&#21335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#20197;&#25913;&#36827;&#26410;&#35265;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#32467;&#21512;&#25351;&#23548;&#35843;&#20248;&#24050;&#32463;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462; (IE) &#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#33853;&#21518;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;IE &#20219;&#21153;&#30340;&#29305;&#28857;&#26159;&#22797;&#26434;&#30340;&#27880;&#37322;&#25351;&#21335;&#65292;&#25551;&#36848;&#20219;&#21153;&#24182;&#32473;&#20986;&#31034;&#20363;&#32473;&#20154;&#31867;&#12290;&#20808;&#21069;&#21033;&#29992;&#36825;&#26679;&#30340;&#20449;&#24687;&#30340;&#23581;&#35797;&#37117;&#22833;&#36133;&#20102;&#65292;&#21363;&#20351;&#20351;&#29992;&#26368;&#22823;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#20063;&#19981;&#33021;&#30452;&#25509;&#36981;&#24490;&#25351;&#21335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20449;&#24687;&#25277;&#21462;&#30340;&#25351;&#21335;&#36981;&#24490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; GoLLIE (Guideline-following Large Language Model for IE)&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24494;&#35843;&#20197;&#36981;&#23432;&#27880;&#37322;&#25351;&#21335;&#65292;&#20174;&#32780;&#33021;&#22815;&#25913;&#36827;&#26410;&#35265; IE &#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;&#20840;&#38754;&#30340;&#35780;&#20272;&#23454;&#35777;&#34920;&#26126;&#65292;GoLLIE &#33021;&#22815;&#27867;&#21270;&#24182;&#36981;&#24490;&#26410;&#35265;&#25351;&#21335;&#65292;&#22312;&#38646;&#26679;&#26412;&#20449;&#24687;&#25277;&#21462;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#23581;&#35797;&#12290;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#35814;&#32454;&#30340;&#25351;&#21335;&#26159;&#21462;&#24471;&#33391;&#22909;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines which describe the task and give examples to humans. Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out-of-the-box. In this paper we propose GoLLIE (Guideline-following Large Language Model for IE), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines. Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed guidelines is key for good results.
&lt;/p&gt;</description></item><item><title>OATS&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#35266;&#28857;&#26041;&#38754;&#30446;&#26631;&#24773;&#24863;&#22235;&#20803;&#32452;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#23427;&#35299;&#20915;&#20102;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#39046;&#22495;&#38480;&#21046;&#21644;&#25968;&#25454;&#31890;&#24230;&#25361;&#25112;&#65292;&#24182;&#22635;&#34917;&#20102;&#39184;&#39302;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#31561;&#24120;&#35265;&#39046;&#22495;&#30340;&#25968;&#25454;&#19981;&#36275;&#21644;&#21477;&#23376;&#19982;&#35780;&#35770;&#32423;&#24773;&#24863;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13297</link><description>&lt;p&gt;
OATS: &#35266;&#28857;&#26041;&#38754;&#30446;&#26631;&#24773;&#24863;&#22235;&#20803;&#32452;&#25277;&#21462;&#25968;&#25454;&#38598;&#29992;&#20110;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
OATS: Opinion Aspect Target Sentiment Quadruple Extraction Dataset for Aspect-Based Sentiment Analysis. (arXiv:2309.13297v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13297
&lt;/p&gt;
&lt;p&gt;
OATS&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#35266;&#28857;&#26041;&#38754;&#30446;&#26631;&#24773;&#24863;&#22235;&#20803;&#32452;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#23427;&#35299;&#20915;&#20102;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#39046;&#22495;&#38480;&#21046;&#21644;&#25968;&#25454;&#31890;&#24230;&#25361;&#25112;&#65292;&#24182;&#22635;&#34917;&#20102;&#39184;&#39302;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#31561;&#24120;&#35265;&#39046;&#22495;&#30340;&#25968;&#25454;&#19981;&#36275;&#21644;&#21477;&#23376;&#19982;&#35780;&#35770;&#32423;&#24773;&#24863;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26088;&#22312;&#29702;&#35299;&#25991;&#26412;&#20869;&#23481;&#20013;&#29305;&#23450;&#35201;&#32032;&#30340;&#24773;&#24863;&#12290;&#23427;&#26088;&#22312;&#20998;&#26512;&#29992;&#25143;&#29983;&#25104;&#30340;&#35780;&#35770;&#65292;&#30830;&#23450;a) &#34987;&#35780;&#35770;&#30340;&#30446;&#26631;&#23454;&#20307;&#65292;b) &#23427;&#25152;&#23646;&#30340;&#39640;&#32423;&#26041;&#38754;&#65292;c) &#29992;&#20110;&#34920;&#36798;&#35266;&#28857;&#30340;&#24773;&#24863;&#35789;&#65292;d) &#23545;&#30446;&#26631;&#21644;&#26041;&#38754;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;&#23613;&#31649;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#25512;&#21160;&#20102;ABSA&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24102;&#26469;&#39046;&#22495;&#38480;&#21046;&#21644;&#25968;&#25454;&#31890;&#24230;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OATS&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#19977;&#20010;&#20840;&#26032;&#30340;&#39046;&#22495;&#65292;&#24182;&#21253;&#21547;20,000&#20010;&#21477;&#23376;&#32423;&#22235;&#20803;&#32452;&#21644;13,000&#20010;&#35780;&#35770;&#32423;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22635;&#34917;&#19968;&#20123;&#29305;&#23450;&#30340;&#35266;&#23519;&#21040;&#30340;&#24046;&#36317;&#65306;&#23545;&#29087;&#24713;&#39046;&#22495;&#65288;&#22914;&#39184;&#39302;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#65289;&#30340;&#21453;&#22797;&#20851;&#27880;&#65292;&#29992;&#20110;&#22797;&#26434;&#22235;&#20803;&#32452;&#25277;&#21462;&#20219;&#21153;&#30340;&#26377;&#38480;&#25968;&#25454;&#65292;&#20197;&#21450;&#20598;&#23572;&#24573;&#35270;&#21477;&#23376;&#21644;&#35780;&#35770;&#32423;&#24773;&#24863;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#38416;&#26126;OATS&#30340;&#28508;&#22312;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment Analysis (ABSA) delves into understanding sentiments specific to distinct elements within textual content. It aims to analyze user-generated reviews to determine a) the target entity being reviewed, b) the high-level aspect to which it belongs, c) the sentiment words used to express the opinion, and d) the sentiment expressed toward the targets and the aspects. While various benchmark datasets have fostered advancements in ABSA, they often come with domain limitations and data granularity challenges. Addressing these, we introduce the OATS dataset, which encompasses three fresh domains and consists of 20,000 sentence-level quadruples and 13,000 review-level tuples. Our initiative seeks to bridge specific observed gaps: the recurrent focus on familiar domains like restaurants and laptops, limited data for intricate quadruple extraction tasks, and an occasional oversight of the synergy between sentence and review-level sentiments. Moreover, to elucidate OATS's pote
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#37319;&#29992;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09778</link><description>&lt;p&gt;
&#36861;&#27714;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22522;&#20110;&#23454;&#38469;&#30340;&#35270;&#35273;&#31354;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models. (arXiv:2308.09778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#37319;&#29992;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#21508;&#31181;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#35745;&#25968;&#12289;&#25351;&#28041;&#34920;&#36798;&#21644;&#19968;&#33324;&#30340;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65289;&#19978;&#30340;&#34920;&#29616;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#65292;&#20154;&#20204;&#23581;&#35797;&#20351;&#29992;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#65288;Liu, Emerson, and Collier 2022) &#25110;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#26469;&#22788;&#29702;&#27492;&#38382;&#39064;&#65292;&#20294;&#37117;&#34920;&#29616;&#20986;&#24615;&#33021;&#19981;&#20339;&#24182;&#19988;&#19982;&#20154;&#31867;&#24615;&#33021;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#26469;&#23545;&#31354;&#38388;&#20174;&#21477;&#36827;&#34892;&#25490;&#21517;&#24182;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#32467;&#21512;&#21644;&#22320;&#38754;&#21270;&#29289;&#20307;&#23545;&#24212;&#30340;&#21517;&#35789;&#30701;&#35821;&#21644;&#23427;&#20204;&#30340;&#20301;&#32622;&#30340;&#35777;&#25454;&#26469;&#35745;&#31639;&#31354;&#38388;&#20174;&#21477;&#30340;&#26368;&#32456;&#25490;&#21517;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advances in large scale vision-and-language models (VLMs) it is of interest to assess their performance on various visual reasoning tasks such as counting, referring expressions and general visual question answering. The focus of this work is to study the ability of these models to understanding spatial relations. Previously, this has been tackled using image-text matching (Liu, Emerson, and Collier 2022) or visual question answering task, both showing poor performance and a large gap compared to human performance. To better understand the gap, we present fine-grained compositional grounding of spatial relationships and propose a bottom up approach for ranking spatial clauses and evaluating the performance of spatial relationship reasoning task. We propose to combine the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative vision-language models (Tan and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20869;&#37096;&#30340;&#20540;&#31354;&#38388;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01154</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#36816;&#31639;&#65306;&#20174;&#35760;&#24518;&#21040;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Arithmetic with Language Models: from Memorization to Computation. (arXiv:2308.01154v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20869;&#37096;&#30340;&#20540;&#31354;&#38388;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26356;&#22909;&#22320;&#29702;&#35299;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#24615;&#35745;&#31639;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#23545;&#20110;&#36827;&#19968;&#27493;&#25913;&#36827;&#23427;&#20204;&#24182;&#25299;&#23485;&#20854;&#36866;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22312;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#12290;&#20108;&#36827;&#21046;&#21152;&#27861;&#21644;&#20056;&#27861;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#27979;&#35797;&#22522;&#30784;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#38750;&#24120;&#23567;&#30340;&#35789;&#27719;&#34920;&#65292;&#24182;&#19988;&#22312;&#36755;&#20837;/&#36755;&#20986;&#19978;&#23637;&#31034;&#20102;&#30456;&#20851;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#20351;&#24471;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#24179;&#28369;&#30340;&#36755;&#20837;&#25554;&#20540;&#26080;&#25928;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#23398;&#20064;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20854;&#22806;&#25512;&#33021;&#21147;&#21644;&#20869;&#37096;&#20449;&#24687;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#36825;&#26679;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#32534;&#30721;-&#22238;&#24402;-&#35299;&#30721;&#26426;&#22120;&#65292;&#19968;&#26086;&#23558;&#36755;&#20837;&#26631;&#35760;&#34920;&#31034;&#26144;&#23556;&#21040;&#21512;&#36866;&#30340;&#20869;&#37096;&#20540;&#31354;&#38388;&#65292;&#35745;&#31639;&#23601;&#22312;&#20540;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypotheses that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate intern
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00071</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#26041;&#27861;&#29992;&#20110;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20351;&#29992;&#20102;&#21253;&#21547;&#22266;&#26377;&#20559;&#35265;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#25345;&#32493;&#31995;&#32479;&#24615;&#27495;&#35270;&#65292;&#22240;&#27492;&#65292;&#23457;&#26597;&#21644;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23558;&#20844;&#24179;&#24615;&#25972;&#21512;&#21040;&#23427;&#20204;&#30340;&#21457;&#23637;&#20013;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20844;&#27491;&#21644;&#26080;&#20559;&#30340;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Vicuna-13B-v1.3&#30340;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#25105;&#20204;&#35266;&#23519;&#21040;&#20174;13B&#21040;33B&#30340;&#35268;&#27169;&#25193;&#23637;&#20250;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25512;&#29702;&#21487;&#33021;&#26159;&#20351;LLMs&#22312;&#21051;&#26495;&#21360;&#35937;&#31561;&#39046;&#22495;&#20219;&#21153;&#19978;&#36229;&#36234;&#35268;&#27169;&#23450;&#24459;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#36873;&#23450;&#30340;&#25512;&#29702;&#36861;&#36394;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#31361;&#20986;&#26174;&#31034;&#20102;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#65292;&#24182;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09329</link><description>&lt;p&gt;
BERTTM: &#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#36827;&#34892;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
BERTTM: Leveraging Contextualized Word Embeddings from Pre-trained Language Models for Neural Topic Modeling. (arXiv:2305.09329v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#65292;&#24182;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20027;&#39064;&#24314;&#27169;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#25198;&#28436;&#30528;&#26085;&#30410;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20027;&#39064;&#27169;&#22411;&#20173;&#28982;&#20381;&#36182;&#20110;&#35789;&#34955;&#65288;BoW&#65289;&#20449;&#24687;&#65292;&#26080;&#35770;&#26159;&#20316;&#20026;&#35757;&#32451;&#36755;&#20837;&#36824;&#26159;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#25417;&#25991;&#26723;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#23548;&#33268;&#23427;&#20204;&#22312;&#22788;&#29702;&#26032;&#25991;&#26723;&#20013;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#21333;&#35789;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#22312;&#35789;&#20041;&#28040;&#27495;&#30340;&#33021;&#21147;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;OOV&#21333;&#35789;&#26102;&#26159;&#26377;&#25928;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of neural topic models in recent years, topic modelling is playing an increasingly important role in natural language understanding. However, most existing topic models still rely on bag-of-words (BoW) information, either as training input or training target. This limits their ability to capture word order information in documents and causes them to suffer from the out-of-vocabulary (OOV) issue, i.e. they cannot handle unobserved words in new documents. Contextualized word embeddings from pre-trained language models show superiority in the ability of word sense disambiguation and prove to be effective in dealing with OOV words. In this work, we developed a novel neural topic model combining contextualized word embeddings from the pre-trained language model BERT. The model can infer the topic distribution of a document without using any BoW information. In addition, the model can infer the topic distribution of each word in a document directly from the contextualize
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.11525</link><description>&lt;p&gt;
SIFT: &#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#20197;&#26368;&#22823;&#38480;&#24230;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#25928;&#29575;&#65288;&#19982;&#35757;&#32451;FLOPS&#30456;&#20851;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#65289;&#12290; &#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;FLOP&#65292;&#20294;&#20351;&#29992;&#31232;&#30095;&#26435;&#37325;&#36827;&#34892;&#35757;&#32451;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#25439;&#22833;&#25110;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#21608;&#26399;&#65292;&#20351;&#24471;&#32467;&#26524;&#30340;&#35757;&#32451;&#25928;&#29575;&#19981;&#22815;&#28165;&#26224;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#31232;&#30095;&#24615;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#21516;&#30340;FLOPS&#65292;&#24182;&#36890;&#36807;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#23637;&#31034;&#35757;&#32451;&#25928;&#29575;&#25552;&#39640;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SIFT&#65292;&#19968;&#32452;&#29992;&#20316;&#23494;&#38598;&#23618;&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#20195;&#21697;&#26469;&#25552;&#39640;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;FLOP&#25928;&#29575;&#30340;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#12290; &#27599;&#20010;&#36716;&#25442;&#37117;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#65288;&#31232;&#30095;&#32423;&#21035;&#65289;&#21442;&#25968;&#21270;&#65292;&#24182;&#25552;&#20379;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31232;&#30095;&#25513;&#33180;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of weight sparsity to improve the training efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs). These works aim to reduce training FLOPs but training with sparse weights often leads to accuracy loss or requires longer train schedules, making the resulting training efficiency less clear. In contrast, we focus on using sparsity to increase accuracy while using the same FLOPS as the dense model and show training efficiency gains through higher accuracy. In this work, we introduce SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in replacements for dense layers to improve their representational capacity and FLOP efficiency. Each transformation is parameterized by a single parameter (sparsity level) and provides a larger search space to find optimal sparse masks. Without changing any training hyperparameters, replacing dense layers with SIFT leads to significant improvements across computer vision (CV) and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.11155</link><description>&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#22312;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification. (arXiv:2203.11155v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11155
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#34920;&#31034;&#25972;&#20010;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#23558;&#23494;&#24230;&#30697;&#38453;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#20219;&#21153;&#21487;&#20197;&#26356;&#21152;&#26377;&#25928;&#22320;&#23454;&#29616;&#38382;&#39064;&#22238;&#31572;&#12290;&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#26032;&#26426;&#21046;&#65292;&#20197;&#24212;&#23545;&#36755;&#20837;&#20026;&#30697;&#38453;&#30340;&#24773;&#20917;&#65292;&#24182;&#23558;&#35813;&#26426;&#21046;&#24212;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;QA&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#20197;&#22686;&#24378;&#32463;&#20856;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#29305;&#24449;&#20449;&#24687;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26032;&#26694;&#26550;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;CNN&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum density matrix represents all the information of the entire quantum system, and novel models of meaning employing density matrices naturally model linguistic phenomena such as hyponymy and linguistic ambiguity, among others in quantum question answering tasks. Naturally, we argue that applying the quantum density matrix into classical Question Answering (QA) tasks can show more effective performance. Specifically, we (i) design a new mechanism based on Long Short-Term Memory (LSTM) to accommodate the case when the inputs are matrixes; (ii) apply the new mechanism to QA problems with Convolutional Neural Network (CNN) and gain the LSTM-based QA model with the quantum density matrix. Experiments of our new model on TREC-QA and WIKI-QA data sets show encouraging results. Similarly, we argue that the quantum density matrix can also enhance the image feature information and the relationship between the features for the classical image classification. Thus, we (i) combine density mat
&lt;/p&gt;</description></item></channel></rss>