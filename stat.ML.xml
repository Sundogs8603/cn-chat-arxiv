<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01454</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#19968;&#31181;&#32479;&#35745;&#22240;&#26524;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#65288;SCD&#65289;&#20013;&#65292;&#23558;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#20316;&#20026;&#32422;&#26463;&#23884;&#20837;&#21040;&#31639;&#27861;&#20013;&#34987;&#24191;&#27867;&#25509;&#21463;&#65292;&#22240;&#20026;&#36825;&#23545;&#20110;&#21019;&#24314;&#19968;&#33268;&#26377;&#24847;&#20041;&#30340;&#22240;&#26524;&#27169;&#22411;&#26159;&#37325;&#35201;&#30340;&#65292;&#23613;&#31649;&#35782;&#21035;&#32972;&#26223;&#30693;&#35782;&#30340;&#25361;&#25112;&#34987;&#35748;&#21487;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23558;LLM&#30340;&#8220;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#65288;SCP&#65289;&#8221;&#19982;SCD&#26041;&#27861;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#22240;&#26524;&#25512;&#26029;&#65288;KBCI&#65289;&#30456;&#32467;&#21512;&#65292;&#23545;SCD&#36827;&#34892;&#20808;&#39564;&#30693;&#35782;&#22686;&#24378;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4&#21487;&#20197;&#20351;LLM-KBCI&#30340;&#36755;&#20986;&#19982;&#24102;&#26377;LLM-KBCI&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;SCD&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#65292;&#22914;&#26524;GPT-4&#32463;&#21382;&#20102;SCP&#65292;&#37027;&#20040;SCD&#30340;&#32467;&#26524;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#12290;&#32780;&#19988;&#65292;&#21363;&#20351;LLM&#19981;&#21547;&#26377;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#65292;LLM&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20854;&#32972;&#26223;&#30693;&#35782;&#26469;&#25913;&#36827;SCD&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#32447;&#24615;&#21270;Laplace&#36817;&#20284;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#21407;&#22987;DNN&#30340;&#39044;&#27979;&#22343;&#20540;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#35757;&#32451;&#25104;&#26412;&#19982;&#35757;&#32451;&#28857;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;</title><link>https://rss.arxiv.org/abs/2302.12565</link><description>&lt;p&gt;
&#21464;&#20998;&#32447;&#24615;&#21270;Laplace&#36817;&#20284;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Variational Linearized Laplace Approximation for Bayesian Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2302.12565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#32447;&#24615;&#21270;Laplace&#36817;&#20284;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#21407;&#22987;DNN&#30340;&#39044;&#27979;&#22343;&#20540;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#35757;&#32451;&#25104;&#26412;&#19982;&#35757;&#32451;&#28857;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#32447;&#24615;&#21270;Laplace&#36817;&#20284;&#65288;LLA&#65289;&#34987;&#29992;&#26469;&#23545;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#39044;&#27979;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#28857;&#25110;DNN&#21442;&#25968;&#36739;&#22810;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20854;&#20182;LLA&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#22914;Kronecker&#20998;&#35299;&#25110;&#23545;&#35282;&#32447;GGN&#30697;&#38453;&#30340;&#36817;&#20284;&#65292;&#34987;&#20351;&#29992;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#30340;LLA&#36817;&#20284;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;GP&#30340;&#23545;&#20598;RKHS&#20844;&#24335;&#65292;&#24182;&#20445;&#30041;&#20102;&#21407;&#22987;DNN&#30340;&#39044;&#27979;&#22343;&#20540;&#12290;&#27492;&#22806;&#65292;&#23427;&#20801;&#35768;&#26377;&#25928;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20013;&#23454;&#29616;&#23376;&#32447;&#24615;&#35757;&#32451;&#26102;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20854;&#35757;&#32451;&#25104;&#26412;&#19982;&#35757;&#32451;&#28857;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#36817;&#20284;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Linearized Laplace Approximation (LLA) has been recently used to perform uncertainty estimation on the predictions of pre-trained deep neural networks (DNNs). However, its widespread application is hindered by significant computational costs, particularly in scenarios with a large number of training points or DNN parameters. Consequently, additional approximations of LLA, such as Kronecker-factored or diagonal approximate GGN matrices, are utilized, potentially compromising the model's performance. To address these challenges, we propose a new method for approximating LLA using a variational sparse Gaussian Process (GP). Our method is based on the dual RKHS formulation of GPs and retains as the predictive mean the output of the original DNN. Furthermore, it allows for efficient stochastic optimization, which results in sub-linear training time in the size of the training dataset. Specifically, its training cost is independent of the number of training points. We compare our propose
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{MultiDimSPCI}$&#30340;&#39034;&#24207;CP&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#26500;&#24314;&#39044;&#27979;&#21306;&#22495;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#39044;&#27979;&#21306;&#22495;&#21644;&#26377;&#25928;&#30340;&#35206;&#30422;&#12290;</title><link>https://arxiv.org/abs/2403.03850</link><description>&lt;p&gt;
&#21033;&#29992;&#26925;&#29699;&#38598;&#36827;&#34892;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#30340;&#21512;&#35268;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction for multi-dimensional time series by ellipsoidal sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03850
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{MultiDimSPCI}$&#30340;&#39034;&#24207;CP&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#26500;&#24314;&#39044;&#27979;&#21306;&#22495;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#39044;&#27979;&#21306;&#22495;&#21644;&#26377;&#25928;&#30340;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#35268;&#39044;&#27979;&#65288;CP&#65289;&#22240;&#20854;&#26080;&#38656;&#20551;&#35774;&#20998;&#24067;&#12289;&#19981;&#21463;&#27169;&#22411;&#38480;&#21046;&#19988;&#22312;&#29702;&#35770;&#19978;&#21487;&#38752;&#32780;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;CP&#26041;&#27861;&#19987;&#27880;&#20110;&#20026;&#21333;&#21464;&#37327;&#21709;&#24212;&#26500;&#24314;&#39044;&#27979;&#21306;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{MultiDimSPCI}$&#30340;&#39034;&#24207;CP&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22810;&#20803;&#21709;&#24212;&#26500;&#24314;&#39044;&#27979;&#21306;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21487;&#20132;&#25442;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29615;&#22659;&#20013;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#26465;&#20214;&#35206;&#30422;&#38388;&#38553;&#30340;&#26377;&#38480;&#26679;&#26412;&#39640;&#27010;&#29575;&#30028;&#38480;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;$\texttt{MultiDimSPCI}$&#22312;&#21508;&#31181;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#19978;&#20445;&#25345;&#26377;&#25928;&#35206;&#30422;&#65292;&#21516;&#26102;&#20135;&#29983;&#27604;CP&#21644;&#38750;CP&#22522;&#32447;&#26356;&#23567;&#30340;&#39044;&#27979;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03850v1 Announce Type: cross  Abstract: Conformal prediction (CP) has been a popular method for uncertainty quantification because it is distribution-free, model-agnostic, and theoretically sound. For forecasting problems in supervised learning, most CP methods focus on building prediction intervals for univariate responses. In this work, we develop a sequential CP method called $\texttt{MultiDimSPCI}$ that builds prediction regions for a multivariate response, especially in the context of multivariate time series, which are not exchangeable. Theoretically, we estimate finite-sample high-probability bounds on the conditional coverage gap. Empirically, we demonstrate that $\texttt{MultiDimSPCI}$ maintains valid coverage on a wide range of multivariate time series while producing smaller prediction regions than CP and non-CP baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#19968;&#20010;&#29305;&#23450;&#30340;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#31361;&#20986;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#65292;&#21516;&#26102;&#20855;&#26377;&#24378;&#22823;&#29983;&#25104;&#22120;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.02957</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#28176;&#36817;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#19968;&#20010;&#29305;&#23450;&#30340;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#31361;&#20986;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#65292;&#21516;&#26102;&#20855;&#26377;&#24378;&#22823;&#29983;&#25104;&#22120;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#21435;&#22122;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#29702;&#35770;&#29702;&#35299;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#29305;&#23450;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65288;CME&#65289;&#65292;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;&#30740;&#31350;&#30340;&#22522;&#20110;DPM&#30340;&#21435;&#22122;&#22120;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19982;DPMs&#20849;&#20139;&#65292;&#20294;&#22312;&#35757;&#32451;&#21518;&#30340;&#36870;&#25512;&#29702;&#36807;&#31243;&#20013;&#20165;&#20256;&#36882;&#26465;&#20214;&#22343;&#20540;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#30340;&#29420;&#29305;&#35270;&#35282;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;&#36870;&#36807;&#31243;&#20013;&#20999;&#25442;&#37325;&#26032;&#37319;&#26679;&#30340;&#26041;&#24335;&#32487;&#25215;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#29983;&#25104;&#22120;&#12290;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#39564;&#35777;&#20102;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02957v1 Announce Type: new  Abstract: Diffusion probabilistic models (DPMs) have recently shown great potential for denoising tasks. Despite their practical utility, there is a notable gap in their theoretical understanding. This paper contributes novel theoretical insights by rigorously proving the asymptotic convergence of a specific DPM denoising strategy to the mean square error (MSE)-optimal conditional mean estimator (CME) over a large number of diffusion steps. The studied DPM-based denoiser shares the training procedure of DPMs but distinguishes itself by forwarding only the conditional mean during the reverse inference process after training. We highlight the unique perspective that DPMs are composed of an asymptotically optimal denoiser while simultaneously inheriting a powerful generator by switching re-sampling in the reverse process on and off. The theoretical findings are validated by numerical results.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.17826</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Prediction-Powered Ranking of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#26681;&#25454;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#36827;&#34892;&#25490;&#21517;--&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#21463;&#20154;&#31867;&#20559;&#22909;&#65292;&#37027;&#20040;&#23427;&#23601;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#26469;&#24357;&#21512;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#21487;&#33021;&#24341;&#20837;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17826v1 Announce Type: cross  Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provid
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#39318;&#27425;&#37319;&#29992;&#21464;&#21387;&#22120;&#39044;&#27979;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;CPLEX&#21644;LSTM&#12290;</title><link>https://arxiv.org/abs/2402.13380</link><description>&lt;p&gt;
&#36808;&#21521;&#21464;&#21387;&#22120;&#65306;&#29992;&#21464;&#21387;&#22120;&#24443;&#24213;&#25913;&#21464;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13380
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#39318;&#27425;&#37319;&#29992;&#21464;&#21387;&#22120;&#39044;&#27979;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;CPLEX&#21644;LSTM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#23481;&#37327;&#38480;&#21046;&#25209;&#37327;&#29983;&#20135;&#38382;&#39064;&#65288;CLSP&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#21033;&#29992;&#21464;&#21387;&#22120;&#26469;&#39044;&#27979;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#22788;&#29702;&#39034;&#24207;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#38750;&#24120;&#36866;&#21512;&#39044;&#27979;&#27599;&#20010;CLSP&#21608;&#26399;&#20013;&#34920;&#31034;&#29983;&#20135;&#35774;&#32622;&#20915;&#31574;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#12290;&#36825;&#20010;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#21160;&#24577;&#30340;&#65292;&#25105;&#20204;&#38656;&#35201;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;CLSP&#35299;&#20915;&#26041;&#26696;&#12290;&#25152;&#25552;&#20986;&#30340;&#21518;&#22788;&#29702;&#21464;&#21387;&#22120;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;CPLEX&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13380v1 Announce Type: new  Abstract: In this study, we introduce an innovative deep learning framework that employs a transformer model to address the challenges of mixed-integer programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP). Our approach, to our knowledge, is the first to utilize transformers to predict the binary variables of a mixed-integer programming (MIP) problem. Specifically, our approach harnesses the encoder decoder transformer's ability to process sequential data, making it well-suited for predicting binary variables indicating production setup decisions in each period of the CLSP. This problem is inherently dynamic, and we need to handle sequential decision making under constraints. We present an efficient algorithm in which CLSP solutions are learned through a transformer neural network. The proposed post-processed transformer algorithm surpasses the state-of-the-art solver, CPLEX and Long Short-Term Memory (LSTM) in solution time
&lt;/p&gt;</description></item><item><title>&#24605;&#32500;&#38142;&#36171;&#20104;&#21464;&#21387;&#22120;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12875</link><description>&lt;p&gt;
&#24605;&#32500;&#38142;&#28608;&#21457;&#21464;&#21387;&#22120;&#35299;&#20915;&#22266;&#26377;&#20018;&#34892;&#38382;&#39064;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought Empowers Transformers to Solve Inherently Serial Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12875
&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#36171;&#20104;&#21464;&#21387;&#22120;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#20013;&#38388;&#27493;&#39588;&#65292;&#21363;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#20934;&#30830;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;CoT&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#19981;&#28165;&#26970;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#34920;&#36798;&#24615;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#23545;&#35299;&#30721;&#22120;&#19987;&#29992;&#21464;&#21387;&#22120;&#30340;CoT&#33021;&#21147;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#27010;&#24565;&#19978;&#65292;CoT&#36171;&#20104;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#31181;&#33021;&#21147;&#22312;&#21464;&#21387;&#22120;&#20013;&#32570;&#20047;&#65292;&#29305;&#21035;&#26159;&#24403;&#28145;&#24230;&#36739;&#20302;&#26102;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#24050;&#32463;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;CoT&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26377;&#38480;&#31934;&#24230;$\mathsf{poly}(n)$&#23884;&#20837;&#23610;&#23544;&#30340;&#24658;&#23450;&#28145;&#24230;&#21464;&#21387;&#22120;&#21482;&#33021;&#22312;$\mathsf{TC}^0$&#20013;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20855;&#26377;&#24120;&#25968;&#20301;&#31934;&#24230;&#30340;&#24658;&#23450;&#28145;&#24230;&#21464;&#21387;&#22120;&#30340;&#26356;&#32039;&#23494;&#30340;&#34920;&#36798;&#24615;&#19978;&#30028;&#65292;&#23427;&#21482;&#33021;&#35299;&#20915;$\mathsf{AC}^0$&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12875v1 Announce Type: new  Abstract: Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\mathsf{poly}(n)$ embedding size can only solve problems in $\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\mathsf{AC}^0$, a 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#26631;&#27880;&#20989;&#25968;&#30340;&#21327;&#20316;&#23398;&#20064;&#20013;&#65292;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#22686;&#24378;&#20551;&#35774;&#31867;&#19978;&#30340;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10445</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#26631;&#27880;&#20989;&#25968;&#30340;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collaborative Learning with Different Labeling Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10445
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#26631;&#27880;&#20989;&#25968;&#30340;&#21327;&#20316;&#23398;&#20064;&#20013;&#65292;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#22686;&#24378;&#20551;&#35774;&#31867;&#19978;&#30340;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181; Collaborative PAC Learning &#30340;&#21464;&#20307;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26088;&#22312;&#23398;&#20064;&#27599;&#20010;$n$&#20010;&#25968;&#25454;&#20998;&#24067;&#30340;&#20934;&#30830;&#20998;&#31867;&#22120;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20174;&#23427;&#20204;&#24635;&#20849;&#25277;&#21462;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#19982;&#36890;&#24120;&#30340;&#21327;&#20316;&#23398;&#20064;&#35774;&#32622;&#19981;&#21516;&#65292;&#19981;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#21516;&#26102;&#23545;&#25152;&#26377;&#20998;&#24067;&#20934;&#30830;&#30340;&#21333;&#19968;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#28385;&#36275;&#36739;&#24369;&#30340;&#21487;&#23454;&#29616;&#24615;&#20551;&#35774;&#26102;&#65292;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#20551;&#35774;&#31867;&#30340;&#19968;&#20010;&#33258;&#28982;&#22686;&#24378;&#65292;&#20998;&#26512;&#20381;&#36182;&#20110;&#23545;&#35813;&#22686;&#24378;&#31867;&#30340;VC&#32500;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10445v1 Announce Type: new  Abstract: We study a variant of Collaborative PAC Learning, in which we aim to learn an accurate classifier for each of the $n$ data distributions, while minimizing the number of samples drawn from them in total. Unlike in the usual collaborative learning setup, it is not assumed that there exists a single classifier that is simultaneously accurate for all distributions.   We show that, when the data distributions satisfy a weaker realizability assumption, sample-efficient learning is still feasible. We give a learning algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation of the hypothesis class, and the analysis relies on an upper bound on the VC dimension of this augmented class.   In terms of the computational efficiency, we show that ERM on the augmented hypothesis class is NP-hard, which gives evidence against the existence of computationally efficient learners in general. On the positive side, for two special cases, 
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65288;GEnBP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#39640;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.08193</link><description>&lt;p&gt;
&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#30340;&#39640;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08193
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65288;GEnBP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#39640;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#25512;&#26029;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#65288;GEnBP&#65289;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#65288;GaBP&#65289;&#26041;&#27861;&#30340;&#32467;&#21512;&#12290;GEnBP&#36890;&#36807;&#22312;&#22270;&#27169;&#22411;&#32467;&#26500;&#20013;&#20256;&#36882;&#20302;&#31209;&#26412;&#22320;&#20449;&#24687;&#26469;&#26356;&#26032;&#38598;&#25104;&#27169;&#22411;&#12290;&#36825;&#31181;&#32452;&#21512;&#32487;&#25215;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26377;&#21033;&#29305;&#24615;&#12290;&#38598;&#25104;&#25216;&#26415;&#20351;&#24471;GEnBP&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#12289;&#22024;&#26434;&#30340;&#40657;&#31665;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#22270;&#27169;&#22411;&#32467;&#26500;&#20013;&#20351;&#29992;&#26412;&#22320;&#20449;&#24687;&#30830;&#20445;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#24182;&#33021;&#39640;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;&#24403;&#38598;&#25104;&#22823;&#23567;&#36828;&#23567;&#20110;&#25512;&#26029;&#32500;&#24230;&#26102;&#65292;GEnBP&#29305;&#21035;&#26377;&#20248;&#21183;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#31354;&#26102;&#24314;&#27169;&#12289;&#22270;&#20687;&#22788;&#29702;&#21644;&#29289;&#29702;&#27169;&#22411;&#21453;&#28436;&#31561;&#39046;&#22495;&#32463;&#24120;&#20986;&#29616;&#12290;GEnBP&#21487;&#20197;&#24212;&#29992;&#20110;&#19968;&#33324;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient inference in high-dimensional models remains a central challenge in machine learning. This paper introduces the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing low-rank local messages in a graphical model structure. This combination inherits favourable qualities from each method. Ensemble techniques allow GEnBP to handle high-dimensional states, parameters and intricate, noisy, black-box generation processes. The use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures. GEnBP is particularly advantageous when the ensemble size is considerably smaller than the inference dimension. This scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion. GEnBP can be applied to general problem s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05569</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Node Classification With Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#25104;&#23545;&#20132;&#20114;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21457;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#25968;&#25454;&#30340;&#24819;&#27861;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HyperGNNs&#65289;&#30340;&#21457;&#23637;&#12290;GNNs&#21644;HyperGNNs&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21516;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#34987;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#20960;&#20309;&#25299;&#25169;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#33410;&#28857;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#22823;&#22810;&#25968;HyperGNNs&#21487;&#20197;&#20351;&#29992;&#24102;&#26377;&#36229;&#22270;&#30340;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;GNN&#26469;&#36817;&#20284;&#12290;&#36825;&#23548;&#33268;&#20102;WCE-GNN&#65292;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;GNN&#21644;&#19968;&#20010;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#65288;WCE&#65289;&#65292;&#29992;&#20110;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23545;&#20110;&#20061;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WCE-GNN&#19981;&#20165;&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#32780;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644;CATE&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#23567;&#21306;&#38388;&#23485;&#24230;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21487;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.04906</link><description>&lt;p&gt;
&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644;CATE&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#23567;&#21306;&#38388;&#23485;&#24230;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21487;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#35782;&#24178;&#39044;&#25928;&#26524;&#65292;&#21363;&#27835;&#30103;&#25928;&#26524;&#65292;&#23545;&#20110;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524; (CATE) &#20272;&#35745;&#31561;&#26041;&#27861;&#36890;&#24120;&#21482;&#25552;&#20379;&#27835;&#30103;&#25928;&#26524;&#30340;&#28857;&#20272;&#35745;&#65292;&#32780;&#24120;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931; (CMC) &#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644; CATE &#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#26469;&#20135;&#29983;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32467;&#26524;&#22122;&#22768;&#20998;&#24067;&#30340;&#29305;&#23450;&#20551;&#35774;&#22914;&#20309;&#20005;&#37325;&#24433;&#21709;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;CMC&#26694;&#26550;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#23567;&#30340;&#21306;&#38388;&#23485;&#24230;&#65292;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge of the effect of interventions, called the treatment effect, is paramount for decision-making. Approaches to estimating this treatment effect, e.g. by using Conditional Average Treatment Effect (CATE) estimators, often only provide a point estimate of this treatment effect, while additional uncertainty quantification is frequently desired instead. Therefore, we present a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to instead produce a predictive distribution usable in individualized decision-making. Furthermore, we show how specific assumptions on the noise distribution of the outcome heavily affect these uncertainty predictions. Nonetheless, the CMC framework shows strong experimental coverage while retaining small interval widths to provide estimates of the true individual treatment effect.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#65292;&#22686;&#21152;&#20102;&#23545;&#20854;&#29702;&#35770;&#29702;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#20110;&#39640;&#26041;&#24046;&#30340;&#19979;&#28216;&#39044;&#27979;&#22120;&#29305;&#21035;&#26377;&#30410;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32463;&#39564;&#27861;&#21017;&#29992;&#20110;&#36873;&#25321;&#36866;&#24403;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03985</link><description>&lt;p&gt;
&#23545;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#38598;&#25104;&#36827;&#34892;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#65292;&#22686;&#21152;&#20102;&#23545;&#20854;&#29702;&#35770;&#29702;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#20110;&#39640;&#26041;&#24046;&#30340;&#19979;&#28216;&#39044;&#27979;&#22120;&#29305;&#21035;&#26377;&#30410;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32463;&#39564;&#27861;&#21017;&#29992;&#20110;&#36873;&#25321;&#36866;&#24403;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20026;&#30417;&#30563;&#23398;&#20064;&#29983;&#25104;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#22686;&#21152;&#20934;&#30830;&#24615;&#12289;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#36873;&#25321;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#36825;&#20123;&#22909;&#22788;&#22312;&#32463;&#39564;&#19978;&#26377;&#26126;&#30830;&#30340;&#25903;&#25345;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#29702;&#35770;&#29702;&#35299;&#30446;&#21069;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#20351;&#29992;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#20960;&#31181;&#35774;&#32622;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#65292;&#26469;&#22686;&#21152;&#29702;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#39044;&#27979;&#65292;&#23545;&#20110;&#39640;&#26041;&#24046;&#30340;&#19979;&#28216;&#39044;&#27979;&#22120;&#65292;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#23558;&#29305;&#21035;&#26377;&#30410;&#65292;&#24182;&#20026;&#22343;&#26041;&#35823;&#24046;&#21644;Brier&#20998;&#25968;&#30340;&#24773;&#20917;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32463;&#39564;&#27861;&#21017;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#25968;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#19968;&#20010;&#38598;&#25104;&#22312;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20960;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#20197;&#21450;&#19979;&#28216;&#39044;&#27979;&#22120;&#19978;&#30340;&#24615;&#33021;&#26469;&#30740;&#31350;&#25105;&#20204;&#30340;&#29702;&#35770;&#22312;&#23454;&#36341;&#20013;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#27934;&#23519;&#20063;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have highlighted the benefits of generating multiple synthetic datasets for supervised learning, from increased accuracy to more effective model selection and uncertainty estimation. These benefits have clear empirical support, but the theoretical understanding of them is currently very light. We seek to increase the theoretical understanding by deriving bias-variance decompositions for several settings of using multiple synthetic datasets. Our theory predicts multiple synthetic datasets to be especially beneficial for high-variance downstream predictors, and yields a simple rule of thumb to select the appropriate number of synthetic datasets in the case of mean-squared error and Brier score. We investigate how our theory works in practice by evaluating the performance of an ensemble over many synthetic datasets for several real datasets and downstream predictors. The results follow our theory, showing that our insights are also practically relevant.
&lt;/p&gt;</description></item><item><title>PARD&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#20013;&#30340;&#37096;&#20998;&#39034;&#24207;&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.03687</link><description>&lt;p&gt;
Pard: &#20855;&#26377;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#29992;&#20110;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03687
&lt;/p&gt;
&lt;p&gt;
PARD&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#20013;&#30340;&#37096;&#20998;&#39034;&#24207;&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#22238;&#24402;&#27169;&#22411;&#23545;&#20110;&#22270;&#30340;&#39034;&#24207;&#25935;&#24863;&#65292;&#20294;&#20854;&#31616;&#21333;&#26377;&#25928;&#65292;&#22312;&#22270;&#29983;&#25104;&#39046;&#22495;&#19968;&#30452;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#32622;&#25442;&#19981;&#21464;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#22270;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#24615;&#29983;&#25104;&#22270;&#65292;&#20294;&#38656;&#35201;&#39069;&#22806;&#30340;&#29305;&#24449;&#21644;&#25104;&#21315;&#19978;&#19975;&#27493;&#30340;&#21435;&#22122;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;PARD&#65292;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#12290;PARD&#21033;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#32622;&#25442;&#19981;&#21464;&#24615;&#65292;&#26080;&#38656;&#20851;&#27880;&#22270;&#30340;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#38598;&#21512;&#19981;&#21516;&#65292;&#22270;&#20013;&#30340;&#20803;&#32032;&#24182;&#19981;&#26159;&#23436;&#20840;&#26080;&#24207;&#30340;&#65292;&#33410;&#28857;&#21644;&#36793;&#26377;&#19968;&#20010;&#29420;&#29305;&#30340;&#37096;&#20998;&#39034;&#24207;&#12290;&#21033;&#29992;&#36825;&#20010;&#37096;&#20998;&#39034;&#24207;&#65292;PARD&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#65292;&#20854;&#20013;&#27599;&#20010;&#22359;&#30340;&#27010;&#29575;&#20026;c&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#33719;&#21462;&#20989;&#25968;&#29992;&#20110;&#20027;&#21160;&#30456;&#20851;&#32858;&#31867;&#65292;&#20998;&#21035;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#27010;&#24565;&#21644;&#20449;&#24687;&#35770;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03587</link><description>&lt;p&gt;
&#20027;&#21160;&#30456;&#20851;&#32858;&#31867;&#30340;&#26377;&#25928;&#33719;&#21462;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Effective Acquisition Functions for Active Correlation Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#33719;&#21462;&#20989;&#25968;&#29992;&#20110;&#20027;&#21160;&#30456;&#20851;&#32858;&#31867;&#65292;&#20998;&#21035;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#27010;&#24565;&#21644;&#20449;&#24687;&#35770;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#32858;&#31867;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#65292;&#25903;&#25345;&#27491;&#21644;&#36127;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#20551;&#35774;&#30456;&#20284;&#24615;&#20107;&#20808;&#26410;&#30693;&#65292;&#32780;&#26159;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#20197;&#19968;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#24335;&#36845;&#20195;&#22320;&#26597;&#35810;&#30456;&#20284;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#33719;&#21462;&#20989;&#25968;&#29992;&#20110;&#22312;&#27492;&#35774;&#32622;&#19979;&#20351;&#29992;&#12290;&#20854;&#20013;&#19968;&#31181;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#27010;&#24565;&#65288;&#21363;&#24403;&#30456;&#20284;&#24615;&#36829;&#21453;&#20256;&#36882;&#24615;&#26102;&#65289;&#12290;&#20854;&#20313;&#20004;&#20010;&#22522;&#20110;&#20449;&#24687;&#35770;&#37327;&#65292;&#21363;&#29109;&#21644;&#20449;&#24687;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correlation clustering is a powerful unsupervised learning paradigm that supports positive and negative similarities. In this paper, we assume the similarities are not known in advance. Instead, we employ active learning to iteratively query similarities in a cost-efficient way. In particular, we develop three effective acquisition functions to be used in this setting. One is based on the notion of inconsistency (i.e., when similarities violate the transitive property). The remaining two are based on information-theoretic quantities, i.e., entropy and information gain.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32452;&#21512;&#29305;&#24449;&#23545;&#40784;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#32452;&#21512;&#36890;&#29992;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;-&#31867;&#21035;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.02851</link><description>&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#29305;&#24449;&#23545;&#40784;&#22686;&#24378;&#32452;&#21512;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Compositional Generalization via Compositional Feature Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02851
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#29305;&#24449;&#23545;&#40784;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#32452;&#21512;&#36890;&#29992;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;-&#31867;&#21035;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#32463;&#24120;&#38754;&#20020;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#22312;&#24120;&#35265;&#30340;&#22810;&#39046;&#22495;&#22810;&#31867;&#21035;&#35774;&#32622;&#20013;&#65292;&#38543;&#30528;&#31867;&#21035;&#21644;&#39046;&#22495;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#24456;&#38590;&#20026;&#27599;&#20010;&#39046;&#22495;-&#31867;&#21035;&#32452;&#21512;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20010;&#25361;&#25112;&#33258;&#28982;&#22320;&#24341;&#21457;&#20102;&#23545;&#20855;&#22791;&#32452;&#21512;&#36890;&#29992;&#24615;&#65288;CG&#65289;&#33021;&#21147;&#30340;&#27169;&#22411;&#30340;&#25506;&#32034;&#65292;&#21363;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;-&#31867;&#21035;&#32452;&#21512;&#12290;&#20026;&#20102;&#28145;&#20837;&#30740;&#31350;CG&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;CG-Bench&#65292;&#36825;&#26159;&#19968;&#22871;&#20174;&#29616;&#26377;&#23454;&#38469;&#22270;&#20687;&#25968;&#25454;&#38598;&#27966;&#29983;&#30340;CG&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#30446;&#21069;&#22312;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;DINOv2&#65289;&#19978;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#36825;&#20010;&#25361;&#25112;&#20013;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32452;&#21512;&#29305;&#24449;&#23545;&#40784;&#65288;CFA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#24494;&#35843;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#19978;&#23398;&#20064;&#20004;&#20010;&#27491;&#20132;&#32447;&#24615;&#22836;&#37096;&#26469;&#23545;&#40784;&#31867;&#21035;&#21644;&#39046;&#22495;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain lab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#65292;&#24182;&#23558;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#22240;&#26524;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02277</link><description>&lt;p&gt;
&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#36890;&#36807;&#22806;&#28304;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Bayesian Optimization via Exogenous Distribution Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#65292;&#24182;&#23558;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#22240;&#26524;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#23558;&#30446;&#26631;&#21464;&#37327;&#26368;&#22823;&#21270;&#20316;&#20026;&#25805;&#20316;&#30446;&#26631;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CBO&#65289;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#25913;&#21464;&#22240;&#26524;&#32467;&#26500;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#30828;&#24178;&#39044;&#65292;&#35201;&#20040;&#24341;&#20837;&#21160;&#20316;&#33410;&#28857;&#21040;&#20869;&#29983;&#21464;&#37327;&#20013;&#65292;&#20197;&#35843;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#36825;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#36890;&#24120;&#34987;&#24573;&#30053;&#25110;&#36890;&#36807;&#26399;&#26395;&#36827;&#34892;&#36793;&#32536;&#21270;&#12290;&#22806;&#28304;&#20998;&#24067;&#23398;&#20064;&#25552;&#39640;&#20102;&#36890;&#24120;&#36890;&#36807;&#26377;&#38480;&#35266;&#27979;&#25968;&#25454;&#35757;&#32451;&#30340;&#20195;&#29702;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#22806;&#28304;&#20998;&#24067;&#23558;&#29616;&#26377;&#30340;CBO&#25193;&#23637;&#21040;&#36229;&#20986;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65288;ANM&#65289;&#30340;&#19968;&#33324;&#22240;&#26524;&#26041;&#26696;&#12290;&#24674;&#22797;&#22806;&#28304;&#21464;&#37327;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#22122;&#22768;&#25110;&#26410;&#35266;&#27979;&#21040;&#30340;&#38544;&#34255;&#21464;&#37327;&#20351;&#29992;&#26356;&#28789;&#27963;&#30340;&#20808;&#39564;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;CBO&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximizing a target variable as an operational objective in a structured causal model is an important problem. Existing Causal Bayesian Optimization (CBO) methods either rely on hard interventions that alter the causal structure to maximize the reward; or introduce action nodes to endogenous variables so that the data generation mechanisms are adjusted to achieve the objective. In this paper, a novel method is introduced to learn the distribution of exogenous variables, which is typically ignored or marginalized through expectation by existing methods.   Exogenous distribution learning improves the approximation accuracy of structured causal models in a surrogate model that is usually trained with limited observational data. Moreover, the learned exogenous distribution extends existing CBO to general causal schemes beyond Additive Noise Models (ANM). The recovery of exogenous variables allows us to use a more flexible prior for noise or unobserved hidden variables. A new CBO method is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#32422;&#31616;&#26041;&#27861;&#65292;&#21033;&#29992;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#25237;&#24433;&#32479;&#19968;&#20102;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#21516;&#26102;&#35299;&#20915;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02239</link><description>&lt;p&gt;
&#20998;&#24067;&#32422;&#31616;&#65306;&#29992;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#25237;&#24433;&#32479;&#19968;&#38477;&#32500;&#21644;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Distributional Reduction: Unifying Dimensionality Reduction and Clustering with Gromov-Wasserstein Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#32422;&#31616;&#26041;&#27861;&#65292;&#21033;&#29992;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#25237;&#24433;&#32479;&#19968;&#20102;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#21516;&#26102;&#35299;&#20915;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#26088;&#22312;&#25429;&#25417;&#28508;&#22312;&#30340;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#32467;&#26500;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#28041;&#21450;&#20351;&#29992;&#38477;&#32500;&#26041;&#27861;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#30340;&#31354;&#38388;&#19978;&#65292;&#25110;&#23558;&#25968;&#25454;&#28857;&#32452;&#32455;&#25104;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26159;&#25353;&#39034;&#24207;&#20351;&#29992;&#30340;&#65292;&#32780;&#19981;&#33021;&#20445;&#35777;&#32858;&#31867;&#19982;&#38477;&#32500;&#30456;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65306;&#20351;&#29992;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#65292;&#25105;&#20204;&#23558;&#32858;&#31867;&#21644;&#38477;&#32500;&#32479;&#19968;&#20026;&#19968;&#20010;&#31216;&#20026;&#20998;&#24067;&#32422;&#31616;&#30340;&#21333;&#19968;&#26694;&#26550;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#21333;&#20010;&#20248;&#21270;&#38382;&#39064;&#21516;&#26102;&#35299;&#20915;&#32858;&#31867;&#21644;&#38477;&#32500;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#22522;&#22240;&#32452;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning aims to capture the underlying structure of potentially large and high-dimensional datasets. Traditionally, this involves using dimensionality reduction methods to project data onto interpretable spaces or organizing points into meaningful clusters. In practice, these methods are used sequentially, without guaranteeing that the clustering aligns well with the conducted dimensionality reduction. In this work, we offer a fresh perspective: that of distributions. Leveraging tools from optimal transport, particularly the Gromov-Wasserstein distance, we unify clustering and dimensionality reduction into a single framework called distributional reduction. This allows us to jointly address clustering and dimensionality reduction with a single optimization problem. Through comprehensive experiments, we highlight the versatility and interpretability of our method and show that it outperforms existing approaches across a variety of image and genomics datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#35757;&#32451;&#19982;&#32463;&#20856;&#21457;&#29616;&#31639;&#27861;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#26356;&#22909;&#30340;&#34920;&#29616;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.01929</link><description>&lt;p&gt;
&#26679;&#26412;&#12289;&#20272;&#35745;&#12289;&#32858;&#21512;&#65306;&#22240;&#26524;&#21457;&#29616;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample, estimate, aggregate: A recipe for causal discovery foundation models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#35757;&#32451;&#19982;&#32463;&#20856;&#21457;&#29616;&#31639;&#27861;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#26356;&#22909;&#30340;&#34920;&#29616;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#26159;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#30340;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#30740;&#31350;&#12289;&#25351;&#23548;&#20915;&#31574;&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#30340;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#20351;&#23427;&#20204;&#21464;&#24471;&#32531;&#24930;&#12289;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#24182;&#19988;&#33030;&#24369;&#12290;&#21463;&#22522;&#30784;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#65292;&#20854;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#35757;&#32451;&#29992;&#20110;&#22788;&#29702;&#22312;&#36739;&#23567;&#30340;&#21464;&#37327;&#23376;&#38598;&#19978;&#36816;&#34892;&#30340;&#32463;&#20856;&#21457;&#29616;&#31639;&#27861;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#20197;&#19979;&#35266;&#23519;&#32467;&#26524;&#65306;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#22312;&#23567;&#38382;&#39064;&#19978;&#35745;&#31639;&#36895;&#24230;&#24555;&#65292;&#23545;&#65288;&#36793;&#38469;&#65289;&#25968;&#25454;&#32467;&#26500;&#20855;&#26377;&#20449;&#24687;&#37327;&#65292;&#19988;&#23427;&#20204;&#30340;&#36755;&#20986;&#32467;&#26500;&#20316;&#20026;&#23545;&#35937;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#65292;&#24182;&#19988;&#25552;&#20379;&#27604;&#29616;&#26377;&#27169;&#22411;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery, the task of inferring causal structure from data, promises to accelerate scientific research, inform policy making, and more. However, the per-dataset nature of existing causal discovery algorithms renders them slow, data hungry, and brittle. Inspired by foundation models, we propose a causal discovery framework where a deep learning model is pretrained to resolve predictions from classical discovery algorithms run over smaller subsets of variables. This method is enabled by the observations that the outputs from classical algorithms are fast to compute for small problems, informative of (marginal) data structure, and their structure outputs as objects remain comparable across datasets. Our method achieves state-of-the-art performance on synthetic and realistic datasets, generalizes to data generating mechanisms not seen during training, and offers inference speeds that are orders of magnitude faster than existing models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#12289;&#26080;&#26631;&#31614;&#12289;&#26410;&#31579;&#36873;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#20197;&#21450;&#37319;&#29992;&#33258;&#30417;&#30563;&#30340;&#24072;&#29983;&#35774;&#32622;&#65292;&#26469;&#25913;&#36827;Tracking-Any-Point&#65288;TAP&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;TAP-Vid&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;TAP&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.00847</link><description>&lt;p&gt;
BootsTAP: &#38024;&#23545;Tracking-Any-Point&#30340;&#33258;&#20030;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BootsTAP: Bootstrapped Training for Tracking-Any-Point
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00847
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#12289;&#26080;&#26631;&#31614;&#12289;&#26410;&#31579;&#36873;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#20197;&#21450;&#37319;&#29992;&#33258;&#30417;&#30563;&#30340;&#24072;&#29983;&#35774;&#32622;&#65292;&#26469;&#25913;&#36827;Tracking-Any-Point&#65288;TAP&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;TAP-Vid&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;TAP&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#27169;&#22411;&#23545;&#29289;&#29702;&#21644;&#36816;&#21160;&#26377;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#35753;&#23427;&#20204;&#33021;&#22815;&#24863;&#30693;&#23454;&#26223;&#20013;&#22266;&#20307;&#34920;&#38754;&#30340;&#31227;&#21160;&#21644;&#21464;&#24418;&#26159;&#24456;&#26377;&#29992;&#30340;&#12290;&#36825;&#21487;&#20197;&#24418;&#24335;&#21270;&#20026;Tracking-Any-Point (TAP)&#65292;&#35201;&#27714;&#31639;&#27861;&#33021;&#22815;&#36861;&#36394;&#35270;&#39057;&#20013;&#19982;&#22266;&#20307;&#34920;&#38754;&#23545;&#24212;&#30340;&#20219;&#24847;&#28857;&#65292;&#21487;&#33021;&#26159;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#23494;&#38598;&#30340;&#12290;&#30446;&#21069;&#65292;TAP&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#30446;&#21069;&#21482;&#33021;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#33719;&#24471;&#26377;&#38480;&#31181;&#31867;&#30340;&#23545;&#35937;&#21644;&#36816;&#21160;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#35268;&#27169;&#12289;&#26080;&#26631;&#31614;&#12289;&#26410;&#31579;&#36873;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#22312;&#20165;&#36827;&#34892;&#26368;&#23567;&#26550;&#26500;&#26356;&#25913;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;TAP&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#37319;&#29992;&#20102;&#33258;&#30417;&#30563;&#30340;&#24072;&#29983;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;TAP-Vid&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20102;&#36229;&#36807;&#20197;&#24448;&#25104;&#26524;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65306;&#20363;&#22914;&#65292;TAP-Vid-DAVIS&#30340;&#24615;&#33021;&#20174;61.3%&#25552;&#21319;&#21040;66.4%&#65292;TAP-Vid-Kinetics&#20174;57.2%&#25552;&#21319;&#21040;61.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes. This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time. Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion. In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup. We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#32570;&#22833;&#20027;&#35201;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26367;&#20195;&#32467;&#26524;&#26469;&#20272;&#35745;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32435;&#20837;&#26367;&#20195;&#32467;&#26524;&#24182;&#36991;&#20813;&#36873;&#25321;&#20559;&#35823;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30340;&#20272;&#35745;&#20540;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#22312;&#26041;&#24046;&#26041;&#38754;&#21487;&#33021;&#27604;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#33391;&#22909;&#23454;&#35777;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00168</link><description>&lt;p&gt;
&#20351;&#29992;&#26367;&#20195;&#32467;&#26524;&#36827;&#34892;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Continuous Treatment Effects with Surrogate Outcomes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#32570;&#22833;&#20027;&#35201;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26367;&#20195;&#32467;&#26524;&#26469;&#20272;&#35745;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32435;&#20837;&#26367;&#20195;&#32467;&#26524;&#24182;&#36991;&#20813;&#36873;&#25321;&#20559;&#35823;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30340;&#20272;&#35745;&#20540;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#22312;&#26041;&#24046;&#26041;&#38754;&#21487;&#33021;&#27604;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#33391;&#22909;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22240;&#26524;&#25512;&#26029;&#24212;&#29992;&#20013;&#65292;&#20027;&#35201;&#32467;&#26524;&#65288;&#26631;&#31614;&#65289;&#24120;&#24120;&#26159;&#37096;&#20998;&#32570;&#22833;&#30340;&#65292;&#29305;&#21035;&#26159;&#22914;&#26524;&#23427;&#20204;&#24456;&#26114;&#36149;&#25110;&#24456;&#38590;&#25910;&#38598;&#12290;&#22914;&#26524;&#32570;&#22833;&#20381;&#36182;&#20110;&#21327;&#21464;&#37327;&#65288;&#21363;&#32570;&#22833;&#19981;&#23436;&#20840;&#38543;&#26426;&#65289;&#65292;&#20165;&#22522;&#20110;&#23436;&#20840;&#35266;&#27979;&#26679;&#26412;&#30340;&#20998;&#26512;&#21487;&#33021;&#23384;&#22312;&#20559;&#35823;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32467;&#21512;&#19982;&#20027;&#35201;&#32467;&#26524;&#30456;&#20851;&#30340;&#23436;&#20840;&#35266;&#27979;&#30340;&#27835;&#30103;&#21518;&#21464;&#37327;&#65288;&#26367;&#20195;&#32467;&#26524;&#65289;&#21487;&#20197;&#25913;&#36827;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26367;&#20195;&#32467;&#26524;&#22312;&#20272;&#35745;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#65292;&#20197;&#39640;&#25928;&#22320;&#23558;&#26367;&#20195;&#32467;&#26524;&#32435;&#20837;&#20998;&#26512;&#20013;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#19978;&#36848;&#36873;&#25321;&#20559;&#35823;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25152;&#25552;&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26041;&#24046;&#30340;&#21487;&#33021;&#25913;&#36827;&#12290;&#24191;&#27867;&#30340;&#27169;&#25311;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#32463;&#39564;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world causal inference applications, the primary outcomes (labels) are often partially missing, especially if they are expensive or difficult to collect. If the missingness depends on covariates (i.e., missingness is not completely at random), analyses based on fully-observed samples alone may be biased. Incorporating surrogates, which are fully observed post-treatment variables related to the primary outcome, can improve estimation in this case. In this paper, we study the role of surrogates in estimating continuous treatment effects and propose a doubly robust method to efficiently incorporate surrogates in the analysis, which uses both labeled and unlabeled data and does not suffer from the above selection bias problem. Importantly, we establish asymptotic normality of the proposed estimator and show possible improvements on the variance compared with methods that solely use labeled data. Extensive simulations show our methods enjoy appealing empirical performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#38024;&#23545;Vapnik-Chervonenkis (VC)&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#24182;&#19988;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26368;&#20339;&#19979;&#30028;&#20445;&#25345;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#24605;&#24819;&#21644;&#29702;&#35770;&#36824;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#26368;&#32456;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;&#12290;</title><link>http://arxiv.org/abs/2312.05134</link><description>&lt;p&gt;
&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Multi-Distribution Learning. (arXiv:2312.05134v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#38024;&#23545;Vapnik-Chervonenkis (VC)&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#24182;&#19988;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26368;&#20339;&#19979;&#30028;&#20445;&#25345;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#24605;&#24819;&#21644;&#29702;&#35770;&#36824;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#26368;&#32456;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20998;&#24067;&#23398;&#20064;&#65288;MDL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;k&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#39118;&#38505;&#65292;&#24050;&#25104;&#20026;&#36866;&#24212;&#20581;&#22766;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#22810;&#32452;&#21512;&#20316;&#31561;&#38656;&#27714;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;MDL&#38656;&#35201;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20063;&#31216;&#20026;&#25353;&#38656;&#37319;&#26679;&#12290;&#28982;&#32780;&#65292;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#19979;&#30028;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#38024;&#23545;Vapnik-Chervonenkis&#65288;VC&#65289;&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#25509;&#36817;&#20110;&#65288;d+k&#65289;/&#949;^2&#65288;&#22312;&#26576;&#20123;&#23545;&#25968;&#22240;&#23376;&#20013;&#65289;&#65292;&#19982;&#24050;&#30693;&#30340;&#26368;&#20339;&#19979;&#30028;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#24605;&#24819;&#21644;&#29702;&#35770;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#65292;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-distribution learning (MDL), which seeks to learn a shared model that minimizes the worst-case risk across $k$ distinct data distributions, has emerged as a unified framework in response to the evolving demand for robustness, fairness, multi-group collaboration, etc. Achieving data-efficient MDL necessitates adaptive sampling, also called on-demand sampling, throughout the learning process. However, there exist substantial gaps between the state-of-the-art upper and lower bounds on the optimal sample complexity. Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$, we propose a novel algorithm that yields an $varepsilon$-optimal randomized hypothesis with a sample complexity on the order of $(d+k)/\varepsilon^2$ (modulo some logarithmic factor), matching the best-known lower bound. Our algorithmic ideas and theory have been further extended to accommodate Rademacher classes. The proposed algorithms are oracle-efficient, which access the hypothesis class solely
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#19981;&#21464;&#21644;&#31561;&#21464;&#30340;&#32463;&#20856;&#21644;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#25552;&#20379;&#24555;&#36895;&#32780;&#39640;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#25351;&#20986;&#36890;&#36807;&#20351;&#29992;&#19981;&#21464;&#36755;&#20837;&#21644;&#31561;&#21464;&#23618;&#65292;&#21487;&#20197;&#22686;&#24378;&#28145;&#24230;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.18672</link><description>&lt;p&gt;
&#19981;&#21464;&#21644;&#31561;&#21464;&#30340;&#32463;&#20856;&#21644;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison Between Invariant and Equivariant Classical and Quantum Graph Neural Networks. (arXiv:2311.18672v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#19981;&#21464;&#21644;&#31561;&#21464;&#30340;&#32463;&#20856;&#21644;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#25552;&#20379;&#24555;&#36895;&#32780;&#39640;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#25351;&#20986;&#36890;&#36807;&#20351;&#29992;&#19981;&#21464;&#36755;&#20837;&#21644;&#31561;&#21464;&#23618;&#65292;&#21487;&#20197;&#22686;&#24378;&#28145;&#24230;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#29702;&#35299;CERN&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;(LHC)&#19978;&#20135;&#29983;&#30340;&#22823;&#37327;&#39640;&#33021;&#31890;&#23376;&#30896;&#25758;&#25968;&#25454;&#26102;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20123;&#30896;&#25758;&#20107;&#20214;&#30340;&#25968;&#25454;&#21487;&#20197;&#33258;&#28982;&#22320;&#29992;&#22270;&#32467;&#26500;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#28145;&#24230;&#20960;&#20309;&#26041;&#27861;&#65292;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#65292;&#24050;&#32463;&#22312;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20998;&#26512;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20219;&#21153;&#26159;&#21943;&#27880;&#26631;&#35760;&#65292;&#20854;&#20013;&#21943;&#27880;&#34987;&#35270;&#20026;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#21644;&#20854;&#32452;&#25104;&#31890;&#23376;&#20043;&#38388;&#30340;&#36793;&#36830;&#25509;&#30340;&#28857;&#20113;&#12290;LHC&#31890;&#23376;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#20197;&#21450;&#29992;&#20110;&#20854;&#20998;&#26512;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22823;&#22823;&#20419;&#36827;&#20102;&#24320;&#21457;&#26367;&#20195;&#24555;&#36895;&#19988;&#39640;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#65292;&#22914;&#37327;&#23376;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#28145;&#24230;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19981;&#21464;&#36755;&#20837;&#21644;&#31561;&#21464;&#23618;&#26469;&#21033;&#29992;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are heavily relied on to understand the vast amounts of data from high-energy particle collisions at the CERN Large Hadron Collider (LHC). The data from such collision events can naturally be represented with graph structures. Therefore, deep geometric methods, such as graph neural networks (GNNs), have been leveraged for various data analysis tasks in high-energy physics. One typical task is jet tagging, where jets are viewed as point clouds with distinct features and edge connections between their constituent particles. The increasing size and complexity of the LHC particle datasets, as well as the computational models used for their analysis, greatly motivate the development of alternative fast and efficient computational paradigms such as quantum computation. In addition, to enhance the validity and robustness of deep networks, one can leverage the fundamental symmetries present in the data through the use of invariant inputs and equivariant layers. In t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#23454;&#29616;&#20102;&#21452;&#21521;&#26144;&#23556;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#38544;&#34255;&#32422;&#26463;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18449</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#20855;&#26377;&#38544;&#34255;&#32422;&#26463;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization with Hidden Constraints via Latent Decision Models. (arXiv:2310.18449v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#23454;&#29616;&#20102;&#21452;&#21521;&#26144;&#23556;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#38544;&#34255;&#32422;&#26463;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#23588;&#20854;&#22312;&#20844;&#20849;&#25919;&#31574;&#39046;&#22495;&#22914;&#35686;&#23519;&#21010;&#21306;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23450;&#20041;&#21487;&#34892;&#21306;&#22495;&#30340;&#22797;&#26434;&#24615;&#21644;&#20915;&#31574;&#30340;&#39640;&#32500;&#24230;&#65292;&#20854;&#22312;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#38544;&#34255;&#32422;&#26463;&#28508;&#22312;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;HC-LSBO&#65289;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#21407;&#22987;&#20915;&#31574;&#31354;&#38388;&#19982;&#36739;&#20302;&#32500;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#21452;&#21521;&#26144;&#23556;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;HC-LSBO&#25429;&#25417;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#22266;&#26377;&#30340;&#38544;&#34255;&#32422;&#26463;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#30340;&#21516;&#26102;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#35780;&#20272;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#35268;&#27169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) has emerged as a potent tool for addressing intricate decision-making challenges, especially in public policy domains such as police districting. However, its broader application in public policymaking is hindered by the complexity of defining feasible regions and the high-dimensionality of decisions. This paper introduces the Hidden-Constrained Latent Space Bayesian Optimization (HC-LSBO), a novel BO method integrated with a latent decision model. This approach leverages a variational autoencoder to learn the distribution of feasible decisions, enabling a two-way mapping between the original decision space and a lower-dimensional latent space. By doing so, HC-LSBO captures the nuances of hidden constraints inherent in public policymaking, allowing for optimization in the latent space while evaluating objectives in the original space. We validate our method through numerical experiments on both synthetic and real data sets, with a specific focus on large-scal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#32654;&#22269;&#34928;&#36864;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#21457;&#29616;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#27169;&#22411;&#22312;&#38271;&#26399;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;SHAP&#26041;&#27861;&#23545;GRU&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.17571</link><description>&lt;p&gt;
&#40657;&#21283;&#23376;&#20869;&#37096;&#65306;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#26102;&#39044;&#27979;&#32654;&#22269;&#34928;&#36864;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Inside the black box: Neural network-based real-time prediction of US recessions. (arXiv:2310.17571v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#32654;&#22269;&#34928;&#36864;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#21457;&#29616;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#27169;&#22411;&#22312;&#38271;&#26399;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;SHAP&#26041;&#27861;&#23545;GRU&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFN&#65289;&#21644;&#20004;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#21363;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#65292;&#23545;1967&#24180;&#33267;2021&#24180;&#30340;&#32654;&#22269;&#34928;&#36864;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#21518;&#21033;&#29992;&#20272;&#35745;&#30340;&#27169;&#22411;&#23545;&#32654;&#22269;&#30340;&#22823;&#34928;&#36864;&#21644;Covid-19&#34928;&#36864;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#23558;&#20854;&#39044;&#27979;&#24615;&#33021;&#19982;&#20256;&#32479;&#32447;&#24615;&#27169;&#22411;&#12289;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#65288;&#26377;&#21644;&#26080;&#23725;&#22238;&#24402;&#24809;&#32602;&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#22806;&#26679;&#26412;&#34920;&#29616;&#34920;&#26126;&#65292;LSTM&#21644;GRU&#22312;&#34928;&#36864;&#39044;&#27979;&#39046;&#22495;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#38271;&#26399;&#39044;&#27979;&#20219;&#21153;&#12290;&#30456;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#32479;&#35745;&#24615;&#33021;&#25351;&#26631;&#65292;&#22312;5&#20010;&#39044;&#27979;&#21608;&#26399;&#20869;&#65292;&#23427;&#20204;&#20248;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;&#32780;&#29992;Shapley&#22686;&#37327;&#35299;&#37322;&#65288;SHAP&#65289;&#26041;&#27861;&#35780;&#20272;&#34913;&#37327;GRU&#22312;&#19981;&#21516;&#39044;&#27979;&#21608;&#26399;&#20869;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#20197;&#28145;&#20837;&#20102;&#35299;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feedforward neural network (FFN) and two specific types of recurrent neural network, long short-term memory (LSTM) and gated recurrent unit (GRU), are used for modeling US recessions in the period from 1967 to 2021. The estimated models are then employed to conduct real-time predictions of the Great Recession and the Covid-19 recession in US. Their predictive performances are compared to those of the traditional linear models, the logistic regression model both with and without the ridge penalty. The out-of-sample performance suggests the application of LSTM and GRU in the area of recession forecasting, especially for the long-term forecasting tasks. They outperform other types of models across 5 forecasting horizons with respect to different types of statistical performance metrics. Shapley additive explanations (SHAP) method is applied to the fitted GRUs across different forecasting horizons to gain insight into the feature importance. The evaluation of predictor importance differs b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;&#22240;&#26524;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20123;&#26681;&#26412;&#24615;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#31561;&#26377;&#30410;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11011</link><description>&lt;p&gt;
&#20174;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#34920;&#31034;&#21040;&#21487;&#25511;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#65306;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling. (arXiv:2310.11011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;&#22240;&#26524;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20123;&#26681;&#26412;&#24615;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#31561;&#26377;&#30410;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#25454;&#23494;&#24230;&#20272;&#35745;&#21644;&#20174;&#26377;&#38480;&#26679;&#26412;&#20013;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#26681;&#26412;&#24615;&#30340;&#32570;&#28857;&#65292;&#22914;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#24341;&#20837;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#24046;&#21170;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21487;&#20197;&#23558;&#22240;&#26524;&#29702;&#35770;&#34701;&#20837;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#20013;&#12290;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#25551;&#36848;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#23545;&#31995;&#32479;&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#22240;&#26524;&#20851;&#31995;&#21644;&#26426;&#21046;&#36827;&#34892;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21487;&#20197;&#19982;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#33258;&#28982;&#22320;&#32467;&#21512;&#12290;&#22240;&#26524;&#27169;&#22411;&#20026;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#20960;&#20010;&#26377;&#30410;&#30340;&#23646;&#24615;&#65292;&#22914;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#32508;&#36848;&#65292;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have shown tremendous success in data density estimation and data generation from finite samples. While these models have shown impressive performance by learning correlations among features in the data, some fundamental shortcomings are their lack of explainability, the tendency to induce spurious correlations, and poor out-of-distribution extrapolation. In an effort to remedy such challenges, one can incorporate the theory of causality in deep generative modeling. Structural causal models (SCMs) describe data-generating processes and model complex causal relationships and mechanisms among variables in a system. Thus, SCMs can naturally be combined with deep generative models. Causal models offer several beneficial properties to deep generative models, such as distribution shift robustness, fairness, and interoperability. We provide a technical survey on causal generative modeling categorized into causal representation learning and controllable counterfactual ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#30340;&#21442;&#25968;&#20272;&#35745;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.04191</link><description>&lt;p&gt;
&#20851;&#20110;&#36923;&#36753;&#22238;&#24402;&#20013;&#21442;&#25968;&#20272;&#35745;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the sample complexity of estimation in logistic regression. (arXiv:2307.04191v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#30340;&#21442;&#25968;&#20272;&#35745;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#26159;&#22122;&#22768;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#65292;&#20197;$\ell_2$&#35823;&#24046;&#20026;&#38480;&#65292;&#20272;&#35745;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#21442;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#32771;&#34385;&#20102;&#32500;&#24230;&#21644;&#36870;&#28201;&#24230;&#30340;&#24433;&#21709;&#12290;&#36870;&#28201;&#24230;&#25511;&#21046;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20449;&#22122;&#27604;&#12290;&#34429;&#28982;&#36923;&#36753;&#22238;&#24402;&#30340;&#24191;&#20041;&#30028;&#38480;&#21644;&#28176;&#36817;&#24615;&#33021;&#24050;&#32463;&#26377;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#20851;&#20110;&#21442;&#25968;&#20272;&#35745;&#30340;&#38750;&#28176;&#36817;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#20043;&#21069;&#30340;&#20998;&#26512;&#20013;&#27809;&#26377;&#35752;&#35770;&#20854;&#19982;&#35823;&#24046;&#21644;&#36870;&#28201;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#20855;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65288;&#25110;&#20020;&#30028;&#28857;&#65289;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The logistic regression model is one of the most popular data generation model in noisy binary classification problems. In this work, we study the sample complexity of estimating the parameters of the logistic regression model up to a given $\ell_2$ error, in terms of the dimension and the inverse temperature, with standard normal covariates. The inverse temperature controls the signal-to-noise ratio of the data generation process. While both generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression are well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. We show that the sample complexity curve has two change-points (or critical points) in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#37319;&#26679;&#36807;&#31243;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#21464;&#20998;&#24418;&#24335;&#30340;&#36335;&#24452;&#31354;&#38388;&#24230;&#37327;&#65292;&#25552;&#20986;&#20102;&#23545;&#25968;&#26041;&#24046;&#25439;&#22833;&#65292;&#20248;&#21270;&#20102;&#37319;&#26679;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01198</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#25913;&#36827;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Improved sampling via learned diffusions. (arXiv:2307.01198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01198
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#37319;&#26679;&#36807;&#31243;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#21464;&#20998;&#24418;&#24335;&#30340;&#36335;&#24452;&#31354;&#38388;&#24230;&#37327;&#65292;&#25552;&#20986;&#20102;&#23545;&#25968;&#26041;&#24046;&#25439;&#22833;&#65292;&#20248;&#21270;&#20102;&#37319;&#26679;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25511;&#21046;&#25193;&#25955;&#36807;&#31243;&#20174;&#38750;&#26631;&#20934;&#21270;&#30446;&#26631;&#23494;&#24230;&#20013;&#37319;&#26679;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#35270;&#20026;Schr&#246;dinger&#26725;&#38382;&#39064;&#30340;&#29305;&#20363;&#65292;&#23547;&#27714;&#32473;&#23450;&#20808;&#39564;&#20998;&#24067;&#21644;&#25351;&#23450;&#30446;&#26631;&#20043;&#38388;&#26368;&#21487;&#33021;&#30340;&#38543;&#26426;&#28436;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26102;&#38388;&#21453;&#28436;&#25193;&#25955;&#36807;&#31243;&#30340;&#36335;&#24452;&#31354;&#38388;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#21464;&#20998;&#24418;&#24335;&#26469;&#25512;&#24191;&#36825;&#20010;&#26694;&#26550;&#12290;&#36825;&#20010;&#25277;&#35937;&#30340;&#35270;&#35282;&#23548;&#33268;&#20102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#30340;&#23454;&#38469;&#25439;&#22833;&#65292;&#24182;&#23558;&#20808;&#21069;&#30340;&#30446;&#26631;&#20316;&#20026;&#29305;&#20363;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#32771;&#34385;&#38500;&#20102;&#24050;&#30693;&#23384;&#22312;&#27169;&#24335;&#22349;&#32553;&#38382;&#39064;&#30340;&#21453;&#21521;Kullback-Leibler&#24046;&#21035;&#20043;&#22806;&#30340;&#20854;&#20182;&#24046;&#21035;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25152;&#35859;&#30340;&#23545;&#25968;&#26041;&#24046;&#25439;&#22833;&#65292;&#23427;&#20855;&#26377;&#33391;&#22909;&#30340;&#25968;&#20540;&#29305;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#25152;&#26377;&#32771;&#34385;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a series of papers proposed deep learning-based approaches to sample from unnormalized target densities using controlled diffusion processes. In this work, we identify these approaches as special cases of the Schr\"odinger bridge problem, seeking the most likely stochastic evolution between a given prior distribution and the specified target. We further generalize this framework by introducing a variational formulation based on divergences between path space measures of time-reversed diffusion processes. This abstract perspective leads to practical losses that can be optimized by gradient-based algorithms and includes previous objectives as special cases. At the same time, it allows us to consider divergences other than the reverse Kullback-Leibler divergence that is known to suffer from mode collapse. In particular, we propose the so-called log-variance loss, which exhibits favorable numerical properties and leads to significantly improved performance across all considered a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21709;&#24212;&#36827;&#34892;&#31995;&#32479;&#26657;&#20934;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.16564</link><description>&lt;p&gt;
&#36890;&#36807;Pareto Optimal&#33258;&#30417;&#30563;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#26657;&#20934;&#21644;&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision. (arXiv:2306.16564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21709;&#24212;&#36827;&#34892;&#31995;&#32479;&#26657;&#20934;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#32463;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#26159;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#22686;&#38271;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#31561;&#20851;&#38190;&#39046;&#22495;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;LLM&#21709;&#24212;&#30340;&#32622;&#20449;&#27700;&#24179;&#65292;&#23545;&#20110;&#33258;&#21160;&#26816;&#27979;&#38169;&#35823;&#24182;&#20419;&#36827;&#20154;&#26426;&#21327;&#20316;&#39564;&#35777;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#26657;&#20934;&#20449;&#21495;&#26469;&#28304;&#26159;&#19987;&#23478;&#25351;&#23450;&#30340;&#32534;&#31243;&#30417;&#30563;&#65292;&#36890;&#24120;&#20855;&#26377;&#36739;&#20302;&#30340;&#25104;&#26412;&#65292;&#20294;&#20063;&#26377;&#20854;&#33258;&#36523;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#22122;&#22768;&#21644;&#35206;&#30422;&#33539;&#22260;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#26469;&#31995;&#32479;&#22320;&#26657;&#20934;LLM&#21709;&#24212;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;&#36825;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#35843;&#21644;&#27169;&#22411;&#26469;&#23454;&#29616;&#65292;&#23558;LLM&#36755;&#20986;&#19982;&#20854;&#20182;&#21487;&#29992;&#30340;&#30417;&#30563;&#26469;&#28304;&#30456;&#21327;&#35843;&#65292;&#23558;&#26356;&#19981;&#30830;&#23450;&#30340;&#21709;&#24212;&#20998;&#37197;&#26356;&#39640;&#30340;&#39118;&#38505;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the confidence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop verification. An important source of calibration signals stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32463;&#39564;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#32479;&#35745;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#36981;&#24490;&#20302;&#22797;&#26434;&#24230;&#36866;&#24212;&#21407;&#21017;&#65292;&#25512;&#23548;&#20986;&#20102;&#20854;&#32479;&#35745;&#30028;&#38480;&#21450;&#21442;&#25968;&#21270;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.13580</link><description>&lt;p&gt;
&#32463;&#39564;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#20302;&#22797;&#26434;&#24230;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Lower Complexity Adaptation for Empirical Entropic Optimal Transport. (arXiv:2306.13580v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32463;&#39564;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#32479;&#35745;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#36981;&#24490;&#20302;&#22797;&#26434;&#24230;&#36866;&#24212;&#21407;&#21017;&#65292;&#25512;&#23548;&#20986;&#20102;&#20854;&#32479;&#35745;&#30028;&#38480;&#21450;&#21442;&#25968;&#21270;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816; (EOT) &#26159;&#20248;&#21270;&#36755;&#36816; (OT) &#30340;&#19968;&#31181;&#26377;&#25928;&#19988;&#35745;&#31639;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25512;&#23548;&#20986;&#20102; EOT &#25104;&#26412;&#30340;&#26032;&#30340;&#32479;&#35745;&#30028;&#38480;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#22312;&#29109;&#27491;&#21017;&#21270;&#21442;&#25968; $\epsilon$ &#21644;&#26679;&#26412;&#22823;&#23567; $n$ &#30340;&#32479;&#35745;&#24615;&#33021;&#20165;&#21462;&#20915;&#20110;&#20004;&#20010;&#27010;&#29575;&#27979;&#24230;&#20043;&#20013;&#36739;&#31616;&#21333;&#30340;&#37027;&#20010;&#12290;&#20363;&#22914;&#65292;&#22312;&#20805;&#20998;&#24179;&#28369;&#30340;&#25104;&#26412;&#19979;&#65292;&#36825;&#20250;&#20135;&#29983;&#20855;&#26377;$\epsilon^{-d/2}$&#22240;&#23376;&#30340;&#21442;&#25968;&#21270;&#36895;&#29575;$n^{-1/2}$&#65292;&#20854;&#20013;$d$&#26159;&#20004;&#20010;&#24635;&#20307;&#27979;&#24230;&#30340;&#26368;&#23567;&#32500;&#24230;&#12290;&#36825;&#30830;&#35748;&#20102;&#32463;&#39564;EOT&#20063;&#36981;&#24490;&#20102;&#26368;&#36817;&#25165;&#20026;&#26410;&#35268;&#21017;&#21270;OT&#30830;&#35748;&#30340;&#20302;&#22797;&#26434;&#24230;&#36866;&#24212;&#21407;&#21017;&#30340;&#26631;&#24535;&#24615;&#29305;&#24449;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19978;&#30340;&#27979;&#24230;&#30340;&#32463;&#39564;&#29109;Gromov-Wasserstein&#36317;&#31163;&#21450;&#20854;&#26410;&#35268;&#21017;&#21270;&#29256;&#26412;&#20063;&#36981;&#24490;&#27492;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entropic optimal transport (EOT) presents an effective and computationally viable alternative to unregularized optimal transport (OT), offering diverse applications for large-scale data analysis. In this work, we derive novel statistical bounds for empirical plug-in estimators of the EOT cost and show that their statistical performance in the entropy regularization parameter $\epsilon$ and the sample size $n$ only depends on the simpler of the two probability measures. For instance, under sufficiently smooth costs this yields the parametric rate $n^{-1/2}$ with factor $\epsilon^{-d/2}$, where $d$ is the minimum dimension of the two population measures. This confirms that empirical EOT also adheres to the lower complexity adaptation principle, a hallmark feature only recently identified for unregularized OT. As a consequence of our theory, we show that the empirical entropic Gromov-Wasserstein distance and its unregularized version for measures on Euclidean spaces also obey this princip
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#23398;&#20064;&#21512;&#36866;&#30340;&#25104;&#26412;&#32467;&#26500;&#26469;&#40723;&#21169;&#26144;&#23556;&#27839;&#30528;&#29305;&#23450;&#30340;&#24037;&#31243;&#29305;&#24449;&#26469;&#20256;&#36865;&#28857;&#65292;&#20854;&#22312;&#25193;&#23637; Monge-Bregman-Occam &#31649;&#36947;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#65292;&#24182;&#22312;&#21305;&#37197;&#30452;&#26041;&#22270;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11895</link><description>&lt;p&gt;
&#23398;&#20064;&#32467;&#26500;&#33945;&#26085;&#20301;&#31227;&#30340;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Learning Costs for Structured Monge Displacements. (arXiv:2306.11895v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#23398;&#20064;&#21512;&#36866;&#30340;&#25104;&#26412;&#32467;&#26500;&#26469;&#40723;&#21169;&#26144;&#23556;&#27839;&#30528;&#29305;&#23450;&#30340;&#24037;&#31243;&#29305;&#24449;&#26469;&#20256;&#36865;&#28857;&#65292;&#20854;&#22312;&#25193;&#23637; Monge-Bregman-Occam &#31649;&#36947;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#65292;&#24182;&#22312;&#21305;&#37197;&#30452;&#26041;&#22270;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#20026;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#22810;&#31181;&#25512;&#26029;&#26679;&#26412;&#38388;&#23494;&#24230;&#21069;&#21521;&#26144;&#23556;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#35813;&#29702;&#35770;&#24050;&#32463;&#35265;&#35777;&#20102;&#35768;&#22810;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#20294;&#20854;&#23454;&#38469;&#23454;&#29616;&#20173;&#28982;&#26497;&#20854;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#21516;&#26102;&#38754;&#20020;&#35745;&#31639;&#21644;&#32479;&#35745;&#19978;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#24456;&#23569;&#26377;&#19981;&#20351;&#29992;&#40664;&#35748;&#36873;&#25321;&#26469;&#20272;&#35745;&#36825;&#20123;&#26144;&#23556;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#31616;&#21333;&#30340;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#20316;&#20026;&#22320;&#38754;&#36153;&#29992;$c(x,y)=\|x-y\|^2_2$&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#37319;&#21462;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20197;\emph{&#23398;&#20064;}&#21512;&#36866;&#30340;&#25104;&#26412;&#32467;&#26500;&#65292;&#40723;&#21169;&#26144;&#23556;&#27839;&#30528;&#29305;&#23450;&#30340;&#24037;&#31243;&#29305;&#24449;&#26469;&#20256;&#36865;&#28857;&#12290;&#25105;&#20204;&#23558;&#26368;&#36817;&#25552;&#20986;&#30340; Monge-Bregman-Occam &#31649;&#36947;~\citep{cuturi2023monge} &#30340;&#33539;&#24335;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#35813;&#33539;&#24335;&#22522;&#20110;&#26367;&#20195;&#30340;&#25104;&#26412;&#20844;&#24335;$c(x,y)=h(x-y)$ &#65292;&#23427;&#20063;&#26159;&#25104;&#26412;&#19981;&#21464;&#30340;&#65292;&#20294;&#37319;&#29992;&#26356;&#19968;&#33324;&#30340;&#24418;&#24335;$h=\tfrac12 \ell_2^2+\tau$&#65292;&#20854;&#20013;$\tau$&#26159;&#36866;&#24403;&#30340;&#20984;&#35268;&#21017;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport theory has provided machine learning with several tools to infer a push-forward map between densities from samples. While this theory has recently seen tremendous methodological developments in machine learning, its practical implementation remains notoriously difficult, because it is plagued by both computational and statistical challenges. Because of such difficulties, existing approaches rarely depart from the default choice of estimating such maps with the simple squared-Euclidean distance as the ground cost, $c(x,y)=\|x-y\|^2_2$. We follow a different path in this work, with the motivation of \emph{learning} a suitable cost structure to encourage maps to transport points along engineered features. We extend the recently proposed Monge-Bregman-Occam pipeline~\citep{cuturi2023monge}, that rests on an alternative cost formulation that is also cost-invariant $c(x,y)=h(x-y)$, but which adopts a more general form as $h=\tfrac12 \ell_2^2+\tau$, where $\tau$ is an approp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27979;&#37327;&#28508;&#22312;&#32467;&#26524;&#22312;&#23384;&#22312;&#25110;&#32570;&#20047;&#27835;&#30103;&#30340;&#24773;&#20917;&#19979;&#30340;&#23614;&#37096;&#34928;&#20943;&#29575;&#21464;&#21270;&#65292;&#26469;&#20272;&#35745;&#26497;&#31471;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65288;ITE$_2$&#65289;&#12290;</title><link>http://arxiv.org/abs/2306.11697</link><description>&lt;p&gt;
&#26497;&#31471;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Individual Treatment Effects in Extreme Regimes. (arXiv:2306.11697v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27979;&#37327;&#28508;&#22312;&#32467;&#26524;&#22312;&#23384;&#22312;&#25110;&#32570;&#20047;&#27835;&#30103;&#30340;&#24773;&#20917;&#19979;&#30340;&#23614;&#37096;&#34928;&#20943;&#29575;&#21464;&#21270;&#65292;&#26469;&#20272;&#35745;&#26497;&#31471;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65288;ITE$_2$&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#26497;&#31471;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#23545;&#20110;&#25551;&#36848;&#19981;&#21516;&#24178;&#39044;&#31574;&#30053;&#30340;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26497;&#31471;&#29615;&#22659;&#25968;&#25454;&#24456;&#38590;&#25910;&#38598;&#65292;&#22240;&#20026;&#23427;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#34987;&#35266;&#23519;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#20272;&#35745;&#26497;&#31471;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65288;ITE$_2$&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#28508;&#22312;&#32467;&#26524;&#22312;&#23384;&#22312;&#25110;&#32570;&#20047;&#27835;&#30103;&#30340;&#24773;&#20917;&#19979;&#30340;&#23614;&#37096;&#34928;&#20943;&#29575;&#21464;&#21270;&#26469;&#37327;&#21270;&#36825;&#31181;&#25928;&#26524;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102; ITE$_2$ &#30340;&#35745;&#31639;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#35745;&#31639;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding individual treatment effects in extreme regimes is important for characterizing risks associated with different interventions. This is hindered by the fact that extreme regime data may be hard to collect, as it is scarcely observed in practice. In addressing this issue, we propose a new framework for estimating the individual treatment effect in extreme regimes (ITE$_2$). Specifically, we quantify this effect by the changes in the tail decay rates of potential outcomes in the presence or absence of the treatment. Subsequently, we establish conditions under which ITE$_2$ may be calculated and develop algorithms for its computation. We demonstrate the efficacy of our proposed method on various synthetic and semi-synthetic datasets.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24120;&#35265;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#20107;&#20808;&#30693;&#36947;&#30495;&#23454;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#25216;&#26415;&#21521;BO&#36807;&#31243;&#20013;&#28155;&#21152;&#38543;&#26426;&#25968;&#25454;&#28857;&#65292;&#37319;&#29992;&#26032;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#36229;&#21442;&#25968;&#20272;&#35745;&#65292;&#20197;&#36798;&#21040;&#27425;&#32447;&#24615;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.06844</link><description>&lt;p&gt;
&#20855;&#26377;&#26080;&#20559;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#20272;&#35745;&#30340;&#21487;&#35777;&#26126;&#39640;&#25928;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Bayesian Optimization with Unbiased Gaussian Process Hyperparameter Estimation. (arXiv:2306.06844v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06844
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24120;&#35265;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#20107;&#20808;&#30693;&#36947;&#30495;&#23454;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#25216;&#26415;&#21521;BO&#36807;&#31243;&#20013;&#28155;&#21152;&#38543;&#26426;&#25968;&#25454;&#28857;&#65292;&#37319;&#29992;&#26032;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#36229;&#21442;&#25968;&#20272;&#35745;&#65292;&#20197;&#36798;&#21040;&#27425;&#32447;&#24615;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#26377;&#25928;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#21644;&#29702;&#35770;&#20445;&#35777;&#65292;&#21462;&#20915;&#20110;&#27491;&#30830;&#20272;&#35745;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#20540;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#24120;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#21487;&#33021;&#20250;&#24341;&#36215;&#25968;&#25454;&#20559;&#24046;&#65292;&#20174;&#32780;&#23548;&#33268;&#36229;&#21442;&#25968;&#20272;&#35745;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#20107;&#20808;&#19981;&#30693;&#36947;&#30495;&#23454;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#24182;&#38656;&#35201;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#20272;&#35745;&#26102;&#65292;&#35813;&#26041;&#27861;&#20063;&#33021;&#22815;&#27425;&#32447;&#24615;&#25910;&#25947;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#25216;&#26415;(EXP3)&#21521;BO&#36807;&#31243;&#20013;&#28155;&#21152;&#38543;&#26426;&#25968;&#25454;&#28857;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#20272;&#35745;&#36807;&#31243;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian process (GP) based Bayesian optimization (BO) is a powerful method for optimizing black-box functions efficiently. The practical performance and theoretical guarantees associated with this approach depend on having the correct GP hyperparameter values, which are usually unknown in advance and need to be estimated from the observed data. However, in practice, these estimations could be incorrect due to biased data sampling strategies commonly used in BO. This can lead to degraded performance and break the sub-linear global convergence guarantee of BO. To address this issue, we propose a new BO method that can sub-linearly converge to the global optimum of the objective function even when the true GP hyperparameters are unknown in advance and need to be estimated from the observed data. Our method uses a multi-armed bandit technique (EXP3) to add random data points to the BO process, and employs a novel training loss function for the GP hyperparameter estimation process that ens
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#32447;&#24615;Bandit&#30340;Pareto&#21069;&#27839;&#35782;&#21035;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25152;&#26377;&#21160;&#20316;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#21516;&#26102;&#20445;&#35777;&#20102;Pareto&#21069;&#27839;&#35782;&#21035;&#21644;Pareto&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.00096</link><description>&lt;p&gt;
&#36890;&#36807;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#27861;&#36827;&#34892;Pareto&#21069;&#27839;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Pareto Front Identification with Regret Minimization. (arXiv:2306.00096v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#32447;&#24615;Bandit&#30340;Pareto&#21069;&#27839;&#35782;&#21035;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25152;&#26377;&#21160;&#20316;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#21516;&#26102;&#20445;&#35777;&#20102;Pareto&#21069;&#27839;&#35782;&#21035;&#21644;Pareto&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#32447;&#24615;Bandit&#24773;&#20917;&#19979;&#30340;Pareto&#21069;&#27839;&#35782;&#21035;&#65288;PFILin&#65289;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#24179;&#22343;&#22870;&#21169;&#21521;&#37327;&#20316;&#20026;&#29615;&#22659;&#30340;&#32447;&#24615;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#19968;&#32452;&#22870;&#21169;&#21521;&#37327;&#19981;&#34987;&#20854;&#20182;&#20219;&#20309;&#21521;&#37327;&#25152;&#21344;&#20248;&#12290;PFILin&#21253;&#25324;&#26368;&#20339;&#21160;&#20316;&#35782;&#21035;&#21644;&#22810;&#30446;&#26631;&#20027;&#21160;&#23398;&#20064;&#31561;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\tilde{O}(d/\Delta^2)$&#65292;&#20854;&#20013;$d$&#26159;&#19978;&#19979;&#25991;&#30340;&#32500;&#25968;&#65292;$\Delta$&#26159;&#38382;&#39064;&#22797;&#26434;&#24615;&#30340;&#19968;&#31181;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#23545;&#25968;&#22240;&#23376;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;&#26412;&#31639;&#27861;&#30340;&#19968;&#20010;&#26032;&#29305;&#28857;&#26159;&#20351;&#29992;&#25152;&#26377;&#21160;&#20316;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#38500;&#20102;&#26377;&#25928;&#22320;&#35782;&#21035;Pareto&#21069;&#27839;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36824;&#20445;&#35777;&#65292;&#22312;&#26679;&#26412;&#25968;&#22823;&#20110;$\Omega(d\log dL)$&#26102;&#65292;&#23545;&#20110;$L$&#32500;&#30690;&#37327;&#22870;&#21169;&#65292;&#30636;&#26102;Pareto&#36951;&#25022;&#30340;$\tilde{O}(\sqrt{d/t})$&#30028;&#38480;&#12290;&#36890;&#36807;&#20351;&#29992;&#25152;&#26377;&#21160;&#20316;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21516;&#26102;&#20026;&#32447;&#24615;Bandit&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;Pareto&#21069;&#27839;&#35782;&#21035;&#21644;Pareto&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider Pareto front identification for linear bandits (PFILin) where the goal is to identify a set of arms whose reward vectors are not dominated by any of the others when the mean reward vector is a linear function of the context. PFILin includes the best arm identification problem and multi-objective active learning as special cases. The sample complexity of our proposed algorithm is $\tilde{O}(d/\Delta^2)$, where $d$ is the dimension of contexts and $\Delta$ is a measure of problem complexity. Our sample complexity is optimal up to a logarithmic factor. A novel feature of our algorithm is that it uses the contexts of all actions. In addition to efficiently identifying the Pareto front, our algorithm also guarantees $\tilde{O}(\sqrt{d/t})$ bound for instantaneous Pareto regret when the number of samples is larger than $\Omega(d\log dL)$ for $L$ dimensional vector rewards. By using the contexts of all arms, our proposed algorithm simultaneously provides efficient Pareto front ide
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312; mini-batch &#20013;&#26174;&#24335;&#22320;&#23398;&#20064;&#35823;&#24046;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#26469;&#25552;&#39640;&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.17028</link><description>&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26356;&#22909;Batch&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Better Batch for Deep Probabilistic Time Series Forecasting. (arXiv:2305.17028v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312; mini-batch &#20013;&#26174;&#24335;&#22320;&#23398;&#20064;&#35823;&#24046;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#26469;&#25552;&#39640;&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22240;&#20854;&#33021;&#22815;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#36807;&#20110;&#31616;&#21333;&#21270;&#38382;&#39064;&#65292;&#20551;&#35774;&#35823;&#24046;&#36807;&#31243;&#26159;&#19982;&#26102;&#38388;&#26080;&#20851;&#30340;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#35823;&#24046;&#36807;&#31243;&#20013;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#12290;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#23545;&#20915;&#31574;&#24615;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#20943;&#24369;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#35823;&#24046;&#33258;&#30456;&#20851;&#24615;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#22686;&#24378;&#27010;&#29575;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#36896;&#19968;&#20010;mini-batch&#65292;&#20316;&#20026;$D$&#20010;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#27573;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#26174;&#24335;&#22320;&#23398;&#20064;&#19968;&#20010;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#35206;&#30422;&#20102;&#30456;&#37051;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#35823;&#24046;&#30456;&#20851;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#21487;&#29992;&#20110;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep probabilistic time series forecasting has gained significant attention due to its ability to provide valuable uncertainty quantification for decision-making tasks. However, many existing models oversimplify the problem by assuming the error process is time-independent, thereby overlooking the serial correlation in the error process. This oversight can potentially diminish the accuracy of the forecasts, rendering these models less effective for decision-making purposes. To overcome this limitation, we propose an innovative training method that incorporates error autocorrelation to enhance the accuracy of probabilistic forecasting. Our method involves constructing a mini-batch as a collection of $D$ consecutive time series segments for model training and explicitly learning a covariance matrix over each mini-batch that encodes the error correlation among adjacent time steps. The resulting covariance matrix can be used to improve prediction accuracy and enhance uncertainty quantifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ReLU&#21333;&#20803;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20248;&#21270;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15912</link><description>&lt;p&gt;
&#25913;&#36827;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#30340;&#31070;&#32463;&#29305;&#24449;&#28608;&#27963;&#20540;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning. (arXiv:2305.15912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ReLU&#21333;&#20803;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20248;&#21270;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#21333;&#20010;ReLU&#21333;&#20803;&#30340;&#29305;&#24449;&#28608;&#27963;&#20540;&#12290;&#25105;&#20204;&#23558;ReLU&#21333;&#20803;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#23545;&#24212;&#30340;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#31216;&#20026;ReLU&#21333;&#20803;&#30340;&#29305;&#24449;&#28608;&#27963;&#38598;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29305;&#24449;&#28608;&#27963;&#38598;&#19982;ReLU&#32593;&#32476;&#20013;&#23398;&#20064;&#29305;&#24449;&#20043;&#38388;&#30340;&#26126;&#30830;&#32852;&#31995;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#25216;&#26415;&#22914;&#20309;&#35268;&#33539;&#21270;&#21644;&#31283;&#23450;SGD&#20248;&#21270;&#12290;&#21033;&#29992;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;ReLU&#32593;&#32476;&#20197;&#25913;&#36827;&#29305;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#20854;&#26377;&#29992;&#24615;&#65292;&#20351;&#29992;&#20102;&#19981;&#37027;&#20040;&#31934;&#24515;&#36873;&#25321;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#21644;&#26356;&#22823;&#30340;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#26356;&#22909;&#30340;&#20248;&#21270;&#31283;&#23450;&#24615;&#65292;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the characteristic activation values of individual ReLU units in neural networks. We refer to the corresponding set for such characteristic activation values in the input space as the characteristic activation set of a ReLU unit. We draw an explicit connection between the characteristic activation set and learned features in ReLU networks. This connection leads to new insights into why various neural network normalization techniques used in modern deep learning architectures regularize and stabilize SGD optimization. Utilizing these insights, we propose a geometric approach to parameterize ReLU networks for improved feature learning. We empirically verify its usefulness with less carefully chosen initialization schemes and larger learning rates. We report improved optimization stability, faster convergence speed, and better generalization performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65288;CEPIA&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#33719;&#24471;&#26377;&#25928;&#30340;&#34892;&#21160;&#24314;&#35758;&#24182;&#20102;&#35299;&#32570;&#22833;&#20540;&#23545;&#24314;&#35758;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.14606</link><description>&lt;p&gt;
&#32570;&#22833;&#20540;&#19979;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanation with Missing Values. (arXiv:2304.14606v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65288;CEPIA&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#33719;&#24471;&#26377;&#25928;&#30340;&#34892;&#21160;&#24314;&#35758;&#24182;&#20102;&#35299;&#32570;&#22833;&#20540;&#23545;&#24314;&#35758;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#19968;&#31181;&#25552;&#20379;&#25200;&#21160;&#20197;&#25913;&#21464;&#20998;&#31867;&#22120;&#39044;&#27979;&#32467;&#26524;&#30340;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#23436;&#25972;&#20449;&#24687;&#30340;&#36755;&#20837;&#65292;&#20294;&#23454;&#38469;&#24773;&#20917;&#20013;&#24448;&#24448;&#20250;&#26377;&#32570;&#22833;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;CE&#26694;&#26550;&#65288;&#31216;&#20026;CEPIA&#65289;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#22312;&#26377;&#32570;&#22833;&#20540;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26377;&#25928;&#30340;&#25805;&#20316;&#65292;&#24182;&#38416;&#26126;&#32570;&#22833;&#20540;&#23545;&#25805;&#20316;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanation (CE) is a post-hoc explanation method that provides a perturbation for altering the prediction result of a classifier. Users can interpret the perturbation as an "action" to obtain their desired decision results. Existing CE methods require complete information on the features of an input instance. However, we often encounter missing values in a given instance, and the previous methods do not work in such a practical situation. In this paper, we first empirically and theoretically show the risk that missing value imputation methods affect the validity of an action, as well as the features that the action suggests changing. Then, we propose a new framework of CE, named Counterfactual Explanation by Pairs of Imputation and Action (CEPIA), that enables users to obtain valid actions even with missing values and clarifies how actions are affected by imputation of the missing values. Specifically, our CEPIA provides a representative set of pairs of an imputation ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26080;&#20851;&#22870;&#21169;&#25506;&#32034;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#20445;&#35777;&#26679;&#26412;&#25910;&#38598;&#25968;&#37327;&#28385;&#36275;&#22810;&#39033;&#24335;&#32423;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#21457;&#29616;&#25152;&#26377;&#32473;&#23450;&#22870;&#21169;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#25506;&#32034;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.07278</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#26080;&#20851;&#22870;&#21169;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning. (arXiv:2304.07278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26080;&#20851;&#22870;&#21169;&#25506;&#32034;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#20445;&#35777;&#26679;&#26412;&#25910;&#38598;&#25968;&#37327;&#28385;&#36275;&#22810;&#39033;&#24335;&#32423;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#21457;&#29616;&#25152;&#26377;&#32473;&#23450;&#22870;&#21169;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#25506;&#32034;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26080;&#20851;&#22870;&#21169;&#25506;&#32034;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#25913;&#36827;&#29616;&#26377;&#25216;&#26415;&#12290;&#30740;&#31350;&#20102;&#20855;&#26377;S&#20010;&#29366;&#24577;&#65292;A&#20010;&#21160;&#20316;&#21644;&#26377;&#38480;&#26102;&#38388;&#27700;&#24179;H&#30340;&#38750;&#24179;&#31283;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#23450;&#25968;&#37327;&#30340;&#26080;&#24341;&#23548;&#22870;&#21169;&#20449;&#24687;&#30340;&#26679;&#26412;&#38598;&#65292;&#22312;&#20445;&#35777;&#25910;&#38598;&#30340;&#25968;&#37327;&#28385;&#36275;&#22810;&#39033;&#24335;&#32423;&#21035;&#26102;&#65292;&#31639;&#27861;&#33021;&#22815;&#21457;&#29616;&#25152;&#26377;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#25506;&#32034;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies reward-agnostic exploration in reinforcement learning (RL) -- a scenario where the learner is unware of the reward functions during the exploration stage -- and designs an algorithm that improves over the state of the art. More precisely, consider a finite-horizon non-stationary Markov decision process with $S$ states, $A$ actions, and horizon length $H$, and suppose that there are no more than a polynomial number of given reward functions of interest. By collecting an order of \begin{align*}  \frac{SAH^3}{\varepsilon^2} \text{ sample episodes (up to log factor)} \end{align*} without guidance of the reward information, our algorithm is able to find $\varepsilon$-optimal policies for all these reward functions, provided that $\varepsilon$ is sufficiently small. This forms the first reward-agnostic exploration scheme in this context that achieves provable minimax optimality. Furthermore, once the sample size exceeds $\frac{S^2AH^3}{\varepsilon^2}$ episodes (up to log f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#26469;&#35774;&#35745;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#32452;&#20214;&#65292;&#20026;&#35299;&#20915;&#28041;&#21450;&#20840;&#23616;&#31354;&#38388;&#21644;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22270;&#24418;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.07482</link><description>&lt;p&gt;
&#29992;&#24352;&#37327;&#32593;&#32476;&#24418;&#24335;&#32479;&#19968;O(3)&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Unifying O(3) Equivariant Neural Networks Design with Tensor-Network Formalism. (arXiv:2211.07482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#26469;&#35774;&#35745;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#32452;&#20214;&#65292;&#20026;&#35299;&#20915;&#28041;&#21450;&#20840;&#23616;&#31354;&#38388;&#21644;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22270;&#24418;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#20174;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#20013;&#23398;&#20064;&#21183;&#33021;&#38754;&#65292;&#28041;&#21450;&#21040;&#20840;&#23616;&#31354;&#38388;&#23545;&#31216;&#24615;&#21644;&#21407;&#23376;&#25110;&#19968;&#33324;&#31890;&#23376;&#20043;&#38388;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#12290;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#20043;&#19968;&#65292;&#20854;&#20013;&#26368;&#25104;&#21151;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#20351;&#29992;&#22312;&#31354;&#38388;&#32676;&#19979;&#21464;&#25442;&#30340;&#21508;&#31181;&#24352;&#37327;&#20043;&#38388;&#30340;&#24352;&#37327;&#31215;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#19981;&#21516;&#24352;&#37327;&#30340;&#25968;&#37327;&#21644;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20445;&#25345;&#31616;&#27905;&#21644;&#31561;&#21464;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#65292;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#27169;&#25311;SU(2)&#23545;&#31216;&#37327;&#23376;&#22810;&#20307;&#38382;&#39064;&#30340;&#25216;&#26415;&#65292;&#26469;&#20026;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#26032;&#30340;&#31561;&#21464;&#32452;&#20214;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#24403;&#24212;&#29992;&#20110;&#32473;&#23450;&#23616;&#37096;&#37051;&#22495;&#20013;&#30340;&#31890;&#23376;&#26102;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#34701;&#21512;&#22359;&#8221;&#30340;&#32467;&#26524;&#32452;&#20214;&#36215;&#21040;&#20102;
&lt;/p&gt;
&lt;p&gt;
Many learning tasks, including learning potential energy surfaces from ab initio calculations, involve global spatial symmetries and permutational symmetry between atoms or general particles. Equivariant graph neural networks are a standard approach to such problems, with one of the most successful methods employing tensor products between various tensors that transform under the spatial group. However, as the number of different tensors and the complexity of relationships between them increase, maintaining parsimony and equivariance becomes increasingly challenging. In this paper, we propose using fusion diagrams, a technique widely employed in simulating SU($2$)-symmetric quantum many-body problems, to design new equivariant components for equivariant neural networks. This results in a diagrammatic approach to constructing novel neural network architectures. When applied to particles within a given local neighborhood, the resulting components, which we term "fusion blocks," serve as 
&lt;/p&gt;</description></item><item><title>GeONet&#26159;&#19968;&#20010;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23398;&#20064;&#20102;&#20174;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#36890;&#36807;&#23398;&#20064;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#65292;GeONet&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#24182;&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;&#27979;&#35797;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934;OT&#27714;&#35299;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.14440</link><description>&lt;p&gt;
GeONet&#65306;&#19968;&#31181;&#23398;&#20064;Wasserstein&#27979;&#22320;&#30340;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
GeONet: a neural operator for learning the Wasserstein geodesic. (arXiv:2209.14440v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14440
&lt;/p&gt;
&lt;p&gt;
GeONet&#26159;&#19968;&#20010;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23398;&#20064;&#20102;&#20174;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#36890;&#36807;&#23398;&#20064;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#65292;GeONet&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#24182;&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;&#27979;&#35797;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934;OT&#27714;&#35299;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;(OT)&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20960;&#20309;&#19978;&#26377;&#24847;&#20041;&#27604;&#36739;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#20256;&#32479;&#30340;&#35745;&#31639;&#27010;&#29575;&#27979;&#24230;&#30340;Wasserstein&#36317;&#31163;&#21644;&#27979;&#22320;&#30340;&#26041;&#27861;&#38656;&#35201;&#32593;&#26684;&#20381;&#36182;&#30340;&#22495;&#31163;&#25955;&#21270;&#65292;&#21516;&#26102;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GeONet&#65292;&#19968;&#31181;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23427;&#23398;&#20064;&#20102;&#23558;&#36755;&#20837;&#30340;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#26144;&#23556;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#22312;&#33073;&#26426;&#35757;&#32451;&#38454;&#27573;&#65292;GeONet&#36890;&#36807;&#32806;&#21512;&#30340;PDE&#31995;&#32479;&#34920;&#24449;&#30340;&#21407;&#22987;&#21644;&#23545;&#20598;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#26368;&#20248;&#26465;&#20214;&#23398;&#20064;&#20102;OT&#38382;&#39064;&#30340;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#12290;&#21518;&#32493;&#30340;&#25512;&#29702;&#38454;&#27573;&#26159;&#30636;&#26102;&#23436;&#25104;&#30340;&#65292;&#24182;&#21487;&#20197;&#22312;&#22312;&#32447;&#23398;&#20064;&#35774;&#32622;&#20013;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GeONet&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) offers a versatile framework to compare complex data distributions in a geometrically meaningful way. Traditional methods for computing the Wasserstein distance and geodesic between probability measures require mesh-dependent domain discretization and suffer from the curse-of-dimensionality. We present GeONet, a mesh-invariant deep neural operator network that learns the non-linear mapping from the input pair of initial and terminal distributions to the Wasserstein geodesic connecting the two endpoint distributions. In the offline training stage, GeONet learns the saddle point optimality conditions for the dynamic formulation of the OT problem in the primal and dual spaces that are characterized by a coupled PDE system. The subsequent inference stage is instantaneous and can be deployed for real-time predictions in the online learning setting. We demonstrate that GeONet achieves comparable testing accuracy to the standard OT solvers on simulation examples and the
&lt;/p&gt;</description></item></channel></rss>