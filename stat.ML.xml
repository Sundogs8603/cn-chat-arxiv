<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20013;&#36827;&#34892;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#32479;&#35745;&#20998;&#26512;&#36807;&#31243;&#20013;&#21482;&#33021;&#35775;&#38382;&#31169;&#26377;&#21270;&#25968;&#25454;&#23548;&#33268;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12781</link><description>&lt;p&gt;
&#20174;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20013;&#36827;&#34892;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Conditional Density Estimations from Privacy-Protected Data. (arXiv:2310.12781v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20013;&#36827;&#34892;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#32479;&#35745;&#20998;&#26512;&#36807;&#31243;&#20013;&#21482;&#33021;&#35775;&#38382;&#31169;&#26377;&#21270;&#25968;&#25454;&#23548;&#33268;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#20195;&#32479;&#35745;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#38656;&#35201;&#22312;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#20102;&#19968;&#31181;&#27491;&#24335;&#30340;&#20445;&#35777;&#65292;&#21363;&#20010;&#20307;&#29992;&#25143;&#20449;&#24687;&#19981;&#20250;&#27844;&#38706;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#38543;&#26426;&#31639;&#27861;&#21521;&#20445;&#23494;&#25968;&#25454;&#27880;&#20837;&#26657;&#20934;&#30340;&#22122;&#22768;&#65292;&#20174;&#32780;&#20135;&#29983;&#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#38598;&#25110;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#22312;&#32479;&#35745;&#20998;&#26512;&#36807;&#31243;&#20013;&#21482;&#33021;&#35775;&#38382;&#31169;&#26377;&#21270;&#25968;&#25454;&#20250;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#21152;&#65292;&#38590;&#20197;&#23545;&#22522;&#30784;&#26426;&#23494;&#25968;&#25454;&#30340;&#21442;&#25968;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#38598;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#29702;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#20316;&#20026;&#19968;&#32452;&#28789;&#27963;&#30340;&#20998;&#24067;&#26469;&#36817;&#20284;&#32473;&#23450;&#35266;&#27979;&#21040;&#30340;&#31169;&#26377;&#26597;&#35810;&#32467;&#26524;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#20256;&#26579;&#30149;&#27169;&#22411;&#19979;&#30340;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20197;&#21450;&#26222;&#36890;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#19978;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many modern statistical analysis and machine learning applications require training models on sensitive user data. Differential privacy provides a formal guarantee that individual-level information about users does not leak. In this framework, randomized algorithms inject calibrated noise into the confidential data, resulting in privacy-protected datasets or queries. However, restricting access to only the privatized data during statistical analysis makes it computationally challenging to perform valid inferences on parameters underlying the confidential data. In this work, we propose simulation-based inference methods from privacy-protected datasets. Specifically, we use neural conditional density estimators as a flexible family of distributions to approximate the posterior distribution of model parameters given the observed private query results. We illustrate our methods on discrete time-series data under an infectious disease model and on ordinary linear regression models. Illustra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Rank-Tuning&#30340;&#35757;&#32451;&#21518;&#31209;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#39640;&#25928;&#21387;&#32553;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#21387;&#32553;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.12688</link><description>&lt;p&gt;
&#20351;&#29992;&#30697;&#38453;&#22240;&#24335;&#20998;&#35299;&#21387;&#32553;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Compression of Recurrent Neural Networks using Matrix Factorization. (arXiv:2310.12688v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Rank-Tuning&#30340;&#35757;&#32451;&#21518;&#31209;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#39640;&#25928;&#21387;&#32553;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#21387;&#32553;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#26102;&#25110;&#23884;&#20837;&#24335;&#24212;&#29992;&#20013;&#37096;&#32626;&#27169;&#22411;&#26102;&#65292;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#20351;&#29992;&#20302;&#31209;&#36817;&#20284;&#23545;&#27169;&#22411;&#30340;&#30697;&#38453;&#36827;&#34892;&#20998;&#35299;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#34429;&#28982;&#22312;&#35757;&#32451;&#20043;&#21069;&#21487;&#20197;&#35774;&#32622;&#31209;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#26082;&#19981;&#28789;&#27963;&#20063;&#19981;&#26368;&#20248;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Rank-Tuning&#30340;&#35757;&#32451;&#21518;&#31209;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#27599;&#20010;&#30697;&#38453;&#36873;&#25321;&#19981;&#21516;&#30340;&#31209;&#12290;&#32467;&#21512;&#35757;&#32451;&#36866;&#24212;&#24615;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#38477;&#20302;&#25110;&#32773;&#26377;&#24456;&#23569;&#24615;&#33021;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39640;&#21387;&#32553;&#29575;&#12290;&#25105;&#20204;&#22312;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#33267;&#26368;&#22810;14&#20493;&#65292;&#19988;&#30456;&#23545;&#24615;&#33021;&#38477;&#20302;&#26368;&#22810;&#20026;1.4%&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressing neural networks is a key step when deploying models for real-time or embedded applications. Factorizing the model's matrices using low-rank approximations is a promising method for achieving compression. While it is possible to set the rank before training, this approach is neither flexible nor optimal. In this work, we propose a post-training rank-selection method called Rank-Tuning that selects a different rank for each matrix. Used in combination with training adaptations, our method achieves high compression rates with no or little performance degradation. Our numerical experiments on signal processing tasks show that we can compress recurrent neural networks up to 14x with at most 1.4% relative performance reduction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STANLEY&#30340;&#31639;&#27861;&#29992;&#20110;&#37319;&#26679;&#39640;&#32500;&#25968;&#25454;&#65292;&#25913;&#21892;&#20102;&#33021;&#37327;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.12667</link><description>&lt;p&gt;
STANLEY&#65306;&#29992;&#20110;&#23398;&#20064;&#33021;&#37327;&#27169;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;&#24322;&#21521;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
STANLEY: Stochastic Gradient Anisotropic Langevin Dynamics for Learning Energy-Based Models. (arXiv:2310.12667v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STANLEY&#30340;&#31639;&#27861;&#29992;&#20110;&#37319;&#26679;&#39640;&#32500;&#25968;&#25454;&#65292;&#25913;&#21892;&#20102;&#33021;&#37327;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STANLEY&#30340;&#38543;&#26426;&#26799;&#24230;&#24322;&#21521;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#29992;&#20110;&#37319;&#26679;&#39640;&#32500;&#25968;&#25454;&#12290;&#36890;&#36807;&#22686;&#24378;&#33021;&#37327;&#27169;&#22411;&#65288;EBM&#65289;&#30340;&#23398;&#20064;&#31639;&#27861;&#26469;&#25913;&#21892;&#37319;&#26679;&#25968;&#25454;&#28857;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;EBM&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20063;&#34987;&#31216;&#20026;&#38750;&#24402;&#19968;&#21270;&#27010;&#29575;&#24314;&#27169;&#12290;&#30001;&#20110;EBMs&#30340;&#26410;&#30693;&#24402;&#19968;&#21270;&#24120;&#25968;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#38590;&#20197;&#22788;&#29702;&#65292;&#37319;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#36890;&#24120;&#26159;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#32500;&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#32454;&#20998;&#38543;&#26426;&#36807;&#31243;&#30340;&#24322;&#21521;&#27493;&#38271;&#21644;&#26799;&#24230;&#20449;&#24687;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#25105;&#20204;&#36890;&#36807;&#35770;&#35777;&#39532;&#23572;&#31185;&#22827;&#38142;&#20013;&#36127;&#26679;&#26412;&#30340;&#24322;&#21521;&#26356;&#26032;&#30340;&#24517;&#35201;&#24615;&#26469;&#35299;&#37322;&#20102;MCMC&#22312;EBM&#35757;&#32451;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose in this paper, STANLEY, a STochastic gradient ANisotropic LangEvin dYnamics, for sampling high dimensional data. With the growing efficacy and potential of Energy-Based modeling, also known as non-normalized probabilistic modeling, for modeling a generative process of different natures of high dimensional data observations, we present an end-to-end learning algorithm for Energy-Based models (EBM) with the purpose of improving the quality of the resulting sampled data points. While the unknown normalizing constant of EBMs makes the training procedure intractable, resorting to Markov Chain Monte Carlo (MCMC) is in general a viable option. Realizing what MCMC entails for the EBM training, we propose in this paper, a novel high dimensional sampling method, based on an anisotropic stepsize and a gradient-informed covariance matrix, embedded into a discretized Langevin diffusion. We motivate the necessity for an anisotropic update of the negative samples in the Markov Chain by the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#20174;&#20855;&#26377;&#30456;&#20284;&#22240;&#26524;&#26426;&#21046;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#27719;&#38598;&#25968;&#25454;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12595</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Causal Similarity-Based Hierarchical Bayesian Models. (arXiv:2310.12595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#20174;&#20855;&#26377;&#30456;&#20284;&#22240;&#26524;&#26426;&#21046;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#27719;&#38598;&#25968;&#25454;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#23545;&#26032;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#30001;&#30456;&#20851;&#20219;&#21153;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#20219;&#21153;&#21487;&#33021;&#22312;&#22240;&#26524;&#26426;&#21046;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#20363;&#22914;&#65292;&#22797;&#26434;&#30142;&#30149;&#30340;&#35266;&#23519;&#24615;&#21307;&#23398;&#25968;&#25454;&#22312;&#19981;&#21516;&#24739;&#32773;&#38388;&#20855;&#26377;&#30142;&#30149;&#22240;&#26524;&#26426;&#21046;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#32473;&#38656;&#35201;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#20043;&#22806;&#30340;&#26032;&#24739;&#32773;&#36827;&#34892;&#27867;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#24120;&#29992;&#30340;&#22788;&#29702;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#21253;&#25324;&#20026;&#25972;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#23398;&#20064;&#26412;&#22320;&#27169;&#22411;&#65292;&#25110;&#32773;&#21033;&#29992;&#20998;&#23618;&#12289;&#20803;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#20174;&#27719;&#38598;&#30340;&#22810;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#27867;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#20174;&#20855;&#26377;&#30456;&#20284;&#22240;&#26524;&#26426;&#21046;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#27719;&#38598;&#25968;&#25454;&#26469;&#25552;&#39640;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#24212;&#29992;&#36825;&#31181;&#36890;&#29992;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The key challenge underlying machine learning is generalisation to new data. This work studies generalisation for datasets consisting of related tasks that may differ in causal mechanisms. For example, observational medical data for complex diseases suffers from heterogeneity in causal mechanisms of disease across patients, creating challenges for machine learning algorithms that need to generalise to new patients outside of the training dataset. Common approaches for learning supervised models with heterogeneous datasets include learning a global model for the entire dataset, learning local models for each tasks' data, or utilising hierarchical, meta-learning and multi-task learning approaches to learn how to generalise from data pooled across multiple tasks. In this paper we propose causal similarity-based hierarchical Bayesian models to improve generalisation to new tasks by learning how to pool data from training tasks with similar causal mechanisms. We apply this general modelling
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26435;&#37325;&#21644;&#36755;&#20986;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;Transformer&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#30740;&#31350;&#32467;&#26524;&#26263;&#31034;&#27169;&#22411;&#35774;&#35745;&#23384;&#22312;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2310.12462</link><description>&lt;p&gt;
&#25581;&#31034;Transformer&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#26435;&#37325;&#30340;&#25968;&#25454;&#24674;&#22797;&#30340;&#29702;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights. (arXiv:2310.12462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26435;&#37325;&#21644;&#36755;&#20986;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;Transformer&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#30740;&#31350;&#32467;&#26524;&#26263;&#31034;&#27169;&#22411;&#35774;&#35745;&#23384;&#22312;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;Transformer&#24050;&#32463;&#25104;&#20026;&#20102;&#19968;&#31181;&#20027;&#23548;&#30340;&#26550;&#26500;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#26377;&#20851;&#36825;&#20123;&#27169;&#22411;&#22788;&#29702;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#30340;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65306;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;Transformer&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#21644;&#36755;&#20986;&#26469;&#24674;&#22797;&#36755;&#20837;&#25968;&#25454;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;$L(X)$&#20174;&#32473;&#23450;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;$W = QK^\top$&#21644;&#36755;&#20986;$B$&#20013;&#24674;&#22797;&#36755;&#20837;&#25968;&#25454;$X$&#65292;&#20854;&#20013;$X \in \mathbb{R}^{d \times n}$&#65292;$W \in \mathbb{R}^{d \times d}$&#65292;$B \in \mathbb{R}^{n \times n}$&#12290;&#36825;&#20010;&#25439;&#22833;&#20989;&#25968;&#25429;&#25417;&#20102;&#39044;&#26399;&#36755;&#20986;&#19982;&#23454;&#38469;&#36755;&#20986;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#23616;&#37096;&#21270;&#20998;&#23618;&#26426;&#21046;&#65288;Localized Layer-wise Mechanism&#65292;LLM&#65289;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#65292;&#34920;&#26126;&#27169;&#22411;&#35774;&#35745;&#23384;&#22312;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of deep learning, transformers have emerged as a dominant architecture, particularly in natural language processing tasks. However, with their widespread adoption, concerns regarding the security and privacy of the data processed by these models have arisen. In this paper, we address a pivotal question: Can the data fed into transformers be recovered using their attention weights and outputs? We introduce a theoretical framework to tackle this problem. Specifically, we present an algorithm that aims to recover the input data $X \in \mathbb{R}^{d \times n}$ from given attention weights $W = QK^\top \in \mathbb{R}^{d \times d}$ and output $B \in \mathbb{R}^{n \times n}$ by minimizing the loss function $L(X)$. This loss function captures the discrepancy between the expected output and the actual output of the transformer. Our findings have significant implications for the Localized Layer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model's design from a s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24418;&#24335;&#22788;&#29702;&#19981;&#21516;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#27169;&#25311;&#36712;&#36857;&#25110;&#35775;&#38382;&#26368;&#20248;&#32806;&#21512;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10649</link><description>&lt;p&gt;
&#29992;&#20110;&#27714;&#35299;Wasserstein Lagrangian&#27969;&#30340;&#35745;&#31639;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Computational Framework for Solving Wasserstein Lagrangian Flows. (arXiv:2310.10649v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24418;&#24335;&#22788;&#29702;&#19981;&#21516;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#27169;&#25311;&#36712;&#36857;&#25110;&#35775;&#38382;&#26368;&#20248;&#32806;&#21512;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#30340;&#22522;&#30784;&#20960;&#20309;&#65288;&#21160;&#33021;&#65289;&#21644;&#23494;&#24230;&#36335;&#24452;&#30340;&#27491;&#21017;&#21270;&#65288;&#21183;&#33021;&#65289;&#65292;&#21487;&#20197;&#23545;&#26368;&#20248;&#36755;&#36816;&#30340;&#21160;&#21147;&#23398;&#24418;&#24335;&#36827;&#34892;&#25512;&#24191;&#12290;&#36825;&#20123;&#32452;&#21512;&#20135;&#29983;&#19981;&#21516;&#30340;&#21464;&#20998;&#38382;&#39064;&#65288;Lagrangians&#65289;&#65292;&#28085;&#30422;&#20102;&#35768;&#22810;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#22914;Schr&#246;dinger&#26725;&#12289;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21644;&#24102;&#26377;&#29289;&#29702;&#32422;&#26463;&#30340;&#26368;&#20248;&#36755;&#36816;&#31561;&#12290;&#19968;&#33324;&#32780;&#35328;&#65292;&#26368;&#20248;&#23494;&#24230;&#36335;&#24452;&#26159;&#26410;&#30693;&#30340;&#65292;&#35299;&#20915;&#36825;&#20123;&#21464;&#20998;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20511;&#21161;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24418;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20174;&#32479;&#19968;&#30340;&#35282;&#24230;&#22788;&#29702;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#27169;&#25311;&#25110;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#36712;&#36857;&#65292;&#20063;&#19981;&#38656;&#35201;&#35775;&#38382;&#26368;&#20248;&#32806;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#36890;&#36807;&#36229;&#36234;&#20102;&#20854;&#20182;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamical formulation of the optimal transport can be extended through various choices of the underlying geometry ($\textit{kinetic energy}$), and the regularization of density paths ($\textit{potential energy}$). These combinations yield different variational problems ($\textit{Lagrangians}$), encompassing many variations of the optimal transport problem such as the Schr\"odinger bridge, unbalanced optimal transport, and optimal transport with physical constraints, among others. In general, the optimal density path is unknown, and solving these variational problems can be computationally challenging. Leveraging the dual formulation of the Lagrangians, we propose a novel deep learning based framework approaching all of these problems from a unified perspective. Our method does not require simulating or backpropagating through the trajectories of the learned dynamics, and does not need access to optimal couplings. We showcase the versatility of the proposed framework by outperformin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#32479;&#19968;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25910;&#25947;&#36895;&#24230;&#20026;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.08237</link><description>&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#32479;&#19968;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift. (arXiv:2310.08237v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#32479;&#19968;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25910;&#25947;&#36895;&#24230;&#20026;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21327;&#21464;&#37327;&#28418;&#31227;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#21363;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#36755;&#20837;&#20998;&#24067;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#23613;&#31649;&#22312;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#20013;&#20855;&#26377;&#23454;&#38469;&#37325;&#35201;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#20851;&#27880;&#20110;&#19968;&#20123;&#29305;&#23450;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#27809;&#26377;&#22312;&#29702;&#35770;&#19978;&#21644;&#25968;&#20540;&#19978;&#24471;&#21040;&#24456;&#22909;&#30340;&#39564;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#32479;&#19968;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#23646;&#20110;&#19968;&#20010;&#20016;&#23500;&#30340;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#30340;&#19968;&#33324;&#25439;&#22833;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#22914;&#22343;&#20540;&#22238;&#24402;&#12289;&#20998;&#20301;&#25968;&#22238;&#24402;&#12289;&#22522;&#20110;&#20284;&#28982;&#30340;&#20998;&#31867;&#21644;&#22522;&#20110;&#36793;&#32536;&#30340;&#20998;&#31867;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20004;&#31867;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20026;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#24314;&#31435;&#20102;&#23574;&#38160;&#30340;&#25910;&#25947;&#36895;&#24230;&#20197;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35813;&#32467;&#26524;&#19982;&#25991;&#29486;&#20013;&#30340;&#26368;&#20248;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Covariate shift occurs prevalently in practice, where the input distributions of the source and target data are substantially different. Despite its practical importance in various learning problems, most of the existing methods only focus on some specific learning tasks and are not well validated theoretically and numerically. To tackle this problem, we propose a unified analysis of general nonparametric methods in a reproducing kernel Hilbert space (RKHS) under covariate shift. Our theoretical results are established for a general loss belonging to a rich loss function family, which includes many commonly used methods as special cases, such as mean regression, quantile regression, likelihood-based classification, and margin-based classification. Two types of covariate shift problems are the focus of this paper and the sharp convergence rates are established for a general loss function to provide a unified theoretical analysis, which concurs with the optimal results in literature wher
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#24674;&#22797;&#25110;&#36229;&#36234;&#20102;&#24050;&#30693;&#30340;&#36335;&#24452;&#33539;&#25968;&#30028;&#38480;&#65292;&#24182;&#25361;&#25112;&#20102;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#19968;&#20123;&#20855;&#20307;&#25215;&#35834;&#12290;</title><link>http://arxiv.org/abs/2310.01225</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#20195;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65306;&#24433;&#21709;&#12289;&#21069;&#26223;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A path-norm toolkit for modern networks: consequences, promises and challenges. (arXiv:2310.01225v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#24674;&#22797;&#25110;&#36229;&#36234;&#20102;&#24050;&#30693;&#30340;&#36335;&#24452;&#33539;&#25968;&#30028;&#38480;&#65292;&#24182;&#25361;&#25112;&#20102;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#19968;&#20123;&#20855;&#20307;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#33021;&#22815;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#36866;&#29992;&#20110;&#26368;&#24191;&#27867;&#30340;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#21487;&#20197;&#24674;&#22797;&#25110;&#36229;&#36234;&#24050;&#30693;&#30340;&#27492;&#31867;&#33539;&#25968;&#30340;&#26368;&#23574;&#38160;&#30028;&#38480;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#36335;&#24452;&#33539;&#25968;&#36824;&#20139;&#26377;&#36335;&#24452;&#33539;&#25968;&#30340;&#24120;&#35268;&#20248;&#28857;&#65306;&#35745;&#31639;&#31616;&#20415;&#12289;&#23545;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#22312;&#21069;&#39304;&#32593;&#32476;&#19978;&#27604;&#25805;&#20316;&#31526;&#33539;&#25968;&#30340;&#20056;&#31215;&#65288;&#21478;&#19968;&#31181;&#24120;&#29992;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#38160;&#24230;&#12290;&#24037;&#20855;&#21253;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#26131;&#20110;&#23454;&#26045;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#25968;&#20540;&#35780;&#20272;&#22312;ImageNet&#19978;&#23545;ResNet&#30340;&#26368;&#23574;&#38160;&#30028;&#38480;&#26469;&#25361;&#25112;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#20855;&#20307;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the first toolkit around path-norms that is fully able to encompass general DAG ReLU networks with biases, skip connections and max pooling. This toolkit notably allows us to establish generalization bounds for real modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms further enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on feedforward networks compared to the product of operators' norms, another complexity measure most commonly used.  The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26367;&#20195;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#20248;&#21270;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10194</link><description>&lt;p&gt;
&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
The Kernel Density Integral Transformation. (arXiv:2309.10194v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26367;&#20195;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#20248;&#21270;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#26041;&#27861;&#20110;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#29305;&#24449;&#39044;&#22788;&#29702;&#32487;&#32493;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32508;&#21512;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#29305;&#24449;&#39044;&#22788;&#29702;&#26041;&#27861;&#20316;&#20026;&#26497;&#38480;&#24773;&#20917;&#65306;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#19981;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#21487;&#20197;&#20316;&#20026;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#23545;&#27599;&#31181;&#26041;&#27861;&#30340;&#24369;&#28857;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#21478;&#22806;&#65292;&#36890;&#36807;&#35843;&#25972;&#19968;&#20010;&#36830;&#32493;&#36229;&#21442;&#25968;&#65292;&#25105;&#20204;&#32463;&#24120;&#20248;&#20110;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#26680;&#23494;&#24230;&#36716;&#25442;&#21487;&#20197;&#26377;&#30410;&#22320;&#24212;&#29992;&#20110;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#20851;&#24615;&#20998;&#26512;&#21644;&#21333;&#21464;&#37327;&#32858;&#31867;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature preprocessing continues to play a critical role when applying machine learning and statistical methods to tabular data. In this paper, we propose the use of the kernel density integral transformation as a feature preprocessing step. Our approach subsumes the two leading feature preprocessing methods as limiting cases: linear min-max scaling and quantile transformation. We demonstrate that, without hyperparameter tuning, the kernel density integral transformation can be used as a simple drop-in replacement for either method, offering robustness to the weaknesses of each. Alternatively, with tuning of a single continuous hyperparameter, we frequently outperform both of these methods. Finally, we show that the kernel density transformation can be profitably applied to statistical data analysis, particularly in correlation analysis and univariate clustering.
&lt;/p&gt;</description></item><item><title>URL&#22522;&#20934;&#26159;&#19968;&#20010;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21457;&#29616;&#19987;&#27880;&#20110;&#34920;&#31034;&#26412;&#36523;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03810</link><description>&lt;p&gt;
URL&#65306;&#19968;&#31181;&#21487;&#36716;&#31227;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates. (arXiv:2307.03810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03810
&lt;/p&gt;
&lt;p&gt;
URL&#22522;&#20934;&#26159;&#19968;&#20010;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21457;&#29616;&#19987;&#27880;&#20110;&#34920;&#31034;&#26412;&#36523;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#23398;&#20064;&#26174;&#33879;&#25512;&#21160;&#20102;&#35813;&#39046;&#22495;&#21457;&#23637;&#20986;&#33021;&#22815;&#20316;&#20026;&#20174;&#38646;&#24320;&#22987;&#36801;&#31227;&#21040;&#26032;&#25968;&#25454;&#38598;&#26102;&#30340;&#26377;&#20215;&#20540;&#36215;&#28857;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#38543;&#30528;&#23545;&#21487;&#38752;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19981;&#20165;&#33021;&#25552;&#20379;&#23884;&#20837;&#21521;&#37327;&#65292;&#36824;&#33021;&#25552;&#20379;&#21487;&#36716;&#31227;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#20026;&#20102;&#24341;&#23548;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;URL&#65288;Uncertainty-aware Representation Learning&#65289;&#22522;&#20934;&#12290;&#38500;&#20102;&#34920;&#31034;&#30340;&#21487;&#36716;&#31227;&#24615;&#20043;&#22806;&#65292;&#23427;&#36824;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#38646;&#26679;&#26412;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#24212;&#29992;URL&#26469;&#35780;&#20272;11&#31181;&#22312;ImageNet&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#36716;&#31227;&#21040;8&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30528;&#37325;&#20110;&#34920;&#31034;&#26412;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#19978;&#28216;&#31867;&#21035;&#30340;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#21487;&#36716;&#31227;&#30340;&#19981;&#30830;&#23450;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning has significantly driven the field to develop pretrained models that can act as a valuable starting point when transferring to new datasets. With the rising demand for reliable machine learning and uncertainty quantification, there is a need for pretrained models that not only provide embeddings but also transferable uncertainty estimates. To guide the development of such models, we propose the Uncertainty-aware Representation Learning (URL) benchmark. Besides the transferability of the representations, it also measures the zero-shot transferability of the uncertainty estimate using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers that are pretrained on ImageNet and transferred to eight downstream datasets. We find that approaches that focus on the uncertainty of the representation itself or estimate the prediction risk directly outperform those that are based on the probabilities of upstream classes. Yet, achieving transferable uncertaint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.09983</link><description>&lt;p&gt;
&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#25110;&#20915;&#31574;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#20154;&#33021;&#21147;&#65292;&#37027;&#20040;&#25105;&#20204;&#35813;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#20195;&#29702;&#20250;&#20135;&#29983;&#20559;&#24046;? &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21069;&#25552;&#26159;&#65292;&#34429;&#28982;&#35780;&#20272;&#36229;&#20154;&#20915;&#31574;&#30340;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#26159;&#22914;&#26524;&#27169;&#22411;&#30340;&#20915;&#31574;&#26410;&#33021;&#28385;&#36275;&#26576;&#20123;&#36923;&#36753;&#19978;&#12289;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21457;&#29616;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#30001;&#20110;&#36229;&#20154;&#27169;&#22411;&#33021;&#21147;&#25110;&#20854;&#20182;&#32570;&#20047;&#22522;&#26412;&#20107;&#23454;&#32780;&#38590;&#20197;&#35780;&#20272;&#65306;&#35780;&#20272;&#22269;&#38469;&#35937;&#26827;&#23616;&#38754;&#12289;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#20316;&#20986;&#27861;&#24459;&#21028;&#26029;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26080;&#35770;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;(&#21487;&#33021;&#26159;&#36229;&#20154;&#30340;)&#65292;&#25105;&#20204;&#37117;&#33021;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65306;&#22269;&#38469;&#35937;&#26827;&#24341;&#25806;&#32473;&#20986;&#23545;&#23616;&#20013;&#26827;&#23376;&#30456;&#23545;&#20272;&#20540;&#30340;&#19981;&#21516;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.06599</link><description>&lt;p&gt;
&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;(Variational Imbalanced Regression)
&lt;/p&gt;
&lt;p&gt;
Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#26102;&#65292;&#29616;&#26377;&#30340;&#22238;&#24402;&#27169;&#22411;&#24448;&#24448;&#22312;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#65292;&#23427;&#19981;&#20165;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#33258;&#28982;&#22320;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#19982;&#20856;&#22411;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20551;&#35774;I.I.D.&#34920;&#31034;&#65288;&#25968;&#25454;&#28857;&#30340;&#34920;&#31034;&#19981;&#30452;&#25509;&#21463;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;VIR&#20511;&#29992;&#20855;&#26377;&#31867;&#20284;&#22238;&#24402;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#35745;&#31639;&#28508;&#22312;&#34920;&#31034;&#30340;&#21464;&#20998;&#20998;&#24067;&#65307;&#27492;&#22806;&#65292;&#19981;&#21516;&#20110;&#20135;&#29983;&#28857;&#20272;&#35745;&#30340;&#30830;&#23450;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292; VIR&#39044;&#27979;&#25972;&#20010;&#27491;&#24577;&#21453;-&#20285;&#29595;&#20998;&#24067;&#24182;&#35843;&#33410;&#30456;&#20851;&#32852;&#30340;&#20849;&#36717;&#20998;&#24067;&#65292;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#26045;&#21152;&#27010;&#29575;&#37325;&#26032;&#21152;&#26435;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#40065;&#26834;&#30340;&#33258;&#36866;&#24212; $\tau$-Lasso &#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20197;&#38477;&#20302;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#23427;&#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.09310</link><description>&lt;p&gt;
&#33258;&#36866;&#24212; $\tau$-Lasso&#65306;&#20854;&#20581;&#22766;&#24615;&#21644;&#26368;&#20248;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Adaptive $\tau$-Lasso: Its Robustness and Oracle Properties. (arXiv:2304.09310v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#40065;&#26834;&#30340;&#33258;&#36866;&#24212; $\tau$-Lasso &#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20197;&#38477;&#20302;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#23427;&#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#40065;&#26834; $\tau$-&#22238;&#24402;&#20272;&#35745;&#22120;&#65292;&#20197;&#24212;&#23545;&#21709;&#24212;&#21464;&#37327;&#21644;&#21327;&#21464;&#37327;&#30340;&#20005;&#37325;&#27745;&#26579;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#20272;&#35745;&#22120;&#20026;&#33258;&#36866;&#24212; $\tau$-Lasso&#65292;&#23427;&#23545;&#24322;&#24120;&#20540;&#21644;&#39640;&#26464;&#26438;&#28857;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#26469;&#20943;&#23569;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20026;&#27599;&#20010;&#22238;&#24402;&#31995;&#25968;&#20998;&#37197;&#19968;&#20010;&#26435;&#37325;&#12290;&#23545;&#20110;&#22266;&#23450;&#25968;&#37327;&#30340;&#39044;&#27979;&#21464;&#37327; $p$&#65292;&#25105;&#20204;&#26174;&#31034;&#20986;&#33258;&#36866;&#24212; $\tau$-Lasso &#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#26029;&#28857;&#21644;&#24433;&#21709;&#20989;&#25968;&#26469;&#34920;&#24449;&#20854;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#25311;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new regularized version of the robust $\tau$-regression estimator for analyzing high-dimensional data sets subject to gross contamination in the response variables and covariates. We call the resulting estimator adaptive $\tau$-Lasso that is robust to outliers and high-leverage points and simultaneously employs adaptive $\ell_1$-norm penalty term to reduce the bias associated with large true regression coefficients. More specifically, this adaptive $\ell_1$-norm penalty term assigns a weight to each regression coefficient. For a fixed number of predictors $p$, we show that the adaptive $\tau$-Lasso has the oracle property with respect to variable-selection consistency and asymptotic normality for the regression vector corresponding to the true support, assuming knowledge of the true regression vector support. We then characterize its robustness via the finite-sample breakdown point and the influence function. We carry-out extensive simulations to compare the per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#65292;&#36890;&#36807;&#23558;&#29702;&#35770;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#24182;&#32467;&#21512;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#37325;&#23376;&#25955;&#23556;&#30340;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2303.14090</link><description>&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#21033;&#29992;&#26263;&#29289;&#36136;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks in the recreation of hydrodynamic simulations from dark matter. (arXiv:2303.14090v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#65292;&#36890;&#36807;&#23558;&#29702;&#35770;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#24182;&#32467;&#21512;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#37325;&#23376;&#25955;&#23556;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#21512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#23558;&#32479;&#35745;&#27169;&#24335;&#19982;&#39046;&#22495;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#20854;&#22522;&#26412;&#29702;&#24565;&#26159;&#36890;&#36807;&#24050;&#30693;&#20851;&#31995;&#26469;&#20016;&#23500;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#20197;&#38480;&#21046;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#12290;&#27700;&#21160;&#21147;&#23398;&#27169;&#25311;&#26159;&#29616;&#20195;&#23431;&#23449;&#23398;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#32780;&#25152;&#38656;&#30340;&#35745;&#31639;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24555;&#36895;&#27169;&#25311;&#26263;&#29289;&#36136;&#38656;&#35201;&#26356;&#23569;&#30340;&#36164;&#28304;&#65292;&#36825;&#23548;&#33268;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25104;&#20026;&#30740;&#31350;&#30340;&#19968;&#20010;&#27963;&#36291;&#39046;&#22495;;&#22312;&#36825;&#37324;&#65292;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#21457;&#29616;&#30340;&#25955;&#23556;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#36827;&#27493;&#21644;&#29289;&#29702;&#32422;&#26463;&#65292;&#23558;&#20851;&#20110;&#37325;&#23376;&#36716;&#21270;&#25928;&#29575;&#30340;&#29702;&#35770;&#27880;&#20837;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#22522;&#20110;&#32467;&#26524;&#22270;&#20687;&#20013;&#21160;&#21147;&#23398;&#21151;&#29575;&#35889;&#20013;&#30340;&#35823;&#24046;&#65292;&#36825;&#20351;&#24471;&#21487;&#20197;&#37327;&#21270;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#25512;&#26029;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks have emerged as a coherent framework for building predictive models that combine statistical patterns with domain knowledge. The underlying notion is to enrich the optimization loss function with known relationships to constrain the space of possible solutions. Hydrodynamic simulations are a core constituent of modern cosmology, while the required computations are both expensive and time-consuming. At the same time, the comparatively fast simulation of dark matter requires fewer resources, which has led to the emergence of machine learning algorithms for baryon inpainting as an active area of research; here, recreating the scatter found in hydrodynamic simulations is an ongoing challenge. This paper presents the first application of physics-informed neural networks to baryon inpainting by combining advances in neural network architectures with physical constraints, injecting theory on baryon conversion efficiency into the model loss function. We also in
&lt;/p&gt;</description></item><item><title>EDGI&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#31561;&#21464;&#25193;&#25955;&#22788;&#29702;&#20869;&#22312;&#23545;&#31216;&#24615;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#37319;&#26679;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#20869;&#22312;&#23545;&#31216;&#24615;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.12410</link><description>&lt;p&gt;
EDGI: &#20869;&#22312;&#23545;&#31216;&#24615;&#35268;&#21010;&#30340;&#31561;&#21464;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
EDGI: Equivariant Diffusion for Planning with Embodied Agents. (arXiv:2303.12410v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12410
&lt;/p&gt;
&lt;p&gt;
EDGI&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#31561;&#21464;&#25193;&#25955;&#22788;&#29702;&#20869;&#22312;&#23545;&#31216;&#24615;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#37319;&#26679;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#20869;&#22312;&#23545;&#31216;&#24615;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#22312;&#23545;&#31216;&#24615;&#26159;&#26102;&#31354;&#21644;&#25490;&#21015;&#19978;&#30340;&#65292;&#22823;&#22810;&#25968;&#35745;&#21010;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#27809;&#26377;&#32771;&#34385;&#36825;&#31181;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#23548;&#33268;&#37319;&#26679;&#25928;&#29575;&#20302;&#21644;&#27867;&#21270;&#33021;&#21147;&#24369;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#22312;&#23545;&#31216;&#24615;&#35268;&#21010;&#30340;&#31561;&#21464;&#25193;&#25955;&#31639;&#27861;(EDGI), &#21487;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#25903;&#25345;&#22810;&#31181;&#34920;&#31034;&#24418;&#24335;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied agents operate in a structured world, often solving tasks with spatial, temporal, and permutation symmetries. Most algorithms for planning and model-based reinforcement learning (MBRL) do not take this rich geometric structure into account, leading to sample inefficiency and poor generalization. We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an algorithm for MBRL and planning that is equivariant with respect to the product of the spatial symmetry group $\mathrm{SE(3)}$, the discrete-time translation group $\mathbb{Z}$, and the object permutation group $\mathrm{S}_n$. EDGI follows the Diffuser framework (Janner et al. 2022) in treating both learning a world model and planning in it as a conditional generative modeling problem, training a diffusion model on an offline trajectory dataset. We introduce a new $\mathrm{SE(3)} \times \mathbb{Z} \times \mathrm{S}_n$-equivariant diffusion model that supports multiple representations. We integrate this model i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#33324;&#29702;&#35770;&#26469;&#38480;&#23450;POMDP&#19982;&#20854;&#30456;&#24212;&#30340;&#26377;&#38480;&#26679;&#26412;&#31890;&#23376;&#20449;&#24565;MDP(PB-MDP)&#36924;&#36817;&#20043;&#38388;&#30340;&#35823;&#24046;&#65292;&#24182;&#23558;&#20219;&#20309;&#37319;&#26679;MDP&#31639;&#27861;&#36866;&#24212;&#21040;POMDP&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35299;&#20915;&#20855;&#26377;&#22823;&#30340;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;POMDP&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05015</link><description>&lt;p&gt;
&#31890;&#23376;&#20449;&#24565;&#36817;&#20284;POMDP&#30340;&#26368;&#20248;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Optimality Guarantees for Particle Belief Approximation of POMDPs. (arXiv:2210.05015v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#33324;&#29702;&#35770;&#26469;&#38480;&#23450;POMDP&#19982;&#20854;&#30456;&#24212;&#30340;&#26377;&#38480;&#26679;&#26412;&#31890;&#23376;&#20449;&#24565;MDP(PB-MDP)&#36924;&#36817;&#20043;&#38388;&#30340;&#35823;&#24046;&#65292;&#24182;&#23558;&#20219;&#20309;&#37319;&#26679;MDP&#31639;&#27861;&#36866;&#24212;&#21040;POMDP&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35299;&#20915;&#20855;&#26377;&#22823;&#30340;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;POMDP&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDP)&#25552;&#20379;&#20102;&#29616;&#23454;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#28789;&#27963;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;POMDP&#30340;&#27714;&#35299;&#38750;&#24120;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#24403;&#29366;&#24577;&#21644;&#35266;&#27979;&#31354;&#38388;&#26159;&#36830;&#32493;&#25110;&#28151;&#21512;&#30340;&#26102;&#20505;&#65292;&#36825;&#22312;&#29289;&#29702;&#31995;&#32479;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#23613;&#31649;&#26368;&#36817;&#20351;&#29992;&#35266;&#27979;&#20284;&#28982;&#26435;&#37325;&#31574;&#21010;&#30340;&#22312;&#32447;&#37319;&#26679;POMDP&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#23454;&#29992;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20808;&#21069;&#24182;&#27809;&#26377;&#25552;&#20986;&#19968;&#33324;&#29702;&#35770;&#26469;&#21051;&#30011;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#31890;&#23376;&#28388;&#27874;&#25216;&#26415;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#38480;&#23450;&#20219;&#20309;POMDP&#19982;&#20854;&#30456;&#24212;&#30340;&#26377;&#38480;&#26679;&#26412;&#31890;&#23376;&#20449;&#24565;MDP(PB-MDP)&#36924;&#36817;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;&#36825;&#31181;PB-MDP&#21644;POMDP&#20043;&#38388;&#30340;&#22522;&#30784;&#26725;&#26753;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#35299;&#20915;&#30456;&#24212;&#30340;&#31890;&#23376;&#20449;&#24565;MDP&#23558;&#20219;&#20309;&#37319;&#26679;MDP&#31639;&#27861;&#36866;&#24212;&#21040;POMDP&#20013;&#65292;&#20174;&#32780;&#23558;MDP&#31639;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#25193;&#23637;&#21040;POMDP&#20013;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#22312;&#35299;&#20915;&#20855;&#26377;&#22823;&#30340;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;POMDP&#26102;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially observable Markov decision processes (POMDPs) provide a flexible representation for real-world decision and control problems. However, POMDPs are notoriously difficult to solve, especially when the state and observation spaces are continuous or hybrid, which is often the case for physical systems. While recent online sampling-based POMDP algorithms that plan with observation likelihood weighting have shown practical effectiveness, a general theory characterizing the approximation error of the particle filtering techniques that these algorithms use has not previously been proposed. Our main contribution is bounding the error between any POMDP and its corresponding finite sample particle belief MDP (PB-MDP) approximation. This fundamental bridge between PB-MDPs and POMDPs allows us to adapt any sampling-based MDP algorithm to a POMDP by solving the corresponding particle belief MDP, thereby extending the convergence guarantees of the MDP algorithm to the POMDP. Practically, thi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#29983;&#25104;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#26469;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#25968;&#25454;&#26657;&#20934;&#38382;&#39064;&#65292;&#24182;&#22312; CIFAR-10&#65292;CIFAR-100 &#21644; SVHN &#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.13001</link><description>&lt;p&gt;
&#28145;&#24230;&#21028;&#21035;&#21040;&#26680;&#29983;&#25104;&#32593;&#32476;&#30340;&#23450;&#26631;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Discriminative to Kernel Generative Networks for Calibrated Inference. (arXiv:2201.13001v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.13001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#29983;&#25104;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#26469;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#25968;&#25454;&#26657;&#20934;&#38382;&#39064;&#65292;&#24182;&#22312; CIFAR-10&#65292;CIFAR-100 &#21644; SVHN &#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21028;&#21035;&#19982;&#29983;&#25104;&#32593;&#32476;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#26234;&#33021;&#30340;&#30740;&#31350;&#20013;&#37117;&#26377;&#20854;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20108;&#32773;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#26680;&#29983;&#25104;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;&#28145;&#24230;&#27169;&#22411;&#35270;&#20026;&#24191;&#20041;&#30340;&#21010;&#20998;&#35268;&#21017;&#65292;&#24182;&#20351;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#30001;&#35757;&#32451;&#25968;&#25454;&#26500;&#25104;&#30340;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#65292;&#26469;&#33719;&#24471;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fight between discriminative versus generative goes deep, in both the study of artificial and natural intelligence. In our view, both camps have complementary values. So, we sought to synergistically combine them. Here, we propose a methodology to convert deep discriminative networks to kernel generative networks. We leveraged the fact that deep models, including both random forests and deep networks, learn internal representations which are unions of polytopes with affine activation functions to conceptualize them both as generalized partitioning rules. We replace the affine function in each polytope populated by the training data with Gaussian kernel that results in a generative model. Theoretically, we derive the conditions under which our generative models are a consistent estimator of the corresponding class conditional density. Moreover, our proposed models obtain well calibrated posteriors for in-distribution, and extrapolate beyond the training data to handle out-of-distrib
&lt;/p&gt;</description></item></channel></rss>