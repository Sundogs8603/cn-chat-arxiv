<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#26102;&#38388;&#20998;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#32479;&#35745;&#26694;&#26550;&#21644;&#26657;&#20934;&#20572;&#27490;&#35268;&#21017;&#23454;&#29616;&#20102;&#23545;&#23436;&#20840;&#20998;&#31867;&#19982;&#26089;&#26399;&#26102;&#38388;&#20998;&#31867;&#20043;&#38388;&#30340;&#20934;&#30830;&#24230;&#38388;&#38548;&#30340;&#26377;&#38480;&#26679;&#26412;&#12289;&#20998;&#24067;&#26080;&#20851;&#30340;&#25511;&#21046;&#12290;&#20854;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#32047;&#35745;&#20572;&#27490;&#26102;&#38388;&#30340;&#26465;&#20214;&#19979;&#25511;&#21046;&#20102;&#19968;&#31181;&#26356;&#24378;&#30340;&#38169;&#35823;&#27010;&#24565;&#30340;&#20934;&#30830;&#24230;&#38388;&#38548;&#12290;</title><link>https://arxiv.org/abs/2402.00857</link><description>&lt;p&gt;
&#26089;&#26399;&#26102;&#38388;&#20998;&#31867;&#20013;&#30340;&#32047;&#31215;&#20934;&#30830;&#24230;&#38388;&#38548;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Early Time Classification with Accumulated Accuracy Gap Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#26102;&#38388;&#20998;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#32479;&#35745;&#26694;&#26550;&#21644;&#26657;&#20934;&#20572;&#27490;&#35268;&#21017;&#23454;&#29616;&#20102;&#23545;&#23436;&#20840;&#20998;&#31867;&#19982;&#26089;&#26399;&#26102;&#38388;&#20998;&#31867;&#20043;&#38388;&#30340;&#20934;&#30830;&#24230;&#38388;&#38548;&#30340;&#26377;&#38480;&#26679;&#26412;&#12289;&#20998;&#24067;&#26080;&#20851;&#30340;&#25511;&#21046;&#12290;&#20854;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#32047;&#35745;&#20572;&#27490;&#26102;&#38388;&#30340;&#26465;&#20214;&#19979;&#25511;&#21046;&#20102;&#19968;&#31181;&#26356;&#24378;&#30340;&#38169;&#35823;&#27010;&#24565;&#30340;&#20934;&#30830;&#24230;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#26102;&#38388;&#20998;&#31867;&#31639;&#27861;&#26088;&#22312;&#22312;&#19981;&#22788;&#29702;&#23436;&#25972;&#36755;&#20837;&#27969;&#30340;&#24773;&#20917;&#19979;&#23545;&#29305;&#24449;&#27969;&#36827;&#34892;&#26631;&#35760;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#24212;&#29992;&#20998;&#31867;&#22120;&#21040;&#25972;&#20010;&#36755;&#20837;&#26102;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#39034;&#24207;&#20998;&#31867;&#22120;&#30340;&#32479;&#35745;&#26694;&#26550;&#65292;&#21046;&#23450;&#20102;&#19968;&#20010;&#26657;&#20934;&#20572;&#27490;&#35268;&#21017;&#12290;&#36825;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#35268;&#21017;&#22312;&#23436;&#20840;&#20998;&#31867;&#21644;&#26089;&#26399;&#26102;&#38388;&#20998;&#31867;&#20043;&#38388;&#30340;&#20934;&#30830;&#24230;&#38388;&#38548;&#19978;&#33719;&#24471;&#20102;&#26377;&#38480;&#26679;&#26412;&#30340;&#12289;&#20998;&#24067;&#26080;&#20851;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20511;&#37492;&#20102;&#23398;&#20064;-&#27979;&#35797;&#26657;&#20934;&#26694;&#26550;&#65292;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#23454;&#20363;&#19978;&#23545;&#36825;&#20010;&#38388;&#38548;&#36827;&#34892;&#20102;&#24179;&#22343;&#25511;&#21046;&#12290;&#30001;&#20110;&#36825;&#31181;&#31639;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#36807;&#39640;&#30340;&#26089;&#20572;&#26102;&#38388;&#20934;&#30830;&#24230;&#38388;&#38548;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#32047;&#35745;&#20572;&#27490;&#26102;&#38388;&#30340;&#26465;&#20214;&#19979;&#25511;&#21046;&#20102;&#19968;&#31181;&#26356;&#24378;&#30340;&#38169;&#35823;&#27010;&#24565;&#65292;&#20854;&#20013;&#20934;&#30830;&#24230;&#38388;&#38548;&#21463;&#21040;&#25511;&#21046;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;&#36866;&#29992;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early time classification algorithms aim to label a stream of features without processing the full input stream, while maintaining accuracy comparable to that achieved by applying the classifier to the entire input. In this paper, we introduce a statistical framework that can be applied to any sequential classifier, formulating a calibrated stopping rule. This data-driven rule attains finite-sample, distribution-free control of the accuracy gap between full and early-time classification. We start by presenting a novel method that builds on the Learn-then-Test calibration framework to control this gap marginally, on average over i.i.d. instances. As this algorithm tends to yield an excessively high accuracy gap for early halt times, our main contribution is the proposal of a framework that controls a stronger notion of error, where the accuracy gap is controlled conditionally on the accumulated halt times. Numerical experiments demonstrate the effectiveness, applicability, and usefulnes
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#31867;&#65292;&#29992;&#20110;&#24178;&#39044;&#33539;&#22260;&#20869;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#36716;&#21270;&#12290;&#31639;&#27861;&#20445;&#35777;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#23558;&#24471;&#20998;&#20989;&#25968;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00849</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65306;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#36716;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score-based Causal Representation Learning: Linear and General Transformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00849
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#31867;&#65292;&#29992;&#20110;&#24178;&#39044;&#33539;&#22260;&#20869;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#36716;&#21270;&#12290;&#31639;&#27861;&#20445;&#35777;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#23558;&#24471;&#20998;&#20989;&#25968;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#38024;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#21644;&#23558;&#28508;&#22312;&#21464;&#37327;&#26144;&#23556;&#21040;&#35266;&#27979;&#21464;&#37327;&#30340;&#26410;&#30693;&#36716;&#21270;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#24178;&#39044;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#12290;&#30740;&#31350;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#36716;&#21270;&#12290;&#36825;&#31687;&#35770;&#25991;&#21516;&#26102;&#35752;&#35770;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#21487;&#35782;&#21035;&#24615;&#26159;&#25351;&#30830;&#23450;&#31639;&#27861;&#19981;&#30456;&#20851;&#30340;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#24674;&#22797;&#30495;&#23454;&#30340;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#21644;&#28508;&#22312;&#22240;&#26524;&#22270;&#12290;&#23454;&#29616;&#24615;&#26159;&#25351;&#31639;&#27861;&#26041;&#38754;&#65292;&#35299;&#20915;&#35774;&#35745;&#31639;&#27861;&#26469;&#23454;&#29616;&#21487;&#35782;&#21035;&#20445;&#35777;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#24471;&#20998;&#20989;&#25968;&#65288;&#21363;&#23494;&#24230;&#20989;&#25968;&#23545;&#25968;&#30340;&#26799;&#24230;&#65289;&#19982;CRL&#20043;&#38388;&#24314;&#31435;&#26032;&#32852;&#31995;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#24471;&#20998;&#20026;&#22522;&#30784;&#30340;&#31639;&#27861;&#31867;&#65292;&#30830;&#20445;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#19987;&#27880;&#20110;&#32447;&#24615;&#36716;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#27599;&#20010;n&#20010;&#38543;&#26426;&#30828;&#24178;&#39044;&#19979;&#35813;&#36716;&#21270;&#30340;&#22240;&#26524;&#34920;&#31034;&#21487;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the \emph{identifiability} and \emph{achievability} aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between \emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \emph{score-based class of algorithms} that ensures both identifiability and achievability. First, the paper focuses on \emph{linear} transformations and shows that one stochastic hard intervention per n
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#12289;&#26080;&#26631;&#31614;&#12289;&#26410;&#31579;&#36873;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#20197;&#21450;&#37319;&#29992;&#33258;&#30417;&#30563;&#30340;&#24072;&#29983;&#35774;&#32622;&#65292;&#26469;&#25913;&#36827;Tracking-Any-Point&#65288;TAP&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;TAP-Vid&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;TAP&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.00847</link><description>&lt;p&gt;
BootsTAP: &#38024;&#23545;Tracking-Any-Point&#30340;&#33258;&#20030;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BootsTAP: Bootstrapped Training for Tracking-Any-Point
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00847
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#12289;&#26080;&#26631;&#31614;&#12289;&#26410;&#31579;&#36873;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#20197;&#21450;&#37319;&#29992;&#33258;&#30417;&#30563;&#30340;&#24072;&#29983;&#35774;&#32622;&#65292;&#26469;&#25913;&#36827;Tracking-Any-Point&#65288;TAP&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;TAP-Vid&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;TAP&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#27169;&#22411;&#23545;&#29289;&#29702;&#21644;&#36816;&#21160;&#26377;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#35753;&#23427;&#20204;&#33021;&#22815;&#24863;&#30693;&#23454;&#26223;&#20013;&#22266;&#20307;&#34920;&#38754;&#30340;&#31227;&#21160;&#21644;&#21464;&#24418;&#26159;&#24456;&#26377;&#29992;&#30340;&#12290;&#36825;&#21487;&#20197;&#24418;&#24335;&#21270;&#20026;Tracking-Any-Point (TAP)&#65292;&#35201;&#27714;&#31639;&#27861;&#33021;&#22815;&#36861;&#36394;&#35270;&#39057;&#20013;&#19982;&#22266;&#20307;&#34920;&#38754;&#23545;&#24212;&#30340;&#20219;&#24847;&#28857;&#65292;&#21487;&#33021;&#26159;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#23494;&#38598;&#30340;&#12290;&#30446;&#21069;&#65292;TAP&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#30446;&#21069;&#21482;&#33021;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#33719;&#24471;&#26377;&#38480;&#31181;&#31867;&#30340;&#23545;&#35937;&#21644;&#36816;&#21160;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#35268;&#27169;&#12289;&#26080;&#26631;&#31614;&#12289;&#26410;&#31579;&#36873;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#22312;&#20165;&#36827;&#34892;&#26368;&#23567;&#26550;&#26500;&#26356;&#25913;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;TAP&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#37319;&#29992;&#20102;&#33258;&#30417;&#30563;&#30340;&#24072;&#29983;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;TAP-Vid&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20102;&#36229;&#36807;&#20197;&#24448;&#25104;&#26524;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65306;&#20363;&#22914;&#65292;TAP-Vid-DAVIS&#30340;&#24615;&#33021;&#20174;61.3%&#25552;&#21319;&#21040;66.4%&#65292;TAP-Vid-Kinetics&#20174;57.2%&#25552;&#21319;&#21040;61.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes. This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time. Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion. In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup. We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%.
&lt;/p&gt;</description></item><item><title>&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#20248;&#21183;&#65292;&#24182;&#25351;&#20986;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#37325;&#28857;&#23558;&#25918;&#22312;&#22914;&#20309;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00809</link><description>&lt;p&gt;
&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#30340;&#31435;&#22330;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00809
&lt;/p&gt;
&lt;p&gt;
&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#20248;&#21183;&#65292;&#24182;&#25351;&#20986;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#37325;&#28857;&#23558;&#25918;&#22312;&#22914;&#20309;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#20154;&#20204;&#20027;&#35201;&#20851;&#27880;&#22312;&#28041;&#21450;&#22823;&#35268;&#27169;&#22270;&#20687;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#25581;&#31034;&#20102;&#35768;&#22810;&#34987;&#24573;&#35270;&#30340;&#24230;&#37327;&#26631;&#20934;&#12289;&#20219;&#21153;&#21644;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#19981;&#30830;&#23450;&#24615;&#12289;&#20027;&#21160;&#21644;&#25345;&#32493;&#23398;&#20064;&#20197;&#21450;&#31185;&#23398;&#25968;&#25454;&#65292;&#36825;&#20123;&#26041;&#38754;&#38656;&#35201;&#20851;&#27880;&#12290;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#26159;&#19968;&#26465;&#26377;&#21069;&#26223;&#30340;&#36947;&#36335;&#65292;&#21487;&#20197;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#25552;&#20379;&#20248;&#21183;&#12290;&#26412;&#25991;&#35748;&#20026;BDL&#21487;&#20197;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#23427;&#37325;&#26032;&#23457;&#35270;&#20102;BDL&#30340;&#20248;&#21183;&#12289;&#25215;&#35748;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#19968;&#20123;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#30340;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#35752;&#35770;&#38598;&#20013;&#22312;&#21487;&#33021;&#30340;&#26041;&#24335;&#19978;&#65292;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;BDL&#30456;&#32467;&#21512;&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#30340;&#28151;&#21512;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#20107;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#21644;&#25805;&#20316;&#26102;&#38388;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00776</link><description>&lt;p&gt;
&#28151;&#21512;&#37327;&#23376;&#35270;&#35273;&#36716;&#25442;&#22120;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hybrid Quantum Vision Transformers for Event Classification in High Energy Physics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#30340;&#28151;&#21512;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#20107;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#21644;&#25805;&#20316;&#26102;&#38388;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#20013;&#37117;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#38543;&#30528;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#22522;&#20110;&#37327;&#23376;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#21487;&#33021;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#21644;&#25805;&#20316;&#26102;&#38388;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#23578;&#19981;&#33021;&#25191;&#34892;&#39640;&#32500;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#26368;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#37327;&#23376;&#28151;&#21512;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#20998;&#31867;&#38382;&#39064;&#65288;&#21306;&#20998;&#30005;&#23376;&#21644;&#20809;&#23376;&#22312;&#30005;&#30913;&#37327;&#33021;&#22120;&#20013;&#65289;&#12290;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#32463;&#20856;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#26550;&#26500;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28151;&#21512;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models based on vision transformer architectures are considered state-of-the-art when it comes to image classification tasks. However, they require extensive computational resources both for training and deployment. The problem is exacerbated as the amount and complexity of the data increases. Quantum-based vision transformer models could potentially alleviate this issue by reducing the training and operating time while maintaining the same predictive power. Although current quantum computers are not yet able to perform high-dimensional tasks yet, they do offer one of the most efficient solutions for the future. In this work, we construct several variations of a quantum hybrid vision transformer for a classification problem in high energy physics (distinguishing photons and electrons in the electromagnetic calorimeter). We test them against classical vision transformer architectures. Our findings indicate that the hybrid models can achieve comparable performance to their classical anal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#30740;&#31350;&#20102;Transformer&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.00743</link><description>&lt;p&gt;
Transformer&#30340;&#22909;&#22788;&#65306;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#30740;&#31350;&#20102;Transformer&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#25512;&#29702;&#38454;&#27573;&#33021;&#22815;&#23398;&#20064;&#19978;&#19979;&#25991;&#20013;&#30340;&#27010;&#24565;&#12290;&#29616;&#26377;&#30340;&#25991;&#29486;&#65292;&#20363;&#22914;\citet{zhang2023trained,huang2023context}&#23545;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#20294;&#26159;&#20182;&#20204;&#20551;&#35774;&#27599;&#20010;&#26679;&#26412;&#30340;&#36755;&#20837;$x_i$&#21644;&#36755;&#20986;$y_i$&#37117;&#34987;&#23884;&#20837;&#21040;&#30456;&#21516;&#30340;&#20196;&#29260;&#20013;&#65288;&#21363;&#32467;&#26500;&#21270;&#25968;&#25454;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#23427;&#20204;&#21576;&#29616;&#20026;&#20004;&#20010;&#20196;&#29260;&#65288;&#21363;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;\cite{wibisono2023role}&#65289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#36827;&#34892;&#20102;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;Transformer&#26550;&#26500;&#30340;&#22909;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#30456;&#24212;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;Transformer&#21487;&#20197;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Transformer&#20013;&#36215;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#20316;&#29992;&#30340;&#30830;&#20999;&#32452;&#20214;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65288;1&#65289;&#24102;&#26377;&#20004;&#23618;softmax&#65288;&#33258;&#25105;&#65289;&#27880;&#24847;&#21147;&#21644;&#21069;&#30651;&#24615;&#27880;&#24847;&#21147;&#25513;&#30721;&#30340;Transformer&#21487;&#20197;&#20174;&#25552;&#31034;&#20013;&#23398;&#20064;&#65292;&#22914;&#26524;$y_i$&#22312;&#20196;&#29260;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., \citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data \cite{wibisono2023role}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Dropout&#25216;&#26415;&#25506;&#32034;&#25289;&#32918;&#33945;&#38598;&#20013;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#24230;&#37327;&#21644;&#20943;&#36731;&#39044;&#27979;&#22810;&#37325;&#24615;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#25512;&#23548;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20215;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#25216;&#26415;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.00728</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#39044;&#27979;&#22810;&#37325;&#24615;&#35780;&#20272;&#30340;&#22522;&#20110;Dropout&#30340;&#25289;&#32918;&#33945;&#38598;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Dropout&#25216;&#26415;&#25506;&#32034;&#25289;&#32918;&#33945;&#38598;&#20013;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#24230;&#37327;&#21644;&#20943;&#36731;&#39044;&#27979;&#22810;&#37325;&#24615;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#25512;&#23548;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20215;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#25216;&#26415;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22810;&#37325;&#24615;&#26159;&#25351;&#20998;&#31867;&#20219;&#21153;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#31454;&#20105;&#27169;&#22411;&#65292;&#23427;&#20204;&#23454;&#29616;&#20102;&#20960;&#20046;&#26368;&#20248;&#24615;&#33021;&#65292;&#20294;&#20026;&#21333;&#20010;&#26679;&#26412;&#29983;&#25104;&#20102;&#30456;&#20114;&#20914;&#31361;&#30340;&#36755;&#20986;&#12290;&#36825;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#24615;&#25490;&#38500;&#12289;&#38590;&#20197;&#35299;&#37322;&#30340;&#27495;&#35270;&#21644;&#19981;&#20844;&#24179;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#22312;&#21487;&#33021;&#24222;&#22823;&#30340;&#20551;&#35774;&#31354;&#38388;&#20013;&#25506;&#32034;&#25152;&#26377;&#36825;&#20123;&#20960;&#20046;&#26368;&#20248;&#30340;&#27169;&#22411;&#65292;&#21363;&#25289;&#32918;&#33945;&#38598;&#65292;&#24230;&#37327;&#21644;&#20943;&#36731;&#39044;&#27979;&#22810;&#37325;&#24615;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;Dropout&#25216;&#26415;&#25506;&#32034;&#25289;&#32918;&#33945;&#38598;&#20013;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#25512;&#23548;&#65292;&#23558;Dropout&#21442;&#25968;&#19982;&#25289;&#32918;&#33945;&#38598;&#30340;&#23646;&#24615;&#30456;&#36830;&#25509;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#23545;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20215;&#12290;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#24615;&#33021;&#19978;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive multiplicity refers to the phenomenon in which classification tasks may admit multiple competing models that achieve almost-equally-optimal performance, yet generate conflicting outputs for individual samples. This presents significant concerns, as it can potentially result in systemic exclusion, inexplicable discrimination, and unfairness in practical applications. Measuring and mitigating predictive multiplicity, however, is computationally challenging due to the need to explore all such almost-equally-optimal models, known as the Rashomon set, in potentially huge hypothesis spaces. To address this challenge, we propose a novel framework that utilizes dropout techniques for exploring models in the Rashomon set. We provide rigorous theoretical derivations to connect the dropout parameters to properties of the Rashomon set, and empirically evaluate our framework through extensive experimentation. Numerical results show that our technique consistently outperforms baselines in
&lt;/p&gt;</description></item><item><title>&#20809;&#35889;&#21464;&#25442;&#26680;&#22238;&#24402;&#26159;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#23398;&#20064;&#20805;&#20998;&#24179;&#28369;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#24863;&#30693;&#33539;&#24335;&#20013;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.00645</link><description>&lt;p&gt;
&#20809;&#35889;&#21464;&#25442;&#26680;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Spectrally Transformed Kernel Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00645
&lt;/p&gt;
&lt;p&gt;
&#20809;&#35889;&#21464;&#25442;&#26680;&#22238;&#24402;&#26159;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#23398;&#20064;&#20805;&#20998;&#24179;&#28369;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#24863;&#30693;&#33539;&#24335;&#20013;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26631;&#31614;&#25968;&#25454;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20316;&#29992;&#26159;&#36890;&#36807;&#22522;&#30784;&#26680;&#65288;&#22914;&#949;-&#37051;&#23621;&#26680;&#25110;&#22270;&#30340;&#37051;&#25509;&#30697;&#38453;&#65289;&#20013;&#32534;&#30721;&#30340;&#30456;&#20284;&#24615;&#20449;&#24687;&#26469;&#23454;&#29616;&#19968;&#31181;&#24179;&#28369;&#24615;&#24418;&#24335;&#12290;&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#20809;&#35889;&#21464;&#25442;&#26680;&#22238;&#24402;&#65288;STKR&#65289;&#30340;&#32463;&#20856;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31867;&#26032;&#30340;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;STKR&#20272;&#35745;&#22120;&#65292;&#33021;&#22815;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#36890;&#36807;&#20809;&#35889;&#21464;&#25442;&#65292;STKR&#21033;&#29992;&#20102;&#26080;&#26631;&#31614;&#25968;&#25454;&#25552;&#20379;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20449;&#24687;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;STKR&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34920;&#24449;&#19968;&#31181;"&#30446;&#26631;&#24179;&#28369;&#24615;"&#30340;&#36890;&#29992;&#31867;&#22411;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#20805;&#20998;&#24179;&#28369;&#30340;&#20989;&#25968;&#37117;&#21487;&#20197;&#36890;&#36807;STKR&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;STKR&#23454;&#29616;&#65292;&#36866;&#29992;&#20110;&#24863;&#30693;&#33539;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33324;&#30340;&#21464;&#25442;&#20989;&#25968;&#65292;&#32780;&#20808;&#21069;&#30340;&#24037;&#20316;&#22823;&#37096;&#20998;&#38480;&#20110;&#25512;&#23548;&#33539;&#24335;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#32479;&#35745;&#23646;&#24615;...
&lt;/p&gt;
&lt;p&gt;
Unlabeled data is a key component of modern machine learning. In general, the role of unlabeled data is to impose a form of smoothness, usually from the similarity information encoded in a base kernel, such as the $\epsilon$-neighbor kernel or the adjacency matrix of a graph. This work revisits the classical idea of spectrally transformed kernel regression (STKR), and provides a new class of general and scalable STKR estimators able to leverage unlabeled data. Intuitively, via spectral transformation, STKR exploits the data distribution for which unlabeled data can provide additional information. First, we show that STKR is a principled and general approach, by characterizing a universal type of "target smoothness", and proving that any sufficiently smooth function can be learned by STKR. Second, we provide scalable STKR implementations for the inductive setting and a general transformation function, while prior work is mostly limited to the transductive setting. Third, we derive stati
&lt;/p&gt;</description></item><item><title>&#20197;&#39640;&#26031;&#36807;&#31243;&#32593;&#32476;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#27169;&#25311;&#24178;&#39044;&#25928;&#26524;&#21644;&#20256;&#25773;&#24178;&#39044;&#25928;&#26524;&#65292;&#36827;&#34892;&#28789;&#27963;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#65292;&#21516;&#26102;&#20197;&#23616;&#37096;&#21464;&#37327;&#20026;&#20989;&#25968;&#20272;&#35745;&#24178;&#39044;&#20998;&#24067;&#24182;&#20351;&#29992;&#21152;&#24615;&#39640;&#26031;&#36807;&#31243;&#23545;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.00623</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Bayesian Causal Inference with Gaussian Process Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00623
&lt;/p&gt;
&lt;p&gt;
&#20197;&#39640;&#26031;&#36807;&#31243;&#32593;&#32476;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#27169;&#25311;&#24178;&#39044;&#25928;&#26524;&#21644;&#20256;&#25773;&#24178;&#39044;&#25928;&#26524;&#65292;&#36827;&#34892;&#28789;&#27963;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#65292;&#21516;&#26102;&#20197;&#23616;&#37096;&#21464;&#37327;&#20026;&#20989;&#25968;&#20272;&#35745;&#24178;&#39044;&#20998;&#24067;&#24182;&#20351;&#29992;&#21152;&#24615;&#39640;&#26031;&#36807;&#31243;&#23545;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#21644;&#25512;&#26029;&#26159;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#23427;&#26082;&#28041;&#21450;&#24314;&#27169;&#38382;&#39064;&#65292;&#20063;&#28041;&#21450;&#35745;&#31639;&#38382;&#39064;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20154;&#20204;&#20250;&#23545;&#32852;&#21512;&#20998;&#24067;&#20570;&#20986;&#20005;&#26684;&#30340;&#20551;&#35774;&#65292;&#22914;&#32447;&#24615;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#39640;&#26031;&#36807;&#31243;&#32593;&#32476;&#65288;GPN&#65289;&#27169;&#22411;&#20013;&#65292;&#23545;&#20551;&#35774;&#24178;&#39044;&#25928;&#26524;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#22240;&#26524;&#26694;&#26550;&#65292;&#21487;&#20197;&#38750;&#21442;&#25968;&#22320;&#25551;&#36848;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#25972;&#20010;&#32593;&#32476;&#19978;&#27169;&#25311;&#24178;&#39044;&#25928;&#26524;&#65292;&#24182;&#22312;&#19979;&#28216;&#21464;&#37327;&#19978;&#20256;&#25773;&#24178;&#39044;&#25928;&#26524;&#26469;&#36827;&#34892;GPN&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#23548;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#35745;&#31639;&#36817;&#20284;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23558;&#24178;&#39044;&#20998;&#24067;&#20272;&#35745;&#20026;&#23616;&#37096;&#21464;&#37327;&#30340;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#21152;&#24615;&#39640;&#26031;&#36807;&#31243;&#23545;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#26694;&#26550;&#25193;&#23637;&#21040;&#20102;&#19981;&#20165;&#20165;&#26159;&#24050;&#30693;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#22240;&#26524;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery and inference from observational data is an essential problem in statistics posing both modeling and computational challenges. These are typically addressed by imposing strict assumptions on the joint distribution such as linearity. We consider the problem of the Bayesian estimation of the effects of hypothetical interventions in the Gaussian Process Network (GPN) model, a flexible causal framework which allows describing the causal relationships nonparametrically. We detail how to perform causal inference on GPNs by simulating the effect of an intervention across the whole network and propagating the effect of the intervention on downstream variables. We further derive a simpler computational approximation by estimating the intervention distribution as a function of local variables only, modeling the conditional distributions via additive Gaussian processes. We extend both frameworks beyond the case of a known causal graph, incorporating uncertainty about the causal s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;Dempster-Shafer&#29702;&#35770;&#23454;&#29616;&#23545;&#27169;&#31946;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00592</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Partial-Label Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;Dempster-Shafer&#29702;&#35770;&#23454;&#29616;&#23545;&#27169;&#31946;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36935;&#21040;&#26631;&#35760;&#27169;&#31946;&#30340;&#25968;&#25454;&#65292;&#21363;&#19981;&#21516;&#30340;&#26631;&#27880;&#32773;&#20026;&#30456;&#21516;&#26679;&#26412;&#20998;&#37197;&#20102;&#20914;&#31361;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20801;&#35768;&#22312;&#36825;&#31181;&#24369;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24050;&#32463;&#20855;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#21463;&#21040;&#38169;&#35823;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#20855;&#26377;&#33391;&#22909;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;Dempster-Shafer&#29702;&#35770;&#12290;&#23545;&#20154;&#24037;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#39118;&#38505;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. Partial-label learning allows training classifiers in this weakly supervised setting. While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates. However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving. In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory. Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance. Additionally, we prove that our algorithm is risk-consistent.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;Transformer&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#26426;&#21046;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#38190;&#21442;&#25968;&#23545;Transformer&#30340;&#20316;&#29992;&#65292;&#24182;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.00522</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#22312;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;Transformer&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#26426;&#21046;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#38190;&#21442;&#25968;&#23545;Transformer&#30340;&#20316;&#29992;&#65292;&#24182;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;Transformer&#22312;&#38271;&#12289;&#31232;&#30095;&#21644;&#22797;&#26434;&#35760;&#24518;&#30340;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;Transformer&#30340;&#19981;&#21516;&#32452;&#20214;&#65288;&#22914;&#28857;&#31215;&#33258;&#27880;&#24847;&#21147;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#21069;&#39304;&#23618;&#65289;&#26159;&#22914;&#20309;&#24433;&#21709;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#26126;&#30830;&#30340;&#36817;&#20284;&#29575;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#32508;&#21512;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#20013;&#20851;&#38190;&#21442;&#25968;&#65288;&#22914;&#23618;&#25968;&#21644;&#27880;&#24847;&#21147;&#22836;&#25968;&#65289;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#36825;&#20123;&#27934;&#23519;&#36824;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads, and these insights also provide natural suggestions for alternative architectures.
&lt;/p&gt;</description></item><item><title>&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#19982;f-&#20998;&#24067;&#26063;&#30340;&#27491;&#21017;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;f-&#20998;&#24067;&#27491;&#21017;&#21270;&#31561;&#25928;&#22320;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.00501</link><description>&lt;p&gt;
&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#19982;f-&#20998;&#24067;&#26063;&#27491;&#21017;&#21270;&#30340;&#31561;&#20215;&#24615;
&lt;/p&gt;
&lt;p&gt;
Equivalence of the Empirical Risk Minimization to Regularization on the Family of f-Divergences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00501
&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#19982;f-&#20998;&#24067;&#26063;&#30340;&#27491;&#21017;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;f-&#20998;&#24067;&#27491;&#21017;&#21270;&#31561;&#25928;&#22320;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;f&#20013;&#30340;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#32473;&#20986;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#19982;f-&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#65288;ERM-$f$DR&#65289;&#30340;&#35299;&#27861;&#12290;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#26368;&#20248;&#27979;&#24230;&#34987;&#35777;&#26126;&#26159;&#21807;&#19968;&#30340;&#12290;&#24182;&#32473;&#20986;&#20102;&#29305;&#23450;&#36873;&#25321;&#20989;&#25968;f&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#21033;&#29992;f-&#20998;&#24067;&#26063;&#30340;&#28789;&#27963;&#24615;&#65292;&#33719;&#24471;&#20102;&#20808;&#21069;&#23545;&#24120;&#35265;&#27491;&#21017;&#21270;&#36873;&#25321;&#30340;&#24050;&#30693;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#21807;&#19968;&#35299;&#65288;Type-I&#21644;Type-II&#65289;&#12290;&#23545;&#35299;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;ERM-$f$DR&#38382;&#39064;&#20013;&#20351;&#29992;f-&#20998;&#24067;&#26102;&#30340;&#20197;&#19979;&#23646;&#24615;&#65306;$i)$ f-&#20998;&#24067;&#27491;&#21017;&#21270;&#24378;&#21046;&#23558;&#35299;&#30340;&#25903;&#25345;&#19982;&#21442;&#32771;&#27979;&#24230;&#30340;&#25903;&#25345;&#37325;&#21512;&#65292;&#24341;&#20837;&#20102;&#22312;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#30340;&#35777;&#25454;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#65307;$ii)$ &#20219;&#20309;f-&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#37117;&#31561;&#20215;&#20110;&#21478;&#19968;&#31181;f-&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The solution to empirical risk minimization with $f$-divergence regularization (ERM-$f$DR) is presented under mild conditions on $f$. Under such conditions, the optimal measure is shown to be unique. Examples of the solution for particular choices of the function $f$ are presented. Previously known solutions to common regularization choices are obtained by leveraging the flexibility of the family of $f$-divergences. These include the unique solutions to empirical risk minimization with relative entropy regularization (Type-I and Type-II). The analysis of the solution unveils the following properties of $f$-divergences when used in the ERM-$f$DR problem: $i\bigl)$ $f$-divergence regularization forces the support of the solution to coincide with the support of the reference measure, which introduces a strong inductive bias that dominates the evidence provided by the training data; and $ii\bigl)$ any $f$-divergence regularization is equivalent to a different $f$-divergence regularization 
&lt;/p&gt;</description></item><item><title>&#39640;&#25928;&#25506;&#32034;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21487;&#20197;&#20197;&#36739;&#23569;&#30340;&#26597;&#35810;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.00396</link><description>&lt;p&gt;
LLMs&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Exploration for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00396
&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#25506;&#32034;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21487;&#20197;&#20197;&#36739;&#23569;&#30340;&#26597;&#35810;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#39640;&#25928;&#25506;&#32034;&#22312;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#19968;&#20010;&#20195;&#29702;&#31243;&#24207;&#22312;&#25910;&#21040;&#21453;&#39304;&#26102;&#23558;&#22870;&#21169;&#27169;&#22411;&#25311;&#21512;&#21040;&#26597;&#35810;&#19978;&#12290;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#20195;&#29702;&#31243;&#24207;&#20351;&#29992;&#21452;Thompson&#37319;&#26679;&#29983;&#25104;&#26597;&#35810;&#65292;&#19981;&#30830;&#23450;&#24615;&#30001;&#35748;&#30693;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#25928;&#25506;&#32034;&#20351;&#24471;&#24615;&#33021;&#27700;&#24179;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#26597;&#35810;&#19979;&#36798;&#21040;&#36739;&#39640;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;CuFun&#27169;&#22411;&#65292;&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#36890;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#24378;&#24230;&#20989;&#25968;&#24314;&#27169;&#12289;&#31215;&#20998;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#38271;&#26399;&#26102;&#24207;&#20381;&#36182;&#24615;&#25429;&#25417;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00388</link><description>&lt;p&gt;
&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#36890;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Cumulative Distribution Function based General Temporal Point Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;CuFun&#27169;&#22411;&#65292;&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#36890;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#24378;&#24230;&#20989;&#25968;&#24314;&#27169;&#12289;&#31215;&#20998;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#38271;&#26399;&#26102;&#24207;&#20381;&#36182;&#24615;&#25429;&#25417;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#28857;&#36807;&#31243;&#22312;&#24314;&#27169;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#20107;&#20214;&#24207;&#21015;&#65288;&#21253;&#25324;&#31038;&#20132;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#65289;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23545;&#25512;&#33616;&#31995;&#32479;&#21644;&#20449;&#24687;&#26816;&#32034;&#31574;&#30053;&#30340;&#36827;&#23637;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#12290;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#20132;&#20114;&#21644;&#20132;&#26131;&#31561;&#20107;&#20214;&#65292;&#26102;&#38388;&#28857;&#36807;&#31243;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#34892;&#20026;&#27169;&#24335;&#27934;&#23519;&#65292;&#26377;&#21161;&#20110;&#39044;&#27979;&#26410;&#26469;&#30340;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#24335;&#30340;&#22797;&#26434;&#24615;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#26102;&#38388;&#28857;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#26102;&#38388;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24314;&#27169;&#24378;&#24230;&#20989;&#25968;&#12289;&#22797;&#26434;&#31215;&#20998;&#35745;&#31639;&#21644;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#26102;&#24207;&#20381;&#36182;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CuFun&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Point Processes (TPPs) hold a pivotal role in modeling event sequences across diverse domains, including social networking and e-commerce, and have significantly contributed to the advancement of recommendation systems and information retrieval strategies. Through the analysis of events such as user interactions and transactions, TPPs offer valuable insights into behavioral patterns, facilitating the prediction of future trends. However, accurately forecasting future events remains a formidable challenge due to the intricate nature of these patterns. The integration of Neural Networks with TPPs has ushered in the development of advanced deep TPP models. While these models excel at processing complex and nonlinear temporal data, they encounter limitations in modeling intensity functions, grapple with computational complexities in integral computations, and struggle to capture long-range temporal dependencies effectively. In this study, we introduce the CuFun model, representing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#30697;&#38453;&#23545;Lasso&#20272;&#35745;&#22120;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24403;&#26368;&#23567;&#22855;&#24322;&#20540;&#24456;&#23567;&#26102;&#65292;Lasso&#20272;&#35745;&#22120;&#22312;&#26368;&#23567;&#21270;&#36895;&#29575;&#26041;&#38754;&#26159;&#27425;&#20248;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#26063;&#35774;&#35745;&#30697;&#38453;&#21644;&#31232;&#30095;&#21442;&#25968;&#65292;&#35777;&#26126;&#26080;&#35770;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#36873;&#25321;&#26159;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#36824;&#26159;&#38543;&#26426;&#30340;&#65292;Lasso&#20272;&#35745;&#22120;&#37117;&#20250;&#22312;&#20272;&#35745;&#36895;&#29575;&#19978;&#34920;&#29616;&#20986;&#22810;&#39033;&#24335;&#22240;&#32032;&#30340;&#27425;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00382</link><description>&lt;p&gt;
&#20851;&#20110;Lasso&#35774;&#35745;&#20381;&#36182;&#24615;&#30340;&#23376;&#20248;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the design-dependent suboptimality of the Lasso
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#30697;&#38453;&#23545;Lasso&#20272;&#35745;&#22120;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24403;&#26368;&#23567;&#22855;&#24322;&#20540;&#24456;&#23567;&#26102;&#65292;Lasso&#20272;&#35745;&#22120;&#22312;&#26368;&#23567;&#21270;&#36895;&#29575;&#26041;&#38754;&#26159;&#27425;&#20248;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#26063;&#35774;&#35745;&#30697;&#38453;&#21644;&#31232;&#30095;&#21442;&#25968;&#65292;&#35777;&#26126;&#26080;&#35770;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#36873;&#25321;&#26159;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#36824;&#26159;&#38543;&#26426;&#30340;&#65292;Lasso&#20272;&#35745;&#22120;&#37117;&#20250;&#22312;&#20272;&#35745;&#36895;&#29575;&#19978;&#34920;&#29616;&#20986;&#22810;&#39033;&#24335;&#22240;&#32032;&#30340;&#27425;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#30697;&#38453;&#23545;&#32447;&#24615;&#22238;&#24402;&#20013;&#31232;&#30095;&#21442;&#25968;&#20272;&#35745;&#33021;&#21147;&#65288;&#25110;&#26080;&#33021;&#21147;&#65289;&#30340;&#24433;&#21709;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23545;&#35774;&#35745;&#30697;&#38453;&#30340;&#26368;&#23567;&#22855;&#24322;&#20540;&#36828;&#31163;&#38646;&#26102;&#30340;&#26368;&#20248;&#20272;&#35745;&#36895;&#29575;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#38500;&#20102;&#36825;&#20010;&#20449;&#24687;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#26377;&#32479;&#35745;&#26368;&#20248;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#22522;&#20110;&#23545;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#36827;&#34892;&#36719;&#38376;&#38480;&#12290;&#26368;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;Lasso&#20272;&#35745;&#22120;&#34987;&#24191;&#27867;&#37319;&#29992;&#29992;&#20110;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#24403;&#26368;&#23567;&#22855;&#24322;&#20540;&#24456;&#23567;&#26102;&#65292;Lasso&#20272;&#35745;&#22120;&#22312;&#26368;&#23567;&#21270;&#36895;&#29575;&#26041;&#38754;&#26159;&#26126;&#26174;&#27425;&#20248;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#26063;&#35774;&#35745;&#30697;&#38453;&#21644;&#31232;&#30095;&#21442;&#25968;&#65292;&#21487;&#20197;&#20445;&#35777;&#26080;&#35770;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#36873;&#25321;&#26159;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#36824;&#26159;&#38543;&#26426;&#30340;&#65292;Lasso&#20272;&#35745;&#22120;&#37117;&#20250;&#22312;&#20272;&#35745;&#36895;&#29575;&#19978;&#34920;&#29616;&#20986;&#22810;&#39033;&#24335;&#22240;&#32032;&#30340;&#27425;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the effect of the design matrix on the ability (or inability) to estimate a sparse parameter in linear regression. More specifically, we characterize the optimal rate of estimation when the smallest singular value of the design matrix is bounded away from zero. In addition to this information-theoretic result, we provide and analyze a procedure which is simultaneously statistically optimal and computationally efficient, based on soft thresholding the ordinary least squares estimator. Most surprisingly, we show that the Lasso estimator -- despite its widespread adoption for sparse linear regression -- is provably minimax rate-suboptimal when the minimum singular value is small. We present a family of design matrices and sparse parameters for which we can guarantee that the Lasso with any choice of regularization parameter -- including those which are data-dependent and randomized -- would fail in the sense that its estimation rate is suboptimal by polynomial fact
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#35757;&#32451;&#31639;&#27861;&#23545;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35889;&#20559;&#24046;&#21644;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#33258;&#36866;&#24212;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#31639;&#27861;&#65288;ARFF&#65289;&#21487;&#20197;&#22312;&#35889;&#20559;&#24046;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#19982;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#65288;SGD&#65289;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00332</link><description>&lt;p&gt;
&#27604;&#36739;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35889;&#20559;&#24046;&#21644;&#40065;&#26834;&#24615;&#65306;SGD&#19982;&#33258;&#36866;&#24212;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#20043;&#38388;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparing Spectral Bias and Robustness For Two-Layer Neural Networks: SGD vs Adaptive Random Fourier Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#35757;&#32451;&#31639;&#27861;&#23545;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35889;&#20559;&#24046;&#21644;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#33258;&#36866;&#24212;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#31639;&#27861;&#65288;ARFF&#65289;&#21487;&#20197;&#22312;&#35889;&#20559;&#24046;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#19982;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#65288;SGD&#65289;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#20102;&#36873;&#25321;&#35757;&#32451;&#31639;&#27861;&#23545;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#20010;&#20851;&#38190;&#24046;&#24322;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#35889;&#20559;&#24046;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#65292;&#32780;&#35889;&#20559;&#24046;&#19982;&#35757;&#32451;&#31639;&#27861;&#30340;&#36873;&#25321;&#20043;&#38388;&#30340;&#20851;&#31995;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#65288;SGD&#65289;&#65292;&#33258;&#36866;&#24212;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#31639;&#27861;&#65288;ARFF&#65289;&#21487;&#20197;&#20135;&#29983;&#26356;&#25509;&#36817;&#38646;&#30340;&#35889;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;SGD&#21644;ARFF&#35757;&#32451;&#20102;&#20004;&#20010;&#23436;&#20840;&#30456;&#21516;&#32467;&#26500;&#30340;&#20998;&#31867;&#22120;&#65292;&#23558;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#21040;&#30456;&#21516;&#27700;&#24179;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#25239;&#22122;&#22768;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present experimental results highlighting two key differences resulting from the choice of training algorithm for two-layer neural networks. The spectral bias of neural networks is well known, while the spectral bias dependence on the choice of training algorithm is less studied. Our experiments demonstrate that an adaptive random Fourier features algorithm (ARFF) can yield a spectral bias closer to zero compared to the stochastic gradient descent optimizer (SGD). Additionally, we train two identically structured classifiers, employing SGD and ARFF, to the same accuracy levels and empirically assess their robustness against adversarial noise attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#23567;&#19990;&#30028;&#32593;&#32476;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#65292;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#38408;&#20540;&#26469;&#21051;&#30011;&#31181;&#26893;&#30340;&#31264;&#23494;&#29615;&#36335;&#30340;&#26816;&#27979;&#21644;&#24674;&#22797;&#38382;&#39064;&#65292;&#36825;&#19982;&#20043;&#21069;&#22522;&#20110;&#35745;&#31639;&#38408;&#20540;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#21516;&#65292;&#25581;&#31034;&#20102;&#32479;&#35745;&#19982;&#35745;&#31639;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.00305</link><description>&lt;p&gt;
&#20449;&#24687;&#35770;&#38408;&#20540;&#23545;&#20110;&#31181;&#26893;&#30340;&#31264;&#23494;&#29615;&#36335;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Thresholds for Planted Dense Cycles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#23567;&#19990;&#30028;&#32593;&#32476;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#65292;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#38408;&#20540;&#26469;&#21051;&#30011;&#31181;&#26893;&#30340;&#31264;&#23494;&#29615;&#36335;&#30340;&#26816;&#27979;&#21644;&#24674;&#22797;&#38382;&#39064;&#65292;&#36825;&#19982;&#20043;&#21069;&#22522;&#20110;&#35745;&#31639;&#38408;&#20540;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#21516;&#65292;&#25581;&#31034;&#20102;&#32479;&#35745;&#19982;&#35745;&#31639;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#29992;&#20110;&#31038;&#20250;&#21644;&#29983;&#29289;&#31185;&#23398;&#20013;&#26080;&#22788;&#19981;&#22312;&#30340;&#23567;&#19990;&#30028;&#32593;&#32476;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#39044;&#26399;&#24102;&#23485;&#20026;$n \tau$&#30340;&#23494;&#38598;&#29615;&#36335;&#65292;&#20195;&#34920;&#20102;&#39030;&#28857;&#20043;&#38388;&#38544;&#34255;&#30340;&#19968;&#32500;&#20960;&#20309;&#20851;&#31995;&#65292;&#31181;&#26893;&#22312;&#20102;&#19968;&#20010;&#21253;&#21547;$n$&#20010;&#39030;&#28857;&#30340;&#38543;&#26426;&#22270;&#20013;&#12290;&#38024;&#23545;&#31181;&#26893;&#30340;&#31264;&#23494;&#29615;&#36335;&#30340;&#26816;&#27979;&#21644;&#24674;&#22797;&#65292;&#25105;&#20204;&#20197;$n$&#12289;$\tau$&#21644;&#36793;&#32536;&#20449;&#22122;&#27604;$\lambda$&#20026;&#21442;&#25968;&#65292;&#21051;&#30011;&#20102;&#20449;&#24687;&#35770;&#38408;&#20540;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#20123;&#20449;&#24687;&#35770;&#38408;&#20540;&#19982;&#26368;&#36817;&#19968;&#20010;&#22522;&#20110;&#20302;&#27425;&#22810;&#39033;&#24335;&#31639;&#27861;&#30340;&#35745;&#31639;&#38408;&#20540;&#19981;&#21516;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#23384;&#22312;&#32479;&#35745;&#19982;&#35745;&#31639;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a random graph model for small-world networks which are ubiquitous in social and biological sciences. In this model, a dense cycle of expected bandwidth $n \tau$, representing the hidden one-dimensional geometry of vertices, is planted in an ambient random graph on $n$ vertices. For both detection and recovery of the planted dense cycle, we characterize the information-theoretic thresholds in terms of $n$, $\tau$, and an edge-wise signal-to-noise ratio $\lambda$. In particular, the information-theoretic thresholds differ from the computational thresholds established in a recent work for low-degree polynomial algorithms, thereby justifying the existence of statistical-to-computational gaps for this problem.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#19968;&#31867;&#20998;&#24067;&#34429;&#28982;&#21487;&#20197;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#20197;&#24635;&#21464;&#24046;&#36317;&#31163;&#36827;&#34892;&#23398;&#20064;&#65292;&#20294;&#21364;&#26080;&#27861;&#22312;&#65288;&#949;&#65292;&#948;&#65289;-&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.00267</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#21487;&#23398;&#20064;&#30340;&#20998;&#24067;&#31867;&#37117;&#33021;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#36827;&#34892;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Not All Learnable Distribution Classes are Privately Learnable
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#19968;&#31867;&#20998;&#24067;&#34429;&#28982;&#21487;&#20197;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#20197;&#24635;&#21464;&#24046;&#36317;&#31163;&#36827;&#34892;&#23398;&#20064;&#65292;&#20294;&#21364;&#26080;&#27861;&#22312;&#65288;&#949;&#65292;&#948;&#65289;-&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#19968;&#31867;&#20998;&#24067;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#21487;&#20197;&#20197;&#24635;&#21464;&#24046;&#36317;&#31163;&#36827;&#34892;&#23398;&#20064;&#65292;&#20294;&#22312;&#65288;&#949;&#65292;&#948;&#65289;-&#24046;&#20998;&#38544;&#31169;&#19979;&#26080;&#27861;&#23398;&#20064;&#12290;&#36825;&#25512;&#32763;&#20102;Ashtiani&#30340;&#19968;&#20010;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give an example of a class of distributions that is learnable in total variation distance with a finite number of samples, but not learnable under $(\varepsilon, \delta)$-differential privacy. This refutes a conjecture of Ashtiani.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#32570;&#22833;&#20027;&#35201;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26367;&#20195;&#32467;&#26524;&#26469;&#20272;&#35745;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32435;&#20837;&#26367;&#20195;&#32467;&#26524;&#24182;&#36991;&#20813;&#36873;&#25321;&#20559;&#35823;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30340;&#20272;&#35745;&#20540;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#22312;&#26041;&#24046;&#26041;&#38754;&#21487;&#33021;&#27604;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#33391;&#22909;&#23454;&#35777;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00168</link><description>&lt;p&gt;
&#20351;&#29992;&#26367;&#20195;&#32467;&#26524;&#36827;&#34892;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Continuous Treatment Effects with Surrogate Outcomes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#32570;&#22833;&#20027;&#35201;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26367;&#20195;&#32467;&#26524;&#26469;&#20272;&#35745;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32435;&#20837;&#26367;&#20195;&#32467;&#26524;&#24182;&#36991;&#20813;&#36873;&#25321;&#20559;&#35823;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30340;&#20272;&#35745;&#20540;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#22312;&#26041;&#24046;&#26041;&#38754;&#21487;&#33021;&#27604;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#33391;&#22909;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22240;&#26524;&#25512;&#26029;&#24212;&#29992;&#20013;&#65292;&#20027;&#35201;&#32467;&#26524;&#65288;&#26631;&#31614;&#65289;&#24120;&#24120;&#26159;&#37096;&#20998;&#32570;&#22833;&#30340;&#65292;&#29305;&#21035;&#26159;&#22914;&#26524;&#23427;&#20204;&#24456;&#26114;&#36149;&#25110;&#24456;&#38590;&#25910;&#38598;&#12290;&#22914;&#26524;&#32570;&#22833;&#20381;&#36182;&#20110;&#21327;&#21464;&#37327;&#65288;&#21363;&#32570;&#22833;&#19981;&#23436;&#20840;&#38543;&#26426;&#65289;&#65292;&#20165;&#22522;&#20110;&#23436;&#20840;&#35266;&#27979;&#26679;&#26412;&#30340;&#20998;&#26512;&#21487;&#33021;&#23384;&#22312;&#20559;&#35823;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32467;&#21512;&#19982;&#20027;&#35201;&#32467;&#26524;&#30456;&#20851;&#30340;&#23436;&#20840;&#35266;&#27979;&#30340;&#27835;&#30103;&#21518;&#21464;&#37327;&#65288;&#26367;&#20195;&#32467;&#26524;&#65289;&#21487;&#20197;&#25913;&#36827;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26367;&#20195;&#32467;&#26524;&#22312;&#20272;&#35745;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#65292;&#20197;&#39640;&#25928;&#22320;&#23558;&#26367;&#20195;&#32467;&#26524;&#32435;&#20837;&#20998;&#26512;&#20013;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#19978;&#36848;&#36873;&#25321;&#20559;&#35823;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25152;&#25552;&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26041;&#24046;&#30340;&#21487;&#33021;&#25913;&#36827;&#12290;&#24191;&#27867;&#30340;&#27169;&#25311;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#32463;&#39564;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world causal inference applications, the primary outcomes (labels) are often partially missing, especially if they are expensive or difficult to collect. If the missingness depends on covariates (i.e., missingness is not completely at random), analyses based on fully-observed samples alone may be biased. Incorporating surrogates, which are fully observed post-treatment variables related to the primary outcome, can improve estimation in this case. In this paper, we study the role of surrogates in estimating continuous treatment effects and propose a doubly robust method to efficiently incorporate surrogates in the analysis, which uses both labeled and unlabeled data and does not suffer from the above selection bias problem. Importantly, we establish asymptotic normality of the proposed estimator and show possible improvements on the variance compared with methods that solely use labeled data. Extensive simulations show our methods enjoy appealing empirical performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#20013;&#25506;&#32034;&#39033;&#30340;&#26032;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#20854;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#21644;&#22686;&#21152;&#26799;&#24230;&#20272;&#35745;&#30340;&#20004;&#31181;&#19981;&#21516;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#35814;&#32454;&#35752;&#35770;&#21644;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24320;&#36767;&#20102;&#26410;&#26469;&#23545;&#36825;&#20123;&#31574;&#30053;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.00162</link><description>&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#25506;&#32034;&#32972;&#21518;&#30340;&#31070;&#35805;
&lt;/p&gt;
&lt;p&gt;
Behind the Myth of Exploration in Policy Gradients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#20013;&#25506;&#32034;&#39033;&#30340;&#26032;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#20854;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#21644;&#22686;&#21152;&#26799;&#24230;&#20272;&#35745;&#30340;&#20004;&#31181;&#19981;&#21516;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#35814;&#32454;&#35752;&#35770;&#21644;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24320;&#36767;&#20102;&#26410;&#26469;&#23545;&#36825;&#20123;&#31574;&#30053;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#26159;&#35299;&#20915;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25511;&#21046;&#38382;&#39064;&#30340;&#26377;&#25928;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#20026;&#20102;&#35745;&#31639;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#22312;&#23454;&#36341;&#20013;&#24517;&#39035;&#22312;&#23398;&#20064;&#30446;&#26631;&#20013;&#21253;&#21547;&#25506;&#32034;&#39033;&#12290;&#23613;&#31649;&#36825;&#20123;&#39033;&#30340;&#26377;&#25928;&#24615;&#36890;&#24120;&#36890;&#36807;&#23545;&#25506;&#32034;&#29615;&#22659;&#30340;&#20869;&#22312;&#38656;&#27714;&#36827;&#34892;&#35777;&#26126;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#20004;&#31181;&#19981;&#21516;&#21547;&#20041;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#20351;&#24471;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#22312;&#20445;&#25345;&#20840;&#23616;&#26368;&#22823;&#20540;&#30340;&#21516;&#26102;&#28040;&#38500;&#20102;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#20462;&#25913;&#20102;&#26799;&#24230;&#20272;&#35745;&#65292;&#22686;&#21152;&#20102;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#26368;&#32456;&#25552;&#20379;&#26368;&#20248;&#31574;&#30053;&#30340;&#27010;&#29575;&#12290;&#22522;&#20110;&#36825;&#20123;&#25928;&#24212;&#65292;&#25105;&#20204;&#35752;&#35770;&#24182;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#31361;&#20986;&#20102;&#20854;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#36825;&#20123;&#31574;&#30053;&#30340;&#26410;&#26469;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy-gradient algorithms are effective reinforcement learning methods for solving control problems with continuous state and action spaces. To compute near-optimal policies, it is essential in practice to include exploration terms in the learning objective. Although the effectiveness of these terms is usually justified by an intrinsic need to explore environments, we propose a novel analysis and distinguish two different implications of these techniques. First, they make it possible to smooth the learning objective and to eliminate local optima while preserving the global maximum. Second, they modify the gradient estimates, increasing the probability that the stochastic parameter update eventually provides an optimal policy. In light of these effects, we discuss and illustrate empirically exploration strategies based on entropy bonuses, highlighting their limitations and opening avenues for future works in the design and analysis of such strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#26356;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#21442;&#25968;&#25968;&#37327;&#26356;&#22810;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#23485;&#30340;&#32593;&#32476;&#65292;&#32780;&#26679;&#26412;&#28857;&#25968;&#37327;&#21644;&#25439;&#22833;&#20989;&#25968;&#35268;&#21017;&#24615;&#26356;&#39640;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#28145;&#30340;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.00152</link><description>&lt;p&gt;
&#26356;&#28145;&#36824;&#26159;&#26356;&#23485;: &#20174;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#35282;&#24230;&#30475;
&lt;/p&gt;
&lt;p&gt;
Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#26356;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#21442;&#25968;&#25968;&#37327;&#26356;&#22810;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#23485;&#30340;&#32593;&#32476;&#65292;&#32780;&#26679;&#26412;&#28857;&#25968;&#37327;&#21644;&#25439;&#22833;&#20989;&#25968;&#35268;&#21017;&#24615;&#26356;&#39640;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#28145;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#26159;&#26426;&#22120;&#23398;&#20064;&#30028;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36861;&#27714;&#65292;&#21040;&#24213;&#26159;&#26356;&#28145;&#36824;&#26159;&#26356;&#23485;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;DeNNs&#65289;&#21644;&#20855;&#26377;&#26377;&#38480;&#38544;&#34255;&#23618;&#30340;&#26356;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;WeNNs&#65289;&#22312;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#20998;&#26512;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21487;&#20197;&#21463;&#21040;&#22810;&#31181;&#22240;&#32032;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#21253;&#25324;&#26679;&#26412;&#28857;&#30340;&#25968;&#37327;&#65292;&#31070;&#32463;&#32593;&#32476;&#20869;&#30340;&#21442;&#25968;&#20197;&#21450;&#25439;&#22833;&#20989;&#25968;&#30340;&#35268;&#21017;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26356;&#22810;&#30340;&#21442;&#25968;&#20542;&#21521;&#20110;&#36873;&#25321;WeNNs&#65292;&#32780;&#26356;&#22810;&#30340;&#26679;&#26412;&#28857;&#21644;&#26356;&#39640;&#30340;&#25439;&#22833;&#20989;&#25968;&#35268;&#21017;&#24615;&#20542;&#21521;&#20110;&#36873;&#25321;DeNNs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#29702;&#35770;&#24212;&#29992;&#20110;&#20351;&#29992;&#28145;&#24230;Ritz&#21644;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing the architecture of a neural network is a challenging pursuit for the machine learning community, and the dilemma of whether to go deeper or wider remains a persistent question. This paper explores a comparison between deeper neural networks (DeNNs) with a flexible number of layers and wider neural networks (WeNNs) with limited hidden layers, focusing on their optimal generalization error in Sobolev losses. Analytical investigations reveal that the architecture of a neural network can be significantly influenced by various factors, including the number of sample points, parameters within the neural networks, and the regularity of the loss function. Specifically, a higher number of parameters tends to favor WeNNs, while an increased number of sample points and greater regularity in the loss function lean towards the adoption of DeNNs. We ultimately apply this theory to address partial differential equations using deep Ritz and physics-informed neural network (PINN) methods,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20013;&#20301;&#25968;-SHAP&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#23376;&#27169;&#22411;&#22312;&#39044;&#27979;&#20010;&#20307;&#29983;&#23384;&#26102;&#38388;&#26041;&#38754;&#20135;&#29983;&#30340;&#35299;&#37322;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00072</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#65306;&#19968;&#31181;&#20013;&#20301;&#25968;-SHAP&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for survival analysis: a median-SHAP approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00072
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20013;&#20301;&#25968;-SHAP&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#23376;&#27169;&#22411;&#22312;&#39044;&#27979;&#20010;&#20307;&#29983;&#23384;&#26102;&#38388;&#26041;&#38754;&#20135;&#29983;&#30340;&#35299;&#37322;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#65292;&#23545;&#20110;&#21307;&#30103;&#24212;&#29992;&#26469;&#35828;&#65292;&#38656;&#35201;&#38024;&#23545;&#24615;&#30340;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;Shapley&#20540;&#22312;&#23616;&#37096;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Shapley&#20540;&#30340;&#35299;&#37322;&#24615;&#24378;&#28872;&#20381;&#36182;&#20110;&#25688;&#35201;&#32479;&#35745;&#37327;&#21644;&#20272;&#35745;&#37327;&#65292;&#36825;&#20123;&#32479;&#35745;&#37327;&#21644;&#20272;&#35745;&#37327;&#23450;&#20041;&#20102;&#25105;&#20204;&#25152;&#35748;&#20026;&#30340;&#8220;&#38170;&#28857;&#8221;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20351;&#29992;&#22343;&#20540;&#38170;&#28857;&#30340;&#24815;&#20363;&#21487;&#33021;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#20013;&#20301;&#25968;-SHAP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#39044;&#27979;&#20010;&#20307;&#29983;&#23384;&#26102;&#38388;&#30340;&#40657;&#30418;&#23376;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the adoption of machine learning into routine clinical practice comes the need for Explainable AI methods tailored to medical applications. Shapley values have sparked wide interest for locally explaining models. Here, we demonstrate their interpretation strongly depends on both the summary statistic and the estimator for it, which in turn define what we identify as an 'anchor point'. We show that the convention of using a mean anchor point may generate misleading interpretations for survival analysis and introduce median-SHAP, a method for explaining black-box models predicting individual survival times.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25551;&#36848;&#24615;&#20998;&#26512;&#20559;&#24207;&#38598;&#21512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#26080;&#20132;&#24182;&#27867;&#28145;&#24230; (ufg) &#27604;&#36739;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#31034;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#22522;&#20110;ufg&#26041;&#27861;&#30340;&#22810;&#26679;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#26377;&#24456;&#22823;&#21306;&#21035;&#12290;</title><link>https://arxiv.org/abs/2312.12839</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#20132;&#24182;&#30340;&#27867;&#28145;&#24230;&#27604;&#36739;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comparing Machine Learning Algorithms by Union-Free Generic Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25551;&#36848;&#24615;&#20998;&#26512;&#20559;&#24207;&#38598;&#21512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#26080;&#20132;&#24182;&#27867;&#28145;&#24230; (ufg) &#27604;&#36739;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#31034;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#22522;&#20110;ufg&#26041;&#27861;&#30340;&#22810;&#26679;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#26377;&#24456;&#22823;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#27010;&#24565;&#30340;&#25551;&#36848;&#24615;&#20998;&#26512;&#20559;&#24207;&#38598;&#21512;&#30340;&#26694;&#26550;&#12290;&#23613;&#31649;&#32447;&#24615;&#31354;&#38388;&#21644;&#24230;&#37327;&#31354;&#38388;&#30340;&#30740;&#31350;&#38750;&#24120;&#28145;&#20837;&#65292;&#20294;&#20851;&#20110;&#20559;&#24207;&#38598;&#21512;&#31561;&#38750;&#26631;&#20934;&#25968;&#25454;&#31867;&#22411;&#30340;&#28145;&#24230;&#20989;&#25968;&#30340;&#35752;&#35770;&#20960;&#20046;&#27809;&#26377;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#20559;&#24207;&#38598;&#21512;&#30340;&#33879;&#21517;&#31616;&#21333;&#28145;&#24230;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#26080;&#20132;&#24182;&#27867;&#28145;&#24230; (ufg)&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;ufg&#28145;&#24230;&#26469;&#27604;&#36739;&#22522;&#20110;&#22810;&#32500;&#24615;&#33021;&#25351;&#26631;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#31034;&#20363;&#65292;&#23545;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#22120;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#24076;&#26395;&#22320;&#23637;&#31034;&#20102;&#22522;&#20110;ufg&#26041;&#27861;&#30340;&#19981;&#21516;&#20998;&#26512;&#26041;&#27861;&#30340;&#24191;&#27867;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31034;&#20363;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#26377;&#24456;&#22823;&#21306;&#21035;&#65292;&#22240;&#27492;&#20026;&#20998;&#31867;&#22120;&#27604;&#36739;&#30340;&#28909;&#28872;&#35752;&#35770;&#22686;&#28155;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions. Despite intensive studies in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders. We introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth. Moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures. Concretely, we provide two examples of classifier comparisons on samples of standard benchmark data sets. Our results demonstrate promisingly the wide variety of different analysis approaches based on ufg methods. Furthermore, the examples outline that our approach differs substantially from existing benchmarking approaches, and thus adds a new perspective to the vivid debate on classifier comparison.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#24863;&#30693;&#30340;&#24402;&#19968;&#21270;Wasserstein&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#65288;CNFs&#65289;&#21644;&#21442;&#25968;&#23376;&#27169;&#22411;&#65292;&#20248;&#21270;&#20102;&#22240;&#26524;&#25512;&#26029;&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#20013;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.18826</link><description>&lt;p&gt;
&#20960;&#20309;&#24863;&#30693;&#30340;&#24402;&#19968;&#21270;Wasserstein&#27969;&#22312;&#26368;&#20248;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Geometry-Aware Normalizing Wasserstein Flows for Optimal Causal Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#24863;&#30693;&#30340;&#24402;&#19968;&#21270;Wasserstein&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#65288;CNFs&#65289;&#21644;&#21442;&#25968;&#23376;&#27169;&#22411;&#65292;&#20248;&#21270;&#20102;&#22240;&#26524;&#25512;&#26029;&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#20013;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#65288;CNFs&#65289;&#19982;&#21442;&#25968;&#23376;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#23427;&#20204;&#23545;&#20960;&#20309;&#25935;&#24863;&#24615;&#65292;&#24182;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#30446;&#26631;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;TMLE&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;CNFs&#25913;&#36827;TMLE&#65292;&#20248;&#21270;Cram\'er-Rao&#30028;&#38480;&#65292;&#24182;&#20174;&#39044;&#23450;&#20041;&#20998;&#24067;$p_0$&#36807;&#28193;&#21040;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#24067;$p_1$&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21019;&#26032;&#22320;&#23558;Wasserstein&#26799;&#24230;&#27969;&#23884;&#20837;&#21040;Fokker-Planck&#26041;&#31243;&#20013;&#65292;&#20174;&#32780;&#22312;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#20013;&#28155;&#21152;&#20102;&#20960;&#20309;&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;CNFs&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a groundbreaking approach to causal inference by integrating continuous normalizing flows (CNFs) with parametric submodels, enhancing their geometric sensitivity and improving upon traditional Targeted Maximum Likelihood Estimation (TMLE). Our method employs CNFs to refine TMLE, optimizing the Cram\'er-Rao bound and transitioning from a predefined distribution $p_0$ to a data-driven distribution $p_1$. We innovate further by embedding Wasserstein gradient flows within Fokker-Planck equations, thus imposing geometric structures that boost the robustness of CNFs, particularly in optimal transport theory.   Our approach addresses the disparity between sample and population distributions, a critical factor in parameter estimation bias. We leverage optimal transport and Wasserstein gradient flows to develop causal inference methodologies with minimal variance in finite-sample settings, outperforming traditional methods like TMLE and AIPW. This novel framework, centered o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PDE&#20449;&#24687;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;PIGP&#65289;&#30340;&#21442;&#25968;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;PDE&#35299;&#24314;&#27169;&#20026;&#39640;&#26031;&#36807;&#31243;&#65292;&#21033;&#29992;PDE&#32467;&#26500;&#24341;&#36215;&#30340;&#32422;&#26463;&#26465;&#20214;&#26469;&#25512;&#26029;&#26410;&#30693;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2212.11880</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#39640;&#26031;&#36807;&#31243;&#21442;&#25968;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Parameter Inference based on Gaussian Processes Informed by Nonlinear Partial Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.11880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PDE&#20449;&#24687;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;PIGP&#65289;&#30340;&#21442;&#25968;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;PDE&#35299;&#24314;&#27169;&#20026;&#39640;&#26031;&#36807;&#31243;&#65292;&#21033;&#29992;PDE&#32467;&#26500;&#24341;&#36215;&#30340;&#32422;&#26463;&#26465;&#20214;&#26469;&#25512;&#26029;&#26410;&#30693;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#24191;&#27867;&#29992;&#20110;&#25551;&#36848;&#29289;&#29702;&#21644;&#24037;&#31243;&#29616;&#35937;&#12290;PDE&#20013;&#28041;&#21450;&#30340;&#19968;&#20123;&#20851;&#38190;&#21442;&#25968;&#20195;&#34920;&#20855;&#26377;&#37325;&#35201;&#31185;&#23398;&#35299;&#37322;&#30340;&#29305;&#23450;&#29289;&#29702;&#24615;&#36136;&#65292;&#30452;&#25509;&#27979;&#37327;&#36825;&#20123;&#21442;&#25968;&#26159;&#22256;&#38590;&#29978;&#33267;&#19981;&#21487;&#33021;&#30340;&#12290;&#20174;&#19982;&#30456;&#20851;&#29289;&#29702;&#37327;&#30340;&#22024;&#26434;&#21644;&#31232;&#30095;&#23454;&#39564;&#25968;&#25454;&#20013;&#20272;&#35745;&#36825;&#20123;&#21442;&#25968;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#35768;&#22810;PDE&#21442;&#25968;&#25512;&#26029;&#26041;&#27861;&#28041;&#21450;&#36890;&#36807;&#26377;&#38480;&#20803;&#26041;&#27861;&#31561;&#31639;&#27861;&#23545;PDE&#30340;&#25968;&#20540;&#35299;&#36827;&#34892;&#22823;&#37327;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#32447;&#24615;PDE&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;PDE&#20013;&#26410;&#30693;&#21442;&#25968;&#25512;&#26029;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;PDE&#20449;&#24687;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;PIGP&#65289;&#22522;&#20110;&#21442;&#25968;&#25512;&#26029;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;PDE&#35299;&#24314;&#27169;&#20026;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#65288;&#32447;&#24615;&#65289;PDE&#32467;&#26500;&#24341;&#36215;&#30340;&#27969;&#24418;&#32422;&#26463;&#65292;&#20351;&#24471;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;GP&#28385;&#36275;PDE&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) are widely used for the description of physical and engineering phenomena. Some key parameters involved in PDEs, which represent certain physical properties with important scientific interpretations, are difficult or even impossible to measure directly. Estimating these parameters from noisy and sparse experimental data of related physical quantities is an important task. Many methods for PDE parameter inference involve a large number of evaluations for numerical solutions to PDE through algorithms such as the finite element method, which can be time-consuming, especially for nonlinear PDEs. In this paper, we propose a novel method for the inference of unknown parameters in PDEs, called the PDE-Informed Gaussian Process (PIGP) based parameter inference method. Through modeling the PDE solution as a Gaussian process (GP), we derive the manifold constraints induced by the (linear) PDE structure such that, under the constraints, the GP satisfies the P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21069;&#39304;&#28508;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#23454;&#29616;&#21160;&#24577;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2207.07624</link><description>&lt;p&gt;
&#21069;&#39304;&#28508;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Feed-Forward Latent Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.07624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21069;&#39304;&#28508;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#23454;&#29616;&#21160;&#24577;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#24230;&#23454;&#29992;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#20351;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#33021;&#22815;&#36866;&#24212;&#20854;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#35748;&#35782;&#21040;&#35774;&#22791;&#30340;&#25968;&#25454;&#24456;&#21487;&#33021;&#26469;&#33258;&#21253;&#21547;&#28151;&#21512;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#30456;&#20851;&#21644;&#39046;&#22495;&#19981;&#30456;&#20851;&#31034;&#20363;&#30340;&#22810;&#20010;&#28508;&#22312;&#39046;&#22495;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#30340;&#28508;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#36793;&#32536;&#35774;&#22791;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#30446;&#26631;&#26159;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20197;&#21069;&#39304;&#26041;&#24335;&#36827;&#34892;&#36866;&#24212;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#26080;&#38656;&#35775;&#38382;&#28304;&#25968;&#25454;&#12290;&#24314;&#27169;&#36825;&#20123;&#29616;&#23454;&#32422;&#26463;&#23558;&#25105;&#20204;&#24102;&#21040;&#21069;&#39304;&#28508;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#39062;&#21644;&#23454;&#29992;&#37325;&#35201;&#30340;&#38382;&#39064;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20803;&#23398;&#20064;&#19968;&#20010;&#32593;&#32476;&#65292;&#33021;&#22815;&#23558;&#28151;&#21512;&#30456;&#20851;&#30446;&#26631;&#25968;&#25454;&#38598;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#21160;&#24577;&#22320;&#36866;&#24212;&#30446;&#26631;&#31034;&#20363;&#30340;&#25512;&#29702;&#12290;&#25152;&#24471;&#21040;&#30340;&#26694;&#26550;&#30456;&#23545;&#20110;&#24378;ERM&#22522;&#32447;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a new highly-practical problem setting that enables resource-constrained edge devices to adapt a pre-trained model to their local data distributions. Recognizing that device's data are likely to come from multiple latent domains that include a mixture of unlabelled domain-relevant and domain-irrelevant examples, we focus on the comparatively under-studied problem of latent domain adaptation. Considering limitations of edge devices, we aim to only use a pre-trained model and adapt it in a feed-forward way, without using back-propagation and without access to the source data. Modelling these realistic constraints bring us to the novel and practically important problem setting of feed-forward latent domain adaptation. Our solution is to meta-learn a network capable of embedding the mixed-relevance target dataset and dynamically adapting inference for target examples using cross-attention. The resulting framework leads to consistent improvements over strong ERM baselines. We also 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#30340;&#21327;&#21516;&#20284;&#28982;&#27604;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#20272;&#35745;&#27599;&#20010;&#33410;&#28857;&#38388;&#30340;&#20284;&#28982;&#27604;&#65292;&#33410;&#28857;&#21487;&#20197;&#21327;&#20316;&#26469;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2205.14461</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#21327;&#21516;&#20284;&#28982;&#27604;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Collaborative likelihood-ratio estimation over graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.14461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#30340;&#21327;&#21516;&#20284;&#28982;&#27604;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#20272;&#35745;&#27599;&#20010;&#33410;&#28857;&#38388;&#30340;&#20284;&#28982;&#27604;&#65292;&#33410;&#28857;&#21487;&#20197;&#21327;&#20316;&#26469;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#25105;&#20204;&#26377;&#26469;&#33258;&#20004;&#20010;&#26410;&#30693;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968; (pdfs) p &#21644; q &#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#35266;&#27979;&#20540;&#65292;&#20284;&#28982;&#27604;&#20272;&#35745;&#65288;LRE&#65289;&#26159;&#19968;&#31181;&#20248;&#38597;&#30340;&#26041;&#27861;&#65292;&#21482;&#20381;&#38752;&#29616;&#26377;&#25968;&#25454;&#26469;&#27604;&#36739;&#36825;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#30446;&#21069;&#20026;&#27490;&#39318;&#20010;&#22522;&#20110;&#22270;&#30340;&#25193;&#23637;&#38382;&#39064;&#65292;&#20854;&#20551;&#35774;&#22266;&#23450;&#22270;&#30340;&#27599;&#20010;&#33410;&#28857; v &#37117;&#21487;&#20197;&#35775;&#38382;&#26469;&#33258;&#20004;&#20010;&#26410;&#30693;&#33410;&#28857;&#29305;&#23450;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968; p_v &#21644; q_v &#30340;&#35266;&#27979;&#20540;&#65292;&#24182;&#19988;&#30446;&#26631;&#26159;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22270;&#32467;&#26500;&#25552;&#20379;&#30340;&#20449;&#24687;&#26469;&#20272;&#35745;&#27599;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#20284;&#28982;&#27604;&#12290;&#33410;&#28857;&#32423;&#21035;&#30340;&#20272;&#35745;&#20219;&#21153;&#24212;&#35813;&#23637;&#29616;&#20986;&#22270;&#20256;&#36882;&#30340;&#30456;&#20284;&#24615;&#65292;&#36825;&#26263;&#31034;&#30528;&#33410;&#28857;&#21487;&#20197;&#21327;&#20316;&#26469;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#23427;&#20204;&#12290;&#25105;&#20204;&#20197;&#19968;&#20010;&#20855;&#20307;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861; GRULSIF &#26469;&#24320;&#21457;&#36825;&#20010;&#24819;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assuming we have iid observations from two unknown probability density functions (pdfs), $p$ and $q$, the likelihood-ratio estimation (LRE) is an elegant approach to compare the two pdfs only by relying on the available data. In this paper, we introduce the first -to the best of our knowledge-graph-based extension of this problem, which reads as follows: Suppose each node $v$ of a fixed graph has access to observations coming from two unknown node-specific pdfs, $p_v$ and $q_v$, and the goal is to estimate for each node the likelihood-ratio between both pdfs by also taking into account the information provided by the graph structure. The node-level estimation tasks are supposed to exhibit similarities conveyed by the graph, which suggests that the nodes could collaborate to solve them more efficiently. We develop this idea in a concrete non-parametric method that we call Graph-based Relative Unconstrained Least-squares Importance Fitting (GRULSIF). We derive convergence rates for our c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31934;&#30830;&#20998;&#26512;&#20102;&#23545;&#23545;&#25239;&#35757;&#32451;&#20013;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#23545;&#24494;&#23567;&#23545;&#25239;&#25200;&#21160;&#38750;&#24120;&#33030;&#24369;&#65292;&#26174;&#31034;&#20102;&#40065;&#26834;&#27867;&#21270;&#30340;&#24615;&#33021;&#26126;&#26174;&#24046;&#20110;&#26631;&#20934;&#27867;&#21270;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2201.05149</link><description>&lt;p&gt;
&#23545;&#23545;&#25239;&#35757;&#32451;&#20013;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#35781;&#21650;&#65306;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#30340;&#40065;&#26834;&#27867;&#21270;&#30340;&#31934;&#30830;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The curse of overparametrization in adversarial training: Precise analysis of robust generalization for random features regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.05149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31934;&#30830;&#20998;&#26512;&#20102;&#23545;&#23545;&#25239;&#35757;&#32451;&#20013;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#23545;&#24494;&#23567;&#23545;&#25239;&#25200;&#21160;&#38750;&#24120;&#33030;&#24369;&#65292;&#26174;&#31034;&#20102;&#40065;&#26834;&#27867;&#21270;&#30340;&#24615;&#33021;&#26126;&#26174;&#24046;&#20110;&#26631;&#20934;&#27867;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#28041;&#21450;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20854;&#21442;&#25968;&#25968;&#37327;&#36229;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#20248;&#28857;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#65288;&#36890;&#36807;&#21452;&#19979;&#38477;&#29616;&#35937;&#65289;&#21644;&#35745;&#31639;&#35282;&#24230;&#65288;&#36890;&#36807;&#20248;&#21270;&#26223;&#35266;&#30340;&#32467;&#26500;&#29305;&#24615;&#65289;&#24050;&#32463;&#24471;&#21040;&#24314;&#31435;&#12290;&#23613;&#31649;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20854;&#36755;&#20837;&#20013;&#30340;&#24494;&#23567;&#23545;&#25239;&#25200;&#21160;&#38750;&#24120;&#33030;&#24369;&#12290;&#21363;&#20351;&#22312;&#32463;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#22312;&#34987;&#25200;&#21160;&#30340;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#65288;&#40065;&#26834;&#27867;&#21270;&#65289;&#20063;&#26126;&#26174;&#27604;&#22312;&#33391;&#24615;&#36755;&#20837;&#19978;&#30340;&#26368;&#20339;&#24615;&#33021;&#65288;&#26631;&#20934;&#27867;&#21270;&#65289;&#35201;&#24046;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#36807;&#24230;&#21442;&#25968;&#21270;&#22914;&#20309;&#20174;&#26681;&#26412;&#19978;&#24433;&#21709;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful deep learning models often involve training neural network architectures that contain more parameters than the number of training samples. Such overparametrized models have been extensively studied in recent years, and the virtues of overparametrization have been established from both the statistical perspective, via the double-descent phenomenon, and the computational perspective via the structural properties of the optimization landscape.   Despite the remarkable success of deep learning architectures in the overparametrized regime, it is also well known that these models are highly vulnerable to small adversarial perturbations in their inputs. Even when adversarially trained, their performance on perturbed inputs (robust generalization) is considerably worse than their best attainable performance on benign inputs (standard generalization). It is thus imperative to understand how overparametrization fundamentally affects robustness.   In this paper, we will provide a preci
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#29699;&#38754;&#25968;&#25454;&#20998;&#26512;&#20013;&#24212;&#29992;&#30340;&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#26680;&#65292;&#25193;&#23637;&#20102;RBF&#26680;&#24182;&#24341;&#20837;&#20102;&#21322;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2112.00365</link><description>&lt;p&gt;
&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#26680;&#22312;&#29699;&#38754;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Probability-Generating Function Kernels for Spherical Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.00365
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#29699;&#38754;&#25968;&#25454;&#20998;&#26512;&#20013;&#24212;&#29992;&#30340;&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#26680;&#65292;&#25193;&#23637;&#20102;RBF&#26680;&#24182;&#24341;&#20837;&#20102;&#21322;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#65288;PGF&#65289;&#26680;&#65292;&#26500;&#25104;&#20102;&#19968;&#31867;&#22312;&#21333;&#20301;&#36229;&#29699;&#19978;&#25903;&#25345;&#30340;&#26680;&#65292;&#29992;&#20110;&#29699;&#38754;&#25968;&#25454;&#20998;&#26512;&#12290;PGF&#26680;&#22312;&#29699;&#38754;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#25512;&#24191;&#20102;RBF&#26680;&#12290;&#30740;&#31350;&#20102;PGF&#26680;&#30340;&#29305;&#24615;&#12290;&#24341;&#20837;&#20102;&#21322;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#29699;&#38754;&#25968;&#25454;&#20013;&#20351;&#29992;PGF&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probability-generating function (PGF) kernels are introduced, which constitute a class of kernels supported on the unit hypersphere, for the purposes of spherical data analysis. PGF kernels generalize RBF kernels in the context of spherical data. The properties of PGF kernels are studied. A semi-parametric learning algorithm is introduced to enable the use of PGF kernels with spherical data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#22312;&#32447;&#22270;&#25299;&#25169;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#23558;VAR&#27169;&#22411;&#25193;&#23637;&#20026;&#30697;&#38453;&#21464;&#37327;&#27169;&#22411;&#20197;&#36866;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36807;&#31243;&#65292;&#38024;&#23545;&#20302;&#32500;&#21644;&#39640;&#32500;&#24773;&#20917;&#24555;&#36895;&#26356;&#26032;&#31995;&#25968;&#30340;&#20272;&#35745;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;Lasso-type&#36827;&#34892;&#25299;&#25169;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2107.08020</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#22312;&#32447;&#22270;&#25299;&#25169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Graph Topology Learning from Matrix-valued Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2107.08020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#22312;&#32447;&#22270;&#25299;&#25169;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#23558;VAR&#27169;&#22411;&#25193;&#23637;&#20026;&#30697;&#38453;&#21464;&#37327;&#27169;&#22411;&#20197;&#36866;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36807;&#31243;&#65292;&#38024;&#23545;&#20302;&#32500;&#21644;&#39640;&#32500;&#24773;&#20917;&#24555;&#36895;&#26356;&#26032;&#31995;&#25968;&#30340;&#20272;&#35745;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;Lasso-type&#36827;&#34892;&#25299;&#25169;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#36825;&#20123;&#25968;&#25454;&#26159;&#22312;&#19968;&#20010;&#20256;&#24863;&#22120;&#32593;&#32476;&#19978;&#25910;&#38598;&#30340;&#65288;&#36890;&#24120;&#26159;&#19968;&#32452;&#31354;&#38388;&#20301;&#32622;&#65289;&#65292;&#35266;&#27979;&#21040;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#22240;&#27492;&#65292;&#27599;&#20010;&#20256;&#24863;&#22120;&#30001;&#19968;&#20010;&#21521;&#37327;&#26102;&#24207;&#21015;&#26469;&#25551;&#36848;&#12290;&#25105;&#20204;&#24076;&#26395;&#35782;&#21035;&#36825;&#20123;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#20381;&#36182;&#32467;&#26500;&#65292;&#24182;&#29992;&#22270;&#24418;&#26469;&#34920;&#31034;&#23427;&#12290;&#24403;&#27599;&#20010;&#20256;&#24863;&#22120;&#21482;&#26377;&#19968;&#20010;&#29305;&#24449;&#26102;&#65292;&#30690;&#37327;&#33258;&#22238;&#24402;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25512;&#26029;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#30340;&#32467;&#26500;&#12290;&#25152;&#24471;&#21040;&#30340;&#22270;&#34987;&#31216;&#20026;&#22240;&#26524;&#22270;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#23558;VAR&#27169;&#22411;&#25193;&#23637;&#20026;&#30697;&#38453;&#21464;&#37327;&#27169;&#22411;&#65292;&#20197;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#30340;&#30446;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36807;&#31243;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#20302;&#32500;&#21644;&#39640;&#32500;&#24773;&#20917;&#65292;&#22312;&#26032;&#26679;&#26412;&#21040;&#36798;&#26102;&#21487;&#20197;&#24555;&#36895;&#26356;&#26032;&#31995;&#25968;&#30340;&#20272;&#35745;&#12290;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;Lasso-type&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#25299;&#25169;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with the statistical analysis of matrix-valued time series. These are data collected over a network of sensors (typically a set of spatial locations) along time, where a vector of features is observed per time instant per sensor. Thus each sensor is characterized by a vectorial time series. We would like to identify the dependency structure among these sensors and represent it by a graph. When there is only one feature per sensor, the vector auto-regressive models have been widely adapted to infer the structure of Granger causality. The resulting graph is referred to as causal graph. Our first contribution is then extending VAR models to matrix-variate models to serve the purpose of graph learning. Secondly, we propose two online procedures respectively in low and high dimensions, which can update quickly the estimates of coefficients when new samples arrive. In particular in high dimensional regime, a novel Lasso-type is introduced and we develop its homotopy a
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20849;&#20139;&#31070;&#32463;&#20803;&#30340;RBF&#32593;&#32476;&#30340;&#38750;&#21442;&#25968;&#21270;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#27835;&#30103;&#35774;&#32622;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#24314;&#27169;&#27835;&#30103;&#32467;&#26524;&#30340;&#20849;&#21516;&#24615;&#65292;&#24182;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#23454;&#29616;&#20272;&#35745;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#25968;&#20540;&#24615;&#33021;&#65292;&#24212;&#29992;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21518;&#20063;&#24471;&#21040;&#20102;&#26377;&#36259;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.16571</link><description>&lt;p&gt;
&#20351;&#29992;&#20849;&#20139;&#31070;&#32463;&#20803;&#30340;RBF&#32593;&#32476;&#20272;&#35745;&#20010;&#20307;&#21270;&#22810;&#27835;&#30103;&#21453;&#24212;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Individualized Multi-Treatment Response Curves Estimation using RBF-net with Shared Neurons. (arXiv:2401.16571v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16571
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20849;&#20139;&#31070;&#32463;&#20803;&#30340;RBF&#32593;&#32476;&#30340;&#38750;&#21442;&#25968;&#21270;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#27835;&#30103;&#35774;&#32622;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#24314;&#27169;&#27835;&#30103;&#32467;&#26524;&#30340;&#20849;&#21516;&#24615;&#65292;&#24182;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#23454;&#29616;&#20272;&#35745;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#25968;&#20540;&#24615;&#33021;&#65292;&#24212;&#29992;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21518;&#20063;&#24471;&#21040;&#20102;&#26377;&#36259;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#26159;&#31934;&#30830;&#21307;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20852;&#36259;&#22312;&#20110;&#22522;&#20110;&#19968;&#20123;&#22806;&#37096;&#21327;&#21464;&#37327;&#65292;&#30830;&#23450;&#19981;&#21516;&#27835;&#30103;&#26041;&#24335;&#30340;&#24046;&#24322;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#21270;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#27835;&#30103;&#35774;&#32622;&#12290;&#25105;&#20204;&#23545;&#21709;&#24212;&#26354;&#32447;&#30340;&#38750;&#21442;&#25968;&#24314;&#27169;&#20381;&#36182;&#20110;&#24102;&#26377;&#20849;&#20139;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;RBF&#65289;&#32593;&#32476;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#21161;&#20110;&#24314;&#27169;&#27835;&#30103;&#32467;&#26524;&#30340;&#20849;&#21516;&#24615;&#12290;&#25105;&#20204;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#24320;&#21457;&#20102;&#20272;&#35745;&#21644;&#25512;&#26029;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#30340;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#36827;&#34892;&#23454;&#29616;&#65292;&#36866;&#24403;&#22320;&#22788;&#29702;&#20102;&#20998;&#26512;&#21508;&#20010;&#26041;&#38754;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#25968;&#20540;&#24615;&#33021;&#12290;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;MIMIC&#25968;&#25454;&#21518;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20851;&#20110;&#19981;&#21516;&#27835;&#30103;&#31574;&#30053;&#23545;ICU&#20303;&#38498;&#26102;&#38388;&#21644;12&#23567;&#26102;SOFA&#35780;&#20998;&#30340;&#24433;&#21709;&#30340;&#19968;&#20123;&#26377;&#36259;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous treatment effect estimation is an important problem in precision medicine. Specific interests lie in identifying the differential effect of different treatments based on some external covariates. We propose a novel non-parametric treatment effect estimation method in a multi-treatment setting. Our non-parametric modeling of the response curves relies on radial basis function (RBF)-nets with shared hidden neurons. Our model thus facilitates modeling commonality among the treatment outcomes. The estimation and inference schemes are developed under a Bayesian framework and implemented via an efficient Markov chain Monte Carlo algorithm, appropriately accommodating uncertainty in all aspects of the analysis. The numerical performance of the method is demonstrated through simulation experiments. Applying our proposed method to MIMIC data, we obtain several interesting findings related to the impact of different treatment strategies on the length of ICU stay and 12-hour SOFA sc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#27169;&#25311;&#24182;&#26377;&#25928;&#27169;&#25311;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#30340;NeuralMD&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24314;&#27169;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15122</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#32423;&#23545;&#31216;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics. (arXiv:2401.15122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15122
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#27169;&#25311;&#24182;&#26377;&#25928;&#27169;&#25311;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#30340;NeuralMD&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24314;&#27169;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#20272;&#35745;&#36816;&#36755;&#24615;&#33021;&#21644;&#25506;&#32034;&#21475;&#34955;&#20301;&#28857;&#12290;&#36890;&#36807;&#25913;&#36827;&#25968;&#20540;&#26041;&#27861;&#20197;&#21450;&#26368;&#36817;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22686;&#24378;MD&#27169;&#25311;&#30340;&#25928;&#29575;&#24050;&#32463;&#26377;&#20102;&#24456;&#38271;&#30340;&#21382;&#21490;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#20934;&#30830;&#24314;&#27169;&#25193;&#23637;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#25311;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuralMD&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;ML&#36741;&#21161;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#29702;&#30340;&#26041;&#27861;&#65292;&#23558;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#20010;&#20351;&#29992;&#21521;&#37327;&#26694;&#26550;&#28385;&#36275;&#32676;&#23545;&#31216;&#24615;&#24182;&#25429;&#33719;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;BindingNet&#27169;&#22411;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#20010;&#22686;&#24378;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#23398;&#20064;&#36712;&#36857;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by augmenting them with machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations of protein-ligand binding dynamics. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural differential equation solver that learns the trajectory und
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#35268;&#33539;&#39044;&#27979;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#65292;&#23545;&#20154;&#26426;&#21327;&#21516;&#20915;&#31574;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.13744</link><description>&lt;p&gt;
&#12298;&#35268;&#33539;&#39044;&#27979;&#38598;&#25552;&#21319;&#20154;&#31867;&#20915;&#31574;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction Sets Improve Human Decision Making. (arXiv:2401.13744v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#35268;&#33539;&#39044;&#27979;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#65292;&#23545;&#20154;&#26426;&#21327;&#21516;&#20915;&#31574;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23545;&#26085;&#24120;&#26597;&#35810;&#30340;&#22238;&#24212;&#65292;&#20154;&#31867;&#26126;&#30830;&#22320;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26367;&#20195;&#31572;&#26696;&#12290;&#36890;&#36807;&#35268;&#33539;&#39044;&#27979;&#36755;&#20986;&#26657;&#20934;&#30340;&#39044;&#27979;&#38598;&#65292;&#27169;&#20223;&#20102;&#20154;&#31867;&#30340;&#36825;&#31181;&#34892;&#20026;&#65307;&#26356;&#22823;&#30340;&#39044;&#27979;&#38598;&#34920;&#31034;&#26356;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#26045;&#39044;&#27880;&#20876;&#30340;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65292;&#24182;&#32473;&#20154;&#31867;&#21463;&#35797;&#32773;&#25552;&#20379;&#35268;&#33539;&#39044;&#27979;&#38598;&#65292;&#30740;&#31350;&#20102;&#35268;&#33539;&#39044;&#27979;&#38598;&#23545;&#20154;&#31867;&#20915;&#31574;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#32479;&#35745;&#23398;&#26174;&#33879;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#20154;&#31867;&#33719;&#24471;&#35268;&#33539;&#39044;&#27979;&#38598;&#26102;&#65292;&#20182;&#20204;&#22312;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#27604;&#20351;&#29992;&#30456;&#21516;&#35206;&#30422;&#20445;&#35777;&#30340;&#22266;&#23450;&#23610;&#23544;&#39044;&#27979;&#38598;&#26102;&#26377;&#25152;&#25552;&#39640;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29992;&#35268;&#33539;&#39044;&#27979;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#26377;&#21161;&#20110;&#20154;&#26426;&#21327;&#21516;&#20915;&#31574;&#21644;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. Machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. In this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. With statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. The results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-AI teams.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#30452;&#25509;&#25512;&#26029;&#38544;&#24335;&#32467;&#26500;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#21487;&#33021;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#21644;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.19390</link><description>&lt;p&gt;
&#38544;&#24335;&#27969;&#24418;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Implicit Manifold Gaussian Process Regression. (arXiv:2310.19390v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#30452;&#25509;&#25512;&#26029;&#38544;&#24335;&#32467;&#26500;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#21487;&#33021;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#21644;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22240;&#20854;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#22788;&#29702;&#23567;&#22411;&#25110;&#31232;&#30095;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#32780;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#32500;&#25968;&#25454;&#65292;&#23427;&#23384;&#22312;&#19968;&#23450;&#22256;&#38590;&#12290;&#19968;&#31181;&#23558;&#36825;&#31181;&#25216;&#26415;&#25193;&#23637;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#21487;&#33021;&#36884;&#24452;&#26159;&#21033;&#29992;&#25968;&#25454;&#23454;&#38469;&#25152;&#22788;&#30340;&#38544;&#24335;&#20302;&#32500;&#27969;&#24418;&#65292;&#36825;&#26159;&#27969;&#24418;&#20551;&#35774;&#25152;&#20551;&#23450;&#30340;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#35201;&#27714;&#26174;&#24335;&#25552;&#20379;&#27969;&#24418;&#32467;&#26500;&#65292;&#21363;&#30001;&#32593;&#26684;&#25110;&#24050;&#30693;&#20026;&#20247;&#25152;&#21608;&#30693;&#30340;&#27969;&#24418;&#20043;&#19968;&#65288;&#22914;&#29699;&#20307;&#65289;&#32473;&#20986;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20197;&#23436;&#20840;&#21487;&#24494;&#30340;&#26041;&#24335;&#20174;&#25968;&#25454;&#65288;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#65289;&#20013;&#25512;&#26029;&#20986;&#38544;&#24335;&#32467;&#26500;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25216;&#26415;&#12290;&#23545;&#20110;&#24471;&#21040;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20854;&#22312;&#20551;&#35774;&#27969;&#24418;&#19978;&#25910;&#25947;&#20110;Mat&#233;rn&#39640;&#26031;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#25193;&#23637;&#21040;&#25968;&#21313;&#19975;&#20010;&#25968;&#25454;&#28857;&#65292;&#24182;&#19988;&#21487;&#33021;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#21644;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian process regression is widely used because of its ability to provide well-calibrated uncertainty estimates and handle small or sparse datasets. However, it struggles with high-dimensional data. One possible way to scale this technique to higher dimensions is to leverage the implicit low-dimensional manifold upon which the data actually lies, as postulated by the manifold hypothesis. Prior work ordinarily requires the manifold structure to be explicitly provided though, i.e. given by a mesh or be known to be one of the well-known manifolds like the sphere. In contrast, in this paper we propose a Gaussian process regression technique capable of inferring implicit structure directly from data (labeled and unlabeled) in a fully differentiable way. For the resulting model, we discuss its convergence to the Mat\'ern Gaussian process on the assumed manifold. Our technique scales up to hundreds of thousands of data points, and may improve the predictive performance and calibration of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#30340;&#22122;&#22768;&#20960;&#20309;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#21457;&#29616;&#22122;&#22768;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#20960;&#20309;&#29305;&#24449;&#26377;&#21033;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SGD&#22312;&#36867;&#33073;&#23574;&#38160;&#26497;&#23567;&#20540;&#26102;&#19982;GD&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#36867;&#33073;&#26041;&#21521;&#22312;&#24179;&#22374;&#26041;&#21521;&#19978;&#26377;&#26174;&#33879;&#20998;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.00692</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#22122;&#22768;&#20960;&#20309;&#65306;&#23450;&#37327;&#21644;&#20998;&#26512;&#29305;&#24449;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization. (arXiv:2310.00692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#30340;&#22122;&#22768;&#20960;&#20309;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#21457;&#29616;&#22122;&#22768;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#20960;&#20309;&#29305;&#24449;&#26377;&#21033;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SGD&#22312;&#36867;&#33073;&#23574;&#38160;&#26497;&#23567;&#20540;&#26102;&#19982;GD&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#36867;&#33073;&#26041;&#21521;&#22312;&#24179;&#22374;&#26041;&#21521;&#19978;&#26377;&#26174;&#33879;&#20998;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#30340;&#22122;&#22768;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#20960;&#20309;&#29305;&#24449;&#26377;&#21033;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#29616;&#35937;&#30340;&#29702;&#35770;&#21644;&#23450;&#37327;&#35299;&#37322;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#23545;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#19978;&#36848;&#8220;&#22122;&#22768;&#20960;&#20309;&#8221;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#32454;&#33268;&#22320;&#30740;&#31350;&#20102;&#24179;&#22343;&#21644;&#26041;&#21521;&#30340;&#19968;&#33268;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#26679;&#26412;&#22823;&#23567;&#21644;&#36755;&#20837;&#25968;&#25454;&#36864;&#21270;&#23545;&#19968;&#33268;&#24615;&#24378;&#24230;&#30340;&#24433;&#21709;&#12290;&#20316;&#20026;&#29305;&#23450;&#24212;&#29992;&#65292;&#25105;&#20204;&#21033;&#29992;&#22122;&#22768;&#20960;&#20309;&#29305;&#24449;&#30740;&#31350;&#20102;SGD&#22914;&#20309;&#20174;&#23574;&#38160;&#26497;&#23567;&#20540;&#20013;&#36867;&#33073;&#65292;&#21457;&#29616;&#36867;&#33073;&#26041;&#21521;&#22312;&#24179;&#22374;&#26041;&#21521;&#19978;&#26377;&#26174;&#33879;&#20998;&#37327;&#65292;&#36825;&#19982;&#21482;&#22312;&#26368;&#23574;&#38160;&#26041;&#21521;&#36867;&#33073;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;GD&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical studies have demonstrated that the noise in stochastic gradient descent (SGD) aligns favorably with the local geometry of loss landscape. However, theoretical and quantitative explanations for this phenomenon remain sparse. In this paper, we offer a comprehensive theoretical investigation into the aforementioned {\em noise geometry} for over-parameterized linear (OLMs) models and two-layer neural networks. We scrutinize both average and directional alignments, paying special attention to how factors like sample size and input data degeneracy affect the alignment strength. As a specific application, we leverage our noise geometry characterizations to study how SGD escapes from sharp minima, revealing that the escape direction has significant components along flat directions. This is in stark contrast to GD, which escapes only along the sharpest directions. To substantiate our theoretical findings, both synthetic and real-world experiments are provided.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#21644;&#35821;&#35328;&#25551;&#36848;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#32780;&#19988;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.05061</link><description>&lt;p&gt;
&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language. (arXiv:2308.05061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#21644;&#35821;&#35328;&#25551;&#36848;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#32780;&#19988;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#22312;&#25512;&#29702;&#38454;&#27573;&#20174;&#25552;&#31034;&#25968;&#25454;&#20013;&#23398;&#20064;&#36816;&#31639;&#31526;&#30340;&#26174;&#33879;&#28508;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#27169;&#22411;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#24573;&#35270;&#36816;&#31639;&#31526;&#30340;&#23453;&#36149;&#30340;&#20154;&#31867;&#27934;&#23519;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;&#36716;&#21270;&#20026;&#19968;&#31181;&#22810;&#27169;&#24335;&#33539;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#8220;&#26631;&#39064;&#8221;&#26469;&#25972;&#21512;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#26041;&#31243;&#24335;&#34920;&#36798;&#30340;&#36816;&#31639;&#31526;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36941;&#24615;&#65292;&#32780;&#19988;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#24182;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#22810;&#27169;&#24335;&#19978;&#19979;&#25991;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#8220;ICON-LM&#8221;&#65292;&#22522;&#20110;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of "captions" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as "ICON-LM", based on a language-model-like architecture. We demonstrate the viability of "ICO
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#19979;&#23454;&#29616;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#21518;&#24724;&#12290;</title><link>http://arxiv.org/abs/2307.13903</link><description>&lt;p&gt;
&#33104;&#36133;&#40065;&#26834;&#30340;Lipschitz&#19978;&#19979;&#25991;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Corruption-Robust Lipschitz Contextual Search. (arXiv:2307.13903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#19979;&#23454;&#29616;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#30740;&#31350;&#20102;&#23398;&#20064;&#20855;&#26377;&#34987;&#31713;&#25913;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#30340;Lipschitz&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#23398;&#20064;&#32773;&#35797;&#22270;&#23398;&#20064;&#19968;&#20010;&#30001;&#23545;&#25163;&#36873;&#25321;&#30340;Lipschitz&#20989;&#25968;$f$&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#23545;&#25163;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#36873;&#25321;&#19968;&#20010;&#19978;&#19979;&#25991;&#21521;&#37327;$x_t$&#65292;&#23398;&#20064;&#32773;&#23545;&#30495;&#23454;&#20989;&#25968;&#20540;$f(x_t)$&#36827;&#34892;&#29468;&#27979;&#65292;&#24182;&#25509;&#25910;&#19968;&#20010;&#25351;&#31034;&#29468;&#27979;&#26159;&#39640;&#36824;&#26159;&#20302;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#12290;&#22312;&#24635;&#20849;$C$&#36718;&#20013;&#65292;&#20449;&#21495;&#21487;&#33021;&#34987;&#31713;&#25913;&#65292;&#20294;&#23398;&#20064;&#32773;&#19981;&#30693;&#36947;$C$&#30340;&#20540;&#12290;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#36896;&#25104;&#23567;&#30340;&#32047;&#31215;&#25439;&#22833;&#12290;&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#28982;&#32780;&#24378;&#22823;&#30340;&#25216;&#26415;&#39564;&#35777;&#65292;&#23545;&#35774;&#35745;&#33104;&#36133;&#40065;&#26834;&#31639;&#27861;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#35774;&#35745;&#20102;&#19968;&#20123;&#31639;&#27861;&#65288;&#23558;Lipschitz&#21442;&#25968;$L$&#35270;&#20026;&#24120;&#25968;&#65289;&#65306;&#23545;&#20110;&#23545;&#31216;&#25439;&#22833;&#65292;&#23398;&#20064;&#32773;&#22312;$d=1$&#26102;&#36798;&#21040;&#21518;&#24724;$O(C\log T)$&#65292;&#22312;$d&gt;1$&#26102;&#36798;&#21040;&#21518;&#24724;$O_d(C\log T + T^{(d-1)/d})$&#65307;&#23545;&#20110;&#35745;&#20215;&#25439;&#22833;&#65292;&#23398;&#20064;&#32773;&#22312;$d/(d+1)$&#26102;&#36798;&#21040;&#21518;&#24724;$\widetilde{O}(T^{d/(d+1)} + C\cdot T^{1/(d+1)})$&#12290;
&lt;/p&gt;
&lt;p&gt;
I study the problem of learning a Lipschitz function with corrupted binary signals. The learner tries to learn a Lipschitz function $f$ that the adversary chooses. In each round, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low. In a total of $C$ rounds, the signal may be corrupted, though the value of $C$ is unknown to the learner. The learner's goal is to incur a small cumulative loss. I present a natural yet powerful technique sanity check, which proves useful in designing corruption-robust algorithms. I design algorithms which (treating the Lipschitz parameter $L$ as constant): for the symmetric loss, the learner achieves regret $O(C\log T)$ with $d = 1$ and $O_d(C\log T + T^{(d-1)/d})$ with $d &gt; 1$; for the pricing loss the learner achieves regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#24046;&#24322;&#12290;&#23427;&#21487;&#20197;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2306.04817</link><description>&lt;p&gt;
SiBBlInGS: &#20351;&#29992;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#27169;&#22359;&#25512;&#29702;&#30340;&#24314;&#27169;&#22359;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States. (arXiv:2306.04817v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#24046;&#24322;&#12290;&#23427;&#21487;&#20197;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#26469;&#35828;&#65292;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#27169;&#22359;&#26159;&#21457;&#29616;&#22797;&#26434;&#31995;&#32479;&#20013;&#26377;&#20215;&#20540;&#35265;&#35299;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;(SiBBlInGS)&#65292;&#29992;&#20110;&#21457;&#29616;&#27169;&#22359;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#33021;&#22815;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;SiBBlInGS&#36824;&#20801;&#35768;&#36328;&#29366;&#24577;&#21464;&#21270;&#27169;&#22359;&#32467;&#26500;&#21644;&#27599;&#27425;&#35797;&#39564;&#30340;&#26102;&#38388;&#21464;&#24322;&#65292;&#24182;&#21487;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable methods for extracting meaningful building blocks (BBs) underlying multi-dimensional time series are vital for discovering valuable insights in complex systems. Existing techniques, however, encounter limitations that restrict their applicability to real-world systems, like reliance on orthogonality assumptions, inadequate incorporation of inter- and intra-state variability, and incapability to handle sessions of varying duration. Here, we present a framework for Similarity-driven Building Block Inference using Graphs across States (SiBBlInGS). SiBBlInGS employs a graph-based dictionary learning approach for BB discovery, simultaneously considers both inter- and intra-state relationships in the data, can extract non-orthogonal components, and allows for variations in session counts and duration across states. Additionally, SiBBlInGS allows for cross-state variations in BB structure and per-trial temporal variability, can identify state-specific vs state-invariant BBs, and
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGDM&#65289;&#21644;&#20854;Polyak-averaging&#29256;&#26412;&#30340;&#29305;&#24615;&#65292;&#34920;&#26126;&#22312;&#36739;&#22823;&#30340;&#25209;&#37327;&#22823;&#23567;&#19979;&#65292;&#23567;&#25209;&#37327;SGDM&#27604;&#23567;&#25209;&#37327;SGD&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#30340;&#37051;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.17665</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#22343;&#21152;&#36895;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65306;&#26377;&#38480;&#26679;&#26412;&#36895;&#29575;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;
&lt;/p&gt;
&lt;p&gt;
Acceleration of stochastic gradient descent with momentum by averaging: finite-sample rates and asymptotic normality. (arXiv:2305.17665v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17665
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGDM&#65289;&#21644;&#20854;Polyak-averaging&#29256;&#26412;&#30340;&#29305;&#24615;&#65292;&#34920;&#26126;&#22312;&#36739;&#22823;&#30340;&#25209;&#37327;&#22823;&#23567;&#19979;&#65292;&#23567;&#25209;&#37327;SGDM&#27604;&#23567;&#25209;&#37327;SGD&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#30340;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGDM&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24212;&#29992;&#20013;&#12290;&#23613;&#31649;SGDM&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20855;&#26377;&#35266;&#23519;&#21040;&#30340;&#32463;&#39564;&#20248;&#21183;&#65292;&#20294;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21160;&#37327;&#23545;&#19981;&#21516;&#23398;&#20064;&#29575;&#30340;&#20316;&#29992;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26159;&#24320;&#25918;&#30340;&#12290;&#25105;&#20204;&#22312;&#24378;&#20984;&#35774;&#32622;&#19979;&#20998;&#26512;&#20102;SGDM&#30340;&#26377;&#38480;&#26679;&#26412;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#34920;&#26126;&#22312;&#36739;&#22823;&#30340;&#25209;&#37327;&#22823;&#23567;&#19979;&#65292;&#23567;&#25209;&#37327;SGDM&#27604;&#23567;&#25209;&#37327;SGD&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#30340;&#37051;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;SGDM&#20272;&#35745;&#37327;&#30340;Polyak&#24179;&#22343;&#29256;&#26412;&#65292;&#24314;&#31435;&#20102;&#23427;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#19982;&#24179;&#22343;SGD&#30340;&#28176;&#36817;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent with momentum (SGDM) has been widely used in many machine learning and statistical applications. Despite the observed empirical benefits of SGDM over traditional SGD, the theoretical understanding of the role of momentum for different learning rates in the optimization process remains widely open. We analyze the finite-sample convergence rate of SGDM under the strongly convex settings and show that, with a large batch size, the mini-batch SGDM converges faster than mini-batch SGD to a neighborhood of the optimal value. Furthermore, we analyze the Polyak-averaging version of the SGDM estimator, establish its asymptotic normality, and justify its asymptotic equivalence to the averaged SGD.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#26816;&#39564;&#21644;&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#26469;&#35780;&#20272;&#26657;&#20934;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#22823;&#32966;&#20877;&#26657;&#20934;&#31574;&#30053;&#65292;&#20351;&#23454;&#36341;&#32773;&#33021;&#22815;&#22312;&#28385;&#36275;&#25152;&#38656;&#30340;&#26657;&#20934;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#36127;&#36131;&#20219;&#22320;&#22686;&#24378;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.03780</link><description>&lt;p&gt;
&#20108;&#20803;&#20107;&#20214;&#30340;&#26657;&#20934;&#35780;&#20272;&#21644;&#22823;&#32966;&#20877;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Calibration Assessment and Boldness-Recalibration for Binary Events. (arXiv:2305.03780v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#26816;&#39564;&#21644;&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#26469;&#35780;&#20272;&#26657;&#20934;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#22823;&#32966;&#20877;&#26657;&#20934;&#31574;&#30053;&#65292;&#20351;&#23454;&#36341;&#32773;&#33021;&#22815;&#22312;&#28385;&#36275;&#25152;&#38656;&#30340;&#26657;&#20934;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#36127;&#36131;&#20219;&#22320;&#22686;&#24378;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#39044;&#27979;&#23545;&#20110;&#21307;&#23398;&#12289;&#32463;&#27982;&#12289;&#22270;&#20687;&#20998;&#31867;&#12289;&#20307;&#32946;&#20998;&#26512;&#12289;&#23089;&#20048;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#27010;&#29575;&#39044;&#27979;&#24212;&#35813; (i) &#26657;&#20934;&#33391;&#22909; (ii) &#20934;&#30830; (iii) &#22823;&#32966;&#65292;&#21363;&#36828;&#31163;&#20107;&#20214;&#30340;&#22522;&#30784;&#39057;&#29575;&#12290;&#28385;&#36275;&#36825;&#19977;&#20010;&#26465;&#20214;&#30340;&#39044;&#27979;&#23545;&#20110;&#20915;&#31574;&#21046;&#23450;&#26159;&#26377;&#20449;&#24687;&#37327;&#30340;&#12290;&#28982;&#32780;&#65292;&#26657;&#20934;&#21644;&#22823;&#32966;&#20043;&#38388;&#23384;&#22312;&#22522;&#26412;&#30340;&#32039;&#24352;&#20851;&#31995;&#65292;&#22240;&#20026;&#24403;&#39044;&#27979;&#36807;&#20110;&#35880;&#24910;&#26102;(&#21363;&#38750;&#22823;&#32966;)&#26657;&#20934;&#24230;&#37327;&#21487;&#20197;&#24456;&#39640;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#24320;&#21457;&#19968;&#31181;&#20551;&#35774;&#26816;&#39564;&#21644;&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#26469;&#35780;&#20272;&#26657;&#20934;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#22823;&#32966;&#20877;&#26657;&#20934;&#31574;&#30053;&#65292;&#20351;&#23454;&#36341;&#32773;&#33021;&#22815;&#22312;&#28385;&#36275;&#25152;&#38656;&#30340;&#26657;&#20934;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#36127;&#36131;&#20219;&#22320;&#22686;&#24378;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20801;&#35768;&#29992;&#25143;&#39044;&#20808;&#25351;&#23450;&#20182;&#20204;&#25152;&#38656;&#30340;&#21518;&#39564;&#26657;&#20934;&#27010;&#29575;&#65292;&#28982;&#21518;&#22312;&#27492;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#22686;&#24378;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probability predictions are essential to inform decision making in medicine, economics, image classification, sports analytics, entertainment, and many other fields. Ideally, probability predictions are (i) well calibrated, (ii) accurate, and (iii) bold, i.e., far from the base rate of the event. Predictions that satisfy these three criteria are informative for decision making. However, there is a fundamental tension between calibration and boldness, since calibration metrics can be high when predictions are overly cautious, i.e., non-bold. The purpose of this work is to develop a hypothesis test and Bayesian model selection approach to assess calibration, and a strategy for boldness-recalibration that enables practitioners to responsibly embolden predictions subject to their required level of calibration. Specifically, we allow the user to pre-specify their desired posterior probability of calibration, then maximally embolden predictions subject to this constraint. We verify the perfo
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#27573;&#24402;&#19968;&#21270;&#27969;&#26041;&#27861;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#20998;&#25104;&#38598;&#32676;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#25311;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#26631;&#20934;&#27491;&#24577;&#22522;&#30784;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.02930</link><description>&lt;p&gt;
&#20998;&#27573;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Piecewise Normalizing Flows. (arXiv:2305.02930v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02930
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#27573;&#24402;&#19968;&#21270;&#27969;&#26041;&#27861;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#20998;&#25104;&#38598;&#32676;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#25311;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#26631;&#20934;&#27491;&#24577;&#22522;&#30784;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#27969;&#26159;&#19968;&#31181;&#36890;&#36807;&#20174;&#22522;&#30784;&#20998;&#24067;&#36827;&#34892;&#21487;&#36870;&#36716;&#25442;&#26469;&#23545;&#22797;&#26434;&#27010;&#29575;&#23494;&#24230;&#36827;&#34892;&#24314;&#27169;&#30340;&#25104;&#29087;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#20998;&#24067;&#33021;&#21542;&#31934;&#30830;&#22320;&#34987;&#24402;&#19968;&#21270;&#27969;&#25152;&#25429;&#25417;&#65292;&#24378;&#28872;&#21463;&#21040;&#22522;&#30784;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;&#30446;&#26631;&#21644;&#22522;&#30784;&#20998;&#24067;&#20043;&#38388;&#30340;&#25299;&#25169;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#24046;&#65292;&#22914;&#23545;&#20110;&#22810;&#27169;&#24577;&#38382;&#39064;&#12290;&#19968;&#20123;&#19981;&#21516;&#30340;&#24037;&#20316;&#35797;&#22270;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411; [Izmailov et al., 2020&#12289;Ardizzone et al., 2020&#12289;Hagemann and Neumayer, 2021] &#25110;&#23398;&#20064;&#25509;&#21463;/&#25298;&#32477;&#37319;&#26679; [Stimper et al., 2022] &#26469;&#20462;&#25913;&#22522;&#30784;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#27573;&#24402;&#19968;&#21270;&#27969;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#20998;&#25104;&#38598;&#32676;&#65292;&#24182;&#35757;&#32451;&#19968;&#31995;&#21015;&#27969;&#26469;&#27169;&#25311;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows are an established approach for modelling complex probability densities through invertible transformations from a base distribution. However, the accuracy with which the target distribution can be captured by the normalizing flow is strongly influenced by the topology of the base distribution. A mismatch between the topology of the target and the base can result in a poor performance, as is the case for multi-modal problems. A number of different works have attempted to modify the topology of the base distribution to better match the target, either through the use of Gaussian Mixture Models [Izmailov et al., 2020, Ardizzone et al., 2020, Hagemann and Neumayer, 2021] or learned accept/reject sampling [Stimper et al., 2022]. We introduce piecewise normalizing flows which divide the target distribution into clusters, with topologies that better match the standard normal base distribution, and train a series of flows to model complex multi-modal targets. The piecewise nat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#26356;&#39640;&#38454;&#28151;&#21512;&#25104;&#21592;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;$\ell_{2,\infty}$&#24352;&#37327;&#25200;&#21160;&#30028;&#38480;&#65292;&#36890;&#36807;&#24352;&#37327;&#28151;&#21512;&#25104;&#21592;&#27169;&#22411;&#23545;&#22810;&#26679;&#21270;&#25968;&#25454;&#30340;&#31038;&#21306;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#39640;&#38454;&#27491;&#20132;&#36845;&#20195;&#31639;&#27861;&#36827;&#34892;&#20272;&#35745;&#36807;&#31243;&#12290;&#36890;&#36807;&#25552;&#20379;&#27599;&#20010;&#33410;&#28857;&#30340;&#35823;&#24046;&#30028;&#38480;&#26469;&#35777;&#26126;&#20102;&#20272;&#35745;&#36807;&#31243;&#30340;&#19968;&#33268;&#24615;&#65292;&#23637;&#31034;&#20102;&#39640;&#38454;&#32467;&#26500;&#23545;&#20272;&#35745;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.08642</link><description>&lt;p&gt;
&#36890;&#36807;$\ell_{2,\infty}$&#24352;&#37327;&#25200;&#21160;&#30028;&#38480;&#20272;&#35745;&#26356;&#39640;&#38454;&#28151;&#21512;&#25104;&#21592;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Estimating Higher-Order Mixed Memberships via the $\ell_{2,\infty}$ Tensor Perturbation Bound. (arXiv:2212.08642v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#26356;&#39640;&#38454;&#28151;&#21512;&#25104;&#21592;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;$\ell_{2,\infty}$&#24352;&#37327;&#25200;&#21160;&#30028;&#38480;&#65292;&#36890;&#36807;&#24352;&#37327;&#28151;&#21512;&#25104;&#21592;&#27169;&#22411;&#23545;&#22810;&#26679;&#21270;&#25968;&#25454;&#30340;&#31038;&#21306;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#39640;&#38454;&#27491;&#20132;&#36845;&#20195;&#31639;&#27861;&#36827;&#34892;&#20272;&#35745;&#36807;&#31243;&#12290;&#36890;&#36807;&#25552;&#20379;&#27599;&#20010;&#33410;&#28857;&#30340;&#35823;&#24046;&#30028;&#38480;&#26469;&#35777;&#26126;&#20102;&#20272;&#35745;&#36807;&#31243;&#30340;&#19968;&#33268;&#24615;&#65292;&#23637;&#31034;&#20102;&#39640;&#38454;&#32467;&#26500;&#23545;&#20272;&#35745;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26356;&#39640;&#38454;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#24182;&#32463;&#24120;&#34920;&#29616;&#20986;&#31867;&#20284;&#31038;&#21306;&#30340;&#32467;&#26500;&#65292;&#20854;&#20013;&#27599;&#20010;&#20998;&#37327;&#65288;&#33410;&#28857;&#65289;&#22312;&#27599;&#20010;&#19981;&#21516;&#30340;&#27169;&#24335;&#19978;&#37117;&#26377;&#19968;&#20010;&#19982;&#20043;&#20851;&#32852;&#30340;&#31038;&#21306;&#25104;&#21592;&#36164;&#26684;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24352;&#37327;&#28151;&#21512;&#25104;&#21592;&#27169;&#22411;&#65292;&#36825;&#26159;&#24352;&#37327;&#22359;&#27169;&#22411;&#30340;&#19968;&#20010;&#25512;&#24191;&#65292;&#20854;&#20551;&#35774;&#25104;&#21592;&#20851;&#31995;&#19981;&#38656;&#35201;&#26159;&#31163;&#25955;&#30340;&#65292;&#32780;&#26159;&#28508;&#22312;&#31038;&#21306;&#30340;&#20984;&#32452;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#38454;&#27491;&#20132;&#36845;&#20195;&#31639;&#27861;&#65288;HOOI&#65289;&#19982;&#21333;&#32431;&#24418;&#35282;&#28857;&#23547;&#25214;&#31639;&#27861;&#32452;&#21512;&#30340;&#35745;&#31639;&#25928;&#29575;&#20272;&#35745;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#27599;&#20010;&#33410;&#28857;&#30340;&#35823;&#24046;&#30028;&#38480;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#20272;&#35745;&#36807;&#31243;&#30340;&#19968;&#33268;&#24615;&#65292;&#36825;&#23637;&#31034;&#20102;&#39640;&#38454;&#32467;&#26500;&#23545;&#20272;&#35745;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#30340;&#19968;&#33268;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#29420;&#31435;&#30340;&#12289;&#21487;&#33021;&#24322;&#26041;&#24046;&#30340;$\ell_{2,\infty}$&#24352;&#37327;&#25200;&#21160;&#30028;&#38480;&#30340;HOOI&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Higher-order multiway data is ubiquitous in machine learning and statistics and often exhibits community-like structures, where each component (node) along each different mode has a community membership associated with it. In this paper we propose the tensor mixed-membership blockmodel, a generalization of the tensor blockmodel positing that memberships need not be discrete, but instead are convex combinations of latent communities. We establish the identifiability of our model and propose a computationally efficient estimation procedure based on the higher-order orthogonal iteration algorithm (HOOI) for tensor SVD composed with a simplex corner-finding algorithm. We then demonstrate the consistency of our estimation procedure by providing a per-node error bound, which showcases the effect of higher-order structures on estimation accuracy. To prove our consistency result, we develop the $\ell_{2,\infty}$ tensor perturbation bound for HOOI under independent, possibly heteroskedastic, su
&lt;/p&gt;</description></item></channel></rss>