<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.18510</link><description>&lt;p&gt;
RNNs&#36824;&#19981;&#26159;Transformer&#65306;&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#20013;&#30340;&#20851;&#38190;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#21644;Transformer&#22312;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#31034;&#33021;&#21147;&#24046;&#36317;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;RNNs&#26159;&#21542;&#33021;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#65292;&#36890;&#36807;Chain-of-Thought (CoT)&#25552;&#31034;&#65292;&#19982;Transformer&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#26174;&#31034;CoT&#21487;&#20197;&#25913;&#36827;RNNs&#65292;&#20294;&#26080;&#27861;&#24357;&#34917;&#19982;Transformer&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20851;&#38190;&#29942;&#39048;&#22312;&#20110;RNNs&#26080;&#27861;&#23436;&#20840;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#21363;&#20351;&#32463;&#36807;CoT&#30340;&#22686;&#24378;&#65306;&#23545;&#20110;&#20960;&#20010;&#26126;&#30830;&#25110;&#38544;&#24335;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#65292;&#22914;&#32852;&#24819;&#21484;&#22238;&#21644;&#30830;&#23450;&#22270;&#26159;&#21542;&#20026;&#26641;&#65292;&#25105;&#20204;&#35777;&#26126;RNNs&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#20197;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#65292;&#32780;Transformer&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#37319;&#29992;&#22686;&#24378;RNNs&#19978;&#19979;&#25991;&#26816;&#32034;&#33021;&#21147;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18510v1 Announce Type: cross  Abstract: This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, inclu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#32467;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.15347</link><description>&lt;p&gt;
&#20449;&#24687;&#35770;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Safe Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15347
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#32467;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#19981;&#35780;&#20272;&#36829;&#21453;&#20808;&#39564;&#26410;&#30693;&#65288;&#23433;&#20840;&#65289;&#32422;&#26463;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#26410;&#30693;&#20989;&#25968;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#22312;&#26410;&#30693;&#20989;&#25968;&#19978;&#25918;&#32622;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#65292;&#24182;&#19988;&#20165;&#20801;&#35768;&#22312;&#39640;&#27010;&#29575;&#23433;&#20840;&#21306;&#22495;&#20869;&#36827;&#34892;&#35780;&#20272;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#22495;&#30340;&#31163;&#25955;&#21270;&#65292;&#24182;&#19988;&#19981;&#33021;&#30452;&#25509;&#25193;&#23637;&#21040;&#36830;&#32493;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21033;&#29992;&#32422;&#26463;&#30340;&#35268;&#21017;&#20551;&#35774;&#30340;&#26041;&#24335;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#23433;&#20840;&#25506;&#32034;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#30452;&#25509;&#21033;&#29992;GP&#21518;&#39564;&#26469;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#30340;&#23433;&#20840;&#21442;&#25968;&#36827;&#34892;&#35780;&#20272;&#12290;&#23558;&#36825;&#19968;&#25506;&#32034;&#20934;&#21017;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#32467;&#21512;&#36215;&#26469;&#65292;&#20135;&#29983;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15347v1 Announce Type: cross  Abstract: We consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint. A common approach is to place a Gaussian process prior on the unknown functions and allow evaluations only in regions that are safe with high probability. Most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case. Moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter. In this paper, we propose an information-theoretic safe exploration criterion that directly exploits the GP posterior to identify the most informative safe parameters to evaluate. The combination of this exploration criterion with a well known Bayesian optimization acquisition function yields a novel safe Bayesian optimization selection criterion. Our approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;logistic-beta&#36807;&#31243;&#29992;&#20110;&#24314;&#27169;&#20855;&#26377;beta&#36793;&#38469;&#20998;&#24067;&#30340;&#30456;&#20851;&#38543;&#26426;&#27010;&#29575;&#12290;&#35813;&#36807;&#31243;&#20855;&#26377;&#28789;&#27963;&#30340;&#30456;&#20851;&#32467;&#26500;&#21644;&#35745;&#31639;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#38750;&#21442;&#25968;&#20108;&#20998;&#31867;&#22238;&#24402;&#27169;&#25311;&#30740;&#31350;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07048</link><description>&lt;p&gt;
&#29992;&#20110;&#24314;&#27169;&#20855;&#26377;beta&#36793;&#38469;&#20998;&#24067;&#30340;&#30456;&#20851;&#38543;&#26426;&#27010;&#29575;&#30340;logistic-beta&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Logistic-beta processes for modeling dependent random probabilities with beta marginals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;logistic-beta&#36807;&#31243;&#29992;&#20110;&#24314;&#27169;&#20855;&#26377;beta&#36793;&#38469;&#20998;&#24067;&#30340;&#30456;&#20851;&#38543;&#26426;&#27010;&#29575;&#12290;&#35813;&#36807;&#31243;&#20855;&#26377;&#28789;&#27963;&#30340;&#30456;&#20851;&#32467;&#26500;&#21644;&#35745;&#31639;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#38750;&#21442;&#25968;&#20108;&#20998;&#31867;&#22238;&#24402;&#27169;&#25311;&#30740;&#31350;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
beta&#20998;&#24067;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#27010;&#29575;&#24314;&#27169;&#65292;&#24182;&#22312;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#23588;&#20854;&#22312;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#39046;&#22495;&#12290;&#23613;&#31649;&#20854;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22312;&#24314;&#27169;&#30456;&#20851;&#38543;&#26426;&#27010;&#29575;&#30340;&#28789;&#27963;&#21644;&#35745;&#31639;&#26041;&#20415;&#30340;&#38543;&#26426;&#36807;&#31243;&#25193;&#23637;&#26041;&#38754;&#65292;&#30456;&#20851;&#24037;&#20316;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#31216;&#20026;logistic-beta&#36807;&#31243;&#65292;&#20854;logistic&#21464;&#25442;&#29983;&#25104;&#20855;&#26377;&#24120;&#35265;beta&#36793;&#38469;&#20998;&#24067;&#30340;&#38543;&#26426;&#36807;&#31243;&#12290;&#31867;&#20284;&#20110;&#39640;&#26031;&#36807;&#31243;&#65292;logistic-beta&#36807;&#31243;&#21487;&#20197;&#24314;&#27169;&#31163;&#25955;&#21644;&#36830;&#32493;&#22495;&#65288;&#20363;&#22914;&#31354;&#38388;&#25110;&#26102;&#38388;&#65289;&#19978;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#26680;&#20989;&#25968;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#30340;&#30456;&#20851;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#23427;&#30340;&#27491;&#24577;&#26041;&#24046;-&#22343;&#20540;&#28151;&#21512;&#34920;&#31034;&#23548;&#33268;&#20102;&#39640;&#25928;&#30340;&#21518;&#39564;&#25512;&#29702;&#31639;&#27861;&#12290;&#36890;&#36807;&#38750;&#21442;&#25968;&#20108;&#20998;&#31867;&#22238;&#24402;&#27169;&#25311;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;logistic-beta&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#21644;&#35745;&#31639;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The beta distribution serves as a canonical tool for modeling probabilities and is extensively used in statistics and machine learning, especially in the field of Bayesian nonparametrics. Despite its widespread use, there is limited work on flexible and computationally convenient stochastic process extensions for modeling dependent random probabilities. We propose a novel stochastic process called the logistic-beta process, whose logistic transformation yields a stochastic process with common beta marginals. Similar to the Gaussian process, the logistic-beta process can model dependence on both discrete and continuous domains, such as space or time, and has a highly flexible dependence structure through correlation kernels. Moreover, its normal variance-mean mixture representation leads to highly effective posterior inference algorithms. The flexibility and computational benefits of logistic-beta processes are demonstrated through nonparametric binary regression simulation studies. Fur
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;Riemann-Lebesgue Forest (RLF)&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#21010;&#20998;&#20989;&#25968;&#30340;&#20540;&#22495;&#20026;&#22810;&#20010;&#21306;&#38388;&#26469;&#36924;&#36817;&#21487;&#27979;&#20989;&#25968;&#30340;&#24605;&#24819;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#23398;&#20064;&#31639;&#27861;Riemann-Lebesgue Tree&#12290;&#36890;&#36807;Hoeffding&#20998;&#35299;&#21644;Stein&#26041;&#27861;&#25512;&#23548;&#20102;RLF&#22312;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;&#30340;&#28176;&#36817;&#24615;&#33021;&#65292;&#24182;&#22312;&#20223;&#30495;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;RLF&#19982;&#21407;&#22987;&#38543;&#26426;&#26862;&#26519;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04550</link><description>&lt;p&gt;
Riemann-Lebesgue Forest&#22238;&#24402;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Riemann-Lebesgue Forest for Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04550
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;Riemann-Lebesgue Forest (RLF)&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#21010;&#20998;&#20989;&#25968;&#30340;&#20540;&#22495;&#20026;&#22810;&#20010;&#21306;&#38388;&#26469;&#36924;&#36817;&#21487;&#27979;&#20989;&#25968;&#30340;&#24605;&#24819;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#23398;&#20064;&#31639;&#27861;Riemann-Lebesgue Tree&#12290;&#36890;&#36807;Hoeffding&#20998;&#35299;&#21644;Stein&#26041;&#27861;&#25512;&#23548;&#20102;RLF&#22312;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;&#30340;&#28176;&#36817;&#24615;&#33021;&#65292;&#24182;&#22312;&#20223;&#30495;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;RLF&#19982;&#21407;&#22987;&#38543;&#26426;&#26862;&#26519;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#31216;&#20026;Riemann-Lebesgue Forest (RLF)&#12290;RLF&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#20989;&#25968;&#30340;&#20540;&#22495;&#21010;&#20998;&#20026;&#20960;&#20010;&#21306;&#38388;&#26469;&#27169;&#25311;&#21487;&#27979;&#20989;&#25968;&#30340;&#36924;&#36817;&#26041;&#24335;&#12290;&#22522;&#20110;&#36825;&#20010;&#24605;&#24819;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;Riemann-Lebesgue Tree&#65292;&#23427;&#22312;&#27599;&#20010;&#38750;&#21494;&#33410;&#28857;&#19978;&#26377;&#26426;&#20250;&#20174;&#21709;&#24212;Y&#25110;&#29305;&#24449;&#31354;&#38388;X&#20013;&#30340;&#26041;&#21521;&#36827;&#34892;&#20999;&#21106;&#12290;&#25105;&#20204;&#36890;&#36807;Hoeffding&#20998;&#35299;&#21644;Stein&#26041;&#27861;&#26469;&#25512;&#23548;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;RLF&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;&#24403;&#24213;&#23618;&#20989;&#25968;Y=f(X)&#36981;&#24490;&#21152;&#27861;&#22238;&#24402;&#27169;&#22411;&#26102;&#65292;RLF&#19982;Scornet&#31561;&#20154;&#30340;&#35770;&#35777;&#65288;2014&#24180;&#65289;&#20445;&#25345;&#19968;&#33268;&#12290;&#36890;&#36807;&#22312;&#20223;&#30495;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;RLF&#19982;&#21407;&#22987;&#38543;&#26426;&#26862;&#26519;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel ensemble method called Riemann-Lebesgue Forest (RLF) for regression. The core idea of RLF is to mimic the way how a measurable function can be approximated by partitioning its range into a few intervals. With this idea in mind, we develop a new tree learner named Riemann-Lebesgue Tree which has a chance to split the node from response $Y$ or a direction in feature space $\mathbf{X}$ at each non-terminal node. We generalize the asymptotic performance of RLF under different parameter settings mainly through Hoeffding decomposition \cite{Vaart} and Stein's method \cite{Chen2010NormalAB}. When the underlying function $Y=f(\mathbf{X})$ follows an additive regression model, RLF is consistent with the argument from \cite{Scornet2014ConsistencyOR}. The competitive performance of RLF against original random forest \cite{Breiman2001RandomF} is demonstrated by experiments in simulation data and real world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Cosmos&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#21644;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#30340;&#39640;&#24615;&#33021;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12690</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#19978;&#30340;&#32452;&#21512;&#24335;&#19990;&#30028;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic Grounding for Compositional World Models. (arXiv:2310.12690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Cosmos&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#21644;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#30340;&#39640;&#24615;&#33021;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Cosmos&#65292;&#19968;&#20010;&#38024;&#23545;&#32452;&#21512;&#27867;&#21270;&#65288;CG&#65289;&#35774;&#35745;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#26694;&#26550;&#65292;&#21363;&#22312;&#36890;&#36807;&#24050;&#30693;&#30340;&#35270;&#35273;&#8220;&#21407;&#23376;&#8221;&#32452;&#21512;&#33719;&#24471;&#30340;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;Cosmos&#30340;&#26680;&#24515;&#27934;&#23519;&#21147;&#26159;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#24037;&#20855;&#65306;&#65288;i&#65289;&#31070;&#32463;&#31526;&#21495;&#21270;&#22330;&#26223;&#32534;&#30721;&#65292;&#20351;&#29992;&#31070;&#32463;&#32534;&#30721;&#22120;&#35745;&#31639;&#27599;&#20010;&#22330;&#26223;&#20013;&#30340;&#23454;&#20307;&#30340;&#23454;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#25551;&#36848;&#23454;&#20307;&#23646;&#24615;&#30340;&#21487;&#32452;&#21512;&#31526;&#21495;&#21521;&#37327;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#31070;&#32463;&#31526;&#21495;&#21270;&#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#36825;&#20123;&#23454;&#20307;&#19982;&#23398;&#20064;&#21040;&#30340;&#20132;&#20114;&#35268;&#21017;&#32465;&#23450;&#36215;&#26469;&#12290;Cosmos&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#65307;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#38656;&#35201;&#25163;&#21160;&#23558;&#34920;&#31034;&#26144;&#23556;&#20026;&#31526;&#21495;&#19981;&#21516;&#65292;&#23427;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#35745;&#31639;&#23454;&#20307;&#30340;&#31526;&#21495;&#23646;&#24615;&#12290;&#36890;&#36807;&#23545;&#24050;&#24314;&#31435;&#30340;blocks&#22330;&#26223;&#36827;&#34892;&#20004;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;CG&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;Cosmos&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Cosmos, a framework for object-centric world modeling that is designed for compositional generalization (CG), i.e., high performance on unseen input scenes obtained through the composition of known visual "atoms." The central insight behind Cosmos is the use of a novel form of neurosymbolic grounding. Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. Cosmos is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CG on an established blocks-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#32467;&#26500;&#20013;&#65292;&#21487;&#20197;&#28789;&#27963;&#12289;&#39640;&#25928;&#22320;&#27169;&#25311;&#20855;&#26377;&#38750;&#24179;&#31283;&#30456;&#20851;&#24615;&#30340;&#26497;&#31471;&#20107;&#20214;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26102;&#38388;&#25928;&#29575;&#21644;&#24615;&#33021;&#19978;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#35768;&#22810;&#20855;&#26377;&#24179;&#31283;&#30456;&#20851;&#24615;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.08079</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#33258;&#21160; &#32534;&#30721;&#22120;&#23454;&#29616;&#28789;&#27963;&#39640;&#25928;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Flexible and efficient spatial extremes emulation via variational autoencoders. (arXiv:2307.08079v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#32467;&#26500;&#20013;&#65292;&#21487;&#20197;&#28789;&#27963;&#12289;&#39640;&#25928;&#22320;&#27169;&#25311;&#20855;&#26377;&#38750;&#24179;&#31283;&#30456;&#20851;&#24615;&#30340;&#26497;&#31471;&#20107;&#20214;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26102;&#38388;&#25928;&#29575;&#21644;&#24615;&#33021;&#19978;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#35768;&#22810;&#20855;&#26377;&#24179;&#31283;&#30456;&#20851;&#24615;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#36807;&#31243;&#20855;&#26377;&#22797;&#26434;&#30340;&#23614;&#20381;&#36182;&#32467;&#26500;&#65292;&#36825;&#31181;&#32467;&#26500;&#26080;&#27861;&#20351;&#29992;&#20256;&#32479;&#30340;&#39640;&#26031;&#36807;&#31243;&#26469;&#25551;&#36848;&#12290;&#26356;&#28789;&#27963;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292; &#22914;&#39640;&#26031;&#23610;&#24230;&#28151;&#21512;&#27169;&#22411;&#21644;&#21333;&#31449;&#28857;&#35843;&#33410;&#27169;&#22411;&#65292;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#26497;&#31471;&#20381;&#36182;&#24615;&#36136;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#25311;&#21512;&#21644;&#27169;&#25311;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#20855;&#26377;&#28789;&#27963;&#21644;&#38750;&#24179;&#31283;&#30340;&#30456;&#20851;&#24615;&#23646;&#24615;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120; (extVAE) &#30340;&#32534;&#30721;-&#35299;&#30721;&#32467;&#26500;&#20013;&#12290; extVAE &#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#26102;&#31354;&#27169;&#25311;&#22120;&#65292;&#23545;&#28508;&#22312;&#30340;&#26426;&#21046;&#27169;&#22411;&#36755;&#20986;&#29366;&#24577;&#30340;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20135;&#29983;&#20855;&#26377;&#19982;&#36755;&#20837;&#30456;&#21516;&#23646;&#24615;&#30340;&#36755;&#20986;&#65292;&#23588;&#20854;&#26159;&#22312;&#23614;&#37096;&#21306;&#22495;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;extVAE&#27604;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26356;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377; &#24179;&#31283;&#30456;&#20851;&#24615;&#32467;&#26500;&#30340;&#35768;&#22810;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#20013;&#34920;&#29616; &#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world processes have complex tail dependence structures that cannot be characterized using classical Gaussian processes. More flexible spatial extremes models such as Gaussian scale mixtures and single-station conditioning models exhibit appealing extremal dependence properties but are often exceedingly prohibitive to fit and simulate from. In this paper, we develop a new spatial extremes model that has flexible and non-stationary dependence properties, and we integrate it in the encoding-decoding structure of a variational autoencoder (extVAE). The extVAE can be used as a spatio-temporal emulator that characterizes the distribution of potential mechanistic model output states and produces outputs that have the same properties as the inputs, especially in the tail. Through extensive simulation studies, we show that our extVAE is vastly more time-efficient than traditional Bayesian inference while also outperforming many spatial extremes models with a stationary dependence str
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#35843;&#25972;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#20272;&#35745;&#30340;&#32479;&#35745;&#24615;&#33021;&#65292;&#20445;&#25345;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.15579</link><description>&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#35843;&#25972;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning. (arXiv:2303.15579v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#35843;&#25972;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#20272;&#35745;&#30340;&#32479;&#35745;&#24615;&#33021;&#65292;&#20445;&#25345;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#32479;&#35745;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#35843;&#25972;&#30340;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#8212;&#8212;&#22522;&#20110;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#65288;WDRO&#65289;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#12290;&#36825;&#31181;&#36716;&#25442;&#23558;&#25552;&#39640;WDRO&#30340;&#32479;&#35745;&#24615;&#33021;&#65292;&#22240;&#20026;&#35843;&#25972;&#21518;&#30340;WDRO&#20272;&#35745;&#22120;&#28176;&#36827;&#26080;&#20559;&#24182;&#19988;&#22343;&#26041;&#35823;&#24046;&#36235;&#36817;&#20110;&#38646;&#12290;&#35843;&#25972;&#21518;&#30340;WDRO&#19981;&#20250;&#21066;&#24369;WDRO&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35843;&#25972;WDRO&#20272;&#35745;&#22120;&#30340;&#23384;&#22312;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#32473;&#20986;&#20102;&#35745;&#31639;&#35843;&#25972;WDRO&#20272;&#35745;&#22120;&#30340;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#22914;&#20309;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#24320;&#21457;&#35843;&#25972;WDRO&#20272;&#35745;&#22120;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35843;&#25972;&#21518;&#30340;&#20272;&#35745;&#22120;&#27604;&#32463;&#20856;&#20272;&#35745;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an adjusted Wasserstein distributionally robust estimator -- based on a nonlinear transformation of the Wasserstein distributionally robust (WDRO) estimator in statistical learning. This transformation will improve the statistical performance of WDRO because the adjusted WDRO estimator is asymptotically unbiased and has an asymptotically smaller mean squared error. The adjusted WDRO will not mitigate the out-of-sample performance guarantee of WDRO. Sufficient conditions for the existence of the adjusted WDRO estimator are presented, and the procedure for the computation of the adjusted WDRO estimator is given. Specifically, we will show how the adjusted WDRO estimator is developed in the generalized linear model. Numerical experiments demonstrate the favorable practical performance of the adjusted estimator over the classic one.
&lt;/p&gt;</description></item></channel></rss>