<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.09983</link><description>&lt;p&gt;
&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#25110;&#20915;&#31574;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#20154;&#33021;&#21147;&#65292;&#37027;&#20040;&#25105;&#20204;&#35813;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#20195;&#29702;&#20250;&#20135;&#29983;&#20559;&#24046;? &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21069;&#25552;&#26159;&#65292;&#34429;&#28982;&#35780;&#20272;&#36229;&#20154;&#20915;&#31574;&#30340;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#26159;&#22914;&#26524;&#27169;&#22411;&#30340;&#20915;&#31574;&#26410;&#33021;&#28385;&#36275;&#26576;&#20123;&#36923;&#36753;&#19978;&#12289;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21457;&#29616;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#30001;&#20110;&#36229;&#20154;&#27169;&#22411;&#33021;&#21147;&#25110;&#20854;&#20182;&#32570;&#20047;&#22522;&#26412;&#20107;&#23454;&#32780;&#38590;&#20197;&#35780;&#20272;&#65306;&#35780;&#20272;&#22269;&#38469;&#35937;&#26827;&#23616;&#38754;&#12289;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#20316;&#20986;&#27861;&#24459;&#21028;&#26029;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26080;&#35770;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;(&#21487;&#33021;&#26159;&#36229;&#20154;&#30340;)&#65292;&#25105;&#20204;&#37117;&#33021;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65306;&#22269;&#38469;&#35937;&#26827;&#24341;&#25806;&#32473;&#20986;&#23545;&#23616;&#20013;&#26827;&#23376;&#30456;&#23545;&#20272;&#20540;&#30340;&#19981;&#21516;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#21644;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#21457;&#29616;&#19981;&#24517;&#35201;&#37319;&#29992;&#39640;&#25104;&#26412;&#30340;&#24378;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#20182;&#35774;&#35745;&#36873;&#25321;&#26469;&#32531;&#35299;&#23545;&#25239;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.09951</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#24378;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#26469;&#31649;&#29702;&#23545;&#25239;&#25915;&#20987;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
You Don't Need Robust Machine Learning to Manage Adversarial Attack Risks. (arXiv:2306.09951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#21644;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#21457;&#29616;&#19981;&#24517;&#35201;&#37319;&#29992;&#39640;&#25104;&#26412;&#30340;&#24378;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#20182;&#35774;&#35745;&#36873;&#25321;&#26469;&#32531;&#35299;&#23545;&#25239;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24050;&#25104;&#20026;&#31038;&#21306;&#20869;&#26085;&#30410;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#33021;&#22815;&#36890;&#36807;&#23545;&#36755;&#20837;&#36827;&#34892;&#35980;&#20284;&#26080;&#20851;&#30340;&#26356;&#25913;&#26469;&#30772;&#22351;&#27169;&#22411;&#65292;&#20174;&#32780;&#23548;&#33268;&#38169;&#35823;&#39044;&#27979;&#30340;&#33021;&#21147;&#20196;&#20154;&#38663;&#24778;&#65292;&#32780;&#25105;&#20204;&#22312;&#26500;&#24314;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#26041;&#38754;&#30340;&#25104;&#25928;&#20063;&#19981;&#23481;&#20048;&#35266;&#12290;&#29616;&#26377;&#30740;&#31350;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#32531;&#35299;&#25514;&#26045;&#24102;&#26469;&#20102;&#24456;&#39640;&#30340;&#25104;&#26412;&#65292;&#21516;&#26102;&#20063;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23384;&#22312;&#20854;&#20182;&#35774;&#35745;&#36873;&#25321;&#21487;&#20197;&#36991;&#20813;&#36825;&#31181;&#39118;&#38505;&#26102;&#65292;&#36825;&#26679;&#30340;&#26435;&#34913;&#21487;&#33021;&#24182;&#19981;&#24517;&#35201;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30524;&#20809;&#20851;&#27880;&#23454;&#36341;&#20013;&#22914;&#20309;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#65292;&#29983;&#20135;&#37096;&#32626;&#30340;&#39118;&#38505;&#20197;&#21450;&#31649;&#29702;&#36825;&#20123;&#39118;&#38505;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#35768;&#22810;AML&#23041;&#32961;&#19981;&#36275;&#20197;&#35777;&#26126;&#36825;&#31181;&#25104;&#26412;&#21644;&#26435;&#34913;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of modern machine learning (ML) models has become an increasing concern within the community. The ability to subvert a model into making errant predictions using seemingly inconsequential changes to input is startling, as is our lack of success in building models robust to this concern. Existing research shows progress, but current mitigations come with a high cost and simultaneously reduce the model's accuracy. However, such trade-offs may not be necessary when other design choices could subvert the risk. In this survey we review the current literature on attacks and their real-world occurrences, or limited evidence thereof, to critically evaluate the real-world risks of adversarial machine learning (AML) for the average entity. This is done with an eye toward how one would then mitigate these attacks in practice, the risks for production deployment, and how those risks could be managed. In doing so we elucidate that many AML threats do not warrant the cost and trade-of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09927</link><description>&lt;p&gt;
&#35757;&#32451;&#22909;&#30340;Transformer&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20363;&#22914;Transformers&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65306;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#30701;&#35821;&#24207;&#21015;&#30340;&#25552;&#31034;&#65292;&#23427;&#20204;&#21487;&#20197;&#21046;&#23450;&#30456;&#20851;&#30340;&#27599;&#20010;&#20196;&#29260;&#21644;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#39044;&#27979;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#36890;&#36807;&#23558;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#24207;&#21015;&#23884;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#36825;&#20351;&#24471;Transformer&#34920;&#29616;&#24471;&#20687;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#35757;&#32451;Transformer&#20307;&#31995;&#32467;&#26500;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#20250;&#27169;&#20223;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26102;&#31354;Tweedie&#27169;&#22411;STTD&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#20998;&#36776;&#29575;OD&#30697;&#38453;&#20013;&#31232;&#30095;&#21644;&#38271;&#23614;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.09882</link><description>&lt;p&gt;
&#26102;&#31354;Tweedie&#27169;&#22411;&#22312;&#39044;&#27979;&#23384;&#22312;&#38646;&#33192;&#32960;&#21644;&#38271;&#23614;&#26053;&#34892;&#38656;&#27714;&#20013;&#30340;&#24212;&#29992;&#21450;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification via Spatial-Temporal Tweedie Model for Zero-inflated and Long-tail Travel Demand Prediction. (arXiv:2306.09882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26102;&#31354;Tweedie&#27169;&#22411;STTD&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#20998;&#36776;&#29575;OD&#30697;&#38453;&#20013;&#31232;&#30095;&#21644;&#38271;&#23614;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26102;&#31354;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38590;&#20197;&#35299;&#20915;&#39640;&#20998;&#36776;&#29575;OD&#30697;&#38453;&#20013;&#31232;&#30095;&#21644;&#38271;&#23614;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#38590;&#20197;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#36825;&#23545;&#20110;&#20132;&#36890;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;&#31354;&#38388;-Tweedie&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STTD&#65289;&#12290;STTD&#23558;Tweedie&#20998;&#24067;&#20316;&#20026;&#20256;&#32479;&#30340;&#8220;&#38646;&#33192;&#32960;&#8221;&#27169;&#22411;&#30340;&#26377;&#21147;&#26367;&#20195;&#21697;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#23884;&#20837;&#26469;&#21442;&#25968;&#21270;&#26053;&#34892;&#38656;&#27714;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;STTD&#22312;&#39640;&#20998;&#36776;&#29575;&#22330;&#26223;&#19979;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#31934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
crucial for transportation management. However, traditional spatial-temporal deep learning models grapple with addressing the sparse and long-tail characteristics in high-resolution O-D matrices and quantifying prediction uncertainty. This dilemma arises from the numerous zeros and over-dispersed demand patterns within these matrices, which challenge the Gaussian assumption inherent to deterministic deep learning models. To address these challenges, we propose a novel approach: the Spatial-Temporal Tweedie Graph Neural Network (STTD). The STTD introduces the Tweedie distribution as a compelling alternative to the traditional 'zero-inflated' model and leverages spatial and temporal embeddings to parameterize travel demand distributions. Our evaluations using real-world datasets highlight STTD's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23454;&#29992;&#30340;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#31639;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#19981;&#33021;&#22815;&#20840;&#31243;&#21521;&#26368;&#20248;&#28857;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2306.09850</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#31639;&#27861;&#19981;&#33021;&#20840;&#31243;&#21521;&#26368;&#20248;&#28857;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima. (arXiv:2306.09850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09850
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23454;&#29992;&#30340;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#31639;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#19981;&#33021;&#22815;&#20840;&#31243;&#21521;&#26368;&#20248;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;(SAM)&#26159;&#19968;&#31181;&#20248;&#21270;&#22120;&#65292;&#23427;&#22522;&#20110;&#24403;&#21069;&#28857;$x_t$&#30340;&#26799;&#24230;&#65292;&#22312;&#25200;&#21160;$y_t=x_t+\rho\frac{\nabla f(x_t)}{\lVert\nabla f(x_t)\rVert}$&#22788;&#36827;&#34892;&#19979;&#38477;&#12290;&#29616;&#26377;&#30740;&#31350;&#35777;&#26126;&#20102;SAM&#23545;&#20110;&#24179;&#28369;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#20551;&#35774;&#25200;&#21160;&#30340;&#22823;&#23567;$\rho$&#36880;&#28176;&#34928;&#20943;&#21644;/&#25110;&#22312;$y_t$&#20013;&#27809;&#26377;&#26799;&#24230;&#24402;&#19968;&#21270;&#65292;&#36825;&#19982;&#23454;&#36341;&#19981;&#31526;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#23454;&#29992;&#37197;&#32622;&#65288;&#21363;&#24120;&#25968;$\rho$&#21644;$y_t$&#20013;&#30340;&#26799;&#24230;&#24402;&#19968;&#21270;&#65289;&#30340;&#30830;&#23450;&#24615;/&#38543;&#26426;&#29256;&#26412;&#30340;SAM&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#65288;&#38750;&#65289;&#20984;&#24615;&#20551;&#35774;&#30340;&#24179;&#28369;&#20989;&#25968;&#19978;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;SAM&#22312;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#25110;&#31283;&#23450;&#28857;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#24179;&#28369;&#24378;&#20984;&#20989;&#25968;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30830;&#23450;&#24615;SAM&#20855;&#26377;&#20005;&#26684;&#30340;&#20840;&#23616;&#25910;&#25947;&#29575;&#20026;$\tilde\Theta(\frac{1}{T^2})$&#65292;&#32780;&#38543;&#26426;SAM&#30340;&#25910;&#25947;&#30028;&#21017;&#21463;&#21040;&#22122;&#22768;&#27700;&#24179;&#38477;&#20302;&#30340;&#24433;&#21709;&#65292;&#36825;&#34920;&#26126;&#20102;&#24179;&#38754;&#30446;&#26631;&#34920;&#38754;&#30340;&#23574;&#38160;&#24230;&#21644;&#24179;&#32531;&#24615;&#20043;&#38388;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step based on the gradient at a perturbation $y_t = x_t + \rho \frac{\nabla f(x_t)}{\lVert \nabla f(x_t) \rVert}$ of the current point $x_t$. Existing studies prove convergence of SAM for smooth functions, but they do so by assuming decaying perturbation size $\rho$ and/or no gradient normalization in $y_t$, which is detached from practice. To address this gap, we study deterministic/stochastic versions of SAM with practical configurations (i.e., constant $\rho$ and gradient normalization in $y_t$) and explore their convergence properties on smooth functions with (non)convexity assumptions. Perhaps surprisingly, in many scenarios, we find out that SAM has limited capability to converge to global minima or stationary points. For smooth strongly convex functions, we show that while deterministic SAM enjoys tight global convergence rates of $\tilde \Theta(\frac{1}{T^2})$, the convergence bound of stochastic SAM suffer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#23376;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#35299;&#37322;&#20026;&#26799;&#24230;&#19979;&#38477;&#30340;&#38543;&#26426;&#26494;&#24347;&#26041;&#27861;&#12290;&#27492;&#20248;&#21270;&#26041;&#27861;&#35777;&#26126;&#20102;&#38646;&#38454;&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#20302;&#25928;&#25110;&#19981;&#20855;&#22791;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20016;&#23500;&#31867;&#21035;&#30340;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#19979;&#20840;&#23616;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.09778</link><description>&lt;p&gt;
&#26799;&#24230;&#30495;&#30340;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#19968;&#20999;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Gradient is All You Need?. (arXiv:2306.09778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#23376;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#35299;&#37322;&#20026;&#26799;&#24230;&#19979;&#38477;&#30340;&#38543;&#26426;&#26494;&#24347;&#26041;&#27861;&#12290;&#27492;&#20248;&#21270;&#26041;&#27861;&#35777;&#26126;&#20102;&#38646;&#38454;&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#20302;&#25928;&#25110;&#19981;&#20855;&#22791;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20016;&#23500;&#31867;&#21035;&#30340;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#19979;&#20840;&#23616;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#23376;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#32467;&#21512;&#26799;&#24230;&#19979;&#38477;&#30475;&#20316;&#38543;&#26426;&#26494;&#24347;&#26041;&#27861;&#65292;&#26469;&#35299;&#37322;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#36890;&#36807;&#31890;&#23376;&#20043;&#38388;&#30340;&#36890;&#35759;&#65292;&#36825;&#31181;&#20248;&#21270;&#26041;&#27861;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#38646;&#38454;&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#20302;&#25928;&#25110;&#19981;&#20855;&#22791;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#20016;&#23500;&#31867;&#21035;&#19979;&#20840;&#23616;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide a novel analytical perspective on the theoretical understanding of gradient-based learning algorithms by interpreting consensus-based optimization (CBO), a recently proposed multi-particle derivative-free optimization method, as a stochastic relaxation of gradient descent. Remarkably, we observe that through communication of the particles, CBO exhibits a stochastic gradient descent (SGD)-like behavior despite solely relying on evaluations of the objective function. The fundamental value of such link between CBO and SGD lies in the fact that CBO is provably globally convergent to global minimizers for ample classes of nonsmooth and nonconvex objective functions, hence, on the one side, offering a novel explanation for the success of stochastic relaxations of gradient descent. On the other side, contrary to the conventional wisdom for which zero-order methods ought to be inefficient or not to possess generalization abilities, our results unveil an intrinsic gradi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;SNDEs&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24378;&#21046;&#20351;&#29992;&#20219;&#24847;&#27969;&#24418;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#31283;&#23450;&#39033;&#20351;&#32422;&#26463;&#27969;&#24418;&#25104;&#20026;&#28176;&#36827;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09739</link><description>&lt;p&gt;
&#23398;&#20064;&#21463;&#38480;&#21160;&#21147;&#23398;&#30340;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stabilized Neural Differential Equations for Learning Constrained Dynamics. (arXiv:2306.09739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;SNDEs&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24378;&#21046;&#20351;&#29992;&#20219;&#24847;&#27969;&#24418;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#31283;&#23450;&#39033;&#20351;&#32422;&#26463;&#27969;&#24418;&#25104;&#20026;&#28176;&#36827;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#20174;&#25968;&#25454;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#25512;&#26029;&#20986;&#30340;&#21160;&#24577;&#31995;&#32479;&#20445;&#30041;&#24050;&#30693;&#32422;&#26463;&#26465;&#20214;&#65288;&#20363;&#22914;&#23432;&#24658;&#23450;&#24459;&#25110;&#23545;&#20801;&#35768;&#30340;&#31995;&#32479;&#29366;&#24577;&#30340;&#38480;&#21046;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;SNDEs&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#24378;&#21046;&#20351;&#29992;&#20219;&#24847;&#27969;&#24418;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#31283;&#23450;&#39033;&#65292;&#24403;&#28155;&#21152;&#21040;&#21407;&#22987;&#21160;&#24577;&#31995;&#32479;&#20013;&#26102;&#65292;&#21487;&#20197;&#23558;&#32422;&#26463;&#27969;&#24418;&#25104;&#20026;&#28176;&#36827;&#31283;&#23450;&#30340;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#25152;&#26377;&#24120;&#35265;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#27169;&#22411;&#20860;&#23481;&#24182;&#24191;&#27867;&#36866;&#29992;&#12290;&#22312;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;SNDE&#22312;&#25193;&#23637;&#21487;&#32435;&#20837;NODE&#35757;&#32451;&#30340;&#32422;&#26463;&#31867;&#22411;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many successful methods to learn dynamical systems from data have recently been introduced. However, assuring that the inferred dynamics preserve known constraints, such as conservation laws or restrictions on the allowed system states, remains challenging. We propose stabilized neural differential equations (SNDEs), a method to enforce arbitrary manifold constraints for neural differential equations. Our approach is based on a stabilization term that, when added to the original dynamics, renders the constraint manifold provably asymptotically stable. Due to its simplicity, our method is compatible with all common neural ordinary differential equation (NODE) models and broadly applicable. In extensive empirical evaluations, we demonstrate that SNDEs outperform existing methods while extending the scope of which types of constraints can be incorporated into NODE training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#22238;&#31572;&#20102;Nesterov-1983&#21644;FISTA&#26159;&#21542;&#22312;&#24378;&#20984;&#20989;&#25968;&#19978;&#32447;&#24615;&#25910;&#25947;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#32447;&#24615;&#25910;&#25947;&#24615;&#19981;&#20381;&#36182;&#20110;&#24378;&#20984;&#24615;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2306.09694</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#20984;&#24615;&#30340; Nesterov-1983 &#30340;&#32447;&#24615;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Linear convergence of Nesterov-1983 with the strong convexity. (arXiv:2306.09694v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#22238;&#31572;&#20102;Nesterov-1983&#21644;FISTA&#26159;&#21542;&#22312;&#24378;&#20984;&#20989;&#25968;&#19978;&#32447;&#24615;&#25910;&#25947;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#32447;&#24615;&#25910;&#25947;&#24615;&#19981;&#20381;&#36182;&#20110;&#24378;&#20984;&#24615;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#20195;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;Nesterov &#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#27861;&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#37324;&#31243;&#30865;&#65292;&#35813;&#26041;&#27861;&#22312;[Nesterov&#65292;1983]&#20013;&#25552;&#20986;&#65292;&#31616;&#31216;&#20026;Nesterov-1983&#12290;&#27492;&#21518;&#65292;&#37325;&#35201;&#30340;&#36827;&#23637;&#20043;&#19968;&#26159;&#23427;&#30340;&#36817;&#31471;&#25512;&#24191;&#65292;&#21517;&#20026;&#24555;&#36895;&#36845;&#20195;&#25910;&#32553;&#38408;&#20540;&#31639;&#27861;&#65288;FISTA&#65289;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#31185;&#23398;&#21644;&#24037;&#31243;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#26410;&#30693;&#36947;Nesterov-1983&#21644;FISTA&#26159;&#21542;&#22312;&#24378;&#20984;&#20989;&#25968;&#19978;&#32447;&#24615;&#25910;&#25947;&#65292;&#32780;&#36825;&#24050;&#34987;&#21015;&#20026;&#32508;&#21512;&#35780;&#23457;[Chambolle&#21644;Pock&#65292;2016&#65292;&#38468;&#24405;B]&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#37319;&#29992;&#30340;&#30456;&#31354;&#38388;&#34920;&#31034;&#19968;&#36215;&#65292;&#26500;&#36896;Lyapunov&#20989;&#25968;&#30340;&#20851;&#38190;&#21306;&#21035;&#22312;&#20110;&#21160;&#33021;&#30340;&#31995;&#25968;&#38543;&#36845;&#20195;&#32780;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#65292;&#19978;&#36848;&#20004;&#31181;&#31639;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#27809;&#26377;&#20381;&#36182;&#20110;&#24378;&#20984;&#20989;&#25968;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
For modern gradient-based optimization, a developmental landmark is Nesterov's accelerated gradient descent method, which is proposed in [Nesterov, 1983], so shorten as Nesterov-1983. Afterward, one of the important progresses is its proximal generalization, named the fast iterative shrinkage-thresholding algorithm (FISTA), which is widely used in image science and engineering. However, it is unknown whether both Nesterov-1983 and FISTA converge linearly on the strongly convex function, which has been listed as the open problem in the comprehensive review [Chambolle and Pock, 2016, Appendix B]. In this paper, we answer this question by the use of the high-resolution differential equation framework. Along with the phase-space representation previously adopted, the key difference here in constructing the Lyapunov function is that the coefficient of the kinetic energy varies with the iteration. Furthermore, we point out that the linear convergence of both the two algorithms above has no d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#30340;&#32039;&#24615;&#29305;&#24449;&#26469;&#32416;&#27491;VAE&#27169;&#22411;&#20013;&#30340;&#29702;&#35770;&#32570;&#38519;&#24182;&#32553;&#23567;&#20869;&#28857;&#19982;&#31163;&#32676;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21516;&#26102;&#32467;&#21512;&#24314;&#27169;&#25216;&#26415;&#21644;&#32467;&#26500;&#30693;&#35782;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09646</link><description>&lt;p&gt;
&#29992;&#20110;&#21387;&#32553;&#28508;&#22312;&#34920;&#31034;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vacant Holes for Unsupervised Detection of the Outliers in Compact Latent Representation. (arXiv:2306.09646v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#30340;&#32039;&#24615;&#29305;&#24449;&#26469;&#32416;&#27491;VAE&#27169;&#22411;&#20013;&#30340;&#29702;&#35770;&#32570;&#38519;&#24182;&#32553;&#23567;&#20869;&#28857;&#19982;&#31163;&#32676;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21516;&#26102;&#32467;&#21512;&#24314;&#27169;&#25216;&#26415;&#21644;&#32467;&#26500;&#30693;&#35782;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#65292;&#23545;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#25805;&#20316;&#65292;&#26816;&#27979;&#24322;&#24120;&#20540;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32780;&#35328;&#65292;&#36825;&#19968;&#28857;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#20123;&#32593;&#32476;&#23545;&#20110;&#27492;&#31867;&#36755;&#20837;&#26174;&#31034;&#20986;&#36807;&#24230;&#33258;&#20449;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;&#20801;&#35768;&#20272;&#35745;&#36755;&#20837;&#27010;&#29575;&#23494;&#24230;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20063;&#38590;&#20197;&#23436;&#25104;&#27492;&#20219;&#21153;&#12290;&#26412;&#25991;&#20027;&#35201;&#38598;&#20013;&#20110;&#36825;&#31867;&#27169;&#22411;&#20013;&#30340;&#19968;&#31181;&#65306;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#32463;&#20856;VAE&#27169;&#22411;&#20551;&#35774;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#29702;&#35770;&#32570;&#38519;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#32039;&#24615;&#20316;&#20026;&#20174;&#28145;&#24230;&#31070;&#32463;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#30340;&#22270;&#20687;&#30340;&#25299;&#25169;&#29305;&#24449;&#26469;&#32416;&#27491;&#36825;&#19968;&#32570;&#38519;&#65292;&#24182;&#33719;&#24471;&#23558;&#22270;&#20687;&#21387;&#32553;&#22312;&#30830;&#23450;&#38480;&#21046;&#20869;&#30340;&#21487;&#35777;&#30028;&#38480;&#26469;&#21516;&#26102;&#21387;&#32553;&#20869;&#28857;&#21644;&#31163;&#32676;&#28857;&#30340;&#25163;&#27573;&#12290;&#25105;&#20204;&#37319;&#29992;&#20004;&#31181;&#26041;&#27861;&#23454;&#29616;&#32039;&#24615;&#65306;&#65288;i&#65289;&#20122;&#21382;&#23665;&#22823;&#22827;&#25193;&#23637;&#21644;&#65288;ii&#65289;&#23545;VAE&#32534;&#30721;&#22120;&#30340;&#26144;&#23556;&#36827;&#34892;&#22266;&#23450;&#30340;Lipschitz&#36830;&#32493;&#24615;&#24120;&#25968;&#12290;&#26368;&#21518;&#20294;&#20063;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21033;&#29992;&#24050;&#26377;&#24314;&#27169;&#25216;&#26415;&#21644;&#32467;&#26500;&#30693;&#35782;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of the outliers is pivotal for any machine learning model deployed and operated in real-world. It is essential for the Deep Neural Networks that were shown to be overconfident with such inputs. Moreover, even deep generative models that allow estimation of the probability density of the input fail in achieving this task. In this work, we concentrate on the specific type of these models: Variational Autoencoders (VAEs). First, we unveil a significant theoretical flaw in the assumption of the classical VAE model. Second, we enforce an accommodating topological property to the image of the deep neural mapping to the latent space: compactness to alleviate the flaw and obtain the means to provably bound the image within the determined limits by squeezing both inliers and outliers together. We enforce compactness using two approaches: (i) Alexandroff extension and (ii) fixed Lipschitz continuity constant on the mapping of the encoder of the VAEs. Finally and most importantly, we di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#38480;&#21046;&#30340;&#29627;&#23572;&#20857;&#26364;&#26426;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#22270;&#20687;&#24314;&#27169;&#20013;&#30340;&#21435;&#22122;&#21644;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#38480;&#21046;&#38544;&#34255;&#21333;&#20803;&#19982;&#21487;&#35265;&#21333;&#20803;&#30340;&#23376;&#38598;&#30340;&#36830;&#25509;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#24182;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09628</link><description>&lt;p&gt;
&#32467;&#26500;&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;&#29992;&#20110;&#22270;&#20687;&#21435;&#22122;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Structural Restricted Boltzmann Machine for image denoising and classification. (arXiv:2306.09628v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#38480;&#21046;&#30340;&#29627;&#23572;&#20857;&#26364;&#26426;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#22270;&#20687;&#24314;&#27169;&#20013;&#30340;&#21435;&#22122;&#21644;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#38480;&#21046;&#38544;&#34255;&#21333;&#20803;&#19982;&#21487;&#35265;&#21333;&#20803;&#30340;&#23376;&#38598;&#30340;&#36830;&#25509;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#24182;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;&#26159;&#29983;&#25104;&#27169;&#22411;&#65292;&#30001;&#19968;&#23618;&#38544;&#21464;&#37327;&#36830;&#25509;&#21040;&#21478;&#19968;&#23618;&#21487;&#35265;&#21333;&#20803;&#65292;&#29992;&#20110;&#24314;&#27169;&#21487;&#35265;&#21464;&#37327;&#30340;&#20998;&#24067;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#39640;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#36890;&#24120;&#20351;&#29992;&#35768;&#22810;&#38544;&#34255;&#21333;&#20803;&#65292;&#36825;&#19982;&#22823;&#37327;&#21487;&#35265;&#21333;&#20803;&#32467;&#21512;&#20351;&#29992;&#20250;&#23548;&#33268;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#24456;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32467;&#26500;&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#25163;&#22836;&#25968;&#25454;&#30340;&#32467;&#26500;&#65292;&#23558;&#38544;&#34255;&#21333;&#20803;&#30340;&#36830;&#25509;&#38480;&#21046;&#21040;&#21487;&#35265;&#21333;&#20803;&#30340;&#23376;&#38598;&#19978;&#65292;&#20197;&#26174;&#33879;&#38477;&#20302;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#20316;&#20026;&#21487;&#33021;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22270;&#20687;&#24314;&#27169;&#12290;&#22522;&#20110;&#22270;&#20687;&#30340;&#29305;&#24615;&#65292;&#36830;&#25509;&#30340;&#32467;&#26500;&#26159;&#36890;&#36807;&#22270;&#20687;&#20687;&#32032;&#19978;&#30340;&#31354;&#38388;&#37051;&#22495;&#32473;&#20986;&#30340;&#65292;&#36825;&#20123;&#20687;&#32032;&#26500;&#25104;&#27169;&#22411;&#30340;&#21487;&#35265;&#21464;&#37327;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restricted Boltzmann Machines are generative models that consist of a layer of hidden variables connected to another layer of visible units, and they are used to model the distribution over visible variables. In order to gain a higher representability power, many hidden units are commonly used, which, in combination with a large number of visible units, leads to a high number of trainable parameters. In this work we introduce the Structural Restricted Boltzmann Machine model, which taking advantage of the structure of the data in hand, constrains connections of hidden units to subsets of visible units in order to reduce significantly the number of trainable parameters, without compromising performance. As a possible area of application, we focus on image modelling. Based on the nature of the images, the structure of the connections is given in terms of spatial neighbourhoods over the pixels of the image that constitute the visible variables of the model. We conduct extensive experiment
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24130;&#24459;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#21807;&#19968;&#24179;&#31283;&#20998;&#24067;&#19988;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#36830;&#32493;&#21644;&#31163;&#25955;&#21270;&#24130;&#24459;&#21160;&#24577;&#30340;&#20986;&#29616;&#26102;&#38388;&#26469;&#25351;&#23548;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09624</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#24130;&#24459;&#21160;&#24577;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Power-law Dynamic arising from machine learning. (arXiv:2306.09624v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24130;&#24459;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#21807;&#19968;&#24179;&#31283;&#20998;&#24067;&#19988;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#36830;&#32493;&#21644;&#31163;&#25955;&#21270;&#24130;&#24459;&#21160;&#24577;&#30340;&#20986;&#29616;&#26102;&#38388;&#26469;&#25351;&#23548;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#36825;&#31181;&#26041;&#31243;&#36215;&#28304;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#30740;&#31350;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#24130;&#24459;&#21160;&#24577;&#65292;&#22240;&#20026;&#20854;&#24179;&#31283;&#20998;&#24067;&#19981;&#33021;&#20855;&#26377;&#20122;&#39640;&#26031;&#23614;&#37096;&#24182;&#26381;&#20174;&#24130;&#24459;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#23398;&#20064;&#29575;&#36275;&#22815;&#23567;&#65292;&#24130;&#24459;&#21160;&#24577;&#26159;&#36941;&#21382;&#30340;&#19988;&#20855;&#26377;&#21807;&#19968;&#30340;&#24179;&#31283;&#20998;&#24067;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#23427;&#30340;&#39318;&#27425;&#23384;&#22312;&#26102;&#38388;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#36830;&#32493;&#30340;&#24130;&#24459;&#21160;&#24577;&#21450;&#20854;&#31163;&#25955;&#21270;&#22312;&#36864;&#20986;&#26102;&#38388;&#19978;&#30340;&#24046;&#24322;&#12290;&#36825;&#31181;&#27604;&#36739;&#21487;&#20197;&#24110;&#21161;&#25351;&#23548;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a kind of new SDE that was arisen from the research on optimization in machine learning, we call it power-law dynamic because its stationary distribution cannot have sub-Gaussian tail and obeys power-law. We prove that the power-law dynamic is ergodic with unique stationary distribution, provided the learning rate is small enough. We investigate its first exist time. In particular, we compare the exit times of the (continuous) power-law dynamic and its discretization. The comparison can help guide machine learning algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#20026;&#19968;&#20010;&#32622;&#20449;&#38598;&#32780;&#38750;&#21333;&#19968;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#24182;&#21457;&#29616;&#65292;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#65292;&#20449;&#20219;&#38598;&#30340;&#20307;&#31215;&#26159;&#19968;&#31181;&#26377;&#24847;&#20041;&#30340;&#34913;&#37327;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#21017;&#27809;&#26377;&#36825;&#31181;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.09586</link><description>&lt;p&gt;
&#19968;&#20010;&#32622;&#20449;&#38598;&#30340;&#25968;&#37327;&#26159;&#21542;&#26159;&#19968;&#31181;&#34913;&#37327;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#22909;&#26041;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is the Volume of a Credal Set a Good Measure for Epistemic Uncertainty?. (arXiv:2306.09586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#20026;&#19968;&#20010;&#32622;&#20449;&#38598;&#32780;&#38750;&#21333;&#19968;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#24182;&#21457;&#29616;&#65292;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#65292;&#20449;&#20219;&#38598;&#30340;&#20307;&#31215;&#26159;&#19968;&#31181;&#26377;&#24847;&#20041;&#30340;&#34913;&#37327;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#21017;&#27809;&#26377;&#36825;&#31181;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20805;&#20998;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#21644;&#37327;&#21270;&#22312;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;&#20316;&#20026;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#25105;&#20204;&#32771;&#34385;&#20449;&#20219;&#38598;&#65288;&#19968;&#32452;&#27010;&#29575;&#20998;&#24067;&#30340;&#20984;&#38598;&#65289;&#12290;&#20449;&#20219;&#38598;&#30340;&#20960;&#20309;&#34920;&#31034;&#20316;&#20026;$d$&#32500;&#22810;&#38754;&#20307;&#24847;&#21619;&#30528;&#23545;&#65288;&#35748;&#30693;&#65289;&#19981;&#30830;&#23450;&#24615;&#30340;&#20960;&#20309;&#30452;&#35273;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20108;&#20803;&#20998;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#20449;&#20219;&#38598;&#30340;&#20960;&#20309;&#34920;&#31034;&#30340;&#20307;&#31215;&#26159;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#31181;&#26377;&#24847;&#20041;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#22312;&#22810;&#31867;&#20998;&#31867;&#26102;&#21017;&#19981;&#37027;&#20040;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#25351;&#23450;&#21644;&#20351;&#29992;&#27491;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26041;&#27861;&#20197;&#21450;&#24847;&#35782;&#21040;&#21487;&#33021;&#30340;&#39118;&#38505;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adequate uncertainty representation and quantification have become imperative in various scientific disciplines, especially in machine learning and artificial intelligence. As an alternative to representing uncertainty via one single probability measure, we consider credal sets (convex sets of probability measures). The geometric representation of credal sets as $d$-dimensional polytopes implies a geometric intuition about (epistemic) uncertainty. In this paper, we show that the volume of the geometric representation of a credal set is a meaningful measure of epistemic uncertainty in the case of binary classification, but less so for multi-class classification. Our theoretical findings highlight the crucial role of specifying and employing uncertainty measures in machine learning in an appropriate way, and for being aware of possible pitfalls.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#20960;&#20309;&#24418;&#29366;&#30340;&#25193;&#23637;&#20989;&#25968;&#21098;&#26525;&#35268;&#21017;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#22810;&#20010;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#27979;&#22810;&#20010;&#21464;&#28857;&#30340;&#38382;&#39064;&#65292;&#22312;&#23567;&#32500;&#24230;&#24773;&#20917;&#19979;&#21487;&#20197;&#27604;&#20989;&#25968;&#21098;&#26525;&#26356;&#24555;&#22320;&#20934;&#30830;&#26816;&#27979;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.09555</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#30340;&#35268;&#21017;&#22312;&#22810;&#20010;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#20013;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#30340;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Geometric-Based Pruning Rules For Change Point Detection in Multiple Independent Time Series. (arXiv:2306.09555v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09555
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#20960;&#20309;&#24418;&#29366;&#30340;&#25193;&#23637;&#20989;&#25968;&#21098;&#26525;&#35268;&#21017;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#22810;&#20010;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#27979;&#22810;&#20010;&#21464;&#28857;&#30340;&#38382;&#39064;&#65292;&#22312;&#23567;&#32500;&#24230;&#24773;&#20917;&#19979;&#21487;&#20197;&#27604;&#20989;&#25968;&#21098;&#26525;&#26356;&#24555;&#22320;&#20934;&#30830;&#26816;&#27979;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#26816;&#27979;&#22810;&#20010;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22810;&#20010;&#21464;&#28857;&#30340;&#38382;&#39064;&#12290;&#23547;&#25214;&#26368;&#20339;&#20998;&#21106;&#21487;&#20197;&#34920;&#36798;&#20026;&#22312;&#32473;&#23450;&#25104;&#26412;&#20989;&#25968;&#19978;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#12290;&#24403;&#21464;&#21270;&#27425;&#25968;&#19982;&#25968;&#25454;&#38271;&#24230;&#25104;&#27604;&#20363;&#26102;&#65292;PELT&#31639;&#27861;&#20013;&#32534;&#30721;&#30340;&#22522;&#20110;&#19981;&#31561;&#24335;&#30340;&#21098;&#26525;&#35268;&#21017;&#20250;&#23548;&#33268;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#21478;&#19968;&#31181;&#31216;&#20026;&#20989;&#25968;&#21098;&#26525;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#23545;&#20110;&#20998;&#26512;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#32780;&#35328;&#65292;&#26080;&#35770;&#21464;&#21270;&#27425;&#25968;&#22914;&#20309;&#65292;&#20854;&#26102;&#38388;&#22797;&#26434;&#24230;&#37117;&#25509;&#36817;&#20110;&#32447;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#20351;&#29992;&#31616;&#21333;&#20960;&#20309;&#24418;&#29366;&#65288;&#29699;&#20307;&#21644;&#36229;&#30697;&#24418;&#65289;&#30340;&#20989;&#25968;&#21098;&#26525;&#30340;&#25193;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#39640;&#26031;&#24773;&#20917;&#65292;&#20294;&#25105;&#20204;&#30340;&#19968;&#20123;&#35268;&#21017;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#25351;&#25968;&#26063;&#12290;&#22312;&#27169;&#25311;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#22522;&#20110;&#20960;&#20309;&#30340;&#21098;&#26525;&#35268;&#21017;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#23567;&#32500;&#24230;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#36229;&#30697;&#24418;&#21644;&#29699;&#20307;&#36827;&#34892;&#21098;&#26525;&#21487;&#20197;&#27604;&#20989;&#25968;&#21098;&#26525;&#26356;&#24555;&#36895;&#22320;&#20934;&#30830;&#26816;&#27979;&#26356;&#22909;&#12290;&#23545;&#20110;&#36739;&#22823;&#32500;&#24230;&#65292;&#36229;&#30697;&#24418;&#21464;&#24471;&#19981;&#37027;&#20040;&#39640;&#25928;&#65292;&#32780;&#29699;&#20307;&#20165;&#22312;&#39640;&#20449;&#22122;&#27604;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of detecting multiple changes in multiple independent time series. The search for the best segmentation can be expressed as a minimization problem over a given cost function. We focus on dynamic programming algorithms that solve this problem exactly. When the number of changes is proportional to data length, an inequality-based pruning rule encoded in the PELT algorithm leads to a linear time complexity. Another type of pruning, called functional pruning, gives a close-to-linear time complexity whatever the number of changes, but only for the analysis of univariate time series.  We propose a few extensions of functional pruning for multiple independent time series based on the use of simple geometric shapes (balls and hyperrectangles). We focus on the Gaussian case, but some of our rules can be easily extended to the exponential family. In a simulation study we compare the computational efficiency of different geometric-based pruning rules. We show that for smal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65292;&#21487;&#20197;&#24212;&#23545;&#37325;&#23614;&#20998;&#24067;&#19988;&#20445;&#35777;&#26377;&#38480;&#30340;&#20551;&#38451;&#24615;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09548</link><description>&lt;p&gt;
&#22312;&#32447;&#37325;&#23614;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Heavy-tailed Change-point detection. (arXiv:2306.09548v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65292;&#21487;&#20197;&#24212;&#23545;&#37325;&#23614;&#20998;&#24067;&#19988;&#20445;&#35777;&#26377;&#38480;&#30340;&#20551;&#38451;&#24615;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979; (OCPD) &#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#26679;&#26412;&#21487;&#33021;&#26159;&#37325;&#23614;&#20998;&#24067;&#65292;&#19968;&#20010;&#25509;&#19968;&#20010;&#22320;&#21576;&#29616;&#65292;&#24182;&#19988;&#24517;&#39035;&#23613;&#26089;&#26816;&#27979;&#21040;&#24213;&#23618;&#22343;&#20540;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35009;&#21098;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477; (SGD) &#30340;&#31639;&#27861;&#65292;&#21363;&#20351;&#25105;&#20204;&#20165;&#20551;&#23450;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#31532;&#20108;&#38454;&#30697;&#26377;&#30028;&#65292;&#35813;&#31639;&#27861;&#20063;&#33021;&#27491;&#24120;&#24037;&#20316;&#12290;&#25105;&#20204;&#27966;&#29983;&#20102;&#22312;&#25152;&#26377;&#20855;&#26377;&#26377;&#30028;&#31532;&#20108;&#30697;&#30340;&#20998;&#24067;&#26063;&#20013;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26377;&#38480;&#26679;&#26412;&#20551;&#38451;&#24615;&#29575; (FPR) &#30340;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#20445;&#35777;&#26377;&#38480;&#26679;&#26412; FPR &#30340; OCPD &#31639;&#27861;&#65292;&#21363;&#20351;&#25968;&#25454;&#26159;&#39640;&#32500;&#30340;&#65292;&#24213;&#23618;&#20998;&#24067;&#26159;&#37325;&#23614;&#30340;&#12290;&#25105;&#20204;&#35770;&#25991;&#30340;&#25216;&#26415;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#35009;&#21098; SGD &#21487;&#20197;&#20272;&#35745;&#38543;&#26426;&#21521;&#37327;&#30340;&#22343;&#20540;&#24182;&#21516;&#26102;&#22312;&#25152;&#26377;&#32622;&#20449;&#24230;&#20540;&#19978;&#25552;&#20379;&#32622;&#20449;&#24230;&#30028;&#38480;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#31283;&#20581;&#30340;&#20272;&#35745;&#19982;&#24182;&#38598;&#36793;&#30028;&#35770;&#35777;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#19968;&#20010;&#26377;&#38480;&#30340;&#39034;&#24207;&#21464;&#28857;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study algorithms for online change-point detection (OCPD), where samples that are potentially heavy-tailed, are presented one at a time and a change in the underlying mean must be detected as early as possible. We present an algorithm based on clipped Stochastic Gradient Descent (SGD), that works even if we only assume that the second moment of the data generating process is bounded. We derive guarantees on worst-case, finite-sample false-positive rate (FPR) over the family of all distributions with bounded second moment. Thus, our method is the first OCPD algorithm that guarantees finite-sample FPR, even if the data is high dimensional and the underlying distributions are heavy-tailed. The technical contribution of our paper is to show that clipped-SGD can estimate the mean of a random vector and simultaneously provide confidence bounds at all confidence values. We combine this robust estimate with a union bound argument and construct a sequential change-point algorithm with finite
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Caus-Modens&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#65292;&#30456;&#27604;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.09520</link><description>&lt;p&gt;
&#38024;&#23545;&#28508;&#22312;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#32467;&#26524;&#30340;&#26356;&#32039;&#23494;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Tighter Prediction Intervals for Causal Outcomes Under Hidden Confounding. (arXiv:2306.09520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Caus-Modens&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#65292;&#30456;&#27604;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30830;&#20999;&#20010;&#20307;&#27835;&#30103;&#32467;&#26524;&#30340;&#22240;&#26524;&#25512;&#26029;&#24456;&#23569;&#21487;&#33021;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25913;&#36827;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#32467;&#26524;&#21306;&#38388;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31867;&#26041;&#27861;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#65292;&#26377;&#26102;&#20250;&#32473;&#20986;&#26080;&#20449;&#24687;&#37327;&#30340;&#21306;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21478;&#31867;&#26041;&#27861;Caus-Modens&#65292;&#29992;&#20110;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#12290;&#21463;&#21040;&#36125;&#21494;&#26031;&#32479;&#35745;&#21644;&#38598;&#25104;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#21551;&#21457;&#65292;Caus-Modens&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#20998;&#31163;&#22522;&#20934;&#27979;&#35797;&#30340;&#24517;&#35201;&#21306;&#38388;&#22823;&#23567;&#26469;&#23454;&#29616;&#36275;&#22815;&#30340;&#35206;&#30422;&#29575;&#12290;&#26368;&#21518;&#19968;&#20010;&#22522;&#20934;&#26159;&#20351;&#29992;&#26410;&#30693;&#20294;&#21487;&#25506;&#26126;&#30340;&#22522;&#30784;&#20107;&#23454;&#24320;&#23637;&#35266;&#23519;&#23454;&#39564;&#30340;GPT-4&#30340;&#26032;&#22411;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference of exact individual treatment outcomes in the presence of hidden confounders is rarely possible. Instead, recent work has adapted conformal prediction to produce outcome intervals. Unfortunately this family of methods tends to be overly conservative, sometimes giving uninformative intervals. We introduce an alternative approach termed Caus-Modens, for characterizing causal outcome intervals by modulated ensembles. Motivated from Bayesian statistics and ensembled uncertainty quantification, Caus-Modens gives tighter outcome intervals in practice, measured by the necessary interval size to achieve sufficient coverage on three separate benchmarks. The last benchmark is a novel usage of GPT-4 for observational experiments with unknown but probeable ground truth.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#29992;&#20110;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#37327;&#23376;&#21487;&#20998;&#24615;&#30340;&#36817;&#20284;&#35299;&#65292;&#36890;&#36807;&#26377;&#25928;&#31639;&#27861;&#36817;&#20284;&#26597;&#25214;&#26368;&#36817;&#30340;&#21487;&#20998;&#31163;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#23558;&#37327;&#23376;&#21487;&#20998;&#24615;&#35270;&#20026;&#20998;&#31867;&#38382;&#39064;&#65292;&#23545;&#20219;&#20309;&#20108;&#32500;&#28151;&#21512;&#29366;&#24577;&#37117;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.09444</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#22797;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#37327;&#23376;&#21487;&#20998;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Quantum Separability Through a Reproducible Machine Learning Lens. (arXiv:2306.09444v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#29992;&#20110;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#37327;&#23376;&#21487;&#20998;&#24615;&#30340;&#36817;&#20284;&#35299;&#65292;&#36890;&#36807;&#26377;&#25928;&#31639;&#27861;&#36817;&#20284;&#26597;&#25214;&#26368;&#36817;&#30340;&#21487;&#20998;&#31163;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#23558;&#37327;&#23376;&#21487;&#20998;&#24615;&#35270;&#20026;&#20998;&#31867;&#38382;&#39064;&#65292;&#23545;&#20219;&#20309;&#20108;&#32500;&#28151;&#21512;&#29366;&#24577;&#37117;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21487;&#20998;&#24615;&#38382;&#39064;&#26159;&#25351;&#22914;&#20309;&#21028;&#26029;&#19968;&#20010;&#20108;&#20998;&#20307;&#23494;&#24230;&#30697;&#38453;&#26159;&#32416;&#32544;&#30340;&#36824;&#26159;&#21487;&#20998;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#25214;&#21040;&#27492;NP-&#38590;&#38382;&#39064;&#30340;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Frank-Wolfe&#30340;&#26377;&#25928;&#31639;&#27861;&#26469;&#36817;&#20284;&#26597;&#25214;&#26368;&#36817;&#30340;&#21487;&#20998;&#31163;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#25512;&#23548;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#23558;&#23494;&#24230;&#30697;&#38453;&#26631;&#35760;&#20026;&#21487;&#20998;&#31163;&#30340;&#25110;&#32416;&#32544;&#30340;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#37327;&#23376;&#21487;&#20998;&#24615;&#35270;&#20026;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#20108;&#32500;&#28151;&#21512;&#29366;&#24577;&#12290;&#23545;3-&#21644;7&#32500;&#24230;&#20013;&#30340;&#37327;&#23376;&#24577;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#31243;&#24207;&#30340;&#25928;&#29575;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#25193;&#23637;&#21040;&#19978;&#21315;&#20010;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#20855;&#26377;&#39640;&#37327;&#23376;&#32416;&#32544;&#26816;&#27979;&#31934;&#24230;&#12290;&#36825;&#19968;&#36827;&#23637;&#26377;&#21161;&#20110;&#22522;&#20934;&#27979;&#35797;&#37327;&#23376;&#21487;&#20998;&#24615;&#65292;&#24182;&#25903;&#25345;&#26356;&#24378;&#22823;&#30340;&#32416;&#32544;&#26816;&#27979;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quantum separability problem consists in deciding whether a bipartite density matrix is entangled or separable. In this work, we propose a machine learning pipeline for finding approximate solutions for this NP-hard problem in large-scale scenarios. We provide an efficient Frank-Wolfe-based algorithm to approximately seek the nearest separable density matrix and derive a systematic way for labeling density matrices as separable or entangled, allowing us to treat quantum separability as a classification problem. Our method is applicable to any two-qudit mixed states. Numerical experiments with quantum states of 3- and 7-dimensional qudits validate the efficiency of the proposed procedure, and demonstrate that it scales up to thousands of density matrices with a high quantum entanglement detection accuracy. This takes a step towards benchmarking quantum separability to support the development of more powerful entanglement detection techniques.
&lt;/p&gt;</description></item><item><title>&#35813;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#40065;&#26834;&#12289;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#22320;&#26816;&#27979;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.09441</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Anomaly Detection via Nonlinear Manifold Learning. (arXiv:2306.09441v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#40065;&#26834;&#12289;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#22320;&#26816;&#27979;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26679;&#26412;&#25351;&#30340;&#26159;&#19982;&#20854;&#20182;&#25968;&#25454;&#26174;&#33879;&#20559;&#31163;&#30340;&#26679;&#26412;&#65292;&#20854;&#26816;&#27979;&#22312;&#26500;&#24314;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#35201;&#20040;&#20165;&#36866;&#29992;&#20110;&#65288;&#21322;&#65289;&#30417;&#30563;&#35774;&#32622;&#65292;&#35201;&#20040;&#22312;&#27809;&#26377;&#24102;&#26631;&#35760;&#24322;&#24120;&#26679;&#26412;&#30340;&#26080;&#30417;&#30563;&#24212;&#29992;&#20013;&#34920;&#29616;&#24456;&#24046;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#30340;&#40065;&#26834;&#12289;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#26816;&#27979;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomalies are samples that significantly deviate from the rest of the data and their detection plays a major role in building machine learning models that can be reliably used in applications such as data-driven design and novelty detection. The majority of existing anomaly detection methods either are exclusively developed for (semi) supervised settings, or provide poor performance in unsupervised applications where there is no training data with labeled anomalous samples. To bridge this research gap, we introduce a robust, efficient, and interpretable methodology based on nonlinear manifold learning to detect anomalies in unsupervised settings. The essence of our approach is to learn a low-dimensional and interpretable latent representation (aka manifold) for all the data points such that normal samples are automatically clustered together and hence can be easily and robustly identified. We learn this low-dimensional manifold by designing a learning algorithm that leverages either a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#20114;&#32852;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#19981;&#20559;&#24494;&#20998;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#26080;&#20559;&#12289;&#20302;&#26041;&#24046;&#21644;&#33258;&#21160;&#30340;&#26041;&#27861;&#23545;&#22797;&#26434;&#23494;&#24230;&#36827;&#34892;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545; MH &#37319;&#26679;&#22120;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.07961</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#20559;&#24494;&#20998;&#23545;&#25239;&#22797;&#26434;&#23494;&#24230;&#29983;&#25104;&#65292;&#22522;&#20110;&#20114;&#32852;&#39532;&#23572;&#31185;&#22827;&#38142;&#19981;&#20559;&#24494;&#20998;&#20248;&#21270; MH &#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentiating Metropolis-Hastings to Optimize Intractable Densities. (arXiv:2306.07961v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#20114;&#32852;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#19981;&#20559;&#24494;&#20998;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#26080;&#20559;&#12289;&#20302;&#26041;&#24046;&#21644;&#33258;&#21160;&#30340;&#26041;&#27861;&#23545;&#22797;&#26434;&#23494;&#24230;&#36827;&#34892;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545; MH &#37319;&#26679;&#22120;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27010;&#29575;&#27169;&#22411;&#25512;&#29702;&#20013;&#65292;&#30446;&#26631;&#23494;&#24230;&#20989;&#25968;&#36890;&#24120;&#21464;&#24471;&#38590;&#20197;&#35745;&#31639;&#65292;&#38656;&#35201;&#20351;&#29992; Monte Carlo &#35745;&#31639;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#20559;&#24494;&#20998; Metropolis-Hastings &#37319;&#26679;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#27010;&#29575;&#25512;&#29702;&#26469;&#36827;&#34892;&#24494;&#20998;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#24494;&#20998;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982; Markov &#38142;&#32806;&#21512;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#20559;&#65292;&#20302;&#26041;&#24046;&#21644;&#33258;&#21160;&#30340;&#31243;&#24207;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#24212;&#29992;&#20110;&#30001;&#20110;&#32321;&#29712;&#30340;&#30446;&#26631;&#23494;&#24230;&#23548;&#33268;&#26399;&#26395;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#25214;&#21040;&#19968;&#20010;&#27169;&#26865;&#20004;&#21487;&#30340;&#35266;&#23519;&#21644;&#22312; Ising &#27169;&#22411;&#20013;&#26368;&#22823;&#21270;&#27604;&#28909;&#26469;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When performing inference on probabilistic models, target densities often become intractable, necessitating the use of Monte Carlo samplers. We develop a methodology for unbiased differentiation of the Metropolis-Hastings sampler, allowing us to differentiate through probabilistic inference. By fusing recent advances in stochastic differentiation with Markov chain coupling schemes, the procedure can be made unbiased, low-variance, and automatic. This allows us to apply gradient-based optimization to objectives expressed as expectations over intractable target densities. We demonstrate our approach by finding an ambiguous observation in a Gaussian mixture model and by maximizing the specific heat in an Ising model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#30740;&#31350;R-learner&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#20272;&#35745;&#22810;&#30740;&#31350;&#20013;&#30340;&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#24212;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#29616;&#23454;&#30284;&#30151;&#25968;&#25454;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#23567;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.01086</link><description>&lt;p&gt;
&#22810;&#30740;&#31350;R-learner&#29992;&#20110;&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-study R-learner for Heterogeneous Treatment Effect Estimation. (arXiv:2306.01086v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#30740;&#31350;R-learner&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#20272;&#35745;&#22810;&#30740;&#31350;&#20013;&#30340;&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#24212;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#29616;&#23454;&#30284;&#30151;&#25968;&#25454;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#23567;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31639;&#27861;&#31867;&#26469;&#20272;&#35745;&#22810;&#20010;&#30740;&#31350;&#20013;&#30340;&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#22810;&#30740;&#31350;R-learner&#65292;&#21487;&#20197;&#27010;&#25324;R-learner&#20197;&#32771;&#34385;&#30740;&#31350;&#38388;&#30340;&#24322;&#36136;&#24615;&#24182;&#23454;&#29616;&#35843;&#25972;&#28151;&#28102;&#30340;&#36328;&#30740;&#31350;&#40065;&#26834;&#24615;&#12290;&#22810;&#30740;&#31350;R-learner&#33021;&#22815;&#28789;&#27963;&#22320;&#34701;&#21512;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20197;&#20272;&#35745;&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#24212;&#12289;&#22256;&#25200;&#20989;&#25968;&#21644;&#25104;&#21592;&#27010;&#29575;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22810;&#30740;&#31350;R-learner&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#22312;&#24207;&#21015;&#20272;&#35745;&#26694;&#26550;&#20869;&#26159;&#28176;&#36817;&#27491;&#24120;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29616;&#23454;&#30284;&#30151;&#25968;&#25454;&#23454;&#39564;&#35777;&#26126;&#65292;&#38543;&#30528;&#30740;&#31350;&#38388;&#30340;&#24322;&#36136;&#24615;&#22686;&#21152;&#65292;&#19982;R-learner&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20272;&#35745;&#35823;&#24046;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a general class of algorithms for estimating heterogeneous treatment effects on multiple studies. Our approach, called the multi-study R-learner, generalizes the R-learner to account for between-study heterogeneity and achieves cross-study robustness of confounding adjustment. The multi-study R-learner is flexible in its ability to incorporate many machine learning techniques for estimating heterogeneous treatment effects, nuisance functions, and membership probabilities. We show that the multi-study R-learner treatment effect estimator is asymptotically normal within the series estimation framework. Moreover, we illustrate via realistic cancer data experiments that our approach results in lower estimation error than the R-learner as between-study heterogeneity increases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;ChemCrow&#65292;&#19968;&#31181;LLM&#21270;&#23398;&#20195;&#29702;&#65292;&#36890;&#36807;&#25972;&#21512;13&#20010;&#19987;&#23478;&#35774;&#35745;&#30340;&#24037;&#20855;&#20174;&#32780;&#22686;&#24378;LLM&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#23454;&#29616;&#33258;&#21160;&#21270;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05376</link><description>&lt;p&gt;
ChemCrow:&#29992;&#21270;&#23398;&#24037;&#20855;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChemCrow: Augmenting large-language models with chemistry tools. (arXiv:2304.05376v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;ChemCrow&#65292;&#19968;&#31181;LLM&#21270;&#23398;&#20195;&#29702;&#65292;&#36890;&#36807;&#25972;&#21512;13&#20010;&#19987;&#23478;&#35774;&#35745;&#30340;&#24037;&#20855;&#20174;&#32780;&#22686;&#24378;LLM&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#23454;&#29616;&#33258;&#21160;&#21270;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#36328;&#39046;&#22495;&#30340;&#20219;&#21153;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#20248;&#21183;&#65292;&#20294;&#22312;&#21270;&#23398;&#30456;&#20851;&#38382;&#39064;&#19978;&#21364;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#35775;&#38382;&#22806;&#37096;&#30693;&#35782;&#28304;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#31185;&#23398;&#24212;&#29992;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChemCrow&#65292;&#19968;&#31181;LLM&#21270;&#23398;&#20195;&#29702;&#65292;&#26088;&#22312;&#23436;&#25104;&#26377;&#26426;&#21512;&#25104;&#12289;&#33647;&#29289;&#21457;&#29616;&#21644;&#26448;&#26009;&#35774;&#35745;&#31561;&#20219;&#21153;&#12290;&#36890;&#36807;&#25972;&#21512;13&#20010;&#19987;&#23478;&#35774;&#35745;&#30340;&#24037;&#20855;&#65292;ChemCrow&#25552;&#39640;&#20102;LLM&#22312;&#21270;&#23398;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#20135;&#29983;&#20102;&#26032;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;LLM&#21644;&#20154;&#31867;&#19987;&#23478;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;ChemCrow&#22312;&#33258;&#21160;&#21270;&#21508;&#31181;&#21270;&#23398;&#20219;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-4&#20316;&#20026;&#35780;&#20272;&#22120;&#26080;&#27861;&#21306;&#20998;&#26126;&#26174;&#38169;&#35823;&#30340;GPT-4&#23436;&#25104;&#21644;GPT-4 + ChemCrow&#24615;&#33021;&#12290;&#36825;&#31181;&#24037;&#20855;&#30340;&#28389;&#29992;&#26377;&#24456;&#22823;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;&#22312;&#36127;&#36131;&#20219;&#30340;&#24773;&#20917;&#19979;&#65292;ChemCrow&#19981;&#20165;&#21487;&#20197;&#24110;&#21161;&#19987;&#19994;&#21270;&#23398;&#23478;&#24182;&#38477;&#20302;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-language models (LLMs) have recently shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 13 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our evaluation, including both LLM and expert human assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and GPT-4 + ChemCrow performance. There is a significant risk of misuse of tools like ChemCrow and we discuss their potential harms. Employed responsibly, ChemCrow not only aids expert chemists and lowers ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27963;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#29702;&#35299;&#25551;&#36848;&#20219;&#21153;&#30340;&#25991;&#26412;&#20449;&#24687;&#26469;&#36866;&#24212;&#25512;&#29702;&#26102;&#30340;&#26032;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#21644;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#38646;&#25968;&#25454;&#38382;&#39064;&#19978;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.03363</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#22686;&#24378;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#27963;&#24615;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language. (arXiv:2303.03363v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27963;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#29702;&#35299;&#25551;&#36848;&#20219;&#21153;&#30340;&#25991;&#26412;&#20449;&#24687;&#26469;&#36866;&#24212;&#25512;&#29702;&#26102;&#30340;&#26032;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#21644;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#38646;&#25968;&#25454;&#38382;&#39064;&#19978;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#24615;&#21644;&#24615;&#36136;&#39044;&#27979;&#27169;&#22411;&#26159;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#26680;&#24515;&#24037;&#20316;&#65292;&#20294;&#30446;&#21069;&#23427;&#20204;&#24517;&#39035;&#32463;&#36807;&#35757;&#32451;&#25110;&#24494;&#35843;&#25165;&#33021;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#31185;&#23398;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#38646;&#25968;&#25454;&#21644;&#23569;&#25968;&#25454;&#26679;&#26412;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#23545;&#20110;&#27492;&#31867;&#20302;&#25968;&#25454;&#20219;&#21153;&#65292;&#26080;&#38656;&#35757;&#32451;&#25110;&#24494;&#35843;&#21363;&#21487;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27963;&#24615;&#39044;&#27979;&#26041;&#38754;&#30340;&#39044;&#27979;&#36136;&#37327;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#27963;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#29702;&#35299;&#25551;&#36848;&#20219;&#21153;&#30340;&#25991;&#26412;&#20449;&#24687;&#26469;&#36866;&#24212;&#25512;&#29702;&#26102;&#30340;&#26032;&#39044;&#27979;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#21270;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#20998;&#31163;&#27169;&#22359;&#65292;&#20197;&#21450;&#22823;&#22411;&#29983;&#29289;&#21270;&#23398;&#25968;&#25454;&#24211;&#20013;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;CLAMP&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#21644;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#38646;&#25968;&#25454;&#38382;&#39064;&#19978;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36827;&#23637;&#24402;&#22240;&#20110;&#24773;&#22659;&#24863;&#30693;&#27169;&#22411;&#21644;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#30340;&#32467;&#21512;&#65292;&#20197;&#21450;&#29992;&#20110;&#32467;&#21512;&#25968;&#25454;&#28304;&#30340;&#23545;&#25239;&#24615;&#33258;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activity and property prediction models are the central workhorses in drug discovery and materials sciences, but currently they have to be trained or fine-tuned for new tasks. Without training or fine-tuning, scientific language models could be used for such low-data tasks through their announced zero- and few-shot capabilities. However, their predictive quality at activity prediction is lacking. In this work, we envision a novel type of activity prediction model that is able to adapt to new prediction tasks at inference time, via understanding textual information describing the task. To this end, we propose a new architecture with separate modules for chemical and natural language inputs, and a contrastive pre-training objective on data from large biochemical databases. In extensive experiments, we show that our method CLAMP yields improved predictive performance on few-shot learning benchmarks and zero-shot problems in drug discovery. We attribute the advances of our method to the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;2&#32500;&#27491;&#24358;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#65292;&#22312;&#25968;&#25454;&#23384;&#22312;&#24322;&#24120;&#20540;&#25110;&#37325;&#23614;&#22122;&#22768;&#26102;&#20855;&#26377;&#20248;&#36234;&#24615;&#24182;&#24471;&#21040;&#24378;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.03229</link><description>&lt;p&gt;
&#20851;&#20110;&#20004;&#32500;&#27491;&#24358;&#27169;&#22411;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Consistency and Asymptotic Normality of Least Absolute Deviation Estimators for 2-dimensional Sinusoidal Model. (arXiv:2301.03229v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;2&#32500;&#27491;&#24358;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#65292;&#22312;&#25968;&#25454;&#23384;&#22312;&#24322;&#24120;&#20540;&#25110;&#37325;&#23614;&#22122;&#22768;&#26102;&#20855;&#26377;&#20248;&#36234;&#24615;&#24182;&#24471;&#21040;&#24378;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;2&#32500;&#27491;&#24358;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046; (LAD) &#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#21442;&#25968;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#25239;&#24178;&#25200;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#23545;&#25968;&#25454;&#20013;&#23384;&#22312;&#24322;&#24120;&#20540;&#25110;&#37325;&#23614;&#22122;&#22768;&#31561;&#38750;&#40065;&#26834;&#20272;&#35745;&#25216;&#26415;&#26080;&#27861;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LAD&#20272;&#35745;&#22120;&#30340;&#37325;&#35201;&#28176;&#36817;&#24615;&#36136;&#65292;&#24182;&#35777;&#26126;&#20102;2&#32500;&#27491;&#24358;&#27169;&#22411;&#20449;&#21495;&#21442;&#25968;&#30340;LAD&#20272;&#35745;&#22120;&#20855;&#26377;&#24378;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;&#20351;&#29992;LAD&#20272;&#35745;&#22120;&#20248;&#20110;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#20248;&#21183;&#12290;&#23545;2&#32500;&#32441;&#29702;&#25968;&#25454;&#30340;&#25968;&#25454;&#20998;&#26512;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;LAD&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation of the parameters of a 2-dimensional sinusoidal model is a fundamental problem in digital signal processing and time series analysis. In this paper, we propose a robust least absolute deviation (LAD) estimators for parameter estimation. The proposed methodology provides a robust alternative to non-robust estimation techniques like the least squares estimators, in situations where outliers are present in the data or in the presence of heavy tailed noise. We study important asymptotic properties of the LAD estimators and establish the strong consistency and asymptotic normality of the LAD estimators of the signal parameters of a 2-dimensional sinusoidal model. We further illustrate the advantage of using LAD estimators over least squares estimators through extensive simulation studies. Data analysis of a 2-dimensional texture data indicates practical applicability of the proposed LAD approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#21453;&#38382;&#39064;&#20808;&#39564;&#30693;&#35782;&#12289;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#21253;&#25324;&#21327;&#26041;&#24046;&#20449;&#24687;&#22312;&#20869;&#30340;&#22270;&#20687;&#24182;&#25552;&#20379;&#26032;&#36317;&#31163;&#24230;&#37327;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#37319;&#26679;&#31232;&#30095;&#21644;&#26377;&#22122;&#22768;&#30340;MRI&#37325;&#24314;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37325;&#24314;&#24615;&#33021;&#24182;&#31361;&#20986;&#20102;&#35299;&#21078;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2210.14586</link><description>&lt;p&gt;
&#24102;&#26377;&#32467;&#26500;&#22270;&#20687;&#21327;&#26041;&#24046;&#30340;VAE&#27491;&#21017;&#21270;&#30340;&#21387;&#32553;&#24863;&#30693;MRI&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Compressed Sensing MRI Reconstruction Regularized by VAEs with Structured Image Covariance. (arXiv:2210.14586v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#21453;&#38382;&#39064;&#20808;&#39564;&#30693;&#35782;&#12289;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#21253;&#25324;&#21327;&#26041;&#24046;&#20449;&#24687;&#22312;&#20869;&#30340;&#22270;&#20687;&#24182;&#25552;&#20379;&#26032;&#36317;&#31163;&#24230;&#37327;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#37319;&#26679;&#31232;&#30095;&#21644;&#26377;&#22122;&#22768;&#30340;MRI&#37325;&#24314;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37325;&#24314;&#24615;&#33021;&#24182;&#31361;&#20986;&#20102;&#35299;&#21078;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22522;&#20110;&#30495;&#23454;&#22270;&#20687;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#21453;&#38382;&#39064;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#24809;&#32602;&#19982;&#29983;&#25104;&#22120;&#26080;&#27861;&#20135;&#29983;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#37325;&#24314;&#24046;&#24322;&#65292;&#26088;&#22312;&#23454;&#29616;&#23398;&#20064;&#30340;&#27491;&#21017;&#21270;&#20808;&#39564;&#36866;&#24212;&#20110;&#22797;&#26434;&#21453;&#38382;&#39064;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;&#21464;&#20998;&#27491;&#21017;&#26041;&#27861;&#30340;&#25511;&#21046;&#21644;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#29983;&#25104;&#19981;&#20165;&#21253;&#21547;&#22270;&#20687;&#20449;&#24687;&#65292;&#36824;&#21253;&#21547;&#27599;&#20010;&#22270;&#20687;&#30340;&#21327;&#26041;&#24046;&#20449;&#24687;&#12290;&#21327;&#26041;&#24046;&#21487;&#20197;&#24314;&#27169;&#22270;&#20687;&#20013;&#30340;&#32467;&#26500;&#21464;&#21270;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20174;&#23398;&#20064;&#22270;&#20687;&#27969;&#24418;&#20013;&#20135;&#29983;&#26032;&#36317;&#31163;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;MRI&#37325;&#24314;&#20219;&#21153;&#65292;&#23545;&#37319;&#26679;&#31232;&#30095;&#21644;&#26377;&#22122;&#22768;&#30340;k&#31354;&#38388;&#25968;&#25454;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37325;&#24314;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#25913;&#21892;&#20102;&#37325;&#24314;&#36136;&#37327;&#24182;&#31361;&#20986;&#20102;&#35299;&#21078;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23398;&#20064;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#30340;&#30495;&#23454;&#24863;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: This paper investigates how generative models, trained on ground-truth images, can be used \changes{as} priors for inverse problems, penalizing reconstructions far from images the generator can produce. The aim is that learned regularization will provide complex data-driven priors to inverse problems while still retaining the control and insight of a variational regularization method. Moreover, unsupervised learning, without paired training data, allows the learned regularizer to remain flexible to changes in the forward problem such as noise level, sampling pattern or coil sensitivities in MRI.  Approach: We utilize variational autoencoders (VAEs) that generate not only an image but also a covariance uncertainty matrix for each image. The covariance can model changing uncertainty dependencies caused by structure in the image, such as edges or objects, and provides a new distance metric from the manifold of learned images.  Main results: We evaluate these novel generative re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23384;&#22312;&#24378;&#20551;&#30456;&#20851;&#30340;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#23454;&#29616;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#30340;&#31639;&#27861; DCWP&#65292;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.05247</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Debiased Subnetworks with Contrastive Weight Pruning. (arXiv:2210.05247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23384;&#22312;&#24378;&#20551;&#30456;&#20851;&#30340;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#23454;&#29616;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#30340;&#31639;&#27861; DCWP&#65292;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23384;&#22312;&#20559;&#32622;&#24615;&#65292;&#23548;&#33268;&#25552;&#20379;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#32479;&#35745;&#35777;&#25454;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#22312;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#21151;&#33021;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#29616;&#26377;&#31639;&#27861;&#22312;&#25506;&#32034;&#20855;&#26377;&#24378;&#20551;&#30456;&#20851;&#24615;&#30340;&#26080;&#20559;&#23376;&#32593;&#32476;&#23384;&#22312;&#38480;&#21046;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#28982;&#21518;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;&#20559;&#24046;&#20914;&#31361;&#26679;&#26412;&#23545;&#32467;&#26500;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22522;&#20110;&#23398;&#20064;&#30340;&#65288;&#20266;&#65289;&#26080;&#20559;&#26679;&#26412;&#21644;&#36873;&#25321;&#24615;&#20559;&#24046;&#20914;&#31361;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#21435;&#20559;&#32622;&#23545;&#27604;&#21098;&#26525;&#65288;DCWP&#65289;&#31639;&#27861;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102; DCWP &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are often biased to spuriously correlated features that provide misleading statistical evidence that does not generalize. This raises an interesting question: ``Does an optimal unbiased functional subnetwork exist in a severely biased network? If so, how to extract such subnetwork?" While empirical evidence has been accumulated about the existence of such unbiased subnetworks, these observations are mainly based on the guidance of ground-truth unbiased samples. Thus, it is unexplored how to discover the optimal subnetworks with biased training datasets in practice. To address this, here we first present our theoretical insight that alerts potential limitations of existing algorithms in exploring unbiased subnetworks in the presence of strong spurious correlations. We then further elucidate the importance of bias-conflicting samples on structure learning. Motivated by these observations, we propose a Debiased Contrastive Weight Pruning (DCWP) algorithm, which probes unbi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#32479;&#35745;&#27867;&#20989;Gateaux&#23548;&#25968;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#30340;Gateaux&#23548;&#25968;&#20272;&#35745;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#22240;&#26524;&#25512;&#26029;&#21644;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#31561;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.13701</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26368;&#20248;&#21270;&#22240;&#26524;&#25512;&#26029;&#24433;&#21709;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Influence Functions for Optimization-Based Causal Inference. (arXiv:2208.13701v4 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#32479;&#35745;&#27867;&#20989;Gateaux&#23548;&#25968;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#30340;Gateaux&#23548;&#25968;&#20272;&#35745;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#22240;&#26524;&#25512;&#26029;&#21644;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#31561;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#32479;&#35745;&#27867;&#20989;Gateaux&#23548;&#25968;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#20986;&#29616;&#30340;&#27867;&#20989;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27010;&#29575;&#20998;&#24067;&#26410;&#30693;&#20294;&#38656;&#35201;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#20272;&#35745;&#30340;&#24773;&#20917;&#12290;&#36825;&#20123;&#20272;&#35745;&#20998;&#24067;&#24341;&#23548;&#20102;&#32463;&#39564;Gateaux&#23548;&#25968;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32463;&#39564;&#12289;&#25968;&#20540;&#21644;&#35299;&#26512;Gateaux&#23548;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20174;&#24178;&#39044;&#22343;&#20540;&#65288;&#24179;&#22343;&#28508;&#22312;&#32467;&#26524;&#65289;&#30340;&#26696;&#20363;&#20837;&#25163;&#65292;&#25105;&#20204;&#21246;&#21202;&#20102;&#26377;&#38480;&#24046;&#20998;&#21644;&#35299;&#26512;Gateaux&#23548;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#25200;&#21160;&#21644;&#24179;&#28369;&#30340;&#25968;&#20540;&#36924;&#36817;&#36895;&#29575;&#35201;&#27714;&#65292;&#20197;&#20445;&#25345;&#21333;&#27493;&#35843;&#25972;&#30340;&#32479;&#35745;&#20248;&#21183;&#65292;&#20363;&#22914;&#36895;&#29575;&#21452;&#37325;&#24378;&#20581;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26356;&#22797;&#26434;&#30340;&#27867;&#20989;&#65292;&#22914;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#12289;&#26080;&#38480;&#26102;&#27573;Markov&#20915;&#31574;&#20013;&#31574;&#30053;&#20248;&#21270;&#30340;&#32447;&#24615;&#35268;&#21010;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a constructive algorithm that approximates Gateaux derivatives for statistical functionals by finite differencing, with a focus on functionals that arise in  causal inference. We study the case where probability distributions are not known a priori but need to be estimated from data. These estimated distributions lead to empirical Gateaux derivatives, and we study the relationships between empirical, numerical, and analytical Gateaux derivatives. Starting with a case study of the interventional mean (average potential outcome), we delineate the relationship between finite differences and the analytical Gateaux derivative. We then derive requirements on the rates of numerical approximation in perturbation and smoothing that preserve the statistical benefits of one-step adjustments, such as rate double robustness. We then study more complicated functionals such as dynamic treatment regimes, the linear-programming formulation for policy optimization in infinite-horizon Markov dec
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#38382;&#39064;&#24341;&#36215;&#20102;&#23545;&#20854;&#21463;&#35797;&#32773;&#36523;&#20221;&#30340;&#20105;&#35758;&#19982;&#30417;&#31649;&#21512;&#35268;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#35813;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#30417;&#31649;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2206.04039</link><description>&lt;p&gt;
&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20247;&#21253;&#24037;&#20316;&#32773;&#30340;&#20154;&#31867;&#21463;&#35797;&#32773;&#36523;&#20221;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04039
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#38382;&#39064;&#24341;&#36215;&#20102;&#23545;&#20854;&#21463;&#35797;&#32773;&#36523;&#20221;&#30340;&#20105;&#35758;&#19982;&#30417;&#31649;&#21512;&#35268;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#35813;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#30417;&#31649;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#22312;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#35299;&#20915;&#38656;&#35201;&#20154;&#31867;&#20132;&#20114;&#25110;&#21028;&#26029;&#30340;&#30740;&#31350;&#38382;&#39064;&#26041;&#38754;&#65292;&#24050;&#32463;&#20005;&#37325;&#20381;&#36182;&#20110;&#20247;&#21253;&#24037;&#20316;&#32773;&#12290;&#30001;&#20110;&#25191;&#34892;&#30340;&#20219;&#21153;&#22810;&#26679;&#21270;&#21644;&#25968;&#25454;&#29992;&#36884;&#30340;&#22810;&#26679;&#24615;&#65292;&#24456;&#38590;&#30830;&#23450;&#20309;&#26102;&#23558;&#20247;&#21253;&#24037;&#20316;&#32773;&#35270;&#20026;&#24037;&#20154;(&#32780;&#38750;&#20154;&#31867;&#21463;&#35797;&#32773;)&#12290;&#36825;&#20123;&#22256;&#38590;&#21152;&#21095;&#20102;&#25919;&#31574;&#30340;&#20914;&#31361;&#65292;&#19968;&#20123;&#26426;&#26500;&#21644;&#30740;&#31350;&#20154;&#21592;&#23558;&#25152;&#26377;ML&#20247;&#21253;&#24037;&#20316;&#32773;&#35270;&#20026;&#20154;&#31867;&#21463;&#35797;&#32773;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#35748;&#20026;&#23427;&#20204;&#24456;&#23569;&#26500;&#25104;&#20154;&#31867;&#21463;&#35797;&#32773;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21253;&#25324;&#20247;&#21253;&#24037;&#20316;&#30340;&#40092;&#26377;ML&#35770;&#25991;&#25552;&#21040;IRB&#30340;&#30417;&#30563;&#65292;&#24341;&#21457;&#20102;&#36829;&#21453;&#36947;&#24503;&#21644;&#27861;&#35268;&#35201;&#27714;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;ML&#20247;&#21253;&#30740;&#31350;&#30340;&#36866;&#24403;&#21010;&#23450;&#65292;&#24182;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#26292;&#38706;&#20986;&#30340;&#29420;&#29305;&#30740;&#31350;&#30417;&#30563;&#25361;&#25112;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#32654;&#22269;&#20844;&#20849;&#35268;&#21017;&#19979;&#65292;&#36825;&#20123;&#21028;&#26029;&#21462;&#20915;&#20110;&#20851;&#20110;&#38382;&#39064;&#30340;&#30830;&#23450;&#65292;&#28041;&#21450;&#35841;(&#25110;&#20160;&#20040;)&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, machine learning (ML) has relied heavily on crowdworkers both for building datasets and for addressing research questions requiring human interaction or judgment. The diverse tasks performed and uses of the data produced render it difficult to determine when crowdworkers are best thought of as workers (versus human subjects). These difficulties are compounded by conflicting policies, with some institutions and researchers regarding all ML crowdworkers as human subjects and others holding that they rarely constitute human subjects. Notably few ML papers involving crowdwork mention IRB oversight, raising the prospect of non-compliance with ethical and regulatory requirements. We investigate the appropriate designation of ML crowdsourcing studies, focusing our inquiry on natural language processing to expose unique challenges for research oversight. Crucially, under the U.S. Common Rule, these judgments hinge on determinations of aboutness, concerning both whom (or what) 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#22312;&#27169;&#22411;&#38169;&#35823;&#19979;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#20272;&#35745;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.06818</link><description>&lt;p&gt;
&#21160;&#24577;&#27835;&#30103;&#25928;&#24212;&#65306;&#27169;&#22411;&#38169;&#35823;&#19979;&#30340;&#39640;&#32500;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Dynamic treatment effects: high-dimensional inference under model misspecification. (arXiv:2111.06818v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#22312;&#27169;&#22411;&#38169;&#35823;&#19979;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#20272;&#35745;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#21160;&#24577;&#27835;&#30103;&#25928;&#24212;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#24178;&#39044;&#30340;&#26102;&#21464;&#22240;&#26524;&#24433;&#21709;&#30340;&#24494;&#22937;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#8220;&#32500;&#25968;&#28798;&#38590;&#8221;&#21644;&#26102;&#21464;&#28151;&#26434;&#30340;&#23384;&#22312;&#65292;&#36825;&#31181;&#20272;&#35745;&#23384;&#22312;&#30528;&#25361;&#25112;&#65292;&#21487;&#33021;&#23548;&#33268;&#20272;&#35745;&#20559;&#35823;&#12290;&#27492;&#22806;&#65292;&#27491;&#30830;&#22320;&#35268;&#23450;&#26085;&#30410;&#22686;&#22810;&#30340;&#27835;&#30103;&#20998;&#37197;&#21644;&#22810;&#37325;&#26292;&#38706;&#30340;&#32467;&#26524;&#27169;&#22411;&#20284;&#20046;&#36807;&#20110;&#22797;&#26434;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#21452;&#37325;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#22312;&#20801;&#35768;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#65292;&#28982;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24182;&#27809;&#26377;&#23454;&#29616;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#23545;&#27835;&#30103;&#20998;&#37197;&#21644;&#32467;&#26524;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#24207;&#21015;&#27169;&#22411;&#21452;&#37325;&#40065;&#26834;&#24615;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35777;&#26126;&#20102;&#24403;&#27599;&#20010;&#26102;&#38388;&#26292;&#38706;&#37117;&#26159;&#21452;&#37325;&#40065;&#26834;&#24615;&#30340;&#26102;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#23454;&#29616;&#21452;&#37325;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#32500;&#29615;&#22659;&#19979;&#21160;&#24577;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating dynamic treatment effects is essential across various disciplines, offering nuanced insights into the time-dependent causal impact of interventions. However, this estimation presents challenges due to the "curse of dimensionality" and time-varying confounding, which can lead to biased estimates. Additionally, correctly specifying the growing number of treatment assignments and outcome models with multiple exposures seems overly complex. Given these challenges, the concept of double robustness, where model misspecification is permitted, is extremely valuable, yet unachieved in practical applications. This paper introduces a new approach by proposing novel, robust estimators for both treatment assignments and outcome models. We present a "sequential model double robust" solution, demonstrating that double robustness over multiple time points can be achieved when each time exposure is doubly robust. This approach improves the robustness and reliability of dynamic treatment effe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#30340;&#26159;&#25552;&#21319;&#23398;&#20064;&#22120;&#30340;&#26041;&#27861;&#65292;&#20851;&#27880;&#24369;&#23398;&#20064;&#22120;&#23646;&#20110;&#19968;&#20010;&#23481;&#37327;&#21463;&#38480;&#30340;&#31867;&#30340;&#20551;&#35774;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#38656;&#35201;&#22810;&#23569;&#20010;&#24369;&#23398;&#20064;&#22120;&#25165;&#33021;&#29983;&#25104;&#20934;&#30830;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21482;&#38656;&#35201;&#32422;$\tilde{O}({1}/{\gamma})$&#20010;&#24369;&#20551;&#35774;&#23601;&#33021;&#22815;&#35268;&#36991;&#32463;&#20856;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2001.11704</link><description>&lt;p&gt;
&#25552;&#21319;&#31616;&#21333;&#23398;&#20064;&#22120;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Simple Learners. (arXiv:2001.11704v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.11704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#30340;&#26159;&#25552;&#21319;&#23398;&#20064;&#22120;&#30340;&#26041;&#27861;&#65292;&#20851;&#27880;&#24369;&#23398;&#20064;&#22120;&#23646;&#20110;&#19968;&#20010;&#23481;&#37327;&#21463;&#38480;&#30340;&#31867;&#30340;&#20551;&#35774;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#38656;&#35201;&#22810;&#23569;&#20010;&#24369;&#23398;&#20064;&#22120;&#25165;&#33021;&#29983;&#25104;&#20934;&#30830;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21482;&#38656;&#35201;&#32422;$\tilde{O}({1}/{\gamma})$&#20010;&#24369;&#20551;&#35774;&#23601;&#33021;&#22815;&#35268;&#36991;&#32463;&#20856;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#26159;&#19968;&#31181;&#33879;&#21517;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#32467;&#21512;&#24369;&#19988;&#20855;&#26377;&#36866;&#24230;&#38169;&#35823;&#30340;&#20551;&#35774;&#65292;&#20197;&#29983;&#25104;&#24378;&#19988;&#20934;&#30830;&#30340;&#20551;&#35774;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#19968;&#31181;&#20551;&#35774;&#65306;&#24369;&#23398;&#20064;&#22120;&#23646;&#20110;&#19968;&#20010;&#26377;&#36793;&#30028;&#23481;&#37327;&#30340;&#31867;&#12290;&#35813;&#20551;&#35774;&#26469;&#33258;&#20110;&#24120;&#35265;&#30340;&#20256;&#32479;&#20570;&#27861;&#65292;&#21363;&#23558;&#24369;&#23398;&#20064;&#22120;&#35270;&#20026;&#8220;&#35268;&#21017;&#8221;&#25110;&#8220;&#22522;&#30784;&#31867;&#8221;&#20013;&#30340;&#8220;&#32463;&#39564;&#27861;&#21017;&#8221;&#65288;Schapire&#21644;Freund'12&#12289;Shalev-Shwartz&#21644;Ben-David'14&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#24369;&#23398;&#20064;&#22120;&#30340;VC&#32500;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;i&#65289;Oracle&#22797;&#26434;&#24230;&#65306;&#38656;&#35201;&#22810;&#23569;&#20010;&#24369;&#23398;&#20064;&#22120;&#25165;&#33021;&#29983;&#25104;&#20934;&#30830;&#30340;&#20551;&#35774;&#65311;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#21319;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#35268;&#36991;&#20102;Freund&#21644;Schapire&#30340;&#32463;&#20856;&#19979;&#30028;&#65288;'95&#65292;'12&#65289;&#12290;&#23613;&#31649;&#19979;&#30028;&#34920;&#26126;&#26377;&#26102;&#38656;&#35201;&#20855;&#26377;$\gamma$-&#38388;&#38553;&#30340;$\Omega({1}/{\gamma^2})$&#20010;&#24369;&#20551;&#35774;&#65292;&#20294;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#21482;&#38656;&#35201;&#25552;&#20379;&#32422;$\tilde{O}({1}/{\gamma})$&#20010;&#24369;&#20551;&#35774;&#65292;&#20551;&#35774;&#20854;
&lt;/p&gt;
&lt;p&gt;
Boosting is a celebrated machine learning approach which is based on the idea of combining weak and moderately inaccurate hypotheses to a strong and accurate one. We study boosting under the assumption that the weak hypotheses belong to a class of bounded capacity. This assumption is inspired by the common convention that weak hypotheses are "rules-of-thumbs" from an "easy-to-learn class". (Schapire and Freund~'12, Shalev-Shwartz and Ben-David '14.) Formally, we assume the class of weak hypotheses has a bounded VC dimension. We focus on two main questions: (i) Oracle Complexity: How many weak hypotheses are needed to produce an accurate hypothesis? We design a novel boosting algorithm and demonstrate that it circumvents a classical lower bound by Freund and Schapire ('95, '12). Whereas the lower bound shows that $\Omega({1}/{\gamma^2})$ weak hypotheses with $\gamma$-margin are sometimes necessary, our new method requires only $\tilde{O}({1}/{\gamma})$ weak hypothesis, provided that the
&lt;/p&gt;</description></item></channel></rss>