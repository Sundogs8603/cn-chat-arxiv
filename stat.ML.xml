<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.01335</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#21487;&#20197;&#23558;&#20854;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#32454;&#35843;&#65288;SFT&#65289;&#21033;&#29992;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#21147;&#37327;&#23545;&#20110;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#38656;&#35201;&#33719;&#21462;&#39069;&#22806;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#25104;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26032;&#30340;&#32454;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#32454;&#35843;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;SPIN&#30340;&#26680;&#24515;&#26159;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#20854;&#20013;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#23454;&#20363;&#23545;&#24328;&#26469;&#25552;&#21319;&#33258;&#24049;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#33258;&#24049;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20248;&#21270;&#33258;&#36523;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#33258;&#25105;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#26469;&#33258;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#22238;&#24212;&#26469;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20026;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20805;&#20998;&#21457;&#25496;&#20154;&#31867;&#26631;&#27880;&#31034;&#33539;&#25968;&#25454;&#22312;SFT&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#26159;&#21487;&#20197;&#36798;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#24555;&#36895;&#30340;&#31639;&#27861;&#65292;&#23558;&#38750;&#20809;&#28369;&#25439;&#22833;&#38382;&#39064;&#36716;&#21270;&#20026;&#24809;&#32602;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#19977;&#38454;&#27573;&#22122;&#22768;&#27880;&#20837;&#26469;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2401.01294</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#31232;&#30095;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046;&#22238;&#24402;&#19982;&#24046;&#20998;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Efficient Sparse Least Absolute Deviation Regression with Differential Privacy. (arXiv:2401.01294v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#24555;&#36895;&#30340;&#31639;&#27861;&#65292;&#23558;&#38750;&#20809;&#28369;&#25439;&#22833;&#38382;&#39064;&#36716;&#21270;&#20026;&#24809;&#32602;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#19977;&#38454;&#27573;&#22122;&#22768;&#27880;&#20837;&#26469;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22240;&#20854;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#30340;&#37325;&#35201;&#24212;&#29992;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#29486;&#20013;&#65292;&#22823;&#22810;&#25968;&#38544;&#31169;&#20445;&#25252;&#31639;&#27861;&#35201;&#27714;&#23398;&#20064;&#30446;&#26631;&#26159;&#24378;&#20984;&#19988;Lipschitz&#24179;&#28369;&#30340;&#65292;&#36825;&#19981;&#33021;&#28085;&#30422;&#24191;&#27867;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#65288;&#20363;&#22914;&#65292;&#20998;&#20301;&#25968;/&#26368;&#23567;&#32477;&#23545;&#25439;&#22833;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#31232;&#30095;&#40065;&#26834;&#22238;&#24402;&#38382;&#39064;&#24320;&#21457;&#19968;&#31181;&#24555;&#36895;&#30340;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23398;&#20064;&#25439;&#22833;&#21253;&#25324;&#19968;&#20010;&#40065;&#26834;&#30340;&#26368;&#23567;&#32477;&#23545;&#25439;&#22833;&#21644;&#19968;&#20010;l1&#31232;&#30095;&#24809;&#32602;&#39033;&#12290;&#20026;&#20102;&#22312;&#32473;&#23450;&#30340;&#38544;&#31169;&#39044;&#31639;&#19979;&#24555;&#36895;&#35299;&#20915;&#38750;&#20809;&#28369;&#25439;&#22833;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#40065;&#26834;&#21644;&#38544;&#31169;&#20445;&#25252;&#20272;&#35745;&#65288;FRAPPE&#65289;&#31639;&#27861;&#26469;&#36827;&#34892;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046;&#22238;&#24402;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#23558;&#31232;&#30095;LAD&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#24809;&#32602;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#19977;&#38454;&#27573;&#22122;&#22768;&#27880;&#20837;&#26469;&#20445;&#35777;&#65288;&#949;&#12289;&#948;&#65289;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, privacy-preserving machine learning algorithms have attracted increasing attention because of their important applications in many scientific fields. However, in the literature, most privacy-preserving algorithms demand learning objectives to be strongly convex and Lipschitz smooth, which thus cannot cover a wide class of robust loss functions (e.g., quantile/least absolute loss). In this work, we aim to develop a fast privacy-preserving learning solution for a sparse robust regression problem. Our learning loss consists of a robust least absolute loss and an $\ell_1$ sparse penalty term. To fast solve the non-smooth loss under a given privacy budget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE) algorithm for least absolute deviation regression. Our algorithm achieves a fast estimation by reformulating the sparse LAD problem as a penalized least square estimation problem and adopts a three-stage noise injection to guarantee the $(\epsilon,\delta)
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20108;&#36827;&#21046;&#20107;&#20214;&#32534;&#30721;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20855;&#26377;&#25512;&#26029;&#26412;&#22320;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01242</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#26681;&#26641;&#20013;&#23545;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#30340;&#20108;&#36827;&#21046;&#20107;&#20214;&#36827;&#34892;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Encoding Binary Events from Continuous Time Series in Rooted Trees using Contrastive Learning. (arXiv:2401.01242v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20108;&#36827;&#21046;&#20107;&#20214;&#32534;&#30721;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20855;&#26377;&#25512;&#26029;&#26412;&#22320;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#22495;&#22522;&#30784;&#35774;&#26045;&#25152;&#26377;&#32773;&#36890;&#24120;&#19981;&#30693;&#36947;&#20182;&#20204;&#30340;&#23458;&#25143;&#22312;&#26412;&#22320;&#32593;&#32476;&#20013;&#26159;&#22914;&#20309;&#36830;&#25509;&#30340;&#65292;&#36825;&#20123;&#32593;&#32476;&#20197;&#26681;&#26641;&#32467;&#26500;&#32452;&#32455;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#20351;&#29992;&#26469;&#33258;&#26641;&#21494;&#65288;&#23458;&#25143;&#65289;&#30340;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25512;&#26029;&#26412;&#22320;&#32593;&#32476;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20108;&#36827;&#21046;&#20107;&#20214;&#32534;&#30721;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#20316;&#20026;&#21021;&#27493;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#26377;&#20215;&#20540;&#30340;&#32534;&#30721;&#22120;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Broadband infrastructure owners do not always know how their customers are connected in the local networks, which are structured as rooted trees. A recent study is able to infer the topology of a local network using discrete time series data from the leaves of the tree (customers). In this study we propose a contrastive approach for learning a binary event encoder from continuous time series data. As a preliminary result, we show that our approach has some potential in learning a valuable encoder.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#33258;&#30001;&#21442;&#25968;&#35299;&#20915;&#20102;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#20551;&#35774;&#20135;&#29983;&#20102;&#26032;&#30340;&#24191;&#20041;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.01148</link><description>&lt;p&gt;
&#26080;&#30028;&#25439;&#22833;&#30340;PAC-Bayes-Chernoff&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayes-Chernoff bounds for unbounded losses. (arXiv:2401.01148v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01148
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#33258;&#30001;&#21442;&#25968;&#35299;&#20915;&#20102;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#20551;&#35774;&#20135;&#29983;&#20102;&#26032;&#30340;&#24191;&#20041;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#12290;&#36825;&#20010;&#32467;&#26524;&#21487;&#20197;&#29702;&#35299;&#20026;Chernoff&#30028;&#38480;&#30340;PAC-Bayes&#29256;&#26412;&#12290;&#35777;&#26126;&#25216;&#24039;&#20381;&#36182;&#20110;&#36890;&#36807;Cram&#233;r&#21464;&#25442;&#23545;&#25439;&#22833;&#36827;&#34892;&#32479;&#19968;&#36793;&#30028;&#30340;&#23614;&#37096;&#38543;&#26426;&#21464;&#37327;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#25105;&#20204;&#20027;&#35201;&#32467;&#26524;&#30340;&#20004;&#20010;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#35299;&#20915;&#20102;&#35768;&#22810;PAC-Bayes&#30028;&#38480;&#19978;&#30340;&#33258;&#30001;&#21442;&#25968;&#20248;&#21270;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#36827;&#34892;&#28789;&#27963;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#24191;&#20041;&#20102;&#20043;&#21069;&#30340;&#30028;&#38480;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#26469;&#33719;&#24471;&#31867;&#20284;Gibbs&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new high-probability PAC-Bayes oracle bound for unbounded losses. This result can be understood as a PAC-Bayes version of the Chernoff bound. The proof technique relies on uniformly bounding the tail of certain random variable based on the Cram\'er transform of the loss. We highlight two applications of our main result. First, we show that our bound solves the open problem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we show that our approach allows working with flexible assumptions on the loss function, resulting in novel bounds that generalize previous ones and can be minimized to obtain Gibbs-like posteriors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;PAC-Bayesian&#22810;&#35270;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#22270;&#24212;&#29992;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#22411;&#36317;&#31163;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.01048</link><description>&lt;p&gt;
PAC-Bayesian&#22810;&#35270;&#22270;&#23398;&#20064;&#39046;&#22495;&#33258;&#36866;&#24212;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Domain Adaptation Bounds for Multi-view learning. (arXiv:2401.01048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;PAC-Bayesian&#22810;&#35270;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#22270;&#24212;&#29992;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#22411;&#36317;&#31163;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#35270;&#22270;&#23398;&#20064;&#35774;&#32622;&#20013;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20851;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#32467;&#26524;&#12290;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#65292;&#24456;&#23569;&#20851;&#27880;&#23558;&#22810;&#20010;&#35270;&#22270;&#32435;&#20837;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Pac-Bayesian&#29702;&#35770;&#30340;&#27867;&#21270;&#30028;&#38480;&#20998;&#26512;&#65292;&#20197;&#25972;&#21512;&#30446;&#21069;&#20998;&#24320;&#22788;&#29702;&#30340;&#20004;&#31181;&#33539;&#24335;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26681;&#25454;Germain&#31561;&#20154;&#30340;&#20197;&#24448;&#30740;&#31350;&#65292;&#23558;&#20854;&#25552;&#20986;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#29992;&#20110;&#22810;&#35270;&#22270;&#23398;&#20064;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#35270;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#35774;&#32622;&#30340;&#26032;&#22411;&#36317;&#31163;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#29992;&#20110;&#20272;&#35745;&#24341;&#20837;&#30340;&#24046;&#24322;&#30340;Pac-Bayesian&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#19981;&#21516;&#30340;&#26032;&#30028;&#38480;&#19982;&#20197;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a series of new results for domain adaptation in the multi-view learning setting. The incorporation of multiple views in the domain adaptation was paid little attention in the previous studies. In this way, we propose an analysis of generalization bounds with Pac-Bayesian theory to consolidate the two paradigms, which are currently treated separately. Firstly, building on previous work by Germain et al., we adapt the distance between distribution proposed by Germain et al. for domain adaptation with the concept of multi-view learning. Thus, we introduce a novel distance that is tailored for the multi-view domain adaptation setting. Then, we give Pac-Bayesian bounds for estimating the introduced divergence. Finally, we compare the different new bounds with the previous studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;Tensor PCA&#27169;&#22411;&#20013;&#30340;&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#38480;&#21046;&#65292;&#24182;&#24314;&#31435;&#20102;&#20851;&#20110;&#25910;&#25947;&#27425;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#21644;&#31639;&#27861;&#38408;&#20540;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20572;&#27490;&#20934;&#21017;&#26469;&#33719;&#24471;&#39640;&#24230;&#30456;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.01047</link><description>&lt;p&gt;
Tensor PCA&#30340;&#21151;&#29575;&#36845;&#20195;&#30340;&#23574;&#38160;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sharp Analysis of Power Iteration for Tensor PCA. (arXiv:2401.01047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;Tensor PCA&#27169;&#22411;&#20013;&#30340;&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#38480;&#21046;&#65292;&#24182;&#24314;&#31435;&#20102;&#20851;&#20110;&#25910;&#25947;&#27425;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#21644;&#31639;&#27861;&#38408;&#20540;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20572;&#27490;&#20934;&#21017;&#26469;&#33719;&#24471;&#39640;&#24230;&#30456;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;Richard&#21644;Montanari&#65288;2014&#65289;&#24341;&#20837;&#30340;Tensor PCA&#27169;&#22411;&#30340;&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#12290;&#20043;&#21069;&#30740;&#31350;Tensor&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#30340;&#24037;&#20316;&#35201;&#20040;&#20165;&#38480;&#20110;&#22266;&#23450;&#27425;&#25968;&#30340;&#36845;&#20195;&#65292;&#35201;&#20040;&#38656;&#35201;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#19982;&#25968;&#25454;&#26080;&#20851;&#30340;&#21021;&#22987;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#23545;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;Tensor&#21151;&#29575;&#36845;&#20195;&#30340;&#21160;&#24577;&#36827;&#34892;&#20102;&#22810;&#39033;&#24335;&#25968;&#37327;&#32423;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#20110;&#24191;&#27867;&#30340;&#20449;&#22122;&#27604;&#33539;&#22260;&#19979;&#65292;&#21151;&#29575;&#36845;&#20195;&#25910;&#25947;&#21040;&#31181;&#26893;&#20449;&#21495;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#23454;&#38469;&#30340;&#31639;&#27861;&#38408;&#20540;&#27604;&#25991;&#29486;&#20013;&#29468;&#27979;&#30340;&#35201;&#23567;&#19968;&#20010;polylog(n)&#30340;&#22240;&#23376;&#65292;&#20854;&#20013;n&#26159;&#29615;&#22659;&#32500;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21151;&#29575;&#36845;&#20195;&#20572;&#27490;&#20934;&#21017;&#65292;&#21487;&#20197;&#20445;&#35777;&#36755;&#20986;&#19982;&#30495;&#23454;&#20449;&#21495;&#39640;&#24230;&#30456;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the power iteration algorithm for the tensor PCA model introduced in Richard and Montanari (2014). Previous work studying the properties of tensor power iteration is either limited to a constant number of iterations, or requires a non-trivial data-independent initialization. In this paper, we move beyond these limitations and analyze the dynamics of randomly initialized tensor power iteration up to polynomially many steps. Our contributions are threefold: First, we establish sharp bounds on the number of iterations required for power method to converge to the planted signal, for a broad range of the signal-to-noise ratios. Second, our analysis reveals that the actual algorithmic threshold for power iteration is smaller than the one conjectured in literature by a polylog(n) factor, where n is the ambient dimension. Finally, we propose a simple and effective stopping criterion for power iteration, which provably outputs a solution that is highly correlated with the true si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#36716;&#20272;&#35745;&#26041;&#31243;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#23558;&#20174;&#20272;&#35745;&#28508;&#22312;&#32467;&#26524;&#22343;&#20540;&#30340;&#22240;&#26524;&#25512;&#26029;&#35299;&#20915;&#26041;&#26696;&#25512;&#24191;&#21040;&#20854;&#20998;&#20301;&#25968;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#28508;&#22312;&#32467;&#26524;&#22343;&#20540;&#21644;&#20998;&#20301;&#25968;&#30340;&#26377;&#25928;&#24433;&#21709;&#20989;&#25968;&#30340;&#19968;&#33324;&#26500;&#36896;&#21644;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.00987</link><description>&lt;p&gt;
&#21453;&#36716;&#20272;&#35745;&#26041;&#31243;&#23545;&#28508;&#22312;&#32467;&#26524;&#20998;&#20301;&#25968;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inverting estimating equations for causal inference on quantiles. (arXiv:2401.00987v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#36716;&#20272;&#35745;&#26041;&#31243;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#23558;&#20174;&#20272;&#35745;&#28508;&#22312;&#32467;&#26524;&#22343;&#20540;&#30340;&#22240;&#26524;&#25512;&#26029;&#35299;&#20915;&#26041;&#26696;&#25512;&#24191;&#21040;&#20854;&#20998;&#20301;&#25968;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#28508;&#22312;&#32467;&#26524;&#22343;&#20540;&#21644;&#20998;&#20301;&#25968;&#30340;&#26377;&#25928;&#24433;&#21709;&#20989;&#25968;&#30340;&#19968;&#33324;&#26500;&#36896;&#21644;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#25991;&#29486;&#32463;&#24120;&#20851;&#27880;&#28508;&#22312;&#32467;&#26524;&#30340;&#22343;&#20540;&#20272;&#35745;&#65292;&#32780;&#28508;&#22312;&#32467;&#26524;&#30340;&#20998;&#20301;&#25968;&#21487;&#33021;&#21253;&#21547;&#37325;&#35201;&#30340;&#39069;&#22806;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#36716;&#20272;&#35745;&#26041;&#31243;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#23558;&#20174;&#20272;&#35745;&#28508;&#22312;&#32467;&#26524;&#22343;&#20540;&#30340;&#24191;&#27867;&#31867;&#21035;&#30340;&#22240;&#26524;&#25512;&#26029;&#35299;&#20915;&#26041;&#26696;&#25512;&#24191;&#21040;&#20854;&#20998;&#20301;&#25968;&#12290;&#25105;&#20204;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#21487;&#29992;&#26469;&#30830;&#23450;&#22522;&#20110;&#38408;&#20540;&#21464;&#25442;&#30340;&#28508;&#22312;&#32467;&#26524;&#22343;&#20540;&#30340;&#30830;&#23450;&#30697;&#20989;&#25968;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#28508;&#22312;&#32467;&#26524;&#20998;&#20301;&#25968;&#30340;&#20272;&#35745;&#26041;&#31243;&#30340;&#20415;&#21033;&#26500;&#36896;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#28508;&#22312;&#32467;&#26524;&#22343;&#20540;&#21644;&#20998;&#20301;&#25968;&#30340;&#26377;&#25928;&#24433;&#21709;&#20989;&#25968;&#30340;&#19968;&#33324;&#26500;&#36896;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#26377;&#25928;&#24433;&#21709;&#20989;&#25968;&#25512;&#23548;&#20986;&#20998;&#20301;&#25968;&#30446;&#26631;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#22312;&#20351;&#29992;&#21442;&#25968;&#27169;&#22411;&#25110;&#25968;&#25454;&#33258;&#36866;&#24212;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26102;&#24320;&#21457;&#20854;&#28176;&#36817;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The causal inference literature frequently focuses on estimating the mean of the potential outcome, whereas the quantiles of the potential outcome may carry important additional information. We propose a universal approach, based on the inverse estimating equations, to generalize a wide class of causal inference solutions from estimating the mean of the potential outcome to its quantiles. We assume that an identifying moment function is available to identify the mean of the threshold-transformed potential outcome, based on which a convenient construction of the estimating equation of quantiles of potential outcome is proposed. In addition, we also give a general construction of the efficient influence functions of the mean and quantiles of potential outcomes, and identify their connection. We motivate estimators for the quantile estimands with the efficient influence function, and develop their asymptotic properties when either parametric models or data-adaptive machine learners are us
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26368;&#20248;&#20256;&#36865;&#20013;&#20351;&#29992;&#24418;&#24335;&#20026;$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$&#30340;&#36153;&#29992;&#20989;&#25968;&#26102;&#30340;&#38646;&#21644;&#38750;&#36127;MTW&#24352;&#37327;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;MTW&#24352;&#37327;&#22312;&#38646;&#21521;&#37327;&#19978;&#20026;&#38646;&#30340;&#26465;&#20214;&#20197;&#21450;&#30456;&#24212;&#30340;&#32447;&#24615;ODE&#30340;&#31616;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#36870;&#20989;&#25968;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#20197;&#21450;&#19968;&#20123;&#20855;&#20307;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.00953</link><description>&lt;p&gt;
&#25317;&#26377;&#38646;&#21644;&#38750;&#36127;MTW&#24352;&#37327;&#30340;&#36153;&#29992;&#26063;&#22312;&#26368;&#20248;&#20256;&#36865;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Families of costs with zero and nonnegative MTW tensor in optimal transport. (arXiv:2401.00953v1 [math.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00953
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26368;&#20248;&#20256;&#36865;&#20013;&#20351;&#29992;&#24418;&#24335;&#20026;$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$&#30340;&#36153;&#29992;&#20989;&#25968;&#26102;&#30340;&#38646;&#21644;&#38750;&#36127;MTW&#24352;&#37327;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;MTW&#24352;&#37327;&#22312;&#38646;&#21521;&#37327;&#19978;&#20026;&#38646;&#30340;&#26465;&#20214;&#20197;&#21450;&#30456;&#24212;&#30340;&#32447;&#24615;ODE&#30340;&#31616;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#36870;&#20989;&#25968;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#20197;&#21450;&#19968;&#20123;&#20855;&#20307;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35745;&#31639;&#20102;&#22312;$\mathbb{R}^n$&#19978;&#20855;&#26377;&#24418;&#24335;$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$&#30340;&#36153;&#29992;&#20989;&#25968;&#30340;&#26368;&#20248;&#20256;&#36865;&#38382;&#39064;&#30340;MTW&#24352;&#37327;&#65288;&#25110;&#20132;&#21449;&#26354;&#29575;&#65289;&#12290;&#20854;&#20013;&#65292;$\mathsf{u}$&#26159;&#19968;&#20010;&#20855;&#26377;&#36870;&#20989;&#25968;$\mathsf{s}$&#30340;&#26631;&#37327;&#20989;&#25968;&#65292;$x^{\ft}y$&#26159;&#23646;&#20110;$\mathbb{R}^n$&#24320;&#23376;&#38598;&#30340;&#21521;&#37327;$x&#65292;y$&#30340;&#38750;&#36864;&#21270;&#21452;&#32447;&#24615;&#37197;&#23545;&#12290;MTW&#24352;&#37327;&#22312;Kim-McCann&#24230;&#37327;&#19979;&#23545;&#20110;&#38646;&#21521;&#37327;&#30340;&#26465;&#20214;&#26159;&#19968;&#20010;&#22235;&#38454;&#38750;&#32447;&#24615;ODE&#65292;&#21487;&#20197;&#34987;&#31616;&#21270;&#20026;&#20855;&#26377;&#24120;&#25968;&#31995;&#25968;$P$&#21644;$S$&#30340;&#24418;&#24335;&#20026;$\mathsf{s}^{(2)} - S\mathsf{s}^{(1)} + P\mathsf{s} = 0$&#30340;&#32447;&#24615;ODE&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#36870;&#20989;&#25968;&#21253;&#25324;Lambert&#21644;&#24191;&#20041;&#21453;&#21452;&#26354;/&#19977;&#35282;&#20989;&#25968;&#12290;&#24179;&#26041;&#27431;&#27663;&#24230;&#37327;&#21644;$\log$&#22411;&#36153;&#29992;&#26159;&#36825;&#20123;&#35299;&#30340;&#23454;&#20363;&#12290;&#36825;&#20010;&#23478;&#26063;&#30340;&#26368;&#20248;&#26144;&#23556;&#20063;&#26159;&#26174;&#24335;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We compute explicitly the MTW tensor (or cross curvature) for the optimal transport problem on $\mathbb{R}^n$ with a cost function of form $\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$, where $\mathsf{u}$ is a scalar function with inverse $\mathsf{s}$, $x^{\ft}y$ is a nondegenerate bilinear pairing of vectors $x, y$ belonging to an open subset of $\mathbb{R}^n$. The condition that the MTW-tensor vanishes on null vectors under the Kim-McCann metric is a fourth-order nonlinear ODE, which could be reduced to a linear ODE of the form $\mathsf{s}^{(2)} - S\mathsf{s}^{(1)} + P\mathsf{s} = 0$ with constant coefficients $P$ and $S$. The resulting inverse functions include {\it Lambert} and {\it generalized inverse hyperbolic\slash trigonometric} functions. The square Euclidean metric and $\log$-type costs are equivalent to instances of these solutions. The optimal map for the family is also explicit. For cost functions of a similar form on a hyperboloid model of the hyperbolic space and u
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#32452;&#22810;&#20803;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#19981;&#21516;&#20989;&#25968;&#21327;&#21464;&#37327;&#30340;&#28508;&#22312;&#21516;&#36136;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.01925</link><description>&lt;p&gt;
&#22312;&#22810;&#20803;&#20989;&#25968;&#22238;&#24402;&#20013;&#30340;&#31995;&#25968;&#24418;&#29366;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Coefficient Shape Alignment in Multivariate Functional Regression. (arXiv:2312.01925v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#32452;&#22810;&#20803;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#19981;&#21516;&#20989;&#25968;&#21327;&#21464;&#37327;&#30340;&#28508;&#22312;&#21516;&#36136;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20803;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#19981;&#21516;&#30340;&#20989;&#25968;&#21327;&#21464;&#37327;&#21487;&#33021;&#20855;&#26377;&#21516;&#36136;&#24615;&#12290;&#38544;&#34255;&#30340;&#21516;&#36136;&#24615;&#32467;&#26500;&#23545;&#20110;&#19981;&#21516;&#21327;&#21464;&#37327;&#30340;&#36830;&#25509;&#25110;&#20851;&#32852;&#20855;&#26377;&#20449;&#24687;&#20215;&#20540;&#12290;&#20855;&#26377;&#26126;&#26174;&#21516;&#36136;&#24615;&#30340;&#21327;&#21464;&#37327;&#21487;&#20197;&#22312;&#21516;&#19968;&#32676;&#32452;&#20013;&#36827;&#34892;&#32852;&#21512;&#20998;&#26512;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31181;&#31616;&#21270;&#24314;&#27169;&#22810;&#20803;&#20989;&#25968;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#32452;&#22810;&#20803;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#37319;&#29992;&#31216;&#20026;&#8220;&#31995;&#25968;&#24418;&#29366;&#23545;&#40784;&#8221;&#30340;&#26032;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#19981;&#21516;&#20989;&#25968;&#21327;&#21464;&#37327;&#30340;&#28508;&#22312;&#21516;&#36136;&#24615;&#38382;&#39064;&#12290;&#24314;&#27169;&#36807;&#31243;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#20351;&#29992;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26816;&#27979;&#26410;&#30693;&#20998;&#32452;&#32467;&#26500;&#65292;&#23558;&#21327;&#21464;&#37327;&#32858;&#21512;&#21040;&#19981;&#30456;&#20132;&#30340;&#32676;&#32452;&#20013;&#65307;&#28982;&#21518;&#65292;&#22522;&#20110;&#26816;&#27979;&#21040;&#30340;&#20998;&#32452;&#32467;&#26500;&#24314;&#31435;&#20998;&#32452;&#22810;&#20803;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26032;&#30340;&#20998;&#32452;&#27169;&#22411;&#20013;&#65292;&#21516;&#19968;&#21516;&#36136;&#32676;&#32452;&#20013;&#30340;&#31995;&#25968;&#20989;&#25968;&#24212;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multivariate functional data analysis, different functional covariates can be homogeneous. The hidden homogeneity structure is informative about the connectivity or association of different covariates. The covariates with pronounced homogeneity can be analyzed jointly within the same group, which gives rise to a way of parsimoniously modeling multivariate functional data. In this paper, a novel grouped multivariate functional regression model with a new regularization approach termed "coefficient shape alignment" is developed to tackle the potential homogeneity of different functional covariates. The modeling procedure includes two main steps: first detect the unknown grouping structure with the new regularization approach to aggregate covariates into disjoint groups; and then the grouped multivariate functional regression model is established based on the detected grouping structure. In this new grouped model, the coefficient functions of covariates in the same homogeneous group sh
&lt;/p&gt;</description></item><item><title>SASSL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.01187</link><description>&lt;p&gt;
SASSL:&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#22686;&#24378;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer. (arXiv:2312.01187v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01187
&lt;/p&gt;
&lt;p&gt;
SASSL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20381;&#36182;&#20110;&#25968;&#25454;&#22686;&#24378;&#26469;&#20174;&#26080;&#26631;&#31614;&#22270;&#20687;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#34920;&#24449;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22686;&#24378;&#27969;&#27700;&#32447;&#21253;&#25324;&#20102;&#21508;&#31181;&#21407;&#22987;&#30340;&#36716;&#25442;&#65292;&#20294;&#36890;&#24120;&#24573;&#30053;&#20102;&#33258;&#28982;&#22270;&#20687;&#30340;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#22686;&#24378;&#26679;&#26412;&#21487;&#33021;&#26174;&#31034;&#20986;&#36864;&#21270;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#20302;&#39118;&#26684;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#21040;&#33258;&#30417;&#30563;&#34920;&#24449;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SASSL&#30340;&#26032;&#22411;&#22686;&#24378;&#25216;&#26415;&#65292;&#23427;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#12290;&#35813;&#26041;&#27861;&#23558;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#35299;&#32806;&#65292;&#24182;&#20165;&#23545;&#39118;&#26684;&#24212;&#29992;&#36716;&#25442;&#65292;&#20445;&#25345;&#20869;&#23481;&#65292;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#26356;&#22909;&#22320;&#20445;&#30041;&#23427;&#20204;&#30340;&#35821;&#20041;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#24191;&#20026;&#25509;&#21463;&#30340;MoCo v2&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;ImageNet&#19978;&#30340;top-1&#20998;&#31867;&#24615;&#33021;&#25552;&#21319;&#36229;&#36807;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning relies heavily on data augmentation to extract meaningful representations from unlabeled images. While existing state-of-the-art augmentation pipelines incorporate a wide range of primitive transformations, these often disregard natural image structure. Thus, augmented samples can exhibit degraded semantic information and low stylistic diversity, affecting downstream performance of self-supervised representations. To overcome this, we propose SASSL: Style Augmentations for Self Supervised Learning, a novel augmentation technique based on Neural Style Transfer. The method decouples semantic and stylistic attributes in images and applies transformations exclusively to the style while preserving content, generating diverse augmented samples that better retain their semantic properties. Experimental results show our technique achieves a top-1 classification performance improvement of more than 2% on ImageNet compared to the well-established MoCo v2. We also measure
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#26631;&#20934;&#21270;&#27969;&#24341;&#20837;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#22791;&#26356;&#28789;&#27963;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.09222</link><description>&lt;p&gt;
&#21452;&#37325;&#26631;&#20934;&#21270;&#27969;&#65306;&#28789;&#27963;&#30340;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Double Normalizing Flows: Flexible Bayesian Gaussian Process ODEs Learning. (arXiv:2309.09222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09222
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#26631;&#20934;&#21270;&#27969;&#24341;&#20837;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#22791;&#26356;&#28789;&#27963;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39640;&#26031;&#36807;&#31243;&#34987;&#29992;&#26469;&#24314;&#27169;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#30340;&#21521;&#37327;&#22330;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#36125;&#21494;&#26031;&#25512;&#26029;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31561;&#20219;&#21153;&#65292;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#22312;&#20855;&#26377;&#38750;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#32422;&#26463;&#20808;&#39564;&#21644;&#22343;&#20540;&#22330;&#21518;&#39564;&#21487;&#33021;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#20934;&#21270;&#27969;&#26469;&#37325;&#26032;&#21442;&#25968;&#21270;ODE&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#26356;&#28789;&#27963;&#12289;&#26356;&#34920;&#36798;&#24615;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26631;&#20934;&#21270;&#27969;&#30340;&#35299;&#26512;&#21487;&#35745;&#31639;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;GP ODE&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#29983;&#25104;&#19968;&#20010;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#12290;&#36890;&#36807;&#36825;&#20123;&#26631;&#20934;&#21270;&#27969;&#30340;&#21452;&#37325;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#20013;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Gaussian processes have been utilized to model the vector field of continuous dynamical systems. Bayesian inference for such models \cite{hegde2022variational} has been extensively studied and has been applied in tasks such as time series prediction, providing uncertain estimates. However, previous Gaussian Process Ordinary Differential Equation (ODE) models may underperform on datasets with non-Gaussian process priors, as their constrained priors and mean-field posteriors may lack flexibility. To address this limitation, we incorporate normalizing flows to reparameterize the vector field of ODEs, resulting in a more flexible and expressive prior distribution. Additionally, due to the analytically tractable probability density functions of normalizing flows, we apply them to the posterior inference of GP ODEs, generating a non-Gaussian posterior. Through these dual applications of normalizing flows, our model improves accuracy and uncertainty estimates for Bayesian Gaussian P
&lt;/p&gt;</description></item><item><title>SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.04365</link><description>&lt;p&gt;
SLEM&#65306;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#36335;&#24452;&#24314;&#27169;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling. (arXiv:2308.04365v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04365
&lt;/p&gt;
&lt;p&gt;
SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#31185;&#23398;&#30340;&#20851;&#38190;&#30446;&#26631;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#24471;&#20986;&#20851;&#20110;&#23545;&#20551;&#23450;&#24178;&#39044;&#30340;&#39044;&#27979;&#30340;&#26377;&#24847;&#20041;&#30340;&#32467;&#35770;&#12290;&#36335;&#24452;&#27169;&#22411;&#12289;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEMs)&#20197;&#21450;&#26356;&#19968;&#33324;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#33021;&#22815;&#26126;&#30830;&#22320;&#25351;&#23450;&#20851;&#20110;&#29616;&#35937;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#20551;&#35774;&#12290;&#19982;DAGs&#19981;&#21516;&#65292;SEMs&#20551;&#35774;&#32447;&#24615;&#20851;&#31995;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20989;&#25968;&#38169;&#35823;&#35268;&#33539;&#65292;&#20174;&#32780;&#38459;&#30861;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#21487;&#38752;&#30340;&#25928;&#26524;&#22823;&#23567;&#20272;&#35745;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;&#65288;SLEM&#65289;&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#38598;&#25104;&#30340;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;SLEM&#33021;&#22815;&#25552;&#20379;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#22312;&#19982;SEMs&#36827;&#34892;&#32447;&#24615;&#27169;&#22411;&#27604;&#36739;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#20248;&#20110;SEMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#19968;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#24182;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#25512;&#23548;&#20986;&#26032;&#30340;&#35823;&#24046;&#30028;&#12290;&#20854;&#26041;&#27861;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.16164</link><description>&lt;p&gt;
&#22312;RKHS&#20013;&#33258;&#36866;&#24212;&#23398;&#20064;&#23494;&#24230;&#27604;&#29575;
&lt;/p&gt;
&lt;p&gt;
Adaptive learning of density ratios in RKHS. (arXiv:2307.16164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#19968;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#24182;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#25512;&#23548;&#20986;&#26032;&#30340;&#35823;&#24046;&#30028;&#12290;&#20854;&#26041;&#27861;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#23494;&#24230;&#35266;&#27979;&#20013;&#20272;&#35745;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#30340;&#27604;&#29575;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#24212;&#29992;&#21253;&#25324;&#21452;&#26679;&#26412;&#26816;&#39564;&#12289;&#20998;&#27495;&#20272;&#35745;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#21327;&#21464;&#37327;&#36716;&#31227;&#36866;&#24212;&#12289;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#21644;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#22823;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#20204;&#36890;&#36807;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#26368;&#23567;&#21270;&#30495;&#23454;&#23494;&#24230;&#27604;&#29575;&#19982;&#27169;&#22411;&#20043;&#38388;&#30340;&#27491;&#21017;Bregman&#36317;&#31163;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#26032;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;Lepskii&#31867;&#22411;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#22312;&#19981;&#30693;&#36947;&#23494;&#24230;&#27604;&#29575;&#30340;&#27491;&#21017;&#24615;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#35823;&#24046;&#30028;&#12290;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#20540;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the ratio of two probability densities from finitely many observations of the densities is a central problem in machine learning and statistics with applications in two-sample testing, divergence estimation, generative modeling, covariate shift adaptation, conditional density estimation, and novelty detection. In this work, we analyze a large class of density ratio estimation methods that minimize a regularized Bregman divergence between the true density ratio and a model in a reproducing kernel Hilbert space (RKHS). We derive new finite-sample error bounds, and we propose a Lepskii type parameter choice principle that minimizes the bounds without knowledge of the regularity of the density ratio. In the special case of quadratic loss, our method adaptively achieves a minimax optimal error rate. A numerical illustration is provided.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#8212;&#8212;&#19978;&#19979;&#25991;&#22871;&#32034;&#65292;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#35299;&#20915;&#35299;&#37322;&#24615;&#21644;&#25311;&#21512;&#33021;&#21147;&#30340;&#30683;&#30462;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#31232;&#30095;&#25311;&#21512;&#65292;&#24182;&#19988;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2302.00878</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#22871;&#32034;&#65306;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#23454;&#29616;&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The contextual lasso: Sparse linear models via deep neural networks. (arXiv:2302.00878v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#8212;&#8212;&#19978;&#19979;&#25991;&#22871;&#32034;&#65292;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#35299;&#20915;&#35299;&#37322;&#24615;&#21644;&#25311;&#21512;&#33021;&#21147;&#30340;&#30683;&#30462;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#31232;&#30095;&#25311;&#21512;&#65292;&#24182;&#19988;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#40644;&#37329;&#26631;&#20934;&#24037;&#20855;&#65292;&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#24378;&#22823;&#30340;&#25311;&#21512;&#33021;&#21147;&#12290;&#19978;&#19979;&#25991;&#22871;&#32034;&#26159;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#23427;&#23558;&#36755;&#20837;&#29305;&#24449;&#20998;&#25104;&#21487;&#35299;&#37322;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#20004;&#32452;&#65292;&#24182;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#36827;&#34892;&#31232;&#30095;&#25311;&#21512;&#65292;&#21516;&#26102;&#20854;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#36825;&#20010;&#36807;&#31243;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#38656;&#21442;&#25968;&#22320;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse linear models are a gold standard tool for interpretable machine learning, a field of emerging importance as predictive models permeate decision-making in many domains. Unfortunately, sparse linear models are far less flexible as functions of their input features than black-box models like deep neural networks. With this capability gap in mind, we study a not-uncommon situation where the input features dichotomize into two groups: explanatory features, which are candidates for inclusion as variables in an interpretable model, and contextual features, which select from the candidate variables and determine their effects. This dichotomy leads us to the contextual lasso, a new statistical estimator that fits a sparse linear model to the explanatory features such that the sparsity pattern and coefficients vary as a function of the contextual features. The fitting process learns this function nonparametrically via a deep neural network. To attain sparse coefficients, we train the net
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#38750;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20248;&#28857;&#26159;&#36991;&#20813;&#20102;&#22312;&#25972;&#20010;&#21487;&#34892;&#38598;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#19988;&#21033;&#29992;&#36895;&#24230;&#26469;&#34920;&#36798;&#32422;&#26463;&#65292;&#20351;&#24471;&#31639;&#27861;&#22312;&#20915;&#31574;&#21464;&#37327;&#25968;&#37327;&#21644;&#32422;&#26463;&#25968;&#37327;&#19978;&#30340;&#22797;&#26434;&#24230;&#22686;&#38271;&#36866;&#24230;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.00316</link><description>&lt;p&gt;
&#21152;&#36895;&#38750;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#19968;&#38454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accelerated First-Order Optimization under Nonlinear Constraints. (arXiv:2302.00316v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00316
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#38750;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20248;&#28857;&#26159;&#36991;&#20813;&#20102;&#22312;&#25972;&#20010;&#21487;&#34892;&#38598;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#19988;&#21033;&#29992;&#36895;&#24230;&#26469;&#34920;&#36798;&#32422;&#26463;&#65292;&#20351;&#24471;&#31639;&#27861;&#22312;&#20915;&#31574;&#21464;&#37327;&#25968;&#37327;&#21644;&#32422;&#26463;&#25968;&#37327;&#19978;&#30340;&#22797;&#26434;&#24230;&#22686;&#38271;&#36866;&#24230;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#32422;&#26463;&#20248;&#21270;&#21644;&#38750;&#20809;&#28369;&#21160;&#21147;&#31995;&#32479;&#20043;&#38388;&#30340;&#31867;&#27604;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#38750;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#12290;&#19982;Frank-Wolfe&#25110;&#25237;&#24433;&#26799;&#24230;&#19981;&#21516;&#65292;&#36825;&#20123;&#31639;&#27861;&#36991;&#20813;&#20102;&#27599;&#27425;&#36845;&#20195;&#22312;&#25972;&#20010;&#21487;&#34892;&#38598;&#19978;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25512;&#23548;&#20102;&#22312;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20013;&#30340;&#20984;&#35774;&#32622;&#30340;&#21152;&#36895;&#29575;&#12290;&#36825;&#20123;&#31639;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24615;&#26159;&#20351;&#29992;&#36895;&#24230;&#32780;&#19981;&#26159;&#20301;&#32622;&#26469;&#34920;&#36798;&#32422;&#26463;&#65292;&#36825;&#33258;&#28982;&#22320;&#23548;&#33268;&#21487;&#34892;&#38598;&#30340;&#31232;&#30095;&#12289;&#23616;&#37096;&#21644;&#20984;&#36817;&#20284;&#65288;&#21363;&#20351;&#21487;&#34892;&#38598;&#26159;&#38750;&#20984;&#30340;&#65289;&#12290;&#22240;&#27492;&#65292;&#22797;&#26434;&#24230;&#22312;&#20915;&#31574;&#21464;&#37327;&#25968;&#37327;&#21644;&#32422;&#26463;&#25968;&#37327;&#19978;&#36866;&#24230;&#22686;&#38271;&#65292;&#20351;&#24471;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#24212;&#29992;&#20110;&#21387;&#32553;&#24863;&#30693;&#21644;&#31232;&#30095;&#183;&#183;&#183;
&lt;/p&gt;
&lt;p&gt;
We exploit analogies between first-order algorithms for constrained optimization and non-smooth dynamical systems to design a new class of accelerated first-order algorithms for constrained optimization. Unlike Frank-Wolfe or projected gradients, these algorithms avoid optimization over the entire feasible set at each iteration. We prove convergence to stationary points even in a nonconvex setting and we derive accelerated rates for the convex setting both in continuous time, as well as in discrete time. An important property of these algorithms is that constraints are expressed in terms of velocities instead of positions, which naturally leads to sparse, local and convex approximations of the feasible set (even if the feasible set is nonconvex). Thus, the complexity tends to grow mildly in the number of decision variables and in the number of constraints, which makes the algorithms suitable for machine learning applications. We apply our algorithms to a compressed sensing and a sparse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#20869;&#23481;&#28508;&#21464;&#37327;&#20197;&#21450;&#21512;&#25104;&#32441;&#29702;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06950</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v5 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#20869;&#23481;&#28508;&#21464;&#37327;&#20197;&#21450;&#21512;&#25104;&#32441;&#29702;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#20248;&#21270;&#30340;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21464;&#25442;&#32534;&#30721;&#33539;&#24335;&#65292;&#23558;&#22270;&#20687;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#20449;&#24687;&#29109;&#32534;&#30721;&#65292;&#28982;&#21518;&#20877;&#26144;&#23556;&#22238;&#25968;&#25454;&#31354;&#38388;&#36827;&#34892;&#37325;&#26500;&#12290;&#19982;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#30340;&#31070;&#32463;&#21387;&#32553;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35299;&#30721;&#22120;&#26159;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#20869;&#23481;&#8221;&#28508;&#21464;&#37327;&#65292;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#20250;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#24182;&#21033;&#29992;&#35813;&#21464;&#37327;&#23384;&#20648;&#22270;&#20687;&#20449;&#24687;&#12290;&#20915;&#23450;&#25193;&#25955;&#36807;&#31243;&#30340;&#21097;&#20313;&#8220;&#32441;&#29702;&#8221;&#21464;&#37327;&#20250;&#22312;&#35299;&#30721;&#26102;&#21512;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#26681;&#25454;&#24863;&#30693;&#24230;&#37327;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#28041;&#21450;&#20102;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#36739;&#20110;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#22909;&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. The approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. In contrast to VAE-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. Our approach thus introduces an additional "content" latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. The remaining "texture" variables characterizing the diffusion process are synthesized at decoding time. We show that the model's performance can be tuned toward perceptual metrics of interest. Our extensive experiments involving multiple datasets and image quality assessment metrics show that our approach yields stronger reported FID scores than the GAN-based mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;Bandits&#20013;&#30340;&#25490;&#21517;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;UCB&#21644;Thompson Sampling&#31867;&#22411;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#24182;&#23545;&#20301;&#32622;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20102;&#24314;&#27169;&#12290;&#30740;&#31350;&#32467;&#26524;&#22312;&#20301;&#32622;&#20381;&#36182;&#24615;&#21644;&#25490;&#21517;&#38382;&#39064;&#19982;&#22270;&#35770;&#30340;&#36830;&#25509;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#25512;&#24191;&#12290;</title><link>http://arxiv.org/abs/2207.00109</link><description>&lt;p&gt;
&#24191;&#20041;&#32447;&#24615;Bandits&#20013;&#30340;&#25490;&#21517;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Ranking In Generalized Linear Bandits. (arXiv:2207.00109v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;Bandits&#20013;&#30340;&#25490;&#21517;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;UCB&#21644;Thompson Sampling&#31867;&#22411;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#24182;&#23545;&#20301;&#32622;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20102;&#24314;&#27169;&#12290;&#30740;&#31350;&#32467;&#26524;&#22312;&#20301;&#32622;&#20381;&#36182;&#24615;&#21644;&#25490;&#21517;&#38382;&#39064;&#19982;&#22270;&#35770;&#30340;&#36830;&#25509;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;Bandits&#20013;&#30340;&#25490;&#21517;&#38382;&#39064;&#12290;&#22312;&#27599;&#20010;&#26102;&#21051;&#65292;&#23398;&#20064;&#20195;&#29702;&#36873;&#25321;&#19968;&#20010;&#26377;&#24207;&#30340;&#29289;&#21697;&#21015;&#34920;&#65292;&#24182;&#35266;&#23519;&#38543;&#26426;&#32467;&#26524;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#26174;&#31034;&#19968;&#20010;&#26377;&#24207;&#30340;&#26368;&#20855;&#21560;&#24341;&#21147;&#30340;&#29289;&#21697;&#21015;&#34920;&#24182;&#19981;&#24635;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#20301;&#32622;&#21644;&#29289;&#21697;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#20363;&#23376;&#26159;&#24403;&#25152;&#26377;&#26368;&#20855;&#21560;&#24341;&#21147;&#30340;&#29289;&#21697;&#37117;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#26102;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23545;&#26377;&#24207;&#21015;&#34920;&#20013;&#30340;&#20301;&#32622;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#35774;&#35745;&#20102;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;UCB&#21644;Thompson Sampling&#31867;&#22411;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#20960;&#20010;&#26041;&#21521;&#19978;&#25512;&#24191;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#20301;&#32622;&#20381;&#36182;&#24615;&#65292;&#20854;&#20013;&#20301;&#32622;&#25240;&#25187;&#26159;&#19968;&#20010;&#29305;&#20363;&#65292;&#24182;&#23558;&#25490;&#21517;&#38382;&#39064;&#19982;&#22270;&#35770;&#30456;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the ranking problem in generalized linear bandits. At each time, the learning agent selects an ordered list of items and observes stochastic outcomes. In recommendation systems, displaying an ordered list of the most attractive items is not always optimal as both position and item dependencies result in a complex reward function. A very naive example is the lack of diversity when all the most attractive items are from the same category. We model the position and item dependencies in the ordered list and design UCB and Thompson Sampling type algorithms for this problem. Our work generalizes existing studies in several directions, including position dependencies where position discount is a particular case, and connecting the ranking problem to graph theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#22240;&#26524;&#26426;&#21046;&#30340;&#20998;&#31163;&#34920;&#31034;&#65292;&#36890;&#36807;&#20272;&#35745;&#21407;&#22987;&#21644;&#26032;&#20998;&#24067;&#20043;&#38388;&#30340;&#26465;&#20214;&#27010;&#29575;&#24046;&#24322;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#36924;&#36817;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#20381;&#36182;&#23398;&#20064;&#32773;&#23545;&#26032;&#20998;&#24067;&#30340;&#36866;&#24212;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#26356;&#21152;&#26679;&#26412;&#39640;&#25928;&#19988;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2201.01942</link><description>&lt;p&gt;
&#39640;&#25928;&#22320;&#35299;&#24320;&#22240;&#26524;&#34920;&#31034;&#20132;&#32455;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Efficiently Disentangle Causal Representations. (arXiv:2201.01942v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#22240;&#26524;&#26426;&#21046;&#30340;&#20998;&#31163;&#34920;&#31034;&#65292;&#36890;&#36807;&#20272;&#35745;&#21407;&#22987;&#21644;&#26032;&#20998;&#24067;&#20043;&#38388;&#30340;&#26465;&#20214;&#27010;&#29575;&#24046;&#24322;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#36924;&#36817;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#20381;&#36182;&#23398;&#20064;&#32773;&#23545;&#26032;&#20998;&#24067;&#30340;&#36866;&#24212;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#26356;&#21152;&#26679;&#26412;&#39640;&#25928;&#19988;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#20998;&#24067;&#21644;&#26032;&#20998;&#24067;&#30340;&#26465;&#20214;&#27010;&#29575;&#20043;&#24046;&#30340;&#22240;&#26524;&#26426;&#21046;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26469;&#36924;&#36817;&#36825;&#31181;&#24046;&#24322;&#65292;&#20351;&#20854;&#36866;&#24212;&#26631;&#20934;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#24182;&#33021;&#22815;&#39640;&#25928;&#22320;&#35745;&#31639;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#23398;&#20064;&#32773;&#23545;&#26032;&#20998;&#24067;&#30340;&#36866;&#24212;&#36895;&#24230;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#21183;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#33410;&#32422;&#26679;&#26412;&#65292;&#36895;&#24230;&#26356;&#24555;&#65292;&#20998;&#21035;&#25552;&#21319;&#20102;1.9-11.0&#20493;&#21644;9.4-32.4&#20493;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312; \url{https://github.com/yuanpeng16/EDCR} &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient approach to learning disentangled representations with causal mechanisms based on the difference of conditional probabilities in original and new distributions. We approximate the difference with models' generalization abilities so that it fits in the standard machine learning framework and can be efficiently computed. In contrast to the state-of-the-art approach, which relies on the learner's adaptation speed to new distribution, the proposed approach only requires evaluating the model's generalization ability. We provide a theoretical explanation for the advantage of the proposed method, and our experiments show that the proposed technique is 1.9--11.0$\times$ more sample efficient and 9.4--32.4 times quicker than the previous method on various tasks. The source code is available at \url{https://github.com/yuanpeng16/EDCR}.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20849;&#21516;&#20272;&#35745;&#22810;&#20010;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#30340;&#36716;&#31227;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#25968;&#25454;&#27719;&#38598;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#37325;&#35201;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2112.10955</link><description>&lt;p&gt;
&#20849;&#21516;&#23398;&#20064;&#32447;&#24615;&#26102;&#19981;&#21464;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Linear Time-Invariant Dynamical Systems. (arXiv:2112.10955v6 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.10955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20849;&#21516;&#20272;&#35745;&#22810;&#20010;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#30340;&#36716;&#31227;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#25968;&#25454;&#27719;&#38598;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#37325;&#35201;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#26159;&#31995;&#32479;&#35770;&#21644;&#24212;&#29992;&#20013;&#38750;&#24120;&#27969;&#34892;&#30340;&#27169;&#22411;&#12290;&#31995;&#32479;&#36776;&#35782;&#20013;&#19968;&#20010;&#26410;&#21463;&#21040;&#20805;&#20998;&#20851;&#27880;&#30340;&#22522;&#26412;&#38382;&#39064;&#26159;&#22914;&#20309;&#21033;&#29992;&#30456;&#20851;&#32447;&#24615;&#31995;&#32479;&#20043;&#38388;&#30340;&#20849;&#24615;&#26469;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#23427;&#20204;&#30340;&#36716;&#31227;&#30697;&#38453;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#21512;&#20272;&#35745;&#22810;&#20010;&#31995;&#32479;&#30340;&#36716;&#31227;&#30697;&#38453;&#30340;&#26041;&#27861;&#12290;&#20551;&#35774;&#36716;&#31227;&#30697;&#38453;&#26159;&#19968;&#20123;&#26410;&#30693;&#20849;&#20139;&#22522;&#30784;&#30697;&#38453;&#30340;&#26410;&#30693;&#32447;&#24615;&#20989;&#25968;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#23436;&#20840;&#21453;&#26144;&#32771;&#34385;&#30340;&#36712;&#36857;&#38271;&#24230;&#12289;&#32500;&#24230;&#21644;&#31995;&#32479;&#25968;&#37327;&#30340;&#26377;&#38480;&#26102;&#38388;&#20272;&#35745;&#35823;&#24046;&#29575;&#12290;&#25152;&#21576;&#29616;&#30340;&#32467;&#26524;&#30456;&#24403;&#26222;&#36941;&#65292;&#24182;&#26174;&#31034;&#20102;&#19982;&#21333;&#29420;&#23398;&#20064;&#27599;&#20010;&#31995;&#32479;&#30456;&#27604;&#65292;&#36890;&#36807;&#31995;&#32479;&#20043;&#38388;&#30340;&#25968;&#25454;&#27719;&#38598;&#21487;&#20197;&#33719;&#24471;&#30340;&#26174;&#33879;&#25910;&#30410;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#36824;&#26174;&#31034;&#20102;&#23545;&#27169;&#22411;&#38169;&#35823;&#35774;&#23450;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#24471;&#21040;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear time-invariant systems are very popular models in system theory and applications. A fundamental problem in system identification that remains rather unaddressed in extant literature is to leverage commonalities amongst related linear systems to estimate their transition matrices more accurately. To address this problem, the current paper investigates methods for jointly estimating the transition matrices of multiple systems. It is assumed that the transition matrices are unknown linear functions of some unknown shared basis matrices. We establish finite-time estimation error rates that fully reflect the roles of trajectory lengths, dimension, and number of systems under consideration. The presented results are fairly general and show the significant gains that can be achieved by pooling data across systems in comparison to learning each system individually. Further, they are shown to be robust against model misspecifications. To obtain the results, we develop novel techniques th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#28151;&#21512;&#21464;&#37327;&#35266;&#27979;&#30340;&#20998;&#31867;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24179;&#28369;&#20811;&#26381;&#20102;&#25968;&#25454;&#20998;&#21106;&#21644;&#24102;&#23485;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2112.07145</link><description>&lt;p&gt;
&#20855;&#26377;&#39640;&#32500;&#28151;&#21512;&#21464;&#37327;&#30340;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Linear Discriminant Analysis with High-dimensional Mixed Variables. (arXiv:2112.07145v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.07145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#28151;&#21512;&#21464;&#37327;&#35266;&#27979;&#30340;&#20998;&#31867;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24179;&#28369;&#20811;&#26381;&#20102;&#25968;&#25454;&#20998;&#21106;&#21644;&#24102;&#23485;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#32463;&#24120;&#36935;&#21040;&#21516;&#26102;&#21253;&#21547;&#20998;&#31867;&#21644;&#36830;&#32493;&#21464;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#38543;&#30528;&#29616;&#20195;&#27979;&#37327;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#36825;&#20123;&#21464;&#37327;&#30340;&#32500;&#24230;&#21487;&#33021;&#38750;&#24120;&#39640;&#12290;&#23613;&#31649;&#22312;&#36830;&#32493;&#21464;&#37327;&#30340;&#39640;&#32500;&#25968;&#25454;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#28151;&#21512;&#21464;&#37327;&#38598;&#21512;&#30340;&#22788;&#29702;&#26041;&#27861;&#20173;&#28982;&#24456;&#31232;&#32570;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#28151;&#21512;&#21464;&#37327;&#35266;&#27979;&#30340;&#20998;&#31867;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#20010;&#20301;&#32622;&#27169;&#22411;&#19978;&#65292;&#20854;&#20013;&#20551;&#35774;&#36830;&#32493;&#21464;&#37327;&#22312;&#32473;&#23450;&#20998;&#31867;&#21464;&#37327;&#19979;&#30340;&#26465;&#20214;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#12290;&#36890;&#36807;&#26680;&#24179;&#28369;&#20811;&#26381;&#20102;&#23558;&#25968;&#25454;&#20998;&#21106;&#25104;&#25351;&#25968;&#32423;&#21035;&#30340;&#21333;&#20803;&#26684;&#25110;&#20998;&#31867;&#21464;&#37327;&#32452;&#21512;&#30340;&#25361;&#25112;&#65292;&#24182;&#23545;&#20854;&#24102;&#23485;&#36873;&#25321;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#20197;&#30830;&#20445;Bochner&#24341;&#29702;&#30340;&#31867;&#27604;&#65292;&#36825;&#19982;&#36890;&#24120;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#19981;&#21516;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#32452;&#21442;&#25968;&#21487;&#20197;&#21516;&#26102;&#20272;&#35745;&#65292;&#20934;&#30830;&#24615;&#20063;&#26377;&#25152;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Datasets containing both categorical and continuous variables are frequently encountered in many areas, and with the rapid development of modern measurement technologies, the dimensions of these variables can be very high. Despite the recent progress made in modelling high-dimensional data for continuous variables, there is a scarcity of methods that can deal with a mixed set of variables. To fill this gap, this paper develops a novel approach for classifying high-dimensional observations with mixed variables. Our framework builds on a location model, in which the distributions of the continuous variables conditional on categorical ones are assumed Gaussian. We overcome the challenge of having to split data into exponentially many cells, or combinations of the categorical variables, by kernel smoothing, and provide new perspectives for its bandwidth choice to ensure an analogue of Bochner's Lemma, which is different to the usual bias-variance tradeoff. We show that the two sets of para
&lt;/p&gt;</description></item></channel></rss>