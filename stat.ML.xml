<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#26500;&#24314;&#22359;&#65292;&#31216;&#20026;MultiresLayer&#65292;&#36890;&#36807;&#22810;&#20998;&#36776;&#29575;&#21367;&#31215;&#25429;&#33719;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#22810;&#23610;&#24230;&#36235;&#21183;&#65292;&#26082;&#20855;&#26377;&#21367;&#31215;&#32593;&#32476;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#21448;&#20855;&#26377;&#23567;&#27874;&#20998;&#35299;&#30340;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#21160;&#26426;&#12290;</title><link>http://arxiv.org/abs/2305.01638</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#21367;&#31215;&#35760;&#24518;&#30340;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Sequence Modeling with Multiresolution Convolutional Memory. (arXiv:2305.01638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#26500;&#24314;&#22359;&#65292;&#31216;&#20026;MultiresLayer&#65292;&#36890;&#36807;&#22810;&#20998;&#36776;&#29575;&#21367;&#31215;&#25429;&#33719;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#22810;&#23610;&#24230;&#36235;&#21183;&#65292;&#26082;&#20855;&#26377;&#21367;&#31215;&#32593;&#32476;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#21448;&#20855;&#26377;&#23567;&#27874;&#20998;&#35299;&#30340;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#25429;&#25417;&#23545;&#20110;&#26576;&#20010;&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#21644;&#29983;&#25104;&#24314;&#27169;&#65289;&#26174;&#33879;&#30340;&#39034;&#24207;&#25968;&#25454;&#28304;&#20013;&#30340;&#38271;&#31243;&#27169;&#24335;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;&#22522;&#20110;&#23567;&#27874;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#26500;&#24314;&#22359;&#65292;&#31216;&#20026;MultiresLayer&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#22810;&#20998;&#36776;&#29575;&#21367;&#31215;&#65292;&#20197;&#25429;&#33719;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#22810;&#23610;&#24230;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;MultiresConv&#21487;&#20197;&#36890;&#36807;&#22312;&#25193;&#24352;&#30340;&#22240;&#26524;&#21367;&#31215;&#26641;&#19978;&#20351;&#29992;&#20849;&#20139;&#36807;&#28388;&#22120;&#26469;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#23427;&#26082;&#20855;&#26377;&#21367;&#31215;&#32593;&#32476;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#21448;&#20855;&#26377;&#23567;&#27874;&#20998;&#35299;&#30340;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently capturing the long-range patterns in sequential data sources salient to a given task -- such as classification and generative modeling -poses a fundamental challenge. Popular approaches in the space tradeoff between the memory burden of brute-force enumeration and comparison, as in transformers, the computational burden of complicated sequential dependencies, as in recurrent neural networks, or the parameter burden of convolutional networks with many or large filters. We instead take inspiration from wavelet-based multiresolution analysis to define a new building block for sequence modeling, which we call a MultiresLayer. The key component of our model is the multiresolution convolution, capturing multiscale trends in the input sequence. Our MultiresConv can be implemented with shared filters across a dilated causal convolution tree. Thus it garners the computational advantages of convolutional networks and the principled theoretical motivation of wavelet decompositions. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26799;&#24230;&#21098;&#20999;&#30340;&#25910;&#25947;&#20445;&#35777;&#26426;&#21046;&#65292;&#19981;&#20877;&#38656;&#35201;&#29305;&#23450;&#30340;&#38408;&#20540;&#21644;&#24378;&#22122;&#22768;&#20551;&#35774;&#65292;&#21516;&#26102;&#21487;&#20197;&#29420;&#31435;&#20110;&#27493;&#38271;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25910;&#25947;&#30340;&#33258;&#30001;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.01588</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26799;&#24230;&#21098;&#20999;&#65306;&#38543;&#26426;&#20559;&#24046;&#21644;&#32039;&#23494;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees. (arXiv:2305.01588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26799;&#24230;&#21098;&#20999;&#30340;&#25910;&#25947;&#20445;&#35777;&#26426;&#21046;&#65292;&#19981;&#20877;&#38656;&#35201;&#29305;&#23450;&#30340;&#38408;&#20540;&#21644;&#24378;&#22122;&#22768;&#20551;&#35774;&#65292;&#21516;&#26102;&#21487;&#20197;&#29420;&#31435;&#20110;&#27493;&#38271;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25910;&#25947;&#30340;&#33258;&#30001;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#21098;&#20999;&#26159;&#26631;&#20934;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#30340;&#19968;&#31181;&#27969;&#34892;&#20462;&#25913;&#26041;&#27861;&#65292;&#27599;&#27425;&#36845;&#20195;&#23558;&#26799;&#24230;&#33539;&#25968;&#38480;&#21046;&#22312;&#26576;&#20010;&#20540;c&gt;0&#12290;&#23427;&#34987;&#24191;&#27867;&#29992;&#20110;&#31283;&#23450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;( Goodfellow et al., 2016 )&#25110;&#24378;&#21046;&#23454;&#26045;&#24046;&#20998;&#38544;&#31169;( Abadi et al., 2016 )&#12290;&#23613;&#31649;&#21098;&#20999;&#26426;&#21046;&#21463;&#27426;&#36814;&#19988;&#31616;&#21333;&#65292;&#20294;&#20854;&#25910;&#25947;&#20445;&#35777;&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#30340;$c$&#20540;&#21644;&#24378;&#22122;&#22768;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#25910;&#25947;&#20445;&#35777;&#65292;&#26174;&#31034;&#20102;&#23545;&#20219;&#24847;&#21098;&#36753;&#38408;&#20540;&#30340;&#31934;&#30830;&#20381;&#36182;&#65292;&#24182;&#19988;&#34920;&#26126;&#25105;&#20204;&#30340;&#20445;&#35777;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#37117;&#26159;&#32039;&#23494;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;(i)&#23545;&#20110;&#30830;&#23450;&#24615;&#30340;&#26799;&#24230;&#19979;&#38477;&#65292;&#21098;&#36753;&#38408;&#20540;&#20165;&#24433;&#21709;&#25910;&#25947;&#30340;&#39640;&#38454;&#39033;&#65292;(ii)&#22312;&#38543;&#26426;&#35774;&#32622;&#20013;&#65292;&#21363;&#20351;&#23545;&#20110;&#20219;&#24847;&#23567;&#30340;&#27493;&#38271;&#65292;&#20063;&#19981;&#33021;&#20445;&#35777;&#25910;&#25947;&#21040;&#30495;&#27491;&#30340;&#26368;&#20248;&#35299;&#22312;&#26631;&#20934;&#30340;&#22122;&#22768;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#29305;&#23450;&#30340;&#38543;&#26426;&#22122;&#22768;&#20551;&#35774;&#65292;&#22312;&#27492;&#20551;&#35774;&#19979;&#65292;&#25910;&#25947;&#26159;&#20445;&#35777;&#30340;&#65292;&#21098;&#20999;&#38408;&#20540;$c$&#21487;&#20197;&#29420;&#31435;&#20110;&#27493;&#38271;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient clipping is a popular modification to standard (stochastic) gradient descent, at every iteration limiting the gradient norm to a certain value $c &gt;0$. It is widely used for example for stabilizing the training of deep learning models (Goodfellow et al., 2016), or for enforcing differential privacy (Abadi et al., 2016). Despite popularity and simplicity of the clipping mechanism, its convergence guarantees often require specific values of $c$ and strong noise assumptions.  In this paper, we give convergence guarantees that show precise dependence on arbitrary clipping thresholds $c$ and show that our guarantees are tight with both deterministic and stochastic gradients. In particular, we show that (i) for deterministic gradient descent, the clipping threshold only affects the higher-order terms of convergence, (ii) in the stochastic setting convergence to the true optimum cannot be guaranteed under the standard noise assumption, even under arbitrary small step-sizes. We give ma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RECODE&#30340;&#38750;&#21442;&#25968;&#26032;&#39062;&#24615;&#25506;&#32034;&#26041;&#27861;&#65292;&#23427;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36319;&#36394;&#29366;&#24577;&#35775;&#38382;&#35745;&#25968;&#65292;&#24182;&#19982;&#26032;&#39062;&#30340;&#36870;&#21160;&#21147;&#23398;&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#30340;&#26368;&#26032;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01521</link><description>&lt;p&gt;
&#25581;&#31192;&#38271;&#26399;&#22522;&#20110;&#26032;&#39062;&#24615;&#25506;&#32034;&#20013;&#34920;&#31034;&#26041;&#27861;&#30340;&#23041;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Power of Representations in Long-term Novelty-based Exploration. (arXiv:2305.01521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RECODE&#30340;&#38750;&#21442;&#25968;&#26032;&#39062;&#24615;&#25506;&#32034;&#26041;&#27861;&#65292;&#23427;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36319;&#36394;&#29366;&#24577;&#35775;&#38382;&#35745;&#25968;&#65292;&#24182;&#19982;&#26032;&#39062;&#30340;&#36870;&#21160;&#21147;&#23398;&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#30340;&#26368;&#26032;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RECODE&#65288;&#22522;&#20110;&#32858;&#31867;&#30340;&#22312;&#32447;&#23494;&#24230;&#20272;&#35745;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65289;&#30340;&#38750;&#21442;&#25968;&#26032;&#39062;&#24615;&#25506;&#32034;&#26041;&#27861;&#65292;&#20854;&#26681;&#25454;&#22312;&#25152;&#36873;&#25321;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#30456;&#20284;&#24615;&#32858;&#21512;&#29366;&#24577;&#24182;&#20272;&#35745;&#20854;&#35775;&#38382;&#27425;&#25968;&#12290;&#36890;&#36807;&#23558;&#32463;&#20856;&#32858;&#31867;&#26041;&#27861;&#36866;&#24212;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#24179;&#31283;&#24615;&#29615;&#22659;&#65292;RECODE&#21487;&#22312;&#25968;&#21315;&#20010;&#22238;&#21512;&#20013;&#26377;&#25928;&#22320;&#36319;&#36394;&#29366;&#24577;&#35775;&#38382;&#35745;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36870;&#21160;&#21147;&#23398;&#25439;&#22833;&#30340;&#27867;&#21270;&#24418;&#24335;&#65292;&#23427;&#21033;&#29992;&#25513;&#30721;&#21464;&#21387;&#22120;&#32467;&#26500;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#12290;RECODE&#19982;&#27492;&#30456;&#32467;&#21512;&#65292;&#22312;DM-Hard-8&#30340;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;3D&#25506;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#22312;&#22256;&#38590;&#30340;Atari&#28216;&#25103;&#20013;&#65292;RECODE&#20063;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#25104;&#20026;&#39318;&#20010;&#25104;&#21151;&#36890;&#20851;"Pitfall!"&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Robust Exploration via Clustering-based Online Density Estimation (RECODE), a non-parametric method for novelty-based exploration that estimates visitation counts for clusters of states based on their similarity in a chosen embedding space. By adapting classical clustering to the nonstationary setting of Deep RL, RECODE can efficiently track state visitation counts over thousands of episodes. We further propose a novel generalization of the inverse dynamics loss, which leverages masked transformer architectures for multi-step prediction; which in conjunction with RECODE achieves a new state-of-the-art in a suite of challenging 3D-exploration tasks in DM-Hard-8. RECODE also sets new state-of-the-art in hard exploration Atari games, and is the first agent to reach the end screen in "Pitfall!".
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;Copula&#28151;&#21512;&#27169;&#22411;&#65288;GCMM&#65289;&#30340;&#24615;&#36136;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#25193;&#23637;&#26399;&#26395;&#26368;&#22823;&#31639;&#27861;&#30340;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;GCMM&#30456;&#27604;&#20110;GMM&#21487;&#20197;&#26356;&#22909;&#22320;&#25311;&#21512;&#25968;&#25454;&#24182;&#23454;&#29616;&#26356;&#28145;&#20837;&#30340;&#25968;&#25454;&#25366;&#25496;&#12290;</title><link>http://arxiv.org/abs/2305.01479</link><description>&lt;p&gt;
&#39640;&#26031;Copula&#28151;&#21512;&#27169;&#22411;&#30340;&#24615;&#36136;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the properties of Gaussian Copula Mixture Models. (arXiv:2305.01479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;Copula&#28151;&#21512;&#27169;&#22411;&#65288;GCMM&#65289;&#30340;&#24615;&#36136;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#25193;&#23637;&#26399;&#26395;&#26368;&#22823;&#31639;&#27861;&#30340;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;GCMM&#30456;&#27604;&#20110;GMM&#21487;&#20197;&#26356;&#22909;&#22320;&#25311;&#21512;&#25968;&#25454;&#24182;&#23454;&#29616;&#26356;&#28145;&#20837;&#30340;&#25968;&#25454;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;Copula&#28151;&#21512;&#27169;&#22411;&#65288;GCMM&#65289;&#26159;&#20351;&#29992;Copula&#27010;&#24565;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#25512;&#24191;&#12290;&#26412;&#25991;&#32473;&#20986;&#20102;&#20854;&#25968;&#23398;&#23450;&#20041;&#65292;&#24182;&#30740;&#31350;&#20102;&#20284;&#28982;&#20989;&#25968;&#30340;&#24615;&#36136;&#12290;&#22522;&#20110;&#36825;&#20123;&#23646;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#25193;&#23637;&#26399;&#26395;&#26368;&#22823;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#28151;&#21512;Copula&#30340;&#21442;&#25968;&#65292;&#32780;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#30340;&#36793;&#38469;&#20998;&#24067;&#21017;&#20351;&#29992;&#21333;&#29420;&#30340;&#38750;&#21442;&#25968;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#20272;&#35745;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;GMM&#65292;GCMM&#22312;&#30456;&#21516;&#25968;&#37327;&#30340;&#32858;&#31867;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25311;&#21512;&#65307;&#27492;&#22806;&#65292;GCMM&#21487;&#20197;&#21033;&#29992;&#27599;&#20010;&#32500;&#24230;&#19978;&#30340;&#19981;&#21516;&#27493;&#25968;&#25454;&#23454;&#29616;&#26356;&#28145;&#20837;&#30340;&#25968;&#25454;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian copula mixture models (GCMM) are the generalization of Gaussian Mixture models using the concept of copula. Its mathematical definition is given and the properties of likelihood function are studied in this paper. Based on these properties, extended Expectation Maximum algorithms are developed for estimating parameters for the mixture of copulas while marginal distributions corresponding to each component is estimated using separate nonparametric statistical methods. In the experiment, GCMM can achieve better goodness-of-fitting given the same number of clusters as GMM; furthermore, GCMM can utilize unsynchronized data on each dimension to achieve deeper mining of data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#19978;&#19979;&#25991;&#30340;&#38543;&#26426;&#24773;&#22659;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#33410;&#28857;&#26631;&#31614;&#30456;&#21516;&#30340;&#33410;&#28857;&#20849;&#20139;&#30456;&#21516;&#30340;&#22870;&#21169;&#20998;&#24067;&#12290;&#23545;&#20110;&#32447;&#22270;&#21644;&#26641;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36951;&#25022;&#30028;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01470</link><description>&lt;p&gt;
&#24102;&#26377;&#22522;&#20110;&#22270;&#30340;&#19978;&#19979;&#25991;&#30340;&#38543;&#26426;&#24773;&#22659;&#36172;&#21338;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Stochastic Contextual Bandits with Graph-based Contexts. (arXiv:2305.01470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#19978;&#19979;&#25991;&#30340;&#38543;&#26426;&#24773;&#22659;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#33410;&#28857;&#26631;&#31614;&#30456;&#21516;&#30340;&#33410;&#28857;&#20849;&#20139;&#30456;&#21516;&#30340;&#22870;&#21169;&#20998;&#24067;&#12290;&#23545;&#20110;&#32447;&#22270;&#21644;&#26641;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36951;&#25022;&#30028;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22312;&#32447;&#22270;&#39044;&#27979;&#38382;&#39064;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#20102;&#38543;&#26426;&#24773;&#22659;&#36172;&#21338;&#38382;&#39064;&#30340;&#19968;&#20010;&#29256;&#26412;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#26159;&#22270;&#20013;&#30340;&#33410;&#28857;&#65292;&#32780;&#22270;&#30340;&#32467;&#26500;&#25552;&#20379;&#20102;&#26377;&#20851;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32473;&#20986;&#19968;&#20010;&#22270;$G = (V&#65292;E)$&#65292;&#20854;&#33410;&#28857;&#38598;$V$&#34920;&#31034;&#20855;&#26377;&#26410;&#30693;&#39030;&#28857;&#26631;&#31614;$y$&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#25105;&#20204;&#30340;&#38543;&#26426;&#24773;&#22659;&#36172;&#21338;&#26426;&#35774;&#32622;&#20013;&#65292;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#39030;&#28857;&#20849;&#20139;&#21516;&#19968;&#22870;&#21169;&#20998;&#24067;&#12290;&#22312;&#22270;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#26631;&#20934;&#30340;&#23454;&#20363;&#38590;&#24230;&#27010;&#24565;&#26159;&#21106;&#22823;&#23567;$f$&#65292;&#21363;&#26377;&#19981;&#21516;&#26631;&#31614;&#32467;&#26463;&#28857;&#30340;&#36793;&#25968;&#12290;&#23545;&#20110;&#32447;&#22270;&#21644;&#26641;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36951;&#25022;&#30028;&#30340;&#31639;&#27861;$O(T^{2/3}K^{1/3}f^{1/3})$&#65292;&#20854;&#20013;$K$&#26159;&#25163;&#33218;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;Zimmert&#21644;Seldin~[AISTAT'19&#65292;JMLR'21]&#30340;&#26368;&#20248;&#38543;&#26426;&#36172;&#24466;&#31639;&#27861;&#12290;&#24403;&#26368;&#20339;&#25163;&#33218;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#25163;&#33218;&#26102;&#65292;&#36951;&#25022;&#30028;&#23558;&#25913;&#21892;&#20026;$\tilde{O}(\sqrt{KT\cdot f})$&#12290;
&lt;/p&gt;
&lt;p&gt;
We naturally generalize the on-line graph prediction problem to a version of stochastic contextual bandit problems where contexts are vertices in a graph and the structure of the graph provides information on the similarity of contexts. More specifically, we are given a graph $G=(V,E)$, whose vertex set $V$ represents contexts with {\em unknown} vertex label $y$. In our stochastic contextual bandit setting, vertices with the same label share the same reward distribution. The standard notion of instance difficulties in graph label prediction is the cutsize $f$ defined to be the number of edges whose end points having different labels. For line graphs and trees we present an algorithm with regret bound of $\tilde{O}(T^{2/3}K^{1/3}f^{1/3})$ where $K$ is the number of arms. Our algorithm relies on the optimal stochastic bandit algorithm by Zimmert and Seldin~[AISTAT'19, JMLR'21]. When the best arm outperforms the other arms, the regret improves to $\tilde{O}(\sqrt{KT\cdot f})$. The regret 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#35760;&#24518;&#23481;&#37327;&#35745;&#31639;&#38382;&#39064;&#12290;&#36890;&#36807;&#21457;&#29616;&#25968;&#20540;&#35780;&#20272;&#30340;&#19981;&#20934;&#30830;&#24615;&#20027;&#35201;&#28304;&#20110;&#25968;&#20540;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#25513;&#30721;&#30697;&#38453;MC&#30456;&#23545;&#20110;&#20013;&#31435;&#24615;&#30340;&#31283;&#20581;&#25968;&#20540;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25968;&#20540;&#35780;&#20272;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01457</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35760;&#24518;&#65306;&#25105;&#20204;&#35745;&#31639;&#24471;&#23545;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Memory of recurrent networks: Do we compute it right?. (arXiv:2305.01457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#35760;&#24518;&#23481;&#37327;&#35745;&#31639;&#38382;&#39064;&#12290;&#36890;&#36807;&#21457;&#29616;&#25968;&#20540;&#35780;&#20272;&#30340;&#19981;&#20934;&#30830;&#24615;&#20027;&#35201;&#28304;&#20110;&#25968;&#20540;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#25513;&#30721;&#30697;&#38453;MC&#30456;&#23545;&#20110;&#20013;&#31435;&#24615;&#30340;&#31283;&#20581;&#25968;&#20540;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25968;&#20540;&#35780;&#20272;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#23545;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35760;&#24518;&#23481;&#37327;&#65288;MC&#65289;&#30340;&#25968;&#20540;&#35780;&#20272;&#24120;&#24120;&#19982;&#24050;&#32463;&#24314;&#31435;&#30340;&#29702;&#35770;&#30028;&#38480;&#30456;&#30683;&#30462;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#24773;&#20917;&#65292;&#23545;&#24212;&#30340;Kalman&#21487;&#25511;&#30697;&#38453;&#30340;&#31209;&#24050;&#34987;&#35777;&#26126;&#31561;&#20110;&#24635;&#35760;&#24518;&#23481;&#37327;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20851;&#20110;&#35760;&#24518;&#19981;&#20934;&#30830;&#30340;&#25968;&#20540;&#35780;&#20272;&#30340;&#21508;&#31181;&#21407;&#22240;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#38382;&#39064;&#26159;&#32431;&#31929;&#25968;&#20540;&#26041;&#38754;&#19978;&#30340;&#65292;&#24448;&#24448;&#22312;&#36817;&#26399;&#25991;&#29486;&#20013;&#34987;&#24573;&#35270;&#12290;&#26356;&#26126;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#32447;&#24615;MC&#30340;Krylov&#32467;&#26500;&#34987;&#24573;&#30053;&#26102;&#65292;&#29702;&#35770;MC&#21644;&#23427;&#30340;&#32463;&#39564;&#20540;&#20043;&#38388;&#20250;&#23384;&#22312;&#24046;&#36317;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#65292;&#21033;&#29992;MC&#30456;&#23545;&#20110;&#36755;&#20837;&#25513;&#30721;&#30697;&#38453;&#30340;&#20013;&#31435;&#24615;&#65292;&#24320;&#21457;&#20986;&#31283;&#20581;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24471;&#21040;&#30340;&#35760;&#24518;&#26354;&#32447;&#19982;&#29702;&#35770;&#23436;&#20840;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerical evaluations of the memory capacity (MC) of recurrent neural networks reported in the literature often contradict well-established theoretical bounds. In this paper, we study the case of linear echo state networks, for which the total memory capacity has been proven to be equal to the rank of the corresponding Kalman controllability matrix. We shed light on various reasons for the inaccurate numerical estimations of the memory, and we show that these issues, often overlooked in the recent literature, are of an exclusively numerical nature. More explicitly, we prove that when the Krylov structure of the linear MC is ignored, a gap between the theoretical MC and its empirical counterpart is introduced. As a solution, we develop robust numerical approaches by exploiting a result of MC neutrality with respect to the input mask matrix. Simulations show that the memory curves that are recovered using the proposed methods fully agree with the theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;TSER&#31639;&#27861;&#65306;FreshPRINCE&#21644;DrCIF&#65292;&#23427;&#20204;&#20998;&#21035;&#30001;&#19968;&#32452;&#27719;&#24635;&#29305;&#24449;&#21644;&#22810;&#20010;&#26465;&#20214;&#25512;&#29702;&#26641;&#26500;&#25104;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;&#26102;&#38388;&#24207;&#21015;&#22806;&#28304;&#22238;&#24402;&#30340;&#38382;&#39064;&#19978;&#39044;&#27979;&#21709;&#24212;&#21464;&#37327;&#65292;&#27604;&#36215;&#20197;&#21069;&#30340;&#35780;&#20272;&#20013;&#20351;&#29992;&#30340;&#22522;&#32447;&#31639;&#27861;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.01429</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#22806;&#28304;&#22238;&#24402;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Feature Based Algorithms for Time Series Extrinsic Regression. (arXiv:2305.01429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;TSER&#31639;&#27861;&#65306;FreshPRINCE&#21644;DrCIF&#65292;&#23427;&#20204;&#20998;&#21035;&#30001;&#19968;&#32452;&#27719;&#24635;&#29305;&#24449;&#21644;&#22810;&#20010;&#26465;&#20214;&#25512;&#29702;&#26641;&#26500;&#25104;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;&#26102;&#38388;&#24207;&#21015;&#22806;&#28304;&#22238;&#24402;&#30340;&#38382;&#39064;&#19978;&#39044;&#27979;&#21709;&#24212;&#21464;&#37327;&#65292;&#27604;&#36215;&#20197;&#21069;&#30340;&#35780;&#20272;&#20013;&#20351;&#29992;&#30340;&#22522;&#32447;&#31639;&#27861;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#22806;&#28304;&#22238;&#24402;&#65288;TSER&#65289;&#28041;&#21450;&#20351;&#29992;&#19968;&#32452;&#35757;&#32451;&#26102;&#38388;&#24207;&#21015;&#26469;&#24418;&#25104;&#19968;&#20010;&#36830;&#32493;&#21709;&#24212;&#21464;&#37327;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#21464;&#37327;&#19982;&#22238;&#24402;&#22120;&#24207;&#21015;&#27809;&#26377;&#30452;&#25509;&#20851;&#31995;&#12290;TSER&#23384;&#26723;&#29992;&#20110;&#27604;&#36739;&#31639;&#27861;&#20110;2022&#24180;&#21457;&#24067;&#65292;&#21253;&#25324;19&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#27492;&#23384;&#26723;&#30340;&#22823;&#23567;&#22686;&#21152;&#21040;63&#20010;&#38382;&#39064;&#65292;&#24182;&#37325;&#29616;&#20197;&#21069;&#31639;&#27861;&#30340;&#22522;&#20934;&#27604;&#36739;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#27604;&#36739;&#65292;&#21253;&#25324;&#26356;&#24191;&#27867;&#30340;&#26631;&#20934;&#22238;&#24402;&#22120;&#21644;&#20197;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#26368;&#26032;&#29256;&#26412;&#30340;TSER&#27169;&#22411;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20197;&#21069;&#35780;&#20272;&#30340;&#22238;&#24402;&#22120;&#37117;&#19981;&#33021;&#32988;&#36807;&#26631;&#20934;&#20998;&#31867;&#22120;&#26059;&#36716;&#26862;&#26519;&#30340;&#22238;&#24402;&#36866;&#24212;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;TSER&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#26159;&#20174;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#30456;&#20851;&#24037;&#20316;&#20013;&#24320;&#21457;&#32780;&#26469;&#12290;FreshPRINCE&#26159;&#19968;&#20010;&#31649;&#36947;&#20272;&#35745;&#22120;&#65292;&#21253;&#25324;&#36716;&#25442;&#21040;&#21508;&#31181;&#27719;&#24635;&#29305;&#24449;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#26059;&#36716;&#26862;&#26519;&#22238;&#24402;&#22120;&#12290;DrCIF&#26159;&#19968;&#20010;&#26641;&#38598;&#21512;&#65292;&#23427;&#26681;&#25454;&#26102;&#38388;&#24207;&#21015;&#30340;&#38543;&#26426;&#22686;&#37327;&#21019;&#24314;&#27719;&#24635;&#32479;&#35745;&#20449;&#24687;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#22810;&#20010;&#26465;&#20214;&#25512;&#29702;&#26641;&#26469;&#39044;&#27979;&#21709;&#24212;&#21464;&#37327;&#12290;&#22312;&#25193;&#23637;&#30340;TSER&#38382;&#39064;&#38598;&#19978;&#65292;FreshPRINCE&#21644;DrCIF&#37117;&#22987;&#32456;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time Series Extrinsic Regression (TSER) involves using a set of training time series to form a predictive model of a continuous response variable that is not directly related to the regressor series. The TSER archive for comparing algorithms was released in 2022 with 19 problems. We increase the size of this archive to 63 problems and reproduce the previous comparison of baseline algorithms. We then extend the comparison to include a wider range of standard regressors and the latest versions of TSER models used in the previous study. We show that none of the previously evaluated regressors can outperform a regression adaptation of a standard classifier, rotation forest. We introduce two new TSER algorithms developed from related work in time series classification. FreshPRINCE is a pipeline estimator consisting of a transform into a wide range of summary features followed by a rotation forest regressor. DrCIF is a tree ensemble that creates features from summary statistics over random i
&lt;/p&gt;</description></item><item><title>&#21307;&#23398;&#24433;&#20687;&#27169;&#22411;&#32534;&#30721;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#24341;&#21457;&#26377;&#20851;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#23481;&#26131;&#25439;&#22833;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#32771;&#34385;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#30340;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.01397</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#19981;&#21464;&#27169;&#22411;&#21644;&#34920;&#31034;&#26159;&#21542;&#20844;&#24179;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are demographically invariant models and representations in medical imaging fair?. (arXiv:2305.01397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01397
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#27169;&#22411;&#32534;&#30721;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#24341;&#21457;&#26377;&#20851;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#23481;&#26131;&#25439;&#22833;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#32771;&#34385;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#30340;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#21307;&#23398;&#25104;&#20687;&#27169;&#22411;&#22312;&#20854;&#28508;&#22312;&#34920;&#31034;&#20013;&#32534;&#30721;&#20102;&#26377;&#20851;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#23398;&#20449;&#24687;&#65288;&#24180;&#40836;&#12289;&#31181;&#26063;&#12289;&#24615;&#21035;&#65289;&#65292;&#36825;&#24341;&#21457;&#20102;&#26377;&#20851;&#20854;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35810;&#38382;&#26159;&#21542;&#21487;&#34892;&#21644;&#20540;&#24471;&#35757;&#32451;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#19981;&#21516;&#31867;&#22411;&#30340;&#19982;&#20154;&#21475;&#32479;&#35745;&#23398;&#23646;&#24615;&#30340;&#19981;&#21464;&#24615;&#65292;&#21363;&#36793;&#38469;&#12289;&#31867;&#26465;&#20214;&#21644;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#65292;&#24182;&#35828;&#26126;&#23427;&#20204;&#19982;&#31639;&#27861;&#20844;&#24179;&#30340;&#26631;&#20934;&#27010;&#24565;&#30340;&#31561;&#20215;&#24615;&#12290;&#26681;&#25454;&#29616;&#26377;&#29702;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;&#36793;&#38469;&#21644;&#31867;&#26465;&#20214;&#30340;&#19981;&#21464;&#24615;&#21487;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#26576;&#20123;&#20844;&#24179;&#27010;&#24565;&#30340;&#36807;&#24230;&#38480;&#21046;&#26041;&#27861;&#65292;&#23548;&#33268;&#26174;&#33879;&#30340;&#39044;&#27979;&#24615;&#33021;&#25439;&#22833;&#12290;&#20851;&#20110;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#23545;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#23646;&#24615;&#65292;&#23450;&#20041;&#21307;&#23398;&#22270;&#20687;&#21453;&#20107;&#23454;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#29978;&#33267;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging models have been shown to encode information about patient demographics (age, race, sex) in their latent representation, raising concerns about their potential for discrimination. Here, we ask whether it is feasible and desirable to train models that do not encode demographic attributes. We consider different types of invariance with respect to demographic attributes marginal, class-conditional, and counterfactual model invariance - and lay out their equivalence to standard notions of algorithmic fairness. Drawing on existing theory, we find that marginal and class-conditional invariance can be considered overly restrictive approaches for achieving certain fairness notions, resulting in significant predictive performance losses. Concerning counterfactual model invariance, we note that defining medical image counterfactuals with respect to demographic attributes is fraught with complexities. Finally, we posit that demographic encoding may even be considered advantageou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#23398;&#20064;&#27169;&#22411;LogSpecT&#21450;&#20854;&#23454;&#38469;&#20844;&#24335;rLogSpecT&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;rSpecT&#25935;&#24863;&#36229;&#21442;&#25968;&#36873;&#25321;&#12289;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;rLogSpecT&#30340;&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;L-ADMM&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01379</link><description>&lt;p&gt;
LogSpecT: &#20174;&#24179;&#31283;&#20449;&#21495;&#20013;&#23398;&#20064;&#21487;&#34892;&#30340;&#22270;&#24418;&#23398;&#20064;&#27169;&#22411;&#24182;&#20855;&#22791;&#24674;&#22797;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
LogSpecT: Feasible Graph Learning Model from Stationary Signals with Recovery Guarantees. (arXiv:2305.01379v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#23398;&#20064;&#27169;&#22411;LogSpecT&#21450;&#20854;&#23454;&#38469;&#20844;&#24335;rLogSpecT&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;rSpecT&#25935;&#24863;&#36229;&#21442;&#25968;&#36873;&#25321;&#12289;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;rLogSpecT&#30340;&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;L-ADMM&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#21495;&#22270;&#24418;&#23398;&#20064;&#26159;&#22270;&#24418;&#20449;&#21495;&#22788;&#29702;&#65288;GSP&#65289;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#23398;&#20064;&#24179;&#31283;&#20449;&#21495;&#22270;&#24418;&#26368;&#24120;&#29992;&#30340;&#27169;&#22411;&#20043;&#19968;&#26159;SpecT&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#23454;&#38469;&#20844;&#24335;rSpecT&#34987;&#35748;&#20026;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#25935;&#24863;&#65292;&#24182;&#19988;&#26356;&#31967;&#30340;&#26159;&#65292;&#23481;&#26131;&#26080;&#27861;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#32473;&#20986;&#20445;&#35777;rSpecT&#26080;&#27861;&#23454;&#29616;&#30340;&#26465;&#20214;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65288;LogSpecT&#65289;&#21450;&#20854;&#23454;&#38469;&#20844;&#24335;&#65288;rLogSpecT&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;rSpecT&#19981;&#21516;&#65292;&#26032;&#30340;&#23454;&#29992;&#27169;&#22411;rLogSpecT&#22987;&#32456;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;rLogSpecT&#30340;&#24674;&#22797;&#20445;&#35777;&#65292;&#36825;&#20123;&#20445;&#35777;&#26469;&#33258;&#20110;&#19982;epi-converg&#8203;&#8203;ence&#30456;&#20851;&#30340;&#29616;&#20195;&#20248;&#21270;&#24037;&#20855;&#12290;&#36825;&#20123;&#24037;&#20855;&#23545;&#20110;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#37117;&#20855;&#26377;&#29420;&#31435;&#30340;&#21033;&#30410;&#21644;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;rLogSpecT&#22312;&#23454;&#36341;&#20013;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#21270;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#26041;&#27861;&#65288;L-ADMM&#65289;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;L-ADMM&#30340;&#23376;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph learning from signals is a core task in Graph Signal Processing (GSP). One of the most commonly used models to learn graphs from stationary signals is SpecT. However, its practical formulation rSpecT is known to be sensitive to hyperparameter selection and, even worse, to suffer from infeasibility. In this paper, we give the first condition that guarantees the infeasibility of rSpecT and design a novel model (LogSpecT) and its practical formulation (rLogSpecT) to overcome this issue. Contrary to rSpecT, the novel practical model rLogSpecT is always feasible. Furthermore, we provide recovery guarantees of rLogSpecT, which are derived from modern optimization tools related to epi-convergence. These tools could be of independent interest and significant for various learning problems. To demonstrate the advantages of rLogSpecT in practice, a highly efficient algorithm based on the linearized alternating direction method of multipliers (L-ADMM) is proposed. The subproblems of L-ADMM a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;(RFD)&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#35745;&#31639;&#20986;&#27493;&#38271;&#24182;&#19988;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30456;&#21516;&#12290;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RFD&#31639;&#27861;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#25552;&#20986;&#30340;heuristic&#25193;&#23637;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.01377</link><description>&lt;p&gt;
&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;&#27861;
&lt;/p&gt;
&lt;p&gt;
Random Function Descent. (arXiv:2305.01377v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;(RFD)&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#35745;&#31639;&#20986;&#27493;&#38271;&#24182;&#19988;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30456;&#21516;&#12290;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RFD&#31639;&#27861;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#25552;&#20986;&#30340;heuristic&#25193;&#23637;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21313;&#20998;&#24120;&#35265;&#65292;&#20294;&#26159;&#36873;&#25321;&#27491;&#30830;&#30340;&#27493;&#38271;&#32463;&#24120;&#38656;&#35201;&#36827;&#34892;&#8220;&#36229;&#21442;&#25968;&#35843;&#25972;&#8221;&#12290;&#36825;&#26159;&#22240;&#20026;&#22238;&#28335;&#31243;&#24207;&#22914;Armijo's&#20934;&#21017;&#20381;&#36182;&#20110;&#27599;&#20010;&#27493;&#39588;&#20013;&#30340;&#36136;&#37327;&#35780;&#20272;&#65292;&#32780;&#36825;&#20123;&#35780;&#20272;&#22312;&#38543;&#26426;&#24773;&#20917;&#19979;&#19981;&#21487;&#29992;&#12290;&#30001;&#20110;&#20248;&#21270;&#26041;&#26696;&#21487;&#20197;&#29992;Taylor&#36924;&#36817;&#26469;&#35299;&#37322;&#65292;&#25105;&#20204;&#23558;Taylor&#36924;&#36817;&#26367;&#25442;&#20026;&#26465;&#20214;&#26399;&#26395;&#65288;&#26368;&#20339;&#30340;$L^2$&#20272;&#35745;&#65289;&#65292;&#25552;&#20986;&#20102;&#8220;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;&#8221;&#65288;RFD&#65289;&#12290; &#22312;Bayesian&#20248;&#21270;&#20013;&#24120;&#35265;&#30340;&#19968;&#20123;&#36731;&#24494;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RFD&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26159;&#30456;&#21516;&#30340;&#65292;&#20294;&#26159;&#22312;&#38543;&#26426;&#24773;&#20917;&#19979;&#20855;&#26377;&#21487;&#35745;&#31639;&#30340;&#27493;&#38271;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#31639;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#25193;&#23637;&#65292;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
While gradient based methods are ubiquitous in machine learning, selecting the right step size often requires "hyperparameter tuning". This is because backtracking procedures like Armijo's rule depend on quality evaluations in every step, which are not available in a stochastic context. Since optimization schemes can be motivated using Taylor approximations, we replace the Taylor approximation with the conditional expectation (the best $L^2$ estimator) and propose "Random Function Descent" (RFD). Under light assumptions common in Bayesian optimization, we prove that RFD is identical to gradient descent, but with calculable step sizes, even in a stochastic context. We beat untuned Adam in synthetic benchmarks. To close the performance gap to tuned Adam, we propose a heuristic extension competitive with tuned Adam.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#32858;&#21512;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#30446;&#26631;&#35823;&#24046;&#28176;&#36817;&#19981;&#21155;&#20110;&#26410;&#30693;&#26368;&#20339;&#32858;&#21512;&#30340;&#20004;&#20493;&#65292;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#28145;&#24230;&#23884;&#20837;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.01281</link><description>&lt;p&gt;
&#36890;&#36807;&#32858;&#21512;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Parameter Choice Issues in Unsupervised Domain Adaptation by Aggregation. (arXiv:2305.01281v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#32858;&#21512;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#30446;&#26631;&#35823;&#24046;&#28176;&#36817;&#19981;&#21155;&#20110;&#26410;&#30693;&#26368;&#20339;&#32858;&#21512;&#30340;&#20004;&#20493;&#65292;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#28145;&#24230;&#23884;&#20837;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#36873;&#25321;&#31639;&#27861;&#36229;&#21442;&#25968;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#28304;&#22495;&#20013;&#26377;&#26631;&#35760;&#25968;&#25454;&#65292;&#22312;&#30446;&#26631;&#22495;&#20013;&#26377;&#26469;&#33258;&#19981;&#21516;&#36755;&#20837;&#20998;&#24067;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#37319;&#29992;&#35745;&#31639;&#20351;&#29992;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#20960;&#20010;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#28982;&#21518;&#35745;&#31639;&#27169;&#22411;&#30340;&#32447;&#24615;&#32858;&#21512;&#12290;&#34429;&#28982;&#23384;&#22312;&#20960;&#20010;&#36981;&#24490;&#36825;&#31181;&#31574;&#30053;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20294;&#26159;&#20173;&#28982;&#32570;&#23569;&#20381;&#36182;&#20110;&#38480;&#21046;&#30446;&#26631;&#35823;&#24046;&#30340;&#24443;&#24213;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#27861;&#25193;&#23637;&#21040;&#21521;&#37327;&#20540;&#20989;&#25968;&#65288;&#20363;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#30446;&#26631;&#35823;&#24046;&#28176;&#36817;&#19981;&#21155;&#20110;&#26410;&#30693;&#26368;&#20339;&#32858;&#21512;&#30340;&#20004;&#20493;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#35777;&#27604;&#36739;&#30740;&#31350;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#33041;&#30005;&#22270;&#12289;&#36523;&#20307;&#20256;&#24863;&#22120;&#20449;&#21495;&#21644;&#25163;&#26426;&#20449;&#21495;&#31561;&#22810;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#28145;&#24230;&#23884;&#20837;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of choosing algorithm hyper-parameters in unsupervised domain adaptation, i.e., with labeled data in a source domain and unlabeled data in a target domain, drawn from a different input distribution. We follow the strategy to compute several models using different hyper-parameters, and, to subsequently compute a linear aggregation of the models. While several heuristics exist that follow this strategy, methods are still missing that rely on thorough theories for bounding the target error. In this turn, we propose a method that extends weighted least squares to vector-valued functions, e.g., deep neural networks. We show that the target error of the proposed algorithm is asymptotically not worse than twice the error of the unknown optimal aggregation. We also perform a large scale empirical comparative study on several datasets, including text, images, electroencephalogram, body sensor signals and signals from mobile phones. Our method outperforms deep embedded valid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#26080;&#19978;&#38480;&#25968;&#25454;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#20998;&#20301;&#25968;&#21644;&#26368;&#22823;&#20540;&#30340;&#35745;&#31639;&#12290;&#35843;&#29992;&#22522;&#26412;&#30340;&#31232;&#30095;&#21521;&#37327;&#25216;&#26415;&#20013;&#30340;$\texttt{AboveThreshold}$&#23376;&#31243;&#24207;&#21487;&#20197;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#26368;&#39640;&#20998;&#20301;&#25968;&#20272;&#35745;&#65292;&#20174;&#32780;&#24212;&#29992;&#20110;&#23545;&#20110;&#24046;&#20998;&#38544;&#31169;&#27714;&#21644;&#21644;&#22343;&#20540;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#30340;&#25968;&#25454;&#21098;&#20999;&#65292;&#35813;&#25216;&#26415;&#30340;&#38544;&#31169;&#20445;&#38556;&#21487;&#20197;&#36890;&#36807;&#26041;&#27861;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.01177</link><description>&lt;p&gt;
&#26080;&#30028;&#24046;&#20998;&#38544;&#31169;&#20998;&#20301;&#25968;&#21644;&#26368;&#22823;&#20540;&#20272;&#31639;
&lt;/p&gt;
&lt;p&gt;
Unbounded Differentially Private Quantile and Maximum Estimation. (arXiv:2305.01177v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#26080;&#19978;&#38480;&#25968;&#25454;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#20998;&#20301;&#25968;&#21644;&#26368;&#22823;&#20540;&#30340;&#35745;&#31639;&#12290;&#35843;&#29992;&#22522;&#26412;&#30340;&#31232;&#30095;&#21521;&#37327;&#25216;&#26415;&#20013;&#30340;$\texttt{AboveThreshold}$&#23376;&#31243;&#24207;&#21487;&#20197;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#26368;&#39640;&#20998;&#20301;&#25968;&#20272;&#35745;&#65292;&#20174;&#32780;&#24212;&#29992;&#20110;&#23545;&#20110;&#24046;&#20998;&#38544;&#31169;&#27714;&#21644;&#21644;&#22343;&#20540;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#30340;&#25968;&#25454;&#21098;&#20999;&#65292;&#35813;&#25216;&#26415;&#30340;&#38544;&#31169;&#20445;&#38556;&#21487;&#20197;&#36890;&#36807;&#26041;&#27861;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#25968;&#25454;&#30340;&#20998;&#20301;&#25968;&#35745;&#31639;&#20013;&#65292;&#23588;&#20854;&#26159;&#26368;&#39640;&#20998;&#20301;&#25968;&#65288;&#22914;&#26368;&#22823;&#20540;&#65289;&#65292;&#22914;&#20309;&#23454;&#29616;&#23545;&#26080;&#30028;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#35745;&#31639;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#35843;&#29992;&#36845;&#20195;&#35843;&#29992;&#22522;&#26412;&#30340;&#31232;&#30095;&#21521;&#37327;&#25216;&#26415;&#20013;&#30340;$\texttt{AboveThreshold}$&#23376;&#31243;&#24207;&#65292;&#21363;&#21487;&#39640;&#25928;&#22320;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#21363;&#20351;&#25968;&#25454;&#27809;&#26377;&#19978;&#38480;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#27492;&#36807;&#31243;&#21487;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#26368;&#39640;&#20998;&#20301;&#25968;&#20272;&#35745;&#65292;&#20174;&#32780;&#24212;&#29992;&#20110;&#23545;&#20110;&#24046;&#20998;&#38544;&#31169;&#27714;&#21644;&#21644;&#22343;&#20540;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#30340;&#25968;&#25454;&#21098;&#20999;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#20010;&#35843;&#29992;&#22914;&#20309;&#22788;&#29702;&#23436;&#20840;&#26080;&#30028;&#30340;&#25968;&#25454;&#35774;&#32622;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#36827;$\texttt{AboveThreshold}$&#30340;&#20998;&#26512;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24191;&#27867;&#20351;&#29992;&#30340;&#31232;&#30095;&#21521;&#37327;&#25216;&#26415;&#30340;&#38544;&#31169;&#20445;&#38556;&#65288;&#36825;&#26159;&#29420;&#31435;&#20110;&#26412;&#25991;&#30340;&#20869;&#23481;&#65289;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#26356;&#26222;&#36941;&#30340;$\texttt{AboveThreshold}$&#38544;&#31169;&#25439;&#22833;&#29305;&#24449;&#65292;&#24182;&#23637;&#31034;&#20102;&#24046;&#20998;&#38544;&#31169;&#30340;&#26631;&#20934;&#32452;&#21512;&#35268;&#21017;&#21487;&#33021;&#20250;&#39640;&#20272;&#24635;&#20307;&#38544;&#31169;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we consider the problem of differentially private computation of quantiles for the data, especially the highest quantiles such as maximum, but with an unbounded range for the dataset. We show that this can be done efficiently through a simple invocation of $\texttt{AboveThreshold}$, a subroutine that is iteratively called in the fundamental Sparse Vector Technique, even when there is no upper bound on the data. In particular, we show that this procedure can give more accurate and robust estimates on the highest quantiles with applications towards clipping that is essential for differentially private sum and mean estimation. In addition, we show how two invocations can handle the fully unbounded data setting. Within our study, we show that an improved analysis of $\texttt{AboveThreshold}$ can improve the privacy guarantees for the widely used Sparse Vector Technique that is of independent interest. We give a more general characterization of privacy loss for $\texttt{AboveTh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#26680;&#21270;Renyi&#29109;&#65292;&#29992;&#20110;&#22312;&#19981;&#20551;&#35774;Lipschitz&#25110;&#20984;&#24615;&#26465;&#20214;&#30340;&#21069;&#25552;&#19979;&#23545;SGD / SGLD&#31561;&#19979;&#38477;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#39640;&#24403;&#21069;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#30340;&#20248;&#21270;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.01143</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#29702;&#35299;&#65306;&#22522;&#20110;&#26680;&#21270;Renyi&#29109;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Understanding the Generalization Ability of Deep Learning Algorithms: A Kernelized Renyi's Entropy Perspective. (arXiv:2305.01143v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#26680;&#21270;Renyi&#29109;&#65292;&#29992;&#20110;&#22312;&#19981;&#20551;&#35774;Lipschitz&#25110;&#20984;&#24615;&#26465;&#20214;&#30340;&#21069;&#25552;&#19979;&#23545;SGD / SGLD&#31561;&#19979;&#38477;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#39640;&#24403;&#21069;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#30340;&#20248;&#21270;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20449;&#24687;&#29702;&#35770;&#20998;&#26512;&#24050;&#25104;&#20026;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#34892;&#20026;&#30340;&#27969;&#34892;&#26694;&#26550;&#12290;&#23427;&#20801;&#35768;&#23545;&#38543;&#26426;&#26799;&#24230;/ Langevin&#19979;&#38477;&#65288;SGD / SGLD&#65289;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#30452;&#25509;&#20998;&#26512;&#65292;&#32780;&#26080;&#38656;&#35832;&#22914;Lipschitz&#25110;&#20984;&#24615;&#26465;&#20214;&#31561;&#24378;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#30340;&#24403;&#21069;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#20173;&#28982;&#36828;&#38750;&#26368;&#20248;&#65292;&#32780;&#23545;&#36825;&#20123;&#30028;&#38480;&#30340;&#23454;&#36136;&#24615;&#25913;&#36827;&#30001;&#20110;&#39640;&#32500;&#20449;&#24687;&#37327;&#30340;&#19981;&#21487;&#35745;&#31639;&#24615;&#32780;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#12290;&#35770;&#34913;&#37327;&#65306;&#22522;&#20110;&#26680;&#21270;Renyi&#29109;&#65292;&#21033;&#29992;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#31639;&#23376;&#34920;&#31034;&#12290;&#23427;&#32487;&#25215;&#20102;&#39321;&#20892;&#29109;&#30340;&#23646;&#24615;&#65292;&#21487;&#36890;&#36807;&#31616;&#21333;&#30340;&#38543;&#26426;&#25277;&#26679;&#36827;&#34892;&#26377;&#25928;&#35745;&#31639;&#65292;&#21516;&#26102;&#20445;&#25345;&#29420;&#31435;&#20110;&#36755;&#20837;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#26680;&#21270;Renyi&#29109;&#19979;&#24314;&#31435;&#20102;SGD / SGLD&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#20854;&#20013;&#30456;&#20114;&#20449;&#24687;...
&lt;/p&gt;
&lt;p&gt;
Recently, information theoretic analysis has become a popular framework for understanding the generalization behavior of deep neural networks. It allows a direct analysis for stochastic gradient/Langevin descent (SGD/SGLD) learning algorithms without strong assumptions such as Lipschitz or convexity conditions. However, the current generalization error bounds within this framework are still far from optimal, while substantial improvements on these bounds are quite challenging due to the intractability of high-dimensional information quantities. To address this issue, we first propose a novel information theoretical measure: kernelized Renyi's entropy, by utilizing operator representation in Hilbert space. It inherits the properties of Shannon's entropy and can be effectively calculated via simple random sampling, while remaining independent of the input dimension. We then establish the generalization error bounds for SGD/SGLD under kernelized Renyi's entropy, where the mutual informati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01094</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performative Prediction with Bandit Feedback: Learning through Reparameterization. (arXiv:2305.01094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#30001;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#30340;&#24773;&#24418;&#19979;&#39044;&#27979;&#30340;&#19968;&#20010;&#26694;&#26550;&#8212;&#8212;&#23454;&#29616;&#24335;&#39044;&#27979;&#12290;&#29616;&#26377;&#30740;&#31350;&#30340;&#37325;&#28857;&#22312;&#20110;&#20248;&#21270;&#20934;&#30830;&#24615;&#65292;&#20294;&#26159;&#20854;&#20551;&#35774;&#24448;&#24448;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#28385;&#36275;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#23618;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#23454;&#29616;&#24335;&#39044;&#27979;&#30446;&#26631;&#65292;&#20174;&#32780;&#23558;&#38750;&#20984;&#30340;&#30446;&#26631;&#36716;&#21270;&#20026;&#20984;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performative prediction, as introduced by Perdomo et al. (2020), is a framework for studying social prediction in which the data distribution itself changes in response to the deployment of a model. Existing work on optimizing accuracy in this setting hinges on two assumptions that are easily violated in practice: that the performative risk is convex over the deployed model, and that the mapping from the model to the data distribution is known to the model designer in advance. In this paper, we initiate the study of tractable performative prediction problems that do not require these assumptions. To tackle this more challenging setting, we develop a two-level zeroth-order optimization algorithm, where one level aims to compute the distribution map, and the other level reparameterizes the performative prediction objective as a function of the induced data distribution. Under mild conditions, this reparameterization allows us to transform the non-convex objective into a convex one and ac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#12289;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#27867;&#21270;&#38590;&#24230;&#30340;&#26041;&#27861;&#8212;&#8212;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#20294;&#35201;&#27714;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2305.01034</link><description>&lt;p&gt;
&#26080;&#29305;&#23450;&#27169;&#22411;&#27867;&#21270;&#38590;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic Measure of Generalization Difficulty. (arXiv:2305.01034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#12289;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#27867;&#21270;&#38590;&#24230;&#30340;&#26041;&#27861;&#8212;&#8212;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#20294;&#35201;&#27714;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24230;&#37327;&#26159;&#20854;&#21487;&#20197;&#25191;&#34892;&#30340;&#20219;&#21153;&#38590;&#24230;&#65292;&#36275;&#22815;&#22256;&#38590;&#30340;&#20219;&#21153;&#26159;&#24378;&#22823;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#30340;&#27867;&#21270;&#38590;&#24230;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25454;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23545;&#20219;&#21153;&#22266;&#26377;&#27867;&#21270;&#38590;&#24230;&#30340;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#12290;&#36890;&#36807;&#27979;&#37327;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#20551;&#35774;&#22312;&#20219;&#21153;&#20013;&#27867;&#21270;&#30340;&#20998;&#25968;&#21344;&#25454;&#30340;&#23481;&#31215;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23427;&#19982;&#27169;&#22411;&#24517;&#39035;&#27867;&#21270;&#30340;&#31354;&#38388;&#30340;&#20869;&#22312;&#32500;&#25968;&#25104;&#25351;&#25968;&#27604;&#20363;&#65292;&#20294;&#20165;&#22312;&#27599;&#20010;&#32500;&#24230;&#30340;&#20998;&#36776;&#29575;&#19978;&#21576;&#22810;&#39033;&#24335;&#27604;&#20363;&#65292;&#34920;&#26126;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#30340;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
The measure of a machine learning algorithm is the difficulty of the tasks it can perform, and sufficiently difficult tasks are critical drivers of strong machine learning models. However, quantifying the generalization difficulty of machine learning benchmarks has remained challenging. We propose what is to our knowledge the first model-agnostic measure of the inherent generalization difficulty of tasks. Our inductive bias complexity measure quantifies the total information required to generalize well on a task minus the information provided by the data. It does so by measuring the fractional volume occupied by hypotheses that generalize on a task given that they fit the training data. It scales exponentially with the intrinsic dimensionality of the space over which the model must generalize but only polynomially in resolution per dimension, showing that tasks which require generalizing over many dimensions are drastically more difficult than tasks involving more detail in fewer dimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20174;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#22359;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#22270;&#32858;&#31867;&#21644;&#23884;&#20837;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00979</link><description>&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#22359;&#27169;&#22411;&#20013;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Spectral clustering in the Gaussian mixture block model. (arXiv:2305.00979v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20174;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#22359;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#22270;&#32858;&#31867;&#21644;&#23884;&#20837;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#22359;&#27169;&#22411;&#26159;&#29992;&#20110;&#27169;&#25311;&#29616;&#20195;&#32593;&#32476;&#30340;&#22270;&#20998;&#24067;&#65306;&#23545;&#20110;&#36825;&#26679;&#30340;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#22270;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#39030;&#28857; $i$ &#19982;&#19968;&#20010;&#20174;&#39640;&#26031;&#28151;&#21512;&#20013;&#25277;&#26679;&#21040;&#30340;&#28508;&#22312;&#29305;&#24449;&#21521;&#37327; $u_i \in \mathbb{R}^d$ &#30456;&#20851;&#32852;&#65292;&#24403;&#19988;&#20165;&#24403;&#29305;&#24449;&#21521;&#37327;&#36275;&#22815;&#30456;&#20284;&#65292;&#21363; $\langle u_i,u_j \rangle \ge \tau$ &#26102;&#65292;&#25105;&#20204;&#25165;&#20250;&#28155;&#21152;&#36793; $(i,j)$&#12290;&#39640;&#26031;&#28151;&#21512;&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#34920;&#31034;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#65292;&#20363;&#22914;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#37117;&#34920;&#31034;&#29420;&#29305;&#31038;&#21306;&#30340;&#19981;&#21516;&#23646;&#24615;&#12290;&#36825;&#20123;&#32593;&#32476;&#28041;&#21450;&#21040;&#30340;&#33258;&#28982;&#31639;&#27861;&#20219;&#21153;&#26377;&#23884;&#20837;&#65288;&#24674;&#22797;&#28508;&#22312;&#30340;&#29305;&#24449;&#21521;&#37327;&#65289;&#21644;&#32858;&#31867;&#65288;&#36890;&#36807;&#20854;&#28151;&#21512;&#32452;&#20998;&#23558;&#33410;&#28857;&#20998;&#32452;&#65289;&#12290;&#26412;&#25991;&#24320;&#21551;&#20102;&#23545;&#20174;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#22359;&#27169;&#22411;&#25277;&#26679;&#30340;&#22270;&#36827;&#34892;&#32858;&#31867;&#21644;&#23884;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian mixture block models are distributions over graphs that strive to model modern networks: to generate a graph from such a model, we associate each vertex $i$ with a latent feature vector $u_i \in \mathbb{R}^d$ sampled from a mixture of Gaussians, and we add edge $(i,j)$ if and only if the feature vectors are sufficiently similar, in that $\langle u_i,u_j \rangle \ge \tau$ for a pre-specified threshold $\tau$. The different components of the Gaussian mixture represent the fact that there may be different types of nodes with different distributions over features -- for example, in a social network each component represents the different attributes of a distinct community. Natural algorithmic tasks associated with these networks are embedding (recovering the latent feature vectors) and clustering (grouping nodes by their mixture component).  In this paper we initiate the study of clustering and embedding graphs sampled from high-dimensional Gaussian mixture block models, where the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#21270;&#23618;&#8212;&#8212;ContraNorm&#65292;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#24335;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#30772;&#22351;&#34920;&#31034;&#65292;&#32531;&#35299;&#20102;&#23436;&#20840;&#22604;&#38519;&#21644;&#32500;&#24230;&#22604;&#38519;&#30340;&#29616;&#35937;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.06562</link><description>&lt;p&gt;
ContraNorm: &#23545;&#20110;&#36807;&#24230;&#24179;&#28369;&#30340;&#23545;&#27604;&#23398;&#20064;&#35270;&#35282;&#21644;&#26356;&#22810;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond. (arXiv:2303.06562v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#21270;&#23618;&#8212;&#8212;ContraNorm&#65292;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#24335;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#30772;&#22351;&#34920;&#31034;&#65292;&#32531;&#35299;&#20102;&#23436;&#20840;&#22604;&#38519;&#21644;&#32500;&#24230;&#22604;&#38519;&#30340;&#29616;&#35937;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;&#22312;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24403;&#23618;&#25968;&#22686;&#21152;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#21464;&#24046;&#12290;&#25105;&#20204;&#20174;&#32500;&#24230;&#25240;&#21472;&#30340;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#35270;&#35282;&#26469;&#25551;&#36848;&#36807;&#24230;&#24179;&#28369;&#30340;&#29616;&#35937;&#65292;&#34920;&#31034;&#20250;&#32858;&#21040;&#19968;&#20010;&#29421;&#31364;&#30340;&#38181;&#24418;&#31354;&#38388;&#20013;&#65292;&#32780;&#19981;&#26159;&#34920;&#31034;&#20250;&#32858;&#21040;&#19968;&#20010;&#28857;&#19978;&#12290;&#21463;&#21040;&#23545;&#25239;&#24615;&#23398;&#20064;&#22312;&#38450;&#27490;&#32500;&#24230;&#25240;&#21472;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#21270;&#23618;&#8212;&#8212;ContraNorm&#12290;&#30452;&#35266;&#19978;&#65292;ContraNorm&#20250;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#38544;&#24335;&#30772;&#22351;&#34920;&#31034;&#65292;&#23548;&#33268;&#26356;&#22343;&#21248;&#30340;&#20998;&#24067;&#21644;&#36731;&#24494;&#30340;&#32500;&#24230;&#25240;&#21472;&#12290;&#22312;&#29702;&#35770;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;ContraNorm&#21487;&#20197;&#32531;&#35299;&#23436;&#20840;&#22604;&#38519;&#21644;&#32500;&#24230;&#22604;&#38519;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35268;&#33539;&#21270;&#23618;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;GNNs&#21644;Transformers&#20013;&#65292;&#19988;&#21442;&#25968;&#24320;&#38144;&#24456;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#21487;&#20197;&#25552;&#39640;GNNs&#21644;Transformers&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oversmoothing is a common phenomenon in a wide range of Graph Neural Networks (GNNs) and Transformers, where performance worsens as the number of layers increases. Instead of characterizing oversmoothing from the view of complete collapse in which representations converge to a single point, we dive into a more general perspective of dimensional collapse in which representations lie in a narrow cone. Accordingly, inspired by the effectiveness of contrastive learning in preventing dimensional collapse, we propose a novel normalization layer called ContraNorm. Intuitively, ContraNorm implicitly shatters representations in the embedding space, leading to a more uniform distribution and a slighter dimensional collapse. On the theoretical analysis, we prove that ContraNorm can alleviate both complete collapse and dimensional collapse under certain conditions. Our proposed normalization layer can be easily integrated into GNNs and Transformers with negligible parameter overhead. Experiments o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;GCDTC&#65292;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#21644;&#24191;&#20041;CP&#20998;&#35299;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#65307;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;SPTC&#65292;&#20316;&#20026;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05881</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#25968;&#20540;&#20808;&#39564;&#30340;&#24191;&#20041;CP&#20998;&#35299;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition. (arXiv:2302.05881v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;GCDTC&#65292;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#21644;&#24191;&#20041;CP&#20998;&#35299;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#65307;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;SPTC&#65292;&#20316;&#20026;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#34917;&#20840;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#36825;&#19968;&#31867;&#21035;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#23545;&#34917;&#20840;&#24352;&#37327;&#26045;&#21152;&#20302;&#31209;&#32467;&#26500;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23578;&#26410;&#32771;&#34385;&#21040;&#24352;&#37327;&#20803;&#32032;&#30340;&#25968;&#20540;&#20808;&#39564;&#20449;&#24687;&#12290;&#24573;&#30053;&#25968;&#20540;&#20808;&#39564;&#23558;&#23548;&#33268;&#20002;&#22833;&#20851;&#20110;&#25968;&#25454;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#22240;&#27492;&#38459;&#27490;&#31639;&#27861;&#36798;&#21040;&#26368;&#20248;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#35797;&#22270;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#21517;&#20026;GCDTC&#65288;&#24191;&#20041;CP&#20998;&#35299;&#24352;&#37327;&#34917;&#20840;&#65289;&#65292;&#20197;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#12290;&#22312;&#36825;&#20010;&#26032;&#24341;&#20837;&#30340;&#26694;&#26550;&#20013;&#65292;&#23558;&#24191;&#20041;&#30340;CP&#20998;&#35299;&#24212;&#29992;&#20110;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPTC&#65288;&#24179;&#28369;&#27850;&#26494;&#24352;&#37327;&#34917;&#20840;&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38750;&#36127;&#25972;&#25968;&#24352;&#37327;&#34917;&#20840;&#65292;&#20316;&#20026;GCDTC&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#30340;&#24352;&#37327;&#34917;&#20840;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor completion is important to many areas such as computer vision, data analysis, and signal processing. Enforcing low-rank structures on completed tensors, a category of methods known as low-rank tensor completion has recently been studied extensively. While such methods attained great success, none considered exploiting numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. This work attempts to construct a new methodological framework called GCDTC (Generalized CP Decomposition Tensor Completion) for leveraging numerical priors and achieving higher accuracy in tensor completion. In this newly introduced framework, a generalized form of CP Decomposition is applied to low-rank tensor completion. This paper also proposes an algorithm known as SPTC (Smooth Poisson Tensor Completion) for nonnegative integer tensor completion as an instantiation of the G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#26469;&#26816;&#27979;&#21464;&#28857;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.17312</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#24207;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Training Neural Networks for Sequential Change-point Detection. (arXiv:2210.17312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#26469;&#26816;&#27979;&#21464;&#28857;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#25968;&#25454;&#27969;&#20013;&#30340;&#31361;&#21464;&#20998;&#24067;&#36716;&#25442;&#65292;&#21363;&#25152;&#35859;&#30340;&#21464;&#28857;&#26816;&#27979;&#65292;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#65292;&#24403;&#21457;&#29983;&#21464;&#28857;&#26102;&#65292;&#35813;&#37327;&#20250;&#26174;&#33879;&#21464;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#21464;&#28857;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting an abrupt distributional shift of a data stream, known as change-point detection, is a fundamental problem in statistics and machine learning. We introduce a novel approach for online change-point detection using neural networks. To be specific, our approach is training neural networks to compute the cumulative sum of a detection statistic sequentially, which exhibits a significant change when a change-point occurs. We demonstrated the superiority and potential of the proposed method in detecting change-point using both synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>Transformer&#27169;&#22411;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#20854;&#24490;&#29615;&#21160;&#24577;&#65292;&#21487;&#20197;&#20351;&#29992;&#27604;&#25512;&#29702;&#27493;&#39588;&#26356;&#23569;&#30340;&#23618;&#25968;&#25191;&#34892;&#20219;&#20309;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#35745;&#31639;&#12290;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340; $O(\log T)$ &#28145;&#24230;&#35299;&#20915;&#26041;&#26696;&#22987;&#32456;&#23384;&#22312;&#65292;&#32780;&#19988;$O(1)$&#28145;&#24230;&#27169;&#25311;&#22120;&#26159;&#38750;&#24120;&#26222;&#36941;&#30340;&#12290;</title><link>http://arxiv.org/abs/2210.10749</link><description>&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#33258;&#21160;&#26426;&#30340;&#24555;&#25463;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Transformers Learn Shortcuts to Automata. (arXiv:2210.10749v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10749
&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#20854;&#24490;&#29615;&#21160;&#24577;&#65292;&#21487;&#20197;&#20351;&#29992;&#27604;&#25512;&#29702;&#27493;&#39588;&#26356;&#23569;&#30340;&#23618;&#25968;&#25191;&#34892;&#20219;&#20309;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#35745;&#31639;&#12290;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340; $O(\log T)$ &#28145;&#24230;&#35299;&#20915;&#26041;&#26696;&#22987;&#32456;&#23384;&#22312;&#65292;&#32780;&#19988;$O(1)$&#28145;&#24230;&#27169;&#25311;&#22120;&#26159;&#38750;&#24120;&#26222;&#36941;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#25512;&#29702;&#38656;&#35201;&#35745;&#31639;&#27169;&#22411;&#30340;&#24490;&#29615;&#33021;&#21147;&#65292;&#22914;&#22270;&#28789;&#26426;&#31561;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22411;&#34429;&#28982;&#32570;&#20047;&#24490;&#29615;&#33021;&#21147;&#65292;&#20294;&#33021;&#22815;&#20351;&#29992;&#27604;&#25512;&#29702;&#27493;&#39588;&#26356;&#23569;&#30340;&#23618;&#25968;&#25191;&#34892;&#27492;&#31867;&#25512;&#29702;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#27973;&#23618;&#27425;&#21644;&#38750;&#24490;&#29615;&#27169;&#22411;&#23398;&#21040;&#20102;&#20160;&#20040;&#35299;&#20915;&#26041;&#26696;&#65311;&#25105;&#20204;&#21457;&#29616;&#65292;&#20302;&#28145;&#24230;Transformer&#21487;&#20197;&#36890;&#36807;&#36880;&#23618;&#37325;&#26032;&#21442;&#25968;&#21270;&#20854;&#24490;&#29615;&#21160;&#24577;&#65292;&#34920;&#31034;&#20219;&#20309;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;&#22240;&#27492;&#65292;&#20219;&#20309;&#26377;&#30028;&#20869;&#23384;&#31639;&#27861;&#65289;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#24449;&#20102;&#24555;&#25463;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#20855;&#26377; $o(T)$ &#23618;&#30340;Transformer&#21487;&#20197;&#31934;&#30830;&#22797;&#21046;&#33258;&#21160;&#26426;&#22312;&#38271;&#24230;&#20026; $T$ &#30340;&#36755;&#20837;&#24207;&#21015;&#19978;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340; $O(\log T)$ &#28145;&#24230;&#35299;&#20915;&#26041;&#26696;&#22987;&#32456;&#23384;&#22312;&#65307;&#27492;&#22806;&#65292;$O(1)$ &#28145;&#24230;&#27169;&#25311;&#22120;&#38750;&#24120;&#26222;&#36941;&#65292;&#21487;&#20197;&#20351;&#29992;&#20174; Krohn-Rhodes &#29702;&#35770;&#21644;&#30005;&#36335;&#22797;&#26434;&#24230;&#29702;&#35770;&#20013;&#30340;&#24037;&#20855;&#26469;&#29702;&#35299;&#12290;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
Algorithmic reasoning requires capabilities which are most naturally understood through recurrent models of computation, like the Turing machine. However, Transformer models, while lacking recurrence, are able to perform such reasoning using far fewer layers than the number of reasoning steps. This raises the question: what solutions are learned by these shallow and non-recurrent models? We find that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics. Our theoretical results characterize shortcut solutions, whereby a Transformer with $o(T)$ layers can exactly replicate the computation of an automaton on an input sequence of length $T$. We find that polynomial-sized $O(\log T)$-depth solutions always exist; furthermore, $O(1)$-depth simulators are surprisingly common, and can be understood using tools from Krohn-Rhodes theory and circuit complexity. Empirical
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#25968;&#25454;&#30340;&#26465;&#20214;&#29305;&#24449;&#37325;&#35201;&#24615;&#26694;&#26550;&#65292;&#20351;&#29992;&#26465;&#20214;&#39044;&#27979;&#24433;&#21709;&#21644;&#39034;&#24207;knockoff&#25277;&#26679;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#24456;&#23569;&#35752;&#35770;&#30340;&#26465;&#20214;&#21644;&#36793;&#32536;&#24230;&#37327;&#20043;&#38388;&#30340;&#37325;&#35201;&#21306;&#21035;&#65292;&#24182;&#25581;&#31034;&#20986;&#20026;&#27979;&#35797;&#26465;&#20214;FI&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#21487;&#29992;&#19988;&#36807;&#21435;&#20174;&#19994;&#32773;&#30001;&#20110;&#25968;&#25454;&#35201;&#27714;&#19981;&#21305;&#37197;&#32780;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2210.03047</link><description>&lt;p&gt;
&#28151;&#21512;&#25968;&#25454;&#30340;&#26465;&#20214;&#29305;&#24449;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Conditional Feature Importance for Mixed Data. (arXiv:2210.03047v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#25968;&#25454;&#30340;&#26465;&#20214;&#29305;&#24449;&#37325;&#35201;&#24615;&#26694;&#26550;&#65292;&#20351;&#29992;&#26465;&#20214;&#39044;&#27979;&#24433;&#21709;&#21644;&#39034;&#24207;knockoff&#25277;&#26679;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#24456;&#23569;&#35752;&#35770;&#30340;&#26465;&#20214;&#21644;&#36793;&#32536;&#24230;&#37327;&#20043;&#38388;&#30340;&#37325;&#35201;&#21306;&#21035;&#65292;&#24182;&#25581;&#31034;&#20986;&#20026;&#27979;&#35797;&#26465;&#20214;FI&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#21487;&#29992;&#19988;&#36807;&#21435;&#20174;&#19994;&#32773;&#30001;&#20110;&#25968;&#25454;&#35201;&#27714;&#19981;&#21305;&#37197;&#32780;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29305;&#24449;&#37325;&#35201;&#24615;&#65288;FI&#65289;&#22312;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#35745;&#20805;&#20998;&#24615;&#24456;&#23569;&#34987;&#35752;&#35770;&#12290;&#20174;&#32479;&#35745;&#35282;&#24230;&#30475;&#65292;&#19968;&#20010;&#20027;&#35201;&#21306;&#21035;&#26159;&#22312;&#35843;&#25972;&#21327;&#21464;&#37327;&#20043;&#21069;&#21644;&#20043;&#21518;&#20998;&#26512;&#21464;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#21363;&#8220;&#36793;&#32536;&#8221;&#21644;&#8220;&#26465;&#20214;&#8221;&#24230;&#37327;&#20043;&#38388;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#36215;&#20102;&#36825;&#31181;&#24456;&#23569;&#34987;&#25215;&#35748;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#21306;&#21035;&#30340;&#27880;&#24847;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27979;&#35797;&#26465;&#20214;FI&#26102;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#21487;&#29992;&#65292;&#32780;&#20174;&#19994;&#32773;&#36807;&#21435;&#30001;&#20110;&#25968;&#25454;&#35201;&#27714;&#19981;&#21305;&#37197;&#32780;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#22823;&#22810;&#25968;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#37117;&#34920;&#29616;&#20986;&#22797;&#26434;&#30340;&#29305;&#24449;&#20381;&#36182;&#24615;&#65292;&#24182;&#21253;&#21547;&#36830;&#32493;&#21644;&#20998;&#31867;&#25968;&#25454;&#65288;&#28151;&#21512;&#25968;&#25454;&#65289;&#12290;&#36825;&#20123;&#23646;&#24615;&#36890;&#24120;&#34987;&#26465;&#20214;FI&#24230;&#37327;&#25152;&#24573;&#30053;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#26465;&#20214;&#39044;&#27979;&#24433;&#21709;&#65288;CPI&#65289;&#26694;&#26550;&#19982;&#39034;&#24207;knockoff&#25277;&#26679;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the popularity of feature importance (FI) measures in interpretable machine learning, the statistical adequacy of these methods is rarely discussed. From a statistical perspective, a major distinction is between analyzing a variable's importance before and after adjusting for covariates i.e., between $\textit{marginal}$ and $\textit{conditional}$ measures. Our work draws attention to this rarely acknowledged, yet crucial distinction and showcases its implications. Further, we reveal that for testing conditional FI, only few methods are available and practitioners have hitherto been severely restricted in method application due to mismatching data requirements. Most real-world data exhibits complex feature dependencies and incorporates both continuous and categorical data (mixed data). Both properties are oftentimes neglected by conditional FI measures. To fill this gap, we propose to combine the conditional predictive impact (CPI) framework with sequential knockoff sampling. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#23558;&#32447;&#24615;&#26102;&#19981;&#21464;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19982;&#22522;&#20110;&#23376;&#31354;&#38388;&#30340;&#26080;&#30417;&#30563;&#38477;&#38454;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#27169;&#22411;&#22312;&#20849;&#20139;&#23376;&#31354;&#38388;&#20013;&#30340;&#34920;&#31034;&#24046;&#24322;&#65292;&#23558;&#22312;&#19968;&#20010;&#39046;&#22495;&#19978;&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#65292;&#29992;&#20110;&#24314;&#31569;&#33021;&#37327;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09456</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#29992;&#20110;&#24314;&#31569;&#33021;&#37327;&#31995;&#32479;&#24314;&#27169;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A physics-based domain adaptation framework for modelling and forecasting building energy systems. (arXiv:2208.09456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#23558;&#32447;&#24615;&#26102;&#19981;&#21464;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19982;&#22522;&#20110;&#23376;&#31354;&#38388;&#30340;&#26080;&#30417;&#30563;&#38477;&#38454;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#27169;&#22411;&#22312;&#20849;&#20139;&#23376;&#31354;&#38388;&#20013;&#30340;&#34920;&#31034;&#24046;&#24322;&#65292;&#23558;&#22312;&#19968;&#20010;&#39046;&#22495;&#19978;&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#65292;&#29992;&#20110;&#24314;&#31569;&#33021;&#37327;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#24314;&#31569;&#33021;&#28304;&#34892;&#20026;&#24314;&#27169;&#21644;&#39044;&#27979;&#26041;&#38754;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#27969;&#34892;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32467;&#26500;&#36890;&#24120;&#24182;&#19981;&#20855;&#26377;&#19982;&#29289;&#29702;&#29616;&#35937;&#30456;&#20851;&#30340;&#26426;&#26800;&#32467;&#26500;&#30456;&#23545;&#24212;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#25104;&#21151;&#22320;&#27867;&#21270;&#20026;&#26410;&#35266;&#27979;&#21040;&#30340;&#26102;&#38388;&#27493;&#21462;&#20915;&#20110;&#25152;&#35266;&#23519;&#21040;&#30340;&#31995;&#32479;&#21160;&#24577;&#22312;&#25968;&#25454;&#20013;&#34920;&#29616;&#30340;&#20195;&#34920;&#24615;&#65292;&#22312;&#25968;&#23383;&#23402;&#29983;&#25511;&#21046;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#30495;&#23454;&#19990;&#30028;&#30340;&#24037;&#31243;&#38382;&#39064;&#20013;&#24456;&#38590;&#24471;&#21040;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#32447;&#24615;&#26102;&#19981;&#21464;&#65288;LTI&#65289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#30340;&#25104;&#25209;&#21442;&#25968;&#27169;&#22411;&#19982;&#22522;&#20110;&#23376;&#31354;&#38388;&#30340;&#26080;&#30417;&#30563;&#38477;&#38454;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#23376;&#31354;&#38388;&#23548;&#21521;&#30340;&#39046;&#22495;&#36866;&#24212;&#65288;SDA&#65289;&#26694;&#26550;&#12290;SDA&#26159;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#20849;&#20139;&#23376;&#31354;&#38388;&#20013;&#30340;&#27169;&#22411;&#34920;&#31034;&#30340;&#24046;&#24322;&#26469;&#23558;&#22312;&#19968;&#20010;&#39046;&#22495;&#19978;&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#12290;&#22312;&#19968;&#20010;&#24314;&#31569;&#33021;&#37327;&#31995;&#32479;&#26696;&#20363;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#33021;&#37327;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art machine-learning-based models are a popular choice for modeling and forecasting energy behavior in buildings because given enough data, they are good at finding spatiotemporal patterns and structures even in scenarios where the complexity prohibits analytical descriptions. However, their architecture typically does not hold physical correspondence to mechanistic structures linked with governing physical phenomena. As a result, their ability to successfully generalize for unobserved timesteps depends on the representativeness of the dynamics underlying the observed system in the data, which is difficult to guarantee in real-world engineering problems such as control and energy management in digital twins. In response, we present a framework that combines lumped-parameter models in the form of linear time-invariant (LTI) state-space models (SSMs) with unsupervised reduced-order modeling in a subspace-based domain adaptation (SDA) framework. SDA is a type of transfer-lear
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#22522;&#30784;&#23398;&#20064;&#22120;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#38469;&#25928;&#30410;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#24212;&#29992;&#33021;&#21147;&#20248;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#21644;&#31616;&#21333;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.01148</link><description>&lt;p&gt;
&#22522;&#20110;Boosting&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Boosted Off-Policy Learning. (arXiv:2208.01148v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01148
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#22522;&#30784;&#23398;&#20064;&#22120;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#38469;&#25928;&#30410;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#24212;&#29992;&#33021;&#21147;&#20248;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#21644;&#31616;&#21333;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26469;&#33258;&#35760;&#24405;&#24335;&#36172;&#21338;&#21453;&#39304;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;Boosting&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#30340;Boosting&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30452;&#25509;&#20248;&#21270;&#20102;&#31574;&#30053;&#39044;&#26399;&#25910;&#30410;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#22914;&#26524;&#22522;&#30784;&#23398;&#20064;&#22120;&#28385;&#36275;&#8220;&#24369;&#8221;&#23398;&#20064;&#26465;&#20214;&#65292;&#37027;&#20040;&#27599;&#19968;&#36718;Boosting&#37117;&#20250;&#20943;&#23567;&#36807;&#37327;&#32463;&#39564;&#39118;&#38505;&#65288;&#21487;&#33021;&#26159;&#25351;&#25968;&#32423;&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#30784;&#23398;&#20064;&#22120;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#25171;&#24320;&#20102;&#24191;&#27867;&#30340;&#22522;&#30784;&#23398;&#20064;&#22120;&#28304;&#65292;&#22914;&#20915;&#31574;&#26641;&#31561;&#65292;&#20855;&#26377;&#23454;&#38469;&#30410;&#22788;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#32487;&#25215;&#20102;&#35768;&#22810;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;Boosting&#31639;&#27861;&#30340;&#20248;&#33391;&#24615;&#36136;&#65288;&#20363;&#22914;&#23545;&#29305;&#24449;&#32553;&#25918;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#40065;&#26834;&#24615;&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#32988;&#36807;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#21644;&#21482;&#26159;&#22238;&#24402;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the first boosting algorithm for off-policy learning from logged bandit feedback. Unlike existing boosting methods for supervised learning, our algorithm directly optimizes an estimate of the policy's expected reward. We analyze this algorithm and prove that the excess empirical risk decreases (possibly exponentially fast) with each round of boosting, provided a ''weak'' learning condition is satisfied by the base learner. We further show how to reduce the base learner to supervised learning, which opens up a broad range of readily available base learners with practical benefits, such as decision trees. Experiments indicate that our algorithm inherits many desirable properties of tree-based boosting algorithms (e.g., robustness to feature scaling and hyperparameter tuning), and that it can outperform off-policy learning with deep neural networks as well as methods that simply regress on the observed rewards.
&lt;/p&gt;</description></item><item><title>LogGENE&#37319;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26694;&#26550;&#39044;&#27979;&#22522;&#22240;&#34920;&#36798;&#27700;&#24179;&#30340;&#23436;&#25972;&#26465;&#20214;&#20998;&#20301;&#25968;&#65292;&#20174;&#32780;&#20026;&#39640;&#36890;&#37327;&#22522;&#22240;&#32452;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#25552;&#20379;&#35299;&#37322;&#21644;&#25253;&#21578;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#40065;&#26834;&#24615;&#24378;&#30340;&#25512;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.09333</link><description>&lt;p&gt;
LogGENE: &#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#21307;&#30103;&#25512;&#29702;&#20219;&#21153;&#30340;&#24179;&#28369;&#26816;&#26597;&#25439;&#22833;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LogGENE: A smooth alternative to check loss for Deep Healthcare Inference Tasks. (arXiv:2206.09333v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09333
&lt;/p&gt;
&lt;p&gt;
LogGENE&#37319;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26694;&#26550;&#39044;&#27979;&#22522;&#22240;&#34920;&#36798;&#27700;&#24179;&#30340;&#23436;&#25972;&#26465;&#20214;&#20998;&#20301;&#25968;&#65292;&#20174;&#32780;&#20026;&#39640;&#36890;&#37327;&#22522;&#22240;&#32452;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#25552;&#20379;&#35299;&#37322;&#21644;&#25253;&#21578;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#40065;&#26834;&#24615;&#24378;&#30340;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#38752;&#30340;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#25366;&#25496;&#22823;&#22411;&#25968;&#25454;&#38598;&#24182;&#20174;&#20013;&#33719;&#24471;&#26657;&#20934;&#30340;&#39044;&#27979;&#20855;&#26377;&#21363;&#26102;&#30456;&#20851;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#34920;&#36798;&#31561;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#19982;&#20856;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#25512;&#26029;&#25216;&#26415;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#33021;&#25552;&#20379;&#35299;&#37322;&#21644;&#25253;&#21578;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26694;&#26550;&#26469;&#39044;&#27979;&#32473;&#23450;&#19968;&#32452;&#22522;&#22240;&#34920;&#36798;&#30340;&#23436;&#25972;&#26465;&#20214;&#20998;&#20301;&#25968;&#12290;&#26465;&#20214;&#20998;&#20301;&#25968;&#38500;&#20102;&#26377;&#21161;&#20110;&#25552;&#20379;&#39044;&#27979;&#30340;&#20016;&#23500;&#35299;&#37322;&#22806;&#65292;&#36824;&#33021;&#22815;&#25269;&#25239;&#27979;&#37327;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#39640;&#36890;&#37327;&#22522;&#22240;&#32452;&#23398;&#20013;&#29305;&#21035;&#37325;&#35201;&#65292;&#36825;&#26159;&#19968;&#20010;&#27491;&#22312;&#24341;&#39046;&#20010;&#24615;&#21270;&#21307;&#30103;&#12289;&#38774;&#21521;&#33647;&#29289;&#35774;&#35745;&#21644;&#20256;&#36882;&#30340;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#39537;&#21160;&#20272;&#35745;&#36807;&#31243;&#30340;&#26816;&#26597;&#25439;&#22833;&#65292;&#22312;&#20998;&#20301;&#25968;&#22238;&#24402;&#20013;&#24182;&#26080;&#19981;&#21516;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining large datasets and obtaining calibrated predictions from tem is of immediate relevance and utility in reliable deep learning. In our work, we develop methods for Deep neural networks based inferences in such datasets like the Gene Expression. However, unlike typical Deep learning methods, our inferential technique, while achieving state-of-the-art performance in terms of accuracy, can also provide explanations, and report uncertainty estimates. We adopt the Quantile Regression framework to predict full conditional quantiles for a given set of housekeeping gene expressions. Conditional quantiles, in addition to being useful in providing rich interpretations of the predictions, are also robust to measurement noise. Our technique is particularly consequential in High-throughput Genomics, an area which is ushering a new era in personalized health care, and targeted drug design and delivery. However, check loss, used in quantile regression to drive the estimation process is not diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20302;&#20445;&#30495;&#27169;&#22411;&#21644;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#30340;&#23398;&#20064;&#65292; &#24182;&#24320;&#21457;&#20102;&#36125;&#21494;&#26031;&#20998;&#23618;&#24314;&#27169;&#26694;&#26550;&#20801;&#35768;&#22810;&#20010;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2206.08201</link><description>&lt;p&gt;
&#36890;&#36807;&#20302;&#20445;&#30495;&#27169;&#22411;&#21644;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#39640;&#26031;&#36807;&#31243;&#23398;&#20064;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#30340;&#29289;&#29702;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning Physics between Digital Twins with Low-Fidelity Models and Physics-Informed Gaussian Processes. (arXiv:2206.08201v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20302;&#20445;&#30495;&#27169;&#22411;&#21644;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#30340;&#23398;&#20064;&#65292; &#24182;&#24320;&#21457;&#20102;&#36125;&#21494;&#26031;&#20998;&#23618;&#24314;&#27169;&#26694;&#26550;&#20801;&#35768;&#22810;&#20010;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#26159;&#19968;&#31181;&#20195;&#34920;&#20010;&#20307;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;&#65292;&#20363;&#22914;&#32452;&#20214;&#12289;&#24739;&#32773;&#25110;&#36807;&#31243;&#12290; &#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24819;&#20174;&#25968;&#25454;&#20013;&#33719;&#21462;&#26377;&#20851;&#20010;&#20307;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#32467;&#21512;&#19981;&#23436;&#32654;&#30340;&#29289;&#29702;&#30693;&#35782;&#65292;&#24182;&#20174;&#20854;&#20182;&#20010;&#20307;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27599;&#20010;&#20010;&#20307;&#30340;&#29289;&#29702;&#21442;&#25968;&#24341;&#36215;&#20852;&#36259;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290; &#22312;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#27169;&#22411;&#20844;&#24335;&#20013;&#24341;&#20837;&#20102;&#27169;&#22411;&#24046;&#24322;&#39033;&#65292;&#20197;&#35299;&#37322;&#20302;&#20445;&#30495;&#27169;&#22411;&#20013;&#32570;&#22833;&#30340;&#29289;&#29702;&#29616;&#35937;&#12290; &#20026;&#20102;&#20801;&#35768;&#20010;&#20307;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#20998;&#23618;&#24314;&#27169;&#26694;&#26550;&#65292;&#20854;&#20013;&#36890;&#36807;&#26032;&#23618;&#23558;&#20010;&#20307;&#27169;&#22411;&#36830;&#25509;&#36215;&#26469;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#19968;&#20010;&#26159;&#20808;&#21069;&#22312;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#29609;&#20855;&#31034;&#20363;&#65292;&#25193;&#23637;&#21040;&#26356;&#22810;&#20010;&#20307;&#30340;&#24773;&#20917;&#65292;&#19968;&#20010;&#26159;&#19982;&#27835;&#30103;&#39640;&#34880;&#21387;&#30456;&#20851;&#30340;&#24515;&#34880;&#31649;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A digital twin is a computer model that represents an individual, for example, a component, a patient or a process. In many situations, we want to gain knowledge about an individual from its data while incorporating imperfect physical knowledge and also learn from data from other individuals. In this paper, we introduce a fully Bayesian methodology for learning between digital twins in a setting where the physical parameters of each individual are of interest. A model discrepancy term is incorporated in the model formulation of each personalized model to account for the missing physics of the low-fidelity model. To allow sharing of information between individuals, we introduce a Bayesian Hierarchical modelling framework where the individual models are connected through a new level in the hierarchy. Our methodology is demonstrated in two case studies, a toy example previously used in the literature extended to more individuals and a cardiovascular model relevant for the treatment of Hyp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#25968;&#25454;&#19978;&#26045;&#21152;&#26356;&#22810;&#27491;&#21017;&#21270;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#24471;&#21040;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#22343;&#20026;&#26368;&#20248;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.03353</link><description>&lt;p&gt;
&#22312;&#19981;&#31283;&#20581;&#26679;&#26412;&#19978;&#26045;&#21152;&#26356;&#22810;&#27491;&#21017;&#21270;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving adversarial robustness by putting more regularizations on less robust samples. (arXiv:2206.03353v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#25968;&#25454;&#19978;&#26045;&#21152;&#26356;&#22810;&#27491;&#21017;&#21270;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#24471;&#21040;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#22343;&#20026;&#26368;&#20248;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#35757;&#32451;&#26159;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#20154;&#31867;&#35270;&#35273;&#26080;&#27861;&#23519;&#35273;&#30340;&#25968;&#25454;&#25200;&#21160;&#19979;&#65292;&#20351;&#32473;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#35823;&#21028;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#24456;&#22909;&#30340;&#35777;&#26126;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30340;&#19968;&#20010;&#26032;&#30340;&#29305;&#28857;&#26159;&#65306;&#23545;&#20110;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#25968;&#25454;&#65292;&#27604;&#20854;&#20182;&#29616;&#26377;&#30340;&#27491;&#21017;&#21270;&#31639;&#27861;&#26356;&#22810;&#22320;&#24212;&#29992;&#27491;&#21017;&#21270;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#19968;&#20010;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#30340;&#27491;&#21017;&#21270;&#31639;&#27861;&#65292;&#23427;&#26469;&#33258;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#39118;&#38505;&#19978;&#30028;&#30340;&#21160;&#26426;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21516;&#26102;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;(&#22312;&#20363;&#23376;&#19978;&#30340;&#20934;&#30830;&#24615;)&#21644;&#40065;&#26834;&#24615;(&#22312;&#23545;&#25239;&#25915;&#20987;&#19978;&#30340;&#20934;&#30830;&#24615;)&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to apply more regularization to data vulnerable to adversarial attacks than other existing regularization algorithms do. Theoretically, we show that our algorithm can be understood as an algorithm of minimizing the regularized empirical risk motivated from a newly derived upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on examples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#21644;&#25506;&#35752;&#20102;&#36793;&#32536;&#20284;&#28982;&#22312;&#26500;&#36896;&#32422;&#26463;&#21644;&#20551;&#35774;&#27979;&#35797;&#26041;&#38754;&#30340;&#26377;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36793;&#32536;&#20284;&#28982;&#20316;&#20026;&#27867;&#21270;&#30340;&#20195;&#29702;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22914;&#20309;&#19982;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30456;&#20851;&#65292;&#21487;&#33021;&#23548;&#33268;&#36229;&#21442;&#25968;&#23398;&#20064;&#20013;&#30340;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2202.11678</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;&#12289;&#36793;&#38469;&#20284;&#28982;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Model Selection, the Marginal Likelihood, and Generalization. (arXiv:2202.11678v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.11678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#21644;&#25506;&#35752;&#20102;&#36793;&#32536;&#20284;&#28982;&#22312;&#26500;&#36896;&#32422;&#26463;&#21644;&#20551;&#35774;&#27979;&#35797;&#26041;&#38754;&#30340;&#26377;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36793;&#32536;&#20284;&#28982;&#20316;&#20026;&#27867;&#21270;&#30340;&#20195;&#29702;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22914;&#20309;&#19982;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30456;&#20851;&#65292;&#21487;&#33021;&#23548;&#33268;&#36229;&#21442;&#25968;&#23398;&#20064;&#20013;&#30340;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#27604;&#36739;&#19982;&#35266;&#27979;&#23436;&#20840;&#19968;&#33268;&#30340;&#20551;&#35774;&#20043;&#38388;&#30340;&#21306;&#21035;&#65311;&#36793;&#38469;&#20284;&#28982;&#65288;&#20134;&#31216;&#20026;&#36125;&#21494;&#26031;&#35777;&#25454;&#65289;&#20316;&#20026;&#29983;&#25104;&#30001;&#20808;&#39564;&#24471;&#21040;&#35266;&#27979;&#32467;&#26524;&#30340;&#27010;&#29575;&#65292;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#32534;&#30721;&#22885;&#21345;&#22982;&#21059;&#20992;&#21407;&#29702;&#12290;&#23613;&#31649;&#24050;&#32463;&#35266;&#23519;&#21040;&#36793;&#38469;&#20284;&#28982;&#21487;&#33021;&#36807;&#25311;&#21512;&#24182;&#19988;&#23545;&#20808;&#39564;&#20551;&#35774;&#24456;&#25935;&#24863;&#65292;&#20294;&#20854;&#22312;&#36229;&#21442;&#25968;&#23398;&#20064;&#21644;&#31163;&#25955;&#27169;&#22411;&#27604;&#36739;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#20808;&#37325;&#28201;&#20102;&#36793;&#38469;&#20284;&#28982;&#30340;&#21560;&#24341;&#20154;&#30340;&#29305;&#28857;&#65292;&#21253;&#25324;&#23398;&#20064;&#32422;&#26463;&#21644;&#20551;&#35774;&#27979;&#35797;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20351;&#29992;&#36793;&#38469;&#20284;&#28982;&#20316;&#20026;&#27867;&#21270;&#30340;&#20195;&#29702;&#23384;&#22312;&#30340;&#27010;&#24565;&#21644;&#23454;&#38469;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36793;&#38469;&#20284;&#28982;&#22914;&#20309;&#19982;&#27867;&#21270;&#21576;&#36127;&#30456;&#20851;&#65292;&#24182;&#23545;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20135;&#29983;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#36229;&#21442;&#25968;&#23398;&#20064;&#20013;&#21487;&#33021;&#23548;&#33268;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We also re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38381;&#21512;&#24314;&#27169;&#26041;&#27861;CD-ROM&#65292;&#29992;&#20110;&#32463;&#20856;&#30340;POD-Galerkin&#38477;&#38454;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38477;&#38454;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.10746</link><description>&lt;p&gt;
CD-ROM&#65306;&#34917;&#20805;&#28145;&#24230;&#20943;&#23569;&#38454;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CD-ROM: Complemented Deep-Reduced Order Model. (arXiv:2202.10746v4 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38381;&#21512;&#24314;&#27169;&#26041;&#27861;CD-ROM&#65292;&#29992;&#20110;&#32463;&#20856;&#30340;POD-Galerkin&#38477;&#38454;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38477;&#38454;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;POD-Galerkin&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#38454;&#25968;&#32422;&#31616;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#27714;&#35299;&#29289;&#29702;&#38382;&#39064;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#39640;&#32500;&#21160;&#21147;&#31995;&#32479;&#22914;Navier-Stokes&#26041;&#31243;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#20135;&#29983;&#19981;&#20934;&#30830;&#19988;&#26377;&#26102;&#19981;&#31283;&#23450;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38381;&#21512;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#32463;&#20856;&#30340;POD-Galerkin&#38477;&#38454;&#27169;&#22411;(ROM)&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#26159;&#26377;&#22522;&#30784;&#30340;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#30740;&#31350;&#24471;&#24403;&#30340;&#36816;&#31639;&#31526;&#12290;&#19982;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#26412;&#25991;&#20013;&#30340;CD-ROM&#26041;&#27861;&#26159;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#36830;&#32493;&#35760;&#24518;&#24418;&#24335;&#65292;&#30001;&#20851;&#20110;&#37096;&#20998;&#35266;&#27979;&#21040;&#30340;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#31616;&#21333;&#20551;&#35774;&#23548;&#20986;&#12290;&#22240;&#27492;&#65292;&#20462;&#27491;&#21518;&#30340;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#22823;&#22810;&#25968;&#32463;&#20856;&#30340;&#26102;&#38388;&#27493;&#36827;&#27169;&#24335;&#36827;&#34892;&#27169;&#25311;&#12290;CD-ROM&#26041;&#27861;&#30340;&#33021;&#21147;&#22312;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#30340;&#20004;&#20010;&#32463;&#20856;&#26696;&#20363;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#34920;&#26126;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38477;&#38454;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model order reduction through the POD-Galerkin method can lead to dramatic gains in terms of computational efficiency in solving physical problems. However, the applicability of the method to non linear high-dimensional dynamical systems such as the Navier-Stokes equations has been shown to be limited, producing inaccurate and sometimes unstable models. This paper proposes a deep learning based closure modeling approach for classical POD-Galerkin reduced order models (ROM). The proposed approach is theoretically grounded, using neural networks to approximate well studied operators. In contrast with most previous works, the present CD-ROM approach is based on an interpretable continuous memory formulation, derived from simple hypotheses on the behavior of partially observed dynamical systems. The final corrected models can hence be simulated using most classical time stepping schemes. The capabilities of the CD-ROM approach are demonstrated on two classical examples from Computational F
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;TUSLA&#31639;&#27861;&#22312;Wasserstein-1&#21644;Wasserstein-2&#36317;&#31163;&#19978;&#30340;&#38750;&#28176;&#36827;&#35823;&#24046;&#30028;&#38480;&#65292;&#36827;&#32780;&#25512;&#23548;&#20102;&#26399;&#26395;&#36807;&#37327;&#39118;&#38505;&#30340;&#38750;&#28176;&#36827;&#20272;&#35745;&#20540;&#12290;&#22312;ReLU&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29702;&#35770;&#21644;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;TUSLA&#31639;&#27861;&#33021;&#22815;&#39640;&#25928;&#19988;&#31934;&#30830;&#22320;&#35299;&#20915;&#27492;&#31867;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2107.08649</link><description>&lt;p&gt;
&#38750;&#20984;&#23398;&#20064;&#20013;TUSLA&#31639;&#27861;&#30340;&#38750;&#28176;&#36827;&#20272;&#35745;&#21450;&#20854;&#22312;ReLU&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Non-asymptotic estimates for TUSLA algorithm for non-convex learning with applications to neural networks with ReLU activation function. (arXiv:2107.08649v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.08649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;TUSLA&#31639;&#27861;&#22312;Wasserstein-1&#21644;Wasserstein-2&#36317;&#31163;&#19978;&#30340;&#38750;&#28176;&#36827;&#35823;&#24046;&#30028;&#38480;&#65292;&#36827;&#32780;&#25512;&#23548;&#20102;&#26399;&#26395;&#36807;&#37327;&#39118;&#38505;&#30340;&#38750;&#28176;&#36827;&#20272;&#35745;&#20540;&#12290;&#22312;ReLU&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29702;&#35770;&#21644;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;TUSLA&#31639;&#27861;&#33021;&#22815;&#39640;&#25928;&#19988;&#31934;&#30830;&#22320;&#35299;&#20915;&#27492;&#31867;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#36229;&#32447;&#24615;&#22686;&#21152;&#21644;&#19981;&#36830;&#32493;&#38543;&#26426;&#26799;&#24230;&#30340;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#23545;Lovas&#31561;&#20154;&#65288;2020&#24180;&#65289;&#24341;&#20837;&#30340;tamed unadjusted stochastic Langevin algorithm&#65288;TUSLA&#65289;&#36827;&#34892;&#20102;&#38750;&#28176;&#36827;&#24615;&#20998;&#26512;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;Wasserstein-1&#21644;Wasserstein-2&#36317;&#31163;&#19978;&#24314;&#31435;&#20102;TUSLA&#31639;&#27861;&#30340;&#38750;&#28176;&#36827;&#35823;&#24046;&#30028;&#38480;&#12290;&#21518;&#19968;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#19968;&#27493;&#25512;&#23548;&#26399;&#26395;&#36807;&#37327;&#39118;&#38505;&#30340;&#38750;&#28176;&#36827;&#20272;&#35745;&#20540;&#12290;&#20026;&#20102;&#35828;&#26126;&#20027;&#35201;&#32467;&#26524;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21253;&#21547;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#36801;&#31227;&#23398;&#20064;&#31034;&#20363;&#65292;&#35813;&#31034;&#20363;&#20195;&#34920;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#33539;&#20363;&#12290;&#25105;&#20204;&#20026;&#19978;&#36848;&#31034;&#20363;&#21576;&#29616;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#37117;&#35777;&#26126;&#20102;TUSLA&#31639;&#27861;&#33021;&#22815;&#39640;&#25928;&#19988;&#31934;&#30830;&#22320;&#35299;&#20915;&#28041;&#21450;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider non-convex stochastic optimization problems where the objective functions have super-linearly growing and discontinuous stochastic gradients. In such a setting, we provide a non-asymptotic analysis for the tamed unadjusted stochastic Langevin algorithm (TUSLA) introduced in Lovas et al. (2020). In particular, we establish non-asymptotic error bounds for the TUSLA algorithm in Wasserstein-1 and Wasserstein-2 distances. The latter result enables us to further derive non-asymptotic estimates for the expected excess risk. To illustrate the applicability of the main results, we consider an example from transfer learning with ReLU neural networks, which represents a key paradigm in machine learning. Numerical experiments are presented for the aforementioned example which support our theoretical findings. Hence, in this setting, we demonstrate both theoretically and numerically that the TUSLA algorithm can solve the optimization problem involving neural networks with ReLU activati
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#19968;&#20123;&#20027;&#35201;&#30340;&#35789;&#21521;&#37327;&#26500;&#24314;&#31574;&#30053;&#65292;&#31216;&#20026;word embeddings&#65292;&#36825;&#20123;&#31574;&#30053;&#22522;&#20110;&#20998;&#24067;&#20551;&#35774;&#65292;&#32534;&#30721;&#20102;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#34987;&#35777;&#26126;&#22312;&#24456;&#22810;NLP&#20219;&#21153;&#20013;&#26159;&#26377;&#29992;&#30340;&#39069;&#22806;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/1901.09069</link><description>&lt;p&gt;
&#35789;&#21521;&#37327;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Word Embeddings: A Survey. (arXiv:1901.09069v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1901.09069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#19968;&#20123;&#20027;&#35201;&#30340;&#35789;&#21521;&#37327;&#26500;&#24314;&#31574;&#30053;&#65292;&#31216;&#20026;word embeddings&#65292;&#36825;&#20123;&#31574;&#30053;&#22522;&#20110;&#20998;&#24067;&#20551;&#35774;&#65292;&#32534;&#30721;&#20102;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#34987;&#35777;&#26126;&#22312;&#24456;&#22810;NLP&#20219;&#21153;&#20013;&#26159;&#26377;&#29992;&#30340;&#39069;&#22806;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21015;&#20986;&#24182;&#25551;&#36848;&#20102;&#36817;&#26399;&#20027;&#35201;&#30340;&#31574;&#30053;&#65292;&#22522;&#20110;&#20998;&#24067;&#20551;&#35774;&#65292;&#29992;&#20110;&#26500;&#24314;&#21333;&#35789;&#30340;&#22266;&#23450;&#38271;&#24230;&#12289;&#23494;&#38598;&#21644;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290; &#36825;&#20123;&#34920;&#31034;&#29616;&#22312;&#36890;&#24120;&#34987;&#31216;&#20026;&#35789;&#21521;&#37327;&#65292;&#24182;&#19988;&#38500;&#20102;&#32534;&#30721;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#22806;&#65292;&#22312;&#35768;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#39069;&#22806;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work lists and describes the main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis. These representations are now commonly called word embeddings and, in addition to encoding surprisingly good syntactic and semantic information, have been proven useful as extra features in many downstream NLP tasks.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;SBM&#27169;&#22411;&#65292;&#21452;&#37325;&#21327;&#21464;&#37327;&#35843;&#25972;&#30340;PCABM&#27169;&#22411;&#28155;&#21152;&#20102;&#20851;&#20110;&#33410;&#28857;&#38388;&#20851;&#31995;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;SCWA&#31639;&#27861;&#23545;PCABM&#27169;&#22411;&#36827;&#34892;&#20102;&#39640;&#25928;&#27714;&#35299;&#12290;&#27169;&#25311;&#23454;&#39564;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#34920;&#26126;PCABM&#27169;&#22411;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/1807.03469</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#37325;&#21327;&#21464;&#37327;&#35843;&#25972;&#30340;&#22359;&#27169;&#22411;&#29992;&#20110;&#31038;&#21306;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Pairwise Covariates-adjusted Block Model for Community Detection. (arXiv:1807.03469v4 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1807.03469
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;SBM&#27169;&#22411;&#65292;&#21452;&#37325;&#21327;&#21464;&#37327;&#35843;&#25972;&#30340;PCABM&#27169;&#22411;&#28155;&#21152;&#20102;&#20851;&#20110;&#33410;&#28857;&#38388;&#20851;&#31995;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;SCWA&#31639;&#27861;&#23545;PCABM&#27169;&#22411;&#36827;&#34892;&#20102;&#39640;&#25928;&#27714;&#35299;&#12290;&#27169;&#25311;&#23454;&#39564;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#34920;&#26126;PCABM&#27169;&#22411;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#32593;&#32476;&#30740;&#31350;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#38543;&#26426;&#22359;&#27169;&#22411;(SBM)&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#27169;&#22411;&#65292;&#24050;&#24320;&#21457;&#20986;&#21508;&#31181;&#20272;&#35745;&#26041;&#27861;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#31038;&#21306;&#26816;&#27979;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;SBM&#21463;&#21040;&#19968;&#31181;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#21363;&#21516;&#19968;&#31038;&#21306;&#20013;&#30340;&#25152;&#26377;&#33410;&#28857;&#37117;&#26159;&#38543;&#26426;&#31561;&#20215;&#30340;&#65292;&#36825;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#21327;&#21464;&#37327;&#35843;&#25972;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;(PCABM)&#65292;&#21363;&#23558;&#21452;&#37325;&#21327;&#21464;&#37327;&#20449;&#24687;&#21512;&#24182;&#21040;SBM&#20013;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21327;&#21464;&#37327;&#31995;&#25968;&#21644;&#31038;&#21306;&#20998;&#37197;&#30340;&#26497;&#22823;&#20284;&#28982;&#20272;&#35745;&#20540;&#12290;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#30340;&#31232;&#30095;&#26465;&#20214;&#19979;&#65292;&#21327;&#21464;&#37327;&#31995;&#25968;&#20272;&#35745;&#21644;&#31038;&#21306;&#20998;&#37197;&#22343;&#19968;&#33268;&#12290;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#35843;&#25972;&#30340;&#35889;&#32858;&#31867;&#65288;SCWA&#65289;&#65292;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;PCABM&#38382;&#39064;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;SCWA&#26816;&#27979;&#31038;&#21306;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#31934;&#30830;&#30340;&#31038;&#21306;&#24674;&#22797;&#12290;&#25968;&#20540;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#34920;&#26126;PCABM&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most fundamental problems in network study is community detection. The stochastic block model (SBM) is a widely used model, for which various estimation methods have been developed with their community detection consistency results unveiled. However, the SBM is restricted by the strong assumption that all nodes in the same community are stochastically equivalent, which may not be suitable for practical applications. We introduce a pairwise covariates-adjusted stochastic block model (PCABM), a generalization of SBM that incorporates pairwise covariate information. We study the maximum likelihood estimates of the coefficients for the covariates as well as the community assignments. It is shown that both the coefficient estimates of the covariates and the community assignments are consistent under suitable sparsity conditions. Spectral clustering with adjustment (SCWA) is introduced to efficiently solve PCABM. Under certain conditions, we derive the error bound of community det
&lt;/p&gt;</description></item></channel></rss>