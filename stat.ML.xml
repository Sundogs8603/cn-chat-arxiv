<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DDPMs)&#37325;&#29616;&#21307;&#23398;&#22270;&#20687;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#19978;&#19979;&#25991;&#27169;&#22411;(SCMs)&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;DDPMs&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#30456;&#20851;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.10817</link><description>&lt;p&gt;
&#35780;&#20272;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#37325;&#29616;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Assessing the capacity of a denoising diffusion probabilistic model to reproduce spatial context. (arXiv:2309.10817v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DDPMs)&#37325;&#29616;&#21307;&#23398;&#22270;&#20687;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#19978;&#19979;&#25991;&#27169;&#22411;(SCMs)&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;DDPMs&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#30456;&#20851;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31867;&#28909;&#38376;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#12290;&#25991;&#29486;&#20013;&#22768;&#31216;&#65292;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#8212;&#8212;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DDPMs)&#22312;&#22270;&#20687;&#21512;&#25104;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#12290;&#30446;&#21069;&#65292;&#36825;&#20123;&#22768;&#26126;&#35201;&#20040;&#36890;&#36807;&#38024;&#23545;&#33258;&#28982;&#22270;&#20687;&#35774;&#35745;&#30340;&#38598;&#21512;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#35201;&#20040;&#36890;&#36807;&#32467;&#26500;&#30456;&#20284;&#24615;&#31561;&#20256;&#32479;&#22270;&#20687;&#36136;&#37327;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#28982;&#38656;&#35201;&#20102;&#35299;DDPMs&#33021;&#22815;&#21487;&#38752;&#22320;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#30456;&#20851;&#20449;&#24687;&#30340;&#31243;&#24230;&#65292;&#21363;&#26412;&#24037;&#20316;&#20013;&#25152;&#31216;&#30340;&#8220;&#31354;&#38388;&#19978;&#19979;&#25991;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#39318;&#27425;&#25253;&#21578;&#20102;&#23545;DDPMs&#23398;&#20064;&#19982;&#21307;&#23398;&#22270;&#20687;&#24212;&#29992;&#30456;&#20851;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#31995;&#32479;&#35780;&#20272;&#12290;&#30740;&#31350;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#20351;&#29992;&#38543;&#26426;&#19978;&#19979;&#25991;&#27169;&#22411;(SCMs)&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35780;&#20272;&#20102;DDPMs&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#30456;&#20851;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a popular family of deep generative models (DGMs). In the literature, it has been claimed that one class of diffusion models -- denoising diffusion probabilistic models (DDPMs) -- demonstrate superior image synthesis performance as compared to generative adversarial networks (GANs). To date, these claims have been evaluated using either ensemble-based methods designed for natural images, or conventional measures of image quality such as structural similarity. However, there remains an important need to understand the extent to which DDPMs can reliably learn medical imaging domain-relevant information, which is referred to as `spatial context' in this work. To address this, a systematic assessment of the ability of DDPMs to learn spatial context relevant to medical imaging applications is reported for the first time. A key aspect of the studies is the use of stochastic context models (SCMs) to produce training data. In this way, the ability of the DDPMs 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#19981;&#21516;&#27169;&#24335;&#30340;&#36861;&#36394;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#26469;&#21306;&#20998;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#21644;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#12290;</title><link>http://arxiv.org/abs/2309.10688</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#19981;&#21516;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
On the different regimes of Stochastic Gradient Descent. (arXiv:2309.10688v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10688
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#19981;&#21516;&#27169;&#24335;&#30340;&#36861;&#36394;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#26469;&#21306;&#20998;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#21644;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#32593;&#32476;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20851;&#38190;&#21442;&#25968;&#26159;&#27599;&#20010;&#27493;&#39588;&#32771;&#34385;&#30340;&#25968;&#25454;&#37327;&#25110;&#25209;&#37327;&#22823;&#23567;B&#20197;&#21450;&#27493;&#38271;&#25110;&#23398;&#20064;&#29575;&#951;&#12290;&#23545;&#20110;&#23567;&#30340;B&#21644;&#22823;&#30340;&#951;&#65292;SGD&#23545;&#24212;&#20110;&#21442;&#25968;&#30340;&#38543;&#26426;&#28436;&#21270;&#65292;&#20854;&#22122;&#22768;&#24133;&#24230;&#30001;&#8220;&#28201;&#24230;&#8221;T=&#951;/B&#25511;&#21046;&#12290;&#28982;&#32780;&#24403;&#25209;&#37327;&#22823;&#23567;B&#8805;B^*&#36275;&#22815;&#22823;&#26102;&#65292;&#36825;&#31181;&#25551;&#36848;&#34987;&#35266;&#23519;&#21040;&#22833;&#25928;&#65292;&#25110;&#32773;&#22312;&#28201;&#24230;&#36275;&#22815;&#23567;&#26102;&#31616;&#21270;&#20026;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#12290;&#29702;&#35299;&#36825;&#20123;&#20132;&#21449;&#21457;&#29983;&#30340;&#20301;&#32622;&#20173;&#28982;&#26159;&#19968;&#20010;&#20013;&#24515;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#19968;&#20010;&#25945;&#24072;-&#23398;&#29983;&#24863;&#30693;&#22120;&#20998;&#31867;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20851;&#38190;&#39044;&#27979;&#20173;&#36866;&#29992;&#20110;&#28145;&#24230;&#32593;&#32476;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;B-&#951;&#24179;&#38754;&#19978;&#33719;&#24471;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#65292;&#23558;&#19977;&#20010;&#21160;&#24577;&#38454;&#27573;&#20998;&#24320;&#65306;&#65288;i&#65289;&#21463;&#28201;&#24230;&#25511;&#21046;&#30340;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#65292;&#65288;ii&#65289;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#21644;
&lt;/p&gt;
&lt;p&gt;
Modern deep networks are trained with stochastic gradient descent (SGD) whose key parameters are the number of data considered at each step or batch size $B$, and the step size or learning rate $\eta$. For small $B$ and large $\eta$, SGD corresponds to a stochastic evolution of the parameters, whose noise amplitude is governed by the `temperature' $T\equiv \eta/B$. Yet this description is observed to break down for sufficiently large batches $B\geq B^*$, or simplifies to gradient descent (GD) when the temperature is sufficiently small. Understanding where these cross-overs take place remains a central challenge. Here we resolve these questions for a teacher-student perceptron classification model, and show empirically that our key predictions still apply to deep networks. Specifically, we obtain a phase diagram in the $B$-$\eta$ plane that separates three dynamical phases: $\textit{(i)}$ a noise-dominated SGD governed by temperature, $\textit{(ii)}$ a large-first-step-dominated SGD and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10639</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#20840;&#23616;${\mathcal L}^2$&#26368;&#23567;&#21270;&#22120;&#30340;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;$L$&#20010;&#38544;&#34255;&#23618;&#65292;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#65292;${\mathcal L}^2$ Schatten&#31867;&#65288;&#25110;Hilbert-Schmidt&#65289;&#25104;&#26412;&#20989;&#25968;&#65292;&#20197;&#21450;&#30456;&#31561;&#32500;&#24230;$Q\geq1$&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#12290;&#38544;&#34255;&#23618;&#20063;&#23450;&#20041;&#22312;${\mathbb R}^{Q}$&#30340;&#31354;&#38388;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#26368;&#26032;&#30340;&#20851;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26524;&#65292;&#22312;$L\geq Q$&#30340;&#24773;&#20917;&#19979;&#26500;&#36896;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#26063;&#26159;&#36864;&#21270;&#30340;&#12290;&#22312;&#36825;&#37324;&#25552;&#21040;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;DL&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#36890;&#36807;&#23545;&#35757;&#32451;&#36755;&#20837;&#30340;&#36882;&#24402;&#25130;&#26029;&#26144;&#23556;&#30340;&#24212;&#29992;&#26469;&#8220;&#25972;&#29702;&#8221;&#35757;&#32451;&#36755;&#20837;&#65292;&#20197;&#26368;&#23567;&#21270;&#22122;&#22768;&#19982;&#20449;&#21495;&#30340;&#27604;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;$2^Q-1$&#20010;&#19981;&#21516;&#30340;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#39044;&#27979;&#21644;&#24314;&#27169;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10553</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#23398;&#20064;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#39044;&#27979;&#30340;&#32852;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Hybrid State Space-based Learning for Sequential Data Prediction with Joint Optimization. (arXiv:2309.10553v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#39044;&#27979;&#21644;&#24314;&#27169;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#32447;&#29615;&#22659;&#20013;&#30340;&#38750;&#32447;&#24615;&#39044;&#27979;/&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#24418;&#24335;&#30340;&#32852;&#21512;&#26426;&#21046;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#20256;&#32479;&#38750;&#32447;&#24615;&#39044;&#27979;&#27169;&#22411;&#20013;&#29305;&#23450;&#39046;&#22495;&#29305;&#24449;&#24037;&#31243;&#38382;&#39064;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#20102;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#32452;&#20998;&#30340;&#26377;&#25928;&#28151;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;&#36882;&#24402;&#32467;&#26500;&#20174;&#21407;&#22987;&#39034;&#24207;&#24207;&#21015;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20256;&#32479;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#22914;&#23395;&#33410;&#24615;&#12289;&#36235;&#21183;&#31561;&#12290;&#19982;&#29616;&#26377;&#30340;&#38598;&#25104;&#25110;&#28151;&#21512;&#27169;&#22411;&#36890;&#24120;&#20197;&#20998;&#31163;&#30340;&#26041;&#24335;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#19981;&#21516;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#65292;&#32780;&#19988;&#30001;&#20110;&#24314;&#27169;&#30340;&#20998;&#38548;&#25110;&#29420;&#31435;&#35757;&#32451;&#32780;&#38750;&#26368;&#20248;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#25991;&#29486;&#20013;&#37319;&#29992;&#32852;&#21512;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#32852;&#21512;&#20248;&#21270;&#22686;&#24378;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LSTM&#65289;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#21644;ARMA&#31995;&#21015;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;SARIMAX&#65289;&#36827;&#34892;&#26377;&#25928;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate nonlinear prediction/regression in an online setting and introduce a hybrid model that effectively mitigates, via a joint mechanism through a state space formulation, the need for domain-specific feature engineering issues of conventional nonlinear prediction models and achieves an efficient mix of nonlinear and linear components. In particular, we use recursive structures to extract features from raw sequential sequences and a traditional linear time series model to deal with the intricacies of the sequential data, e.g., seasonality, trends. The state-of-the-art ensemble or hybrid models typically train the base models in a disjoint manner, which is not only time consuming but also sub-optimal due to the separation of modeling or independent training. In contrast, as the first time in the literature, we jointly optimize an enhanced recurrent neural network (LSTM) for automatic feature extraction from raw data and an ARMA-family time series model (SARIMAX) for effectivel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#25193;&#25955;&#35757;&#32451;&#30446;&#26631;&#20013;&#22686;&#21152;&#22343;&#26041;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10457</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#19982;&#21152;&#26435;&#29983;&#25104;-&#30417;&#30563;&#23398;&#20064;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based speech enhancement with a weighted generative-supervised learning loss. (arXiv:2309.10457v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#25193;&#25955;&#35757;&#32451;&#30446;&#26631;&#20013;&#22686;&#21152;&#22343;&#26041;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#21463;&#21040;&#20851;&#27880;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#30417;&#30563;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#22411;&#23558;&#24178;&#20928;&#35821;&#38899;&#35757;&#32451;&#26679;&#26412;&#36716;&#21270;&#20026;&#20197;&#22122;&#22768;&#35821;&#38899;&#20026;&#20013;&#24515;&#30340;&#39640;&#26031;&#22122;&#22768;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#23398;&#20064;&#19968;&#20010;&#21442;&#25968;&#21270;&#27169;&#22411;&#26469;&#36870;&#36716;&#36825;&#20010;&#36807;&#31243;&#65292;&#26377;&#26465;&#20214;&#22320;&#26681;&#25454;&#22122;&#22768;&#35821;&#38899;&#36827;&#34892;&#39044;&#27979;&#12290;&#19982;&#30417;&#30563;&#26041;&#27861;&#19981;&#21516;&#65292;&#22522;&#20110;&#29983;&#25104;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#36890;&#24120;&#20165;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#25439;&#22833;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#26377;&#26465;&#20214;&#22122;&#22768;&#35821;&#38899;&#30340;&#34701;&#21512;&#19981;&#22815;&#39640;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22343;&#26041;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#26469;&#22686;&#24378;&#21407;&#22987;&#30340;&#25193;&#25955;&#35757;&#32451;&#30446;&#26631;&#65292;&#22312;&#27599;&#27425;&#36870;&#36716;&#36807;&#31243;&#36845;&#20195;&#20013;&#65292;&#27979;&#37327;&#20272;&#35745;&#22686;&#24378;&#35821;&#38899;&#19982;&#30495;&#23454;&#24178;&#20928;&#35821;&#38899;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models have recently gained attention in speech enhancement (SE), providing an alternative to conventional supervised methods. These models transform clean speech training samples into Gaussian noise centered at noisy speech, and subsequently learn a parameterized model to reverse this process, conditionally on noisy speech. Unlike supervised methods, generative-based SE approaches usually rely solely on an unsupervised loss, which may result in less efficient incorporation of conditioned noisy speech. To address this issue, we propose augmenting the original diffusion training objective with a mean squared error (MSE) loss, measuring the discrepancy between estimated enhanced speech and ground-truth clean speech at each reverse process iteration. Experimental results demonstrate the effectiveness of our proposed methodology.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#36890;&#36807;&#23398;&#20064;&#24178;&#20928;&#35821;&#38899;&#20808;&#39564;&#20998;&#24067;&#21644;&#22122;&#22768;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#35821;&#38899;&#20449;&#21495;&#30340;&#25512;&#26029;&#12290;&#36825;&#26159;&#30446;&#21069;&#23578;&#26080;&#30340;&#22788;&#29702;&#35821;&#38899;&#22686;&#24378;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10450</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#24335;&#29983;&#25104;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Unsupervised speech enhancement with diffusion-based generative models. (arXiv:2309.10450v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10450
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#36890;&#36807;&#23398;&#20064;&#24178;&#20928;&#35821;&#38899;&#20808;&#39564;&#20998;&#24067;&#21644;&#22122;&#22768;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#35821;&#38899;&#20449;&#21495;&#30340;&#25512;&#26029;&#12290;&#36825;&#26159;&#30446;&#21069;&#23578;&#26080;&#30340;&#22788;&#29702;&#35821;&#38899;&#22686;&#24378;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#39046;&#22495;&#65292;&#26465;&#20214;&#24471;&#20998;&#22411;&#25193;&#25955;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25512;&#24191;&#21040;&#26410;&#30693;&#26465;&#20214;&#26102;&#21487;&#33021;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#36816;&#20316;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;STFT&#65289;&#22495;&#20013;&#23398;&#20064;&#20102;&#19968;&#20010;&#24178;&#20928;&#35821;&#38899;&#20808;&#39564;&#20998;&#24067;&#65292;&#20351;&#20854;&#33021;&#26080;&#26465;&#20214;&#22320;&#20174;&#39640;&#26031;&#22122;&#22768;&#29983;&#25104;&#24178;&#20928;&#35821;&#38899;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#23398;&#21040;&#30340;&#24178;&#20928;&#35821;&#38899;&#20808;&#39564;&#19982;&#22122;&#22768;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#35821;&#38899;&#22686;&#24378;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#35821;&#38899;&#20449;&#21495;&#25512;&#26029;&#12290;&#22122;&#22768;&#21442;&#25968;&#26159;&#36890;&#36807;&#36845;&#20195;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26041;&#27861;&#19982;&#24178;&#20928;&#35821;&#38899;&#20272;&#35745;&#21516;&#26102;&#23398;&#20064;&#30340;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#24320;&#21457;&#36825;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#35821;&#38899;&#22686;&#24378;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, conditional score-based diffusion models have gained significant attention in the field of supervised speech enhancement, yielding state-of-the-art performance. However, these methods may face challenges when generalising to unseen conditions. To address this issue, we introduce an alternative approach that operates in an unsupervised manner, leveraging the generative power of diffusion models. Specifically, in a training phase, a clean speech prior distribution is learnt in the short-time Fourier transform (STFT) domain using score-based diffusion models, allowing it to unconditionally generate clean speech from Gaussian noise. Then, we develop a posterior sampling methodology for speech enhancement by combining the learnt clean speech prior with a noise model for speech signal inference. The noise parameters are simultaneously learnt along with clean speech estimation through an iterative expectationmaximisation (EM) approach. To the best of our knowledge, this is the first
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#21518;&#37319;&#26679;&#31639;&#27861;&#29992;&#20110;&#26080;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#37319;&#26679;&#21518;&#39564;&#20998;&#24067;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#35821;&#38899;&#22686;&#24378;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10439</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#24402;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#30340;&#21518;&#37319;&#26679;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Posterior sampling algorithms for unsupervised speech enhancement with recurrent variational autoencoder. (arXiv:2309.10439v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#21518;&#37319;&#26679;&#31639;&#27861;&#29992;&#20110;&#26080;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#37319;&#26679;&#21518;&#39564;&#20998;&#24067;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#35821;&#38899;&#22686;&#24378;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#36882;&#24402;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#35813;&#26041;&#27861;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#30456;&#36739;&#20110;&#26377;&#30417;&#30563;&#26041;&#27861;&#26377;&#30528;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#27979;&#35797;&#26102;&#28041;&#21450;&#21040;&#30340;&#36845;&#20195;&#21464;&#20998;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;VEM&#65289;&#36807;&#31243;&#65292;&#20381;&#36182;&#20110;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26391;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#21644;Metropolis-Hasting&#31639;&#27861;&#30340;&#26377;&#25928;&#37319;&#26679;&#25216;&#26415;&#65292;&#29992;&#20110;&#22522;&#20110;EM&#30340;&#35821;&#38899;&#22686;&#24378;&#21644;RVAE&#12290;&#36890;&#36807;&#30452;&#25509;&#23545;EM&#36807;&#31243;&#20013;&#30340;&#38590;&#20197;&#35745;&#31639;&#30340;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#65292;&#25105;&#20204;&#32469;&#36807;&#20102;&#21464;&#20998;&#25512;&#29702;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;VEM&#20197;&#21450;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;VEM&#65292;&#19981;&#20165;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#65292;&#32780;&#19988;&#22312;&#35821;&#38899;&#22686;&#24378;&#25928;&#26524;&#19978;&#20063;&#25552;&#21319;&#20102;&#24456;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the unsupervised speech enhancement problem based on recurrent variational autoencoder (RVAE). This approach offers promising generalization performance over the supervised counterpart. Nevertheless, the involved iterative variational expectation-maximization (VEM) process at test time, which relies on a variational inference method, results in high computational complexity. To tackle this issue, we present efficient sampling techniques based on Langevin dynamics and Metropolis-Hasting algorithms, adapted to the EM-based speech enhancement with RVAE. By directly sampling from the intractable posterior distribution within the EM process, we circumvent the intricacies of variational inference. We conduct a series of experiments, comparing the proposed methods with VEM and a state-of-the-art supervised speech enhancement approach based on diffusion models. The results reveal that our sampling-based algorithms significantly outperform VEM, not only in terms of com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;ReLU-Like&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#22312;&#32039;&#33268;&#22495;&#19978;&#23558;$L^p$&#20989;&#25968;&#20174;$[0,1]^{d_x}$&#36924;&#36817;&#21040;$\mathbb R^{d_y}$&#25152;&#38656;&#30340;&#26368;&#23567;&#23485;&#24230;&#20026;$\max\{d_x,d_y,2\}$&#65292;&#20174;&#32780;&#34920;&#26126;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36924;&#36817;&#27604;&#22312;${\mathbb R^{d_x}}$&#19978;&#30340;&#36924;&#36817;&#26356;&#23481;&#26131;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#21253;&#25324;ReLU&#22312;&#20869;&#30340;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#33268;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;&#19979;&#30028;&#20026;$w_{\min}\ge d_y+1$&#65288;&#24403;$d_x&lt;d_y\le2d_x$&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.10402</link><description>&lt;p&gt;
&#20351;&#29992;ReLU&#32593;&#32476;&#22312;&#32039;&#33268;&#22495;&#19978;&#36827;&#34892;&#36890;&#29992;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;
&lt;/p&gt;
&lt;p&gt;
Minimum width for universal approximation using ReLU networks on compact domain. (arXiv:2309.10402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;ReLU-Like&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#22312;&#32039;&#33268;&#22495;&#19978;&#23558;$L^p$&#20989;&#25968;&#20174;$[0,1]^{d_x}$&#36924;&#36817;&#21040;$\mathbb R^{d_y}$&#25152;&#38656;&#30340;&#26368;&#23567;&#23485;&#24230;&#20026;$\max\{d_x,d_y,2\}$&#65292;&#20174;&#32780;&#34920;&#26126;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36924;&#36817;&#27604;&#22312;${\mathbb R^{d_x}}$&#19978;&#30340;&#36924;&#36817;&#26356;&#23481;&#26131;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#21253;&#25324;ReLU&#22312;&#20869;&#30340;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#33268;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;&#19979;&#30028;&#20026;$w_{\min}\ge d_y+1$&#65288;&#24403;$d_x&lt;d_y\le2d_x$&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#30740;&#31350;&#65292;&#38480;&#21046;&#23485;&#24230;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#24050;&#32463;&#20316;&#20026;&#28145;&#24230;&#38480;&#21046;&#32593;&#32476;&#30340;&#32463;&#20856;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#30340;&#23545;&#20598;&#36827;&#34892;&#30740;&#31350;&#12290;&#24050;&#32463;&#26377;&#20960;&#27425;&#23581;&#35797;&#26469;&#34920;&#24449;&#20351;&#24471;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#25104;&#31435;&#30340;&#26368;&#23567;&#23485;&#24230;$w_{\min}$&#65292;&#20294;&#21482;&#26377;&#24456;&#23569;&#20960;&#20010;&#25214;&#21040;&#20102;&#30830;&#20999;&#30340;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20174;$[0,1]^{d_x}$&#21040;$\mathbb R^{d_y}$&#30340;$L^p$&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;&#65292;&#22914;&#26524;&#28608;&#27963;&#20989;&#25968;&#26159;ReLU-Like&#65288;&#20363;&#22914;ReLU&#65292;GELU&#65292;Softplus&#65289;&#65292;&#37027;&#20040;&#23427;&#30340;&#30830;&#20999;&#20540;&#26159;$\max\{d_x,d_y,2\}$&#12290;&#19982;&#24050;&#30693;&#30340;&#32467;&#26524;$w_{\min}=\max\{d_x+1,d_y\}$&#30456;&#27604;&#65292;&#24403;&#22495;&#20026;${\mathbb R^{d_x}}$&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#39318;&#27425;&#34920;&#26126;&#65292;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36924;&#36817;&#35201;&#27714;&#27604;&#22312;${\mathbb R^{d_x}}$&#19978;&#30340;&#35201;&#27714;&#26356;&#23567;&#12290;&#25105;&#20204;&#25509;&#19979;&#26469;&#21033;&#29992;&#21253;&#25324;ReLU&#22312;&#20869;&#30340;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#19968;&#33268;&#36924;&#36817;&#30340;&#26368;&#23567;&#23485;&#24230;$w_{\min}$&#35777;&#26126;&#20102;&#19968;&#20010;&#19979;&#30028;&#65306;&#22914;&#26524;$d_x&lt;d_y\le2d_x$&#65292;&#21017;$w_{\min}\ge d_y+1$&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;&#20102;&#19968;&#20010;&#20108;&#20998;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The universal approximation property of width-bounded networks has been studied as a dual of the classical universal approximation theorem for depth-bounded ones. There were several attempts to characterize the minimum width $w_{\min}$ enabling the universal approximation property; however, only a few of them found the exact values. In this work, we show that the minimum width for the universal approximation of $L^p$ functions from $[0,1]^{d_x}$ to $\mathbb R^{d_y}$ is exactly $\max\{d_x,d_y,2\}$ if an activation function is ReLU-Like (e.g., ReLU, GELU, Softplus). Compared to the known result $w_{\min}=\max\{d_x+1,d_y\}$ when the domain is ${\mathbb R^{d_x}}$, our result first shows that approximation on a compact domain requires smaller width than on ${\mathbb R^{d_x}}$. We next prove a lower bound on $w_{\min}$ for uniform approximation using general activation functions including ReLU: $w_{\min}\ge d_y+1$ if $d_x&lt;d_y\le2d_x$. Together with our first result, this shows a dichotomy be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2309.10370</link><description>&lt;p&gt;
&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#35299;&#37322;&#65306;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#30001;&#19968;&#20010;&#38544;&#34255;&#23618;&#12289;&#19968;&#20010;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#12289;&#19968;&#20010;${\mathcal L}^2$&#35889;&#33539;&#31867;&#65288;&#25110;&#32773;Hilbert-Schmidt&#65289;&#30340;&#20195;&#20215;&#20989;&#25968;&#12289;&#36755;&#20837;&#31354;&#38388;${\mathbb R}^M$&#12289;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#65288;&#20854;&#20013;$Q\leq M$&#65289;&#65292;&#20197;&#21450;&#35757;&#32451;&#36755;&#20837;&#26679;&#26412;&#25968;&#37327;$N&gt;QM$&#25152;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;$O(\delta_P)$&#30340;&#19978;&#30028;&#65292;&#20854;&#20013;$\delta_P$&#34913;&#37327;&#20102;&#35757;&#32451;&#36755;&#20837;&#30340;&#20449;&#22122;&#27604;&#12290;&#25105;&#20204;&#20351;&#29992;&#36866;&#24212;&#20110;&#23646;&#20110;&#21516;&#19968;&#36755;&#20986;&#21521;&#37327;$y_j$&#30340;&#35757;&#32451;&#36755;&#20837;&#21521;&#37327;$\overline{x_{0,j}}$&#30340;&#25237;&#24433;&#26469;&#33719;&#24471;&#36817;&#20284;&#30340;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;$j=1,\dots,Q$&#12290;&#22312;&#29305;&#27530;&#24773;&#20917;$M=Q$&#19979;&#65292;&#25105;&#20204;&#26126;&#30830;&#30830;&#23450;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#19968;&#20010;&#30830;&#20999;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#65307;&#36825;&#20010;&#23574;&#38160;&#30340;&#20540;&#19982;&#23545;&#20110;$Q\leq M$&#25152;&#33719;&#24471;&#30340;&#19978;&#30028;&#20043;&#38388;&#26377;&#19968;&#20010;&#30456;&#23545;&#35823;&#24046;$O(\delta_P^2)$&#12290;&#19978;&#30028;&#35777;&#26126;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26500;&#36896;&#24615;&#35757;&#32451;&#30340;&#32593;&#32476;&#65307;&#25105;&#20204;&#35777;&#26126;&#23427;&#27979;&#24230;&#20102;$Q$&#32500;&#31354;&#38388;&#20013;&#30340;&#32473;&#23450;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N&gt;QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#20960;&#20309;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#31995;&#32479;&#26469;&#25512;&#23548;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#65288;CRNs&#65289;&#30340;&#21453;&#24212;&#36895;&#29575;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;CRNs&#20013;&#23637;&#31034;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#26356;&#20026;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.10334</link><description>&lt;p&gt;
&#19968;&#33324;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#30340;&#20449;&#24687;&#20960;&#20309;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Information geometric bound on general chemical reaction networks. (arXiv:2309.10334v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10334
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#20960;&#20309;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#31995;&#32479;&#26469;&#25512;&#23548;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#65288;CRNs&#65289;&#30340;&#21453;&#24212;&#36895;&#29575;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;CRNs&#20013;&#23637;&#31034;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#65288;CRNs&#65289;&#30340;&#21160;&#21147;&#23398;&#65292;&#30446;&#26631;&#26159;&#25512;&#23548;&#20986;&#23427;&#20204;&#21453;&#24212;&#36895;&#29575;&#30340;&#19978;&#30028;&#12290;&#30001;&#20110;CRNs&#30340;&#38750;&#32447;&#24615;&#24615;&#36136;&#21644;&#31163;&#25955;&#32467;&#26500;&#65292;&#36825;&#20010;&#20219;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20449;&#24687;&#20960;&#20309;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#26799;&#24230;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#21487;&#20197;&#24471;&#21040;CRN&#21160;&#21147;&#23398;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;CRNs&#20013;&#23637;&#31034;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#20010;&#31867;&#21035;&#30340;&#29305;&#24449;&#21253;&#25324;&#21270;&#23398;&#21697;&#30340;&#25968;&#37327;&#12289;&#21270;&#23398;&#21453;&#24212;&#30340;&#21270;&#23398;&#35745;&#37327;&#31995;&#25968;&#30340;&#26368;&#22823;&#20540;&#21644;&#21453;&#24212;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;CRNs&#30340;&#21453;&#24212;&#36895;&#29575;&#30340;&#19978;&#30028;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;CRNs&#65292;&#20294;&#20174;&#33258;&#28982;&#31185;&#23398;&#21040;&#24037;&#31243;&#23398;&#20013;&#36229;&#22270;&#30340;&#26222;&#36941;&#23384;&#22312;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#33021;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20449;&#24687;&#31185;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the dynamics of chemical reaction networks (CRNs) with the goal of deriving an upper bound on their reaction rates. This task is challenging due to the nonlinear nature and discrete structure inherent in CRNs. To address this, we employ an information geometric approach, using the natural gradient, to develop a nonlinear system that yields an upper bound for CRN dynamics. We validate our approach through numerical simulations, demonstrating faster convergence in a specific class of CRNs. This class is characterized by the number of chemicals, the maximum value of stoichiometric coefficients of the chemical reactions, and the number of reactions. We also compare our method to a conventional approach, showing that the latter cannot provide an upper bound on reaction rates of CRNs. While our study focuses on CRNs, the ubiquity of hypergraphs in fields from natural sciences to engineering suggests that our method may find broader applications, including in information scienc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#26465;&#20214;&#19981;&#21464;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.10301</link><description>&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#26465;&#20214;&#19981;&#21464;&#32452;&#20214;&#30340;&#31361;&#20986;&#20316;&#29992;&#65306;&#29702;&#35770;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms. (arXiv:2309.10301v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10301
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#26465;&#20214;&#19981;&#21464;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#19968;&#20010;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#24403;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#28304;&#25968;&#25454;&#20998;&#24067;&#19982;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#26102;&#20986;&#29616;&#12290;&#34429;&#28982;&#35768;&#22810;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#24050;&#32463;&#35777;&#26126;&#20102;&#30456;&#24403;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#26159;&#30450;&#30446;&#24212;&#29992;&#36825;&#20123;&#31639;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#22312;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#35201;&#30340;&#26159;&#28548;&#28165;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#22312;&#20855;&#22791;&#33391;&#22909;&#30446;&#26631;&#24615;&#33021;&#30340;&#20551;&#35774;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#39044;&#27979;&#20013;&#20855;&#22791;&#26465;&#20214;&#19981;&#21464;&#30340;&#32452;&#20214;&#65288;CICs&#65289;&#30340;&#23384;&#22312;&#20551;&#35774;&#65292;&#36825;&#20123;&#32452;&#20214;&#22312;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#20445;&#25345;&#26465;&#20214;&#19981;&#21464;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CICs&#65292;&#36890;&#36807;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#65288;CIP&#65289;&#21487;&#20197;&#20272;&#35745;&#65292;&#20855;&#22791;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#25552;&#20379;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#30340;&#19977;&#20010;&#31361;&#20986;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CICs&#30340;&#26032;&#31639;&#27861;&#65292;&#21363;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#65288;IW-CIP&#65289;&#65292;&#23427;&#22312;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#26041;&#38754;&#36229;&#36234;&#20102;&#31616;&#21333;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation (DA) is a statistical learning problem that arises when the distribution of the source data used to train a model differs from that of the target data used to evaluate the model. While many DA algorithms have demonstrated considerable empirical success, blindly applying these algorithms can often lead to worse performance on new datasets. To address this, it is crucial to clarify the assumptions under which a DA algorithm has good target performance. In this work, we focus on the assumption of the presence of conditionally invariant components (CICs), which are relevant for prediction and remain conditionally invariant across the source and target data. We demonstrate that CICs, which can be estimated through conditional invariant penalty (CIP), play three prominent roles in providing target risk guarantees in DA. First, we propose a new algorithm based on CICs, importance-weighted conditional invariant penalty (IW-CIP), which has target risk guarantees beyond simple 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#31995;&#32479;&#36716;&#21464;&#36335;&#24452;&#65306;&#19968;&#31181;&#26159;&#36890;&#36807;&#20559;&#32622;&#21407;&#22987;&#21160;&#21147;&#23398;&#26469;&#20419;&#36827;&#36716;&#21464;&#65292;&#21478;&#19968;&#31181;&#26159;&#23558;&#21407;&#22987;&#36716;&#21464;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#36716;&#21464;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#25968;&#25454;&#20016;&#23500;&#21644;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#37117;&#23637;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10276</link><description>&lt;p&gt;
&#29983;&#25104;&#36807;&#28193;&#36335;&#24452;&#30340;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diffusion Methods for Generating Transition Paths. (arXiv:2309.10276v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#31995;&#32479;&#36716;&#21464;&#36335;&#24452;&#65306;&#19968;&#31181;&#26159;&#36890;&#36807;&#20559;&#32622;&#21407;&#22987;&#21160;&#21147;&#23398;&#26469;&#20419;&#36827;&#36716;&#21464;&#65292;&#21478;&#19968;&#31181;&#26159;&#23558;&#21407;&#22987;&#36716;&#21464;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#36716;&#21464;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#25968;&#25454;&#20016;&#23500;&#21644;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#37117;&#23637;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#20351;&#29992;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#27169;&#25311;&#31232;&#26377;&#30340;&#20122;&#31283;&#24577;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36716;&#21464;&#36335;&#24452;&#30340;&#39640;&#25928;&#26041;&#27861;&#23545;&#20110;&#30740;&#31350;&#20998;&#23376;&#31995;&#32479;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#36335;&#24452;&#29983;&#25104;&#26041;&#27861;&#65306;&#22522;&#20110;&#38142;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#20013;&#28857;&#30340;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#36890;&#36807;&#20559;&#32622;&#21407;&#22987;&#21160;&#21147;&#23398;&#26469;&#20419;&#36827;&#36716;&#21464;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#21017;&#37319;&#29992;&#20998;&#35010;&#25216;&#26415;&#24182;&#23558;&#21407;&#22987;&#36716;&#21464;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#36716;&#21464;&#12290;&#22312;M\"uller&#21183;&#21644;&#20108;&#32957;&#22522;&#19993;&#27688;&#37240;&#30340;&#29983;&#25104;&#36716;&#21464;&#36335;&#24452;&#30340;&#25968;&#20540;&#32467;&#26524;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25968;&#25454;&#20016;&#23500;&#21644;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#37117;&#23637;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we seek to simulate rare transitions between metastable states using score-based generative models. An efficient method for generating high-quality transition paths is valuable for the study of molecular systems since data is often difficult to obtain. We develop two novel methods for path generation in this paper: a chain-based approach and a midpoint-based approach. The first biases the original dynamics to facilitate transitions, while the second mirrors splitting techniques and breaks down the original transition into smaller transitions. Numerical results of generated transition paths for the M\"uller potential and for Alanine dipeptide demonstrate the effectiveness of these approaches in both the data-rich and data-scarce regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26367;&#20195;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#20248;&#21270;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10194</link><description>&lt;p&gt;
&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
The Kernel Density Integral Transformation. (arXiv:2309.10194v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26367;&#20195;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#20248;&#21270;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#26041;&#27861;&#20110;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#29305;&#24449;&#39044;&#22788;&#29702;&#32487;&#32493;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32508;&#21512;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#29305;&#24449;&#39044;&#22788;&#29702;&#26041;&#27861;&#20316;&#20026;&#26497;&#38480;&#24773;&#20917;&#65306;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#19981;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#21487;&#20197;&#20316;&#20026;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#23545;&#27599;&#31181;&#26041;&#27861;&#30340;&#24369;&#28857;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#21478;&#22806;&#65292;&#36890;&#36807;&#35843;&#25972;&#19968;&#20010;&#36830;&#32493;&#36229;&#21442;&#25968;&#65292;&#25105;&#20204;&#32463;&#24120;&#20248;&#20110;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#26680;&#23494;&#24230;&#36716;&#25442;&#21487;&#20197;&#26377;&#30410;&#22320;&#24212;&#29992;&#20110;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#20851;&#24615;&#20998;&#26512;&#21644;&#21333;&#21464;&#37327;&#32858;&#31867;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature preprocessing continues to play a critical role when applying machine learning and statistical methods to tabular data. In this paper, we propose the use of the kernel density integral transformation as a feature preprocessing step. Our approach subsumes the two leading feature preprocessing methods as limiting cases: linear min-max scaling and quantile transformation. We demonstrate that, without hyperparameter tuning, the kernel density integral transformation can be used as a simple drop-in replacement for either method, offering robustness to the weaknesses of each. Alternatively, with tuning of a single continuous hyperparameter, we frequently outperform both of these methods. Finally, we show that the kernel density transformation can be profitably applied to statistical data analysis, particularly in correlation analysis and univariate clustering.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#21033;&#29992;&#20960;&#20309;&#32467;&#26500;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20960;&#20309;&#65292;&#23558;&#32479;&#35745;&#20381;&#36182;&#21644;&#29305;&#24449;&#32479;&#19968;&#21040;&#21516;&#19968;&#31354;&#38388;&#20013;&#65292;&#24182;&#20351;&#29992;&#23884;&#22871;&#25216;&#26415;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#21464;&#37327;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.10140</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#30340;&#20960;&#20309;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Geometric Framework for Neural Feature Learning. (arXiv:2309.10140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#21033;&#29992;&#20960;&#20309;&#32467;&#26500;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20960;&#20309;&#65292;&#23558;&#32479;&#35745;&#20381;&#36182;&#21644;&#29305;&#24449;&#32479;&#19968;&#21040;&#21516;&#19968;&#31354;&#38388;&#20013;&#65292;&#24182;&#20351;&#29992;&#23884;&#22871;&#25216;&#26415;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#21464;&#37327;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#23398;&#20064;&#31995;&#32479;&#35774;&#35745;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#24449;&#20960;&#20309;&#65292;&#23427;&#23558;&#32479;&#35745;&#20381;&#36182;&#21644;&#29305;&#24449;&#32479;&#19968;&#21040;&#21516;&#19968;&#20010;&#20855;&#26377;&#20960;&#20309;&#32467;&#26500;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#12290;&#36890;&#36807;&#24212;&#29992;&#29305;&#24449;&#20960;&#20309;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#35299;&#20915;&#30001;&#23398;&#20064;&#35774;&#32622;&#25351;&#23450;&#30340;&#20381;&#36182;&#32452;&#20214;&#30340;&#26368;&#20339;&#29305;&#24449;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;&#25216;&#26415;&#26469;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#26368;&#20339;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#20248;&#21270;&#22120;&#12290;&#20026;&#20102;&#23637;&#31034;&#23884;&#22871;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#22810;&#21464;&#37327;&#23398;&#20064;&#38382;&#39064;&#65292;&#21253;&#25324;&#26465;&#20214;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#20339;&#29305;&#24449;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#19982;&#32463;&#20856;&#26041;&#27861;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel framework for learning system design based on neural feature extractors by exploiting geometric structures in feature spaces. First, we introduce the feature geometry, which unifies statistical dependence and features in the same functional space with geometric structures. By applying the feature geometry, we formulate each learning problem as solving the optimal feature approximation of the dependence component specified by the learning setting. We propose a nesting technique for designing learning algorithms to learn the optimal features from data samples, which can be applied to off-the-shelf network architectures and optimizers. To demonstrate the application of the nesting technique, we further discuss multivariate learning problems, including conditioned inference and multimodal learning, where we present the optimal features and reveal their connections to classical approaches.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#19979;&#65292;&#20219;&#24847;&#20998;&#24067;&#20559;&#31227;&#19968;&#33324;&#19981;&#20855;&#26377;&#19981;&#21464;&#21644;&#31283;&#20581;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#36890;&#36807;&#38480;&#21046;&#20998;&#24067;&#20559;&#31227;&#31867;&#21035;&#21644;&#36873;&#25321;&#35780;&#20272;&#25351;&#26631;&#65292;&#21487;&#20197;&#22312;&#29305;&#23450;&#27169;&#22411;&#20013;&#23454;&#29616;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10083</link><description>&lt;p&gt;
&#19981;&#21464;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Invariant Probabilistic Prediction. (arXiv:2309.10083v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10083
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#19979;&#65292;&#20219;&#24847;&#20998;&#24067;&#20559;&#31227;&#19968;&#33324;&#19981;&#20855;&#26377;&#19981;&#21464;&#21644;&#31283;&#20581;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#36890;&#36807;&#38480;&#21046;&#20998;&#24067;&#20559;&#31227;&#31867;&#21035;&#21644;&#36873;&#25321;&#35780;&#20272;&#25351;&#26631;&#65292;&#21487;&#20197;&#22312;&#29305;&#23450;&#27169;&#22411;&#20013;&#23454;&#29616;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#20998;&#24067;&#21464;&#21270;&#19979;&#34920;&#29616;&#31283;&#20581;&#30340;&#32479;&#35745;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#34429;&#28982;&#22823;&#37096;&#20998;&#30456;&#20851;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#30340;&#28857;&#39044;&#27979;&#19978;&#65292;&#20294;&#26412;&#25991;&#23558;&#28966;&#28857;&#36716;&#21521;&#20102;&#27010;&#29575;&#39044;&#27979;&#65292;&#26088;&#22312;&#20840;&#38754;&#37327;&#21270;&#32473;&#23450;&#21327;&#21464;&#37327;&#30340;&#32467;&#26524;&#21464;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27010;&#29575;&#39044;&#27979;&#22312;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#19979;&#30340;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#24847;&#20998;&#24067;&#20559;&#31227;&#19968;&#33324;&#19981;&#20855;&#26377;&#19981;&#21464;&#21644;&#31283;&#20581;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#19982;&#28857;&#39044;&#27979;&#30340;&#24773;&#20917;&#30456;&#21453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;&#35780;&#20272;&#25351;&#26631;&#24182;&#38480;&#21046;&#20998;&#24067;&#20559;&#31227;&#31867;&#21035;&#65292;&#20197;&#23454;&#29616;&#21407;&#22411;&#39640;&#26031;&#24322;&#26041;&#24046;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#19981;&#21464;&#24615;&#12290;&#22312;&#36825;&#20123;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20135;&#29983;&#19981;&#21464;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a growing interest in statistical methods that exhibit robust performance under distribution changes between training and test data. While most of the related research focuses on point predictions with the squared error loss, this article turns the focus towards probabilistic predictions, which aim to comprehensively quantify the uncertainty of an outcome variable given covariates. Within a causality-inspired framework, we investigate the invariance and robustness of probabilistic predictions with respect to proper scoring rules. We show that arbitrary distribution shifts do not, in general, admit invariant and robust probabilistic predictions, in contrast to the setting of point prediction. We illustrate how to choose evaluation metrics and restrict the class of distribution shifts to allow for identifiability and invariance in the prototypical Gaussian heteroscedastic linear model. Motivated by these findings, we propose a method to yield invariant pro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#26469;&#25506;&#35752;&#38750;&#24179;&#31283;&#26680;&#22312;&#28145;&#23618;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.10068</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#26680;&#23545;&#28145;&#23618;&#39640;&#26031;&#36807;&#31243;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Unifying Perspective on Non-Stationary Kernels for Deeper Gaussian Processes. (arXiv:2309.10068v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#26469;&#25506;&#35752;&#38750;&#24179;&#31283;&#26680;&#22312;&#28145;&#23618;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#29992;&#20110;&#25968;&#25454;&#30340;&#38543;&#26426;&#20989;&#25968;&#36817;&#20284;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#32479;&#35745;&#25216;&#26415;&#12290;&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#30001;&#20110;&#20854;&#20248;&#36234;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#30095;&#24773;&#20917;&#19979;&#65292;&#20197;&#21450;&#20854;&#22266;&#26377;&#30340;&#25552;&#20379;&#24378;&#20581;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;GP&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#26680;&#24515;&#26041;&#27861;&#30340;&#22797;&#26434;&#23450;&#21046;&#65292;&#36825;&#24448;&#24448;&#22312;&#20351;&#29992;&#26631;&#20934;&#35774;&#32622;&#21644;&#29616;&#25104;&#36719;&#20214;&#24037;&#20855;&#26102;&#20351;&#20174;&#19994;&#32773;&#19981;&#28385;&#24847;&#12290;&#21487;&#20197;&#35828;&#65292;GP&#26368;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#26680;&#20989;&#25968;&#65292;&#23427;&#25198;&#28436;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#35282;&#33394;&#12290;Mat\'ern&#31867;&#30340;&#24179;&#31283;&#26680;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#30740;&#31350;&#20013;&#34987;&#20351;&#29992;&#65307;&#20302;&#25928;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#29616;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24448;&#24448;&#26159;&#20854;&#32467;&#26524;&#12290;&#38750;&#24179;&#31283;&#26680;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#26356;&#21152;&#22797;&#26434;&#30340;&#23646;&#24615;&#65292;&#24456;&#23569;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably the most important building block of a GP is the kernel function which assumes the role of a covariance operator. Stationary kernels of the Mat\'ern class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#24674;&#22797;&#22270;&#30340;&#25299;&#25169;&#23646;&#24615;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20063;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09924</link><description>&lt;p&gt;
&#22522;&#20110;&#28909;&#21644;&#27874;&#21160;&#21160;&#21147;&#23398;&#29305;&#24449;&#30340;&#22270;&#25299;&#25169;&#23646;&#24615;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Graph topological property recovery with heat and wave dynamics-based features on graphsD. (arXiv:2309.09924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#24674;&#22797;&#22270;&#30340;&#25299;&#25169;&#23646;&#24615;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20063;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#19978;&#30340;PDE&#35299;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#33719;&#24471;&#36830;&#32493;&#30340;&#33410;&#28857;&#21644;&#22270;&#32423;&#34920;&#31034;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#19982;&#22270;&#30340;&#35889;&#29305;&#24615;&#20197;&#21450;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#28216;&#36208;&#22312;&#22270;&#19978;&#34892;&#20026;&#20043;&#38388;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#38543;&#26426;&#22270;&#29983;&#25104;&#21442;&#25968;&#12289;Ricci&#26354;&#29575;&#21644;&#25345;&#20037;&#21516;&#35843;&#31561;&#26041;&#24335;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#33021;&#22815;&#25429;&#25417;&#21040;&#22270;&#24418;&#20960;&#20309;&#21644;&#25299;&#25169;&#30340;&#26174;&#33879;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;GDeNet&#22312;&#21253;&#25324;&#24341;&#29992;&#22270;&#12289;&#33647;&#29289;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#22312;&#20869;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Graph Differential Equation Network (GDeNet), an approach that harnesses the expressive power of solutions to PDEs on a graph to obtain continuous node- and graph-level representations for various downstream tasks. We derive theoretical results connecting the dynamics of heat and wave equations to the spectral properties of the graph and to the behavior of continuous-time random walks on graphs. We demonstrate experimentally that these dynamics are able to capture salient aspects of graph geometry and topology by recovering generating parameters of random graphs, Ricci curvature, and persistent homology. Furthermore, we demonstrate the superior performance of GDeNet on real-world datasets including citation graphs, drug-like molecules, and proteins.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#32447;&#24615;MDP&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#20197;&#22312;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#23398;&#20064;&#20986;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.09457</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#35745;&#31639;&#22797;&#26434;&#24615;&#26080;&#27861;&#35299;&#20915;&#30340;&#39044;&#35328;&#26426;&#65292;&#22312;&#31232;&#30095;&#32447;&#24615;MDP&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring and Learning in Sparse Linear MDPs without Computationally Intractable Oracles. (arXiv:2309.09457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#32447;&#24615;MDP&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#20197;&#22312;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#23398;&#20064;&#20986;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#23398;&#20064;&#32773;&#21487;&#20197;&#35775;&#38382;&#24050;&#30693;&#30340;&#29305;&#24449;&#26144;&#23556;$ \phi&#65288;x&#65292;a&#65289;$&#65292;&#35813;&#26144;&#23556;&#23558;&#29366;&#24577;-&#21160;&#20316;&#23545;&#26144;&#23556;&#21040;$d$&#32500;&#21521;&#37327;&#65292;&#24182;&#19988;&#22870;&#21169;&#21644;&#36716;&#25442;&#26159;&#27492;&#34920;&#31034;&#20013;&#30340;&#32447;&#24615;&#20989;&#25968;&#12290;&#20294;&#26159;&#36825;&#20123;&#29305;&#24449;&#20174;&#21738;&#37324;&#26469;&#65311;&#22312;&#27809;&#26377;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#31181;&#35825;&#20154;&#30340;&#31574;&#30053;&#26159;&#20351;&#29992;&#8220;&#21416;&#25151;&#27700;&#27133;&#8221;&#26041;&#27861;&#65292;&#24182;&#24076;&#26395;&#30495;&#23454;&#29305;&#24449;&#21253;&#21547;&#22312;&#19968;&#20010;&#26356;&#22823;&#30340;&#28508;&#22312;&#29305;&#24449;&#38598;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#32447;&#24615;MDP&#12290;&#22312;$k$-&#31232;&#30095;&#32447;&#24615;MDP&#20013;&#65292;&#23384;&#22312;&#19968;&#20010;&#26410;&#30693;&#30340;&#22823;&#23567;&#20026;$k$&#30340;&#23376;&#38598;$S \subset [d]$&#65292;&#20854;&#20013;&#21253;&#21547;&#25152;&#26377;&#30456;&#20851;&#29305;&#24449;&#65292;&#30446;&#26631;&#26159;&#22312;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#20165;&#32463;&#36807;poly$(k,\log d)$&#27425;&#23398;&#20064;&#65292;&#23398;&#20064;&#20986;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#26089;&#26399;&#30340;&#30740;&#31350;&#35201;&#20040;&#20570;&#20986;&#20102;&#26126;&#26174;&#30340;&#20551;&#35774;&#65292;&#20351;&#24471;&#25506;&#32034;&#26080;&#20851;&#32039;&#35201;&#65292;&#35201;&#20040;&#25552;&#20379;&#20102;&#25351;&#25968;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The key assumption underlying linear Markov Decision Processes (MDPs) is that the learner has access to a known feature map $\phi(x, a)$ that maps state-action pairs to $d$-dimensional vectors, and that the rewards and transitions are linear functions in this representation. But where do these features come from? In the absence of expert domain knowledge, a tempting strategy is to use the ``kitchen sink" approach and hope that the true features are included in a much larger set of potential features. In this paper we revisit linear MDPs from the perspective of feature selection. In a $k$-sparse linear MDP, there is an unknown subset $S \subset [d]$ of size $k$ containing all the relevant features, and the goal is to learn a near-optimal policy in only poly$(k,\log d)$ interactions with the environment. Our main result is the first polynomial-time algorithm for this problem. In contrast, earlier works either made prohibitively strong assumptions that obviated the need for exploration, o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28388;&#27874;/&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.03557</link><description>&lt;p&gt;
&#35770;&#22810;&#26234;&#33021;&#20307;&#38750;&#32447;&#24615;&#28388;&#27874;&#21644;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
On the dynamics of multi agent nonlinear filtering and learning. (arXiv:2309.03557v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28388;&#27874;/&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#36890;&#36807;&#20998;&#25955;&#19968;&#33268;&#24615;&#23547;&#27714;&#21160;&#21147;&#23398;&#26469;&#23436;&#25104;&#39640;&#24230;&#22797;&#26434;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#20854;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#35745;&#31639;&#26234;&#33021;&#31038;&#21306;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28388;&#27874;/&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#31995;&#32479;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#30340;&#19968;&#33324;&#34920;&#36848;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#29616;&#21327;&#21516;&#23398;&#20064;&#34892;&#20026;&#30340;&#26465;&#20214;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36824;&#20171;&#32461;&#20102;&#35813;&#25512;&#23548;&#26694;&#26550;&#22312;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiagent systems aim to accomplish highly complex learning tasks through decentralised consensus seeking dynamics and their use has garnered a great deal of attention in the signal processing and computational intelligence societies. This article examines the behaviour of multiagent networked systems with nonlinear filtering/learning dynamics. To this end, a general formulation for the actions of an agent in multiagent networked systems is presented and conditions for achieving a cohesive learning behaviour is given. Importantly, application of the so derived framework in distributed and federated learning scenarios are presented.
&lt;/p&gt;</description></item><item><title>BayOTIDE&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#20302;&#31209;&#26102;&#24207;&#22240;&#23376;&#32452;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#36827;&#34892;&#25554;&#34917;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20840;&#23616;&#36235;&#21183;&#21644;&#21608;&#26399;&#24615;&#27169;&#24335;&#30340;&#24573;&#30053;&#20197;&#21450;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#22788;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14906</link><description>&lt;p&gt;
BayOTIDE: &#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition. (arXiv:2308.14906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14906
&lt;/p&gt;
&lt;p&gt;
BayOTIDE&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#20302;&#31209;&#26102;&#24207;&#22240;&#23376;&#32452;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#36827;&#34892;&#25554;&#34917;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20840;&#23616;&#36235;&#21183;&#21644;&#21608;&#26399;&#24615;&#27169;&#24335;&#30340;&#24573;&#30053;&#20197;&#21450;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#22788;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#22914;&#20132;&#36890;&#21644;&#33021;&#28304;&#65292;&#32463;&#24120;&#35266;&#23519;&#21040;&#20855;&#26377;&#32570;&#22833;&#20540;&#21644;&#22122;&#22768;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#29978;&#33267;&#26159;&#19981;&#35268;&#21017;&#37319;&#26679;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25554;&#34917;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#25968;&#21482;&#36866;&#29992;&#20110;&#23616;&#37096;&#35270;&#35282;&#65292;&#21363;&#23558;&#38271;&#24207;&#21015;&#25286;&#20998;&#20026;&#36866;&#24403;&#22823;&#23567;&#30340;&#25209;&#27425;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#23616;&#37096;&#35270;&#35282;&#21487;&#33021;&#20351;&#27169;&#22411;&#24573;&#30053;&#20840;&#23616;&#36235;&#21183;&#25110;&#21608;&#26399;&#24615;&#27169;&#24335;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20960;&#20046;&#25152;&#26377;&#26041;&#27861;&#37117;&#20551;&#35774;&#35266;&#27979;&#20540;&#22312;&#35268;&#21017;&#30340;&#26102;&#38388;&#38388;&#38548;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#19988;&#26080;&#27861;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#24212;&#29992;&#30340;&#22797;&#26434;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22312;&#31163;&#32447;&#29366;&#24577;&#19979;&#36827;&#34892;&#23398;&#20064;&#30340;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#37027;&#20123;&#26377;&#24555;&#36895;&#21040;&#36798;&#30340;&#27969;&#25968;&#25454;&#30340;&#24212;&#29992;&#26469;&#35828;&#65292;&#23427;&#20204;&#24182;&#19981;&#21512;&#36866;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BayOTIDE&#65306;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios like traffic and energy, massive time-series data with missing values and noises are widely observed, even sampled irregularly. While many imputation methods have been proposed, most of them work with a local horizon, which means models are trained by splitting the long sequence into batches of fit-sized patches. This local horizon can make models ignore global trends or periodic patterns. More importantly, almost all methods assume the observations are sampled at regular time stamps, and fail to handle complex irregular sampled time series arising from different applications. Thirdly, most existing methods are learned in an offline manner. Thus, it is not suitable for many applications with fast-arriving streaming data. To overcome these limitations, we propose \ours: Bayesian Online Multivariate Time series Imputation with functional decomposition. We treat the multivariate time series as the weighted combination of groups of low-rank temporal factors with dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.12044</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20013;&#38750;&#24120;&#29702;&#24819;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#20102;&#25968;&#20540;&#25928;&#29575;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;(&#30001;&#20110;&#30456;&#20851;&#29305;&#24449;&#30340;&#25968;&#37327;&#36739;&#23569;)&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#22522;&#20110;&#32447;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#22312;$\ell^1$&#33539;&#25968;(&#21363;&#38646;&#26435;&#37325;)&#30340;&#26368;&#31232;&#30095;&#35299;&#21644;&#38750;&#27491;&#21017;&#21270;&#35299;&#20043;&#38388;&#23384;&#22312;&#19968;&#26465;&#36830;&#25509;&#36335;&#24452;&#65292;&#36825;&#26465;&#36335;&#24452;&#34987;&#31216;&#20026;&#27491;&#21017;&#21270;&#36335;&#24452;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#32463;&#39564;&#25439;&#22833;&#21644;&#31232;&#30095;&#24615;($\ell^1$&#33539;&#25968;)&#20316;&#20026;&#20004;&#20010;&#20914;&#31361;&#30340;&#26631;&#20934;&#65292;&#24182;&#35299;&#20915;&#30001;&#27492;&#20135;&#29983;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#23558;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;DNNs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;$\ell^1$&#33539;&#25968;&#30340;&#19981;&#20809;&#28369;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#39640;&#24230;&#65292;&#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#26159;&#24456;&#26377;&#25928;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#25972;&#20010;&#24085;&#32047;&#25176;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#31454;&#25216;&#20307;&#32946;&#25216;&#33021;&#35780;&#32423;&#20027;&#35201;&#26041;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#24182;&#25552;&#20986;&#20102;&#37319;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#35270;&#35282;&#30340;&#24314;&#35758;&#12290;&#36890;&#36807;&#20351;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#35270;&#35282;&#65292;&#29609;&#23478;&#30340;&#25216;&#33021;&#21487;&#20197;&#34920;&#31034;&#20026;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#21464;&#37327;&#65292;&#32780;&#27604;&#36187;&#32467;&#26524;&#21017;&#26159;&#21807;&#19968;&#30340;&#35266;&#27979;&#37327;&#12290;&#35813;&#35270;&#35282;&#26377;&#21161;&#20110;&#35299;&#32806;&#24314;&#27169;&#21644;&#25512;&#29702;&#65292;&#24182;&#20419;&#36827;&#36890;&#29992;&#25512;&#29702;&#24037;&#20855;&#30340;&#21457;&#23637;&#12290;&#22312;&#26500;&#24314;&#25216;&#33021;&#35780;&#32423;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26041;&#38754;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#26412;&#27493;&#39588;&#65292;&#21516;&#26102;&#36824;&#35752;&#35770;&#20102;&#28388;&#27874;&#12289;&#24179;&#28369;&#21644;&#21442;&#25968;&#20272;&#35745;&#31561;&#25512;&#29702;&#38454;&#27573;&#12290;&#22312;&#38754;&#23545;&#39640;&#32500;&#22330;&#26223;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#26102;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#25152;&#37319;&#29992;&#30340;&#36817;&#20284;&#21644;&#31616;&#21270;&#26041;&#27861;&#12290;&#35813;&#25991;&#25552;&#20379;&#20102;&#23545;&#35760;&#24405;&#30340;&#27969;&#34892;&#26041;&#27861;&#30340;&#31616;&#26126;&#24635;&#32467;&#12290;</title><link>http://arxiv.org/abs/2308.02414</link><description>&lt;p&gt;
&#23545;&#22312;&#32447;&#25216;&#33021;&#35780;&#32423;&#24314;&#27169;&#21644;&#25512;&#29702;&#30340;&#29366;&#24577;&#31354;&#38388;&#35270;&#35282;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A State-Space Perspective on Modelling and Inference for Online Skill Rating. (arXiv:2308.02414v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#31454;&#25216;&#20307;&#32946;&#25216;&#33021;&#35780;&#32423;&#20027;&#35201;&#26041;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#24182;&#25552;&#20986;&#20102;&#37319;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#35270;&#35282;&#30340;&#24314;&#35758;&#12290;&#36890;&#36807;&#20351;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#35270;&#35282;&#65292;&#29609;&#23478;&#30340;&#25216;&#33021;&#21487;&#20197;&#34920;&#31034;&#20026;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#21464;&#37327;&#65292;&#32780;&#27604;&#36187;&#32467;&#26524;&#21017;&#26159;&#21807;&#19968;&#30340;&#35266;&#27979;&#37327;&#12290;&#35813;&#35270;&#35282;&#26377;&#21161;&#20110;&#35299;&#32806;&#24314;&#27169;&#21644;&#25512;&#29702;&#65292;&#24182;&#20419;&#36827;&#36890;&#29992;&#25512;&#29702;&#24037;&#20855;&#30340;&#21457;&#23637;&#12290;&#22312;&#26500;&#24314;&#25216;&#33021;&#35780;&#32423;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26041;&#38754;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#26412;&#27493;&#39588;&#65292;&#21516;&#26102;&#36824;&#35752;&#35770;&#20102;&#28388;&#27874;&#12289;&#24179;&#28369;&#21644;&#21442;&#25968;&#20272;&#35745;&#31561;&#25512;&#29702;&#38454;&#27573;&#12290;&#22312;&#38754;&#23545;&#39640;&#32500;&#22330;&#26223;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#26102;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#25152;&#37319;&#29992;&#30340;&#36817;&#20284;&#21644;&#31616;&#21270;&#26041;&#27861;&#12290;&#35813;&#25991;&#25552;&#20379;&#20102;&#23545;&#35760;&#24405;&#30340;&#27969;&#34892;&#26041;&#27861;&#30340;&#31616;&#26126;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#29992;&#20110;&#31454;&#25216;&#20307;&#32946;&#25216;&#33021;&#35780;&#32423;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20513;&#37319;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#35270;&#35282;&#65292;&#23558;&#29609;&#23478;&#30340;&#25216;&#33021;&#34920;&#31034;&#20026;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#65292;&#27604;&#36187;&#32467;&#26524;&#20316;&#20026;&#21807;&#19968;&#30340;&#35266;&#27979;&#37327;&#12290;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#35270;&#35282;&#26377;&#21161;&#20110;&#35299;&#32806;&#24314;&#27169;&#21644;&#25512;&#29702;&#65292;&#20174;&#32780;&#20351;&#24471;&#26356;&#21152;&#27880;&#37325;&#27169;&#22411;&#20551;&#35774;&#30340;&#26041;&#27861;&#24471;&#20197;&#31361;&#20986;&#65292;&#24182;&#20419;&#36827;&#20102;&#36890;&#29992;&#25512;&#29702;&#24037;&#20855;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#26500;&#24314;&#25216;&#33021;&#35780;&#32423;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#22522;&#26412;&#27493;&#39588;&#65292;&#24182;&#35752;&#35770;&#20102;&#25512;&#29702;&#30340;&#19977;&#20010;&#38454;&#27573;&#65306;&#28388;&#27874;&#12289;&#24179;&#28369;&#21644;&#21442;&#25968;&#20272;&#35745;&#12290;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#28041;&#21450;&#22823;&#37327;&#29609;&#23478;&#21644;&#27604;&#36187;&#30340;&#39640;&#32500;&#22330;&#26223;&#20013;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#29992;&#20110;&#26377;&#25928;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#36817;&#20284;&#21644;&#31616;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25991;&#29486;&#20013;&#35760;&#24405;&#30340;&#27969;&#34892;&#26041;&#27861;&#30340;&#31616;&#26126;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper offers a comprehensive review of the main methodologies used for skill rating in competitive sports. We advocate for a state-space model perspective, wherein players' skills are represented as time-varying, and match results serve as the sole observed quantities. The state-space model perspective facilitates the decoupling of modeling and inference, enabling a more focused approach highlighting model assumptions, while also fostering the development of general-purpose inference tools. We explore the essential steps involved in constructing a state-space model for skill rating before turning to a discussion on the three stages of inference: filtering, smoothing and parameter estimation. Throughout, we examine the computational challenges of scaling up to high-dimensional scenarios involving numerous players and matches, highlighting approximations and reductions used to address these challenges effectively. We provide concise summaries of popular methods documented in the lit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#40065;&#26834;&#26816;&#27979;&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32858;&#31867;&#25216;&#26415;&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#39046;&#20808;&#28382;&#21518;&#20272;&#35745;&#30340;&#32858;&#21512;&#65292;&#20174;&#32780;&#24378;&#21270;&#20102;&#23545;&#21407;&#22987;&#23431;&#23449;&#20013;&#30340;&#19968;&#33268;&#20851;&#31995;&#30340;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.06704</link><description>&lt;p&gt;
&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#30340;&#40065;&#26834;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models. (arXiv:2305.06704v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#40065;&#26834;&#26816;&#27979;&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32858;&#31867;&#25216;&#26415;&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#39046;&#20808;&#28382;&#21518;&#20272;&#35745;&#30340;&#32858;&#21512;&#65292;&#20174;&#32780;&#24378;&#21270;&#20102;&#23545;&#21407;&#22987;&#23431;&#23449;&#20013;&#30340;&#19968;&#33268;&#20851;&#31995;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#21457;&#29616;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#65292;&#21487;&#20197;&#33719;&#24471;&#20851;&#38190;&#20449;&#24687;&#65292;&#36825;&#25351;&#30340;&#26159;&#20004;&#20010;&#30456;&#23545;&#26102;&#38388;&#20114;&#31227;&#30340;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#29992;&#20110;&#25511;&#21046;&#12289;&#39044;&#27979;&#25110;&#32858;&#31867;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#26816;&#27979;&#28382;&#21518;&#22810;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#39046;&#20808;&#28382;&#21518;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25152;&#35774;&#24819;&#30340;&#31649;&#36947;&#25509;&#25910;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#20174;&#27599;&#20010;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#19968;&#32452;&#23376;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#21508;&#31181;&#32858;&#31867;&#25216;&#26415;&#65288;&#20363;&#22914;K-means++&#21644;&#35889;&#32858;&#31867;&#65289;&#65292;&#37319;&#29992;&#21508;&#31181;&#25104;&#23545;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#12290;&#19968;&#26086;&#32858;&#31867;&#34987;&#25552;&#21462;&#20986;&#26469;&#65292;&#36328;&#32858;&#31867;&#30340;&#39046;&#20808;&#28382;&#21518;&#20272;&#35745;&#34987;&#32858;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#23545;&#21407;&#22987;&#23431;&#23449;&#20013;&#19968;&#33268;&#20851;&#31995;&#30340;&#35782;&#21035;&#12290;&#30001;&#20110;&#22810;
&lt;/p&gt;
&lt;p&gt;
In multivariate time series systems, key insights can be obtained by discovering lead-lag relationships inherent in the data, which refer to the dependence between two time series shifted in time relative to one another, and which can be leveraged for the purposes of control, forecasting or clustering. We develop a clustering-driven methodology for the robust detection of lead-lag relationships in lagged multi-factor models. Within our framework, the envisioned pipeline takes as input a set of time series, and creates an enlarged universe of extracted subsequence time series from each input time series, by using a sliding window approach. We then apply various clustering techniques (e.g, K-means++ and spectral clustering), employing a variety of pairwise similarity measures, including nonlinear ones. Once the clusters have been extracted, lead-lag estimates across clusters are aggregated to enhance the identification of the consistent relationships in the original universe. Since multi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21464;&#37327;&#31215;&#20998;&#26102;&#38388;&#21644;&#37096;&#20998;&#36895;&#24230;&#21047;&#26032;&#65292;&#29702;&#24819;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#22120;&#22312;&#20108;&#27425;&#21183;&#20989;&#25968;&#19978;&#30340;&#25928;&#29575;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#24182;&#19988;&#38543;&#26426;&#31215;&#20998;&#22120;&#22312;&#27169;&#25311;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#19978;&#20063;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2209.07438</link><description>&lt;p&gt;
&#20851;&#20110;&#29702;&#24819;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#22120;&#32791;&#25955;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Dissipation of Ideal Hamiltonian Monte Carlo Sampler. (arXiv:2209.07438v3 [stat.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07438
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21464;&#37327;&#31215;&#20998;&#26102;&#38388;&#21644;&#37096;&#20998;&#36895;&#24230;&#21047;&#26032;&#65292;&#29702;&#24819;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#22120;&#22312;&#20108;&#27425;&#21183;&#20989;&#25968;&#19978;&#30340;&#25928;&#29575;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#24182;&#19988;&#38543;&#26426;&#31215;&#20998;&#22120;&#22312;&#27169;&#25311;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#19978;&#20063;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;&#21464;&#37327;&#31215;&#20998;&#26102;&#38388;&#21644;&#29702;&#24819;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#22120;&#30340;&#37096;&#20998;&#36895;&#24230;&#21047;&#26032;&#20043;&#38388;&#20284;&#20046;&#23384;&#22312;&#30528;&#26377;&#36259;&#30340;&#32852;&#31995;&#65292;&#36825;&#20004;&#32773;&#37117;&#21487;&#20197;&#29992;&#20110;&#20943;&#23569;&#21160;&#21147;&#23398;&#30340;&#32791;&#25955;&#34892;&#20026;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#20108;&#27425;&#21183;&#20989;&#25968;&#19978;&#65292;&#36890;&#36807;&#36825;&#20123;&#25163;&#27573;&#65292;&#19982;&#32463;&#20856;&#30340;&#24120;&#37327;&#31215;&#20998;&#26102;&#38388;&#12289;&#23436;&#20840;&#21047;&#26032;&#30340;HMC&#30456;&#27604;&#65292;&#25928;&#29575;&#21487;&#20197;&#25552;&#39640;&#19968;&#20010;$\sqrt{\kappa}$&#30340;Wasserstein-2&#36317;&#31163;&#22240;&#23376;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22312;&#26356;&#39640;&#38454;&#27491;&#21017;&#24615;&#26465;&#20214;&#19979;&#27169;&#25311;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#30340;&#38543;&#26426;&#31215;&#20998;&#22120;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report on what seems to be an intriguing connection between variable integration time and partial velocity refreshment of Ideal Hamiltonian Monte Carlo samplers, both of which can be used for reducing the dissipative behavior of the dynamics. More concretely, we show that on quadratic potentials, efficiency can be improved through these means by a $\sqrt{\kappa}$ factor in Wasserstein-2 distance, compared to classical constant integration time, fully refreshed HMC. We additionally explore the benefit of randomized integrators for simulating the Hamiltonian dynamics under higher order regularity conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19982;&#38543;&#26426;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#36830;&#25509;&#65292;&#23545;&#33609;&#22270;&#25237;&#24433;&#26041;&#27861;&#36827;&#34892;&#20102;&#23574;&#38160;&#20998;&#26512;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#33609;&#22270;&#22823;&#23567;&#19982;&#25910;&#25947;&#36895;&#24230;&#30340;&#32447;&#24615;&#20851;&#31995;&#65292;&#21516;&#26102;&#35299;&#37322;&#20102;&#31232;&#30095;&#33609;&#22270;&#30697;&#38453;&#23545;&#27599;&#27425;&#36845;&#20195;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2208.09585</link><description>&lt;p&gt;
&#36890;&#36807;&#19982;&#38543;&#26426;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#36830;&#25509;&#65292;&#23545;&#33609;&#22270;&#25237;&#24433;&#26041;&#27861;&#36827;&#34892;&#20102;&#23574;&#38160;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sharp Analysis of Sketch-and-Project Methods via a Connection to Randomized Singular Value Decomposition. (arXiv:2208.09585v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19982;&#38543;&#26426;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#36830;&#25509;&#65292;&#23545;&#33609;&#22270;&#25237;&#24433;&#26041;&#27861;&#36827;&#34892;&#20102;&#23574;&#38160;&#20998;&#26512;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#33609;&#22270;&#22823;&#23567;&#19982;&#25910;&#25947;&#36895;&#24230;&#30340;&#32447;&#24615;&#20851;&#31995;&#65292;&#21516;&#26102;&#35299;&#37322;&#20102;&#31232;&#30095;&#33609;&#22270;&#30697;&#38453;&#23545;&#27599;&#27425;&#36845;&#20195;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33609;&#22270;&#25237;&#24433;&#26159;&#19968;&#20010;&#32479;&#19968;&#20102;&#35768;&#22810;&#24050;&#30693;&#36845;&#20195;&#26041;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#21464;&#20307;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#32447;&#24615;&#31995;&#32479;&#21644;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#33719;&#24471;&#33609;&#22270;&#25237;&#24433;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#23574;&#38160;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#27425;&#23637;&#31034;&#20102;&#65306;&#65288;1&#65289;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#19982;&#33609;&#22270;&#22823;&#23567;&#32447;&#24615;&#22686;&#21152;&#65292;&#24403;&#25968;&#25454;&#30697;&#38453;&#20855;&#26377;&#19968;&#23450;&#30340;&#35889;&#34928;&#20943;&#26102;&#65292;&#25910;&#25947;&#36895;&#24230;&#22686;&#21152;&#26356;&#24555;&#65307;&#65288;2&#65289;&#20801;&#35768;&#20351;&#29992;&#31232;&#30095;&#30340;&#33609;&#22270;&#30697;&#38453;&#65292;&#36825;&#27604;&#23494;&#38598;&#33609;&#22270;&#26356;&#26377;&#25928;&#29575;&#65292;&#27604;&#23376;&#37319;&#26679;&#26041;&#27861;&#26356;&#31283;&#20581;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#35299;&#37322;&#20102;&#19968;&#20010;&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#65292;&#21363;&#23545;&#33609;&#22270;&#30697;&#38453;&#36827;&#34892;&#28608;&#36827;&#30340;&#31232;&#30095;&#21270;&#19981;&#20250;&#24433;&#21709;&#33609;&#22270;&#25237;&#24433;&#30340;&#27599;&#27425;&#36845;&#20195;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sketch-and-project is a framework which unifies many known iterative methods for solving linear systems and their variants, as well as further extensions to non-linear optimization problems. It includes popular methods such as randomized Kaczmarz, coordinate descent, variants of the Newton method in convex optimization, and others. In this paper, we develop a theoretical framework for obtaining sharp guarantees on the convergence rate of sketch-and-project methods. Our approach is the first to: (1) show that the convergence rate improves at least linearly with the sketch size, and even faster when the data matrix exhibits certain spectral decays; and (2) allow for sparse sketching matrices, which are more efficient than dense sketches and more robust than sub-sampling methods. In particular, our results explain an observed phenomenon that a radical sparsification of the sketching matrix does not affect the per iteration convergence rate of sketch-and-project. To obtain our results, we 
&lt;/p&gt;</description></item><item><title>&#22312;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#32676;&#36873;&#25321;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#22238;&#24402;&#20989;&#25968;&#36229;&#36807;&#39044;&#35774;&#38408;&#20540;&#30340;&#29305;&#24449;&#31354;&#38388;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#30830;&#23450;&#20102;&#22312;&#26679;&#26412;&#35268;&#27169;&#21644;&#31867;&#22411;I&#38169;&#35823;&#27010;&#29575;&#19978;&#36951;&#25022;&#30340;&#26368;&#20339;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2109.01077</link><description>&lt;p&gt;
&#26368;&#20339;&#23376;&#32676;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Optimal subgroup selection. (arXiv:2109.01077v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01077
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#32676;&#36873;&#25321;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#22238;&#24402;&#20989;&#25968;&#36229;&#36807;&#39044;&#35774;&#38408;&#20540;&#30340;&#29305;&#24449;&#31354;&#38388;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#30830;&#23450;&#20102;&#22312;&#26679;&#26412;&#35268;&#27169;&#21644;&#31867;&#22411;I&#38169;&#35823;&#27010;&#29575;&#19978;&#36951;&#25022;&#30340;&#26368;&#20339;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#35797;&#39564;&#21644;&#20854;&#20182;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#30475;&#21040;&#29305;&#24449;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#26377;&#36259;&#30340;&#34892;&#20026;&#21306;&#22495;&#65292;&#20294;&#19981;&#28165;&#26970;&#36825;&#20123;&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#26159;&#21542;&#22312;&#24635;&#20307;&#27700;&#24179;&#19978;&#26377;&#25152;&#21453;&#26144;&#12290;&#38024;&#23545;&#22238;&#24402;&#35774;&#32622;&#65292;&#25105;&#20204;&#32771;&#34385;&#23376;&#32676;&#36873;&#25321;&#25361;&#25112;&#65292;&#21363;&#35782;&#21035;&#19968;&#20010;&#29305;&#24449;&#31354;&#38388;&#30340;&#21306;&#22495;&#65292;&#22312;&#35813;&#21306;&#22495;&#19978;&#65292;&#22238;&#24402;&#20989;&#25968;&#36229;&#36807;&#20102;&#39044;&#35774;&#30340;&#38408;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#23547;&#25214;&#19968;&#20010;&#20302;&#22797;&#26434;&#24230;&#12289;&#25968;&#25454;&#30456;&#20851;&#30340;&#36873;&#25321;&#38598;&#65292;&#22312;&#36825;&#20010;&#36873;&#25321;&#38598;&#19978;&#65292;&#22238;&#24402;&#20989;&#25968;&#26377;&#33267;&#23569;&#19982;&#38408;&#20540;&#19968;&#26679;&#22823;&#30340;&#27010;&#29575;&#65292;&#21516;&#26102;&#35201;&#27714;&#35813;&#21306;&#22495;&#22312;&#36793;&#32536;&#29305;&#24449;&#20998;&#24067;&#19979;&#30340;&#36136;&#37327;&#23613;&#21487;&#33021;&#22823;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#36951;&#25022;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#30830;&#23450;&#20102;&#36951;&#25022;&#22312;&#26679;&#26412;&#35268;&#27169;&#21644;&#31532;&#19968;&#31867;&#38169;&#35823;&#27010;&#29575;&#19978;&#30340;&#26368;&#20248;&#20540;&#12290;&#36825;&#20010;&#26368;&#20248;&#20540;&#28041;&#21450;&#21040;&#26679;&#26412;&#22823;&#23567;&#21644;&#31867;&#22411;I&#38169;&#35823;&#27010;&#29575;&#30340;&#24494;&#22937;&#30456;&#20114;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In clinical trials and other applications, we often see regions of the feature space that appear to exhibit interesting behaviour, but it is unclear whether these observed phenomena are reflected at the population level. Focusing on a regression setting, we consider the subgroup selection challenge of identifying a region of the feature space on which the regression function exceeds a pre-determined threshold. We formulate the problem as one of constrained optimisation, where we seek a low-complexity, data-dependent selection set on which, with a guaranteed probability, the regression function is uniformly at least as large as the threshold; subject to this constraint, we would like the region to contain as much mass under the marginal feature distribution as possible. This leads to a natural notion of regret, and our main contribution is to determine the minimax optimal rate for this regret in both the sample size and the Type I error probability. The rate involves a delicate interpla
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#65292;&#38543;&#30528;&#26356;&#26032;&#27425;&#25968;&#36235;&#36817;&#26080;&#31351;&#22823;&#65292;&#24102;&#26377;&#34920;&#26684;&#21442;&#25968;&#21270;&#30340;&#22312;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#25910;&#25947;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#34892;&#20026;&#21644;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2108.08655</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22312;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;ODE&#26497;&#38480;&#20840;&#23616;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Global Convergence of the ODE Limit for Online Actor-Critic Algorithms in Reinforcement Learning. (arXiv:2108.08655v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.08655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#65292;&#38543;&#30528;&#26356;&#26032;&#27425;&#25968;&#36235;&#36817;&#26080;&#31351;&#22823;&#65292;&#24102;&#26377;&#34920;&#26684;&#21442;&#25968;&#21270;&#30340;&#22312;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#25910;&#25947;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#34892;&#20026;&#21644;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#22312;&#32447;&#25968;&#25454;&#26679;&#26412;&#30340;&#21040;&#26469;&#65292;&#20854;&#22312;&#25968;&#23398;&#19978;&#20998;&#26512;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25968;&#25454;&#26679;&#26412;&#30340;&#20998;&#24067;&#38543;&#30528;&#27169;&#22411;&#30340;&#26356;&#26032;&#32780;&#21160;&#24577;&#21464;&#21270;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#20998;&#24067;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#22797;&#26434;&#30340;&#21453;&#39304;&#24490;&#29615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26102;&#38388;&#37325;&#32553;&#25918;&#19979;&#65292;&#24102;&#26377;&#34920;&#26684;&#21442;&#25968;&#21270;&#30340;&#22312;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#26356;&#26032;&#27425;&#25968;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#35777;&#26126;&#39318;&#20808;&#22312;&#22266;&#23450;&#30340;&#28436;&#21592;&#31574;&#30053;&#19979;&#24314;&#31435;&#25968;&#25454;&#26679;&#26412;&#30340;&#20960;&#20309;&#36941;&#21382;&#24615;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#27850;&#26494;&#26041;&#31243;&#65292;&#25105;&#20204;&#35777;&#26126;&#38543;&#30528;&#26356;&#26032;&#27425;&#25968;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#65292;&#25968;&#25454;&#26679;&#26412;&#20851;&#20110;&#19968;&#31181;&#21160;&#24577;&#27010;&#29575;&#27979;&#24230;&#30340;&#27874;&#21160;&#22312;&#28436;&#21464;&#30340;&#28436;&#21592;&#27169;&#22411;&#30340;&#20989;&#25968;&#19979;&#28040;&#22833;&#12290;&#19968;&#26086;&#24471;&#21040;ODE&#26497;&#38480;&#65292;&#25105;&#20204;&#20351;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#20998;&#26512;&#30740;&#31350;&#20854;&#25910;&#25947;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor-critic algorithms are widely used in reinforcement learning, but are challenging to mathematically analyse due to the online arrival of non-i.i.d. data samples. The distribution of the data samples dynamically changes as the model is updated, introducing a complex feedback loop between the data distribution and the reinforcement learning algorithm. We prove that, under a time rescaling, the online actor-critic algorithm with tabular parametrization converges to an ordinary differential equation (ODE) as the number of updates becomes large. The proof first establishes the geometric ergodicity of the data samples under a fixed actor policy. Then, using a Poisson equation, we prove that the fluctuations of the data samples around a dynamic probability measure, which is a function of the evolving actor model, vanish as the number of updates become large. Once the ODE limit has been derived, we study its convergence properties using a two time-scale analysis which asymptotically de-co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#22810;&#32500;&#22797;&#26434;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#22122;&#22768;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#24674;&#22797;ODE&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#32500;&#24230;&#28798;&#38590;&#21644;&#22797;&#26434;ODE&#32467;&#26500;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#27169;&#22359;&#21270;&#32467;&#26500;&#21644;&#36866;&#24403;&#36873;&#25321;&#32593;&#32476;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#34987;&#35777;&#26126;&#26159;&#19968;&#33268;&#30340;&#12290;</title><link>http://arxiv.org/abs/2106.03591</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#22810;&#32500;&#22797;&#26434;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#22122;&#22768;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Calibrating multi-dimensional complex ODE from noisy data via deep neural networks. (arXiv:2106.03591v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03591
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#22810;&#32500;&#22797;&#26434;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#22122;&#22768;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#24674;&#22797;ODE&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#32500;&#24230;&#28798;&#38590;&#21644;&#22797;&#26434;ODE&#32467;&#26500;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#27169;&#22359;&#21270;&#32467;&#26500;&#21644;&#36866;&#24403;&#36873;&#25321;&#32593;&#32476;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#34987;&#35777;&#26126;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#12289;&#24037;&#31243;&#12289;&#37329;&#34701;&#12289;&#29289;&#29702;&#31561;&#39046;&#22495;&#30340;&#22797;&#26434;&#21160;&#24577;&#12290;&#20351;&#29992;&#22122;&#22768;&#25968;&#25454;&#26657;&#20934;&#22797;&#26434;ODE&#31995;&#32479;&#36890;&#24120;&#38750;&#24120;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#38750;&#21442;&#25968;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#36793;&#30028;&#26680;&#26041;&#27861;&#25552;&#21462;&#21435;&#22122;&#25968;&#25454;&#21450;&#20854;&#39640;&#38454;&#23548;&#25968;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#36755;&#20837;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31232;&#30095;&#36830;&#25509;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#24674;&#22797;ODE&#31995;&#32479;&#65292;&#32780;&#19981;&#21463;&#32500;&#24230;&#28798;&#38590;&#21644;&#22797;&#26434;ODE&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;&#24403;ODE&#20855;&#26377;&#19968;&#33324;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#27599;&#20010;&#27169;&#22359;&#32452;&#20214;&#20165;&#28041;&#21450;&#23569;&#37327;&#36755;&#20837;&#21464;&#37327;&#65292;&#24182;&#19988;&#32593;&#32476;&#26550;&#26500;&#34987;&#36866;&#24403;&#36873;&#25321;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#19968;&#33268;&#30340;&#12290;&#29702;&#35770;&#24615;&#36136;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#24471;&#21040;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ordinary differential equations (ODEs) are widely used to model complex dynamics that arises in biology, chemistry, engineering, finance, physics, etc. Calibration of a complicated ODE system using noisy data is generally very difficult. In this work, we propose a two-stage nonparametric approach to address this problem. We first extract the de-noised data and their higher order derivatives using boundary kernel method, and then feed them into a sparsely connected deep neural network with ReLU activation function. Our method is able to recover the ODE system without being subject to the curse of dimensionality and complicated ODE structure. When the ODE possesses a general modular structure, with each modular component involving only a few input variables, and the network architecture is properly chosen, our method is proven to be consistent. Theoretical properties are corroborated by an extensive simulation study that demonstrates the validity and effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25512;&#24191;&#20102;&#22871;&#32034;&#26041;&#27861;&#22312;&#39640;&#26031;&#30456;&#20851;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#8220;&#22266;&#23450;&#35774;&#35745;&#8221;&#27169;&#22411;&#26469;&#31934;&#30830;&#21051;&#30011;&#22871;&#32034;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#22238;&#24402;&#20013;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2007.13716</link><description>&lt;p&gt;
&#24102;&#26377;&#19968;&#33324;&#39640;&#26031;&#35774;&#35745;&#30340;&#22871;&#32034;&#26041;&#27861;&#21450;&#20854;&#22312;&#20551;&#35774;&#26816;&#39564;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Lasso with general Gaussian designs with applications to hypothesis testing. (arXiv:2007.13716v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.13716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25512;&#24191;&#20102;&#22871;&#32034;&#26041;&#27861;&#22312;&#39640;&#26031;&#30456;&#20851;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#8220;&#22266;&#23450;&#35774;&#35745;&#8221;&#27169;&#22411;&#26469;&#31934;&#30830;&#21051;&#30011;&#22871;&#32034;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#22238;&#24402;&#20013;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22871;&#32034;&#26041;&#27861;&#26159;&#19968;&#31181;&#39640;&#32500;&#22238;&#24402;&#26041;&#27861;&#65292;&#24403;&#33258;&#21464;&#37327;&#30340;&#25968;&#37327;$p$&#19982;&#35266;&#27979;&#25968;&#37327;$n$&#30456;&#21516;&#25110;&#26356;&#22823;&#26102;&#65292;&#29616;&#22312;&#36890;&#24120;&#20351;&#29992;&#12290;&#30001;&#20110;&#20004;&#20010;&#22522;&#26412;&#21407;&#22240;&#65292;&#32463;&#20856;&#30340;&#28176;&#36817;&#27491;&#24577;&#29702;&#35770;&#19981;&#36866;&#29992;&#20110;&#35813;&#27169;&#22411;&#65306;(1) &#27491;&#21017;&#21270;&#39118;&#38505;&#26159;&#38750;&#20809;&#28369;&#30340;&#65307;(2) &#20272;&#35745;&#22120;$\widehat{\boldsymbol{\theta}}$&#19982;&#30495;&#23454;&#21442;&#25968;&#21521;&#37327;$\boldsymbol{\theta}^*$&#20043;&#38388;&#30340;&#36317;&#31163;&#19981;&#33021;&#24573;&#30053;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#28176;&#36817;&#27491;&#24577;&#29702;&#35770;&#30340;&#22522;&#30784;&#8212;&#8212;&#26631;&#20934;&#30340;&#25668;&#21160;&#35770;&#35777;&#22833;&#36133;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;$n$&#21644;$p$&#37117;&#24456;&#22823;&#19988;$n/p$&#20026;1&#38454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#25551;&#36848;&#22871;&#32034;&#20272;&#35745;&#22120;&#12290;&#36825;&#20010;&#25551;&#36848;&#39318;&#20808;&#26159;&#22312;&#20855;&#26377;&#29420;&#31435;&#21516;&#20998;&#24067;&#33258;&#21464;&#37327;&#30340;&#39640;&#26031;&#35774;&#35745;&#24773;&#20917;&#19979;&#24471;&#21040;&#30340;&#65306;&#25105;&#20204;&#23558;&#20854;&#25512;&#24191;&#21040;&#20855;&#26377;&#38750;&#22855;&#24322;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#39640;&#26031;&#30456;&#20851;&#35774;&#35745;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#8220;&#22266;&#23450;&#35774;&#35745;&#8221;&#27169;&#22411;&#26469;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Lasso is a method for high-dimensional regression, which is now commonly used when the number of covariates $p$ is of the same order or larger than the number of observations $n$. Classical asymptotic normality theory does not apply to this model due to two fundamental reasons: $(1)$ The regularized risk is non-smooth; $(2)$ The distance between the estimator $\widehat{\boldsymbol{\theta}}$ and the true parameters vector $\boldsymbol{\theta}^*$ cannot be neglected. As a consequence, standard perturbative arguments that are the traditional basis for asymptotic normality fail.  On the other hand, the Lasso estimator can be precisely characterized in the regime in which both $n$ and $p$ are large and $n/p$ is of order one. This characterization was first obtained in the case of Gaussian designs with i.i.d. covariates: here we generalize it to Gaussian correlated designs with non-singular covariance structure. This is expressed in terms of a simpler ``fixed-design'' model. We establish
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#26684;&#24335;&#65292;&#20174;&#32780;&#25506;&#32034;&#20102;NLP&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#20840;&#35980;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/1910.10683</link><description>&lt;p&gt;
&#25506;&#32034;&#20351;&#29992;&#32479;&#19968;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#22120;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. (arXiv:1910.10683v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.10683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#26684;&#24335;&#65292;&#20174;&#32780;&#25506;&#32034;&#20102;NLP&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#20840;&#35980;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#20043;&#21069;&#39318;&#20808;&#22312;&#25968;&#25454;&#20016;&#23500;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#36801;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#20652;&#29983;&#20102;&#22810;&#31181;&#26041;&#27861;&#12289;&#26041;&#27861;&#35770;&#21644;&#23454;&#36341;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#23558;&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#26684;&#24335;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#25506;&#32034;&#20102;NLP&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#20840;&#35980;&#12290;&#25105;&#20204;&#23545;&#35768;&#22810;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#39044;&#35757;&#32451;&#30446;&#26631;&#12289;&#26550;&#26500;&#12289;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#12289;&#36801;&#31227;&#26041;&#27861;&#21644;&#20854;&#20182;&#22240;&#32032;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#25506;&#32034;&#30340;&#35265;&#35299;&#19982;&#35268;&#27169;&#21644;&#25105;&#20204;&#30340;&#26032;&#30340;&#8220;&#24222;&#22823;&#24178;&#20928;&#25235;&#21462;&#35821;&#26009;&#24211;&#8221;&#30456;&#32467;&#21512;&#65292;&#22312;&#35768;&#22810;&#28041;&#21450;&#25688;&#35201;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#20998;&#31867;&#31561;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;&#20026;&#20102;&#20419;&#36827;NLP&#39046;&#22495;&#30340;&#26410;&#26469;&#36801;&#31227;&#23398;&#20064;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained mode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26144;&#23556;&#22120;&#22312;&#22270;&#24418;&#19978;&#36827;&#34892;&#20445;&#25345;&#21516;&#24577;&#30340;&#22810;&#23610;&#24230;&#22270;&#24418;&#39592;&#26550;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#21333;&#20010;&#21442;&#25968;&#23454;&#29616;&#39592;&#26550;&#21270;&#30340;&#22810;&#23610;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36719;&#20214;&#24037;&#20855;&#26469;&#36827;&#34892;&#20132;&#20114;&#24335;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/1804.11242</link><description>&lt;p&gt;
&#20351;&#29992;&#26144;&#23556;&#22120;&#22312;&#22270;&#24418;&#19978;&#36827;&#34892;&#20445;&#25345;&#21516;&#24577;&#30340;&#22810;&#23610;&#24230;&#22270;&#24418;&#39592;&#26550;&#21270;
&lt;/p&gt;
&lt;p&gt;
Homology-Preserving Multi-Scale Graph Skeletonization Using Mapper on Graphs. (arXiv:1804.11242v5 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1804.11242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26144;&#23556;&#22120;&#22312;&#22270;&#24418;&#19978;&#36827;&#34892;&#20445;&#25345;&#21516;&#24577;&#30340;&#22810;&#23610;&#24230;&#22270;&#24418;&#39592;&#26550;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#21333;&#20010;&#21442;&#25968;&#23454;&#29616;&#39592;&#26550;&#21270;&#30340;&#22810;&#23610;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36719;&#20214;&#24037;&#20855;&#26469;&#36827;&#34892;&#20132;&#20114;&#24335;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#38142;&#25509;&#22270;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#34920;&#31034;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25429;&#25417;&#20010;&#20307;&#12289;&#20225;&#19994;&#12289;&#34507;&#30333;&#36136;&#21644;&#30005;&#20449;&#31471;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#20960;&#30334;&#20010;&#33410;&#28857;&#30340;&#20013;&#31561;&#35268;&#27169;&#25968;&#25454;&#65292;&#33410;&#28857;&#38142;&#25509;&#22270;&#21487;&#33021;&#26080;&#27861;&#20256;&#36798;&#26377;&#20851;&#22270;&#32467;&#26500;&#30340;&#35265;&#35299;&#65292;&#22240;&#20026;&#20250;&#26377;&#35270;&#35273;&#28151;&#20081;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#26144;&#23556;&#22120;&#26500;&#36896;&#24212;&#29992;&#20110;&#22270;&#24418;&#21487;&#35270;&#21270;&#65292;&#35813;&#26500;&#36896;&#22312;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#22312;&#20445;&#30041;&#26680;&#24515;&#32467;&#26500;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#23545;&#25968;&#25454;&#30340;&#24635;&#32467;&#30340;&#24378;&#22823;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;&#21152;&#26435;&#26080;&#21521;&#22270;&#30340;&#26144;&#23556;&#22120;&#26500;&#36896;&#30340;&#21464;&#31181;&#65292;&#31216;&#20026;{\mog}&#65292;&#23427;&#29983;&#25104;&#20445;&#25345;&#21516;&#35843;&#24615;&#30340;&#22270;&#24418;&#39592;&#26550;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#21333;&#20010;&#21442;&#25968;&#26469;&#23454;&#29616;&#36755;&#20837;&#22270;&#24418;&#30340;&#22810;&#23610;&#24230;&#39592;&#26550;&#21270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36719;&#20214;&#24037;&#20855;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#36825;&#20123;&#39592;&#26550;&#36827;&#34892;&#20132;&#20114;&#24335;&#25506;&#32034;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node-link diagrams are a popular method for representing graphs that capture relationships between individuals, businesses, proteins, and telecommunication endpoints. However, node-link diagrams may fail to convey insights regarding graph structures, even for moderately sized data of a few hundred nodes, due to visual clutter. We propose to apply the mapper construction -- a popular tool in topological data analysis -- to graph visualization, which provides a strong theoretical basis for summarizing the data while preserving their core structures. We develop a variation of the mapper construction targeting weighted, undirected graphs, called {\mog}, which generates homology-preserving skeletons of graphs. We further show how the adjustment of a single parameter enables multi-scale skeletonization of the input graph. We provide a software tool that enables interactive explorations of such skeletons and demonstrate the effectiveness of our method for synthetic and real-world data.
&lt;/p&gt;</description></item></channel></rss>