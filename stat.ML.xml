<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#39640;&#26031;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#22312;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#35745;&#31639;&#26377;&#25928;&#31639;&#27861;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#22797;&#26434;&#24230;&#26159;&#20805;&#20998;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.05529</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#26031;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Computational Complexity of Learning Gaussian Single-Index Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#39640;&#26031;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#22312;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#35745;&#31639;&#26377;&#25928;&#31639;&#27861;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#22797;&#26434;&#24230;&#26159;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#25351;&#25968;&#27169;&#22411;&#26159;&#20855;&#26377;&#26893;&#20837;&#32467;&#26500;&#30340;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#26631;&#31614;&#20381;&#36182;&#20110;&#36890;&#36807;&#36890;&#29992;&#12289;&#38750;&#32447;&#24615;&#21644;&#28508;&#22312;&#38750;&#30830;&#23450;&#24615;&#36716;&#25442;&#30340;&#36755;&#20837;&#30340;&#26410;&#30693;&#19968;&#32500;&#25237;&#24433;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#32479;&#35745;&#25512;&#26029;&#20219;&#21153;&#31867;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#27169;&#26495;&#65292;&#29992;&#20110;&#30740;&#31350;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#25240;&#34935;&#12290;&#23613;&#31649;&#24674;&#22797;&#38544;&#34255;&#26041;&#21521;&#30340;&#20449;&#24687;&#35770;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#32500;&#24230;$d$&#26159;&#32447;&#24615;&#30340;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#32479;&#35745;&#26597;&#35810;&#65288;SQ&#65289;&#26694;&#26550;&#21644;&#20302;&#38454;&#22810;&#39033;&#24335;&#65288;LDP&#65289;&#26694;&#26550;&#20869;&#65292;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#24517;&#39035;&#38656;&#35201;$\Omega(d^{k^\star/2})$&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;$k^\star$&#26159;&#25105;&#20204;&#26126;&#30830;&#34920;&#24449;&#30340;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#8220;&#29983;&#25104;&#8221;&#25351;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#20351;&#29992;&#37096;&#20998;&#36857;&#30340;&#21305;&#37197;&#19978;&#30028;&#26469;&#35777;&#26126;&#36825;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#20063;&#26159;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05529v1 Announce Type: new  Abstract: Single-Index Models are high-dimensional regression problems with planted structure, whereby labels depend on an unknown one-dimensional projection of the input via a generic, non-linear, and potentially non-deterministic transformation. As such, they encompass a broad class of statistical inference tasks, and provide a rich template to study statistical and computational trade-offs in the high-dimensional regime.   While the information-theoretic sample complexity to recover the hidden direction is linear in the dimension $d$, we show that computationally efficient algorithms, both within the Statistical Query (SQ) and the Low-Degree Polynomial (LDP) framework, necessarily require $\Omega(d^{k^\star/2})$ samples, where $k^\star$ is a "generative" exponent associated with the model that we explicitly characterize. Moreover, we show that this sample complexity is also sufficient, by establishing matching upper bounds using a partial-trace
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#34920;&#31034;&#23398;&#20064;&#30446;&#26631;&#20248;&#21270;&#21305;&#37197;&#22810;&#20010;&#30456;&#20851;&#35270;&#22270;&#65292;&#22312;ImageNet1k&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#20110;SimCLR&#27169;&#22411;&#65292;&#22810;&#35270;&#22270;&#23545;&#27604;&#27169;&#22411;&#22312;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#25968;&#21644;&#26356;&#23567;&#30340;&#25209;&#22823;&#23567;&#19979;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>https://arxiv.org/abs/2403.05490</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Poly-View Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#34920;&#31034;&#23398;&#20064;&#30446;&#26631;&#20248;&#21270;&#21305;&#37197;&#22810;&#20010;&#30456;&#20851;&#35270;&#22270;&#65292;&#22312;ImageNet1k&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#20110;SimCLR&#27169;&#22411;&#65292;&#22810;&#35270;&#22270;&#23545;&#27604;&#27169;&#22411;&#22312;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#25968;&#21644;&#26356;&#23567;&#30340;&#25209;&#22823;&#23567;&#19979;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#36890;&#24120;&#20250;&#21305;&#37197;&#19968;&#32452;&#19981;&#30456;&#20851;&#30340;&#36127;&#35270;&#22270;&#20013;&#30456;&#20851;&#35270;&#22270;&#30340;&#37197;&#23545;&#12290;&#35270;&#22270;&#21487;&#20197;&#26159;&#29983;&#25104;&#30340;&#65288;&#20363;&#22914;&#36890;&#36807;&#22686;&#24378;&#65289;&#25110;&#34987;&#35266;&#23519;&#21040;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#23384;&#22312;&#22810;&#20110;&#20004;&#20010;&#30456;&#20851;&#35270;&#22270;&#26102;&#30340;&#21305;&#37197;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22810;&#35270;&#22270;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#20449;&#24687;&#26368;&#22823;&#21270;&#21644;&#20805;&#20998;&#32479;&#35745;&#23548;&#20986;&#20102;&#26032;&#30340;&#34920;&#31034;&#23398;&#20064;&#30446;&#26631;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#35745;&#31639;&#36164;&#28304;&#26080;&#38480;&#26102;&#65292;&#24212;&#26368;&#22823;&#21270;&#30456;&#20851;&#35270;&#22270;&#30340;&#25968;&#37327;&#65307;&#32780;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#20943;&#23569;&#29420;&#29305;&#26679;&#26412;&#30340;&#25968;&#37327;&#21516;&#26102;&#22686;&#21152;&#36825;&#20123;&#26679;&#26412;&#30340;&#35270;&#22270;&#25968;&#37327;&#26159;&#26377;&#30410;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#20197;256&#30340;&#25209;&#22823;&#23567;&#35757;&#32451;128&#36718;&#30340;&#22810;&#35270;&#22270;&#23545;&#27604;&#27169;&#22411;&#22312;ImageNet1k&#19978;&#34920;&#29616;&#20248;&#20110;&#22312;&#25209;&#22823;&#23567;&#20026;4096&#19988;&#36827;&#34892;1024&#36718;&#35757;&#32451;&#30340;SimCLR&#27169;&#22411;&#65292;&#25361;&#25112;&#20102;&#23545;&#27604;&#27169;&#22411;&#38656;&#35201;&#22823;&#25209;&#22823;&#23567;&#21644;&#22810;&#27425;&#35757;&#32451;&#36718;&#25968;&#30340;&#20449;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05490v1 Announce Type: cross  Abstract: Contrastive learning typically matches pairs of related views among a number of unrelated negative views. Views can be generated (e.g. by augmentations) or be observed. We investigate matching when there are more than two related views which we call poly-view tasks, and derive new representation learning objectives using information maximization and sufficient statistics. We show that with unlimited computation, one should maximize the number of related views, and with a fixed compute budget, it is beneficial to decrease the number of unique samples whilst increasing the number of views of those samples. In particular, poly-view contrastive models trained for 128 epochs with batch size 256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k, challenging the belief that contrastive models require large batch sizes and many training epochs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#20998;&#24067;&#28418;&#31227;&#24773;&#20917;&#19979;&#23398;&#20064;&#31163;&#25955;&#20998;&#24067;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#36807;&#21435;&#26679;&#26412;&#25968;&#37327;&#36873;&#25321;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#30340;&#30028;&#26469;&#34920;&#24449;&#32479;&#35745;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.05446</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#28418;&#31227;&#31163;&#25955;&#20998;&#24067;&#30340;&#25913;&#36827;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Improved Algorithm for Learning Drifting Discrete Distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05446
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#20998;&#24067;&#28418;&#31227;&#24773;&#20917;&#19979;&#23398;&#20064;&#31163;&#25955;&#20998;&#24067;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#36807;&#21435;&#26679;&#26412;&#25968;&#37327;&#36873;&#25321;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#30340;&#30028;&#26469;&#34920;&#24449;&#32479;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20998;&#24067;&#28418;&#31227;&#24773;&#20917;&#19979;&#23398;&#20064;&#31163;&#25955;&#20998;&#24067;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35266;&#23519;&#26469;&#33258;&#19968;&#20010;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#31163;&#25955;&#20998;&#24067;&#30340;&#29420;&#31435;&#26679;&#26412;&#24207;&#21015;&#65292;&#30446;&#26631;&#26159;&#20272;&#35745;&#24403;&#21069;&#20998;&#24067;&#12290;&#30001;&#20110;&#25105;&#20204;&#27599;&#20010;&#26102;&#38388;&#27493;&#21482;&#33021;&#35775;&#38382;&#19968;&#20010;&#26679;&#26412;&#65292;&#19968;&#20010;&#33391;&#22909;&#30340;&#20272;&#35745;&#38656;&#35201;&#35880;&#24910;&#36873;&#25321;&#35201;&#20351;&#29992;&#30340;&#36807;&#21435;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#20351;&#29992;&#26356;&#22810;&#26679;&#26412;&#65292;&#25105;&#20204;&#24517;&#39035;&#35785;&#35832;&#26356;&#26089;&#30340;&#26679;&#26412;&#65292;&#36825;&#20250;&#22240;&#20026;&#20998;&#24067;&#21464;&#21270;&#24341;&#20837;&#30340;&#20559;&#24046;&#32780;&#20135;&#29983;&#28418;&#31227;&#35823;&#24046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#25105;&#20204;&#20351;&#29992;&#36739;&#23569;&#30340;&#36807;&#21435;&#26679;&#26412;&#65292;&#20272;&#35745;&#20250;&#26377;&#36739;&#39640;&#30340;&#26041;&#24046;&#65292;&#23548;&#33268;&#36739;&#22823;&#30340;&#32479;&#35745;&#35823;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#36825;&#31181;&#26435;&#34913;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#28418;&#31227;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#19982;&#20197;&#21069;&#30340;&#33258;&#36866;&#24212;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#30340;&#30028;&#26469;&#34920;&#24449;&#32479;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05446v1 Announce Type: new  Abstract: We present a new adaptive algorithm for learning discrete distributions under distribution drift. In this setting, we observe a sequence of independent samples from a discrete distribution that is changing over time, and the goal is to estimate the current distribution. Since we have access to only a single sample for each time step, a good estimation requires a careful choice of the number of past samples to use. To use more samples, we must resort to samples further in the past, and we incur a drift error due to the bias introduced by the change in distribution. On the other hand, if we use a small number of past samples, we incur a large statistical error as the estimation has a high variance. We present a novel adaptive algorithm that can solve this trade-off without any prior knowledge of the drift. Unlike previous adaptive results, our algorithm characterizes the statistical error using data-dependent bounds. This technicality enab
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20004;&#27493;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#24179;&#22343;&#26041;&#24046;&#20272;&#35745;&#26041;&#27861;&#35782;&#21035;&#30446;&#26631;&#20989;&#25968;&#30340;&#26377;&#25928;&#38477;&#32500;&#23376;&#31354;&#38388;&#65292;&#28982;&#21518;&#22312;&#35813;&#23376;&#31354;&#38388;&#20869;&#26500;&#24314;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#24182;&#36827;&#34892;&#20248;&#21270;&#65292;&#33021;&#22815;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#26435;&#34913;&#65292;&#25552;&#39640;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;</title><link>https://arxiv.org/abs/2403.05425</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#33258;&#36866;&#24212;&#38477;&#32500;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Dimension Reduction Estimation Method for High-dimensional Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05425
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20004;&#27493;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#24179;&#22343;&#26041;&#24046;&#20272;&#35745;&#26041;&#27861;&#35782;&#21035;&#30446;&#26631;&#20989;&#25968;&#30340;&#26377;&#25928;&#38477;&#32500;&#23376;&#31354;&#38388;&#65292;&#28982;&#21518;&#22312;&#35813;&#23376;&#31354;&#38388;&#20869;&#26500;&#24314;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#24182;&#36827;&#34892;&#20248;&#21270;&#65292;&#33021;&#22815;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#26435;&#34913;&#65292;&#25552;&#39640;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20302;&#21040;&#20013;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;&#25193;&#23637;&#21040;&#39640;&#32500;&#35774;&#32622;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20004;&#27493;&#20248;&#21270;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#23567;&#24179;&#22343;&#26041;&#24046;&#20272;&#35745;&#65288;MAVE&#65289;&#26041;&#27861;&#35782;&#21035;&#30446;&#26631;&#20989;&#25968;&#30340;&#26377;&#25928;&#38477;&#32500;&#65288;EDR&#65289;&#23376;&#31354;&#38388;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22312;&#35813;EDR&#23376;&#31354;&#38388;&#20869;&#26500;&#24314;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#39044;&#26399;&#25913;&#36827;&#20934;&#21017;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#28789;&#27963;&#22320;&#21516;&#26102;&#25110;&#39034;&#24207;&#22320;&#25191;&#34892;&#36825;&#20123;&#27493;&#39588;&#12290;&#22312;&#39034;&#24207;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#23376;&#31354;&#38388;&#20272;&#35745;&#21644;&#20989;&#25968;&#20248;&#21270;&#20043;&#38388;&#20998;&#37197;&#37319;&#26679;&#39044;&#31639;&#26469;&#20180;&#32454;&#24179;&#34913;&#25506;&#32034;&#21033;&#29992;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#24050;&#32463;&#24471;&#21040;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05425v1 Announce Type: new  Abstract: Bayesian optimization (BO) has shown impressive results in a variety of applications within low-to-moderate dimensional Euclidean spaces. However, extending BO to high-dimensional settings remains a significant challenge. We address this challenge by proposing a two-step optimization framework. Initially, we identify the effective dimension reduction (EDR) subspace for the objective function using the minimum average variance estimation (MAVE) method. Subsequently, we construct a Gaussian process model within this EDR subspace and optimize it using the expected improvement criterion. Our algorithm offers the flexibility to operate these steps either concurrently or in sequence. In the sequential approach, we meticulously balance the exploration-exploitation trade-off by distributing the sampling budget between subspace estimation and function optimization, and the convergence rate of our algorithm in high-dimensional contexts has been es
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#30452;&#25509;&#35299;&#20915;&#30340;&#20248;&#21270;&#20219;&#21153;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#20272;&#35745;&#24847;&#35265;&#21160;&#24577;ABM&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.05358</link><description>&lt;p&gt;
&#24847;&#35265;&#21160;&#24577;&#27169;&#22411;&#20013;&#21442;&#25968;&#30340;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Variational Inference of Parameters in Opinion Dynamics Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05358
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#30452;&#25509;&#35299;&#20915;&#30340;&#20248;&#21270;&#20219;&#21153;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#20272;&#35745;&#24847;&#35265;&#21160;&#24577;ABM&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#20195;&#29702;&#20154;&#30340;&#27169;&#22411;&#65288;ABMs&#65289;&#22312;&#30740;&#31350;&#31038;&#20250;&#29616;&#35937;&#20013;&#34987;&#39057;&#32321;&#20351;&#29992;&#65292;&#20294;&#21442;&#25968;&#20272;&#35745;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#20272;&#35745;&#24847;&#35265;&#21160;&#24577;ABM&#30340;&#21442;&#25968;&#65292;&#36890;&#36807;&#23558;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#30452;&#25509;&#35299;&#20915;&#30340;&#20248;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05358v1 Announce Type: cross  Abstract: Despite the frequent use of agent-based models (ABMs) for studying social phenomena, parameter estimation remains a challenge, often relying on costly simulation-based heuristics. This work uses variational inference to estimate the parameters of an opinion dynamics ABM, by transforming the estimation problem into an optimization task that can be solved directly.   Our proposal relies on probabilistic generative ABMs (PGABMs): we start by synthesizing a probabilistic generative model from the ABM rules. Then, we transform the inference process into an optimization problem suitable for automatic differentiation. In particular, we use the Gumbel-Softmax reparameterization for categorical agent attributes and stochastic variational inference for parameter estimation. Furthermore, we explore the trade-offs of using variational distributions with different complexity: normal distributions and normalizing flows.   We validate our method on a
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21160;&#37327;&#23545;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#36712;&#36857;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#26041;&#27861;&#25214;&#21040;&#22266;&#26377;&#37327; $\lambda$&#65292;&#23545;&#20110;&#24674;&#22797;&#31232;&#30095;&#35299;&#20855;&#26377;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2403.05293</link><description>&lt;p&gt;
&#21033;&#29992;&#36830;&#32493;&#26102;&#38388;&#29702;&#35299;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#21160;&#37327;
&lt;/p&gt;
&lt;p&gt;
Leveraging Continuous Time to Understand Momentum When Training Diagonal Linear Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05293
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21160;&#37327;&#23545;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#36712;&#36857;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#26041;&#27861;&#25214;&#21040;&#22266;&#26377;&#37327; $\lambda$&#65292;&#23545;&#20110;&#24674;&#22797;&#31232;&#30095;&#35299;&#20855;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21160;&#37327;&#23545;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#36712;&#36857;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21033;&#29992;&#36830;&#32493;&#26102;&#38388;&#26041;&#27861;&#20998;&#26512;&#24102;&#26377;&#27493;&#38271; $\gamma$ &#21644;&#21160;&#37327;&#21442;&#25968; $\beta$ &#30340;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#65292;&#20174;&#32780;&#25214;&#21040;&#19968;&#20010;&#22266;&#26377;&#37327; $\lambda = \frac{ \gamma }{ (1 - \beta)^2 }$&#65292;&#36825;&#19968;&#37327;&#21807;&#19968;&#23450;&#20041;&#20102;&#20248;&#21270;&#36335;&#24452;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21152;&#36895;&#35268;&#21017;&#12290;&#22312;&#36229;&#21442;&#25968;&#21270;&#22238;&#24402;&#35774;&#32622;&#20013;&#35757;&#32451; $2$ &#23618;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#26102;&#65292;&#36890;&#36807;&#19968;&#20010;&#38544;&#24335;&#27491;&#21017;&#21270;&#38382;&#39064;&#26469;&#34920;&#24449;&#24674;&#22797;&#30340;&#35299;&#12290;&#28982;&#21518;&#35777;&#26126;&#23567;&#30340; $\lambda$ &#20540;&#26377;&#21161;&#20110;&#24674;&#22797;&#31232;&#30095;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#38543;&#26426;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#30340;&#31867;&#20284;&#20294;&#36739;&#24369;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#25903;&#25345;&#25105;&#20204;&#35770;&#26029;&#30340;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05293v1 Announce Type: new  Abstract: In this work, we investigate the effect of momentum on the optimisation trajectory of gradient descent. We leverage a continuous-time approach in the analysis of momentum gradient descent with step size $\gamma$ and momentum parameter $\beta$ that allows us to identify an intrinsic quantity $\lambda = \frac{ \gamma }{ (1 - \beta)^2 }$ which uniquely defines the optimisation path and provides a simple acceleration rule. When training a $2$-layer diagonal linear network in an overparametrised regression setting, we characterise the recovered solution through an implicit regularisation problem. We then prove that small values of $\lambda$ help to recover sparse solutions. Finally, we give similar but weaker results for stochastic momentum gradient descent. We provide numerical experiments which support our claims.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#20026;&#20219;&#20309;Copula&#29983;&#25104;&#20934;&#38543;&#26426;&#26679;&#26412;&#30340;&#39640;&#25928;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.05281</link><description>&lt;p&gt;
&#19968;&#31181;&#39640;&#25928;&#30340;&#29992;&#20110;Copulas&#30340;&#20934;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Quasi-Random Sampling for Copulas
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05281
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#20026;&#20219;&#20309;Copula&#29983;&#25104;&#20934;&#38543;&#26426;&#26679;&#26412;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#33945;&#29305;&#21345;&#32599;&#35745;&#31639;&#20013;&#29992;&#20110;Copulas&#30340;&#39640;&#25928;&#20934;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#26465;&#20214;&#20998;&#24067;&#27861;&#65288;CDM&#65289;&#22312;&#22788;&#29702;&#39640;&#32500;&#25110;&#38544;&#24335;Copulas&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#25351;&#30340;&#26159;&#37027;&#20123;&#26080;&#27861;&#36890;&#36807;&#29616;&#26377;&#21442;&#25968;Copulas&#20934;&#30830;&#34920;&#31034;&#30340;Copulas&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#65292;&#20026;&#20219;&#20309;Copula&#29983;&#25104;&#20934;&#38543;&#26426;&#26679;&#26412;&#12290;GANs&#26159;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#31616;&#21270;&#25277;&#26679;&#36807;&#31243;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;GANs&#34987;&#29992;&#26469;&#23398;&#20064;&#20174;&#22343;&#21248;&#20998;&#24067;&#21040;Copulas&#30340;&#26144;&#23556;&#12290;&#19968;&#26086;&#23398;&#20064;&#20102;&#36825;&#31181;&#26144;&#23556;&#65292;&#20174;Copula&#33719;&#21462;&#20934;&#38543;&#26426;&#26679;&#26412;&#21482;&#38656;&#36755;&#20837;&#26469;&#33258;&#22343;&#21248;&#20998;&#24067;&#30340;&#20934;&#38543;&#26426;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#20219;&#20309;Copula&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05281v1 Announce Type: new  Abstract: This paper examines an efficient method for quasi-random sampling of copulas in Monte Carlo computations. Traditional methods, like conditional distribution methods (CDM), have limitations when dealing with high-dimensional or implicit copulas, which refer to those that cannot be accurately represented by existing parametric copulas. Instead, this paper proposes the use of generative models, such as Generative Adversarial Networks (GANs), to generate quasi-random samples for any copula. GANs are a type of implicit generative models used to learn the distribution of complex data, thus facilitating easy sampling. In our study, GANs are employed to learn the mapping from a uniform distribution to copulas. Once this mapping is learned, obtaining quasi-random samples from the copula only requires inputting quasi-random samples from the uniform distribution. This approach offers a more flexible method for any copula. Additionally, we provide t
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#23481;&#26131;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#19968;&#38382;&#39064;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#25345;&#32493;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05175</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#19982;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Continual Learning and Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05175
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#23481;&#26131;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#19968;&#38382;&#39064;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#25345;&#32493;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#31456;&#33410;&#25506;&#35752;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#21160;&#24577;&#36807;&#31243;&#65292;&#21363;&#20174;&#38750;&#38745;&#24577;&#25968;&#25454;&#27969;&#20013;&#36880;&#27493;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#25345;&#32493;&#23398;&#20064;&#26159;&#20154;&#33041;&#30340;&#19968;&#31181;&#33258;&#28982;&#25216;&#33021;&#65292;&#20294;&#23545;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#21364;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;&#22312;&#23398;&#20064;&#26032;&#30693;&#35782;&#26102;&#65292;&#36825;&#20123;&#32593;&#32476;&#24448;&#24448;&#20250;&#36805;&#36895;&#32780;&#24443;&#24213;&#22320;&#24536;&#35760;&#20197;&#21069;&#25152;&#23398;&#30340;&#20869;&#23481;&#65292;&#36825;&#19968;&#29616;&#35937;&#34987;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25345;&#32493;&#23398;&#20064;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#26412;&#20070;&#31456;&#33410;&#22238;&#39038;&#20102;&#36825;&#19968;&#39046;&#22495;&#20135;&#29983;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05175v1 Announce Type: cross  Abstract: This book chapter delves into the dynamics of continual learning, which is the process of incrementally learning from a non-stationary stream of data. Although continual learning is a natural skill for the human brain, it is very challenging for artificial neural networks. An important reason is that, when learning something new, these networks tend to quickly and drastically forget what they had learned before, a phenomenon known as catastrophic forgetting. Especially in the last decade, continual learning has become an extensively studied topic in deep learning. This book chapter reviews the insights that this field has generated.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#36138;&#23146;&#26041;&#24335;&#22312;&#27599;&#19968;&#27493;&#35782;&#21035;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#29702;&#35770;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05138</link><description>&lt;p&gt;
&#22522;&#20110;&#36138;&#23146;&#26041;&#27861;&#30340;&#20998;&#31867;&#22120;&#30456;&#20851;&#29305;&#24449;&#36873;&#25321;: &#36138;&#23146;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
Greedy feature selection: Classifier-dependent feature selection via greedy methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#36138;&#23146;&#26041;&#24335;&#22312;&#27599;&#19968;&#27493;&#35782;&#21035;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#29702;&#35770;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#25490;&#21517;&#26041;&#27861;&#65292;&#21363;&#36138;&#23146;&#29305;&#24449;&#36873;&#25321;&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;&#22312;&#32479;&#35745;&#23398;&#20064;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#19982;&#24212;&#29992;&#20110;&#21033;&#29992;&#20943;&#23569;&#25968;&#37327;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#30340;&#20998;&#31867;&#22120;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#12290;&#30456;&#21453;&#65292;&#36138;&#23146;&#29305;&#24449;&#36873;&#25321;&#26681;&#25454;&#36873;&#23450;&#30340;&#20998;&#31867;&#22120;&#22312;&#27599;&#19968;&#27493;&#35782;&#21035;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#20174;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#26696;&#30340;&#20248;&#28857;&#65292;&#22914;Vapnik-Chervonenkis&#65288;VC&#65289;&#32500;&#24230;&#25110;&#26680;&#23545;&#40784;&#31561;&#27169;&#22411;&#33021;&#21147;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#22826;&#38451;&#27963;&#21160;&#30340;&#22320;&#36136;&#25928;&#24212;&#34920;&#29616;&#38382;&#39064;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#25968;&#20540;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05138v1 Announce Type: cross  Abstract: The purpose of this study is to introduce a new approach to feature ranking for classification tasks, called in what follows greedy feature selection. In statistical learning, feature selection is usually realized by means of methods that are independent of the classifier applied to perform the prediction using that reduced number of features. Instead, greedy feature selection identifies the most important feature at each step and according to the selected classifier. In the paper, the benefits of such scheme are investigated theoretically in terms of model capacity indicators, such as the Vapnik-Chervonenkis (VC) dimension or the kernel alignment, and tested numerically by considering its application to the problem of predicting geo-effective manifestations of the active Sun.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25351;&#20986;&#22312;&#23545;&#25239;&#24615;&#21644;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#65292;&#22522;&#20110;&#38543;&#26426;&#25200;&#21160;&#30340;Follow-the-Perturbed-Leader&#31574;&#30053;&#20855;&#26377;&#24343;&#27463;&#27888;&#23572;&#20998;&#24067;&#23614;&#37096;&#26368;&#20248;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#36873;&#25321;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05134</link><description>&lt;p&gt;
&#24102;&#26377;&#24343;&#27463;&#27888;&#23572;&#20998;&#24067;&#30340;&#25200;&#21160;&#39046;&#23548;&#32773;&#36861;&#36394;&#65306;&#22312;&#23545;&#25239;&#24615;&#36172;&#21338;&#26426;&#21644;&#26368;&#20339;&#36873;&#25321;&#20013;&#30340;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Follow-the-Perturbed-Leader with Fr\'{e}chet-type Tail Distributions: Optimality in Adversarial Bandits and Best-of-Both-Worlds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05134
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25351;&#20986;&#22312;&#23545;&#25239;&#24615;&#21644;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#65292;&#22522;&#20110;&#38543;&#26426;&#25200;&#21160;&#30340;Follow-the-Perturbed-Leader&#31574;&#30053;&#20855;&#26377;&#24343;&#27463;&#27888;&#23572;&#20998;&#24067;&#23614;&#37096;&#26368;&#20248;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#36873;&#25321;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#24615;&#21644;&#38543;&#26426;$K$&#33218;&#32769;&#34382;&#26426;&#20013;Follow-the-Perturbed-Leader&#65288;FTPL&#65289;&#31574;&#30053;&#30340;&#26368;&#20248;&#24615;&#12290;&#23613;&#31649;Follow-the-Regularized-Leader&#65288;FTRL&#65289;&#26694;&#26550;&#22312;&#21508;&#31181;&#27491;&#21017;&#21270;&#36873;&#25321;&#19979;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20381;&#36182;&#20110;&#38543;&#26426;&#25200;&#21160;&#30340;FTPL&#26694;&#26550;&#21364;&#40092;&#26377;&#20851;&#27880;&#65292;&#23613;&#31649;&#20854;&#22266;&#26377;&#30340;&#31616;&#21333;&#24615;&#12290;&#22312;&#23545;&#25239;&#24615;&#36172;&#21338;&#26426;&#20013;&#65292;FTPL&#33509;&#25200;&#21160;&#36981;&#24490;&#20855;&#26377;&#24343;&#27463;&#27888;&#23572;&#23614;&#37096;&#30340;&#20998;&#24067;&#65292;&#26377;&#20154;&#29468;&#24819;&#21487;&#33021;&#21487;&#20197;&#23454;&#29616;$\mathcal{O}(\sqrt{KT})$&#30340;&#21518;&#24724;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#24418;&#29366;$\alpha=2$&#30340;Fr\'{e}chet&#20998;&#24067;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;FTPL&#30830;&#23454;&#36798;&#21040;&#20102;&#36825;&#19968;&#30028;&#38480;&#65292;&#24182;&#19988;&#22312;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#23545;&#25968;&#21518;&#24724;&#65292;&#24847;&#21619;&#30528;FTPL&#20855;&#22791;&#20102;&#26368;&#20339;&#36873;&#25321;&#65288;BOBW&#65289;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#32467;&#26524;&#21482;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#19978;&#36848;&#29468;&#24819;&#65292;&#22240;&#20026;&#20182;&#20204;&#30340;&#20998;&#26512;&#20005;&#37325;&#20381;&#36182;&#20110;Fr\'{e
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05134v1 Announce Type: cross  Abstract: This paper studies the optimality of the Follow-the-Perturbed-Leader (FTPL) policy in both adversarial and stochastic $K$-armed bandits. Despite the widespread use of the Follow-the-Regularized-Leader (FTRL) framework with various choices of regularization, the FTPL framework, which relies on random perturbations, has not received much attention, despite its inherent simplicity. In adversarial bandits, there has been conjecture that FTPL could potentially achieve $\mathcal{O}(\sqrt{KT})$ regrets if perturbations follow a distribution with a Fr\'{e}chet-type tail. Recent work by Honda et al. (2023) showed that FTPL with Fr\'{e}chet distribution with shape $\alpha=2$ indeed attains this bound and, notably logarithmic regret in stochastic bandits, meaning the Best-of-Both-Worlds (BOBW) capability of FTPL. However, this result only partly resolves the above conjecture because their analysis heavily relies on the specific form of the Fr\'{e
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#26041;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#20010;&#20307;&#19981;&#21516;&#20559;&#22909;&#30340;&#20803;&#23398;&#20064;&#19982;&#19981;&#21516;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#30340;&#37319;&#29992;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;RLHF&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#24182;&#24179;&#34913;&#22810;&#20010;&#20010;&#20307;&#20559;&#22909;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05006</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#20803;&#20154;&#31867;&#21453;&#39304;&#30340;&#21487;&#35777;&#26126;&#22810;&#26041;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-Party Reinforcement Learning with Diverse Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05006
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#26041;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#20010;&#20307;&#19981;&#21516;&#20559;&#22909;&#30340;&#20803;&#23398;&#20064;&#19982;&#19981;&#21516;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#30340;&#37319;&#29992;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;RLHF&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#24182;&#24179;&#34913;&#22810;&#20010;&#20010;&#20307;&#20559;&#22909;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#33539;&#24335;&#65292;&#26088;&#22312;&#23558;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#26126;&#30830;&#24314;&#27169;&#22810;&#20010;&#20010;&#20307;&#19981;&#21516;&#20559;&#22909;&#30340;&#22810;&#26041;RLHF&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20256;&#32479;RLHF&#26041;&#27861;&#22914;&#20309;&#22833;&#36133;&#65292;&#22240;&#20026;&#23398;&#20064;&#21333;&#19968;&#22870;&#21169;&#20989;&#25968;&#26080;&#27861;&#25429;&#25417;&#21644;&#24179;&#34913;&#22810;&#20010;&#20010;&#20307;&#30340;&#20559;&#22909;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#32467;&#21512;&#20803;&#23398;&#20064;&#26469;&#23398;&#20064;&#22810;&#20010;&#20559;&#22909;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#26469;&#25972;&#21512;&#22810;&#26041;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#20851;&#27880;&#31163;&#32447;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#20026;&#20248;&#21270;&#19981;&#21516;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#65288;&#22914;Nash&#12289;Utilitarian&#21644;Leximin&#31119;&#21033;&#65289;&#24314;&#31435;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#21516;&#26102;&#25552;&#20379;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05006v1 Announce Type: cross  Abstract: Reinforcement learning with human feedback (RLHF) is an emerging paradigm to align models with human preferences. Typically, RLHF aggregates preferences from multiple individuals who have diverse viewpoints that may conflict with each other. Our work \textit{initiates} the theoretical study of multi-party RLHF that explicitly models the diverse preferences of multiple individuals. We show how traditional RLHF approaches can fail since learning a single reward function cannot capture and balance the preferences of multiple individuals. To overcome such limitations, we incorporate meta-learning to learn multiple preferences and adopt different social welfare functions to aggregate the preferences across multiple parties. We focus on the offline learning setting and establish sample complexity bounds, along with efficiency and fairness guarantees, for optimizing diverse social welfare functions such as Nash, Utilitarian, and Leximin welfa
&lt;/p&gt;</description></item><item><title>Stacking&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#35299;&#37322;&#65292;&#21363;&#23454;&#29616;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#24418;&#24335;&#65292;&#24182;&#35777;&#26126;&#23545;&#20110;&#26576;&#20123;&#28145;&#24230;&#32447;&#24615;&#27531;&#24046;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#21152;&#36895;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.04978</link><description>&lt;p&gt;
Stacking&#20316;&#20026;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stacking as Accelerated Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04978
&lt;/p&gt;
&lt;p&gt;
Stacking&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#35299;&#37322;&#65292;&#21363;&#23454;&#29616;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#24418;&#24335;&#65292;&#24182;&#35777;&#26126;&#23545;&#20110;&#26576;&#20123;&#28145;&#24230;&#32447;&#24615;&#27531;&#24046;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#21152;&#36895;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stacking&#26159;&#19968;&#31181;&#21551;&#21457;&#24335;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#23618;&#25968;&#24182;&#36890;&#36807;&#20174;&#26087;&#23618;&#22797;&#21046;&#21442;&#25968;&#26469;&#21021;&#22987;&#21270;&#26032;&#23618;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;Stacking&#26377;&#25928;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;&#65306;&#21363;&#65292;Stacking&#23454;&#29616;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#30340;&#19968;&#31181;&#24418;&#24335;&#12290;&#35813;&#29702;&#35770;&#36824;&#28085;&#30422;&#20102;&#35832;&#22914;&#25552;&#21319;&#26041;&#27861;&#20013;&#26500;&#24314;&#30340;&#21152;&#27861;&#38598;&#25104;&#31561;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#24182;&#20026;&#27599;&#19968;&#36718;&#25552;&#21319;&#36807;&#31243;&#20013;&#21021;&#22987;&#21270;&#26032;&#20998;&#31867;&#22120;&#30340;&#31867;&#20284;&#24191;&#27867;&#20351;&#29992;&#30340;&#23454;&#29992;&#21551;&#21457;&#24335;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23545;&#20110;&#26576;&#20123;&#28145;&#24230;&#32447;&#24615;&#27531;&#24046;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#30340;&#28508;&#33021;&#20989;&#25968;&#20998;&#26512;&#65292;Stacking&#30830;&#23454;&#25552;&#20379;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#20174;&#32780;&#20801;&#35768;&#26356;&#26032;&#20013;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04978v1 Announce Type: new  Abstract: Stacking, a heuristic technique for training deep residual networks by progressively increasing the number of layers and initializing new layers by copying parameters from older layers, has proven quite successful in improving the efficiency of training deep neural networks. In this paper, we propose a theoretical explanation for the efficacy of stacking: viz., stacking implements a form of Nesterov's accelerated gradient descent. The theory also covers simpler models such as the additive ensembles constructed in boosting methods, and provides an explanation for a similar widely-used practical heuristic for initializing the new classifier in each round of boosting. We also prove that for certain deep linear residual networks, stacking does provide accelerated training, via a new potential function analysis of the Nesterov's accelerated gradient method which allows errors in updates. We conduct proof-of-concept experiments to validate our
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#35299;&#20915;&#26377;&#38480;&#29366;&#24577;&#22343;&#22330;&#21338;&#24328;&#30340;&#20027;&#26041;&#31243;&#65292;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04975</link><description>&lt;p&gt;
&#28145;&#24230;&#21453;&#21521;&#21644;Galerkin&#26041;&#27861;&#29992;&#20110;&#26377;&#38480;&#29366;&#24577;&#20027;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep Backward and Galerkin Methods for the Finite State Master Equation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04975
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#35299;&#20915;&#26377;&#38480;&#29366;&#24577;&#22343;&#22330;&#21338;&#24328;&#30340;&#20027;&#26041;&#31243;&#65292;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26377;&#38480;&#29366;&#24577;&#22343;&#22330;&#21338;&#24328;&#30340;&#20027;&#26041;&#31243;&#12290;&#35299;&#20915;MFGs&#20026;&#20855;&#26377;&#26377;&#38480;&#20294;&#22823;&#37327;&#20195;&#29702;&#20154;&#32676;&#30340;&#38543;&#26426;&#12289;&#24494;&#20998;&#21338;&#24328;&#25552;&#20379;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#12290;&#20027;&#26041;&#31243;&#26159;&#19968;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#20854;&#35299;&#34920;&#24449;&#20219;&#20309;&#21487;&#33021;&#30340;&#21021;&#22987;&#20998;&#24067;&#19979;&#30340;MFG&#22343;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#22312;&#26102;&#38388;&#20998;&#37327;&#19978;&#30340;&#21453;&#21521;&#24402;&#32435;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#30452;&#25509;&#35299;&#20915;&#20102;PDE&#65292;&#32780;&#26080;&#38656;&#31163;&#25955;&#21270;&#26102;&#38388;&#12290;&#23545;&#20110;&#20004;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#32467;&#26524;&#65306;&#23384;&#22312;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#20351;&#31639;&#27861;&#30340;&#25439;&#22833;&#20989;&#25968;&#20219;&#24847;&#23567;&#65292;&#24182;&#19988;&#21453;&#20043;&#65292;&#22914;&#26524;&#25439;&#22833;&#24456;&#23567;&#65292;&#21017;&#31070;&#32463;&#32593;&#32476;&#26159;&#20027;&#26041;&#31243;&#35299;&#30340;&#33391;&#22909;&#36817;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#25991;&#29486;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#36827;&#34892;&#20102;&#32500;&#24230;&#20026;15&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#20570;&#20986;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04975v1 Announce Type: cross  Abstract: This paper proposes and analyzes two neural network methods to solve the master equation for finite-state mean field games (MFGs). Solving MFGs provides approximate Nash equilibria for stochastic, differential games with finite but large populations of agents. The master equation is a partial differential equation (PDE) whose solution characterizes MFG equilibria for any possible initial distribution. The first method we propose relies on backward induction in a time component while the second method directly tackles the PDE without discretizing time. For both approaches, we prove two types of results: there exist neural networks that make the algorithms' loss functions arbitrarily small, and conversely, if the losses are small, then the neural networks are good approximations of the master equation's solution. We conclude the paper with numerical experiments on benchmark problems from the literature up to dimension 15, and a compariso
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;R&#233;nyi-DP&#25512;&#23548;&#36890;&#36807;&#23376;&#25277;&#26679;&#30340;&#25918;&#22823;&#20445;&#35777;&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#38544;&#31169;&#26680;&#31639;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#20063;&#20855;&#26377;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04867</link><description>&lt;p&gt;
&#32452;&#38544;&#31169;&#25918;&#22823;&#21644;&#23376;&#25277;&#26679;&#30340;R&#233;nyi&#24046;&#20998;&#38544;&#31169;&#32479;&#19968;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Group Privacy Amplification and Unified Amplification by Subsampling for R\'enyi Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;R&#233;nyi-DP&#25512;&#23548;&#36890;&#36807;&#23376;&#25277;&#26679;&#30340;&#25918;&#22823;&#20445;&#35777;&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#38544;&#31169;&#26680;&#31639;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#20063;&#20855;&#26377;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;(DP)&#20855;&#26377;&#22810;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#22914;&#23545;&#21518;&#22788;&#29702;&#30340;&#40065;&#26834;&#24615;&#12289;&#32452;&#38544;&#31169;&#21644;&#36890;&#36807;&#23376;&#25277;&#26679;&#25918;&#22823;&#65292;&#36825;&#20123;&#23646;&#24615;&#21487;&#20197;&#30456;&#20114;&#29420;&#31435;&#25512;&#23548;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#26159;&#21542;&#36890;&#36807;&#32852;&#21512;&#32771;&#34385;&#36825;&#20123;&#23646;&#24615;&#20013;&#30340;&#22810;&#20010;&#21487;&#20197;&#33719;&#24471;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32452;&#38544;&#31169;&#21644;&#36890;&#36807;&#23376;&#25277;&#26679;&#25918;&#22823;&#30340;&#32452;&#21512;&#12290;&#20026;&#20102;&#25552;&#20379;&#36866;&#21512;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20445;&#35777;&#65292;&#25105;&#20204;&#22312;R&#233;nyi-DP&#26694;&#26550;&#20013;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#36825;&#27604;$(\epsilon,\delta)$-DP&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#32452;&#21512;&#23646;&#24615;&#12290;&#20316;&#20026;&#36825;&#20010;&#20998;&#26512;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;R&#233;nyi-DP&#25512;&#23548;&#36890;&#36807;&#23376;&#25277;&#26679;&#30340;&#25918;&#22823;&#20445;&#35777;&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#38544;&#31169;&#26680;&#31639;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#20063;&#20855;&#26377;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23427;&#19981;&#20165;&#35753;&#25105;&#20204;&#25913;&#36827;&#21644;&#27867;&#21270;&#29616;&#26377;&#30340;&#25918;&#22823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04867v1 Announce Type: cross  Abstract: Differential privacy (DP) has various desirable properties, such as robustness to post-processing, group privacy, and amplification by subsampling, which can be derived independently of each other. Our goal is to determine whether stronger privacy guarantees can be obtained by considering multiple of these properties jointly. To this end, we focus on the combination of group privacy and amplification by subsampling. To provide guarantees that are amenable to machine learning algorithms, we conduct our analysis in the framework of R\'enyi-DP, which has more favorable composition properties than $(\epsilon,\delta)$-DP. As part of this analysis, we develop a unified framework for deriving amplification by subsampling guarantees for R\'enyi-DP, which represents the first such framework for a privacy accounting method and is of independent interest. We find that it not only lets us improve upon and generalize existing amplification results 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#32467;&#26500;&#20449;&#24687;&#26469;&#24341;&#23548;&#20462;&#21098;&#30340;&#26041;&#27861; DASH &#22312;&#23398;&#20064;&#21160;&#24577;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#27169;&#22411;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#26356;&#26377;&#24847;&#20041;&#30340;&#29983;&#29289;&#23398;&#35265;&#35299;</title><link>https://arxiv.org/abs/2403.04805</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#31080;&#25454;&#37117;&#26159;&#24179;&#31561;&#30340;&#65292;&#32780;&#25105;&#20204;&#30693;&#36947;&#65306;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#24341;&#23548;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Not all tickets are equal and we know it: Guiding pruning with domain-specific knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04805
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#32467;&#26500;&#20449;&#24687;&#26469;&#24341;&#23548;&#20462;&#21098;&#30340;&#26041;&#27861; DASH &#22312;&#23398;&#20064;&#21160;&#24577;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#27169;&#22411;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#26356;&#26377;&#24847;&#20041;&#30340;&#29983;&#29289;&#23398;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32467;&#26500;&#23398;&#20064;&#23545;&#20110;&#31185;&#23398;&#21457;&#29616;&#21644;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#20391;&#37325;&#20110;&#35745;&#31639;&#36164;&#28304;&#25928;&#29575;&#30340;&#20462;&#21098;&#31639;&#27861;&#22312;&#36873;&#25321;&#31526;&#21512;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#30340;&#26377;&#24847;&#20041;&#27169;&#22411;&#26041;&#38754;&#38754;&#20020;&#31639;&#27861;&#38556;&#30861;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DASH&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#32467;&#26500;&#20449;&#24687;&#26469;&#24341;&#23548;&#20462;&#21098;&#12290;&#22312;&#23398;&#20064;&#21160;&#24577;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DASH&#19982;&#29616;&#26377;&#19968;&#33324;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19982;&#29983;&#29289;&#23398;&#19968;&#33268;&#30340;&#25968;&#25454;&#29305;&#23450;&#35265;&#35299;&#12290;&#23545;&#20110;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20855;&#26377;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#30340;&#21512;&#25104;&#25968;&#25454;&#21644;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;DASH&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#24456;&#22823;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#26377;&#24847;&#20041;&#30340;&#29983;&#29289;&#23398;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#32467;&#26500;&#20449;&#24687;&#20855;&#26377;&#25552;&#39640;&#27169;&#22411;&#34893;&#29983;&#31185;&#23398;&#27934;&#35265;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04805v1 Announce Type: new  Abstract: Neural structure learning is of paramount importance for scientific discovery and interpretability. Yet, contemporary pruning algorithms that focus on computational resource efficiency face algorithmic barriers to select a meaningful model that aligns with domain expertise. To mitigate this challenge, we propose DASH, which guides pruning by available domain-specific structural information. In the context of learning dynamic gene regulatory network models, we show that DASH combined with existing general knowledge on interaction partners provides data-specific insights aligned with biology. For this task, we show on synthetic data with ground truth information and two real world applications the effectiveness of DASH, which outperforms competing methods by a large margin and provides more meaningful biological insights. Our work shows that domain specific structural information bears the potential to improve model-derived scientific insi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ShapleyBO&#26694;&#26550;&#65292;&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#35758;&#65292;&#37327;&#21270;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20248;&#21270;&#36807;&#31243;&#30340;&#36129;&#29486;&#65292;&#24182;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25506;&#32034;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.04629</link><description>&lt;p&gt;
&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04629
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ShapleyBO&#26694;&#26550;&#65292;&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#35758;&#65292;&#37327;&#21270;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20248;&#21270;&#36807;&#31243;&#30340;&#36129;&#29486;&#65292;&#24182;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25506;&#32034;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#19982;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#40657;&#21283;&#23376;&#20248;&#21270;&#38382;&#39064;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;BO&#26412;&#36523;&#20063;&#24120;&#24120;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#40657;&#21283;&#23376;&#65292;&#32570;&#20047;&#25552;&#20379;&#20026;&#20309;&#25552;&#35758;&#35780;&#20272;&#26576;&#20123;&#21442;&#25968;&#30340;&#29702;&#30001;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;ShapleyBO&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#21338;&#24328;&#35770;Shapley&#20540;&#35299;&#37322;BO&#25552;&#35758;&#30340;&#26694;&#26550;&#12290;&#23427;&#37327;&#21270;&#20102;&#27599;&#20010;&#21442;&#25968;&#23545;BO&#30340;&#25910;&#33719;&#20989;&#25968;&#30340;&#36129;&#29486;&#12290;&#21033;&#29992;Shapley&#20540;&#30340;&#32447;&#24615;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#36827;&#19968;&#27493;&#30830;&#23450;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20687;&#32622;&#20449;&#36793;&#30028;&#36825;&#26679;&#30340;&#21152;&#27861;&#25910;&#33719;&#20989;&#25968;&#25512;&#21160;BO&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#24378;&#24230;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ShapleyBO&#33021;&#22815;&#35299;&#20915;&#25506;&#32034;&#23545;&#20110;&#21208;&#25506;aleatoric&#21644;&#35748;&#35782;epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04629v1 Announce Type: cross  Abstract: Bayesian optimization (BO) with Gaussian processes (GP) has become an indispensable algorithm for black box optimization problems. Not without a dash of irony, BO is often considered a black box itself, lacking ways to provide reasons as to why certain parameters are proposed to be evaluated. This is particularly relevant in human-in-the-loop applications of BO, such as in robotics. We address this issue by proposing ShapleyBO, a framework for interpreting BO's proposals by game-theoretic Shapley values.They quantify each parameter's contribution to BO's acquisition function. Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO's exploration and exploitation for additive acquisition functions like the confidence bound. We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty. Moreover, our method 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.04493</link><description>&lt;p&gt;
&#20351;&#22270;&#20687;&#30495;&#23454;&#30340;&#22240;&#32032;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What makes an image realistic?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04493
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#25105;&#20204;&#22312;&#29983;&#25104;&#30475;&#36215;&#26469;&#30495;&#23454;&#30340;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#26080;&#35770;&#26159;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#36824;&#26159;&#35270;&#39057;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#20043;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#21363;&#37327;&#21270;&#29616;&#23454;&#20027;&#20041;&#65292;&#21363;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#12290;&#20174;&#31639;&#27861;&#20449;&#24687;&#29702;&#35770;&#30340;&#35266;&#28857;&#20986;&#21457;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20026;&#20160;&#20040;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#20026;&#20160;&#20040;&#19968;&#20010;&#22909;&#30340;&#29983;&#25104;&#27169;&#22411;&#21333;&#29420;&#19981;&#33021;&#35299;&#20915;&#23427;&#65292;&#20197;&#21450;&#19968;&#20010;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#24212;&#35813;&#26159;&#20160;&#20040;&#26679;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#65292;&#19981;&#20687;&#23545;&#25239;&#24615;&#35780;&#35770;&#32773;&#37027;&#26679;&#38656;&#35201;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#23613;&#31649;&#36890;&#29992;&#35780;&#35770;&#32773;&#24182;&#19981;&#31435;&#21363;&#23454;&#29992;&#65292;&#20294;&#23427;&#20204;&#26082;&#21487;&#20197;&#20316;&#20026;&#24341;&#23548;&#23454;&#38469;&#23454;&#29616;&#30340;&#21271;&#26497;&#26143;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04493v1 Announce Type: new  Abstract: The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI. Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training. While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#27169;&#22411;&#30340;&#28508;&#22312;&#28151;&#21512;&#24230;&#37327;&#30340;&#26641;&#29366;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#24635;&#32467;&#21644;&#36873;&#25321;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#21442;&#25968;&#20165;&#20855;&#26377;&#36739;&#24369;&#21487;&#35782;&#21035;&#24615;&#26102;&#19968;&#33268;&#22320;&#36873;&#25321;&#30495;&#23454;&#28151;&#21512;&#32452;&#20998;&#30340;&#25968;&#37327;&#65292;&#24182;&#20174;&#26641;&#20013;&#33719;&#24471;&#21442;&#25968;&#20272;&#35745;&#30340;&#36880;&#28857;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01684</link><description>&lt;p&gt;
&#28151;&#21512;&#24230;&#37327;&#30340;&#26641;&#29366;&#22270;&#65306;&#23398;&#20064;&#28508;&#22312;&#23618;&#27425;&#32467;&#26500;&#21644;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#30340;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Dendrogram of mixing measures: Learning latent hierarchy and model selection for finite mixture models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01684
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#27169;&#22411;&#30340;&#28508;&#22312;&#28151;&#21512;&#24230;&#37327;&#30340;&#26641;&#29366;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#24635;&#32467;&#21644;&#36873;&#25321;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#21442;&#25968;&#20165;&#20855;&#26377;&#36739;&#24369;&#21487;&#35782;&#21035;&#24615;&#26102;&#19968;&#33268;&#22320;&#36873;&#25321;&#30495;&#23454;&#28151;&#21512;&#32452;&#20998;&#30340;&#25968;&#37327;&#65292;&#24182;&#20174;&#26641;&#20013;&#33719;&#24471;&#21442;&#25968;&#20272;&#35745;&#30340;&#36880;&#28857;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#36807;&#24230;&#25311;&#21512;&#30340;&#28508;&#22312;&#28151;&#21512;&#24230;&#37327;&#30340;&#23618;&#27425;&#32858;&#31867;&#26641;&#65288;&#26641;&#29366;&#22270;&#65289;&#26469;&#27719;&#24635;&#21644;&#36873;&#25321;&#28151;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36830;&#25509;&#20102;&#20957;&#32858;&#24335;&#23618;&#27425;&#32858;&#31867;&#21644;&#28151;&#21512;&#24314;&#27169;&#12290;&#26641;&#29366;&#22270;&#30340;&#26500;&#24314;&#28304;&#33258;&#28151;&#21512;&#24230;&#37327;&#30340;&#25910;&#25947;&#29702;&#35770;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26082;&#21487;&#20197;&#19968;&#33268;&#22320;&#36873;&#25321;&#30495;&#23454;&#28151;&#21512;&#32452;&#20998;&#30340;&#25968;&#37327;&#65292;&#20063;&#21487;&#20197;&#20174;&#26641;&#20013;&#33719;&#24471;&#21442;&#25968;&#20272;&#35745;&#30340;&#36880;&#28857;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#21363;&#20351;&#27169;&#22411;&#21442;&#25968;&#20165;&#20855;&#26377;&#36739;&#24369;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#23427;&#38416;&#36848;&#20102;&#22312;&#23618;&#27425;&#32858;&#31867;&#20013;&#36873;&#25321;&#26368;&#20339;&#32676;&#38598;&#25968;&#30340;&#36873;&#25321;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#28151;&#21512;&#27169;&#22411;&#27719;&#24635;&#26041;&#24335;&#30456;&#27604;&#65292;&#26641;&#29366;&#22270;&#25581;&#31034;&#20102;&#26377;&#20851;&#20122;&#32676;&#23618;&#27425;&#30340;&#26356;&#22810;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#39033;&#27169;&#25311;&#30740;&#31350;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#24212;&#29992;&#31243;&#24207;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01684v1 Announce Type: cross  Abstract: We present a new way to summarize and select mixture models via the hierarchical clustering tree (dendrogram) of an overfitted latent mixing measure. Our proposed method bridges agglomerative hierarchical clustering and mixture modeling. The dendrogram's construction is derived from the theory of convergence of the mixing measures, and as a result, we can both consistently select the true number of mixing components and obtain the pointwise optimal convergence rate for parameter estimation from the tree, even when the model parameters are only weakly identifiable. In theory, it explicates the choice of the optimal number of clusters in hierarchical clustering. In practice, the dendrogram reveals more information on the hierarchy of subpopulations compared to traditional ways of summarizing mixture models. Several simulation studies are carried out to support our theory. We also illustrate the methodology with an application to single-c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#36125;&#21494;&#26031;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#26469;&#26377;&#25928;&#22320;&#27719;&#24635;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#65292;&#20197;&#25913;&#36827;&#26080;&#20154;&#26426;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17987</link><description>&lt;p&gt;
&#22810;&#24577;&#38647;&#36798;&#23545;&#31354;&#20013;&#39134;&#34892;&#22120;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A Bayesian Fusion Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17987
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#36125;&#21494;&#26031;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#26469;&#26377;&#25928;&#22320;&#27719;&#24635;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#65292;&#20197;&#25913;&#36827;&#26080;&#20154;&#26426;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17987v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#26080;&#20154;&#26426;&#30340;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#65288;RATR&#65289;&#28041;&#21450;&#21457;&#23556;&#30005;&#30913;&#27874;&#24182;&#23545;&#25509;&#25910;&#21040;&#30340;&#38647;&#36798;&#22238;&#27874;&#25191;&#34892;&#30446;&#26631;&#31867;&#22411;&#35782;&#21035;&#65292;&#23545;&#22269;&#38450;&#21644;&#33322;&#31354;&#33322;&#22825;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#22810;&#24577;&#38647;&#36798;&#37197;&#32622;&#22312;RATR&#20013;&#20248;&#20110;&#21333;&#24577;&#38647;&#36798;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22810;&#24577;&#38647;&#36798;&#37197;&#32622;&#20013;&#30340;&#34701;&#21512;&#26041;&#27861;&#36890;&#24120;&#20197;&#27010;&#29575;&#26041;&#24335;&#27425;&#20248;&#22320;&#32452;&#21512;&#26469;&#33258;&#21508;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#21521;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#36125;&#21494;&#26031;RATR&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#65288;OBF&#65289;&#26469;&#32858;&#21512;&#26469;&#33258;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#12290;OBF&#22522;&#20110;&#26399;&#26395;0-1&#25439;&#22833;&#65292;&#26681;&#25454;&#22810;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#21382;&#21490;&#35266;&#27979;&#26356;&#26032;&#30446;&#26631;&#26080;&#20154;&#26426;&#31867;&#22411;&#30340;&#36882;&#24402;&#36125;&#21494;&#26031;&#20998;&#31867;&#65288;RBC&#65289;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#30340;&#38543;&#26426;&#34892;&#36208;&#36712;&#36857;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#20849;&#28041;&#21450;&#19971;&#31181;&#26426;&#21160;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17987v1 Announce Type: cross  Abstract: Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs) involves transmitting Electromagnetic Waves (EMWs) and performing target type recognition on the received radar echo, crucial for defense and aerospace applications. Previous studies highlighted the advantages of multistatic radar configurations over monostatic ones in RATR. However, fusion methods in multistatic radar configurations often suboptimally combine classification vectors from individual radars probabilistically. To address this, we propose a fully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to aggregate classification probability vectors from multiple radars. OBF, based on expected 0-1 loss, updates a Recursive Bayesian Classification (RBC) posterior distribution for target UAV type, conditioned on historical observations across multiple time steps. We evaluate the approach using simulated random walk trajectories for seven dro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25429;&#25417;&#28151;&#21512;&#26102;&#38388;&#21160;&#24577;&#30340;&#26032;&#39062;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#20351;&#29992;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#22312;&#23376;&#38388;&#38548;&#20869;&#23398;&#20064;&#20107;&#20214;&#38388;&#30340;&#20381;&#36182;&#22270;&#65292;&#25552;&#39640;&#20102;&#22312;&#39044;&#27979;&#20107;&#20214;&#38388;&#38548;&#26102;&#38388;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.16083</link><description>&lt;p&gt;
&#29992;&#20110;&#21160;&#24577;&#28508;&#22312;&#22270;&#30340;&#31070;&#32463;&#26102;&#24207;&#28857;&#36807;&#31243;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Variational Autoencoder for Neural Temporal Point Processes with Dynamic Latent Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16083
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25429;&#25417;&#28151;&#21512;&#26102;&#38388;&#21160;&#24577;&#30340;&#26032;&#39062;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#20351;&#29992;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#22312;&#23376;&#38388;&#38548;&#20869;&#23398;&#20064;&#20107;&#20214;&#38388;&#30340;&#20381;&#36182;&#22270;&#65292;&#25552;&#39640;&#20102;&#22312;&#39044;&#27979;&#20107;&#20214;&#38388;&#38548;&#26102;&#38388;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#35266;&#23519;&#21040;&#30340;&#20107;&#20214;&#21457;&#29983;&#24448;&#24448;&#34920;&#29616;&#20986;&#33258;&#28608;&#21644;&#20114;&#28608;&#25928;&#24212;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#29992;&#26102;&#24207;&#28857;&#36807;&#31243;&#27169;&#22411;&#21270;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36825;&#20123;&#20107;&#20214;&#21160;&#24577;&#20063;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#20855;&#26377;&#26576;&#31181;&#21608;&#26399;&#24615;&#36235;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#36825;&#31181;&#28151;&#21512;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36755;&#20837;&#24207;&#21015;&#30340;&#25972;&#20010;&#26102;&#38388;&#38388;&#38548;&#34987;&#21010;&#20998;&#20026;&#19968;&#32452;&#23376;&#38388;&#38548;&#12290;&#20551;&#35774;&#27599;&#20010;&#23376;&#38388;&#38548;&#20869;&#30340;&#20107;&#20214;&#21160;&#24577;&#26159;&#31283;&#23450;&#30340;&#65292;&#20294;&#22312;&#36825;&#20123;&#23376;&#38388;&#38548;&#20043;&#38388;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#27599;&#20010;&#23376;&#38388;&#38548;&#20013;&#35266;&#23519;&#32500;&#24230;&#20043;&#38388;&#30340;&#20381;&#36182;&#22270;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#20381;&#36182;&#22270;&#26469;&#28040;&#38500;&#36807;&#21435;&#20107;&#20214;&#30340;&#38750;&#36129;&#29486;&#24433;&#21709;&#65292;&#39044;&#27979;&#26410;&#26469;&#30340;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#20107;&#20214;&#38388;&#38548;&#26102;&#38388;&#19978;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16083v2 Announce Type: replace  Abstract: Continuously-observed event occurrences, often exhibit self- and mutually-exciting effects, which can be well modeled using temporal point processes. Beyond that, these event dynamics may also change over time, with certain periodic trends. We propose a novel variational auto-encoder to capture such a mixture of temporal dynamics. More specifically, the whole time interval of the input sequence is partitioned into a set of sub-intervals. The event dynamics are assumed to be stationary within each sub-interval, but could be changing across those sub-intervals. In particular, we use a sequential latent variable model to learn a dependency graph between the observed dimensions, for each sub-interval. The model predicts the future event times, by using the learned dependency graph to remove the noncontributing influences of past events. By doing so, the proposed model demonstrates its higher accuracy in predicting inter-event times and e
&lt;/p&gt;</description></item><item><title>&#31383;&#21475;&#24335;&#23433;&#24503;&#26862;&#21152;&#36895;&#22312;&#23545;&#31216;&#19981;&#21160;&#28857;&#36845;&#20195;&#20013;&#20855;&#26377;&#25913;&#36827;&#30340;&#26681;&#32447;&#24615;&#25910;&#25947;&#29575;&#65292;&#27169;&#25311;&#21644;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20854;&#36229;&#36234;&#26631;&#20934;&#19981;&#21160;&#28857;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.02490</link><description>&lt;p&gt;
&#23545;&#31216;&#19981;&#21160;&#28857;&#36845;&#20195;&#30340;&#31383;&#21475;&#24335;&#23433;&#24503;&#26862;&#21152;&#36895;&#25910;&#25947;&#29575;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Convergence Rates of Windowed Anderson Acceleration for Symmetric Fixed-Point Iterations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02490
&lt;/p&gt;
&lt;p&gt;
&#31383;&#21475;&#24335;&#23433;&#24503;&#26862;&#21152;&#36895;&#22312;&#23545;&#31216;&#19981;&#21160;&#28857;&#36845;&#20195;&#20013;&#20855;&#26377;&#25913;&#36827;&#30340;&#26681;&#32447;&#24615;&#25910;&#25947;&#29575;&#65292;&#27169;&#25311;&#21644;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20854;&#36229;&#36234;&#26631;&#20934;&#19981;&#21160;&#28857;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24120;&#29992;&#30340;&#31383;&#21475;&#24335;&#23433;&#24503;&#26862;&#21152;&#36895;&#65288;AA&#65289;&#31639;&#27861;&#29992;&#20110;&#19981;&#21160;&#28857;&#26041;&#27861;&#65292;$x^{(k+1)}=q(x^{(k)})$&#12290;&#23427;&#39318;&#27425;&#35777;&#26126;&#20102;&#24403;&#31639;&#23376;$q$&#26159;&#32447;&#24615;&#19988;&#23545;&#31216;&#26102;&#65292;&#20351;&#29992;&#20808;&#21069;&#36845;&#20195;&#30340;&#28369;&#21160;&#31383;&#21475;&#30340;&#31383;&#21475;&#24335;AA&#31639;&#27861;&#33021;&#22815;&#25913;&#36827;&#26681;&#32447;&#24615;&#25910;&#25947;&#22240;&#23376;&#65292;&#36229;&#36807;&#19981;&#21160;&#28857;&#36845;&#20195;&#12290;&#24403;$q$&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#20294;&#22312;&#22266;&#23450;&#28857;&#22788;&#20855;&#26377;&#23545;&#31216;&#38597;&#21487;&#27604;&#30697;&#38453;&#26102;&#65292;&#32463;&#36807;&#30053;&#24494;&#20462;&#25913;&#30340;AA&#31639;&#27861;&#34987;&#35777;&#26126;&#23545;&#27604;&#19981;&#21160;&#28857;&#36845;&#20195;&#20855;&#26377;&#31867;&#20284;&#30340;&#26681;&#32447;&#24615;&#25910;&#25947;&#22240;&#23376;&#25913;&#36827;&#12290;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35266;&#23519;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;Tyler&#30340;M&#20272;&#35745;&#20013;&#65292;AA&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#30340;&#19981;&#21160;&#28857;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02490v2 Announce Type: replace-cross  Abstract: This paper studies the commonly utilized windowed Anderson acceleration (AA) algorithm for fixed-point methods, $x^{(k+1)}=q(x^{(k)})$. It provides the first proof that when the operator $q$ is linear and symmetric the windowed AA, which uses a sliding window of prior iterates, improves the root-linear convergence factor over the fixed-point iterations. When $q$ is nonlinear, yet has a symmetric Jacobian at a fixed point, a slightly modified AA algorithm is proved to have an analogous root-linear convergence factor improvement over fixed-point iterations. Simulations verify our observations. Furthermore, experiments with different data models demonstrate AA is significantly superior to the standard fixed-point methods for Tyler's M-estimation.
&lt;/p&gt;</description></item><item><title>SRLDA&#26041;&#27861;&#22312;&#27874;&#32441;&#27169;&#22411;&#20551;&#35774;&#19979;&#20855;&#26377;&#32447;&#24615;&#20998;&#31867;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;RLDA&#21644;ILDA&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2210.03859</link><description>&lt;p&gt;
&#20855;&#26377;&#27874;&#32441;&#21327;&#26041;&#24046;&#27169;&#22411;&#30340;&#35889;&#26657;&#27491;&#21644;&#27491;&#21017;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Spectrally-Corrected and Regularized Linear Discriminant Analysis for Spiked Covariance Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.03859
&lt;/p&gt;
&lt;p&gt;
SRLDA&#26041;&#27861;&#22312;&#27874;&#32441;&#27169;&#22411;&#20551;&#35774;&#19979;&#20855;&#26377;&#32447;&#24615;&#20998;&#31867;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;RLDA&#21644;ILDA&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#26041;&#27861;&#65292;&#31216;&#20026;&#35889;&#26657;&#27491;&#21644;&#27491;&#21017;&#21270;LDA&#65288;SRLDA&#65289;&#12290;&#35813;&#26041;&#27861;&#25972;&#21512;&#20102;&#26679;&#26412;&#35889;&#26657;&#27491;&#21327;&#26041;&#24046;&#30697;&#38453;&#21644;&#27491;&#21017;&#21270;&#21028;&#21035;&#20998;&#26512;&#30340;&#35774;&#35745;&#24605;&#24819;&#12290;&#22312;&#22823;&#32500;&#38543;&#26426;&#30697;&#38453;&#20998;&#26512;&#26694;&#26550;&#30340;&#25903;&#25345;&#19979;&#65292;&#35777;&#26126;&#20102;SRLDA&#22312;&#27874;&#32441;&#27169;&#22411;&#20551;&#35774;&#19979;&#20855;&#26377;&#32447;&#24615;&#20998;&#31867;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#20223;&#30495;&#25968;&#25454;&#20998;&#26512;&#65292;&#35777;&#26126;SRLDA&#20998;&#31867;&#22120;&#20248;&#20110;RLDA&#21644;ILDA&#65292;&#24182;&#25509;&#36817;&#29702;&#35770;&#20998;&#31867;&#22120;&#12290;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SRLDA&#31639;&#27861;&#22312;&#20998;&#31867;&#21644;&#38477;&#32500;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#20351;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.03859v3 Announce Type: replace-cross  Abstract: This paper proposes an improved linear discriminant analysis called spectrally-corrected and regularized LDA (SRLDA). This method integrates the design ideas of the sample spectrally-corrected covariance matrix and the regularized discriminant analysis. With the support of a large-dimensional random matrix analysis framework, it is proved that SRLDA has a linear classification global optimal solution under the spiked model assumption. According to simulation data analysis, the SRLDA classifier performs better than RLDA and ILDA and is closer to the theoretical classifier. Experiments on different data sets show that the SRLDA algorithm performs better in classification and dimensionality reduction than currently used tools.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#65288;&#25110;&#8220;&#25554;&#20214;&#8221;&#65289;&#26041;&#27861;&#22312;&#26631;&#31614;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#23454;&#29616;&#20102;&#26080;&#28903;&#24405;&#25104;&#26412;&#30340;&#26497;&#23567;&#26497;&#20248;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2204.05275</link><description>&lt;p&gt;
&#35299;&#20915;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Settling the Sample Complexity of Model-Based Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.05275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#65288;&#25110;&#8220;&#25554;&#20214;&#8221;&#65289;&#26041;&#27861;&#22312;&#26631;&#31614;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#23454;&#29616;&#20102;&#26080;&#28903;&#24405;&#25104;&#26412;&#30340;&#26497;&#23567;&#26497;&#20248;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#23427;&#21033;&#29992;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26377;&#25928;&#30340;&#31163;&#32447;RL&#24212;&#33021;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#21644;&#26377;&#38480;&#30340;&#25968;&#25454;&#35206;&#30422;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#31639;&#27861;&#25110;&#20998;&#26512;&#35201;&#20040;&#21463;&#21040;&#27425;&#20248;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#22256;&#25200;&#65292;&#35201;&#20040;&#20135;&#29983;&#39640;&#26114;&#30340;&#28903;&#24405;&#25104;&#26412;&#20197;&#36798;&#21040;&#26679;&#26412;&#26368;&#20248;&#24615;&#65292;&#20174;&#32780;&#23545;&#26679;&#26412;&#21294;&#20047;&#24212;&#29992;&#20013;&#30340;&#39640;&#25928;&#31163;&#32447;RL&#26500;&#25104;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.05275v3 Announce Type: replace-cross  Abstract: This paper is concerned with offline reinforcement learning (RL), which learns using pre-collected data without further exploration. Effective offline RL would be able to accommodate distribution shift and limited data coverage. However, prior algorithms or analyses either suffer from suboptimal sample complexities or incur high burn-in cost to reach sample optimality, thus posing an impediment to efficient offline RL in sample-starved applications.   We demonstrate that the model-based (or "plug-in") approach achieves minimax-optimal sample complexity without burn-in cost for tabular Markov decision processes (MDPs). Concretely, consider a finite-horizon (resp. $\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$ (resp. effective horizon $\frac{1}{1-\gamma}$), and suppose the distribution shift of data is reflected by some single-policy clipped concentrability coefficient $C^{\star}_{\text{clipped}}$. We p
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#26368;&#20248;Q&#20989;&#25968;&#30340;&#38750;&#24179;&#31283;&#24615;&#24182;&#24320;&#21457;&#24207;&#36143;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2203.01707</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27979;&#35797;&#24179;&#31283;&#24615;&#21644;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Testing Stationarity and Change Point Detection in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.01707
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#26368;&#20248;Q&#20989;&#25968;&#30340;&#38750;&#24179;&#31283;&#24615;&#24182;&#24320;&#21457;&#24207;&#36143;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#21487;&#33021;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;&#35768;&#22810;&#25991;&#29486;&#20013;&#29616;&#26377;&#30340;RL&#31639;&#27861;&#20381;&#36182;&#20110;&#38656;&#35201;&#31995;&#32479;&#36716;&#25442;&#21644;&#22870;&#21169;&#20989;&#25968;&#38543;&#26102;&#38388;&#20445;&#25345;&#24658;&#23450;&#30340;&#24179;&#31283;&#24615;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#23454;&#36341;&#20013;&#24179;&#31283;&#24615;&#20551;&#35774;&#26159;&#26377;&#38480;&#21046;&#30340;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24456;&#21487;&#33021;&#34987;&#36829;&#21453;&#65292;&#21253;&#25324;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#31227;&#21160;&#20581;&#24247;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19968;&#33268;&#30340;&#31243;&#24207;&#65292;&#22522;&#20110;&#39044;&#20808;&#25910;&#38598;&#30340;&#21382;&#21490;&#25968;&#25454;&#27979;&#35797;&#26368;&#20248;Q&#20989;&#25968;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22312;&#32447;&#25968;&#25454;&#25910;&#38598;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#26816;&#39564;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#39034;&#24207;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;RL&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#29702;&#35770;&#32467;&#26524;&#12289;&#20223;&#30495;&#30740;&#31350;&#21644;&#23454;&#36341;&#20013;&#30340;&#26696;&#20363;&#24471;&#21040;&#20102;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.01707v3 Announce Type: replace-cross  Abstract: We consider offline reinforcement learning (RL) methods in possibly nonstationary environments. Many existing RL algorithms in the literature rely on the stationarity assumption that requires the system transition and the reward function to be constant over time. However, the stationarity assumption is restrictive in practice and is likely to be violated in a number of applications, including traffic signal control, robotics and mobile health. In this paper, we develop a consistent procedure to test the nonstationarity of the optimal Q-function based on pre-collected historical data, without additional online data collection. Based on the proposed test, we further develop a sequential change point detection method that can be naturally coupled with existing state-of-the-art RL methods for policy optimization in nonstationary environments. The usefulness of our method is illustrated by theoretical results, simulation studies, an
&lt;/p&gt;</description></item><item><title>&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#26041;&#27861;&#34429;&#28982;&#34987;&#29992;&#26469;&#33719;&#21462;&#21487;&#38752;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#20294;&#32570;&#20047;&#24418;&#24335;&#20445;&#35777;&#65292;&#20854;&#35777;&#20070;&#24182;&#19981;&#21512;&#29702;&#65292;&#22240;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#24433;&#21709;&#65307;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#32500;&#24230;&#35781;&#21650;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22312;&#20005;&#26684;&#30340;&#38480;&#21046;&#26465;&#20214;&#19979;&#20351;&#29992;&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#12290;</title><link>https://arxiv.org/abs/2110.05365</link><description>&lt;p&gt;
&#36755;&#20837;&#30456;&#20851;&#38543;&#26426;&#24179;&#28369;&#30340;&#26377;&#36259;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intriguing Properties of Input-dependent Randomized Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.05365
&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#26041;&#27861;&#34429;&#28982;&#34987;&#29992;&#26469;&#33719;&#21462;&#21487;&#38752;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#20294;&#32570;&#20047;&#24418;&#24335;&#20445;&#35777;&#65292;&#20854;&#35777;&#20070;&#24182;&#19981;&#21512;&#29702;&#65292;&#22240;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#24433;&#21709;&#65307;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#32500;&#24230;&#35781;&#21650;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22312;&#20005;&#26684;&#30340;&#38480;&#21046;&#26465;&#20214;&#19979;&#20351;&#29992;&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#30446;&#21069;&#34987;&#35748;&#20026;&#26159;&#33719;&#24471;&#21487;&#38752;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#24615;&#33021;&#26174;&#33879;&#65292;&#20294;&#35813;&#26041;&#27861;&#23384;&#22312;&#35832;&#22914;&#8220;&#35748;&#35777;&#20934;&#30830;&#24615;&#28689;&#24067;&#8221;&#12289;&#35748;&#35777;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#29978;&#33267;&#20844;&#24179;&#24615;&#38382;&#39064;&#31561;&#20005;&#37325;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#38519;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#36755;&#20837;&#30456;&#20851;&#30340;&#24179;&#28369;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#24418;&#24335;&#20445;&#35777;&#65292;&#22240;&#27492;&#24471;&#21040;&#30340;&#35777;&#20070;&#24182;&#19981;&#21512;&#29702;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#26041;&#24046;&#20989;&#25968;&#20855;&#26377;&#36739;&#20302;&#30340;&#21322;&#24377;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#21363;&#20351;&#22312;&#32500;&#24230;&#35781;&#21650;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22312;&#20005;&#26684;&#30340;&#38480;&#21046;&#26465;&#20214;&#19979;&#20351;&#29992;&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#24179;&#28369;&#26041;&#24046;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.05365v3 Announce Type: replace-cross  Abstract: Randomized smoothing is currently considered the state-of-the-art method to obtain certifiably robust classifiers. Despite its remarkable performance, the method is associated with various serious problems such as "certified accuracy waterfalls", certification vs.\ accuracy trade-off, or even fairness issues. Input-dependent smoothing approaches have been proposed with intention of overcoming these flaws. However, we demonstrate that these methods lack formal guarantees and so the resulting certificates are not justified. We show that in general, the input-dependent smoothing suffers from the curse of dimensionality, forcing the variance function to have low semi-elasticity. On the other hand, we provide a theoretical and practical framework that enables the usage of input-dependent smoothing even in the presence of the curse of dimensionality, under strict restrictions. We present one concrete design of the smoothing variance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#32467;&#26500;&#21270;&#31232;&#30095;&#20272;&#35745;&#22120;&#65292;&#32467;&#21512;&#32452;&#23376;&#38598;&#36873;&#25321;&#19982;&#25910;&#32553;&#65292;&#36866;&#29992;&#20110;&#31232;&#30095;&#21322;&#21442;&#25968;&#21152;&#24615;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#20248;&#21270;&#26694;&#26550;&#21644;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2105.12081</link><description>&lt;p&gt;
&#32452;&#36873;&#25321;&#21644;&#25910;&#32553;&#65306;&#21322;&#21442;&#25968;&#21152;&#24615;&#27169;&#22411;&#30340;&#32467;&#26500;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Group selection and shrinkage: Structured sparsity for semiparametric additive models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2105.12081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#32467;&#26500;&#21270;&#31232;&#30095;&#20272;&#35745;&#22120;&#65292;&#32467;&#21512;&#32452;&#23376;&#38598;&#36873;&#25321;&#19982;&#25910;&#32553;&#65292;&#36866;&#29992;&#20110;&#31232;&#30095;&#21322;&#21442;&#25968;&#21152;&#24615;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#20248;&#21270;&#26694;&#26550;&#21644;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#22238;&#24402;&#21644;&#20998;&#31867;&#20272;&#35745;&#22120;&#23562;&#37325;&#32452;&#32467;&#26500;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#21040;&#31232;&#30095;&#21152;&#24615;&#24314;&#27169;&#20877;&#21040;&#20998;&#23618;&#36873;&#25321;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#32467;&#26500;&#21270;&#31232;&#30095;&#20272;&#35745;&#22120;&#65292;&#23558;&#32452;&#23376;&#38598;&#36873;&#25321;&#19982;&#25910;&#32553;&#32467;&#21512;&#36215;&#26469;&#12290;&#20026;&#20102;&#36866;&#24212;&#22797;&#26434;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#20801;&#35768;&#32452;&#20043;&#38388;&#23384;&#22312;&#20219;&#24847;&#37325;&#21472;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#26469;&#25311;&#21512;&#38750;&#20984;&#27491;&#21017;&#21270;&#26354;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#22238;&#24402;&#20989;&#25968;&#20272;&#35745;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#38480;&#12290;&#20316;&#20026;&#38656;&#35201;&#32467;&#26500;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31232;&#30095;&#21322;&#21442;&#25968;&#21152;&#24615;&#24314;&#27169;&#65292;&#36825;&#26159;&#19968;&#31181;&#20801;&#35768;&#27599;&#20010;&#39044;&#27979;&#21464;&#37327;&#25928;&#24212;&#20026;&#38646;&#12289;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#30340;&#36807;&#31243;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#19982;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;&#26032;&#30340;&#20272;&#35745;&#22120;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#22312;&#20960;&#20010;&#24230;&#37327;&#19978;&#26377;&#25152;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#27169;&#22411;&#24314;&#31435;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2105.12081v3 Announce Type: replace-cross  Abstract: Sparse regression and classification estimators that respect group structures have application to an assortment of statistical and machine learning problems, from multitask learning to sparse additive modeling to hierarchical selection. This work introduces structured sparse estimators that combine group subset selection with shrinkage. To accommodate sophisticated structures, our estimators allow for arbitrary overlap between groups. We develop an optimization framework for fitting the nonconvex regularization surface and present finite-sample error bounds for estimation of the regression function. As an application requiring structure, we study sparse semiparametric additive modeling, a procedure that allows the effect of each predictor to be zero, linear, or nonlinear. For this task, the new estimators improve across several metrics on synthetic data compared to alternatives. Finally, we demonstrate their efficacy in modelin
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#31163;&#25955;&#25968;&#25454;&#25512;&#26029;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#30340;&#38382;&#39064;&#24314;&#27169;&#20026;&#24102;&#26377;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#32858;&#31867;&#20013;&#34701;&#21512;&#20102;&#30828;&#32858;&#31867;&#21644;&#36719;&#32858;&#31867;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#24403;&#21442;&#25968;$\lambda&gt;1$&#26102;&#21487;&#20197;&#25552;&#39640;&#25512;&#26029;&#24615;&#33021;&#65292;&#24403;$\lambda\to 0$&#26102;&#36866;&#29992;&#20110;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/1711.04366</link><description>&lt;p&gt;
&#19968;&#20010;&#24102;&#26377;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#30828;&#32858;&#31867;&#21644;&#36719;&#32858;&#31867;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A unified framework for hard and soft clustering with regularized optimal transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1711.04366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#31163;&#25955;&#25968;&#25454;&#25512;&#26029;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#30340;&#38382;&#39064;&#24314;&#27169;&#20026;&#24102;&#26377;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#32858;&#31867;&#20013;&#34701;&#21512;&#20102;&#30828;&#32858;&#31867;&#21644;&#36719;&#32858;&#31867;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#24403;&#21442;&#25968;$\lambda&gt;1$&#26102;&#21487;&#20197;&#25552;&#39640;&#25512;&#26029;&#24615;&#33021;&#65292;&#24403;$\lambda\to 0$&#26102;&#36866;&#29992;&#20110;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20174;&#31163;&#25955;&#25968;&#25454;&#25512;&#26029;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#30340;&#38382;&#39064;&#38416;&#36848;&#20026;&#19968;&#20010;&#24102;&#26377;&#21442;&#25968;$\lambda\geq 0$&#30340;&#29109;&#27491;&#21017;&#21270;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32479;&#19968;&#20102;&#30828;&#32858;&#31867;&#21644;&#36719;&#32858;&#31867;&#65292;&#24403;$\lambda=1$&#26102;&#65292;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#34987;&#23436;&#20840;&#24674;&#22797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#32858;&#31867;&#31639;&#27861;&#26063;&#20381;&#36182;&#20110;&#20351;&#29992;&#20132;&#26367;&#26368;&#23567;&#21270;&#26469;&#35299;&#20915;&#38750;&#20984;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#24191;&#20041;$\lambda$-EM&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25512;&#26029;&#25351;&#25968;&#26063;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#26102;&#65292;&#26368;&#23567;&#21270;&#36807;&#31243;&#20013;&#30340;&#27599;&#19968;&#27493;&#37117;&#26377;&#19968;&#20010;&#23553;&#38381;&#24418;&#24335;&#30340;&#35299;&#12290;&#23454;&#39564;&#31361;&#20986;&#20102;&#37319;&#29992;&#21442;&#25968;$\lambda&gt;1$&#26469;&#25552;&#39640;&#25512;&#26029;&#24615;&#33021;&#20197;&#21450;$\lambda\to 0$&#29992;&#20110;&#20998;&#31867;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1711.04366v2 Announce Type: replace  Abstract: In this paper, we formulate the problem of inferring a Finite Mixture Model from discrete data as an optimal transport problem with entropic regularization of parameter $\lambda\geq 0$. Our method unifies hard and soft clustering, the Expectation-Maximization (EM) algorithm being exactly recovered for $\lambda=1$. The family of clustering algorithm we propose rely on the resolution of nonconvex problems using alternating minimization. We study the convergence property of our generalized $\lambda-$EM algorithms and show that each step in the minimization process has a closed form solution when inferring finite mixture models of exponential families. Experiments highlight the benefits of taking a parameter $\lambda&gt;1$ to improve the inference performance and $\lambda\to 0$ for classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#36827;&#34892;&#20102;&#24615;&#33021;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#20851;&#38190;&#22240;&#32032;&#22914;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.12924</link><description>&lt;p&gt;
&#23545;&#20110;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#30340;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Performance Analysis of Support Vector Machine (SVM) on Challenging Datasets for Forest Fire Detection. (arXiv:2401.12924v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#36827;&#34892;&#20102;&#24615;&#33021;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#20851;&#38190;&#22240;&#32032;&#22914;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#30340;&#24615;&#33021;&#21644;&#21033;&#29992;&#24773;&#20917;&#12290;&#38543;&#30528;&#26862;&#26519;&#28779;&#28798;&#23545;&#29983;&#24577;&#31995;&#32479;&#21644;&#20154;&#31867;&#23450;&#23621;&#28857;&#30340;&#23041;&#32961;&#26085;&#30410;&#22686;&#21152;&#65292;&#36805;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#31995;&#32479;&#30340;&#38656;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;SVM&#20197;&#20854;&#24378;&#22823;&#30340;&#20998;&#31867;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#22312;&#22270;&#20687;&#20013;&#35782;&#21035;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#27169;&#24335;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#24230;&#12290;&#36890;&#36807;&#22312;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;SVM&#33719;&#24471;&#20102;&#35782;&#21035;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#29420;&#29305;&#23646;&#24615;&#30340;&#33021;&#21147;&#65292;&#22914;&#28779;&#28976;&#12289;&#28895;&#38654;&#25110;&#26862;&#26519;&#21306;&#22495;&#35270;&#35273;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20351;&#29992;SVM&#30340;&#21508;&#20010;&#35201;&#32032;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;&#20005;&#26684;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#23454;&#38469;&#36866;&#29992;&#24615;&#31561;&#21442;&#25968;&#12290;&#20174;&#36825;&#39033;&#30740;&#31350;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#26377;&#21161;&#20110;&#24320;&#21457;&#39640;&#25928;&#30340;&#26862;&#26519;&#28779;&#28798;&#26816;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article delves into the analysis of performance and utilization of Support Vector Machines (SVMs) for the critical task of forest fire detection using image datasets. With the increasing threat of forest fires to ecosystems and human settlements, the need for rapid and accurate detection systems is of utmost importance. SVMs, renowned for their strong classification capabilities, exhibit proficiency in recognizing patterns associated with fire within images. By training on labeled data, SVMs acquire the ability to identify distinctive attributes associated with fire, such as flames, smoke, or alterations in the visual characteristics of the forest area. The document thoroughly examines the use of SVMs, covering crucial elements like data preprocessing, feature extraction, and model training. It rigorously evaluates parameters such as accuracy, efficiency, and practical applicability. The knowledge gained from this study aids in the development of efficient forest fire detection sy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36229;&#21442;&#25968;&#35843;&#20248;&#21644;&#30828;&#20214;&#22788;&#29702;&#22120;&#30340;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#21462;&#24471;&#20102;&#30495;&#23454;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06782</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#21315;&#20806;&#32423;&#25968;&#25454;&#38598;&#29992;&#20110;&#31890;&#23376;&#27969;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Scalable neural network models and terascale datasets for particle-flow reconstruction. (arXiv:2309.06782v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36229;&#21442;&#25968;&#35843;&#20248;&#21644;&#30828;&#20214;&#22788;&#29702;&#22120;&#30340;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#21462;&#24471;&#20102;&#30495;&#23454;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#22522;&#20110;&#39640;&#24230;&#31890;&#24230;&#25506;&#27979;&#22120;&#27169;&#25311;&#30340;&#23436;&#25972;&#20107;&#20214;&#37325;&#24314;&#65292;&#30740;&#31350;&#20102;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#31890;&#23376;&#27969;&#65288;PF&#65289;&#37325;&#24314;&#21487;&#36890;&#36807;&#36319;&#36394;&#21644;&#37327;&#33021;&#22120;&#22242;&#31751;&#25110;&#20987;&#20013;&#26469;&#26500;&#24314;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#20869;&#26680;&#30340;&#21464;&#25442;&#22120;&#65292;&#24182;&#35777;&#26126;&#20004;&#32773;&#37117;&#36991;&#20813;&#20102;&#20108;&#27425;&#20869;&#23384;&#20998;&#37197;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#30495;&#23454;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25152;&#24471;&#27169;&#22411;&#22312;&#30828;&#20214;&#22788;&#29702;&#22120;&#19978;&#20855;&#26377;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#25903;&#25345;NVIDIA, AMD&#21644;&#33521;&#29305;&#23572; Habana&#21345;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#21487;&#20197;&#22312;&#30001;&#36319;&#36394;&#21644;&#37327;&#33021;&#22120;&#20987;&#20013;&#32452;&#25104;&#30340;&#39640;&#31890;&#24230;&#36755;&#20837;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33719;&#24471;&#19982;&#22522;&#20934;&#30456;&#31454;&#20105;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;&#26377;&#20851;&#22797;&#29616;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24050;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study scalable machine learning models for full event reconstruction in high-energy electron-positron collisions based on a highly granular detector simulation. Particle-flow (PF) reconstruction can be formulated as a supervised learning task using tracks and calorimeter clusters or hits. We compare a graph neural network and kernel-based transformer and demonstrate that both avoid quadratic memory allocation and computational cost while achieving realistic PF reconstruction. We show that hyperparameter tuning on a supercomputer significantly improves the physics performance of the models. We also demonstrate that the resulting model is highly portable across hardware processors, supporting Nvidia, AMD, and Intel Habana cards. Finally, we demonstrate that the model can be trained on highly granular inputs consisting of tracks and calorimeter hits, resulting in a competitive physics performance with the baseline. Datasets and software to reproduce the studies are published following 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#29992;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#26465;&#20214;&#19979;&#65292;&#21033;&#29992;&#21322;&#21442;&#25968;&#25928;&#29575;&#29702;&#35770;&#65292;&#39640;&#25928;&#20272;&#35745;&#30446;&#26631;&#24635;&#20307;&#39118;&#38505;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16406</link><description>&lt;p&gt;
&#36890;&#29992;&#24418;&#24335;&#19979;&#30340;&#39640;&#25928;&#19988;&#22810;&#37325;&#31283;&#20581;&#30340;&#39118;&#38505;&#20272;&#35745;&#26041;&#27861;&#22312;&#25968;&#25454;&#36716;&#31227;&#20013;
&lt;/p&gt;
&lt;p&gt;
Efficient and Multiply Robust Risk Estimation under General Forms of Dataset Shift. (arXiv:2306.16406v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#29992;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#26465;&#20214;&#19979;&#65292;&#21033;&#29992;&#21322;&#21442;&#25968;&#25928;&#29575;&#29702;&#35770;&#65292;&#39640;&#25928;&#20272;&#35745;&#30446;&#26631;&#24635;&#20307;&#39118;&#38505;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32463;&#24120;&#38754;&#20020;&#26469;&#33258;&#24863;&#20852;&#36259;&#24635;&#20307;&#30340;&#26377;&#38480;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#26159;&#21033;&#29992;&#26469;&#33258;&#36741;&#21161;&#28304;&#24635;&#20307;&#30340;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#19982;&#30446;&#26631;&#39046;&#22495;&#30340;&#26576;&#20123;&#26465;&#20214;&#20998;&#24067;&#30456;&#21516;&#25110;&#20197;&#20854;&#20182;&#26041;&#24335;&#30456;&#36830;&#12290;&#21033;&#29992;&#36825;&#31181;"&#25968;&#25454;&#36716;&#31227;"&#26465;&#20214;&#30340;&#25216;&#26415;&#34987;&#31216;&#20026;"&#39046;&#22495;&#36866;&#24212;"&#25110;"&#36801;&#31227;&#23398;&#20064;"&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#25968;&#25454;&#36716;&#31227;&#30340;&#25991;&#29486;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#36741;&#21161;&#24635;&#20307;&#26469;&#25552;&#39640;&#30446;&#26631;&#24635;&#20307;&#19978;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#39118;&#38505;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21322;&#21442;&#25968;&#25928;&#29575;&#29702;&#35770;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#26465;&#20214;&#19979;&#39640;&#25928;&#20272;&#35745;&#30446;&#26631;&#24635;&#20307;&#39118;&#38505;&#30340;&#19968;&#33324;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#36890;&#29992;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#26465;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#31181;&#27969;&#34892;&#26465;&#20214;&#8212;&#8212;&#21327;&#21464;&#37327;&#12289;&#26631;&#31614;&#21644;&#27010;&#24565;&#36716;&#31227;&#8212;&#8212;&#20316;&#20026;&#29305;&#20363;&#12290;&#25105;&#20204;&#20801;&#35768;&#37096;&#20998;&#38750;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical machine learning methods often face the challenge of limited data available from the population of interest. One remedy is to leverage data from auxiliary source populations, which share some conditional distributions or are linked in other ways with the target domain. Techniques leveraging such \emph{dataset shift} conditions are known as \emph{domain adaptation} or \emph{transfer learning}. Despite extensive literature on dataset shift, limited works address how to efficiently use the auxiliary populations to improve the accuracy of risk evaluation for a given machine learning task in the target population.  In this paper, we study the general problem of efficiently estimating target population risk under various dataset shift conditions, leveraging semiparametric efficiency theory. We consider a general class of dataset shift conditions, which includes three popular conditions -- covariate, label and concept shift -- as special cases. We allow for partially non-overlappi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#21387;&#32553;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#23450;&#27604;&#29305;&#29575;&#19979;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.18231</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
High-Fidelity Image Compression with Score-based Generative Models. (arXiv:2305.18231v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#21387;&#32553;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#23450;&#27604;&#29305;&#29575;&#19979;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#22270;&#20687;&#21387;&#32553;&#39046;&#22495;&#22797;&#21046;&#36825;&#20010;&#25104;&#21151;&#21364;&#24456;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#32473;&#23450;&#27604;&#29305;&#29575;&#19979;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#36890;&#36807; FID &#20998;&#25968;&#35780;&#20272;&#65292;&#34920;&#29616;&#36229;&#36234;&#20102; PO-ELIC &#21644; HiFiC &#30340;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#20294;&#22312;&#29702;&#35770;&#19978;&#26377;&#21160;&#26426;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#20197; MSE &#20026;&#30446;&#26631;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#36827;&#19968;&#27493;&#22522;&#20110;&#20998;&#25968;&#30340;&#35299;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#23558;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#23454;&#29616;&#32454;&#33410;&#24456;&#37325;&#35201;&#65292;&#26368;&#20339;&#35774;&#35745;&#20915;&#31574;&#21487;&#33021;&#19982;&#20856;&#22411;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#26377;&#24456;&#22823;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the tremendous success of diffusion generative models in text-to-image generation, replicating this success in the domain of image compression has proven difficult. In this paper, we demonstrate that diffusion can significantly improve perceptual quality at a given bit-rate, outperforming state-of-the-art approaches PO-ELIC and HiFiC as measured by FID score. This is achieved using a simple but theoretically motivated two-stage approach combining an autoencoder targeting MSE followed by a further score-based decoder. However, as we will show, implementation details matter and the optimal design decisions can differ greatly from typical text-to-image models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#39034;&#24207;&#26816;&#39564;&#21644;&#32622;&#20449;&#21306;&#38388;&#65292;&#22312;&#19968;&#33324;&#38750;&#21442;&#25968;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19979;&#25552;&#20379;&#20102;&#31867;&#22411;I&#38169;&#35823;&#21644;&#26399;&#26395;&#25298;&#32477;&#26102;&#38388;&#20445;&#35777;&#65292;&#25552;&#39640;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.14411</link><description>&lt;p&gt;
&#36817;&#20284;&#26368;&#20248;&#30340;&#38750;&#21442;&#25968;&#39034;&#24207;&#26816;&#39564;&#21644;&#20855;&#26377;&#21487;&#33021;&#30456;&#20851;&#35266;&#27979;&#30340;&#32622;&#20449;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Non-Parametric Sequential Tests and Confidence Sequences with Possibly Dependent Observations. (arXiv:2212.14411v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#39034;&#24207;&#26816;&#39564;&#21644;&#32622;&#20449;&#21306;&#38388;&#65292;&#22312;&#19968;&#33324;&#38750;&#21442;&#25968;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19979;&#25552;&#20379;&#20102;&#31867;&#22411;I&#38169;&#35823;&#21644;&#26399;&#26395;&#25298;&#32477;&#26102;&#38388;&#20445;&#35777;&#65292;&#25552;&#39640;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#26816;&#39564;&#21644;&#20854;&#38544;&#21547;&#30340;&#32622;&#20449;&#21306;&#38388;&#22312;&#20219;&#24847;&#20572;&#27490;&#26102;&#38388;&#19979;&#37117;&#33021;&#25552;&#20379;&#28789;&#27963;&#30340;&#32479;&#35745;&#25512;&#26029;&#21644;&#21363;&#26102;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24378;&#26377;&#21147;&#30340;&#20445;&#35777;&#20165;&#36866;&#29992;&#20110;&#22312;&#23454;&#36341;&#20013;&#20302;&#20272;&#25110;&#27987;&#24230;&#30028;&#38480;&#20026;&#22522;&#30784;&#30340;&#39034;&#24207;&#24207;&#21015;&#65292;&#32780;&#36825;&#20123;&#24207;&#21015;&#20855;&#26377;&#27425;&#20248;&#30340;&#25298;&#32477;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#32599;&#23486;&#26031;&#65288;Robbins&#65289;1970&#24180;&#30340;&#24310;&#36831;&#21551;&#21160;&#27491;&#24577;&#28151;&#21512;&#39034;&#24207;&#27010;&#29575;&#27604;&#26816;&#39564;&#65292;&#24182;&#22312;&#19968;&#33324;&#38750;&#21442;&#25968;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19979;&#25552;&#20379;&#20102;&#39318;&#20010;&#28176;&#36817;&#31867;&#22411;I&#38169;&#35823;&#21644;&#26399;&#26395;&#25298;&#32477;&#26102;&#38388;&#20445;&#35777;&#65292;&#20854;&#20013;&#28176;&#36817;&#24615;&#36136;&#30001;&#27979;&#35797;&#30340;&#28903;&#20837;&#26102;&#38388;&#30830;&#23450;&#12290;&#31867;&#22411;I&#38169;&#35823;&#30340;&#32467;&#26524;&#20027;&#35201;&#20381;&#36182;&#20110;&#38789;&#24378;&#19981;&#21464;&#21407;&#29702;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#26816;&#39564;&#65288;&#21450;&#20854;&#38544;&#21547;&#30340;&#32622;&#20449;&#21306;&#38388;&#65289;&#20855;&#26377;&#25509;&#36817;&#25152;&#38656;&#945;&#27700;&#24179;&#30340;&#31867;&#22411;I&#38169;&#35823;&#29575;&#12290;&#26399;&#26395;&#25298;&#32477;&#26102;&#38388;&#30340;&#32467;&#26524;&#20027;&#35201;&#21033;&#29992;&#20102;&#19968;&#31181;&#21463;&#20234;&#34276;&#24341;&#29702;&#21551;&#21457;&#30340;&#24658;&#31561;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential tests and their implied confidence sequences, which are valid at arbitrary stopping times, promise flexible statistical inference and on-the-fly decision making. However, strong guarantees are limited to parametric sequential tests that under-cover in practice or concentration-bound-based sequences that over-cover and have suboptimal rejection times. In this work, we consider \cite{robbins1970boundary}'s delayed-start normal-mixture sequential probability ratio tests, and we provide the first asymptotic type-I-error and expected-rejection-time guarantees under general non-parametric data generating processes, where the asymptotics are indexed by the test's burn-in time. The type-I-error results primarily leverage a martingale strong invariance principle and establish that these tests (and their implied confidence sequences) have type-I error rates approaching a desired $\alpha$-level. The expected-rejection-time results primarily leverage an identity inspired by It\^o's lemm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#24102;&#26469;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#65292;&#25552;&#39640;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.07290</link><description>&lt;p&gt;
&#21452;&#25511;&#21046;&#21464;&#37327;&#21152;&#36895;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Dual control variate for faster black-box variational inference. (arXiv:2210.07290v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#24102;&#26469;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#65292;&#25552;&#39640;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#25512;&#26029;&#26694;&#26550;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#26799;&#24230;&#20272;&#35745;&#20013;&#30340;&#39640;&#26041;&#24046;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#24046;&#26469;&#33258;&#20004;&#20010;&#38543;&#26426;&#28304;&#65306;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#12290;&#29616;&#26377;&#30340;&#25511;&#21046;&#21464;&#37327;&#20165;&#35299;&#20915;&#33945;&#29305;&#21345;&#32599;&#22122;&#22768;&#65292;&#32780;&#22686;&#37327;&#26799;&#24230;&#26041;&#27861;&#36890;&#24120;&#20165;&#35299;&#20915;&#25968;&#25454;&#23376;&#25277;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21452;&#8221;&#25511;&#21046;&#21464;&#37327;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#20004;&#31181;&#22122;&#22768;&#28304;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#30830;&#35748;&#36825;&#23548;&#33268;&#20102;&#20943;&#23569;&#26041;&#24046;&#21644;&#22312;&#22810;&#20010;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#25552;&#39640;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box variational inference is a widely-used framework for Bayesian posterior inference, but in some cases suffers from high variance in gradient estimates, harming accuracy and efficiency. This variance comes from two sources of randomness: Data subsampling and Monte Carlo sampling. Whereas existing control variates only address Monte Carlo noise and incremental gradient methods typically only address data subsampling, we propose a new "dual" control variate capable of jointly reducing variance from both sources of noise. We confirm that this leads to reduced variance and improved optimization in several real-world applications.
&lt;/p&gt;</description></item></channel></rss>