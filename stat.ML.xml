<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#24050;&#30693;&#29289;&#29702;&#23398;&#30693;&#35782;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18471</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#22240;&#26524;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Causal disentanglement of multimodal data. (arXiv:2310.18471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18471
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#24050;&#30693;&#29289;&#29702;&#23398;&#30693;&#35782;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#21457;&#29616;&#20102;&#25968;&#25454;&#30340;&#36739;&#20302;&#32500;&#24230;&#34920;&#31034;&#65292;&#21487;&#20197;&#23545;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#65307;&#30001;&#20110;&#23454;&#29616;&#36825;&#26679;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#35768;&#22810;&#22240;&#26524;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#20102;&#25351;&#31034;&#20808;&#39564;&#20449;&#24687;&#30340;&#20803;&#32032;&#65292;&#20363;&#22914;&#65288;&#32447;&#24615;&#65289;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#12289;&#24178;&#39044;&#25968;&#25454;&#25110;&#24369;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#22312;&#25506;&#32034;&#24615;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#36825;&#20123;&#20803;&#32032;&#21644;&#20808;&#39564;&#20449;&#24687;&#21487;&#33021;&#19981;&#21487;&#29992;&#25110;&#19981;&#21512;&#36866;&#12290;&#30456;&#21453;&#65292;&#31185;&#23398;&#25968;&#25454;&#38598;&#36890;&#24120;&#20855;&#26377;&#22810;&#20010;&#27169;&#24577;&#25110;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#32422;&#26463;&#65292;&#24182;&#19988;&#24050;&#32463;&#35777;&#26126;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#36825;&#31181;&#31185;&#23398;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#21487;&#20197;&#25913;&#21892;&#22240;&#26524;&#20998;&#35299;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65288;causalPIMA&#65289;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#24050;&#30693;&#30340;&#29289;&#29702;&#23398;&#30693;&#35782;&#21457;&#29616;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#31639;&#27861;&#21033;&#29992;&#26032;&#30340;&#21487;&#24494;&#21442;&#25968;&#21270;&#26469;&#23398;&#20064;&#36825;&#31181;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal representation learning algorithms discover lower-dimensional representations of data that admit a decipherable interpretation of cause and effect; as achieving such interpretable representations is challenging, many causal learning algorithms utilize elements indicating prior information, such as (linear) structural causal models, interventional data, or weak supervision. Unfortunately, in exploratory causal representation learning, such elements and prior information may not be available or warranted. Alternatively, scientific datasets often have multiple modalities or physics-based constraints, and the use of such scientific, multimodal data has been shown to improve disentanglement in fully unsupervised settings. Consequently, we introduce a causal representation learning algorithm (causalPIMA) that can use multimodal data and known physics to discover important features with causal relationships. Our innovative algorithm utilizes a new differentiable parametrization to lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21407;&#22987;&#26041;&#27861;&#26469;&#35299;&#20915;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26465;&#20214;&#37319;&#26679;&#20013;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#37319;&#26679;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09078</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#36817;&#20284;&#31062;&#20808;&#37319;&#26679;&#23454;&#29616;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26465;&#20214;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling. (arXiv:2308.09078v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21407;&#22987;&#26041;&#27861;&#26469;&#35299;&#20915;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26465;&#20214;&#37319;&#26679;&#20013;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#37319;&#26679;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#22914;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#65292;&#38656;&#35201;&#23545;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#36827;&#34892;&#26465;&#20214;&#37319;&#26679;&#65292;&#20294;&#36825;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#28176;&#36817;&#31934;&#30830;&#26465;&#20214;&#37319;&#26679;&#30340;&#21407;&#21017;&#36873;&#25321;&#26159;Metropolis-within-Gibbs&#65288;MWG&#65289;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;VAE&#20542;&#21521;&#20110;&#23398;&#20064;&#32467;&#26500;&#21270;&#28508;&#21464;&#37327;&#31354;&#38388;&#65292;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#26399;&#26395;&#29305;&#24615;&#65292;&#20294;&#21364;&#23548;&#33268;MWG&#37319;&#26679;&#22120;&#36828;&#31163;&#30446;&#26631;&#20998;&#24067;&#12290;&#26412;&#25991;&#20811;&#26381;&#20102;MWG&#30340;&#23616;&#38480;&#24615;&#65306;&#25105;&#20204;&#31995;&#32479;&#22320;&#27010;&#36848;&#20102;&#22312;VAE&#19978;&#19978;&#36848;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21407;&#22987;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#22312;&#19968;&#32452;&#37319;&#26679;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional sampling of variational autoencoders (VAEs) is needed in various applications, such as missing data imputation, but is computationally intractable. A principled choice for asymptotically exact conditional sampling is Metropolis-within-Gibbs (MWG). However, we observe that the tendency of VAEs to learn a structured latent space, a commonly desired property, can cause the MWG sampler to get "stuck" far from the target distribution. This paper mitigates the limitations of MWG: we systematically outline the pitfalls in the context of VAEs, propose two original methods that address these pitfalls, and demonstrate an improved performance of the proposed methods on a set of sampling tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;MSDA&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22495;&#34920;&#31034;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#26681;&#25454;&#35813;&#23383;&#20856;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#21644;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14953</link><description>&lt;p&gt;
&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#36827;&#34892;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space. (arXiv:2307.14953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;MSDA&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22495;&#34920;&#31034;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#26681;&#25454;&#35813;&#23383;&#20856;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#21644;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#26088;&#22312;&#22312;&#20174;&#22810;&#20010;&#26631;&#35760;&#30340;&#28304;&#22495;&#36716;&#31227;&#30693;&#35782;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#26102;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;MSDA&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;MSDA&#20013;&#30340;&#27599;&#20010;&#22495;&#35299;&#37322;&#20026;&#32463;&#39564;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#22495;&#34920;&#36798;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#65292;&#36825;&#20123;&#21407;&#23376;&#26159;&#32463;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#23567;&#25209;&#37327;&#23398;&#20064;&#30340;&#31639;&#27861;DaDiL&#65306;&#65288;i&#65289;&#21407;&#23376;&#20998;&#24067;&#65307;&#65288;ii&#65289;&#37325;&#24515;&#22352;&#26631;&#30697;&#38453;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23383;&#20856;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65306;DaDiL-R&#65292;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#65307;DaDiL-E&#65292;&#22522;&#20110;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;3&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;Caltech-Office&#12289;Office 31&#21644;CRWU&#65292;&#22312;&#20998;&#31867;&#19978;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;3.15&#65285;&#12289;2.29&#65285;&#21644;7.71&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA framework based on dictionary learning and optimal transport. We interpret each domain in MSDA as an empirical distribution. As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates. Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions. We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#31639;&#27861;&#25552;&#20986;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#21033;&#29992;&#19981;&#31283;&#23450;&#29305;&#24449;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09933</link><description>&lt;p&gt;
Spuriosity&#24182;&#27809;&#26377;&#23548;&#33268;&#20998;&#31867;&#22120;&#22833;&#36133;&#65306;&#21033;&#29992;&#19981;&#21464;&#30340;&#39044;&#27979;&#26469;&#21033;&#29992;&#34394;&#20551;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Spuriosity Didn't Kill the Classifier: Using Invariant Predictions to Harness Spurious Features. (arXiv:2307.09933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#31639;&#27861;&#25552;&#20986;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#21033;&#29992;&#19981;&#31283;&#23450;&#29305;&#24449;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36991;&#20813;&#22312;&#22495;&#22806;&#25968;&#25454;&#19978;&#30340;&#22833;&#36133;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#25552;&#21462;&#20855;&#26377;&#19982;&#26631;&#31614;&#22312;&#19981;&#21516;&#22495;&#20043;&#38388;&#31283;&#23450;&#25110;&#19981;&#21464;&#20851;&#31995;&#30340;&#29305;&#24449;&#65292;&#33293;&#24323;&#19982;&#26631;&#31614;&#22312;&#19981;&#21516;&#22495;&#20043;&#38388;&#20851;&#31995;&#21464;&#21270;&#30340;"&#34394;&#20551;"&#25110;&#19981;&#31283;&#23450;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#19981;&#31283;&#23450;&#29305;&#24449;&#24120;&#24120;&#25658;&#24102;&#20851;&#20110;&#26631;&#31614;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#22914;&#26524;&#22312;&#27979;&#35797;&#22495;&#20013;&#27491;&#30830;&#20351;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26174;&#31034;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#22914;&#20309;&#22312;&#27979;&#35797;&#22495;&#20013;&#20351;&#29992;&#36825;&#20123;&#19981;&#31283;&#23450;&#29305;&#24449;&#26159;&#21487;&#33021;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#22522;&#20110;&#31283;&#23450;&#29305;&#24449;&#30340;&#20266;&#26631;&#31614;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#25351;&#23548;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#21069;&#25552;&#26159;&#22312;&#32473;&#23450;&#26631;&#31614;&#30340;&#26465;&#20214;&#19979;&#65292;&#31283;&#23450;&#29305;&#24449;&#21644;&#19981;&#31283;&#23450;&#29305;&#24449;&#26159;&#26465;&#20214;&#29420;&#31435;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#29702;&#35770;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#29305;&#24449;&#22686;&#24378;&#65288;SFB&#65289;&#31639;&#27861;&#65306;(i)&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#20998;&#31163;&#31283;&#23450;&#29305;&#24449;&#21644;&#26465;&#20214;&#29420;&#31435;&#19981;&#31283;&#23450;&#29305;&#24449;&#30340;&#39044;&#27979;&#22120;&#65307;(ii)&#20351;&#29992;&#31283;&#23450;&#29305;&#24449;&#39044;&#27979;&#26469;&#36866;&#24212;&#27979;&#35797;&#22495;
&lt;/p&gt;
&lt;p&gt;
To avoid failures on out-of-distribution data, recent works have sought to extract features that have a stable or invariant relationship with the label across domains, discarding the "spurious" or unstable features whose relationship with the label changes across domains. However, unstable features often carry complementary information about the label that could boost performance if used correctly in the test domain. Our main contribution is to show that it is possible to learn how to use these unstable features in the test domain without labels. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#38750;&#27491;&#24577;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;&#26500;&#24314;&#21435;&#20559;&#20272;&#35745;&#37327;&#65292;&#24182;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32972;&#26223;&#19979;&#20445;&#25345;&#20102;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07320</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Adaptive Linear Estimating Equations. (arXiv:2307.07320v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#38750;&#27491;&#24577;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;&#26500;&#24314;&#21435;&#20559;&#20272;&#35745;&#37327;&#65292;&#24182;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32972;&#26223;&#19979;&#20445;&#25345;&#20102;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25968;&#25454;&#25910;&#38598;&#24050;&#25104;&#20026;&#22686;&#24378;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#25928;&#29575;&#30340;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#26426;&#21046;&#24120;&#24120;&#32473;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#24341;&#20837;&#22797;&#26434;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#65292;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#65288;OLS&#65289;&#20272;&#35745;&#37327;&#21487;&#33021;&#34920;&#29616;&#20986;&#38750;&#27491;&#24577;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#20174;&#32780;&#23545;&#20934;&#30830;&#30340;&#25512;&#26029;&#21644;&#35299;&#37322;&#25552;&#20986;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#21435;&#20559;&#20272;&#35745;&#37327;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;&#30340;&#24605;&#24819;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#23454;&#29616;&#36817;&#20284;&#26368;&#20248;&#28176;&#36817;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#37327;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#65292;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#37327;&#20445;&#30041;&#20102;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#24037;&#20316;&#35299;&#20915;&#20102;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#38750;&#27491;&#24577;&#28176;&#36817;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;&#32479;&#35745;&#25512;&#26029;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential data collection has emerged as a widely adopted technique for enhancing the efficiency of data gathering processes. Despite its advantages, such data collection mechanism often introduces complexities to the statistical inference procedure. For instance, the ordinary least squares (OLS) estimator in an adaptive linear regression model can exhibit non-normal asymptotic behavior, posing challenges for accurate inference and interpretation. In this paper, we propose a general method for constructing debiased estimator which remedies this issue. It makes use of the idea of adaptive linear estimating equations, and we establish theoretical guarantees of asymptotic normality, supplemented by discussions on achieving near-optimal asymptotic variance. A salient feature of our estimator is that in the context of multi-armed bandits, our estimator retains the non-asymptotic performance of the least square estimator while obtaining asymptotic normality property. Consequently, this work
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#20215;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#65292;&#36824;&#33021;&#22815;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#65292;&#21363;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;KGF&#21644;KRR&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;KRR&#27867;&#21270;&#65292;&#20351;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.16838</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Kernel Ridge Regression with Gradient-Based Optimization Methods. (arXiv:2306.16838v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#20215;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#65292;&#36824;&#33021;&#22815;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#65292;&#21363;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;KGF&#21644;KRR&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;KRR&#27867;&#21270;&#65292;&#20351;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#26159;&#32447;&#24615;&#23725;&#22238;&#24402;&#30340;&#38750;&#32447;&#24615;&#25512;&#24191;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KRR&#30446;&#26631;&#20989;&#25968;&#30340;&#31561;&#20215;&#24418;&#24335;&#65292;&#20026;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#21644;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#25171;&#24320;&#20102;&#21487;&#33021;&#12290;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#8212;&#8212;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#35753;&#25105;&#20204;&#33021;&#22815;&#22312;KGF&#21644;KRR&#20043;&#38388;&#29702;&#35770;&#19978;&#30028;&#23450;&#24046;&#24322;&#12290;&#25105;&#20204;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#23558;KRR&#27867;&#21270;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;KGF&#21644;KRR&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#29992;&#36825;&#20123;&#24809;&#32602;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#20351;&#29992;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#65288;&#20063;&#31216;&#20026;&#22352;&#26631;&#19979;&#38477;&#65289;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel ridge regression, KRR, is a non-linear generalization of linear ridge regression. Here, we introduce an equivalent formulation of the objective function of KRR, opening up both for using other penalties than the ridge penalty and for studying kernel ridge regression from the perspective of gradient descent. Using a continuous-time perspective, we derive a closed-form solution, kernel gradient flow, KGF, with regularization through early stopping, which allows us to theoretically bound the differences between KGF and KRR. We generalize KRR by replacing the ridge penalty with the $\ell_1$ and $\ell_\infty$ penalties and utilize the fact that analogously to the similarities between KGF and KRR, the solutions obtained when using these penalties are very similar to those obtained from forward stagewise regression (also known as coordinate descent) and sign gradient descent in combination with early stopping. Thus the need for computationally heavy proximal gradient descent algorithms
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#65292;&#22312;&#26377;&#30028;&#21644;&#19968;&#33324;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#20013;&#22343;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30028;&#38480;&#36824;&#33021;&#22815;&#20445;&#25345;&#38543;&#26102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12214</link><description>&lt;p&gt;
&#26356;&#22810;&#30340;PAC-Bayes Bounds&#65306;&#20174;&#26377;&#30028;&#25439;&#22833;&#21040;&#20855;&#26377;&#19968;&#33324;&#24615;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#65292;&#21040;&#20219;&#20309;&#26102;&#38388;&#22343;&#26377;&#25928;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime-validity. (arXiv:2306.12214v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#65292;&#22312;&#26377;&#30028;&#21644;&#19968;&#33324;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#20013;&#22343;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30028;&#38480;&#36824;&#33021;&#22815;&#20445;&#25345;&#38543;&#26102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25439;&#22833;&#25552;&#20986;&#20102;&#26032;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#38480;&#12290;&#39318;&#20808;&#65292;&#38024;&#23545;&#26377;&#30028;&#33539;&#22260;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Catoni&#30028;&#30340;&#21152;&#24378;&#29256;&#26412;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#21442;&#25968;&#20540;&#30340;&#32479;&#19968;&#30028;&#12290;&#36825;&#23548;&#33268;&#20102;&#26032;&#30340;&#24555;&#36895;&#36895;&#29575;&#21644;&#28151;&#21512;&#36895;&#29575;&#19978;&#38480;&#65292;&#36825;&#20123;&#19978;&#38480;&#21487;&#35299;&#37322;&#24615;&#24378;&#19988;&#27604;&#25991;&#29486;&#20013;&#20808;&#21069;&#30028;&#38480;&#26356;&#32039;&#12290;&#20854;&#27425;&#65292;&#38024;&#23545;&#26356;&#19968;&#33324;&#30340;&#23614;&#37096;&#34892;&#20026;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#26080;&#21442;&#25968;&#19978;&#38480;&#65306;&#24403;&#25439;&#22833;&#30340;&#32047;&#31215;&#29983;&#25104;&#20989;&#25968;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;PAC-Bayes Chernoff&#31867;&#27604;&#65292;&#21478;&#19968;&#20010;&#19978;&#38480;&#26159;&#25439;&#22833;&#30340;&#20108;&#38454;&#30697;&#26377;&#30028;&#12290;&#36825;&#20004;&#20010;&#19978;&#38480;&#26159;&#21033;&#29992;&#19968;&#31181;&#22522;&#20110;&#21487;&#33021;&#20107;&#20214;&#31354;&#38388;&#30340;&#31163;&#25955;&#21270;&#30340;&#26032;&#25216;&#26415;&#33719;&#24471;&#30340;&#65292;&#8220;&#22312;&#27010;&#29575;&#8221;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30028;&#38480;&#30340;&#31616;&#21333;&#25216;&#26415;&#23558;&#25152;&#26377;&#20808;&#21069;&#32467;&#26524;&#25193;&#23637;&#21040;&#20219;&#20309;&#26102;&#38388;&#26377;&#25928;&#30340;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present new high-probability PAC-Bayes bounds for different types of losses. Firstly, for losses with a bounded range, we present a strengthened version of Catoni's bound that holds uniformly for all parameter values. This leads to new fast rate and mixed rate bounds that are interpretable and tighter than previous bounds in the literature. Secondly, for losses with more general tail behaviors, we introduce two new parameter-free bounds: a PAC-Bayes Chernoff analogue when the loss' cumulative generating function is bounded, and a bound when the loss' second moment is bounded. These two bounds are obtained using a new technique based on a discretization of the space of possible events for the "in probability" parameter optimization problem. Finally, we extend all previous results to anytime-valid bounds using a simple technique applicable to any existing bound.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#19968;&#31181;&#22522;&#20110;&#28857;&#31215;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#24179;&#22343;&#28857;&#31215;&#21512;&#24182;&#32858;&#31867;&#65292;&#24182;&#19988;&#36755;&#20986;&#30340;&#26641;&#32467;&#26500;&#21487;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#25968;&#25454;&#30340;&#29983;&#25104;&#23618;&#27425;&#32467;&#26500;&#65292;&#26641;&#24418;&#24674;&#22797;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15022</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#31215;&#30340;&#23618;&#27425;&#32858;&#31867;&#21487;&#20197;&#24674;&#22797;&#38544;&#34255;&#30340;&#26641;&#24418;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Hierarchical clustering with dot products recovers hidden tree structure. (arXiv:2305.15022v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#19968;&#31181;&#22522;&#20110;&#28857;&#31215;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#24179;&#22343;&#28857;&#31215;&#21512;&#24182;&#32858;&#31867;&#65292;&#24182;&#19988;&#36755;&#20986;&#30340;&#26641;&#32467;&#26500;&#21487;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#25968;&#25454;&#30340;&#29983;&#25104;&#23618;&#27425;&#32467;&#26500;&#65292;&#26641;&#24418;&#24674;&#22797;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#20110;&#24050;&#26377;&#20957;&#32858;&#32858;&#31867;&#31639;&#27861;&#30340;&#26032;&#35270;&#35282;&#65292;&#19987;&#27880;&#20110;&#23618;&#27425;&#32467;&#26500;&#30340;&#24674;&#22797;&#12290;&#25105;&#20204;&#24314;&#35758;&#19968;&#31181;&#31616;&#21333;&#30340;&#26631;&#20934;&#31639;&#27861;&#21464;&#20307;&#65292;&#20854;&#20013;&#32858;&#31867;&#26159;&#36890;&#36807;&#26368;&#22823;&#24179;&#22343;&#28857;&#31215;&#32780;&#19981;&#26159;&#26368;&#23567;&#36317;&#31163;&#25110;&#31751;&#20869;&#26041;&#24046;&#26469;&#21512;&#24182;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27492;&#31639;&#27861;&#36755;&#20986;&#30340;&#26641;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#23618;&#27425;&#32467;&#26500;&#30340;&#21487;&#38752;&#20272;&#35745;&#12290;&#20851;&#38190;&#25216;&#26415;&#21019;&#26032;&#22312;&#20110;&#29702;&#35299;&#27169;&#22411;&#20013;&#30340;&#23618;&#27425;&#20449;&#24687;&#22914;&#20309;&#36716;&#21270;&#20026;&#21487;&#20174;&#25968;&#25454;&#20013;&#24674;&#22797;&#30340;&#26641;&#24418;&#20960;&#20309;&#20449;&#24687;&#65292;&#24182;&#21516;&#26102;&#22686;&#38271;&#26679;&#26412;&#22823;&#23567;&#21644;&#25968;&#25454;&#32500;&#25968;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;UPGMA&#12289;Ward's&#26041;&#27861;&#21644;HDBSCAN&#65289;&#30340;&#26641;&#24418;&#24674;&#22797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we offer a new perspective on the well established agglomerative clustering algorithm, focusing on recovery of hierarchical structure. We recommend a simple variant of the standard algorithm, in which clusters are merged by maximum average dot product and not, for example, by minimum distance or within-cluster variance. We demonstrate that the tree output by this algorithm provides a bona fide estimate of generative hierarchical structure in data, under a generic probabilistic graphical model. The key technical innovations are to understand how hierarchical information in this model translates into tree geometry which can be recovered from data, and to characterise the benefits of simultaneously growing sample size and data dimension. We demonstrate superior tree recovery performance with real data over existing approaches such as UPGMA, Ward's method, and HDBSCAN.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;WCAM&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#65292;&#30830;&#23450;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#38416;&#26126;&#32553;&#25918;&#22914;&#20309;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14979</link><description>&lt;p&gt;
&#23610;&#24230;&#24456;&#37325;&#35201;&#65306;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;&#35299;&#37322;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scale Matters: Attribution Meets the Wavelet Domain to Explain Model Sensitivity to Image Corruptions. (arXiv:2305.14979v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;WCAM&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#65292;&#30830;&#23450;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#38416;&#26126;&#32553;&#25918;&#22914;&#20309;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#30001;&#20110;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#23646;&#24615;&#26041;&#27861;&#23545;&#20110;&#35299;&#37322;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#26159;&#26080;&#25928;&#30340;&#65292;&#32780;&#24378;&#20581;&#24615;&#39046;&#22495;&#30340;&#25991;&#29486;&#20165;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#20687;&#25439;&#22351;&#30340;&#24773;&#20917;&#19979;&#65292;&#23457;&#26597;&#27169;&#22411;&#30340;&#34892;&#20026;&#33021;&#21147;&#23545;&#20110;&#25552;&#39640;&#29992;&#25143;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Wavelet sCale Attribution Method (WCAM)&#65292;&#23427;&#26159;&#20174;&#20687;&#32032;&#22495;&#21040;&#31354;&#38388;&#23610;&#24230;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;&#30340;&#27010;&#25324;&#12290;&#22312;&#31354;&#38388;&#23610;&#24230;&#22495;&#20013;&#36827;&#34892;&#23646;&#24615;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#20851;&#27880;&#28857;&#21644;&#23610;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;WCAM&#35299;&#37322;&#20102;&#27169;&#22411;&#22312;&#22270;&#20687;&#30772;&#22351;&#19979;&#30340;&#22833;&#25928;&#65292;&#30830;&#23450;&#20102;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#35299;&#37322;&#20102;&#22914;&#20309;&#36890;&#36807;&#32553;&#25918;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have shown remarkable performance in computer vision, but their deployment in real-world scenarios is challenging due to their sensitivity to image corruptions. Existing attribution methods are uninformative for explaining the sensitivity to image corruptions, while the literature on robustness only provides model-based explanations. However, the ability to scrutinize models' behavior under image corruptions is crucial to increase the user's trust. Towards this end, we introduce the Wavelet sCale Attribution Method (WCAM), a generalization of attribution from the pixel domain to the space-scale domain. Attribution in the space-scale domain reveals where and on what scales the model focuses. We show that the WCAM explains models' failures under image corruptions, identifies sufficient information for prediction, and explains how zoom-in increases accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;AD&#12290;</title><link>http://arxiv.org/abs/2302.07849</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Batch-Level Anomaly Detection. (arXiv:2302.07849v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;AD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#22312;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36866;&#24212;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#35843;&#25972;&#65292;&#29305;&#21035;&#26159;&#24403;&#27809;&#26377;&#38024;&#23545;&#8220;&#26032;&#27491;&#24120;&#8221;&#36827;&#34892;&#35757;&#32451;&#30340;&#25968;&#25454;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23548;&#33268;&#20135;&#29983;&#20102;&#38646;&#26679;&#26412;AD&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#65288;ACR&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;AD&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65288;&#20363;&#22914;&#28145;&#24230;SVDD&#65289;&#26469;&#36866;&#24212;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#20854;&#33021;&#22815;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#25209;&#37327;&#24402;&#19968;&#21270;&#21152;&#20803;&#35757;&#32451;&#65292;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#21644;&#22810;&#21151;&#33021;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#38646;&#26679;&#26412;AD&#32467;&#26524;&#65292;&#24182;&#22312;&#26469;&#33258;&#19987;&#19994;&#39046;&#22495;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#27573;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) plays a crucial role in many safety-critical application domains. The challenge of adapting an anomaly detector to drift in the normal data distribution, especially when no training data is available for the "new normal," has led to the development of zero-shot AD techniques. In this paper, we propose a simple yet effective method called Adaptive Centered Representations (ACR) for zero-shot batch-level AD. Our approach trains off-the-shelf deep anomaly detectors (such as deep SVDD) to adapt to a set of inter-related training data distributions in combination with batch normalization, enabling automatic zero-shot generalization for unseen AD tasks. This simple recipe, batch normalization plus meta-training, is a highly effective and versatile tool. Our results demonstrate the first zero-shot AD results for tabular data and outperform existing methods in zero-shot anomaly detection and segmentation on image data from specialized domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#65292;&#21487;&#29992;&#20110;&#21442;&#25968;&#21270;&#30340;&#20107;&#20214;&#29983;&#25104;&#65292;&#24322;&#24120;&#20449;&#21495;&#25506;&#27979;&#20197;&#21450;&#31890;&#23376;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.00695</link><description>&lt;p&gt;
&#22810;&#21151;&#33021;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Versatile Energy-Based Probabilistic Models for High Energy Physics. (arXiv:2302.00695v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#65292;&#21487;&#29992;&#20110;&#21442;&#25968;&#21270;&#30340;&#20107;&#20214;&#29983;&#25104;&#65292;&#24322;&#24120;&#20449;&#21495;&#25506;&#27979;&#20197;&#21450;&#31890;&#23376;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#32463;&#20856;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20855;&#26377;&#33021;&#37327;&#20989;&#25968;&#24418;&#24335;&#28789;&#27963;&#24615;&#30340;&#22825;&#28982;&#20248;&#21183;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24314;&#27169;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#19982;&#36825;&#20123;&#36827;&#23637;&#19968;&#33268;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#26469;&#33258;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#30340;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#19968;&#20010;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25551;&#36848;&#20102;&#26356;&#39640;&#38454;&#30340;&#31890;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32534;&#30721;&#20307;&#31995;&#32467;&#26500;&#21644;&#38544;&#24335;&#29983;&#25104;&#12290;&#22312;&#24212;&#29992;&#26041;&#38754;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#24378;&#22823;&#30340;&#21442;&#25968;&#21270;&#20107;&#20214;&#29983;&#25104;&#22120;&#29992;&#20110;&#29289;&#29702;&#20223;&#30495;&#65292;&#19968;&#31181;&#27867;&#29992;&#30340;&#26080;&#20551;&#35774;&#20851;&#32852;&#30340;&#24322;&#24120;&#20449;&#21495;&#25506;&#27979;&#22120;&#65292;&#20197;&#21450;&#29992;&#20110;&#31890;&#23376;&#35782;&#21035;&#30340;&#22686;&#24378;&#20107;&#20214;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a classical generative modeling approach, energy-based models have the natural advantage of flexibility in the form of the energy function. Recently, energy-based models have achieved great success in modeling high-dimensional data in computer vision and natural language processing. In line with these advancements, we build a multi-purpose energy-based probabilistic model for High Energy Physics events at the Large Hadron Collider. This framework builds on a powerful generative model and describes higher-order inter-particle interactions.It suits different encoding architectures and builds on implicit generation. As for applicational aspects, it can serve as a powerful parameterized event generator for physics simulation, a generic anomalous signal detector free from spurious correlations, and an augmented event classifier for particle identification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31038;&#21306;&#26816;&#27979;&#30446;&#26631;&#19982;&#20854;&#23545;&#24212;&#30340;&#38544;&#24335;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#30456;&#32852;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#35745;&#31639;&#32593;&#32476;&#22312;&#20219;&#24847;&#30446;&#26631;&#19979;&#30340;&#25551;&#36848;&#38271;&#24230;&#65292;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#35775;&#38382;&#38544;&#24335;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.09186</link><description>&lt;p&gt;
&#38544;&#24335;&#27169;&#22411;&#12289;&#28508;&#22312;&#21387;&#32553;&#12289;&#20869;&#22312;&#20559;&#24046;&#21644;&#24265;&#20215;&#21320;&#39184;&#22312;&#31038;&#21306;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Implicit models, latent compression, intrinsic biases, and cheap lunches in community detection. (arXiv:2210.09186v6 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31038;&#21306;&#26816;&#27979;&#30446;&#26631;&#19982;&#20854;&#23545;&#24212;&#30340;&#38544;&#24335;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#30456;&#32852;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#35745;&#31639;&#32593;&#32476;&#22312;&#20219;&#24847;&#30446;&#26631;&#19979;&#30340;&#25551;&#36848;&#38271;&#24230;&#65292;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#35775;&#38382;&#38544;&#24335;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#30340;&#20219;&#21153;&#26088;&#22312;&#23558;&#32593;&#32476;&#21010;&#20998;&#20026;&#33410;&#28857;&#38598;&#32676;&#65292;&#20197;&#24635;&#32467;&#20854;&#22823;&#35268;&#27169;&#32467;&#26500;&#65292;&#24050;&#32463;&#24341;&#20986;&#20102;&#35768;&#22810;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#31454;&#20105;&#31639;&#27861;&#12290; &#19968;&#20123;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#26159;&#25512;&#26029;&#24615;&#30340;&#65292;&#36890;&#36807;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#26126;&#30830;&#22320;&#23548;&#20986;&#32858;&#31867;&#30446;&#26631;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#26159;&#25551;&#36848;&#24615;&#30340;&#65292;&#26681;&#25454;&#29305;&#23450;&#24212;&#29992;&#30340;&#30446;&#26631;&#23558;&#32593;&#32476;&#20998;&#25104;&#23376;&#38598;&#65292;&#36825;&#20351;&#24471;&#22312;&#21516;&#19968;&#35268;&#27169;&#19979;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#20219;&#20309;&#31038;&#21306;&#26816;&#27979;&#30446;&#26631;&#65288;&#25512;&#26029;&#24615;&#25110;&#25551;&#36848;&#24615;&#65289;&#19982;&#20854;&#30456;&#24212;&#30340;&#38544;&#24335;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#30456;&#32852;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35745;&#31639;&#32593;&#32476;&#21450;&#20854;&#22312;&#20219;&#24847;&#30446;&#26631;&#19979;&#30340;&#20998;&#21306;&#30340;&#25551;&#36848;&#38271;&#24230;&#65292;&#26080;&#38656;&#8220;&#22320;&#38754;&#23454;&#20917;&#8221;&#26631;&#31614;&#21363;&#21487;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#35775;&#38382;&#38544;&#24335;&#27169;&#22411;&#65292;&#36825;&#26159;&#20854;&#20182;&#26041;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of community detection, which aims to partition a network into clusters of nodes to summarize its large-scale structure, has spawned the development of many competing algorithms with varying objectives. Some community detection methods are inferential, explicitly deriving the clustering objective through a probabilistic generative model, while other methods are descriptive, dividing a network according to an objective motivated by a particular application, making it challenging to compare these methods on the same scale. Here we present a solution to this problem that associates any community detection objective, inferential or descriptive, with its corresponding implicit network generative model. This allows us to compute the description length of a network and its partition under arbitrary objectives, providing a principled measure to compare the performance of different algorithms without the need for "ground truth" labels. Our approach also gives access to instances of the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#24102;&#26469;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#65292;&#25552;&#39640;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.07290</link><description>&lt;p&gt;
&#21452;&#25511;&#21046;&#21464;&#37327;&#21152;&#36895;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Dual control variate for faster black-box variational inference. (arXiv:2210.07290v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#24102;&#26469;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#24046;&#65292;&#25552;&#39640;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#25512;&#26029;&#26694;&#26550;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#26799;&#24230;&#20272;&#35745;&#20013;&#30340;&#39640;&#26041;&#24046;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#24046;&#26469;&#33258;&#20004;&#20010;&#38543;&#26426;&#28304;&#65306;&#25968;&#25454;&#23376;&#25277;&#26679;&#21644;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#12290;&#29616;&#26377;&#30340;&#25511;&#21046;&#21464;&#37327;&#20165;&#35299;&#20915;&#33945;&#29305;&#21345;&#32599;&#22122;&#22768;&#65292;&#32780;&#22686;&#37327;&#26799;&#24230;&#26041;&#27861;&#36890;&#24120;&#20165;&#35299;&#20915;&#25968;&#25454;&#23376;&#25277;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21452;&#8221;&#25511;&#21046;&#21464;&#37327;&#65292;&#33021;&#22815;&#21516;&#26102;&#20943;&#23569;&#20004;&#31181;&#22122;&#22768;&#28304;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#30830;&#35748;&#36825;&#23548;&#33268;&#20102;&#20943;&#23569;&#26041;&#24046;&#21644;&#22312;&#22810;&#20010;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#25552;&#39640;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box variational inference is a widely-used framework for Bayesian posterior inference, but in some cases suffers from high variance in gradient estimates, harming accuracy and efficiency. This variance comes from two sources of randomness: Data subsampling and Monte Carlo sampling. Whereas existing control variates only address Monte Carlo noise and incremental gradient methods typically only address data subsampling, we propose a new "dual" control variate capable of jointly reducing variance from both sources of noise. We confirm that this leads to reduced variance and improved optimization in several real-world applications.
&lt;/p&gt;</description></item></channel></rss>