<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#19981;&#21516;&#30340;&#25955;&#24230;&#25490;&#24207;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#24182;&#19988;&#22240;&#23376;&#21270;&#36817;&#20284;&#26080;&#27861;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;</title><link>https://arxiv.org/abs/2403.13748</link><description>&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#20013;&#22240;&#23376;&#21270;&#39640;&#26031;&#36817;&#20284;&#30340;&#24046;&#24322;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
An Ordering of Divergences for Variational Inference with Factorized Gaussian Approximations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13748
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#25955;&#24230;&#25490;&#24207;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#24182;&#19988;&#22240;&#23376;&#21270;&#36817;&#20284;&#26080;&#27861;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#20998;&#24067;$p$&#65292;&#38382;&#39064;&#26159;&#20174;&#19968;&#20123;&#26356;&#26131;&#22788;&#29702;&#30340;&#26063;$\mathcal{Q}$&#20013;&#35745;&#31639;&#26368;&#20339;&#36817;&#20284;$q$&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#36817;&#20284;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;Kullback-Leibler (KL)&#25955;&#24230;&#26469;&#25214;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20854;&#20182;&#26377;&#25928;&#30340;&#25955;&#24230;&#36873;&#25321;&#65292;&#24403;$\mathcal{Q}$&#19981;&#21253;&#21547;$p$&#26102;&#65292;&#27599;&#20010;&#25955;&#24230;&#37117;&#25903;&#25345;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#39640;&#26031;&#30340;&#23494;&#38598;&#21327;&#26041;&#24046;&#30697;&#38453;&#34987;&#23545;&#35282;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#36817;&#20284;&#25152;&#24433;&#21709;&#30340;VI&#32467;&#26524;&#20013;&#65292;&#25955;&#24230;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;VI&#32467;&#26524;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#21516;&#30340;&#25955;&#24230;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#22914;&#26041;&#24046;&#12289;&#31934;&#24230;&#21644;&#29109;&#65292;&#36827;&#34892;\textit{&#25490;&#24207;}&#12290;&#25105;&#20204;&#36824;&#24471;&#20986;&#19968;&#20010;&#19981;&#21487;&#33021;&#23450;&#29702;&#65292;&#34920;&#26126;&#26080;&#27861;&#36890;&#36807;&#22240;&#23376;&#21270;&#36817;&#20284;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;&#65307;&#22240;&#27492;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13748v1 Announce Type: cross  Abstract: Given an intractable distribution $p$, the problem of variational inference (VI) is to compute the best approximation $q$ from some more tractable family $\mathcal{Q}$. Most commonly the approximation is found by minimizing a Kullback-Leibler (KL) divergence. However, there exist other valid choices of divergences, and when $\mathcal{Q}$ does not contain~$p$, each divergence champions a different solution. We analyze how the choice of divergence affects the outcome of VI when a Gaussian with a dense covariance matrix is approximated by a Gaussian with a diagonal covariance matrix. In this setting we show that different divergences can be \textit{ordered} by the amount that their variational approximations misestimate various measures of uncertainty, such as the variance, precision, and entropy. We also derive an impossibility theorem showing that no two of these measures can be simultaneously matched by a factorized approximation; henc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#21160;&#24577;&#31995;&#32479;&#27010;&#29575;&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#38543;&#26426;&#25554;&#20540;&#22120;&#26500;&#24314;&#34394;&#26500;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26080;&#20559;&#35265;&#22320;&#29983;&#25104;&#26410;&#26469;&#31995;&#32479;&#29366;&#24577;&#30340;&#27010;&#29575;&#24615;&#39044;&#27979;&#38598;&#21512;</title><link>https://arxiv.org/abs/2403.13724</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#25554;&#20540;&#22120;&#21644;F\"ollmer&#36807;&#31243;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting with Stochastic Interpolants and F\"ollmer Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#21160;&#24577;&#31995;&#32479;&#27010;&#29575;&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#38543;&#26426;&#25554;&#20540;&#22120;&#26500;&#24314;&#34394;&#26500;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26080;&#20559;&#35265;&#22320;&#29983;&#25104;&#26410;&#26469;&#31995;&#32479;&#29366;&#24577;&#30340;&#27010;&#29575;&#24615;&#39044;&#27979;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#21160;&#24577;&#31995;&#32479;&#27010;&#29575;&#39044;&#27979;&#26694;&#26550;&#12290;&#22312;&#32473;&#23450;&#31995;&#32479;&#38543;&#26102;&#38388;&#30340;&#29366;&#24577;&#35266;&#27979;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#39044;&#27979;&#38382;&#39064;&#26500;&#24314;&#20026;&#20174;&#32473;&#23450;&#24403;&#21069;&#29366;&#24577;&#30340;&#26465;&#20214;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#24471;&#21040;&#26410;&#26469;&#31995;&#32479;&#29366;&#24577;&#30340;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#38543;&#26426;&#25554;&#20540;&#22120;&#30340;&#26694;&#26550;&#65292;&#36825;&#26377;&#21161;&#20110;&#26500;&#24314;&#22312;&#20219;&#24847;&#22522;&#30784;&#20998;&#24067;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#34394;&#26500;&#30340;&#12289;&#38750;&#29289;&#29702;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#20854;&#20197;&#24403;&#21069;&#31995;&#32479;&#29366;&#24577;&#20316;&#20026;&#21021;&#22987;&#26465;&#20214;&#65292;&#24182;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26080;&#20559;&#35265;&#22320;&#29983;&#25104;&#19968;&#20010;&#26469;&#33258;&#30446;&#26631;&#26465;&#20214;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#36807;&#31243;&#23558;&#20197;&#24403;&#21069;&#29366;&#24577;&#20026;&#20013;&#24515;&#30340;&#28857;&#29366;&#36136;&#37327;&#26144;&#23556;&#21040;&#19968;&#20010;&#27010;&#29575;&#24615;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23454;&#29616;&#36825;&#19968;&#20219;&#21153;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDE)&#20013;&#30340;&#28418;&#31227;&#31995;&#25968;&#26159;&#38750;&#22855;&#24322;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13724v1 Announce Type: new  Abstract: We propose a framework for probabilistic forecasting of dynamical systems based on generative modeling. Given observations of the system state over time, we formulate the forecasting problem as sampling from the conditional distribution of the future system state given its current state. To this end, we leverage the framework of stochastic interpolants, which facilitates the construction of a generative model between an arbitrary base distribution and the target. We design a fictitious, non-physical stochastic dynamics that takes as initial condition the current system state and produces as output a sample from the target conditional distribution in finite time and without bias. This process therefore maps a point mass centered at the current state onto a probabilistic ensemble of forecasts. We prove that the drift coefficient entering the stochastic differential equation (SDE) achieving this task is non-singular, and that it can be lear
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#30340;Mann-Whitney U&#26816;&#39564;&#65292;&#20197;&#30830;&#23450;&#22312;&#38544;&#31169;&#20445;&#25252;&#21512;&#25104;&#25968;&#25454;&#19978;&#25191;&#34892;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#26159;&#21542;&#21487;&#33021;&#23548;&#33268;&#27979;&#35797;&#26377;&#25928;&#24615;&#30340;&#20007;&#22833;&#25110;&#21151;&#29575;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.13612</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#33021;&#23548;&#33268;&#21512;&#25104;&#21457;&#29616;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Differentially Private Synthetic Data Lead to Synthetic Discoveries?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13612
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#30340;Mann-Whitney U&#26816;&#39564;&#65292;&#20197;&#30830;&#23450;&#22312;&#38544;&#31169;&#20445;&#25252;&#21512;&#25104;&#25968;&#25454;&#19978;&#25191;&#34892;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#26159;&#21542;&#21487;&#33021;&#23548;&#33268;&#27979;&#35797;&#26377;&#25928;&#24615;&#30340;&#20007;&#22833;&#25110;&#21151;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;&#20849;&#20139;&#25935;&#24863;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#21311;&#21517;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#21512;&#25104;&#25968;&#25454;&#24212;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#30340;&#32467;&#26500;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#21516;&#26102;&#20445;&#25252;&#20010;&#20307;&#20027;&#20307;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30446;&#21069;&#34987;&#35748;&#20026;&#26159;&#24179;&#34913;&#36825;&#31181;&#26435;&#34913;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#35780;&#20272;&#22312;&#24046;&#20998;&#38544;&#31169;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;Mann-Whitney U&#26816;&#39564;&#22312;I&#22411;&#21644;II&#22411;&#38169;&#35823;&#26041;&#38754;&#65292;&#20197;&#30830;&#23450;&#22312;&#38544;&#31169;&#20445;&#25252;&#21512;&#25104;&#25968;&#25454;&#19978;&#25191;&#34892;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#26159;&#21542;&#21487;&#33021;&#23548;&#33268;&#27979;&#35797;&#26377;&#25928;&#24615;&#30340;&#20007;&#22833;&#25110;&#21151;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13612v1 Announce Type: new  Abstract: Background: Synthetic data has been proposed as a solution for sharing anonymized versions of sensitive biomedical datasets. Ideally, synthetic data should preserve the structure and statistical properties of the original data, while protecting the privacy of the individual subjects. Differential privacy (DP) is currently considered the gold standard approach for balancing this trade-off.   Objectives: The aim of this study is to evaluate the Mann-Whitney U test on DP-synthetic biomedical data in terms of Type I and Type II errors, in order to establish whether statistical hypothesis testing performed on privacy preserving synthetic data is likely to lead to loss of test's validity or decreased power.   Methods: We evaluate the Mann-Whitney U test on DP-synthetic data generated from real-world data, including a prostate cancer dataset (n=500) and a cardiovascular dataset (n=70 000), as well as on data drawn from two Gaussian distribution
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#33258;&#36866;&#24212;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;&#21644;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.13565</link><description>&lt;p&gt;
AdaTrans&#65306;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#29305;&#24449;&#33258;&#36866;&#24212;&#19982;&#26679;&#26412;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13565
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#33258;&#36866;&#24212;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;&#21644;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#39640;&#32500;&#32972;&#26223;&#19979;&#30340;&#36801;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#29305;&#24449;&#32500;&#24230;&#22823;&#20110;&#26679;&#26412;&#22823;&#23567;&#12290;&#20026;&#20102;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20449;&#24687;&#65292;&#35813;&#20449;&#24687;&#21487;&#33021;&#22312;&#29305;&#24449;&#25110;&#28304;&#26679;&#26412;&#20043;&#38388;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;-wise (F-AdaTrans)&#25110;&#26679;&#26412;-wise (S-AdaTrans)&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#34701;&#21512;&#24809;&#32602;&#26041;&#27861;&#65292;&#32467;&#21512;&#26435;&#37325;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#36827;&#34892;&#35843;&#25972;&#12290;&#20026;&#20102;&#36873;&#25321;&#26435;&#37325;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#36807;&#31243;&#65292;&#20351;&#24471; F-AdaTrans &#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#23558;&#21487;&#36801;&#31227;&#30340;&#20449;&#21495;&#19982;&#30446;&#26631;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#28388;&#38500;&#38750;&#21487;&#36801;&#31227;&#30340;&#20449;&#21495;&#65292;S-AdaTrans&#21017;&#21487;&#20197;&#33719;&#24471;&#27599;&#20010;&#28304;&#26679;&#26412;&#20256;&#36882;&#30340;&#20449;&#24687;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#36895;&#29575;&#65292;&#21487;&#20197;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#24674;&#22797;&#29616;&#26377;&#30340;&#36817;&#26368;&#23567;&#20284;&#20046;&#26368;&#20248;&#36895;&#29575;&#12290;&#25928;&#26524;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13565v1 Announce Type: cross  Abstract: We consider the transfer learning problem in the high dimensional setting, where the feature dimension is larger than the sample size. To learn transferable information, which may vary across features or the source samples, we propose an adaptive transfer learning method that can detect and aggregate the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable structures. We achieve this by employing a novel fused-penalty, coupled with weights that can adapt according to the transferable structure. To choose the weight, we propose a theoretically informed, data-driven procedure, enabling F-AdaTrans to selectively fuse the transferable signals with the target while filtering out non-transferable signals, and S-AdaTrans to obtain the optimal combination of information transferred from each source sample. The non-asymptotic rates are established, which recover existing near-minimax optimal rates in special cases. The effectivene
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#26088;&#22312;&#31995;&#32479;&#27604;&#36739;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#27010;&#29575;&#24615;&#22825;&#27668;&#39044;&#27979;&#65292;&#36229;&#36234;&#20256;&#32479;&#22522;&#20110;&#29289;&#29702;&#30340;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.13458</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22825;&#27668;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification for data-driven weather models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13458
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#26088;&#22312;&#31995;&#32479;&#27604;&#36739;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#27010;&#29575;&#24615;&#22825;&#27668;&#39044;&#27979;&#65292;&#36229;&#36234;&#20256;&#32479;&#22522;&#20110;&#29289;&#29702;&#30340;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39537;&#21160;&#30340;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#22312;&#36807;&#21435;&#20960;&#24180;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20877;&#20998;&#26512;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#21464;&#37327;&#21644;&#35780;&#20272;&#25351;&#26631;&#19978;&#23637;&#31034;&#20102;&#26126;&#26174;&#25913;&#36827;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#12290;&#38500;&#20102;&#25913;&#36827;&#30340;&#39044;&#27979;&#22806;&#65292;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#27169;&#22411;&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#23427;&#20204;&#26174;&#33879;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#19968;&#26086;&#27169;&#22411;&#34987;&#35757;&#32451;&#23601;&#33021;&#26356;&#24555;&#22320;&#29983;&#25104;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#27979;&#30340;&#21162;&#21147;&#37117;&#23616;&#38480;&#20110;&#30830;&#23450;&#24615;&#30340;&#12289;&#28857;&#20540;&#39044;&#27979;&#65292;&#20351;&#24471;&#26080;&#27861;&#37327;&#21270;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23545;&#20110;&#30740;&#31350;&#21644;&#24212;&#29992;&#20013;&#30340;&#26368;&#20339;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#25972;&#20307;&#30446;&#26631;&#26159;&#31995;&#32479;&#22320;&#30740;&#31350;&#21644;&#27604;&#36739;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#27010;&#29575;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13458v1 Announce Type: cross  Abstract: Artificial intelligence (AI)-based data-driven weather forecasting models have experienced rapid progress over the last years. Recent studies, with models trained on reanalysis data, achieve impressive results and demonstrate substantial improvements over state-of-the-art physics-based numerical weather prediction models across a range of variables and evaluation metrics. Beyond improved predictions, the main advantages of data-driven weather models are their substantially lower computational costs and the faster generation of forecasts, once a model has been trained. However, most efforts in data-driven weather forecasting have been limited to deterministic, point-valued predictions, making it impossible to quantify forecast uncertainties, which is crucial in research and for optimal decision making in applications. Our overarching aim is to systematically study and compare uncertainty quantification methods to generate probabilistic 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#21253;&#25216;&#26415;&#35777;&#26126;&#21453;&#21521;&#25311;&#21512;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#26680;&#22810;&#37325;&#32593;&#26684;&#31639;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22686;&#24378;&#21453;&#21521;&#25311;&#21512;&#65292;&#36866;&#29992;&#20110;&#32467;&#26500;&#21270;&#21644;&#20998;&#25955;&#25968;&#25454;&#30340;&#21152;&#24615;GPs&#12290;</title><link>https://arxiv.org/abs/2403.13300</link><description>&lt;p&gt;
&#26680;&#22810;&#37325;&#32593;&#26684;&#65306;&#36890;&#36807;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#21152;&#36895;&#21453;&#21521;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Kernel Multigrid: Accelerate Back-fitting via Sparse Gaussian Process Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#21253;&#25216;&#26415;&#35777;&#26126;&#21453;&#21521;&#25311;&#21512;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#26680;&#22810;&#37325;&#32593;&#26684;&#31639;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22686;&#24378;&#21453;&#21521;&#25311;&#21512;&#65292;&#36866;&#29992;&#20110;&#32467;&#26500;&#21270;&#21644;&#20998;&#25955;&#25968;&#25454;&#30340;&#21152;&#24615;GPs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28155;&#21152;&#39640;&#26031;&#36807;&#31243;(GPs)&#26159;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#24120;&#35265;&#35757;&#32451;&#26041;&#27861;&#26159;&#36125;&#21494;&#26031;&#21453;&#21521;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#21152;&#24615;GPs&#26102;&#65292;&#21453;&#21521;&#25311;&#21512;&#30340;&#25910;&#25947;&#36895;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;&#26680;&#21253;(KP)&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21453;&#21521;&#25311;&#21512;&#30340;&#25910;&#25947;&#36895;&#24230;&#19981;&#20250;&#27604;$(1-\mathcal{O}(\frac{1}{n}))^t$&#26356;&#24555;&#65292;&#20854;&#20013;$n$&#21644;$t$&#20998;&#21035;&#34920;&#31034;&#25968;&#25454;&#22823;&#23567;&#21644;&#36845;&#20195;&#27425;&#25968;&#12290;&#22240;&#27492;&#65292;&#21453;&#21521;&#25311;&#21512;&#38656;&#35201;&#26368;&#23569;$\mathcal{O}(n\log n)$&#27425;&#36845;&#20195;&#25165;&#33021;&#23454;&#29616;&#25910;&#25947;&#12290;&#22522;&#20110;KP&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26680;&#22810;&#37325;&#32593;&#26684;(KMG)&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;(GPR)&#32435;&#20837;&#27599;&#20010;&#21453;&#21521;&#25311;&#21512;&#36845;&#20195;&#20043;&#21518;&#22788;&#29702;&#27531;&#24046;&#26469;&#22686;&#24378;&#21453;&#21521;&#25311;&#21512;&#12290;&#23427;&#36866;&#29992;&#20110;&#20855;&#26377;&#32467;&#26500;&#21270;&#21644;&#20998;&#25955;&#25968;&#25454;&#30340;&#21152;&#24615;GPs&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;K
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13300v1 Announce Type: cross  Abstract: Additive Gaussian Processes (GPs) are popular approaches for nonparametric feature selection. The common training method for these models is Bayesian Back-fitting. However, the convergence rate of Back-fitting in training additive GPs is still an open problem. By utilizing a technique called Kernel Packets (KP), we prove that the convergence rate of Back-fitting is no faster than $(1-\mathcal{O}(\frac{1}{n}))^t$, where $n$ and $t$ denote the data size and the iteration number, respectively. Consequently, Back-fitting requires a minimum of $\mathcal{O}(n\log n)$ iterations to achieve convergence. Based on KPs, we further propose an algorithm called Kernel Multigrid (KMG). This algorithm enhances Back-fitting by incorporating a sparse Gaussian Process Regression (GPR) to process the residuals subsequent to each Back-fitting iteration. It is applicable to additive GPs with both structured and scattered data. Theoretically, we prove that K
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#26679;&#30340;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22823;&#23646;&#24615;&#22270;&#20013;&#22788;&#29702;&#33410;&#28857;&#12289;&#36793;&#21644;&#36335;&#24452;&#20551;&#35774;&#65292;&#36890;&#36807;&#25552;&#20986;&#36335;&#24452;&#20551;&#35774;&#24863;&#30693;&#37319;&#26679;&#22120; PHASE &#20197;&#21450; PHASEopt&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#25277;&#26679;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20551;&#35774;&#26816;&#39564;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.13286</link><description>&lt;p&gt;
&#22522;&#20110;&#25277;&#26679;&#30340;&#22823;&#23646;&#24615;&#22270;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Sampling-based Framework for Hypothesis Testing on Large Attributed Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#26679;&#30340;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22823;&#23646;&#24615;&#22270;&#20013;&#22788;&#29702;&#33410;&#28857;&#12289;&#36793;&#21644;&#36335;&#24452;&#20551;&#35774;&#65292;&#36890;&#36807;&#25552;&#20986;&#36335;&#24452;&#20551;&#35774;&#24863;&#30693;&#37319;&#26679;&#22120; PHASE &#20197;&#21450; PHASEopt&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#25277;&#26679;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20551;&#35774;&#26816;&#39564;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#26816;&#39564;&#26159;&#19968;&#31181;&#29992;&#20110;&#20174;&#26679;&#26412;&#25968;&#25454;&#20013;&#24471;&#20986;&#20851;&#20110;&#24635;&#20307;&#30340;&#32467;&#35770;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#36890;&#24120;&#29992;&#34920;&#26684;&#34920;&#31034;&#12290;&#38543;&#30528;&#29616;&#23454;&#24212;&#29992;&#20013;&#22270;&#34920;&#31034;&#30340;&#26222;&#21450;&#65292;&#22270;&#20013;&#30340;&#20551;&#35774;&#26816;&#39564;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#36335;&#24452;&#20551;&#35774;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#26679;&#30340;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#21487;&#20197;&#23481;&#32435;&#29616;&#26377;&#30340;&#20551;&#35774;&#19981;&#21487;&#30693;&#30340;&#22270;&#25277;&#26679;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#25277;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36335;&#24452;&#20551;&#35774;&#24863;&#30693;&#37319;&#26679;&#22120; PHASE&#65292;&#23427;&#26159;&#19968;&#31181;&#32771;&#34385;&#20551;&#35774;&#20013;&#25351;&#23450;&#36335;&#24452;&#30340; m-&#32500;&#38543;&#26426;&#28216;&#36208;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#20854;&#26102;&#38388;&#25928;&#29575;&#24182;&#25552;&#20986;&#20102; PHASEopt&#12290;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#21033;&#29992;&#24120;&#35265;&#30340;&#22270;&#25277;&#26679;&#26041;&#27861;&#36827;&#34892;&#20551;&#35774;&#26816;&#39564;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#25928;&#29575;&#26041;&#38754;&#20551;&#35774;&#24863;&#30693;&#25277;&#26679;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13286v1 Announce Type: cross  Abstract: Hypothesis testing is a statistical method used to draw conclusions about populations from sample data, typically represented in tables. With the prevalence of graph representations in real-life applications, hypothesis testing in graphs is gaining importance. In this work, we formalize node, edge, and path hypotheses in attributed graphs. We develop a sampling-based hypothesis testing framework, which can accommodate existing hypothesis-agnostic graph sampling methods. To achieve accurate and efficient sampling, we then propose a Path-Hypothesis-Aware SamplEr, PHASE, an m- dimensional random walk that accounts for the paths specified in a hypothesis. We further optimize its time efficiency and propose PHASEopt. Experiments on real datasets demonstrate the ability of our framework to leverage common graph sampling methods for hypothesis testing, and the superiority of hypothesis-aware sampling in terms of accuracy and time efficiency.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#29305;&#23450;&#32593;&#32476;&#29305;&#24449;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#21306;&#20998;&#29983;&#25104;&#27169;&#22411;&#12289;&#29702;&#35299;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#21644;&#29983;&#25104;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;</title><link>https://arxiv.org/abs/2403.13215</link><description>&lt;p&gt;
&#20160;&#20040;&#36896;&#23601;&#20102;&#19968;&#20010;&#23567;&#19990;&#30028;&#32593;&#32476;&#65311;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#32593;&#32476;&#30340;&#31283;&#20581;&#39044;&#27979;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
What makes a small-world network? Leveraging machine learning for the robust prediction and classification of networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13215
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#29305;&#23450;&#32593;&#32476;&#29305;&#24449;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#21306;&#20998;&#29983;&#25104;&#27169;&#22411;&#12289;&#29702;&#35299;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#21644;&#29983;&#25104;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#27969;&#34892;&#30149;&#23398;&#21040;&#35745;&#31639;&#26426;&#31185;&#23398;&#65292;&#22522;&#20110;&#23454;&#35777;&#25968;&#25454;&#27169;&#25311;&#30495;&#23454;&#32593;&#32476;&#30340;&#33021;&#21147;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#36890;&#24120;&#65292;&#27169;&#25311;&#26041;&#27861;&#28041;&#21450;&#36873;&#25321;&#36866;&#21512;&#30340;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;Erd\"os-R\'enyi&#25110;&#23567;&#19990;&#30028;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20855;&#21487;&#29992;&#20110;&#37327;&#21270;&#29305;&#23450;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#36866;&#21512;&#25429;&#25417;&#32473;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#25110;&#32452;&#32455;&#12290;&#25105;&#20204;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#65292;&#26681;&#25454;&#21508;&#31181;&#32593;&#32476;&#23646;&#24615;&#20197;&#21450;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#23545;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#23545;&#27169;&#25311;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#29305;&#23450;&#32593;&#32476;&#29305;&#24449;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#22312;&#21306;&#20998;&#29983;&#25104;&#27169;&#22411;&#12289;&#29702;&#35299;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#20197;&#21450;&#24418;&#25104;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13215v1 Announce Type: cross  Abstract: The ability to simulate realistic networks based on empirical data is an important task across scientific disciplines, from epidemiology to computer science. Often simulation approaches involve selecting a suitable network generative model such as Erd\"os-R\'enyi or small-world. However, few tools are available to quantify if a particular generative model is suitable for capturing a given network structure or organization. We utilize advances in interpretable machine learning to classify simulated networks by our generative models based on various network attributes, using both primary features and their interactions. Our study underscores the significance of specific network features and their interactions in distinguishing generative models, comprehending complex network structures, and forming real-world networks
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#20869;&#22810;&#26679;&#24615;&#21644;&#24179;&#22374;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;DASH&#65292;&#36890;&#36807;&#40723;&#21169;&#22522;&#30784;&#23398;&#20064;&#22120;&#21521;&#26368;&#23567;&#38160;&#24230;&#21306;&#22495;&#30340;&#20302;&#25439;&#22833;&#21306;&#22495;&#21457;&#25955;&#31227;&#21160;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13204</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#24863;&#30693;&#30340;&#26080;&#20559;&#23567;&#20154;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diversity-Aware Agnostic Ensemble of Sharpness Minimizers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13204
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#20869;&#22810;&#26679;&#24615;&#21644;&#24179;&#22374;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;DASH&#65292;&#36890;&#36807;&#40723;&#21169;&#22522;&#30784;&#23398;&#20064;&#22120;&#21521;&#26368;&#23567;&#38160;&#24230;&#21306;&#22495;&#30340;&#20302;&#25439;&#22833;&#21306;&#22495;&#21457;&#25955;&#31227;&#21160;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#26377;&#22823;&#37327;&#29702;&#35770;&#21644;&#32463;&#39564;&#35777;&#25454;&#25903;&#25345;&#38598;&#25104;&#23398;&#20064;&#30340;&#25104;&#21151;&#12290;&#29305;&#21035;&#26159;&#28145;&#24230;&#38598;&#25104;&#21033;&#29992;&#35757;&#32451;&#20013;&#30340;&#38543;&#26426;&#24615;&#21644;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#65292;&#33719;&#24471;&#39044;&#27979;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#26368;&#32456;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#12289;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22312;&#27867;&#21270;&#26041;&#38754;&#65292;&#21457;&#29616;&#36861;&#27714;&#26356;&#24191;&#27867;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#20250;&#23548;&#33268;&#27169;&#22411;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#36716;&#21464;&#26356;&#21152;&#40065;&#26834;&#12290;&#22522;&#20110;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#65292;&#22914;&#26524;&#38598;&#25104;&#23398;&#20064;&#21644;&#25439;&#22833;&#38160;&#24230;&#26368;&#23567;&#21270;&#30456;&#32467;&#21512;&#65292;&#26159;&#21542;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#36825;&#31181;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#20869;&#22810;&#26679;&#24615;&#21644;&#24179;&#22374;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;DASH&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;DASH&#40723;&#21169;&#22522;&#30784;&#23398;&#20064;&#22120;&#21521;&#26368;&#23567;&#38160;&#24230;&#21306;&#22495;&#30340;&#20302;&#25439;&#22833;&#21306;&#22495;&#21457;&#25955;&#31227;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13204v1 Announce Type: new  Abstract: There has long been plenty of theoretical and empirical evidence supporting the success of ensemble learning. Deep ensembles in particular take advantage of training randomness and expressivity of individual neural networks to gain prediction diversity, ultimately leading to better generalization, robustness and uncertainty estimation. In respect of generalization, it is found that pursuing wider local minima result in models being more robust to shifts between training and testing sets. A natural research question arises out of these two approaches as to whether a boost in generalization ability can be achieved if ensemble learning and loss sharpness minimization are integrated. Our work investigates this connection and proposes DASH - a learning algorithm that promotes diversity and flatness within deep ensembles. More concretely, DASH encourages base learners to move divergently towards low-loss regions of minimal sharpness. We provid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ADAPT&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65292;&#22686;&#24378;&#35270;&#35273;Transformer&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13196</link><description>&lt;p&gt;
&#20351;Prompt&#35843;&#20248;&#35270;&#35273;Transformer&#26356;&#20026;&#20581;&#22766;&#30340;ADAPT
&lt;/p&gt;
&lt;p&gt;
ADAPT to Robustify Prompt Tuning Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ADAPT&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65292;&#22686;&#24378;&#35270;&#35273;Transformer&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35270;&#35273;Transformer&#65292;&#24050;&#30693;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#29616;&#26377;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20381;&#36182;&#20110;&#23545;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#24494;&#35843;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#20219;&#21153;&#23384;&#20648;&#25972;&#20010;&#27169;&#22411;&#30340;&#21103;&#26412;&#65292;&#32780;&#27169;&#22411;&#21487;&#33021;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;prompt&#35843;&#20248;&#34987;&#29992;&#26469;&#36866;&#24212;&#22823;&#22411;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#20445;&#23384;&#22823;&#22411;&#21103;&#26412;&#12290;&#26412;&#25991;&#20174;&#31283;&#20581;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#23545;&#35270;&#35273;Transformer&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#21442;&#25968;&#39640;&#25928;prompt&#35843;&#20248;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20043;&#21069;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#22312;&#24212;&#29992;&#21040;prompt&#35843;&#20248;&#33539;&#24335;&#26102;&#65292;&#23384;&#22312;&#26799;&#24230;&#27169;&#31946;&#24182;&#23481;&#26131;&#21463;&#21040;&#33258;&#36866;&#24212;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ADAPT&#65292;&#19968;&#31181;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#25191;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13196v1 Announce Type: new  Abstract: The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#20934;&#30830;&#24615;&#20294;&#20302;&#35299;&#37322;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PSI-KT&#30340;&#20998;&#23618;&#29983;&#25104;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#20010;&#20307;&#35748;&#30693;&#29305;&#24449;&#21644;&#30693;&#35782;&#32467;&#26500;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#23454;&#29616;&#20102;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#23398;&#20064;&#32773;&#32676;&#20307;&#30340;&#39640;&#25928;&#20010;&#24615;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.13179</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#39046;&#22495;&#19978;&#30340;&#39044;&#27979;&#24615;&#12289;&#21487;&#20280;&#32553;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30693;&#35782;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Predictive, scalable and interpretable knowledge tracing on structured domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#20934;&#30830;&#24615;&#20294;&#20302;&#35299;&#37322;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PSI-KT&#30340;&#20998;&#23618;&#29983;&#25104;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#20010;&#20307;&#35748;&#30693;&#29305;&#24449;&#21644;&#30693;&#35782;&#32467;&#26500;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#23454;&#29616;&#20102;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#23398;&#20064;&#32773;&#32676;&#20307;&#30340;&#39640;&#25928;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#36890;&#36807;&#20248;&#21270;&#23398;&#20064;&#26448;&#26009;&#30340;&#36873;&#25321;&#21644;&#26102;&#38388;&#23433;&#25490;&#26469;&#22686;&#24378;&#29702;&#35299;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;&#36825;&#38656;&#35201;&#23545;&#23398;&#20064;&#32773;&#30340;&#36827;&#24230;&#65288;''&#30693;&#35782;&#36861;&#36394;''; KT&#65289;&#21644;&#23398;&#20064;&#39046;&#22495;&#30340;&#20808;&#20915;&#26465;&#20214;&#32467;&#26500;&#65288;''&#30693;&#35782;&#26144;&#23556;''&#65289;&#36827;&#34892;&#20272;&#35745;&#12290;&#22312;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#39640;KT&#20934;&#30830;&#24615;&#26159;&#21487;&#20197;&#23454;&#29616;&#30340;&#65292;&#20294;&#36825;&#26159;&#20197;&#29306;&#29298;&#24515;&#29702;&#21551;&#21457;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#20026;&#20195;&#20215;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#31181;&#26435;&#34913;&#30340;&#26041;&#26696;&#12290;PSI-KT&#26159;&#19968;&#31181;&#20998;&#23618;&#29983;&#25104;&#26041;&#27861;&#65292;&#26126;&#30830;&#24314;&#27169;&#20102;&#20010;&#20307;&#35748;&#30693;&#29305;&#24449;&#21644;&#30693;&#35782;&#30340;&#20808;&#20915;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#21160;&#24577;&#65292;&#20174;&#32780;&#36890;&#36807;&#35774;&#35745;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;PSI-KT&#38024;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#39640;&#25928;&#20010;&#24615;&#21270;&#30340;&#38656;&#27714;&#65292;&#21363;&#20351;&#26377;&#30528;&#19981;&#26029;&#22686;&#38271;&#30340;&#23398;&#20064;&#32773;&#32676;&#20307;&#21644;&#23398;&#20064;&#21382;&#21490;&#12290;&#22312;&#22312;&#32447;&#23398;&#20064;&#24179;&#21488;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13179v1 Announce Type: new  Abstract: Intelligent tutoring systems optimize the selection and timing of learning materials to enhance understanding and long-term retention. This requires estimates of both the learner's progress (''knowledge tracing''; KT), and the prerequisite structure of the learning domain (''knowledge mapping''). While recent deep learning models achieve high KT accuracy, they do so at the expense of the interpretability of psychologically-inspired models. In this work, we present a solution to this trade-off. PSI-KT is a hierarchical generative approach that explicitly models how both individual cognitive traits and the prerequisite structure of knowledge influence learning dynamics, thus achieving interpretability by design. Moreover, by using scalable Bayesian inference, PSI-KT targets the real-world need for efficient personalization even with a growing body of learners and learning histories. Evaluated on three datasets from online learning platform
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Kalman&#28388;&#27874;&#33539;&#24335;&#30340;&#26032;&#39062;&#21644;&#21487;&#25193;&#23637;&#30340;&#37319;&#26679;&#31639;&#27861;LKTD&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#25277;&#21462;&#26679;&#26412;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#21518;&#39564;&#26679;&#26412;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.13178</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24555;&#36895;&#20215;&#20540;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Fast Value Tracking for Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13178
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Kalman&#28388;&#27874;&#33539;&#24335;&#30340;&#26032;&#39062;&#21644;&#21487;&#25193;&#23637;&#30340;&#37319;&#26679;&#31639;&#27861;LKTD&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#25277;&#21462;&#26679;&#26412;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#21518;&#39564;&#26679;&#26412;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36890;&#36807;&#21019;&#24314;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;Agent&#26469;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#36890;&#24120;&#23558;&#36825;&#20123;&#38382;&#39064;&#35270;&#20026;&#38745;&#24577;&#38382;&#39064;&#65292;&#19987;&#27880;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#28857;&#20272;&#35745;&#20197;&#26368;&#22823;&#21270;&#39044;&#26399;&#22870;&#21169;&#65292;&#24573;&#35270;&#20102;Agent-Environment&#20114;&#21160;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#33539;&#24335;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#31216;&#20026;Langevinized Kalman Temporal-Difference&#65288;LKTD&#65289;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#31181;&#31639;&#27861;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;SGMCMC&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#25277;&#21462;&#26679;&#26412;&#12290;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LKTD&#31639;&#27861;&#29983;&#25104;&#30340;&#21518;&#39564;&#26679;&#26412;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#20998;&#24067;&#12290;&#36825;&#31181;&#25910;&#25947;&#19981;&#20165;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13178v1 Announce Type: cross  Abstract: Reinforcement learning (RL) tackles sequential decision-making problems by creating agents that interacts with their environment. However, existing algorithms often view these problem as static, focusing on point estimates for model parameters to maximize expected rewards, neglecting the stochastic dynamics of agent-environment interactions and the critical role of uncertainty quantification. Our research leverages the Kalman filtering paradigm to introduce a novel and scalable sampling algorithm called Langevinized Kalman Temporal-Difference (LKTD) for deep reinforcement learning. This algorithm, grounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently draws samples from the posterior distribution of deep neural network parameters. Under mild conditions, we prove that the posterior samples generated by the LKTD algorithm converge to a stationary distribution. This convergence not only enables us to quantify uncer
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35780;&#20998;&#35268;&#21017;&#35757;&#32451;&#29983;&#23384;&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#31867;&#21035;&#20013;&#24182;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#20363;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;&#20248;&#20110;&#22522;&#20110;&#20284;&#28982;&#24615;&#26041;&#27861;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13150</link><description>&lt;p&gt;
&#20351;&#29992;&#35780;&#20998;&#35268;&#21017;&#35757;&#32451;&#29983;&#23384;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Survival Models using Scoring Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13150
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35780;&#20998;&#35268;&#21017;&#35757;&#32451;&#29983;&#23384;&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#31867;&#21035;&#20013;&#24182;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#20363;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;&#20248;&#20110;&#22522;&#20110;&#20284;&#28982;&#24615;&#26041;&#27861;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#37096;&#20998;&#19981;&#23436;&#25972;&#30340;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#25968;&#25454;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;&#23427;&#20063;&#26159;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#25552;&#26696;&#20197;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#24335;&#21033;&#29992;&#20102;&#39044;&#27979;&#30340;&#27010;&#29575;&#24615;&#36136;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#25311;&#21512;&#36807;&#31243;&#20013;&#20351;&#29992;&#65288;&#21512;&#36866;&#30340;&#65289;&#35780;&#20998;&#35268;&#21017;&#32780;&#38750;&#22522;&#20110;&#20284;&#28982;&#24615;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19981;&#21516;&#30340;&#21442;&#25968;&#21270;&#21644;&#38750;&#21442;&#25968;&#21270;&#23376;&#26694;&#26550;&#65292;&#20801;&#35768;&#19981;&#21516;&#31243;&#24230;&#30340;&#28789;&#27963;&#24615;&#12290;&#23558;&#20854;&#28151;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;&#35745;&#31639;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#20363;&#31243;&#65292;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24674;&#22797;&#21508;&#31181;&#21442;&#25968;&#21270;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#22312;&#19982;&#22522;&#20110;&#20284;&#28982;&#24615;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#65292;&#20248;&#21270;&#25928;&#26524;&#21516;&#26679;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13150v1 Announce Type: new  Abstract: Survival Analysis provides critical insights for partially incomplete time-to-event data in various domains. It is also an important example of probabilistic machine learning. The probabilistic nature of the predictions can be exploited by using (proper) scoring rules in the model fitting process instead of likelihood-based optimization. Our proposal does so in a generic manner and can be used for a variety of model classes. We establish different parametric and non-parametric sub-frameworks that allow different degrees of flexibility. Incorporated into neural networks, it leads to a computationally efficient and scalable optimization routine, yielding state-of-the-art predictive performance. Finally, we show that using our framework, we can recover various parametric models and demonstrate that optimization works equally well when compared to likelihood-based methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#19968;&#33324;&#22810;&#21464;&#37327;&#20989;&#25968;&#20026;&#31616;&#21333;&#20989;&#25968;&#26641;&#30340;&#26041;&#27861;&#65292;&#35813;&#26641;&#33021;&#22815;&#24555;&#36895;&#35782;&#21035;&#21644;&#35745;&#31639;&#20989;&#25968;&#30340;&#20027;&#35201;&#21644;&#20132;&#20114;&#25928;&#24212;&#30452;&#33267;&#39640;&#38454;&#65292;&#20197;&#22270;&#24418;&#21270;&#26041;&#24335;&#23637;&#31034;&#28041;&#21450;&#21040;&#22235;&#20010;&#21464;&#37327;&#30340;&#20132;&#20114;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.13141</link><description>&lt;p&gt;
Function Trees: &#36879;&#26126;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Function Trees: Transparent Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13141
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#19968;&#33324;&#22810;&#21464;&#37327;&#20989;&#25968;&#20026;&#31616;&#21333;&#20989;&#25968;&#26641;&#30340;&#26041;&#27861;&#65292;&#35813;&#26641;&#33021;&#22815;&#24555;&#36895;&#35782;&#21035;&#21644;&#35745;&#31639;&#20989;&#25968;&#30340;&#20027;&#35201;&#21644;&#20132;&#20114;&#25928;&#24212;&#30452;&#33267;&#39640;&#38454;&#65292;&#20197;&#22270;&#24418;&#21270;&#26041;&#24335;&#23637;&#31034;&#28041;&#21450;&#21040;&#22235;&#20010;&#21464;&#37327;&#30340;&#20132;&#20114;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36755;&#20986;&#36890;&#24120;&#21487;&#20197;&#29992;&#20854;&#36755;&#20837;&#21464;&#37327;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#22810;&#21464;&#37327;&#20989;&#25968;&#34920;&#31034;&#12290;&#20102;&#35299;&#36825;&#31867;&#20989;&#25968;&#30340;&#20840;&#23616;&#29305;&#24615;&#26377;&#21161;&#20110;&#29702;&#35299;&#29983;&#25104;&#25968;&#25454;&#30340;&#31995;&#32479;&#65292;&#20197;&#21450;&#35299;&#37322;&#21644;&#38416;&#37322;&#30456;&#24212;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19968;&#33324;&#22810;&#21464;&#37327;&#20989;&#25968;&#34920;&#31034;&#20026;&#31616;&#21333;&#20989;&#25968;&#26641;&#30340;&#26041;&#27861;&#12290;&#36825;&#26869;&#26641;&#36890;&#36807;&#25581;&#31034;&#21644;&#25551;&#36848;&#20854;&#36755;&#20837;&#21464;&#37327;&#23376;&#38598;&#30340;&#32852;&#21512;&#24433;&#21709;&#65292;&#26469;&#26292;&#38706;&#20989;&#25968;&#30340;&#20840;&#23616;&#20869;&#37096;&#32467;&#26500;&#12290;&#26681;&#25454;&#36755;&#20837;&#21644;&#23545;&#24212;&#30340;&#20989;&#25968;&#20540;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#20197;&#24555;&#36895;&#35782;&#21035;&#21644;&#35745;&#31639;&#20989;&#25968;&#30340;&#25152;&#26377;&#20027;&#35201;&#21644;&#20132;&#20114;&#25928;&#24212;&#30452;&#33267;&#39640;&#38454;&#30340;&#20989;&#25968;&#26641;&#12290;&#28041;&#21450;&#21040;&#22235;&#20010;&#21464;&#37327;&#30340;&#20132;&#20114;&#25928;&#24212;&#36827;&#34892;&#20102;&#22270;&#24418;&#21270;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13141v1 Announce Type: cross  Abstract: The output of a machine learning algorithm can usually be represented by one or more multivariate functions of its input variables. Knowing the global properties of such functions can help in understanding the system that produced the data as well as interpreting and explaining corresponding model predictions. A method is presented for representing a general multivariate function as a tree of simpler functions. This tree exposes the global internal structure of the function by uncovering and describing the combined joint influences of subsets of its input variables. Given the inputs and corresponding function values, a function tree is constructed that can be used to rapidly identify and compute all of the function's main and interaction effects up to high order. Interaction effects involving up to four variables are graphically visualized.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#23545;&#25239;&#35757;&#32451;&#19979;&#30340;&#40065;&#26834;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#27867;&#21270;&#29702;&#35770;&#65292;&#26377;&#26395;&#26497;&#22823;&#22320;&#25512;&#21160;NAS&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.13134</link><description>&lt;p&gt;
&#38024;&#23545;&#23545;&#25239;&#35757;&#32451;&#30340;&#40065;&#26834;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65306;&#22522;&#20934;&#12289;&#29702;&#35770;&#21450;&#20854;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Robust NAS under adversarial training: benchmark, theory, and beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13134
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#23545;&#25239;&#35757;&#32451;&#19979;&#30340;&#40065;&#26834;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#27867;&#21270;&#29702;&#35770;&#65292;&#26377;&#26395;&#26497;&#22823;&#22320;&#25512;&#21160;NAS&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#30340;&#21457;&#23637;&#24378;&#35843;&#32771;&#34385;&#38024;&#23545;&#24694;&#24847;&#25968;&#25454;&#30340;&#40065;&#26834;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#25628;&#32034;&#36825;&#20123;&#40065;&#26834;&#32467;&#26500;&#26102;&#65292;&#22312;&#32771;&#34385;&#23545;&#25239;&#35757;&#32451;&#26102;&#23384;&#22312;&#30528;&#26126;&#26174;&#30340;&#32570;&#20047;&#22522;&#20934;&#35780;&#20272;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#20570;&#20986;&#21452;&#37325;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;NAS-Bench-201&#25628;&#32034;&#31354;&#38388;&#30340;&#24191;&#27867;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#32463;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#32593;&#32476;&#30340;&#24178;&#20928;&#31934;&#24230;&#21644;&#40065;&#26834;&#31934;&#24230;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#24037;&#20855;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#23545;&#25239;&#35757;&#32451;&#19979;&#25628;&#32034;&#32467;&#26500;&#30340;&#27867;&#21270;&#29702;&#35770;&#65292;&#20197;&#24178;&#20928;&#31934;&#24230;&#21644;&#40065;&#26834;&#31934;&#24230;&#20316;&#20026;&#32771;&#37327;&#12290;&#25105;&#20204;&#22362;&#20449;&#25105;&#20204;&#30340;&#22522;&#20934;&#21644;&#29702;&#35770;&#35265;&#35299;&#23558;&#26497;&#22823;&#22320;&#26377;&#30410;&#20110;NAS&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13134v1 Announce Type: new  Abstract: Recent developments in neural architecture search (NAS) emphasize the significance of considering robust architectures against malicious data. However, there is a notable absence of benchmark evaluations and theoretical guarantees for searching these robust architectures, especially when adversarial training is considered. In this work, we aim to address these two challenges, making twofold contributions. First, we release a comprehensive data set that encompasses both clean accuracy and robust accuracy for a vast array of adversarially trained networks from the NAS-Bench-201 search space on image datasets. Then, leveraging the neural tangent kernel (NTK) tool from deep learning theory, we establish a generalization theory for searching architecture in terms of clean accuracy and robust accuracy under multi-objective adversarial training. We firmly believe that our benchmark and theoretical insights will significantly benefit the NAS com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20803;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#26032;&#22411;&#27169;&#24577;&#20998;&#26512;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#31232;&#30095;&#26102;&#38388;&#19981;&#35268;&#21017;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.13118</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20803;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#23545;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#27169;&#24577;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Modal Analysis of Spatiotemporal Data via Multivariate Gaussian Process Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13118
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20803;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#26032;&#22411;&#27169;&#24577;&#20998;&#26512;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#31232;&#30095;&#26102;&#38388;&#19981;&#35268;&#21017;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#24577;&#20998;&#26512;&#24050;&#25104;&#20026;&#29702;&#35299;&#22797;&#26434;&#27969;&#20307;&#30340;&#19968;&#31181;&#37325;&#35201;&#24037;&#20855;&#12290;&#20256;&#32479;&#30340;&#27169;&#24577;&#20998;&#26512;&#26041;&#27861;&#65292;&#22914;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;DMD&#65289;&#21644;&#35889;Proper Orthogonal Decomposition&#65288;SPOD&#65289;&#65292;&#20381;&#36182;&#20110;&#22312;&#26102;&#38388;&#19978;&#23450;&#26399;&#21462;&#26679;&#30340;&#20805;&#20998;&#25968;&#25454;&#37327;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#38656;&#35201;&#22788;&#29702;&#31232;&#30095;&#30340;&#26102;&#38388;&#19981;&#35268;&#21017;&#25968;&#25454;&#65292;&#20363;&#22914;&#30001;&#20110;&#23454;&#39564;&#27979;&#37327;&#21644;&#20223;&#30495;&#31639;&#27861;&#12290;&#20026;&#20102;&#20811;&#26381;&#25968;&#25454;&#31232;&#32570;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20803;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;MVGPR&#65289;&#30340;&#26032;&#22411;&#27169;&#24577;&#20998;&#26512;&#25216;&#26415;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#30340;&#35282;&#24230;&#24314;&#31435;&#20102;MVGPR&#19982;&#29616;&#26377;&#27169;&#24577;&#20998;&#26512;&#25216;&#26415;DMD&#21644;SPOD&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25509;&#19979;&#26469;&#65292;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;MVGPR&#30340;&#27169;&#24577;&#20998;&#26512;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#21069;&#36848;&#30340;&#38480;&#21046;&#12290;MVGPR&#30340;&#21151;&#33021;&#26159;&#36890;&#36807;&#20854;&#35880;&#24910;&#30340;&#21028;&#26029;&#33021;&#21147;&#36171;&#20104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13118v1 Announce Type: cross  Abstract: Modal analysis has become an essential tool to understand the coherent structure of complex flows. The classical modal analysis methods, such as dynamic mode decomposition (DMD) and spectral proper orthogonal decomposition (SPOD), rely on a sufficient amount of data that is regularly sampled in time. However, often one needs to deal with sparse temporally irregular data, e.g., due to experimental measurements and simulation algorithm. To overcome the limitations of data scarcity and irregular sampling, we propose a novel modal analysis technique using multi-variate Gaussian process regression (MVGPR). We first establish the connection between MVGPR and the existing modal analysis techniques, DMD and SPOD, from a linear system identification perspective. Next, leveraging this connection, we develop a MVGPR-based modal analysis technique that addresses the aforementioned limitations. The capability of MVGPR is endowed by its judiciously 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20248;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19968;&#27493;&#20013;&#23398;&#20064;&#23454;&#29616;&#20108;&#27425;&#25104;&#26412;&#19979;&#30340;&#30452;&#32447; OT &#20301;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.13117</link><description>&lt;p&gt;
&#26368;&#20248;&#27969;&#21305;&#37197;&#65306;&#22312;&#19968;&#27493;&#20013;&#23398;&#20064;&#30452;&#32447;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Optimal Flow Matching: Learning Straight Trajectories in Just One Step
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13117
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20248;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19968;&#27493;&#20013;&#23398;&#20064;&#23454;&#29616;&#20108;&#27425;&#25104;&#26412;&#19979;&#30340;&#30452;&#32447; OT &#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#27969;&#21305;&#37197;&#26041;&#27861;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#24471;&#21040;&#20102;&#34028;&#21187;&#21457;&#23637;&#12290;&#31038;&#21306;&#36861;&#27714;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#23646;&#24615;&#26159;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#30452;&#32447;&#36712;&#36857;&#30340;&#27969;&#65292;&#36825;&#20123;&#36712;&#36857;&#23454;&#29616;&#20102;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#32622;&#25442;&#12290;&#30452;&#32447;&#24615;&#23545;&#20110;&#24555;&#36895;&#38598;&#25104;&#23398;&#20064;&#27969;&#30340;&#36335;&#24452;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#27969;&#30452;&#32447;&#21270;&#26041;&#27861;&#37117;&#22522;&#20110;&#38750;&#24179;&#20961;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#31215;&#32047;&#35823;&#24046;&#25110;&#21033;&#29992;&#21551;&#21457;&#24335;&#23567;&#25209;&#37327;OT&#36817;&#20284;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20248;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#20165;&#36890;&#36807;&#19968;&#27425;&#27969;&#21305;&#37197;&#27493;&#39588;&#21363;&#21487;&#20026;&#20108;&#27425;&#25104;&#26412;&#24674;&#22797;&#30452;&#32447;OT&#32622;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13117v1 Announce Type: cross  Abstract: Over the several recent years, there has been a boom in development of flow matching methods for generative modeling. One intriguing property pursued by the community is the ability to learn flows with straight trajectories which realize the optimal transport (OT) displacements. Straightness is crucial for fast integration of the learned flow's paths. Unfortunately, most existing flow straightening methods are based on non-trivial iterative procedures which accumulate the error during training or exploit heuristic minibatch OT approximations. To address this issue, we develop a novel optimal flow matching approach which recovers the straight OT displacement for the quadratic cost in just one flow matching step.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;</title><link>https://arxiv.org/abs/2403.13041</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#31169;&#23494;&#39044;&#22788;&#29702;&#30340;&#21487;&#35777;&#26126;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Provable Privacy with Non-Private Pre-Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20998;&#26512;&#24046;&#20998;&#31169;&#23494;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26102;&#65292;&#36890;&#24120;&#20250;&#24573;&#30053;&#25968;&#25454;&#30456;&#20851;&#30340;&#39044;&#22788;&#29702;&#30340;&#28508;&#22312;&#38544;&#31169;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#30001;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#26032;&#30340;&#25216;&#26415;&#27010;&#24565;&#24314;&#31435;&#20102;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;&#65306;&#19968;&#31181;&#31216;&#20026;&#24179;&#28369;DP&#30340;DP&#21464;&#20307;&#20197;&#21450;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13041v1 Announce Type: cross  Abstract: When analysing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27700;&#21360;LLMs&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#31639;&#27861;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22833;&#30495;&#21644;&#26816;&#27979;&#33021;&#21147;&#20043;&#38388;&#30340;&#26368;&#20248;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.13027</link><description>&lt;p&gt;
&#26356;&#22909;&#22320;&#32479;&#35745;&#29702;&#35299;&#27700;&#21360;LLMs
&lt;/p&gt;
&lt;p&gt;
Towards Better Statistical Understanding of Watermarking LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27700;&#21360;LLMs&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#31639;&#27861;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22833;&#30495;&#21644;&#26816;&#27979;&#33021;&#21147;&#20043;&#38388;&#30340;&#26368;&#20248;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27700;&#21360;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#27169;&#22411;&#22833;&#30495;&#21644;&#26816;&#27979;&#33021;&#21147;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#23558;&#20854;&#26500;&#24314;&#20026;&#22522;&#20110;Kirchenbauer&#31561;&#20154;&#65288;2023a&#65289;&#30340;&#32511;-&#32418;&#31639;&#27861;&#30340;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#20139;&#26377;&#33391;&#22909;&#30340;&#20998;&#26512;&#24615;&#36136;&#65292;&#36825;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#24182;&#21551;&#21457;&#27700;&#21360;&#36807;&#31243;&#30340;&#31639;&#27861;&#35774;&#35745;&#12290;&#25105;&#20204;&#26681;&#25454;&#36825;&#19968;&#20248;&#21270;&#20844;&#24335;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#21452;&#26799;&#24230;&#19978;&#21319;&#27700;&#21360;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#27169;&#22411;&#22833;&#30495;&#21644;&#26816;&#27979;&#33021;&#21147;&#20043;&#38388;&#30340;&#28176;&#36817;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#12290;&#36825;&#26679;&#30340;&#32467;&#26524;&#20445;&#35777;&#20102;&#24179;&#22343;&#22686;&#21152;&#30340;&#32511;&#33394;&#21015;&#34920;&#27010;&#29575;&#65292;&#20174;&#32780;&#26126;&#30830;&#25552;&#39640;&#20102;&#26816;&#27979;&#33021;&#21147;&#65288;&#19982;&#20808;&#21069;&#32467;&#26524;&#30456;&#27604;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27700;&#21360;&#38382;&#39064;&#30340;&#27169;&#22411;&#22833;&#30495;&#24230;&#37327;&#30340;&#36873;&#25321;&#36827;&#34892;&#20102;&#31995;&#32479;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13027v1 Announce Type: cross  Abstract: In this paper, we study the problem of watermarking large language models (LLMs). We consider the trade-off between model distortion and detection ability and formulate it as a constrained optimization problem based on the green-red algorithm of Kirchenbauer et al. (2023a). We show that the optimal solution to the optimization problem enjoys a nice analytical property which provides a better understanding and inspires the algorithm design for the watermarking process. We develop an online dual gradient ascent watermarking algorithm in light of this optimization formulation and prove its asymptotic Pareto optimality between model distortion and detection ability. Such a result guarantees an averaged increased green list probability and henceforth detection ability explicitly (in contrast to previous results). Moreover, we provide a systematic discussion on the choice of the model distortion metrics for the watermarking problem. We justi
&lt;/p&gt;</description></item><item><title>&#23558;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#35270;&#20026;&#21477;&#23376;&#24182;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#65292;&#35777;&#23454;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.12984</link><description>&lt;p&gt;
&#24403;SMILES&#25317;&#26377;&#35821;&#35328;&#65306;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#23545;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12984
&lt;/p&gt;
&lt;p&gt;
&#23558;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#35270;&#20026;&#21477;&#23376;&#24182;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#65292;&#35777;&#23454;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#21270;&#23398;&#32467;&#26500;&#65292;&#22914;&#33647;&#29289;&#65292;&#36890;&#24120;&#30001;SMILES&#23383;&#31526;&#20018;&#26469;&#23450;&#20041;&#65292;&#20316;&#20026;&#20998;&#23376;&#21644;&#38190;&#30340;&#24207;&#21015;&#12290;&#36825;&#20123;SMILES&#23383;&#31526;&#20018;&#22312;&#19981;&#21516;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20851;&#30740;&#31350;&#21644;&#34920;&#31034;&#24037;&#20316;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25670;&#33073;&#22797;&#26434;&#30340;&#34920;&#31034;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#26524;&#25105;&#20204;&#23558;&#33647;&#29289;SMILES&#35270;&#20026;&#24120;&#35268;&#21477;&#23376;&#65292;&#24182;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#20197;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#20250;&#24590;&#26679;&#65311;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#33719;&#24471;&#20102;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#25968;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27599;&#20010;&#21407;&#23376;&#21644;&#38190;&#35270;&#20026;&#21477;&#23376;&#32452;&#20214;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#22522;&#26412;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23545;&#33647;&#29289;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#34920;&#26126;&#22797;&#26434;&#30340;&#38382;&#39064;&#20063;&#21487;&#20197;&#29992;&#26356;&#31616;&#21333;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#25214;&#21040;&#65306;https://github.com/azminewasi/Drug-Classification-NLP&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12984v1 Announce Type: cross  Abstract: Complex chemical structures, like drugs, are usually defined by SMILES strings as a sequence of molecules and bonds. These SMILES strings are used in different complex machine learning-based drug-related research and representation works. Escaping from complex representation, in this work, we pose a single question: What if we treat drug SMILES as conventional sentences and engage in text classification for drug classification? Our experiments affirm the possibility with very competitive scores. The study explores the notion of viewing each atom and bond as sentence components, employing basic NLP methods to categorize drug types, proving that complex problems can also be solved with simpler perspectives. The data and code are available here: https://github.com/azminewasi/Drug-Classification-NLP.
&lt;/p&gt;</description></item><item><title>&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#23384;&#22312;&#25361;&#25112;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#24494;&#20998;&#26041;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#23545;&#24418;&#24577;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#29575;&#30340;&#29702;&#35770;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.12975</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#20123;&#29702;&#35770;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Training morphological neural networks with gradient descent: some theoretical insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12975
&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#23384;&#22312;&#25361;&#25112;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#24494;&#20998;&#26041;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#23545;&#24418;&#24577;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#29575;&#30340;&#29702;&#35770;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#25110;&#23618;&#21487;&#20197;&#25104;&#20026;&#25552;&#21319;&#25968;&#23398;&#24418;&#24577;&#23398;&#36827;&#23637;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26080;&#35770;&#26159;&#22312;&#29702;&#35770;&#26041;&#38754;&#65292;&#22914;&#23436;&#25972;&#26684;&#31639;&#23376;&#30340;&#34920;&#31034;&#65292;&#36824;&#26159;&#22312;&#22270;&#20687;&#22788;&#29702;&#27969;&#31243;&#30340;&#24320;&#21457;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#26550;&#26500;&#21253;&#21547;&#22810;&#23618;&#24418;&#24577;&#23398;&#26102;&#65292;&#33267;&#23569;&#22312;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#31639;&#27861;&#30340;&#27969;&#34892;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;&#36825;&#20123;&#32593;&#32476;&#24456;&#38590;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#24494;&#20998;&#26041;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#24212;&#29992;&#20110;&#24418;&#24577;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#32771;&#34385;&#21040;Bouligand&#23548;&#25968;&#30340;&#38750;&#20809;&#28369;&#20248;&#21270;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35265;&#35299;&#21644;&#39318;&#20010;&#29702;&#35770;&#25351;&#21335;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12975v1 Announce Type: cross  Abstract: Morphological neural networks, or layers, can be a powerful tool to boost the progress in mathematical morphology, either on theoretical aspects such as the representation of complete lattice operators, or in the development of image processing pipelines. However, these architectures turn out to be difficult to train when they count more than a few morphological layers, at least within popular machine learning frameworks which use gradient descent based optimization algorithms. In this paper we investigate the potential and limitations of differentiation based approaches and back-propagation applied to morphological networks, in light of the non-smooth optimization concept of Bouligand derivative. We provide insights and first theoretical guidelines, in particular regarding initialization and learning rates.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#36827;&#34892;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#20248;&#21270;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#31361;&#26174;&#20854;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#21644;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12166</link><description>&lt;p&gt;
&#23569;&#25968;&#20010;&#20307;&#30340;&#21147;&#37327;&#65306;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#21152;&#36895;&#21644;&#20248;&#21270;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
The Power of Few: Accelerating and Enhancing Data Reweighting with Coreset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12166
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#36827;&#34892;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#20248;&#21270;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#31361;&#26174;&#20854;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#21644;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19981;&#26029;&#21457;&#23637;&#65292;&#36235;&#21183;&#26159;&#25910;&#38598;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#23558;&#35745;&#31639;&#25104;&#26412;&#25552;&#39640;&#21040;&#19981;&#21487;&#25345;&#32493;&#30340;&#27700;&#24179;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24494;&#22937;&#30340;&#24179;&#34913;&#65292;&#36825;&#26159;&#35813;&#39046;&#22495;&#20013;&#19968;&#30452;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#24515;&#23376;&#38598;&#36873;&#25321;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#30340;&#26032;&#26041;&#27861;&#65292;&#26377;&#25928;&#20248;&#21270;&#20102;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110; strategically selected coreset&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#34920;&#31034;&#65292;&#22240;&#20026;&#23427;&#26377;&#25928;&#22320;&#26368;&#23567;&#21270;&#20102;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#37325;&#26032;&#26657;&#20934;&#30340;&#26435;&#37325;&#34987;&#26144;&#23556;&#22238;&#24182;&#20256;&#25773;&#21040;&#25972;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#23427;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#21644;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12166v1 Announce Type: new  Abstract: As machine learning tasks continue to evolve, the trend has been to gather larger datasets and train increasingly larger models. While this has led to advancements in accuracy, it has also escalated computational costs to unsustainable levels. Addressing this, our work aims to strike a delicate balance between computational efficiency and model accuracy, a persisting challenge in the field. We introduce a novel method that employs core subset selection for reweighting, effectively optimizing both computational time and model performance. By focusing on a strategically selected coreset, our approach offers a robust representation, as it efficiently minimizes the influence of outliers. The re-calibrated weights are then mapped back to and propagated across the entire dataset. Our experimental results substantiate the effectiveness of this approach, underscoring its potential as a scalable and precise solution for model training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#26469;&#23454;&#29616;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.12143</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#31561;&#21464;&#34920;&#31034;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Learning Equivariant Representations of Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#26469;&#23454;&#29616;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35832;&#22914;&#20998;&#31867;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#39044;&#27979;&#27867;&#21270;&#38169;&#35823;&#31561;&#39046;&#22495;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22266;&#26377;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#22797;&#26434;&#30340;&#26435;&#37325;&#20849;&#20139;&#27169;&#24335;&#26469;&#23454;&#29616;&#31561;&#21464;&#24615;&#65292;&#21516;&#26102;&#24573;&#30053;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#24378;&#22823;&#30340;&#20445;&#30041;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#23545;&#20855;&#26377;&#22810;&#26679;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#20998;&#31867;&#21644;&#32534;&#36753;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#39044;&#27979;&#27867;&#21270;&#38169;&#35823;&#31561;&#22810;&#31181;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12143v1 Announce Type: cross  Abstract: Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalizati
&lt;/p&gt;</description></item><item><title>&#22312;&#20449;&#21495;&#21152;&#38543;&#26426;&#39640;&#26031;&#22122;&#22768;&#30697;&#38453;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#25193;&#23637;&#20102;&#23545;&#22855;&#24322;&#21521;&#37327;&#21644;&#22855;&#24322;&#23376;&#31354;&#38388;&#25200;&#21160;&#30340;Wedin-Davis-Kahan&#23450;&#29702;&#65292;&#33719;&#24471;&#20102;&#22855;&#24322;&#21521;&#37327;&#21644;&#22855;&#24322;&#23376;&#31354;&#38388;&#30340;&#32454;&#31890;&#24230;&#20998;&#26512;&#32467;&#26524;&#65292;&#24182;&#25506;&#32034;&#20102;&#19982;&#22855;&#24322;&#21521;&#37327;&#30456;&#20851;&#30340;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#20989;&#25968;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#36825;&#20123;&#21457;&#29616;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#23376;&#30697;&#38453;&#23450;&#20301;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.09170</link><description>&lt;p&gt;
&#38543;&#26426;&#25200;&#21160;&#19979;&#22855;&#24322;&#23376;&#31354;&#38388;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of singular subspaces under random perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09170
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#21495;&#21152;&#38543;&#26426;&#39640;&#26031;&#22122;&#22768;&#30697;&#38453;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#25193;&#23637;&#20102;&#23545;&#22855;&#24322;&#21521;&#37327;&#21644;&#22855;&#24322;&#23376;&#31354;&#38388;&#25200;&#21160;&#30340;Wedin-Davis-Kahan&#23450;&#29702;&#65292;&#33719;&#24471;&#20102;&#22855;&#24322;&#21521;&#37327;&#21644;&#22855;&#24322;&#23376;&#31354;&#38388;&#30340;&#32454;&#31890;&#24230;&#20998;&#26512;&#32467;&#26524;&#65292;&#24182;&#25506;&#32034;&#20102;&#19982;&#22855;&#24322;&#21521;&#37327;&#30456;&#20851;&#30340;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#20989;&#25968;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#36825;&#20123;&#21457;&#29616;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#23376;&#30697;&#38453;&#23450;&#20301;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#20449;&#21495;&#21152;&#38543;&#26426;&#39640;&#26031;&#22122;&#22768;&#30697;&#38453;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#22855;&#24322;&#21521;&#37327;&#21644;&#22855;&#24322;&#23376;&#31354;&#38388;&#30340;&#25200;&#21160;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#20551;&#35774;&#19968;&#20010;&#20302;&#31209;&#20449;&#21495;&#30697;&#38453;&#65292;&#25105;&#20204;&#20197;&#19968;&#31181;&#23436;&#20840;&#27867;&#21270;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;Wedin-Davis-Kahan&#23450;&#29702;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#37193;&#19981;&#21464;&#30697;&#38453;&#33539;&#25968;&#65292;&#25193;&#23637;&#20102;O'Rourke&#12289;Vu&#21644;&#20316;&#32773;&#20043;&#21069;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#33719;&#24471;&#20102;&#32454;&#31890;&#24230;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#21253;&#25324;&#22855;&#24322;&#21521;&#37327;&#30340;$\ell_\infty$&#20998;&#26512;&#65292;&#22855;&#24322;&#23376;&#31354;&#38388;&#30340;$\ell_{2,\infty}$&#20998;&#26512;&#65292;&#20197;&#21450;&#19982;&#22855;&#24322;&#21521;&#37327;&#30456;&#20851;&#30340;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#20989;&#25968;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#21457;&#29616;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#23376;&#30697;&#38453;&#23450;&#20301;&#38382;&#39064;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09170v1 Announce Type: cross  Abstract: We present a comprehensive analysis of singular vector and singular subspace perturbations in the context of the signal plus random Gaussian noise matrix model. Assuming a low-rank signal matrix, we extend the Wedin-Davis-Kahan theorem in a fully generalized manner, applicable to any unitarily invariant matrix norm, extending previous results of O'Rourke, Vu and the author. We also obtain the fine-grained results, which encompass the $\ell_\infty$ analysis of singular vectors, the $\ell_{2, \infty}$ analysis of singular subspaces, as well as the exploration of linear and bilinear functions related to the singular vectors. Moreover, we explore the practical implications of these findings, in the context of the Gaussian mixture model and the submatrix localization problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#22240;&#26524;&#26694;&#26550;&#65292;&#34701;&#20837;&#29359;&#32618;&#34892;&#20026;&#65292;&#29992;&#20110;&#35780;&#20272;&#25191;&#27861;&#31995;&#32479;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#65292;&#20197;&#35299;&#20915;&#20197;&#24448;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#23545;&#20559;&#35265;&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#30830;&#23450;&#20027;&#35201;&#20559;&#35265;&#26469;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.14959</link><description>&lt;p&gt;
&#35780;&#20272;&#25191;&#27861;&#31995;&#32479;&#20013;&#31181;&#26063;&#20559;&#35265;&#30340;&#22240;&#26524;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Causal Framework to Evaluate Racial Bias in Law Enforcement Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#22240;&#26524;&#26694;&#26550;&#65292;&#34701;&#20837;&#29359;&#32618;&#34892;&#20026;&#65292;&#29992;&#20110;&#35780;&#20272;&#25191;&#27861;&#31995;&#32479;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#65292;&#20197;&#35299;&#20915;&#20197;&#24448;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#23545;&#20559;&#35265;&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#30830;&#23450;&#20027;&#35201;&#20559;&#35265;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#35780;&#20272;&#25191;&#27861;&#31995;&#32479;&#20013;&#31181;&#26063;&#35825;&#21457;&#30340;&#20559;&#35265;&#12290; &#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#24050;&#32463;&#35752;&#35770;&#20102;&#22312;&#35686;&#27665;&#20114;&#21160;&#32972;&#26223;&#19979;&#20351;&#29992;&#35686;&#23519;&#20572;&#36710;&#25968;&#25454;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#12290; &#39318;&#20808;&#65292;&#21482;&#26377;&#22312;&#23558;&#30495;&#23454;&#29359;&#32618;&#34892;&#20026;&#32771;&#34385;&#22312;&#20869;&#26102;&#65292;&#20559;&#35265;&#25165;&#33021;&#24471;&#21040;&#24688;&#24403;&#37327;&#21270;&#65292;&#20294;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#32570;&#20047;&#12290; &#31532;&#20108;&#65292;&#25191;&#27861;&#31995;&#32479;&#26159;&#22810;&#38454;&#27573;&#30340;&#65292;&#22240;&#27492;&#37325;&#35201;&#30340;&#26159;&#22312;&#8220;&#22240;&#26524;&#20132;&#20114;&#38142;&#8221;&#20013;&#23396;&#31435;&#20986;&#20559;&#35265;&#30340;&#30495;&#27491;&#26469;&#28304;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#26368;&#32456;&#32467;&#26524;&#65307; &#36825;&#26377;&#21161;&#20110;&#24341;&#23548;&#25913;&#38761;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21253;&#21547;&#29359;&#32618;&#34892;&#20026;&#30340;&#22810;&#38454;&#27573;&#22240;&#26524;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290; &#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#29305;&#24449;&#21644;&#19968;&#20010;&#30456;&#20851;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#35780;&#20272;(a)&#20219;&#20309;&#24418;&#24335;&#30340;&#31181;&#26063;&#20559;&#35265;&#30340;&#23384;&#22312;&#65292;&#20197;&#21450;(b)&#22914;&#26524;&#26159;&#36825;&#26679;&#65292;&#36825;&#31181;&#20559;&#35265;&#30340;&#20027;&#35201;&#26469;&#28304;&#26159;&#31181;&#26063;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14959v1 Announce Type: cross  Abstract: We are interested in developing a data-driven method to evaluate race-induced biases in law enforcement systems. While the recent works have addressed this question in the context of police-civilian interactions using police stop data, they have two key limitations. First, bias can only be properly quantified if true criminality is accounted for in addition to race, but it is absent in prior works. Second, law enforcement systems are multi-stage and hence it is important to isolate the true source of bias within the "causal chain of interactions" rather than simply focusing on the end outcome; this can help guide reforms. In this work, we address these challenges by presenting a multi-stage causal framework incorporating criminality. We provide a theoretical characterization and an associated data-driven method to evaluate (a) the presence of any form of racial bias, and (b) if so, the primary source of such a bias in terms of race and
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25193;&#25955;&#27169;&#22411;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#26377;&#25928;&#22320;&#20174;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03008</link><description>&lt;p&gt;
&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Diffusive Gibbs Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03008
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25193;&#25955;&#27169;&#22411;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#26377;&#25928;&#22320;&#20174;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#28151;&#21512;&#19981;&#36275;&#26041;&#38754;&#23384;&#22312;&#30528;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#21513;&#24067;&#26031;&#37319;&#26679;&#65288;DiGS&#65289;&#65292;&#29992;&#20110;&#26377;&#25928;&#37319;&#26679;&#20855;&#26377;&#36828;&#31243;&#21644;&#26029;&#24320;&#27169;&#24577;&#29305;&#24449;&#30340;&#20998;&#24067;&#12290;DiGS&#38598;&#25104;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#21033;&#29992;&#39640;&#26031;&#21367;&#31215;&#21019;&#24314;&#19968;&#20010;&#36741;&#21161;&#22122;&#22768;&#20998;&#24067;&#65292;&#20197;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#36830;&#25509;&#23396;&#31435;&#30340;&#27169;&#24577;&#65292;&#24182;&#24212;&#29992;&#21513;&#24067;&#26031;&#37319;&#26679;&#20174;&#20004;&#20010;&#31354;&#38388;&#20013;&#20132;&#26367;&#25277;&#21462;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37319;&#26679;&#22810;&#27169;&#24577;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#24182;&#34892;&#28201;&#24230;&#27861;&#31561;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#37319;&#26679;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#12289;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. Our approach exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering. We demonstrate that our sampler attains substantially improved results across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#30340;&#28151;&#21512;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#20107;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#21644;&#25805;&#20316;&#26102;&#38388;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00776</link><description>&lt;p&gt;
&#28151;&#21512;&#37327;&#23376;&#35270;&#35273;&#36716;&#25442;&#22120;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hybrid Quantum Vision Transformers for Event Classification in High Energy Physics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#30340;&#28151;&#21512;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#20107;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#21644;&#25805;&#20316;&#26102;&#38388;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#20013;&#37117;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#38543;&#30528;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#22522;&#20110;&#37327;&#23376;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#21487;&#33021;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#21644;&#25805;&#20316;&#26102;&#38388;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#23578;&#19981;&#33021;&#25191;&#34892;&#39640;&#32500;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#26368;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#37327;&#23376;&#28151;&#21512;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#20998;&#31867;&#38382;&#39064;&#65288;&#21306;&#20998;&#30005;&#23376;&#21644;&#20809;&#23376;&#22312;&#30005;&#30913;&#37327;&#33021;&#22120;&#20013;&#65289;&#12290;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#32463;&#20856;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#26550;&#26500;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28151;&#21512;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models based on vision transformer architectures are considered state-of-the-art when it comes to image classification tasks. However, they require extensive computational resources both for training and deployment. The problem is exacerbated as the amount and complexity of the data increases. Quantum-based vision transformer models could potentially alleviate this issue by reducing the training and operating time while maintaining the same predictive power. Although current quantum computers are not yet able to perform high-dimensional tasks yet, they do offer one of the most efficient solutions for the future. In this work, we construct several variations of a quantum hybrid vision transformer for a classification problem in high energy physics (distinguishing photons and electrons in the electromagnetic calorimeter). We test them against classical vision transformer architectures. Our findings indicate that the hybrid models can achieve comparable performance to their classical anal
&lt;/p&gt;</description></item><item><title>&#24120;&#29992;&#30340;&#20855;&#26377;&#26080;&#33618;&#21407;&#35777;&#26126;&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#22312;&#36827;&#34892;&#21021;&#22987;&#25968;&#25454;&#37319;&#38598;&#38454;&#27573;&#20174;&#37327;&#23376;&#35774;&#22791;&#20013;&#25910;&#38598;&#19968;&#20123;&#32463;&#20856;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#32463;&#20856;&#27169;&#25311;</title><link>https://arxiv.org/abs/2312.09121</link><description>&lt;p&gt;
&#35777;&#23454;&#26080;&#33618;&#21407;&#23384;&#22312;&#26159;&#21542;&#24847;&#21619;&#30528;&#32463;&#20856;&#27169;&#25311;&#65311;&#25110;&#32773;&#65292;&#20026;&#20160;&#20040;&#25105;&#20204;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#21464;&#20998;&#37327;&#23376;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Does provable absence of barren plateaus imply classical simulability? Or, why we need to rethink variational quantum computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09121
&lt;/p&gt;
&lt;p&gt;
&#24120;&#29992;&#30340;&#20855;&#26377;&#26080;&#33618;&#21407;&#35777;&#26126;&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#22312;&#36827;&#34892;&#21021;&#22987;&#25968;&#25454;&#37319;&#38598;&#38454;&#27573;&#20174;&#37327;&#23376;&#35774;&#22791;&#20013;&#25910;&#38598;&#19968;&#20123;&#32463;&#20856;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#32463;&#20856;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#33618;&#21407;&#29616;&#35937;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290; &#22312;&#36825;&#31687;&#35266;&#28857;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#38754;&#23545;&#20102;&#36234;&#26469;&#36234;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35768;&#22810;&#20154;&#26263;&#31034;&#20294;&#23578;&#26410;&#26126;&#30830;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;&#20801;&#35768;&#36991;&#20813;&#33618;&#21407;&#30340;&#32467;&#26500;&#26159;&#21542;&#20063;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#26377;&#25928;&#22320;&#32463;&#20856;&#27169;&#25311;&#25439;&#22833;&#65311; &#25105;&#20204;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#24120;&#29992;&#30340;&#20855;&#26377;&#26080;&#33618;&#21407;&#35777;&#26126;&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#22312;&#36827;&#34892;&#21021;&#22987;&#25968;&#25454;&#37319;&#38598;&#38454;&#27573;&#20174;&#37327;&#23376;&#35774;&#22791;&#20013;&#25910;&#38598;&#19968;&#20123;&#32463;&#20856;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#32463;&#20856;&#27169;&#25311;&#12290; &#36825;&#26159;&#22240;&#20026;&#33618;&#21407;&#29616;&#35937;&#26159;&#30001;&#32500;&#24230;&#30340;&#35781;&#21650;&#23548;&#33268;&#30340;&#65292;&#32780;&#30446;&#21069;&#35299;&#20915;&#38382;&#39064;&#30340;&#26041;&#27861;&#26368;&#32456;&#23558;&#38382;&#39064;&#32534;&#30721;&#21040;&#19968;&#20123;&#23567;&#30340;&#12289;&#32463;&#20856;&#21487;&#27169;&#25311;&#30340;&#23376;&#31354;&#38388;&#20013;&#12290; &#22240;&#27492;&#65292;&#23613;&#31649;&#24378;&#35843;&#37327;&#23376;&#35745;&#31639;&#21487;&#20197;&#26159;&#25910;&#38598;&#25968;&#25454;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#24605;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09121v2 Announce Type: replace-cross  Abstract: A large amount of effort has recently been put into understanding the barren plateau phenomenon. In this perspective article, we face the increasingly loud elephant in the room and ask a question that has been hinted at by many but not explicitly addressed: Can the structure that allows one to avoid barren plateaus also be leveraged to efficiently simulate the loss classically? We present strong evidence that commonly used models with provable absence of barren plateaus are also classically simulable, provided that one can collect some classical data from quantum devices during an initial data acquisition phase. This follows from the observation that barren plateaus result from a curse of dimensionality, and that current approaches for solving them end up encoding the problem into some small, classically simulable, subspaces. Thus, while stressing quantum computers can be essential for collecting data, our analysis sheds seriou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25193;&#23637;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#33267;&#38477;&#27700;&#36229;&#20998;&#36776;&#29575;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30830;&#23450;&#24615;&#38477;&#23610;&#24230;&#22120;&#21644;&#26242;&#26102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#22122;&#22768;&#29305;&#24449;&#21644;&#39640;&#39057;&#29575;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.06071</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#31354;&#35270;&#39057;&#25193;&#25955;&#30340;&#38477;&#27700;&#38477;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Precipitation Downscaling with Spatiotemporal Video Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06071
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#23637;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#33267;&#38477;&#27700;&#36229;&#20998;&#36776;&#29575;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30830;&#23450;&#24615;&#38477;&#23610;&#24230;&#22120;&#21644;&#26242;&#26102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#22122;&#22768;&#29305;&#24449;&#21644;&#39640;&#39057;&#29575;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27668;&#20505;&#31185;&#23398;&#21644;&#27668;&#35937;&#23398;&#39046;&#22495;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#23616;&#37096;&#38477;&#27700;&#65288;&#38632;&#38634;&#65289;&#39044;&#27979;&#21463;&#21040;&#22522;&#20110;&#27169;&#25311;&#26041;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#12290;&#32479;&#35745;&#38477;&#23610;&#24230;&#65292;&#25110;&#32773;&#31216;&#20026;&#36229;&#20998;&#36776;&#29575;&#65292;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20854;&#20013;&#20302;&#20998;&#36776;&#29575;&#39044;&#27979;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#24471;&#21040;&#25913;&#36827;&#12290;&#19982;&#20256;&#32479;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19981;&#21516;&#65292;&#22825;&#27668;&#21644;&#27668;&#20505;&#24212;&#29992;&#38656;&#35201;&#25429;&#25417;&#32473;&#23450;&#20302;&#20998;&#36776;&#29575;&#27169;&#24335;&#30340;&#39640;&#20998;&#36776;&#29575;&#30340;&#20934;&#30830;&#26465;&#20214;&#20998;&#24067;&#65292;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#38598;&#21512;&#24179;&#22343;&#21644;&#26497;&#31471;&#20107;&#20214;&#65288;&#22914;&#26292;&#38632;&#65289;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#26412;&#30740;&#31350;&#23558;&#26368;&#26032;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#38477;&#27700;&#36229;&#20998;&#36776;&#29575;&#65292;&#20351;&#29992;&#30830;&#23450;&#24615;&#38477;&#23610;&#24230;&#22120;&#65292;&#28982;&#21518;&#26159;&#26242;&#26102;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#22122;&#22768;&#29305;&#24449;&#21644;&#39640;&#39057;&#29575;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;FV3GFS&#36755;&#20986;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#24050;&#24314;&#31435;&#30340;&#22823;&#35268;&#27169;&#20840;&#29699;&#22823;&#27668;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06071v2 Announce Type: replace-cross  Abstract: In climate science and meteorology, high-resolution local precipitation (rain and snowfall) predictions are limited by the computational costs of simulation-based methods. Statistical downscaling, or super-resolution, is a common workaround where a low-resolution prediction is improved using statistical approaches. Unlike traditional computer vision tasks, weather and climate applications require capturing the accurate conditional distribution of high-resolution given low-resolution patterns to assure reliable ensemble averages and unbiased estimates of extreme events, such as heavy rain. This work extends recent video diffusion models to precipitation super-resolution, employing a deterministic downscaler followed by a temporally-conditioned diffusion model to capture noise characteristics and high-frequency patterns. We test our approach on FV3GFS output, an established large-scale global atmosphere model, and compare it agai
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#26032;&#35774;&#35745;&#32593;&#32476;&#23618;&#26469;&#20445;&#25345;&#26399;&#26395;&#30340;&#28608;&#27963;&#12289;&#26435;&#37325;&#21644;&#26356;&#26032;&#24133;&#24230;&#65292;&#28040;&#38500;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#28418;&#31227;&#21644;&#19981;&#24179;&#34913;&#65292;&#20174;&#32780;&#22312;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#19979;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32593;&#32476;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.02696</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Analyzing and Improving the Training Dynamics of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02696
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#35774;&#35745;&#32593;&#32476;&#23618;&#26469;&#20445;&#25345;&#26399;&#26395;&#30340;&#28608;&#27963;&#12289;&#26435;&#37325;&#21644;&#26356;&#26032;&#24133;&#24230;&#65292;&#28040;&#38500;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#28418;&#31227;&#21644;&#19981;&#24179;&#34913;&#65292;&#20174;&#32780;&#22312;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#19979;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32593;&#32476;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30446;&#21069;&#22312;&#25968;&#25454;&#39537;&#21160;&#22270;&#20687;&#21512;&#25104;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#20854;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26080;&#19982;&#20262;&#27604;&#30340;&#25193;&#23637;&#33021;&#21147;&#12290;&#26412;&#25991;&#22312;&#19981;&#25913;&#21464;&#20854;&#39640;&#32423;&#32467;&#26500;&#30340;&#21069;&#25552;&#19979;&#65292;&#35782;&#21035;&#24182;&#32416;&#27491;&#20102;&#27969;&#34892;&#30340;ADM&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#20013;&#23548;&#33268;&#19981;&#22343;&#21248;&#21644;&#20302;&#25928;&#35757;&#32451;&#30340;&#20960;&#20010;&#21407;&#22240;&#12290;&#35266;&#23519;&#21040;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32593;&#32476;&#28608;&#27963;&#21644;&#26435;&#37325;&#30340;&#19981;&#21463;&#25511;&#21046;&#30340;&#24133;&#24230;&#21464;&#21270;&#21644;&#19981;&#24179;&#34913;&#65292;&#25105;&#20204;&#37325;&#26032;&#35774;&#35745;&#20102;&#32593;&#32476;&#23618;&#20197;&#20445;&#25345;&#26399;&#26395;&#19978;&#30340;&#28608;&#27963;&#12289;&#26435;&#37325;&#21644;&#26356;&#26032;&#24133;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31995;&#32479;&#24212;&#29992;&#36825;&#19968;&#29702;&#24565;&#28040;&#38500;&#20102;&#35266;&#23519;&#21040;&#30340;&#28418;&#31227;&#21644;&#19981;&#24179;&#34913;&#65292;&#23548;&#33268;&#30456;&#24403;&#26356;&#22909;&#30340;&#32593;&#32476;&#22312;&#31561;&#25928;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#19979;&#12290;&#25105;&#20204;&#30340;&#20462;&#25913;&#23558;&#20043;&#21069;&#22312;ImageNet-512&#21512;&#25104;&#20013;&#30340;&#35760;&#24405;FID&#20174;2.41&#25913;&#36827;&#21040;&#20102;1.81&#65292;&#37319;&#29992;&#20102;&#24555;&#36895;&#30830;&#23450;&#24615;&#37319;&#26679;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02696v2 Announce Type: replace-cross  Abstract: Diffusion models currently dominate the field of data-driven image synthesis with their unparalleled scaling to large datasets. In this paper, we identify and rectify several causes for uneven and ineffective training in the popular ADM diffusion model architecture, without altering its high-level structure. Observing uncontrolled magnitude changes and imbalances in both the network activations and weights over the course of training, we redesign the network layers to preserve activation, weight, and update magnitudes on expectation. We find that systematic application of this philosophy eliminates the observed drifts and imbalances, resulting in considerably better networks at equal computational complexity. Our modifications improve the previous record FID of 2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic sampling.   As an independent contribution, we present a method for setting the exponential mov
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20855;&#26377;&#20223;&#23556;&#32467;&#26500;&#30340;&#31616;&#21333;&#23398;&#20064;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#22810;&#29615;&#22659;&#27867;&#21270;&#65292;&#33021;&#22815;&#35782;&#21035;&#29289;&#29702;&#31995;&#32479;&#30340;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2312.00477</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;&#31995;&#32479;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interpretable Meta-Learning of Physical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00477
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20855;&#26377;&#20223;&#23556;&#32467;&#26500;&#30340;&#31616;&#21333;&#23398;&#20064;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#22810;&#29615;&#22659;&#27867;&#21270;&#65292;&#33021;&#22815;&#35782;&#21035;&#29289;&#29702;&#31995;&#32479;&#30340;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#22312;&#31185;&#23398;&#36807;&#31243;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#38754;&#23545;&#25968;&#25454;&#26469;&#33258;&#19981;&#22343;&#21248;&#23454;&#39564;&#26465;&#20214;&#30340;&#25361;&#25112;&#24615;&#24773;&#22659;&#12290;&#36817;&#26399;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#65292;&#23548;&#33268;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#26377;&#38480;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#35748;&#20026;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;&#23545;&#20110;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#20223;&#23556;&#32467;&#26500;&#65292;&#26469;&#23454;&#29616;&#22810;&#29615;&#22659;&#27867;&#21270;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26550;&#26500;&#33021;&#22815;&#35782;&#21035;&#31995;&#32479;&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;&#29289;&#29702;&#31995;&#32479;&#19978;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20174;&#29609;&#20855;&#27169;&#22411;&#21040;&#22797;&#26434;&#30340;&#38750;&#35299;&#26512;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#31454;&#20105;&#24615;&#27867;&#21270;&#24615;&#33021;&#21644;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00477v2 Announce Type: replace  Abstract: Machine learning methods can be a valuable aid in the scientific process, but they need to face challenging settings where data come from inhomogeneous experimental conditions. Recent meta-learning methods have made significant progress in multi-task learning, but they rely on black-box neural networks, resulting in high computational costs and limited interpretability. Leveraging the structure of the learning problem, we argue that multi-environment generalization can be achieved using a simpler learning model, with an affine structure with respect to the learning task. Crucially, we prove that this architecture can identify the physical parameters of the system, enabling interpreable learning. We demonstrate the competitive generalization performance and the low computational cost of our method by comparing it to state-of-the-art algorithms on physical systems, ranging from toy models to complex, non-analytical systems. The interpr
&lt;/p&gt;</description></item><item><title>&#21482;&#26377;&#24403;&#32771;&#34385;&#30340;&#25439;&#22833;&#20989;&#25968;&#20026;&#20984;&#20989;&#25968;&#26102;&#65292;&#38598;&#25104;&#27169;&#22411;&#19968;&#30452;&#22312;&#21464;&#24471;&#26356;&#22909;&#65292;&#24403;&#25439;&#22833;&#20989;&#25968;&#20026;&#38750;&#20984;&#20989;&#25968;&#26102;&#65292;&#22909;&#27169;&#22411;&#30340;&#38598;&#25104;&#21464;&#24471;&#26356;&#22909;&#65292;&#22351;&#27169;&#22411;&#30340;&#38598;&#25104;&#21464;&#24471;&#26356;&#31967;&#12290;</title><link>https://arxiv.org/abs/2311.17885</link><description>&lt;p&gt;
&#38598;&#25104;&#27169;&#22411;&#26159;&#21542;&#19968;&#30452;&#22312;&#19981;&#26029;&#36827;&#27493;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Ensembles Getting Better all the Time?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17885
&lt;/p&gt;
&lt;p&gt;
&#21482;&#26377;&#24403;&#32771;&#34385;&#30340;&#25439;&#22833;&#20989;&#25968;&#20026;&#20984;&#20989;&#25968;&#26102;&#65292;&#38598;&#25104;&#27169;&#22411;&#19968;&#30452;&#22312;&#21464;&#24471;&#26356;&#22909;&#65292;&#24403;&#25439;&#22833;&#20989;&#25968;&#20026;&#38750;&#20984;&#20989;&#25968;&#26102;&#65292;&#22909;&#27169;&#22411;&#30340;&#38598;&#25104;&#21464;&#24471;&#26356;&#22909;&#65292;&#22351;&#27169;&#22411;&#30340;&#38598;&#25104;&#21464;&#24471;&#26356;&#31967;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#32467;&#21512;&#20102;&#20960;&#20010;&#22522;&#30784;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26159;&#21542;&#22987;&#32456;&#23558;&#26356;&#22810;&#27169;&#22411;&#32435;&#20837;&#38598;&#25104;&#20250;&#25552;&#21319;&#20854;&#24179;&#22343;&#24615;&#33021;&#12290;&#36825;&#20010;&#38382;&#39064;&#21462;&#20915;&#20110;&#25152;&#32771;&#34385;&#30340;&#38598;&#25104;&#31867;&#22411;&#65292;&#20197;&#21450;&#36873;&#25321;&#30340;&#39044;&#27979;&#24230;&#37327;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25152;&#26377;&#38598;&#25104;&#25104;&#21592;&#34987;&#39044;&#26399;&#34920;&#29616;&#30456;&#21516;&#30340;&#24773;&#20917;&#65292;&#36825;&#26159;&#20960;&#31181;&#27969;&#34892;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#25110;&#28145;&#24230;&#38598;&#25104;&#65289;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#31181;&#35774;&#23450;&#19979;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#21482;&#26377;&#24403;&#32771;&#34385;&#30340;&#25439;&#22833;&#20989;&#25968;&#20026;&#20984;&#20989;&#25968;&#26102;&#65292;&#38598;&#25104;&#25165;&#20250;&#19968;&#30452;&#21464;&#24471;&#26356;&#22909;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38598;&#25104;&#30340;&#24179;&#22343;&#25439;&#22833;&#26159;&#27169;&#22411;&#25968;&#37327;&#30340;&#20943;&#20989;&#25968;&#12290;&#24403;&#25439;&#22833;&#20989;&#25968;&#20026;&#38750;&#20984;&#20989;&#25968;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#32467;&#26524;&#65292;&#21487;&#20197;&#24635;&#32467;&#20026;&#65306;&#22909;&#27169;&#22411;&#30340;&#38598;&#25104;&#20250;&#21464;&#24471;&#26356;&#22909;&#65292;&#22351;&#27169;&#22411;&#30340;&#38598;&#25104;&#20250;&#21464;&#24471;&#26356;&#31967;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20851;&#20110;&#23614;&#27010;&#29575;&#21333;&#35843;&#24615;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17885v2 Announce Type: replace-cross  Abstract: Ensemble methods combine the predictions of several base models. We study whether or not including more models always improves their average performance. This question depends on the kind of ensemble considered, as well as the predictive metric chosen. We focus on situations where all members of the ensemble are a priori expected to perform as well, which is the case of several popular methods such as random forests or deep ensembles. In this setting, we show that ensembles are getting better all the time if, and only if, the considered loss function is convex. More precisely, in that case, the average loss of the ensemble is a decreasing function of the number of models. When the loss function is nonconvex, we show a series of results that can be summarised as: ensembles of good models keep getting better, and ensembles of bad models keep getting worse. To this end, we prove a new result on the monotonicity of tail probabiliti
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#24179;&#22343;&#22238;&#25253;MDP&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#26159;&#39318;&#20010;&#22312;&#25152;&#26377;&#21442;&#25968;&#26041;&#38754;&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.13469</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#24230;&#30340;&#24179;&#22343;&#22238;&#25253;MDP&#30340;&#26368;&#20248;&#37319;&#26679;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Span-Based Optimal Sample Complexity for Average Reward MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#24179;&#22343;&#22238;&#25253;MDP&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#26159;&#39318;&#20010;&#22312;&#25152;&#26377;&#21442;&#25968;&#26041;&#38754;&#37117;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#19979;&#23398;&#20064;&#24179;&#22343;&#22238;&#25253;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#30340;$\varepsilon$-&#26368;&#20248;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22797;&#26434;&#24230;&#30028;&#38480;$\widetilde{O}\left(SA\frac{H}{\varepsilon^2} \right)$&#65292;&#20854;&#20013;$H$&#26159;&#26368;&#20248;&#31574;&#30053;&#30340;&#20559;&#24046;&#20989;&#25968;&#30340;&#36328;&#24230;&#65292;$SA$&#26159;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22522;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#22312;&#25152;&#26377;&#21442;&#25968;$S,A,H$&#21644;$\varepsilon$&#20013;&#65288;&#26368;&#22810;&#23545;&#25968;&#22240;&#23376;&#65289;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#29616;&#26377;&#24037;&#20316;&#35201;&#20040;&#20551;&#35774;&#25152;&#26377;&#31574;&#30053;&#30340;&#28151;&#21512;&#26102;&#38388;&#22343;&#21248;&#26377;&#30028;&#65292;&#35201;&#20040;&#23545;&#21442;&#25968;&#26377;&#27425;&#26368;&#20248;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22522;&#20110;&#23558;&#24179;&#22343;&#22238;&#25253;MDP&#38477;&#32423;&#20026;&#25240;&#25187;MDP&#12290;&#20026;&#20102;&#24314;&#31435;&#36825;&#31181;&#38477;&#32423;&#30340;&#26368;&#20248;&#24615;&#65292;&#25105;&#20204;&#20026;$\gamma$-&#25240;&#25187;MDP&#24320;&#21457;&#20102;&#25913;&#36827;&#30340;&#30028;&#38480;&#65292;&#34920;&#26126;$\widetilde{O}\left(SA\frac{H}{(1-\gamma)^2\varepsilon^2} \right)$&#20010;&#26679;&#26412;&#36275;&#20197;&#23398;&#20064;&#24369;ly c
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13469v2 Announce Type: replace  Abstract: We study the sample complexity of learning an $\varepsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model. We establish the complexity bound $\widetilde{O}\left(SA\frac{H}{\varepsilon^2} \right)$, where $H$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,H$ and $\varepsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters.   Our result is based on reducing the average-reward MDP to a discounted MDP. To establish the optimality of this reduction, we develop improved bounds for $\gamma$-discounted MDPs, showing that $\widetilde{O}\left(SA\frac{H}{(1-\gamma)^2\varepsilon^2} \right)$ samples suffice to learn a $\varepsilon$-optimal policy in weakly c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#27425;&#24615;&#20998;&#27835;&#20272;&#35745;&#21644;&#22810;&#36718;&#20272;&#35745;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#23545;&#21322;&#21442;&#25968;&#20108;&#20803;&#36873;&#25321;&#27169;&#22411;&#30340;&#26032;&#20272;&#35745;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#35823;&#24046;&#30340;&#36229;&#32447;&#24615;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2210.08393</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#25512;&#26029;&#29992;&#20110;&#21322;&#21442;&#25968;&#20108;&#20803;&#21709;&#24212;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Distributed Estimation and Inference for Semi-parametric Binary Response Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.08393
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#27425;&#24615;&#20998;&#27835;&#20272;&#35745;&#21644;&#22810;&#36718;&#20272;&#35745;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#23545;&#21322;&#21442;&#25968;&#20108;&#20803;&#36873;&#25321;&#27169;&#22411;&#30340;&#26032;&#20272;&#35745;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#35823;&#24046;&#30340;&#36229;&#32447;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25216;&#26415;&#30340;&#21457;&#23637;&#20351;&#24471;&#25968;&#25454;&#25910;&#38598;&#30340;&#35268;&#27169;&#21069;&#25152;&#26410;&#26377;&#65292;&#36825;&#32473;&#35768;&#22810;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#38382;&#39064;&#24102;&#26469;&#20102;&#26032;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#35745;&#31639;&#29615;&#22659;&#19979;&#23545;&#21322;&#21442;&#25968;&#20108;&#20803;&#36873;&#25321;&#27169;&#22411;&#30340;&#26368;&#22823;&#20998;&#25968;&#20272;&#35745;&#22120;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#25351;&#23450;&#22122;&#22768;&#20998;&#24067;&#12290;&#20256;&#32479;&#30340;&#20998;&#27835;&#20272;&#35745;&#22120;&#22312;&#35745;&#31639;&#19978;&#26114;&#36149;&#65292;&#24182;&#21463;&#21040;&#26426;&#22120;&#25968;&#37327;&#30340;&#38750;&#27491;&#21017;&#32422;&#26463;&#30340;&#38480;&#21046;&#65292;&#36825;&#26159;&#30001;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#39640;&#24230;&#38750;&#20809;&#28369;&#24615;&#36136;&#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;(1)&#22312;&#24179;&#28369;&#30446;&#26631;&#20043;&#21518;&#36827;&#34892;&#19968;&#27425;&#24615;&#20998;&#27835;&#20272;&#35745;&#20197;&#25918;&#23485;&#32422;&#26463;&#65292;&#20197;&#21450;(2)&#36890;&#36807;&#36845;&#20195;&#24179;&#28369;&#23436;&#20840;&#21435;&#38500;&#32422;&#26463;&#30340;&#22810;&#36718;&#20272;&#35745;&#12290;&#25105;&#20204;&#25351;&#23450;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#26680;&#24179;&#28369;&#22120;&#36873;&#25321;&#65292;&#36890;&#36807;&#39034;&#24207;&#32553;&#23567;&#24102;&#23485;&#22312;&#22810;&#27425;&#36845;&#20195;&#20013;&#23454;&#29616;&#20102;&#23545;&#20248;&#21270;&#35823;&#24046;&#30340;&#36229;&#32447;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.08393v3 Announce Type: replace-cross  Abstract: The development of modern technology has enabled data collection of unprecedented size, which poses new challenges to many statistical estimation and inference problems. This paper studies the maximum score estimator of a semi-parametric binary choice model under a distributed computing environment without pre-specifying the noise distribution. An intuitive divide-and-conquer estimator is computationally expensive and restricted by a non-regular constraint on the number of machines, due to the highly non-smooth nature of the objective function. We propose (1) a one-shot divide-and-conquer estimator after smoothing the objective to relax the constraint, and (2) a multi-round estimator to completely remove the constraint via iterative smoothing. We specify an adaptive choice of kernel smoother with a sequentially shrinking bandwidth to achieve the superlinear improvement of the optimization error over the multiple iterations. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#31867;&#22411;&#29983;&#25104;&#26862;&#26519;&#30340;&#27491;&#21017;&#21270;&#30913; Laplacian &#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#32780;&#23494;&#38598;&#22270;&#30340;&#35889;&#36817;&#20284;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2208.14797</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#31867;&#22411;&#29983;&#25104;&#26862;&#26519;&#30340;&#27491;&#21017;&#21270;&#30913; Laplacian &#30340;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sparsification of the regularized magnetic Laplacian with multi-type spanning forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.14797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#31867;&#22411;&#29983;&#25104;&#26862;&#26519;&#30340;&#27491;&#21017;&#21270;&#30913; Laplacian &#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#32780;&#23494;&#38598;&#22270;&#30340;&#35889;&#36817;&#20284;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010; ${\rm U}(1)$-connection &#22270;&#65292;&#21363;&#19968;&#20010;&#22270;&#65292;&#20854;&#20013;&#27599;&#26465;&#26377;&#21521;&#36793;&#37117;&#36171;&#20104;&#19968;&#20010;&#21333;&#20301;&#27169;&#22797;&#25968;&#65292;&#22312;&#21453;&#21521;&#26102;&#21462;&#20849;&#36717;&#12290;&#23545;&#20110;&#32452;&#21512; Laplacian &#30340;&#19968;&#20010;&#33258;&#28982;&#26367;&#20195;&#29289;&#26159;&#30913; Laplacian&#65292;&#19968;&#20010;&#21253;&#21547;&#20102;&#20851;&#20110;&#22270;&#36830;&#25509;&#20449;&#24687;&#30340; Hermite &#30697;&#38453;&#12290;&#30913; Laplacians &#22312;&#35282;&#24230;&#21516;&#27493;&#38382;&#39064;&#20013;&#20986;&#29616;&#12290;&#22312;&#22823;&#32780;&#23494;&#38598;&#30340;&#22270;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30913; Laplacian $\Delta$ &#30340;&#31232;&#30095;&#21270;&#65292;&#21363;&#22522;&#20110;&#20855;&#26377;&#23569;&#37327;&#36793;&#30340;&#23376;&#22270;&#30340;&#35889;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#33258;&#23450;&#20041;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#23545;&#22810;&#31867;&#22411;&#29983;&#25104;&#26862;&#26519;&#65288;MTSFs&#65289;&#36827;&#34892;&#25277;&#26679;&#65292;&#36825;&#26159;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#65292;&#26377;&#21033;&#20110;&#22810;&#26679;&#24615;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;MTSF &#26159;&#19968;&#20010;&#29983;&#25104;&#23376;&#22270;&#65292;&#20854;&#36830;&#36890;&#20998;&#37327;&#20026;&#26641;&#25110;&#29615;&#26681;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.14797v2 Announce Type: replace-cross  Abstract: In this paper, we consider a ${\rm U}(1)$-connection graph, that is, a graph where each oriented edge is endowed with a unit modulus complex number that is conjugated under orientation flip. A natural replacement for the combinatorial Laplacian is then the magnetic Laplacian, an Hermitian matrix that includes information about the graph's connection. Magnetic Laplacians appear, e.g., in the problem of angular synchronization. In the context of large and dense graphs, we study here sparsifiers of the magnetic Laplacian $\Delta$, i.e., spectral approximations based on subgraphs with few edges. Our approach relies on sampling multi-type spanning forests (MTSFs) using a custom determinantal point process, a probability distribution over edges that favours diversity. In a word, an MTSF is a spanning subgraph whose connected components are either trees or cycle-rooted trees. The latter partially capture the angular inconsistencies of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20026;&#27599;&#20010;&#33410;&#28857;-&#23545;&#35937;&#24341;&#20837;&#23616;&#37096;&#22352;&#26631;&#31995;&#65292;&#20197;&#35825;&#23548;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#22270;&#20855;&#26377;&#26059;&#36716;-&#24179;&#31227;&#19981;&#21464;&#24615;&#12290;</title><link>https://arxiv.org/abs/2110.14961</link><description>&lt;p&gt;
&#29992;&#20110;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#30340;Roto-translated&#23616;&#37096;&#22352;&#26631;&#31995;
&lt;/p&gt;
&lt;p&gt;
Roto-translated Local Coordinate Frames For Interacting Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.14961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20026;&#27599;&#20010;&#33410;&#28857;-&#23545;&#35937;&#24341;&#20837;&#23616;&#37096;&#22352;&#26631;&#31995;&#65292;&#20197;&#35825;&#23548;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#22270;&#20855;&#26377;&#26059;&#36716;-&#24179;&#31227;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#30456;&#20114;&#20316;&#29992;&#22312;&#23398;&#20064;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21363;&#30456;&#20114;&#20316;&#29992;&#23545;&#35937;&#20855;&#26377;&#39640;&#24230;&#38750;&#32447;&#24615;&#21644;&#26102;&#21464;&#34892;&#20026;&#30340;&#31995;&#32479;&#12290;&#22312;$\textit{&#20960;&#20309;&#22270;}$&#65292;$\textit{&#21363;}$&#65292;&#33410;&#28857;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#25918;&#32622;&#30340;&#22270;&#24418;&#20013;&#65292;&#21363;&#20351;&#26159;&#22312;$\textit{&#20219;&#24847;}$&#36873;&#25321;&#30340;&#20840;&#23616;&#22352;&#26631;&#31995;&#20013;&#65292;&#21487;&#20197;&#24418;&#24335;&#21270;&#22320;&#34920;&#31034;&#22823;&#31867;&#36825;&#26679;&#30340;&#31995;&#32479;&#65292;&#20363;&#22914;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#36710;&#36742;&#12290;&#23613;&#31649;&#20840;&#23616;&#22352;&#26631;&#31995;&#26159;&#20219;&#24847;&#36873;&#25321;&#30340;&#65292;&#20294;&#21508;&#33258;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#21160;&#21147;&#23398;&#19981;&#21464;&#20110;&#26059;&#36716;&#21644;&#24179;&#31227;&#65292;&#20063;&#34987;&#31216;&#20026;$\textit{&#20285;&#21033;&#30053;&#19981;&#21464;&#24615;}$&#12290;&#24573;&#30053;&#36825;&#20123;&#19981;&#21464;&#24615;&#20250;&#23548;&#33268;&#26356;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#27599;&#20010;&#33410;&#28857;&#23545;&#35937;&#30340;&#23616;&#37096;&#22352;&#26631;&#31995;&#65292;&#20197;&#35825;&#23548;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#22270;&#20855;&#26377;&#26059;&#36716;-&#24179;&#31227;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#23616;&#37096;&#22352;&#26631;&#31995;&#20801;&#35768;&#33258;&#28982;&#23450;&#20041;&#21508;&#21521;&#24322;&#24615;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.14961v3 Announce Type: replace  Abstract: Modelling interactions is critical in learning complex dynamical systems, namely systems of interacting objects with highly non-linear and time-dependent behaviour. A large class of such systems can be formalized as $\textit{geometric graphs}$, $\textit{i.e.}$, graphs with nodes positioned in the Euclidean space given an $\textit{arbitrarily}$ chosen global coordinate system, for instance vehicles in a traffic scene. Notwithstanding the arbitrary global coordinate system, the governing dynamics of the respective dynamical systems are invariant to rotations and translations, also known as $\textit{Galilean invariance}$. As ignoring these invariances leads to worse generalization, in this work we propose local coordinate frames per node-object to induce roto-translation invariance to the geometric graph of the interacting dynamical system. Further, the local coordinate frames allow for a natural definition of anisotropic filtering in g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#19968;&#33268;&#30340;&#20551;&#35774;&#25214;&#21040;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#24191;&#27867;&#31867;&#21035;&#30340;&#24369;&#20984;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2105.06251</link><description>&lt;p&gt;
&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#23398;&#20064;&#24369;&#20984;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Learning Weakly Convex Sets in Metric Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2105.06251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#19968;&#33268;&#30340;&#20551;&#35774;&#25214;&#21040;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#24191;&#27867;&#31867;&#21035;&#30340;&#24369;&#20984;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#20013;&#30740;&#31350;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#23545;&#20110;&#32473;&#23450;&#31867;&#21035;&#30340;&#20551;&#35774;&#65292;&#26159;&#21542;&#21487;&#33021;&#26377;&#25928;&#22320;&#25214;&#21040;&#19968;&#20010;{&#19968;&#33268;&#30340;}&#20551;&#35774;&#65292;&#21363;&#20855;&#26377;&#38646;&#35757;&#32451;&#35823;&#24046;&#30340;&#20551;&#35774;&#12290;&#23613;&#31649;&#28041;&#21450;{\em &#20984;}&#20551;&#35774;&#30340;&#38382;&#39064;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#30001;&#21487;&#33021;&#26377;&#20960;&#20010;&#19981;&#36830;&#32493;&#21306;&#22495;&#32452;&#25104;&#30340;&#38750;&#20984;&#20551;&#35774;&#26159;&#21542;&#21487;&#20197;&#36827;&#34892;&#26377;&#25928;&#23398;&#20064;&#30340;&#38382;&#39064;&#20173;&#19981;&#22826;&#28165;&#26970;&#12290;&#34429;&#28982;&#24456;&#20037;&#20197;&#21069;&#23601;&#24050;&#32463;&#34920;&#26126;&#23545;&#20110;&#24067;&#23572;&#20989;&#25968;&#30340;&#29305;&#27530;&#24773;&#20917;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#24369;&#20984;&#20551;&#35774;&#65288;&#20984;&#20551;&#35774;&#30340;&#21442;&#25968;&#21270;&#35843;&#25972;&#65289;&#65292;&#20294;&#33267;&#20170;&#23578;&#26410;&#30740;&#31350;&#36825;&#20010;&#24819;&#27861;&#26159;&#21542;&#21487;&#20197;&#21457;&#23637;&#20026;&#36890;&#29992;&#33539;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#31215;&#26497;&#31572;&#22797;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#24191;&#27867;&#31867;&#21035;&#30340;&#24369;&#20984;&#20551;&#35774;&#30340;&#19968;&#33268;&#20551;&#35774;&#25214;&#21040;&#38382;&#39064;&#30830;&#23454;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2105.06251v2 Announce Type: replace  Abstract: One of the central problems studied in the theory of machine learning is the question of whether, for a given class of hypotheses, it is possible to efficiently find a {consistent} hypothesis, i.e., which has zero training error. While problems involving {\em convex} hypotheses have been extensively studied, the question of whether efficient learning is possible for non-convex hypotheses composed of possibly several disconnected regions is still less understood. Although it has been shown quite a while ago that efficient learning of weakly convex hypotheses, a parameterized relaxation of convex hypotheses, is possible for the special case of Boolean functions, the question of whether this idea can be developed into a generic paradigm has not been studied yet. In this paper, we provide a positive answer and show that the consistent hypothesis finding problem can indeed be solved in polynomial time for a broad class of weakly convex hy
&lt;/p&gt;</description></item><item><title>&#21457;&#29616;&#36138;&#23146;&#31639;&#27861;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#19981;&#21512;&#29702;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#33258;&#30001;&#25506;&#32034;&#65292;&#23545;&#36138;&#23146;&#31639;&#27861;&#26377;&#30410;&#12290;</title><link>https://arxiv.org/abs/2002.10121</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#35768;&#22810;&#33218;&#30340;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#65292;&#36138;&#23146;&#31639;&#27861;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Greedy Algorithms in Multi-Armed Bandit with Many Arms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2002.10121
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#36138;&#23146;&#31639;&#27861;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#19981;&#21512;&#29702;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#33258;&#30001;&#25506;&#32034;&#65292;&#23545;&#36138;&#23146;&#31639;&#27861;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;$k$-&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#22312;\emph{&#20247;&#22810;&#33218;}&#24773;&#26223;&#19979;&#65292;&#20854;&#20013;$k \geq \sqrt{T}$&#65292;$T$&#20195;&#34920;&#26102;&#38388;&#36328;&#24230;&#12290;&#26368;&#21021;&#65292;&#19982;&#26368;&#36817;&#26377;&#20851;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#25991;&#29486;&#19968;&#33268;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23376;&#37319;&#26679;&#22312;&#35774;&#35745;&#26368;&#20248;&#31639;&#27861;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65307;&#20256;&#32479;&#30340;&#19978;&#32622;&#20449;&#30028;&#65288;UCB&#65289;&#31639;&#27861;&#26159;&#27425;&#20248;&#30340;&#65292;&#32780;&#19968;&#20010;&#23376;&#37319;&#26679;&#30340;UCB&#65288;SS-UCB&#65289;&#65292;&#22312;UCB&#26694;&#26550;&#19979;&#36873;&#25321;$\Theta(\sqrt{T})$&#20010;&#33218;&#36827;&#34892;&#25191;&#34892;&#65292;&#36798;&#21040;&#20102;&#36895;&#29575;&#26368;&#20248;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SS-UCB&#22312;&#29702;&#35770;&#19978;&#25215;&#35834;&#20102;&#26368;&#20248;&#36951;&#25022;&#65292;&#20294;&#22312;&#23454;&#39564;&#20013;&#19982;&#19968;&#31181;&#22987;&#32456;&#36873;&#25321;&#32463;&#39564;&#19978;&#26368;&#20339;&#33218;&#30340;&#36138;&#23146;&#31639;&#27861;&#30456;&#27604;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#19968;&#21457;&#29616;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#27169;&#25311;&#24310;&#20280;&#21040;&#20102;&#24773;&#22659;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#31034;&#19968;&#31181;&#23545;&#20110;&#22810;&#33218;&#24773;&#20917;&#19979;&#36138;&#23146;&#31639;&#27861;&#26377;&#30410;&#30340;&#26032;&#24418;&#24335;&#30340;\emph{&#33258;&#30001;&#25506;&#32034;}&#65292;&#20174;&#26681;&#26412;&#19978;&#19982;&#33218;&#30340;&#20808;&#39564;&#20998;&#24067;&#30456;&#20851;&#30340;&#19968;&#20010;&#23614;&#20107;&#20214;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2002.10121v4 Announce Type: replace  Abstract: We investigate a Bayesian $k$-armed bandit problem in the \emph{many-armed} regime, where $k \geq \sqrt{T}$ and $T$ represents the time horizon. Initially, and aligned with recent literature on many-armed bandit problems, we observe that subsampling plays a key role in designing optimal algorithms; the conventional UCB algorithm is sub-optimal, whereas a subsampled UCB (SS-UCB), which selects $\Theta(\sqrt{T})$ arms for execution under the UCB framework, achieves rate-optimality. However, despite SS-UCB's theoretical promise of optimal regret, it empirically underperforms compared to a greedy algorithm that consistently chooses the empirically best arm. This observation extends to contextual settings through simulations with real-world data. Our findings suggest a new form of \emph{free exploration} beneficial to greedy algorithms in the many-armed context, fundamentally linked to a tail event concerning the prior distribution of arm
&lt;/p&gt;</description></item><item><title>DDMI&#26159;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#26080;&#20851;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#33258;&#36866;&#24212;&#20301;&#32622;&#23884;&#20837;&#32780;&#19981;&#26159;&#32593;&#32476;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#29983;&#25104;&#36136;&#37327;&#36739;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12517</link><description>&lt;p&gt;
DDMI: &#38754;&#21521;&#39046;&#22495;&#26080;&#20851;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations. (arXiv:2401.12517v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12517
&lt;/p&gt;
&lt;p&gt;
DDMI&#26159;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#26080;&#20851;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#33258;&#36866;&#24212;&#20301;&#32622;&#23884;&#20837;&#32780;&#19981;&#26159;&#32593;&#32476;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#29983;&#25104;&#36136;&#37327;&#36739;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31867;&#29992;&#20110;&#21512;&#25104;&#21508;&#20010;&#39046;&#22495;&#20013;&#20219;&#24847;&#36830;&#32493;&#20449;&#21495;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#39046;&#22495;&#26080;&#20851;&#30340;&#29983;&#25104;&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#65292;&#20294;&#24448;&#24448;&#26080;&#27861;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#21442;&#25968;&#21270;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#22266;&#23450;&#30340;&#20301;&#32622;&#23884;&#20837;&#26469;&#35780;&#20272;&#32593;&#32476;&#12290;&#21487;&#20197;&#35828;&#65292;&#36825;&#31181;&#26550;&#26500;&#38480;&#21046;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23548;&#33268;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#29983;&#25104;&#30340;&#36136;&#37327;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#26080;&#20851;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; (DDMI)&#65292;&#20854;&#29983;&#25104;&#33258;&#36866;&#24212;&#20301;&#32622;&#23884;&#20837;&#32780;&#19981;&#26159;&#32593;&#32476;&#26435;&#37325;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31163;&#25955;&#21040;&#36830;&#32493;&#31354;&#38388;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120; (D2C-VAE)&#65292;&#23427;&#22312;&#20849;&#20139;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#32541;&#36830;&#25509;&#31163;&#25955;&#25968;&#25454;&#21644;&#36830;&#32493;&#20449;&#21495;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Recent studies have introduced a new class of generative models for synthesizing implicit neural representations (INRs) that capture arbitrary continuous signals in various domains. These models opened the door for domain-agnostic generative models, but they often fail to achieve high-quality generation. We observed that the existing methods generate the weights of neural networks to parameterize INRs and evaluate the network with fixed positional embeddings (PEs). Arguably, this architecture limits the expressive power of generative models and results in low-quality INR generation. To address this limitation, we propose Domain-agnostic Latent Diffusion Model for INRs (DDMI) that generates adaptive positional embeddings instead of neural networks' weights. Specifically, we develop a Discrete-to-continuous space Variational AutoEncoder (D2C-VAE), which seamlessly connects discrete data and the continuous signal functions in the shared latent space. Additionally, we introduce a novel con
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#24182;&#25552;&#20379;&#20102;&#26465;&#20214;&#12290;&#36825;&#26159;&#23545;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#30340;&#36845;&#20195;&#26041;&#27861;&#30340;&#19968;&#31181;&#37325;&#35201;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2401.05394</link><description>&lt;p&gt;
&#36845;&#20195;&#27491;&#21017;&#21270;&#19982;k&#25903;&#25745;&#33539;&#25968;&#65306;&#31232;&#30095;&#24674;&#22797;&#30340;&#37325;&#35201;&#34917;&#20805;
&lt;/p&gt;
&lt;p&gt;
Iterative Regularization with k-Support Norm: an Important Complement to Sparse Recovery. (arXiv:2401.05394v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05394
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#24182;&#25552;&#20379;&#20102;&#26465;&#20214;&#12290;&#36825;&#26159;&#23545;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#30340;&#36845;&#20195;&#26041;&#27861;&#30340;&#19968;&#31181;&#37325;&#35201;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24674;&#22797;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#30001;&#20110;&#31232;&#30095;&#24674;&#22797;&#30340;NP&#22256;&#38590;&#24615;&#36136;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#21463;&#38480;&#20110;&#36866;&#29992;&#26465;&#20214;&#65288;&#29978;&#33267;&#26410;&#30693;&#65289;&#65292;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#26368;&#36817;&#65292;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#20986;&#29616;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#19968;&#27425;&#36890;&#36807;&#26469;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#26041;&#27861;&#20013;&#32321;&#29712;&#30340;&#32593;&#26684;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#36845;&#20195;&#26041;&#27861;&#37117;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#65292;&#38656;&#35201;&#21463;&#38480;&#30340;&#36866;&#29992;&#26465;&#20214;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#26356;&#24191;&#27867;&#30340;&#26465;&#20214;&#19979;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#22522;&#20110;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#32780;&#19981;&#26159;$\ell_1$&#33539;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;IRKSN&#36827;&#34892;&#31232;&#30095;&#24674;&#22797;&#30340;&#26465;&#20214;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse recovery is ubiquitous in machine learning and signal processing. Due to the NP-hard nature of sparse recovery, existing methods are known to suffer either from restrictive (or even unknown) applicability conditions, or high computational cost. Recently, iterative regularization methods have emerged as a promising fast approach because they can achieve sparse recovery in one pass through early stopping, rather than the tedious grid-search used in the traditional methods. However, most of those iterative methods are based on the $\ell_1$ norm which requires restrictive applicability conditions and could fail in many cases. Therefore, achieving sparse recovery with iterative regularization methods under a wider range of conditions has yet to be further explored. To address this issue, we propose a novel iterative regularization algorithm, IRKSN, based on the $k$-support norm regularizer rather than the $\ell_1$ norm. We provide conditions for sparse recovery with IRKSN, and compar
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#33041;&#21160;&#21147;&#23398;&#22810;&#23610;&#24230;&#22240;&#26524;&#39592;&#26550;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22240;&#26524;&#21160;&#21147;&#22312;&#19981;&#21516;&#39057;&#29575;&#19979;&#21463;&#19981;&#21516;&#33041;&#21306;&#39537;&#21160;&#65292;&#36825;&#20026;&#29702;&#35299;&#33041;&#21151;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2311.00118</link><description>&lt;p&gt;
&#25552;&#21462;&#33041;&#21160;&#21147;&#23398;&#30340;&#22810;&#23610;&#24230;&#22240;&#26524;&#39592;&#26550;
&lt;/p&gt;
&lt;p&gt;
Extracting the Multiscale Causal Backbone of Brain Dynamics. (arXiv:2311.00118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00118
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#33041;&#21160;&#21147;&#23398;&#22810;&#23610;&#24230;&#22240;&#26524;&#39592;&#26550;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22240;&#26524;&#21160;&#21147;&#22312;&#19981;&#21516;&#39057;&#29575;&#19979;&#21463;&#19981;&#21516;&#33041;&#21306;&#39537;&#21160;&#65292;&#36825;&#20026;&#29702;&#35299;&#33041;&#21151;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#20851;&#20110;&#33041;&#36830;&#25509;&#24615;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#33041;&#21306;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#32852;&#19978;&#65292;&#36825;&#19982;&#32479;&#27835;&#33041;&#21160;&#21147;&#23398;&#30340;&#22240;&#26524;&#26426;&#21046;&#19981;&#30452;&#25509;&#30456;&#20851;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#22240;&#26524;&#39592;&#26550;&#65288;MCB&#65289;&#65292;&#23427;&#26159;&#22312;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#20849;&#20139;&#30340;&#19968;&#32452;&#20010;&#20307;&#30340;&#33041;&#21160;&#21147;&#23398;&#29305;&#24449;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#23427;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22810;&#23610;&#24230;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#20248;&#21270;&#20102;&#27169;&#22411;&#25311;&#21512;&#19982;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#35268;&#33539;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#30340;&#22522;&#32447;&#12290;&#24403;&#24212;&#29992;&#20110;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#24038;&#21491;&#33041;&#21322;&#29699;&#37117;&#26377;&#31232;&#30095;&#30340;MCB&#12290;&#30001;&#20110;&#20854;&#22810;&#23610;&#24230;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#26126;&#22312;&#20302;&#39057;&#24102;&#19978;&#65292;&#22240;&#26524;&#21160;&#21147;&#26469;&#33258;&#19982;&#39640;&#32423;&#35748;&#30693;&#21151;&#33021;&#30456;&#20851;&#30340;&#33041;&#21306;&#65307;&#32780;&#22312;&#26356;&#39640;&#30340;&#39057;&#29575;&#19978;&#65292;&#30001;nod&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
The bulk of the research effort on brain connectivity revolves around statistical associations among brain regions, which do not directly relate to the causal mechanisms governing brain dynamics. Here we propose the multiscale causal backbone (MCB) of brain dynamics shared by a set of individuals across multiple temporal scales, and devise a principled methodology to extract it.  Our approach leverages recent advances in multiscale causal structure learning and optimizes the trade-off between the model fitting and its complexity. Empirical assessment on synthetic data shows the superiority of our methodology over a baseline based on canonical functional connectivity networks. When applied to resting-state fMRI data, we find sparse MCBs for both the left and right brain hemispheres. Thanks to its multiscale nature, our approach shows that at low-frequency bands, causal dynamics are driven by brain regions associated with high-level cognitive functions; at higher frequencies instead, nod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31515;&#21345;&#23572;&#31215;&#21644;&#31070;&#32463;&#22330;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#20013;&#21457;&#29616;&#23616;&#37096;&#29289;&#20307;&#30456;&#20114;&#20316;&#29992;&#21644;&#20840;&#23616;&#22330;&#25928;&#24212;&#30340;&#28508;&#22312;&#21147;&#22330;&#12290;</title><link>http://arxiv.org/abs/2310.20679</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#22330;&#22312;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#20013;&#21457;&#29616;&#28508;&#22312;&#22330;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Latent Field Discovery In Interacting Dynamical Systems With Neural Fields. (arXiv:2310.20679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31515;&#21345;&#23572;&#31215;&#21644;&#31070;&#32463;&#22330;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#20013;&#21457;&#29616;&#23616;&#37096;&#29289;&#20307;&#30456;&#20114;&#20316;&#29992;&#21644;&#20840;&#23616;&#22330;&#25928;&#24212;&#30340;&#28508;&#22312;&#21147;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20114;&#20316;&#29992;&#23545;&#35937;&#30340;&#31995;&#32479;&#22312;&#20854;&#21160;&#21147;&#23398;&#20013;&#36890;&#24120;&#20250;&#21463;&#21040;&#22330;&#25928;&#24212;&#30340;&#24433;&#21709;&#65292;&#28982;&#32780;&#20197;&#24448;&#30340;&#30740;&#31350;&#24120;&#24120;&#24573;&#30053;&#20102;&#36825;&#20123;&#25928;&#24212;&#65292;&#20551;&#35774;&#31995;&#32479;&#22312;&#30495;&#31354;&#20013;&#28436;&#21270;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#21457;&#29616;&#36825;&#20123;&#22330;&#25928;&#24212;&#65292;&#24182;&#20165;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#21160;&#21147;&#23398;&#26469;&#36827;&#34892;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#30452;&#25509;&#35266;&#27979;&#23427;&#20204;&#12290;&#25105;&#20204;&#20551;&#35774;&#23384;&#22312;&#28508;&#22312;&#30340;&#21147;&#22330;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#31070;&#32463;&#22330;&#26469;&#23398;&#20064;&#23427;&#20204;&#12290;&#30001;&#20110;&#35266;&#23519;&#21040;&#30340;&#21160;&#21147;&#23398;&#26159;&#23616;&#37096;&#29289;&#20307;&#30456;&#20114;&#20316;&#29992;&#21644;&#25972;&#20307;&#22330;&#25928;&#24212;&#30340;&#32508;&#21512;&#32467;&#26524;&#65292;&#26368;&#36817;&#27969;&#34892;&#30340;&#31561;&#21464;&#32593;&#32476;&#26080;&#27861;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#25429;&#25417;&#21040;&#20840;&#23616;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#23616;&#37096;&#29289;&#20307;&#30456;&#20114;&#20316;&#29992;&#65288;SE(n)&#31561;&#21464;&#30340;&#65292;&#20381;&#36182;&#20110;&#30456;&#23545;&#29366;&#24577;&#65289;&#19982;&#22806;&#37096;&#20840;&#23616;&#22330;&#25928;&#24212;&#65288;&#20381;&#36182;&#20110;&#32477;&#23545;&#29366;&#24577;&#65289;&#30456;&#20998;&#31163;&#12290;&#25105;&#20204;&#20351;&#29992;&#31561;&#21464;&#22270;&#32593;&#32476;&#23545;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23558;&#20854;&#19982;&#31070;&#32463;&#22330;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#34701;&#21512;&#20102;&#22330;&#25928;&#24212;&#30340;&#22270;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systems of interacting objects often evolve under the influence of field effects that govern their dynamics, yet previous works have abstracted away from such effects, and assume that systems evolve in a vacuum. In this work, we focus on discovering these fields, and infer them from the observed dynamics alone, without directly observing them. We theorize the presence of latent force fields, and propose neural fields to learn them. Since the observed dynamics constitute the net effect of local object interactions and global field effects, recently popularized equivariant networks are inapplicable, as they fail to capture global information. To address this, we propose to disentangle local object interactions -- which are $\mathrm{SE}(n)$ equivariant and depend on relative states -- from external global field effects -- which depend on absolute states. We model interactions with equivariant graph networks, and combine them with neural fields in a novel graph network that integrates fiel
&lt;/p&gt;</description></item><item><title>MCRAGE&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#22686;&#24378;&#19981;&#24179;&#34913;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23569;&#25968;&#32676;&#20307;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18430</link><description>&lt;p&gt;
MCRAGE: &#20844;&#24179;&#24615;&#30340;&#21512;&#25104;&#21307;&#30103;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
MCRAGE: Synthetic Healthcare Data for Fairness. (arXiv:2310.18430v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18430
&lt;/p&gt;
&lt;p&gt;
MCRAGE&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#22686;&#24378;&#19981;&#24179;&#34913;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23569;&#25968;&#32676;&#20307;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#24320;&#21457;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#31649;&#29702;&#21307;&#30103;&#36164;&#28304;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#30103;&#25968;&#25454;&#38598;&#22312;&#31181;&#26063;/&#27665;&#26063;&#12289;&#24615;&#21035;&#21644;&#24180;&#40836;&#31561;&#25935;&#24863;&#23646;&#24615;&#26041;&#38754;&#24448;&#24448;&#23384;&#22312;&#19981;&#24179;&#34913;&#12290;&#22312;&#31867;&#19981;&#24179;&#34913;&#30340;EHR&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#37096;&#32626;&#26102;&#65292;&#23545;&#20110;&#23569;&#25968;&#32676;&#20307;&#30340;&#20010;&#20307;&#32780;&#35328;&#65292;&#34920;&#29616;&#26174;&#33879;&#19981;&#22914;&#22810;&#25968;&#32676;&#20307;&#30340;&#26679;&#26412;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23569;&#25968;&#32676;&#20307;&#30340;&#19981;&#20844;&#24179;&#21307;&#30103;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Minority Class Rebalancing through Augmentation by Generative modeling (MCRAGE)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30001;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#26469;&#22686;&#24378;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12290;MCRAGE&#36807;&#31243;&#21253;&#25324;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#20174;&#23569;&#25968;&#32676;&#20307;&#20013;&#20135;&#29983;&#39640;&#36136;&#37327;&#21512;&#25104;EHR&#26679;&#26412;&#30340;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;CDDPM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of healthcare, electronic health records (EHR) serve as crucial training data for developing machine learning models for diagnosis, treatment, and the management of healthcare resources. However, medical datasets are often imbalanced in terms of sensitive attributes such as race/ethnicity, gender, and age. Machine learning models trained on class-imbalanced EHR datasets perform significantly worse in deployment for individuals of the minority classes compared to samples from majority classes, which may lead to inequitable healthcare outcomes for minority groups. To address this challenge, we propose Minority Class Rebalancing through Augmentation by Generative modeling (MCRAGE), a novel approach to augment imbalanced datasets using samples generated by a deep generative model. The MCRAGE process involves training a Conditional Denoising Diffusion Probabilistic Model (CDDPM) capable of generating high-quality synthetic EHR samples from underrepresented classes. We use this 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#24314;&#31435;&#20102;&#19968;&#33268;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#32032;&#25551;&#23725;&#22238;&#24402;&#38598;&#21512;&#30340;&#39044;&#27979;&#39118;&#38505;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#27491;&#21017;&#21270;&#21644;&#32032;&#25551;&#21442;&#25968;&#30340;&#39640;&#25928;&#19968;&#33268;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2310.04357</link><description>&lt;p&gt;
&#28176;&#36827;&#20813;&#36153;&#32032;&#25551;&#31232;&#30095;&#23725;&#38598;&#21512;&#65306;&#39118;&#38505;&#65292;&#20132;&#21449;&#39564;&#35777;&#21644;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Asymptotically free sketched ridge ensembles: Risks, cross-validation, and tuning. (arXiv:2310.04357v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04357
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#24314;&#31435;&#20102;&#19968;&#33268;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#32032;&#25551;&#23725;&#22238;&#24402;&#38598;&#21512;&#30340;&#39044;&#27979;&#39118;&#38505;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#27491;&#21017;&#21270;&#21644;&#32032;&#25551;&#21442;&#25968;&#30340;&#39640;&#25928;&#19968;&#33268;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#65292;&#24314;&#31435;&#20102;&#25512;&#24191;&#20132;&#21449;&#39564;&#35777;&#65288;GCV&#65289;&#29992;&#20110;&#20272;&#35745;&#32032;&#25551;&#23725;&#22238;&#24402;&#38598;&#21512;&#30340;&#39044;&#27979;&#39118;&#38505;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#27491;&#21017;&#21270;&#21644;&#32032;&#25551;&#21442;&#25968;&#30340;&#39640;&#25928;&#19968;&#33268;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;&#28176;&#36827;&#20813;&#36153;&#32032;&#25551;&#65292;&#23545;&#25968;&#25454;&#20551;&#35774;&#38750;&#24120;&#28201;&#21644;&#12290;&#23545;&#20110;&#24179;&#26041;&#39044;&#27979;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20998;&#35299;&#25104;&#31561;&#25928;&#38750;&#32032;&#25551;&#38544;&#21547;&#23725;&#20559;&#24046;&#21644;&#22522;&#20110;&#32032;&#25551;&#30340;&#26041;&#24046;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#39118;&#38505;&#21487;&#20197;&#36890;&#36807;&#20165;&#35843;&#25972;&#26080;&#38480;&#38598;&#21512;&#20013;&#30340;&#32032;&#25551;&#22823;&#23567;&#26469;&#20840;&#23616;&#20248;&#21270;&#12290;&#23545;&#20110;&#19968;&#33324;&#30340;&#20122;&#20108;&#27425;&#39044;&#27979;&#39118;&#38505;&#20989;&#25968;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;GCV&#26469;&#26500;&#24314;&#19968;&#33268;&#30340;&#39118;&#38505;&#20272;&#35745;&#65292;&#20174;&#32780;&#22312;Wasserstein-2&#24230;&#37327;&#19979;&#33719;&#24471;&#20102;GCV&#20462;&#27491;&#30340;&#39044;&#27979;&#30340;&#20998;&#24067;&#25910;&#25947;&#24615;&#12290;&#36825;&#29305;&#21035;&#20801;&#35768;&#22312;&#35757;&#32451;&#25968;&#25454;&#26465;&#20214;&#19979;&#26500;&#24314;&#20855;&#26377;&#28176;&#36827;&#27491;&#30830;&#35206;&#30422;&#29575;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#38598;&#21512;&#25216;&#24039;&#8221;&#65292;&#36890;&#36807;&#36825;&#31181;&#25216;&#24039;&#21487;&#20197;&#25512;&#26029;&#26410;&#32463;&#36807;&#25551;&#32472;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
We employ random matrix theory to establish consistency of generalized cross validation (GCV) for estimating prediction risks of sketched ridge regression ensembles, enabling efficient and consistent tuning of regularization and sketching parameters. Our results hold for a broad class of asymptotically free sketches under very mild data assumptions. For squared prediction risk, we provide a decomposition into an unsketched equivalent implicit ridge bias and a sketching-based variance, and prove that the risk can be globally optimized by only tuning sketch size in infinite ensembles. For general subquadratic prediction risk functionals, we extend GCV to construct consistent risk estimators, and thereby obtain distributional convergence of the GCV-corrected predictions in Wasserstein-2 metric. This in particular allows construction of prediction intervals with asymptotically correct coverage conditional on the training data. We also propose an "ensemble trick" whereby the risk for unsket
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Forman-Ricci&#26354;&#29575;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#35266;&#23519;&#31163;&#25955;&#26354;&#29575;&#65292;&#21487;&#20197;&#28155;&#21152;&#25110;&#21024;&#38500;&#36793;&#20197;&#20943;&#36731;&#36825;&#20004;&#31181;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.09384</link><description>&lt;p&gt;
&#20351;&#29992;Forman-Ricci&#26354;&#29575;&#30340;&#25193;&#23637;&#26469;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Over-Smoothing and Over-Squashing using Augmentations of Forman-Ricci Curvature. (arXiv:2309.09384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Forman-Ricci&#26354;&#29575;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#35266;&#23519;&#31163;&#25955;&#26354;&#29575;&#65292;&#21487;&#20197;&#28155;&#21152;&#25110;&#21024;&#38500;&#36793;&#20197;&#20943;&#36731;&#36825;&#20004;&#31181;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26368;&#36817;&#25551;&#36848;&#20102;&#20960;&#20010;&#28508;&#22312;&#30340;&#38519;&#38449;&#12290;&#36825;&#20123;&#21253;&#25324;&#26080;&#27861;&#20934;&#30830;&#21033;&#29992;&#32534;&#30721;&#22312;&#38271;&#36317;&#31163;&#36830;&#25509;&#20013;&#30340;&#20449;&#24687;&#65288;&#36807;&#24230;&#21387;&#32553;&#65289;&#65292;&#20197;&#21450;&#22312;&#32593;&#32476;&#28145;&#24230;&#22686;&#21152;&#26102;&#38590;&#20197;&#21306;&#20998;&#38468;&#36817;&#33410;&#28857;&#30340;&#23398;&#20064;&#34920;&#31034;&#65288;&#36807;&#24230;&#24179;&#28369;&#65289;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#34920;&#24449;&#36825;&#20004;&#31181;&#25928;&#24212;&#30340;&#26041;&#27861;&#26159;&#31163;&#25955;&#26354;&#29575;&#65306;&#23548;&#33268;&#36807;&#24230;&#21387;&#32553;&#25928;&#24212;&#30340;&#38271;&#36317;&#31163;&#36830;&#25509;&#20855;&#26377;&#20302;&#26354;&#29575;&#65292;&#32780;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#30340;&#36793;&#20855;&#26377;&#39640;&#26354;&#29575;&#12290;&#36825;&#20010;&#35266;&#23519;&#24341;&#21457;&#20102;&#19968;&#20123;&#37325;&#36830;&#25216;&#26415;&#65292;&#36890;&#36807;&#22686;&#21152;&#25110;&#21024;&#38500;&#36793;&#26469;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21033;&#29992;&#22270;&#29305;&#24449;&#65288;&#22914;&#26354;&#29575;&#25110;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#35889;&#65289;&#30340;&#37325;&#36830;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#26354;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#23376;&#22270;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Graph Neural Networks (GNNs) have been successfully leveraged for learning on graph-structured data across domains, several potential pitfalls have been described recently. Those include the inability to accurately leverage information encoded in long-range connections (over-squashing), as well as difficulties distinguishing the learned representations of nearby nodes with growing network depth (over-smoothing). An effective way to characterize both effects is discrete curvature: Long-range connections that underlie over-squashing effects have low curvature, whereas edges that contribute to over-smoothing have high curvature. This observation has given rise to rewiring techniques, which add or remove edges to mitigate over-smoothing and over-squashing. Several rewiring approaches utilizing graph characteristics, such as curvature or the spectrum of the graph Laplacian, have been proposed. However, existing methods, especially those based on curvature, often require expensive subr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#35780;&#20272;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#27604;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#24182;&#19988;&#36825;&#19968;&#32467;&#26524;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.00736</link><description>&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#20013;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Prediction Error Estimation in Random Forests. (arXiv:2309.00736v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#35780;&#20272;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#27604;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#24182;&#19988;&#36825;&#19968;&#32467;&#26524;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#37327;&#35780;&#20272;&#20102;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#12290;&#22312;Bates&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24314;&#31435;&#30340;&#21021;&#27493;&#29702;&#35770;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#20174;&#29702;&#35770;&#21644;&#32463;&#39564;&#35282;&#24230;&#25506;&#35752;&#20102;&#38543;&#26426;&#26862;&#26519;&#20013;&#24120;&#35265;&#30340;&#21508;&#31181;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#22312;&#30495;&#23454;&#35823;&#24046;&#29575;&#21644;&#26399;&#26395;&#35823;&#24046;&#29575;&#26041;&#38754;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#24179;&#22343;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#32780;&#19981;&#26159;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#12290;&#19982;Bates&#31561;&#20154;&#65288;2023&#24180;&#65289;&#23545;&#36923;&#36753;&#22238;&#24402;&#30340;&#30740;&#31350;&#32467;&#26524;&#30456;&#21453;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#36825;&#20010;&#32467;&#26524;&#36866;&#29992;&#20110;&#20132;&#21449;&#39564;&#35777;&#12289;&#33258;&#20030;&#21644;&#25968;&#25454;&#21010;&#20998;&#31561;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, error estimates of classification Random Forests are quantitatively assessed. Based on the initial theoretical framework built by Bates et al. (2023), the true error rate and expected error rate are theoretically and empirically investigated in the context of a variety of error estimation methods common to Random Forests. We show that in the classification case, Random Forests' estimates of prediction error is closer on average to the true error rate instead of the average prediction error. This is opposite the findings of Bates et al. (2023) which were given for logistic regression. We further show that this result holds across different error estimation strategies such as cross-validation, bagging, and data splitting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.06101</link><description>&lt;p&gt;
Prodigy: &#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#38646;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#20013;&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#20004;&#31181;&#25216;&#26415;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#35777;&#26126;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20197;&#20415;&#26368;&#20248;&#35774;&#32622;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#22522;&#20110;&#23398;&#20064;&#29575;&#33258;&#30001;&#30340;D-Adaptation&#26041;&#27861;&#30340;&#20462;&#25913;&#65292;&#24182;&#36890;&#36807;$O(\sqrt{\log(D/d_0)})$&#30340;&#22240;&#23376;&#25552;&#39640;&#20102;D-Adaptation&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$d_0$&#26159;$D$&#30340;&#21021;&#22987;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#22312;12&#20010;&#24120;&#35265;&#30340;&#36923;&#36753;&#22238;&#24402;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#22312;CIFAR10&#19978;&#35757;&#32451;&#30340;VGG11&#21644;ResNet-50&#12289;&#22312;Imagenet&#19978;&#35757;&#32451;&#30340;ViT&#12289;&#22312;IWSLT14&#19978;&#35757;&#32451;&#30340;LSTM&#12289;&#22312;Criteo&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;DLRM&#12289;&#22312;Knee MRI&#25968;&#25454;&#38598;&#19978;&#30340;VarNet&#65292;&#20197;&#21450;&#22312;BookWiki&#19978;&#35757;&#32451;&#30340;RoBERTa&#21644;GPT transformer&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;D-Adaptation&#65292;&#24182;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating the learning rate in adaptive methods, such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to provably estimate the distance to the solution $D$, which is needed to set the learning rate optimally. Our techniques are modifications of the D-Adaptation method for learning-rate-free learning. Our methods improve upon the convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where $d_0$ is the initial estimate of $D$. We test our methods on 12 common logistic-regression benchmark datasets, VGG11 and ResNet-50 training on CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT transformer training on BookWiki. Our experimental results show that our approaches consistently outperform D-Adaptation and reach test accuracy values close to that of hand-tuned Adam.
&lt;/p&gt;</description></item></channel></rss>