<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#27880;&#24847;&#21147;&#23618;&#20855;&#26377;&#36739;&#39640;&#30340;&#35789;&#25935;&#24863;&#24615;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;transformers&#30340;&#25104;&#21151;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2402.02969</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#23618;&#30340;&#35789;&#25935;&#24863;&#24615;&#30340;&#29702;&#35299;&#65306;&#36890;&#36807;&#38543;&#26426;&#29305;&#24449;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02969
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#27880;&#24847;&#21147;&#23618;&#20855;&#26377;&#36739;&#39640;&#30340;&#35789;&#25935;&#24863;&#24615;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;transformers&#30340;&#25104;&#21151;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;transformers&#24322;&#24120;&#25104;&#21151;&#32972;&#21518;&#21407;&#22240;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#20026;&#20160;&#20040;&#27880;&#24847;&#21147;&#23618;&#36866;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20123;&#20219;&#21153;&#35201;&#27714;&#39044;&#27979;&#27169;&#22411;&#25429;&#25417;&#19978;&#19979;&#25991;&#21547;&#20041;&#65292;&#21363;&#20351;&#21477;&#23376;&#24456;&#38271;&#65292;&#36825;&#24448;&#24448;&#21462;&#20915;&#20110;&#19968;&#20010;&#25110;&#20960;&#20010;&#35789;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#38543;&#26426;&#29305;&#24449;&#30340;&#20856;&#22411;&#35774;&#32622;&#20013;&#30740;&#31350;&#20102;&#36825;&#19968;&#20851;&#38190;&#23646;&#24615;&#65292;&#31216;&#20026;&#35789;&#25935;&#24863;&#24615;&#65288;WS&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27880;&#24847;&#21147;&#23618;&#20855;&#26377;&#36739;&#39640;&#30340;WS&#65292;&#21363;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23384;&#22312;&#19968;&#20010;&#21521;&#37327;&#65292;&#33021;&#22815;&#22823;&#24133;&#25200;&#21160;&#38543;&#26426;&#27880;&#24847;&#21147;&#29305;&#24449;&#26144;&#23556;&#12290;&#36825;&#20010;&#35770;&#28857;&#20851;&#38190;&#22320;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;softmax&#30340;&#20316;&#29992;&#65292;&#31361;&#26174;&#20102;&#23427;&#30456;&#23545;&#20110;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#65289;&#30340;&#20248;&#21183;&#12290;&#30456;&#21453;&#65292;&#26631;&#20934;&#38543;&#26426;&#29305;&#24449;&#30340;WS&#26159;$1/\sqrt{n}$&#38454;&#30340;&#65292;$n$&#26159;&#25991;&#26412;&#26679;&#26412;&#20013;&#30340;&#21333;&#35789;&#25968;&#65292;&#22240;&#27492;&#23427;&#38543;&#19978;&#19979;&#25991;&#30340;&#38271;&#24230;&#32780;&#34928;&#20943;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#20851;&#20110;&#35789;&#25935;&#24863;&#24615;&#30340;&#32467;&#26524;&#36716;&#21270;&#20026;&#27867;&#21270;&#30028;&#65306;&#30001;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Unveiling the reasons behind the exceptional success of transformers requires a better understanding of why attention layers are suitable for NLP tasks. In particular, such tasks require predictive models to capture contextual meaning which often depends on one or few words, even if the sentence is long. Our work studies this key property, dubbed word sensitivity (WS), in the prototypical setting of random features. We show that attention layers enjoy high WS, namely, there exists a vector in the space of embeddings that largely perturbs the random attention features map. The argument critically exploits the role of the softmax in the attention layer, highlighting its benefit compared to other activations (e.g., ReLU). In contrast, the WS of standard random features is of order $1/\sqrt{n}$, $n$ being the number of words in the textual sample, and thus it decays with the length of the context. We then translate these results on the word sensitivity into generalization bounds: due to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24212;&#29992;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17582</link><description>&lt;p&gt;
&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of flow-based generative models via proximal gradient descent in Wasserstein space. (arXiv:2310.17582v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24212;&#29992;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#35745;&#31639;&#25968;&#25454;&#29983;&#25104;&#21644;&#20284;&#28982;&#20989;&#25968;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#26368;&#36817;&#22312;&#23454;&#35777;&#34920;&#29616;&#19978;&#26174;&#31034;&#20986;&#31454;&#20105;&#21147;&#12290;&#19982;&#30456;&#20851;&#22522;&#20110;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#31215;&#32047;&#29702;&#35770;&#30740;&#31350;&#30456;&#27604;&#65292;&#23545;&#20110;&#22312;&#27491;&#21521;&#65288;&#25968;&#25454;&#21040;&#22122;&#22768;&#65289;&#21644;&#21453;&#21521;&#65288;&#22122;&#22768;&#21040;&#25968;&#25454;&#65289;&#26041;&#21521;&#19978;&#37117;&#26159;&#30830;&#23450;&#24615;&#30340;&#27969;&#27169;&#22411;&#30340;&#20998;&#26512;&#36824;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#24402;&#19968;&#21270;&#27969;&#32593;&#32476;&#20013;&#23454;&#26045;Jordan-Kinderleherer-Otto&#65288;JKO&#65289;&#26041;&#26696;&#30340;&#25152;&#35859;JKO&#27969;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#36890;&#36807;&#28176;&#36827;&#27969;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#21033;&#29992;Wasserstein&#31354;&#38388;&#20013;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#25351;&#25968;&#25910;&#25947;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;JKO&#27969;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#30340;Kullback-Leibler&#65288;KL&#65289;&#20445;&#35777;&#20026;$O(\varepsilon^2)$&#65292;&#20854;&#20013;&#20351;&#29992;$N \lesssim \log (1/\varepsilon)$&#20010;JKO&#27493;&#39588;&#65288;&#27969;&#20013;&#30340;$N$&#20010;&#27531;&#24046;&#22359;&#65289;&#65292;&#20854;&#20013;$\varepsilon$&#26159;&#27599;&#27493;&#19968;&#38454;&#26465;&#20214;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flow-based generative models enjoy certain advantages in computing the data generation and the likelihood, and have recently shown competitive empirical performance. Compared to the accumulating theoretical studies on related score-based diffusion models, analysis of flow-based models, which are deterministic in both forward (data-to-noise) and reverse (noise-to-data) directions, remain sparse. In this paper, we provide a theoretical guarantee of generating data distribution by a progressive flow model, the so-called JKO flow model, which implements the Jordan-Kinderleherer-Otto (JKO) scheme in a normalizing flow network. Leveraging the exponential convergence of the proximal gradient descent (GD) in Wasserstein space, we prove the Kullback-Leibler (KL) guarantee of data generation by a JKO flow model to be $O(\varepsilon^2)$ when using $N \lesssim \log (1/\varepsilon)$ many JKO steps ($N$ Residual Blocks in the flow) where $\varepsilon $ is the error in the per-step first-order condit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#20960;&#20309;&#24418;&#29366;&#30340;&#25193;&#23637;&#20989;&#25968;&#21098;&#26525;&#35268;&#21017;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#22810;&#20010;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#27979;&#22810;&#20010;&#21464;&#28857;&#30340;&#38382;&#39064;&#65292;&#22312;&#23567;&#32500;&#24230;&#24773;&#20917;&#19979;&#21487;&#20197;&#27604;&#20989;&#25968;&#21098;&#26525;&#26356;&#24555;&#22320;&#20934;&#30830;&#26816;&#27979;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.09555</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#30340;&#35268;&#21017;&#22312;&#22810;&#20010;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#20013;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#30340;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Geometric-Based Pruning Rules For Change Point Detection in Multiple Independent Time Series. (arXiv:2306.09555v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09555
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#20960;&#20309;&#24418;&#29366;&#30340;&#25193;&#23637;&#20989;&#25968;&#21098;&#26525;&#35268;&#21017;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#22810;&#20010;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#27979;&#22810;&#20010;&#21464;&#28857;&#30340;&#38382;&#39064;&#65292;&#22312;&#23567;&#32500;&#24230;&#24773;&#20917;&#19979;&#21487;&#20197;&#27604;&#20989;&#25968;&#21098;&#26525;&#26356;&#24555;&#22320;&#20934;&#30830;&#26816;&#27979;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#26816;&#27979;&#22810;&#20010;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22810;&#20010;&#21464;&#28857;&#30340;&#38382;&#39064;&#12290;&#23547;&#25214;&#26368;&#20339;&#20998;&#21106;&#21487;&#20197;&#34920;&#36798;&#20026;&#22312;&#32473;&#23450;&#25104;&#26412;&#20989;&#25968;&#19978;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#12290;&#24403;&#21464;&#21270;&#27425;&#25968;&#19982;&#25968;&#25454;&#38271;&#24230;&#25104;&#27604;&#20363;&#26102;&#65292;PELT&#31639;&#27861;&#20013;&#32534;&#30721;&#30340;&#22522;&#20110;&#19981;&#31561;&#24335;&#30340;&#21098;&#26525;&#35268;&#21017;&#20250;&#23548;&#33268;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#21478;&#19968;&#31181;&#31216;&#20026;&#20989;&#25968;&#21098;&#26525;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#23545;&#20110;&#20998;&#26512;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#32780;&#35328;&#65292;&#26080;&#35770;&#21464;&#21270;&#27425;&#25968;&#22914;&#20309;&#65292;&#20854;&#26102;&#38388;&#22797;&#26434;&#24230;&#37117;&#25509;&#36817;&#20110;&#32447;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#20351;&#29992;&#31616;&#21333;&#20960;&#20309;&#24418;&#29366;&#65288;&#29699;&#20307;&#21644;&#36229;&#30697;&#24418;&#65289;&#30340;&#20989;&#25968;&#21098;&#26525;&#30340;&#25193;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#39640;&#26031;&#24773;&#20917;&#65292;&#20294;&#25105;&#20204;&#30340;&#19968;&#20123;&#35268;&#21017;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#25351;&#25968;&#26063;&#12290;&#22312;&#27169;&#25311;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#22522;&#20110;&#20960;&#20309;&#30340;&#21098;&#26525;&#35268;&#21017;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#23567;&#32500;&#24230;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#36229;&#30697;&#24418;&#21644;&#29699;&#20307;&#36827;&#34892;&#21098;&#26525;&#21487;&#20197;&#27604;&#20989;&#25968;&#21098;&#26525;&#26356;&#24555;&#36895;&#22320;&#20934;&#30830;&#26816;&#27979;&#26356;&#22909;&#12290;&#23545;&#20110;&#36739;&#22823;&#32500;&#24230;&#65292;&#36229;&#30697;&#24418;&#21464;&#24471;&#19981;&#37027;&#20040;&#39640;&#25928;&#65292;&#32780;&#29699;&#20307;&#20165;&#22312;&#39640;&#20449;&#22122;&#27604;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of detecting multiple changes in multiple independent time series. The search for the best segmentation can be expressed as a minimization problem over a given cost function. We focus on dynamic programming algorithms that solve this problem exactly. When the number of changes is proportional to data length, an inequality-based pruning rule encoded in the PELT algorithm leads to a linear time complexity. Another type of pruning, called functional pruning, gives a close-to-linear time complexity whatever the number of changes, but only for the analysis of univariate time series.  We propose a few extensions of functional pruning for multiple independent time series based on the use of simple geometric shapes (balls and hyperrectangles). We focus on the Gaussian case, but some of our rules can be easily extended to the exponential family. In a simulation study we compare the computational efficiency of different geometric-based pruning rules. We show that for smal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;ERM&#35757;&#32451;&#27169;&#22411;&#23545;&#25239;&#24378;&#22823;&#40657;&#30418;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#25351;&#26631;&#37327;&#21270;&#27169;&#22411;&#23433;&#20840;&#24615;&#65306;&#21333;&#20010;&#26679;&#26412;&#30340;&#31283;&#23450;&#24615;&#21644;&#26597;&#35810;&#19982;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#23545;&#40784;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;RF&#21644;NTK&#22238;&#24402;&#65292;&#35777;&#26126;&#38543;&#30528;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#38544;&#31169;&#20445;&#25252;&#21487;&#20197;&#24471;&#21040;&#21152;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.12100</link><description>&lt;p&gt;
&#31283;&#23450;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#65306;&#23545;&#20110;&#38543;&#26426;&#29305;&#24449;&#21644;NTK&#29305;&#24449;&#30340;&#31934;&#30830;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stability, Generalization and Privacy: Precise Analysis for Random and NTK Features. (arXiv:2305.12100v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;ERM&#35757;&#32451;&#27169;&#22411;&#23545;&#25239;&#24378;&#22823;&#40657;&#30418;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#25351;&#26631;&#37327;&#21270;&#27169;&#22411;&#23433;&#20840;&#24615;&#65306;&#21333;&#20010;&#26679;&#26412;&#30340;&#31283;&#23450;&#24615;&#21644;&#26597;&#35810;&#19982;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#23545;&#40784;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;RF&#21644;NTK&#22238;&#24402;&#65292;&#35777;&#26126;&#38543;&#30528;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#38544;&#31169;&#20445;&#25252;&#21487;&#20197;&#24471;&#21040;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24674;&#22797;&#25915;&#20987;&#65292;&#24341;&#36215;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30340;&#25285;&#24551;&#12290;&#38024;&#23545;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#31561;&#24120;&#35265;&#31639;&#27861;&#36890;&#24120;&#19981;&#33021;&#30452;&#25509;&#23454;&#26045;&#23433;&#20840;&#20445;&#38556;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;ERM&#35757;&#32451;&#27169;&#22411;&#23545;&#25239;&#29305;&#23450;&#24378;&#22823;&#40657;&#30418;&#23376;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36890;&#36807;&#20004;&#20010;&#30475;&#20284;&#19981;&#21516;&#20294;&#26377;&#32852;&#31995;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#27169;&#22411;&#23433;&#20840;&#24615;&#65306;&#19968;&#26159;&#30456;&#23545;&#20110;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#27169;&#22411;&#31283;&#23450;&#24615;&#65292;&#21478;&#19968;&#20010;&#26159;&#25915;&#20987;&#26597;&#35810;&#21644;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#29305;&#24449;&#23545;&#40784;&#12290;&#34429;&#28982;&#21069;&#32773;&#22312;&#23398;&#20064;&#29702;&#35770;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#38416;&#36848;&#65292;&#24182;&#19982;&#32463;&#20856;&#24037;&#20316;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#30456;&#20851;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#31532;&#20108;&#31181;&#29305;&#24615;&#26159;&#26032;&#39062;&#30340;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#32467;&#26524;&#20026;&#20004;&#31181;&#21407;&#22411;&#35774;&#32622;&#25552;&#20379;&#20102;&#29305;&#24449;&#23545;&#40784;&#30340;&#31934;&#30830;&#21051;&#30011;&#65306;&#38543;&#26426;&#29305;&#24449;&#65288;RF&#65289;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#22238;&#24402;&#12290;&#36825;&#35777;&#26126;&#65292;&#38543;&#30528;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#38544;&#31169;&#20445;&#25252;&#33021;&#22815;&#24471;&#21040;&#21152;&#24378;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20854;&#20182;&#26377;&#36259;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models can be vulnerable to recovery attacks, raising privacy concerns to users, and widespread algorithms such as empirical risk minimization (ERM) often do not directly enforce safety guarantees. In this paper, we study the safety of ERM-trained models against a family of powerful black-box attacks. Our analysis quantifies this safety via two separate terms: (i) the model stability with respect to individual training samples, and (ii) the feature alignment between the attacker query and the original data. While the first term is well established in learning theory and it is connected to the generalization error in classical work, the second one is, to the best of our knowledge, novel. Our key technical result provides a precise characterization of the feature alignment for the two prototypical settings of random features (RF) and neural tangent kernel (NTK) regression. This proves that privacy strengthens with an increase in the generalization capability, unveiling also
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#30340;&#32508;&#21512;$\alpha$-&#39118;&#38505;&#29256;&#26412;&#21644;&#22522;&#20110;&#39118;&#38505;&#33719;&#24471;&#26368;&#20248;&#32452;&#21512;&#30340;&#35745;&#31639;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#38889;&#22269;&#32929;&#31080;&#30340;&#24754;&#35266;&#26368;&#20248;&#32452;&#21512;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.07158</link><description>&lt;p&gt;
&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#21644;&#26368;&#20248;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Uniform Pessimistic Risk and Optimal Portfolio. (arXiv:2303.07158v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#30340;&#32508;&#21512;$\alpha$-&#39118;&#38505;&#29256;&#26412;&#21644;&#22522;&#20110;&#39118;&#38505;&#33719;&#24471;&#26368;&#20248;&#32452;&#21512;&#30340;&#35745;&#31639;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#38889;&#22269;&#32929;&#31080;&#30340;&#24754;&#35266;&#26368;&#20248;&#32452;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a version of integrated $\alpha$-risk called the uniform pessimistic risk and a computational algorithm to obtain an optimal portfolio based on the risk. The proposed method can be used to estimate the pessimistic optimal portfolio models for Korean stocks.
&lt;/p&gt;
&lt;p&gt;
&#36164;&#20135;&#37197;&#32622;&#30340;&#26368;&#20248;&#24615;&#24050;&#32463;&#22312;&#39118;&#38505;&#24230;&#37327;&#30340;&#29702;&#35770;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#35752;&#35770;&#12290;&#24754;&#35266;&#20027;&#20041;&#26159;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#26368;&#20248;&#32452;&#21512;&#27169;&#22411;&#30340;&#26368;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;$\alpha$-&#39118;&#38505;&#22312;&#25512;&#23548;&#20986;&#24191;&#27867;&#30340;&#24754;&#35266;&#26368;&#20248;&#32452;&#21512;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#24754;&#35266;&#39118;&#38505;&#35780;&#20272;&#30340;&#26368;&#20248;&#32452;&#21512;&#30340;&#20272;&#35745;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#21487;&#29992;&#30340;&#20272;&#35745;&#27169;&#22411;&#21644;&#35745;&#31639;&#31639;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#30340;&#32508;&#21512;$\alpha$-&#39118;&#38505;&#29256;&#26412;&#21644;&#22522;&#20110;&#39118;&#38505;&#33719;&#24471;&#26368;&#20248;&#32452;&#21512;&#30340;&#35745;&#31639;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#20998;&#20301;&#25968;&#22238;&#24402;&#12289;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#19977;&#20010;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#25152;&#25552;&#20986;&#30340;&#39118;&#38505;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#21516;&#26102;&#65292;&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#34987;&#24212;&#29992;&#20110;&#20272;&#35745;&#38889;&#22269;&#32929;&#31080;&#30340;&#24754;&#35266;&#26368;&#20248;&#32452;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimality of allocating assets has been widely discussed with the theoretical analysis of risk measures. Pessimism is one of the most attractive approaches beyond the conventional optimal portfolio model, and the $\alpha$-risk plays a crucial role in deriving a broad class of pessimistic optimal portfolios. However, estimating an optimal portfolio assessed by a pessimistic risk is still challenging due to the absence of an available estimation model and a computational algorithm. In this study, we propose a version of integrated $\alpha$-risk called the uniform pessimistic risk and the computational algorithm to obtain an optimal portfolio based on the risk. Further, we investigate the theoretical properties of the proposed risk in view of three different approaches: multiple quantile regression, the proper scoring rule, and distributionally robust optimization. Also, the uniform pessimistic risk is applied to estimate the pessimistic optimal portfolio models for the Korean stock 
&lt;/p&gt;</description></item></channel></rss>