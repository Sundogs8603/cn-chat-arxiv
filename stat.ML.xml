<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#26080;&#38480;&#32500;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#20960;&#20309;&#20808;&#39564;&#20197;&#22788;&#29702;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#24102;&#26377;&#23545;&#31216;&#24615;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#23545;&#31216;&#32676;&#21464;&#25442;&#30340;&#20960;&#20309;&#39640;&#26031;&#36807;&#31243;&#21644;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24471;&#20998;&#65292;&#29983;&#25104;&#20989;&#25968;&#27169;&#22411;&#20063;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05431</link><description>&lt;p&gt;
&#20960;&#20309;&#31070;&#32463;&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Geometric Neural Diffusion Processes. (arXiv:2307.05431v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#26080;&#38480;&#32500;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#20960;&#20309;&#20808;&#39564;&#20197;&#22788;&#29702;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#24102;&#26377;&#23545;&#31216;&#24615;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#23545;&#31216;&#32676;&#21464;&#25442;&#30340;&#20960;&#20309;&#39640;&#26031;&#36807;&#31243;&#21644;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24471;&#20998;&#65292;&#29983;&#25104;&#20989;&#25968;&#27169;&#22411;&#20063;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#29983;&#25104;&#24314;&#27169;&#33539;&#24335;&#12290;&#26368;&#36817;&#23558;&#20854;&#25193;&#23637;&#21040;&#26080;&#38480;&#32500;&#27431;&#27663;&#31354;&#38388;&#20351;&#24471;&#21487;&#20197;&#23545;&#38543;&#26426;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#31185;&#23398;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#37117;&#28041;&#21450;&#23545;&#31216;&#24615;&#21644;&#23384;&#22312;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#25193;&#23637;&#21040;&#26080;&#38480;&#32500;&#24314;&#27169;&#20013;&#24341;&#20837;&#19968;&#31995;&#21015;&#20960;&#20309;&#20808;&#39564;&#12290;&#25105;&#20204;&#36890;&#36807; a) &#26500;&#24314;&#19968;&#20010;&#22122;&#22768;&#36807;&#31243;&#65292;&#20854;&#26497;&#38480;&#20998;&#24067;&#26159;&#22312;&#24863;&#20852;&#36259;&#30340;&#23545;&#31216;&#32676;&#19979;&#21464;&#25442;&#30340;&#20960;&#20309;&#39640;&#26031;&#36807;&#31243;&#65292;&#24182; b) &#20351;&#29992;&#23545;&#36825;&#20010;&#32676;&#20855;&#26377;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#24471;&#20998;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#29983;&#25104;&#20989;&#25968;&#27169;&#22411;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110; Langevin &#30340;&#26465;&#20214;&#37319;&#26679;&#22120;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23481;&#37327;&#24615;&#65292;&#20197;&#36866;&#24212;&#22797;&#26434;&#30340;&#26631;&#37327;&#21644;&#21521;&#37327;&#22330;&#65292;&#36825;&#20123;&#22330;&#23384;&#22312;&#20110;&#27431;&#27663;&#31354;&#38388;&#21644;&#29699;&#24418;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models have proven to be a flexible and effective paradigm for generative modelling. Their recent extension to infinite dimensional Euclidean spaces has allowed for the modelling of stochastic processes. However, many problems in the natural sciences incorporate symmetries and involve data living in non-Euclidean spaces. In this work, we extend the framework of diffusion models to incorporate a series of geometric priors in infinite-dimension modelling. We do so by a) constructing a noising process which admits, as limiting distribution, a geometric Gaussian process that transforms under the symmetry group of interest, and b) approximating the score with a neural network that is equivariant w.r.t. this group. We show that with these conditions, the generative functional model admits the same symmetry. We demonstrate scalability and capacity of the model, using a novel Langevin-based conditional sampler, to fit complex scalar and vector fields, with Euclidean and sph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#23884;&#22871;&#26500;&#25104;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#40065;&#26834;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#30697;&#38453;&#27714;&#36870;&#25110;&#23567;&#25209;&#37327;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.05384</link><description>&lt;p&gt;
&#38543;&#26426;&#23884;&#22871;&#26500;&#25104;&#30340;&#21452;&#23618;&#20248;&#21270;&#29992;&#20110;&#40065;&#26834;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stochastic Nested Compositional Bi-level Optimization for Robust Feature Learning. (arXiv:2307.05384v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#23884;&#22871;&#26500;&#25104;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#40065;&#26834;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#30697;&#38453;&#27714;&#36870;&#25110;&#23567;&#25209;&#37327;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;&#35299;&#20915;&#23884;&#22871;&#26500;&#25104;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#12290;&#36825;&#20123;&#38382;&#39064;&#28041;&#21450;&#21040;&#19978;&#23618;&#30340;$T$&#20010;&#28508;&#22312;&#38750;&#20984;&#24179;&#28369;&#20989;&#25968;&#30340;&#23884;&#22871;&#26500;&#36896;&#65292;&#20197;&#21450;&#19979;&#23618;&#30340;&#24179;&#28369;&#19988;&#24378;&#20984;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#30697;&#38453;&#27714;&#36870;&#25110;&#23567;&#25209;&#37327;&#36755;&#20837;&#65292;&#24182;&#19988;&#21487;&#20197;&#20197;&#36817;&#20284;$\tilde{O}_T(1/\epsilon^{2})$&#30340;&#39044;&#31639;&#22797;&#26434;&#24230;&#23454;&#29616;$\epsilon$-&#31283;&#23450;&#35299;&#65292;&#20551;&#35774;&#33021;&#22815;&#24471;&#21040;&#19978;&#23618;&#32452;&#25104;&#20013;&#30340;&#20010;&#20307;&#20989;&#25968;&#21644;&#19979;&#23618;&#20989;&#25968;&#30340;&#38543;&#26426;&#19968;&#38454;&#35834;&#22467;&#23572;&#65292;&#36825;&#20123;&#19968;&#38454;&#35834;&#22467;&#23572;&#26159;&#26080;&#20559;&#19988;&#20855;&#26377;&#26377;&#30028;&#30697;&#12290;&#36825;&#37324;&#65292;$\tilde{O}_T$&#21487;&#20197;&#38544;&#34255;&#22810;&#39033;&#23545;&#25968;&#31995;&#25968;&#21644;&#24120;&#25968;&#65292;&#20381;&#36182;&#20110;$T$&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop and analyze stochastic approximation algorithms for solving nested compositional bi-level optimization problems. These problems involve a nested composition of $T$ potentially non-convex smooth functions in the upper-level, and a smooth and strongly convex function in the lower-level. Our proposed algorithm does not rely on matrix inversions or mini-batches and can achieve an $\epsilon$-stationary solution with an oracle complexity of approximately $\tilde{O}_T(1/\epsilon^{2})$, assuming the availability of stochastic first-order oracles for the individual functions in the composition and the lower-level, which are unbiased and have bounded moments. Here, $\tilde{O}_T$ hides polylog factors and constants that depend on $T$. The key challenge we address in establishing this result relates to handling three distinct sources of bias in the stochastic gradients. The first source arises from the compositional nature of the upper-level, the second stems from the bi-level structure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#36947;&#20272;&#35745;&#65292;&#36890;&#36807;&#23545;&#26465;&#20214;&#39640;&#26031;&#20449;&#36947;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#36827;&#34892;&#21442;&#25968;&#21270;&#36924;&#36817;&#26469;&#33719;&#24471;&#22343;&#26041;&#26681;&#35823;&#24046;&#26368;&#20248;&#20449;&#36947;&#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#20272;&#35745;&#22120;&#30340;&#23454;&#29992;&#24615;&#32771;&#34385;&#21644;&#19977;&#31181;&#19981;&#21516;&#35757;&#32451;&#26041;&#24335;&#30340;&#20272;&#35745;&#22120;&#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2307.05352</link><description>&lt;p&gt;
&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#21442;&#25968;&#21270;MMSE&#20449;&#36947;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Leveraging Variational Autoencoders for Parameterized MMSE Channel Estimation. (arXiv:2307.05352v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#36947;&#20272;&#35745;&#65292;&#36890;&#36807;&#23545;&#26465;&#20214;&#39640;&#26031;&#20449;&#36947;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#36827;&#34892;&#21442;&#25968;&#21270;&#36924;&#36817;&#26469;&#33719;&#24471;&#22343;&#26041;&#26681;&#35823;&#24046;&#26368;&#20248;&#20449;&#36947;&#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#20272;&#35745;&#22120;&#30340;&#23454;&#29992;&#24615;&#32771;&#34385;&#21644;&#19977;&#31181;&#19981;&#21516;&#35757;&#32451;&#26041;&#24335;&#30340;&#20272;&#35745;&#22120;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#36947;&#20272;&#35745;&#12290;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#23558;&#30495;&#23454;&#20294;&#26410;&#30693;&#30340;&#20449;&#36947;&#20998;&#24067;&#24314;&#27169;&#20026;&#26465;&#20214;&#39640;&#26031;&#20998;&#24067;&#12290;&#25152;&#24471;&#21040;&#30340;&#20449;&#36947;&#20272;&#35745;&#22120;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#20869;&#37096;&#32467;&#26500;&#23545;&#26469;&#33258;&#26465;&#20214;&#39640;&#26031;&#20449;&#36947;&#27169;&#22411;&#30340;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#20272;&#35745;&#22120;&#36827;&#34892;&#21442;&#25968;&#21270;&#36924;&#36817;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20160;&#20040;&#26465;&#20214;&#19979;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#20272;&#35745;&#22120;&#26159;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#20272;&#35745;&#22120;&#23454;&#29992;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#20272;&#35745;&#22120;&#21464;&#20307;&#65292;&#23427;&#20204;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#38454;&#27573;&#23545;&#20449;&#36947;&#30693;&#35782;&#30340;&#33719;&#21462;&#26041;&#24335;&#19981;&#21516;&#12290;&#29305;&#21035;&#22320;&#65292;&#20165;&#22522;&#20110;&#22122;&#22768;&#23548;&#39057;&#35266;&#27979;&#36827;&#34892;&#35757;&#32451;&#30340;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#21464;&#20307;&#38750;&#24120;&#20540;&#24471;&#27880;&#24847;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#33719;&#21462;&#20449;&#36947;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this manuscript, we propose to utilize the generative neural network-based variational autoencoder for channel estimation. The variational autoencoder models the underlying true but unknown channel distribution as a conditional Gaussian distribution in a novel way. The derived channel estimator exploits the internal structure of the variational autoencoder to parameterize an approximation of the mean squared error optimal estimator resulting from the conditional Gaussian channel models. We provide a rigorous analysis under which conditions a variational autoencoder-based estimator is mean squared error optimal. We then present considerations that make the variational autoencoder-based estimator practical and propose three different estimator variants that differ in their access to channel knowledge during the training and evaluation phase. In particular, the proposed estimator variant trained solely on noisy pilot observations is particularly noteworthy as it does not require access
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#24773;&#22659;&#36172;&#21338;&#20013;&#30340;&#26368;&#26174;&#33879;&#21464;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#35745;&#31639;&#26174;&#33879;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#23616;&#37096;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05341</link><description>&lt;p&gt;
&#36319;&#36394;&#38750;&#21442;&#25968;&#24773;&#22659;&#36172;&#21338;&#20013;&#26368;&#26174;&#33879;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tracking Most Significant Shifts in Nonparametric Contextual Bandits. (arXiv:2307.05341v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05341
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#24773;&#22659;&#36172;&#21338;&#20013;&#30340;&#26368;&#26174;&#33879;&#21464;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#35745;&#31639;&#26174;&#33879;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#23616;&#37096;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#24773;&#22659;&#36172;&#21338;&#65292;&#20854;&#20013;Lipschitz&#22343;&#20540;&#22870;&#21169;&#20989;&#25968;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#36825;&#20010;&#36739;&#23569;&#34987;&#29702;&#35299;&#30340;&#24773;&#22659;&#19979;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#29575;&#30340;&#26497;&#23567;&#26497;&#22823;&#20540;&#65292;&#36825;&#20123;&#20540;&#19982;&#21464;&#21270;&#25968;&#37327;L&#21644;&#24635;&#21464;&#24046;V&#26377;&#20851;&#65292;&#20004;&#32773;&#37117;&#21487;&#20197;&#25429;&#25417;&#21040;&#19978;&#19979;&#25991;&#31354;&#38388;&#30340;&#25152;&#26377;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#24773;&#22659;&#19979;&#26159;&#27425;&#20248;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#31181;&#24773;&#22659;&#19979;&#30340;&#36866;&#24212;&#24615;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#30693;&#36947;L&#25110;V&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26497;&#23567;&#26497;&#22823;&#20540;&#12290;&#38750;&#24120;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;X_t&#22788;&#65292;&#36172;&#21338;&#38382;&#39064;&#22312;&#19978;&#19979;&#25991;&#31354;&#38388;&#20854;&#20182;&#37096;&#20998;&#20013;&#30340;&#22870;&#21169;&#21464;&#21270;&#19981;&#24212;&#35813;&#20135;&#29983;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#21270;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32463;&#39564;&#26174;&#33879;&#21464;&#21270;&#65292;&#26356;&#22909;&#22320;&#32771;&#34385;&#20102;&#23616;&#37096;&#24615;&#65292;&#22240;&#27492;&#27604;L&#21644;V&#35745;&#25968;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;&#31867;&#20284;&#20110;&#26368;&#36817;&#22312;&#38750;&#24179;&#31283;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#24037;&#20316;&#65288;Suk&#21644;Kpotufe&#65292;2022&#65289;&#65292;&#32463;&#39564;&#26174;&#33879;&#21464;&#21270;&#21482;&#35745;&#31639;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study nonparametric contextual bandits where Lipschitz mean reward functions may change over time. We first establish the minimax dynamic regret rate in this less understood setting in terms of number of changes $L$ and total-variation $V$, both capturing all changes in distribution over context space, and argue that state-of-the-art procedures are suboptimal in this setting.  Next, we tend to the question of an adaptivity for this setting, i.e. achieving the minimax rate without knowledge of $L$ or $V$. Quite importantly, we posit that the bandit problem, viewed locally at a given context $X_t$, should not be affected by reward changes in other parts of context space $\cal X$. We therefore propose a notion of change, which we term experienced significant shifts, that better accounts for locality, and thus counts considerably less changes than $L$ and $V$. Furthermore, similar to recent work on non-stationary MAB (Suk &amp; Kpotufe, 2022), experienced significant shifts only count the m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;MAP&#21644;MLE&#30340;&#25945;&#23398;&#26041;&#27861;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#26681;&#25454;&#35266;&#23519;&#32467;&#26524;&#25512;&#26029;&#38544;&#34255;&#30340;&#27010;&#24565;&#65292;&#25945;&#24072;&#35797;&#22270;&#25214;&#21040;&#26368;&#23567;&#30340;&#35266;&#23519;&#38598;&#21512;&#20197;&#20351;&#24471;&#23398;&#20064;&#32773;&#36820;&#22238;&#29305;&#23450;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2307.05252</link><description>&lt;p&gt;
&#22522;&#20110;MAP&#21644;MLE&#30340;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
MAP- and MLE-Based Teaching. (arXiv:2307.05252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;MAP&#21644;MLE&#30340;&#25945;&#23398;&#26041;&#27861;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#26681;&#25454;&#35266;&#23519;&#32467;&#26524;&#25512;&#26029;&#38544;&#34255;&#30340;&#27010;&#24565;&#65292;&#25945;&#24072;&#35797;&#22270;&#25214;&#21040;&#26368;&#23567;&#30340;&#35266;&#23519;&#38598;&#21512;&#20197;&#20351;&#24471;&#23398;&#20064;&#32773;&#36820;&#22238;&#29305;&#23450;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#19968;&#20010;&#23398;&#20064;&#32773;L&#35797;&#22270;&#20174;&#19968;&#31995;&#21015;&#35266;&#23519;&#20013;&#25512;&#26029;&#20986;&#19968;&#20010;&#38544;&#34255;&#30340;&#27010;&#24565;&#12290;&#22312;Ferri&#31561;&#20154;&#30340;&#24037;&#20316;[4]&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20551;&#35774;&#23398;&#20064;&#32773;&#30001;&#20808;&#39564;P(c)&#21644;&#26465;&#20214;&#27010;&#29575;P(z|c)&#21442;&#25968;&#21270;&#65292;&#20854;&#20013;c&#33539;&#22260;&#22312;&#32473;&#23450;&#31867;&#21035;C&#20013;&#30340;&#25152;&#26377;&#27010;&#24565;&#19978;&#65292;z&#33539;&#22260;&#22312;&#35266;&#23519;&#38598;&#21512;Z&#20013;&#30340;&#25152;&#26377;&#35266;&#23519;&#19978;&#12290;&#22914;&#26524;L&#23558;&#19968;&#32452;&#35266;&#23519;&#30475;&#20316;&#26159;&#38543;&#26426;&#26679;&#26412;&#65292;&#24182;&#36820;&#22238;&#20855;&#26377;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#30340;&#27010;&#24565;&#65288;&#30456;&#24212;&#22320;&#65292;&#36820;&#22238;&#26368;&#22823;&#21270;S&#30340;c&#26465;&#20214;&#27010;&#29575;&#30340;&#27010;&#24565;&#65289;&#65292;&#21017;L&#34987;&#31216;&#20026;MAP&#23398;&#20064;&#22120;&#65288;resp. MLE&#23398;&#20064;&#22120;&#65289;&#12290;&#26681;&#25454;L&#26159;&#21542;&#20551;&#35774;S&#26159;&#20174;&#26377;&#24207;&#25110;&#26080;&#24207;&#37319;&#26679;&#65288;resp. &#26377;&#26367;&#25442;&#25110;&#26080;&#26367;&#25442;&#37319;&#26679;&#65289;&#33719;&#24471;&#30340;&#65292;&#21487;&#20197;&#21306;&#20998;&#22235;&#31181;&#19981;&#21516;&#30340;&#37319;&#26679;&#27169;&#24335;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#30446;&#26631;&#27010;&#24565;c&#22312;C&#20013;&#65292;&#23545;&#20110;MAP&#23398;&#20064;&#22120;L&#26469;&#35828;&#65292;&#25945;&#24072;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#26368;&#23567;&#30340;&#35266;&#23519;&#38598;&#21512;&#65292;&#20351;&#24471;L&#36820;&#22238;c&#12290;&#36825;&#31181;&#26041;&#27861;&#33258;&#28982;&#22320;&#23548;&#33268;&#20102;&#21508;&#31181;MAP&#25110;MLE&#25945;&#23398;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imagine a learner L who tries to infer a hidden concept from a collection of observations. Building on the work [4] of Ferri et al., we assume the learner to be parameterized by priors P(c) and by c-conditional likelihoods P(z|c) where c ranges over all concepts in a given class C and z ranges over all observations in an observation set Z. L is called a MAP-learner (resp. an MLE-learner) if it thinks of a collection S of observations as a random sample and returns the concept with the maximum a-posteriori probability (resp. the concept which maximizes the c-conditional likelihood of S). Depending on whether L assumes that S is obtained from ordered or unordered sampling resp. from sampling with or without replacement, we can distinguish four different sampling modes. Given a target concept c in C, a teacher for a MAP-learner L aims at finding a smallest collection of observations that causes L to return c. This approach leads in a natural manner to various notions of a MAP- or MLE-teac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31283;&#20581;&#23494;&#24230;&#21151;&#29575;&#20998;&#27495;&#65288;DPD&#65289;&#22312;&#19968;&#33324;&#21442;&#25968;&#23494;&#24230;&#27169;&#22411;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20256;&#32479;&#30340;&#38543;&#26426;&#20248;&#21270;&#29702;&#35770;&#26469;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05251</link><description>&lt;p&gt;
&#29992;&#20110;&#19968;&#33324;&#21442;&#25968;&#23494;&#24230;&#27169;&#22411;&#30340;&#26368;&#23567;&#21270;&#31283;&#20581;&#23494;&#24230;&#21151;&#29575;&#20998;&#27495;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A stochastic optimization approach to minimize robust density power-based divergences for general parametric density models. (arXiv:2307.05251v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31283;&#20581;&#23494;&#24230;&#21151;&#29575;&#20998;&#27495;&#65288;DPD&#65289;&#22312;&#19968;&#33324;&#21442;&#25968;&#23494;&#24230;&#27169;&#22411;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20256;&#32479;&#30340;&#38543;&#26426;&#20248;&#21270;&#29702;&#35770;&#26469;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#24230;&#21151;&#29575;&#20998;&#27495;&#65288;DPD&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#22320;&#20272;&#35745;&#35266;&#27979;&#25968;&#25454;&#28508;&#22312;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#35201;&#20272;&#35745;&#30340;&#21442;&#25968;&#23494;&#24230;&#27169;&#22411;&#30340;&#24130;&#30340;&#31215;&#20998;&#39033;&#12290;&#34429;&#28982;&#23545;&#20110;&#19968;&#20123;&#29305;&#23450;&#30340;&#23494;&#24230;&#65288;&#22914;&#27491;&#24577;&#23494;&#24230;&#21644;&#25351;&#25968;&#23494;&#24230;&#65289;&#21487;&#20197;&#24471;&#21040;&#31215;&#20998;&#39033;&#30340;&#26174;&#24335;&#24418;&#24335;&#65292;&#20294;DPD&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20351;&#24471;&#20854;&#26080;&#27861;&#24212;&#29992;&#20110;&#26356;&#19968;&#33324;&#30340;&#21442;&#25968;&#23494;&#24230;&#27169;&#22411;&#65292;&#36825;&#24050;&#32463;&#36229;&#36807;&#20102;DPD&#25552;&#20986;&#30340;25&#24180;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19968;&#33324;&#21442;&#25968;&#23494;&#24230;&#27169;&#22411;&#26368;&#23567;&#21270;DPD&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21442;&#32771;&#38543;&#26426;&#20248;&#21270;&#30340;&#20256;&#32479;&#29702;&#35770;&#35828;&#26126;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26410;&#24402;&#19968;&#21270;&#27169;&#22411;&#26469;&#26368;&#23567;&#21270;&#21478;&#19968;&#20010;&#22522;&#20110;&#23494;&#24230;&#21151;&#29575;&#30340;&#947;-&#31163;&#24046;[Kanamori&#21644;Fujisawa&#65288;2015&#65289;&#65292;Biometrika]&#12290;
&lt;/p&gt;
&lt;p&gt;
Density power divergence (DPD) [Basu et al. (1998), Biometrika], designed to estimate the underlying distribution of the observations robustly, comprises an integral term of the power of the parametric density models to be estimated. While the explicit form of the integral term can be obtained for some specific densities (such as normal density and exponential density), its computational intractability has prohibited the application of DPD-based estimation to more general parametric densities, over a quarter of a century since the proposal of DPD. This study proposes a stochastic optimization approach to minimize DPD for general parametric density models and explains its adequacy by referring to conventional theories on stochastic optimization. The proposed approach also can be applied to the minimization of another density power-based $\gamma$-divergence with the aid of unnormalized models [Kanamori and Fujisawa (2015), Biometrika].
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#33021;&#22815;&#23454;&#29616;&#24046;&#20998;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.05194</link><description>&lt;p&gt;
&#36890;&#36807;$\beta$-&#20998;&#35299;&#19968;&#21518;&#39564;&#37319;&#26679;&#23454;&#29616;&#24046;&#20998;&#35745;&#31639;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling. (arXiv:2307.05194v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05194
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#33021;&#22815;&#23454;&#29616;&#24046;&#20998;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#31169;&#23494;&#24615;&#30830;&#20445;&#20102;&#21253;&#21547;&#25935;&#24863;&#25968;&#25454;&#30340;&#32479;&#35745;&#20998;&#26512;&#32467;&#26524;&#21487;&#20197;&#22312;&#19981;&#25439;&#23475;&#20219;&#20309;&#20010;&#20307;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21457;&#24067;&#12290;&#23454;&#29616;&#36825;&#31181;&#20445;&#35777;&#36890;&#24120;&#38656;&#35201;&#22312;&#21442;&#25968;&#20272;&#35745;&#25110;&#20272;&#35745;&#36807;&#31243;&#20013;&#30452;&#25509;&#27880;&#20837;&#22122;&#38899;&#12290;&#32780;&#37319;&#26679;&#26469;&#33258;&#36125;&#21494;&#26031;&#21518;&#39564;&#20998;&#24067;&#24050;&#34987;&#35777;&#26126;&#26159;&#25351;&#25968;&#26426;&#21046;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#33268;&#19988;&#39640;&#25928;&#30340;&#31169;&#23494;&#20272;&#35745;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#30340;&#24212;&#29992;&#21463;&#21040;&#36739;&#24378;&#30340;&#36793;&#30028;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#20551;&#35774;&#23545;&#20110;&#22522;&#26412;&#27169;&#22411;&#65288;&#22914;&#31616;&#21333;&#30340;&#32447;&#24615;&#22238;&#24402;&#22120;&#65289;&#24182;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#20174;&#24191;&#20041;&#21518;&#39564;&#20013;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#30340;&#26041;&#26696;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#27169;&#22411;&#19982;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#12290;&#36825;&#25552;&#20379;&#20102;&#31169;&#23494;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy guarantees allow the results of a statistical analysis involving sensitive data to be released without compromising the privacy of any individual taking part. Achieving such guarantees generally requires the injection of noise, either directly into parameter estimates or into the estimation process. Instead of artificially introducing perturbations, sampling from Bayesian posterior distributions has been shown to be a special case of the exponential mechanism, producing consistent, and efficient private estimates without altering the data generative process. The application of current approaches has, however, been limited by their strong bounding assumptions which do not hold for basic models, such as simple linear regressors. To ameliorate this, we propose $\beta$D-Bayes, a posterior sampling scheme from a generalised posterior targeting the minimisation of the $\beta$-divergence between the model and the data generating process. This provides private estimation t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21512;&#35268;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#36873;&#25321;&#21464;&#37327;&#22312;&#36755;&#20837;&#25968;&#25454;&#24494;&#23567;&#25200;&#21160;&#19979;&#30340;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#20540;&#24310;&#25299;&#25216;&#26415;&#39640;&#25928;&#36924;&#36817;&#35299;&#20915;&#26041;&#26696;&#36335;&#24452;&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#21512;&#35268;&#21270;&#38598;&#21512;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.05109</link><description>&lt;p&gt;
&#31232;&#30095;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21512;&#35268;&#21270;
&lt;/p&gt;
&lt;p&gt;
Conformalization of Sparse Generalized Linear Models. (arXiv:2307.05109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21512;&#35268;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#36873;&#25321;&#21464;&#37327;&#22312;&#36755;&#20837;&#25968;&#25454;&#24494;&#23567;&#25200;&#21160;&#19979;&#30340;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#20540;&#24310;&#25299;&#25216;&#26415;&#39640;&#25928;&#36924;&#36817;&#35299;&#20915;&#26041;&#26696;&#36335;&#24452;&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#21512;&#35268;&#21270;&#38598;&#21512;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#31995;&#21015;&#21487;&#35266;&#27979;&#21464;&#37327;{(x1&#65292;y1)&#65292;&#8230;&#65292;(xn&#65292;yn)}&#65292;&#21512;&#35268;&#21270;&#39044;&#27979;&#26041;&#27861;&#36890;&#36807;&#20165;&#20551;&#35774;&#25968;&#25454;&#30340;&#32852;&#21512;&#20998;&#24067;&#26159;&#32622;&#25442;&#19981;&#21464;&#30340;&#65292;&#20026;&#32473;&#23450;x_{n+1}&#20272;&#35745;y_{n+1}&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#36825;&#20010;&#32622;&#20449;&#21306;&#38388;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#26679;&#26412;&#37327;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;&#23613;&#31649;&#26377;&#21560;&#24341;&#21147;&#65292;&#22312;&#22823;&#22810;&#25968;&#22238;&#24402;&#38382;&#39064;&#20013;&#35745;&#31639;&#36825;&#26679;&#30340;&#32622;&#20449;&#21306;&#38388;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#26410;&#30693;&#21464;&#37327;y_{n+1}&#21487;&#20197;&#21462;&#26080;&#38480;&#22810;&#20010;&#21487;&#33021;&#30340;&#20505;&#36873;&#20540;&#65292;&#24182;&#19988;&#29983;&#25104;&#21512;&#35268;&#21270;&#38598;&#21512;&#38656;&#35201;&#20026;&#27599;&#20010;&#20505;&#36873;&#37325;&#26032;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20165;&#20351;&#29992;&#23376;&#38598;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#30340;&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#25968;&#20540;&#24310;&#25299;&#25216;&#26415;&#39640;&#25928;&#36924;&#36817;&#35299;&#20915;&#26041;&#26696;&#36335;&#24452;&#12290;&#25105;&#20204;&#21033;&#29992;&#30340;&#20851;&#38190;&#29305;&#24615;&#26159;&#25152;&#36873;&#21464;&#37327;&#38598;&#22312;&#36755;&#20837;&#25968;&#25454;&#30340;&#24494;&#23567;&#25200;&#21160;&#19979;&#26159;&#19981;&#21464;&#30340;&#12290;&#22240;&#27492;&#65292;&#21482;&#38656;&#35201;&#22312;&#21464;&#21270;&#28857;&#26522;&#20030;&#21644;&#37325;&#26032;&#25311;&#21512;&#27169;&#22411;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a sequence of observable variables $\{(x_1, y_1), \ldots, (x_n, y_n)\}$, the conformal prediction method estimates a confidence set for $y_{n+1}$ given $x_{n+1}$ that is valid for any finite sample size by merely assuming that the joint distribution of the data is permutation invariant. Although attractive, computing such a set is computationally infeasible in most regression problems. Indeed, in these cases, the unknown variable $y_{n+1}$ can take an infinite number of possible candidate values, and generating conformal sets requires retraining a predictive model for each candidate. In this paper, we focus on a sparse linear model with only a subset of variables for prediction and use numerical continuation techniques to approximate the solution path efficiently. The critical property we exploit is that the set of selected variables is invariant under a small perturbation of the input data. Therefore, it is sufficient to enumerate and refit the model only at the change points of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#23454;&#29616;&#36873;&#25321;&#24615;&#37319;&#26679;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#21482;&#26377;&#22122;&#22768;&#19987;&#23478;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#31639;&#27861;&#19981;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#21363;&#21487;&#25104;&#21151;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#22238;&#24402;&#21644;&#26597;&#35810;&#27425;&#25968;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.04998</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#36827;&#34892;&#36873;&#25321;&#24615;&#37319;&#26679;&#21644;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Selective Sampling and Imitation Learning via Online Regression. (arXiv:2307.04998v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#23454;&#29616;&#36873;&#25321;&#24615;&#37319;&#26679;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#21482;&#26377;&#22122;&#22768;&#19987;&#23478;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#31639;&#27861;&#19981;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#21363;&#21487;&#25104;&#21151;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#22238;&#24402;&#21644;&#26597;&#35810;&#27425;&#25968;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#20027;&#21160;&#26597;&#35810;&#22024;&#26434;&#30340;&#19987;&#23478;&#26469;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#27169;&#20223;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#20551;&#35774;&#21487;&#20197;&#33719;&#24471;&#26080;&#22122;&#22768;&#30340;&#19987;&#23478;&#21453;&#39304;&#65292;&#32780;&#36825;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#24403;&#21482;&#33021;&#33719;&#24471;&#22024;&#26434;&#30340;&#19987;&#23478;&#21453;&#39304;&#26102;&#65292;&#20381;&#36182;&#32431;&#31163;&#32447;&#25968;&#25454;&#30340;&#31639;&#27861;&#65288;&#38750;&#20132;&#20114;&#24335;IL&#65289;&#34987;&#35777;&#26126;&#38656;&#35201;&#22823;&#37327;&#30340;&#26679;&#26412;&#25165;&#33021;&#25104;&#21151;&#12290;&#30456;&#21453;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;IL&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#37319;&#26679;&#26469;&#20027;&#21160;&#26597;&#35810;&#22024;&#26434;&#30340;&#19987;&#23478;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36890;&#29992;&#20989;&#25968;&#31867;&#21644;&#22810;&#20010;&#21160;&#20316;&#30340;&#26032;&#36873;&#25321;&#24615;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#22238;&#24402;&#21644;&#26597;&#35810;&#27425;&#25968;&#30028;&#38480;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#20998;&#26512;&#25193;&#23637;&#21040;&#20102;&#20855;&#26377;&#22024;&#26434;&#19987;&#23478;&#21453;&#39304;&#30340;IL&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;IL&#31639;&#27861;&#26469;&#36827;&#34892;&#26377;&#38480;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of Imitation Learning (IL) by actively querying noisy expert for feedback. While imitation learning has been empirically successful, much of prior work assumes access to noiseless expert feedback which is not practical in many applications. In fact, when one only has access to noisy expert feedback, algorithms that rely on purely offline data (non-interactive IL) can be shown to need a prohibitively large number of samples to be successful. In contrast, in this work, we provide an interactive algorithm for IL that uses selective sampling to actively query the noisy expert for feedback. Our contributions are twofold: First, we provide a new selective sampling algorithm that works with general function classes and multiple actions, and obtains the best-known bounds for the regret and the number of queries. Next, we extend this analysis to the problem of IL with noisy expert feedback and provide a new IL algorithm that makes limited queries.  Our algorithm for sele
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#20013;&#20351;&#29992;&#24191;&#20041;&#36816;&#31639;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38750;&#32047;&#31215;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.04957</link><description>&lt;p&gt;
&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Non-Cumulative Objective. (arXiv:2307.04957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#20013;&#20351;&#29992;&#24191;&#20041;&#36816;&#31639;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38750;&#32047;&#31215;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30446;&#26631;&#20960;&#20046;&#24635;&#26159;&#23450;&#20041;&#20026;&#27839;&#36807;&#31243;&#20013;&#22870;&#21169;&#30340;\emph{&#32047;&#31215;}&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#36890;&#20449;&#21644;&#32593;&#32476;&#39046;&#22495;&#20013;&#65292;&#30446;&#26631;&#24182;&#19981;&#33258;&#28982;&#22320;&#34920;&#36798;&#20026;&#22870;&#21169;&#30340;&#27714;&#21644;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#21508;&#31181;&#38382;&#39064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#20197;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#35768;&#22810;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65306;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#12290;&#20026;&#20102;&#20248;&#21270;&#38750;&#32047;&#31215;&#30446;&#26631;&#65292;&#25105;&#20204;&#29992;&#19982;&#30446;&#26631;&#30456;&#23545;&#24212;&#30340;&#24191;&#20041;&#36816;&#31639;&#26367;&#25442;&#20102;&#36125;&#23572;&#26364;&#26356;&#26032;&#35268;&#21017;&#20013;&#30340;&#21407;&#22987;&#27714;&#21644;&#36816;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24191;&#20041;&#36816;&#31639;&#24418;&#24335;&#30340;&#36275;&#22815;&#26465;&#20214;&#20197;&#21450;&#23545;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning, the objective is almost always defined as a \emph{cumulative} function over the rewards along the process. However, there are many optimal control and reinforcement learning problems in various application fields, especially in communications and networking, where the objectives are not naturally expressed as summations of the rewards. In this paper, we recognize the prevalence of non-cumulative objectives in various problems, and propose a modification to existing algorithms for optimizing such objectives. Specifically, we dive into the fundamental building block for many optimal control and reinforcement learning algorithms: the Bellman optimality equation. To optimize a non-cumulative objective, we replace the original summation operation in the Bellman update rule with a generalized operation corresponding to the objective. Furthermore, we provide sufficient conditions on the form of the generalized operation as well as assumptions on the Markov decision 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#38544;&#39532;&#23572;&#21487;&#22827;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#30701;&#26399;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20132;&#36890;&#21464;&#37327;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;&#36825;&#31181;&#27169;&#22411;&#32467;&#21512;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#25429;&#25417;&#20132;&#36890;&#31995;&#32479;&#30340;&#22797;&#26434;&#21160;&#24577;&#27169;&#24335;&#21644;&#38750;&#24179;&#31283;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04954</link><description>&lt;p&gt;
&#28151;&#21512;&#38544;&#39532;&#23572;&#21487;&#22827;LSTM&#29992;&#20110;&#30701;&#26399;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hybrid hidden Markov LSTM for short-term traffic flow prediction. (arXiv:2307.04954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#38544;&#39532;&#23572;&#21487;&#22827;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#30701;&#26399;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20132;&#36890;&#21464;&#37327;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;&#36825;&#31181;&#27169;&#22411;&#32467;&#21512;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#25429;&#25417;&#20132;&#36890;&#31995;&#32479;&#30340;&#22797;&#26434;&#21160;&#24577;&#27169;&#24335;&#21644;&#38750;&#24179;&#31283;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20132;&#36890;&#21464;&#37327;&#30340;&#30701;&#26399;&#21644;&#36817;&#30701;&#26399;&#26410;&#26469;&#26041;&#38754;&#24050;&#32463;&#20248;&#20110;&#21442;&#25968;&#27169;&#22411;&#65292;&#22914;&#21382;&#21490;&#24179;&#22343;&#12289;ARIMA&#21644;&#20854;&#21464;&#20307;&#65292;&#36825;&#23545;&#20110;&#20132;&#36890;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21450;&#20854;&#21464;&#20307;&#65288;&#20363;&#22914;&#38271;&#30701;&#26399;&#35760;&#24518;&#65289;&#34987;&#35774;&#35745;&#29992;&#20110;&#20445;&#30041;&#38271;&#26399;&#26102;&#24207;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#29992;&#20110;&#24314;&#27169;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#22810;&#21046;&#24230;&#27169;&#22411;&#20551;&#35774;&#20132;&#36890;&#31995;&#32479;&#20197;&#19981;&#21516;&#29305;&#24449;&#30340;&#22810;&#20010;&#29366;&#24577;&#65288;&#20363;&#22914;&#30021;&#36890;&#12289;&#25317;&#22581;&#65289;&#28436;&#21464;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#19981;&#21516;&#27169;&#22411;&#20197;&#34920;&#24449;&#27599;&#20010;&#21046;&#24230;&#20869;&#30340;&#20132;&#36890;&#21160;&#24577;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#36827;&#34892;&#21046;&#24230;&#35782;&#21035;&#30340;&#39532;&#23572;&#21487;&#22827;&#20999;&#25442;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#21160;&#24577;&#27169;&#24335;&#21644;&#38750;&#24179;&#31283;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;LSTM&#37117;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20174;&#19968;&#32452;&#28508;&#22312;&#30340;&#25110;&#38544;&#34255;&#29366;&#24577;&#21464;&#37327;&#20013;&#30340;&#35266;&#23519;&#24207;&#21015;&#12290;&#22312;LSTM&#20013;&#65292;&#28508;&#22312;&#21464;&#37327;&#21487;&#20197;&#20174;&#19978;&#19968;&#20010;&#26102;&#38388;&#27493;&#30340;&#38544;&#34255;&#29366;&#24577;&#21464;&#37327;&#20256;&#36882;&#36807;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) methods have outperformed parametric models such as historical average, ARIMA and variants in predicting traffic variables into short and near-short future, that are critical for traffic management. Specifically, recurrent neural network (RNN) and its variants (e.g. long short-term memory) are designed to retain long-term temporal correlations and therefore are suitable for modeling sequences. However, multi-regime models assume the traffic system to evolve through multiple states (say, free-flow, congestion in traffic) with distinct characteristics, and hence, separate models are trained to characterize the traffic dynamics within each regime. For instance, Markov-switching models with a hidden Markov model (HMM) for regime identification is capable of capturing complex dynamic patterns and non-stationarity. Interestingly, both HMM and LSTM can be used for modeling an observation sequence from a set of latent or, hidden state variables. In LSTM, the latent variable 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#23376;&#37319;&#26679;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#20250;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2307.04841</link><description>&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Temporal Difference Reinforcement Learning. (arXiv:2307.04841v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04841
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#23376;&#37319;&#26679;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#20250;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#38656;&#35201;&#23398;&#20064;&#22312;&#21453;&#39304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#34892;&#21160;&#30340;&#22810;&#20010;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#36825;&#31181;&#32463;&#39564;&#19978;&#30340;&#25104;&#21151;&#65292;&#20173;&#28982;&#27809;&#26377;&#23545;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#21644;&#29992;&#20110;&#34920;&#31034;&#29366;&#24577;&#30340;&#29305;&#24449;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#25511;&#21046;&#23398;&#20064;&#21160;&#24577;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#26159;&#22312;&#19968;&#20010;&#39640;&#26031;&#31561;&#25928;&#20551;&#35774;&#19979;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#20854;&#20013;&#23545;&#38543;&#26426;&#36712;&#36857;&#30340;&#24179;&#22343;&#20540;&#34987;&#26367;&#25442;&#20026;&#26102;&#24577;&#30456;&#20851;&#30340;&#39640;&#26031;&#29305;&#24449;&#24179;&#22343;&#20540;&#65292;&#24182;&#19988;&#25105;&#20204;&#22312;&#23567;&#35268;&#27169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#23545;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#36827;&#34892;&#23376;&#37319;&#26679;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#65292;&#36825;&#19982;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been successful across several applications in which agents have to learn to act in environments with sparse feedback. However, despite this empirical success there is still a lack of theoretical understanding of how the parameters of reinforcement learning models and the features used to represent states interact to control the dynamics of learning. In this work, we use concepts from statistical physics, to study the typical case learning curves for temporal difference learning of a value function with linear function approximators. Our theory is derived under a Gaussian equivalence hypothesis where averages over the random trajectories are replaced with temporally correlated Gaussian feature averages and we validate our assumptions on small scale Markov Decision Processes. We find that the stochastic semi-gradient noise due to subsampling the space of possible episodes leads to significant plateaus in the value error, unlike in traditional gradient descent 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#20004;&#23618;&#21644;&#26080;&#38480;&#23485;&#24230;&#24773;&#20917;&#19979;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35757;&#32451;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#26696;&#30340;&#22823;&#25968;&#23450;&#24459;&#12290;&#36825;&#20123;&#26041;&#27861;&#37117;&#25910;&#25947;&#21040;&#30456;&#21516;&#30340;&#22343;&#22330;&#26497;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.04779</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#35757;&#32451;&#30340;&#36125;&#21494;&#26031;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#25968;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Law of Large Numbers for Bayesian two-layer Neural Network trained with Variational Inference. (arXiv:2307.04779v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#20004;&#23618;&#21644;&#26080;&#38480;&#23485;&#24230;&#24773;&#20917;&#19979;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35757;&#32451;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#26696;&#30340;&#22823;&#25968;&#23450;&#24459;&#12290;&#36825;&#20123;&#26041;&#27861;&#37117;&#25910;&#25947;&#21040;&#30456;&#21516;&#30340;&#22343;&#22330;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#35757;&#32451;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#20004;&#23618;&#21644;&#26080;&#38480;&#23485;&#24230;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#24102;&#26377;&#27491;&#21017;&#21270;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#23427;&#34987;&#20998;&#35299;&#20026;&#25968;&#25454;&#30340;&#26399;&#26395;&#23545;&#25968;&#20284;&#28982;&#21644;&#20808;&#39564;&#20998;&#24067;&#19982;&#21464;&#20998;&#21518;&#39564;&#20043;&#38388;&#30340;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#12290;&#36890;&#36807;&#36866;&#24403;&#21152;&#26435;KL&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#26696;&#30340;&#22823;&#25968;&#23450;&#24459;&#65306;&#65288;i&#65289;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#20934;&#30830;&#20272;&#35745;&#22810;&#20803;&#39640;&#26031;&#31215;&#20998;&#65292;&#65288;ii&#65289;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#30340;&#23567;&#25209;&#37327;&#26041;&#26696;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;Bayes by Backprop&#65292;&#65288;iii&#65289;&#19968;&#31181;&#26032;&#30340;&#12289;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;Minimal VI&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#32467;&#26524;&#26159;&#25152;&#26377;&#26041;&#27861;&#37117;&#25910;&#25947;&#21040;&#30456;&#21516;&#30340;&#22343;&#22330;&#26497;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#30340;&#25512;&#23548;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a rigorous analysis of training by variational inference (VI) of Bayesian neural networks in the two-layer and infinite-width case. We consider a regression problem with a regularized evidence lower bound (ELBO) which is decomposed into the expected log-likelihood of the data and the Kullback-Leibler (KL) divergence between the a priori distribution and the variational posterior. With an appropriate weighting of the KL, we prove a law of large numbers for three different training schemes: (i) the idealized case with exact estimation of a multiple Gaussian integral from the reparametrization trick, (ii) a minibatch scheme using Monte Carlo sampling, commonly known as Bayes by Backprop, and (iii) a new and computationally cheaper algorithm which we introduce as Minimal VI. An important result is that all methods converge to the same mean-field limit. Finally, we illustrate our results numerically and discuss the need for the derivation of a central limit theorem.
&lt;/p&gt;</description></item><item><title>&#26631;&#20934;&#21270;&#24402;&#19968;&#20114;&#20449;&#24687;&#26159;&#19968;&#31181;&#20559;&#20506;&#24230;&#37327;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#26465;&#20214;&#34920;&#30340;&#20449;&#24687;&#20869;&#23481;&#24182;&#19988;&#23545;&#31639;&#27861;&#36755;&#20986;&#26377;&#22122;&#22768;&#20381;&#36182;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;&#29256;&#26412;&#30340;&#20114;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23545;&#32593;&#32476;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#30340;&#27979;&#35797;&#35777;&#26126;&#20102;&#20351;&#29992;&#26080;&#20559;&#24230;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01282</link><description>&lt;p&gt;
&#26631;&#20934;&#21270;&#24402;&#19968;&#20114;&#20449;&#24687;&#26159;&#20998;&#31867;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#19968;&#31181;&#20559;&#20506;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Normalized mutual information is a biased measure for classification and community detection. (arXiv:2307.01282v1 [cs.SI] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01282
&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#21270;&#24402;&#19968;&#20114;&#20449;&#24687;&#26159;&#19968;&#31181;&#20559;&#20506;&#24230;&#37327;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#26465;&#20214;&#34920;&#30340;&#20449;&#24687;&#20869;&#23481;&#24182;&#19988;&#23545;&#31639;&#27861;&#36755;&#20986;&#26377;&#22122;&#22768;&#20381;&#36182;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;&#29256;&#26412;&#30340;&#20114;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23545;&#32593;&#32476;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#30340;&#27979;&#35797;&#35777;&#26126;&#20102;&#20351;&#29992;&#26080;&#20559;&#24230;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#24402;&#19968;&#20114;&#20449;&#24687;&#34987;&#24191;&#27867;&#29992;&#20316;&#35780;&#20272;&#32858;&#31867;&#21644;&#20998;&#31867;&#31639;&#27861;&#24615;&#33021;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#12290;&#26412;&#25991;&#34920;&#26126;&#26631;&#20934;&#21270;&#24402;&#19968;&#20114;&#20449;&#24687;&#30340;&#32467;&#26524;&#26377;&#20004;&#20010;&#20559;&#20506;&#22240;&#32032;&#65306;&#39318;&#20808;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#30053;&#20102;&#26465;&#20214;&#34920;&#30340;&#20449;&#24687;&#20869;&#23481;&#65307;&#20854;&#27425;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#23545;&#31216;&#24402;&#19968;&#21270;&#24341;&#20837;&#20102;&#23545;&#31639;&#27861;&#36755;&#20986;&#30340;&#22122;&#22768;&#20381;&#36182;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;&#29256;&#26412;&#30340;&#20114;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#32570;&#38519;&#12290;&#36890;&#36807;&#23545;&#32593;&#32476;&#31038;&#21306;&#26816;&#27979;&#20013;&#19968;&#31726;&#23376;&#27969;&#34892;&#31639;&#27861;&#36827;&#34892;&#22823;&#37327;&#25968;&#20540;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26080;&#20559;&#24230;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#26174;&#31034;&#20256;&#32479;&#20114;&#20449;&#24687;&#20013;&#30340;&#20559;&#20506;&#23545;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#30340;&#32467;&#35770;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalized mutual information is widely used as a similarity measure for evaluating the performance of clustering and classification algorithms. In this paper, we show that results returned by the normalized mutual information are biased for two reasons: first, because they ignore the information content of the contingency table and, second, because their symmetric normalization introduces spurious dependence on algorithm output. We introduce a modified version of the mutual information that remedies both of these shortcomings. As a practical demonstration of the importance of using an unbiased measure, we perform extensive numerical tests on a basket of popular algorithms for network community detection and show that one's conclusions about which algorithm is best are significantly affected by the biases in the traditional mutual information.
&lt;/p&gt;</description></item><item><title>BayesFlow&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21151;&#33021;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#36825;&#31181;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2306.16015</link><description>&lt;p&gt;
BayesFlow: &#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25674;&#36824;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
BayesFlow: Amortized Bayesian Workflows With Neural Networks. (arXiv:2306.16015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16015
&lt;/p&gt;
&lt;p&gt;
BayesFlow&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21151;&#33021;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#36825;&#31181;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36125;&#21494;&#26031;&#25512;&#26029;&#28041;&#21450;&#19968;&#31995;&#21015;&#35745;&#31639;&#25216;&#26415;&#65292;&#29992;&#20110;&#20272;&#35745;&#12289;&#39564;&#35777;&#21644;&#20174;&#27010;&#29575;&#27169;&#22411;&#20013;&#24471;&#20986;&#32467;&#35770;&#65292;&#20316;&#20026;&#25968;&#25454;&#20998;&#26512;&#20013;&#26377;&#21407;&#21017;&#30340;&#24037;&#20316;&#27969;&#30340;&#19968;&#37096;&#20998;&#12290;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#20013;&#30340;&#20856;&#22411;&#38382;&#39064;&#21253;&#25324;&#36817;&#20284;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#27169;&#22411;&#31867;&#22411;&#65292;&#20197;&#21450;&#36890;&#36807;&#22797;&#26434;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#27604;&#36739;&#21516;&#19968;&#36807;&#31243;&#30340;&#31454;&#20105;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Python&#24211;BayesFlow&#65292;&#29992;&#20110;&#22522;&#20110;&#20223;&#30495;&#35757;&#32451;&#24050;&#24314;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#25674;&#36824;&#25968;&#25454;&#21387;&#32553;&#21644;&#25512;&#26029;&#12290;&#22312;BayesFlow&#20013;&#23454;&#29616;&#30340;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#36825;&#20123;&#32593;&#32476;&#37325;&#29992;&#20110;&#27169;&#22411;&#30340;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#30001;&#20110;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#21487;&#20197;&#20960;&#20046;&#21363;&#26102;&#22320;&#25191;&#34892;&#25512;&#26029;&#65292;&#22240;&#27492;&#21069;&#26399;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#24456;&#24555;&#23601;&#33021;&#22815;&#25674;&#36824;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern Bayesian inference involves a mixture of computational techniques for estimating, validating, and drawing conclusions from probabilistic models as part of principled workflows for data analysis. Typical problems in Bayesian workflows are the approximation of intractable posterior distributions for diverse model types and the comparison of competing models of the same process in terms of their complexity and predictive performance. This manuscript introduces the Python library BayesFlow for simulation-based training of established neural network architectures for amortized data compression and inference. Amortized Bayesian inference, as implemented in BayesFlow, enables users to train custom neural networks on model simulations and re-use these networks for any subsequent application of the models. Since the trained networks can perform inference almost instantaneously, the upfront neural network training is quickly amortized.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25209;&#35268;&#33539;&#21270;&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#38544;&#24335;&#20559;&#24046;&#65292;&#24182;&#35777;&#26126;&#25209;&#35268;&#33539;&#21270;&#23545;&#20110;&#22343;&#21248;&#38388;&#38548;&#20855;&#26377;&#38544;&#21547;&#20559;&#24046;&#12290;&#36890;&#36807;&#20004;&#20010;&#20363;&#23376;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#29305;&#23450;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#22343;&#21248;&#38388;&#38548;&#20998;&#31867;&#22120;&#30340;&#34920;&#29616;&#29978;&#33267;&#20248;&#20110;&#26368;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.11680</link><description>&lt;p&gt;
&#25209;&#35268;&#33539;&#21270;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
The Implicit Bias of Batch Normalization in Linear Models and Two-layer Linear Convolutional Neural Networks. (arXiv:2306.11680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25209;&#35268;&#33539;&#21270;&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#38544;&#24335;&#20559;&#24046;&#65292;&#24182;&#35777;&#26126;&#25209;&#35268;&#33539;&#21270;&#23545;&#20110;&#22343;&#21248;&#38388;&#38548;&#20855;&#26377;&#38544;&#21547;&#20559;&#24046;&#12290;&#36890;&#36807;&#20004;&#20010;&#20363;&#23376;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#29305;&#23450;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#22343;&#21248;&#38388;&#38548;&#20998;&#31867;&#22120;&#30340;&#34920;&#29616;&#29978;&#33267;&#20248;&#20110;&#26368;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30001;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#25209;&#35268;&#33539;&#21270;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#25209;&#35268;&#33539;&#21270;&#35757;&#32451;&#20108;&#20998;&#31867;&#32447;&#24615;&#27169;&#22411;&#26102;&#65292;&#26799;&#24230;&#19979;&#38477;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#20855;&#26377;&#22343;&#21248;&#38388;&#38548;&#30340;&#20998;&#31867;&#22120;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;$\exp&#65288;- \Omega&#65288;\log ^ 2 t&#65289;&#65289;$&#12290;&#36825;&#23558;&#25209;&#35268;&#33539;&#21270;&#30340;&#32447;&#24615;&#27169;&#22411;&#19982;&#19981;&#20351;&#29992;&#25209;&#35268;&#33539;&#21270;&#30340;&#27169;&#22411;&#21306;&#20998;&#24320;&#26469;&#65292;&#20854;&#38544;&#21547;&#20559;&#24046;&#21644;&#25910;&#25947;&#36895;&#24230;&#22343;&#19981;&#21516;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#19968;&#31867;&#20004;&#23618;&#21333;&#28388;&#27874;&#22120;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#34920;&#26126;&#25209;&#35268;&#33539;&#21270;&#23545;&#20110;&#22343;&#21248;&#38388;&#38548;&#20855;&#26377;&#38544;&#21547;&#20559;&#24046;&#12290;&#36890;&#36807;&#20004;&#20010;&#20363;&#23376;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29305;&#23450;&#23398;&#20064;&#38382;&#39064;&#20013;&#22343;&#21248;&#38388;&#38548;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#21487;&#20197;&#20248;&#20110;&#26368;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;&#25209;&#35268;&#33539;&#21270;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the implicit bias of batch normalization trained by gradient descent. We show that when learning a linear model with batch normalization for binary classification, gradient descent converges to a uniform margin classifier on the training data with an $\exp(-\Omega(\log^2 t))$ convergence rate. This distinguishes linear models with batch normalization from those without batch normalization in terms of both the type of implicit bias and the convergence rate. We further extend our result to a class of two-layer, single-filter linear convolutional neural networks, and show that batch normalization has an implicit bias towards a patch-wise uniform margin. Based on two examples, we demonstrate that patch-wise uniform margin classifiers can outperform the maximum margin classifiers in certain learning problems. Our results contribute to a better theoretical understanding of batch normalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#35299;&#20915;&#24191;&#20041;&#33258;&#30001;&#33021;&#65288;FE&#65289;&#30446;&#26631;&#30340;&#21512;&#25104;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#30340;&#21464;&#20998;&#20449;&#24687;&#26356;&#26032;&#21644;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;T&#24418;&#36855;&#23467;&#23548;&#33322;&#20219;&#21153;&#30340;&#27169;&#25311;&#27604;&#36739;&#65292;&#34920;&#26126;AIF&#21487;&#24341;&#36215;&#35748;&#30693;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.02733</link><description>&lt;p&gt;
&#23454;&#29616;&#21512;&#25104;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#65292;&#31532;&#20108;&#37096;&#20998;&#65306;&#21464;&#20998;&#20449;&#24687;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Realising Synthetic Active Inference Agents, Part II: Variational Message Updates. (arXiv:2306.02733v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#35299;&#20915;&#24191;&#20041;&#33258;&#30001;&#33021;&#65288;FE&#65289;&#30446;&#26631;&#30340;&#21512;&#25104;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#30340;&#21464;&#20998;&#20449;&#24687;&#26356;&#26032;&#21644;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;T&#24418;&#36855;&#23467;&#23548;&#33322;&#20219;&#21153;&#30340;&#27169;&#25311;&#27604;&#36739;&#65292;&#34920;&#26126;AIF&#21487;&#24341;&#36215;&#35748;&#30693;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30001;&#33021;&#21407;&#29702;&#65288;FEP&#65289;&#25551;&#36848;&#29983;&#29289;&#20195;&#29702;&#36890;&#36807;&#30456;&#24212;&#29615;&#22659;&#30340;&#29983;&#25104;&#27169;&#22411;&#26368;&#23567;&#21270;&#21464;&#20998;&#33258;&#30001;&#33021;&#65288;FE&#65289;&#12290;&#20027;&#21160;&#25512;&#29702;&#65288;AIF&#65289;&#26159;FEP&#30340;&#25512;&#35770;&#65292;&#25551;&#36848;&#20102;&#20195;&#29702;&#20154;&#36890;&#36807;&#26368;&#23567;&#21270;&#26399;&#26395;&#30340;FE&#30446;&#26631;&#26469;&#25506;&#32034;&#21644;&#21033;&#29992;&#20854;&#29615;&#22659;&#12290;&#22312;&#20004;&#31687;&#30456;&#20851;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#30001;&#24418;&#24335;Forney-style&#22240;&#23376;&#22270;&#65288;FFG&#65289;&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#25551;&#36848;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21512;&#25104;AIF&#20195;&#29702;&#30340;&#35748;&#30693;&#26041;&#27861;&#12290;&#26412;&#25991;&#65288;&#31532;&#20108;&#37096;&#20998;&#65289;&#26681;&#25454;&#21464;&#20998;&#28436;&#31639;&#27861;&#65292;&#23548;&#20986;&#20102;&#26368;&#23567;&#21270;CFFG&#19978;&#65288;&#24191;&#20041;&#65289;FE&#30446;&#26631;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#12290;&#27604;&#36739;&#20102;&#27169;&#25311;Bethe&#21644;&#24191;&#20041;FE&#20195;&#29702;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#35828;&#26126;&#20102;&#21512;&#25104;AIF&#22914;&#20309;&#22312;T&#24418;&#36855;&#23467;&#23548;&#33322;&#20219;&#21153;&#19978;&#24341;&#36215;&#35748;&#30693;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;AIF&#20195;&#29702;&#30340;&#23436;&#25972;&#28040;&#24687;&#20256;&#36882;&#25551;&#36848;&#65292;&#21487;&#20197;&#25512;&#23548;&#21644;&#37325;&#29992;&#35813;&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Free Energy Principle (FEP) describes (biological) agents as minimising a variational Free Energy (FE) with respect to a generative model of their environment. Active Inference (AIF) is a corollary of the FEP that describes how agents explore and exploit their environment by minimising an expected FE objective. In two related papers, we describe a scalable, epistemic approach to synthetic AIF agents, by message passing on free-form Forney-style Factor Graphs (FFGs). A companion paper (part I) introduces a Constrained FFG (CFFG) notation that visually represents (generalised) FE objectives for AIF. The current paper (part II) derives message passing algorithms that minimise (generalised) FE objectives on a CFFG by variational calculus. A comparison between simulated Bethe and generalised FE agents illustrates how synthetic AIF induces epistemic behaviour on a T-maze navigation task. With a full message passing account of synthetic AIF agents, it becomes possible to derive and reuse 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; DISDE &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27169;&#22411;&#22312;&#19981;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;&#35813;&#26041;&#27861;&#23558;&#24615;&#33021;&#19979;&#38477;&#20998;&#35299;&#20026;&#19977;&#20010;&#26041;&#38754;&#65306;&#38590;&#24230;&#26356;&#22823;&#20294;&#26356;&#39057;&#32321;&#20986;&#29616;&#30340;&#31034;&#20363;&#22686;&#21152;&#12289;&#29305;&#24449;&#21644;&#32467;&#26524;&#20043;&#38388;&#20851;&#31995;&#30340;&#21464;&#21270;&#21644;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#39057;&#32321;&#25110;&#26410;&#35265;&#36807;&#30340;&#31034;&#20363;&#24615;&#33021;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.02011</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#35786;&#26029;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Diagnosing Model Performance Under Distribution Shift. (arXiv:2303.02011v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; DISDE &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27169;&#22411;&#22312;&#19981;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;&#35813;&#26041;&#27861;&#23558;&#24615;&#33021;&#19979;&#38477;&#20998;&#35299;&#20026;&#19977;&#20010;&#26041;&#38754;&#65306;&#38590;&#24230;&#26356;&#22823;&#20294;&#26356;&#39057;&#32321;&#20986;&#29616;&#30340;&#31034;&#20363;&#22686;&#21152;&#12289;&#29305;&#24449;&#21644;&#32467;&#26524;&#20043;&#38388;&#20851;&#31995;&#30340;&#21464;&#21270;&#21644;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#39057;&#32321;&#25110;&#26410;&#35265;&#36807;&#30340;&#31034;&#20363;&#24615;&#33021;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#27169;&#22411;&#22312;&#19981;&#21516;&#20110;&#35757;&#32451;&#20998;&#24067;&#30340;&#30446;&#26631;&#20998;&#24067;&#19979;&#36816;&#34892;&#26102;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#20123;&#25805;&#20316;&#22833;&#36133;&#27169;&#24335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31216;&#20026; DIstribution Shift DEcomposition&#65288;DISDE&#65289;&#65292;&#23558;&#24615;&#33021;&#19979;&#38477;&#24402;&#22240;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#24615;&#33021;&#19979;&#38477;&#20998;&#35299;&#20026;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#65306;1&#65289;&#26469;&#33258;&#35757;&#32451;&#30340;&#26356;&#38590;&#20294;&#26356;&#39057;&#32321;&#30340;&#31034;&#20363;&#22686;&#21152;&#65307;2&#65289;&#29305;&#24449;&#21644;&#32467;&#26524;&#20043;&#38388;&#20851;&#31995;&#30340;&#21464;&#21270;&#65307;3&#65289;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#39057;&#32321;&#25110;&#26410;&#35265;&#36807;&#30340;&#31034;&#20363;&#24615;&#33021;&#24046;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#22266;&#23450; $X$ &#30340;&#20998;&#24067;&#30340;&#21516;&#26102;&#25913;&#21464; $Y \mid X$ &#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#25110;&#22312;&#22266;&#23450; $Y \mid X$ &#30340;&#26465;&#20214;&#20998;&#24067;&#30340;&#21516;&#26102;&#25913;&#21464; $X$ &#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#19968;&#20010;&#20851;&#20110; $X$ &#30340;&#20551;&#35774;&#20998;&#24067;&#65292;&#20854;&#20013;&#21253;&#21547;&#35757;&#32451;&#21644;&#30446;&#26631;&#20013;&#20849;&#21516;&#30340;&#20540;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#27604;&#36739; $Y \mid X$ &#24182;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction models can perform poorly when deployed to target distributions different from the training distribution. To understand these operational failure modes, we develop a method, called DIstribution Shift DEcomposition (DISDE), to attribute a drop in performance to different types of distribution shifts. Our approach decomposes the performance drop into terms for 1) an increase in harder but frequently seen examples from training, 2) changes in the relationship between features and outcomes, and 3) poor performance on examples infrequent or unseen during training. These terms are defined by fixing a distribution on $X$ while varying the conditional distribution of $Y \mid X$ between training and target, or by fixing the conditional distribution of $Y \mid X$ while varying the distribution on $X$. In order to do this, we define a hypothetical distribution on $X$ consisting of values common in both training and target, over which it is easy to compare $Y \mid X$ and thus predictive
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;BBOB&#19978;&#20116;&#31181;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#20197;&#21450;CMA-ES&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;... (&#26681;&#25454;&#35770;&#25991;&#30340;&#20855;&#20307;&#20869;&#23481;&#36827;&#34892;&#24635;&#32467;)</title><link>http://arxiv.org/abs/2303.00890</link><description>&lt;p&gt;
BBOB&#19978;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of High-Dimensional Bayesian Optimization Algorithms on BBOB. (arXiv:2303.00890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;BBOB&#19978;&#20116;&#31181;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#20197;&#21450;CMA-ES&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;... (&#26681;&#25454;&#35770;&#25991;&#30340;&#20855;&#20307;&#20869;&#23481;&#36827;&#34892;&#24635;&#32467;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31867;&#22522;&#20110;&#40657;&#30418;&#12289;&#22522;&#20110;&#20195;&#29702;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20248;&#21270;&#35780;&#20272;&#25104;&#26412;&#39640;&#12289;&#21482;&#33021;&#25317;&#26377;&#26377;&#38480;&#30340;&#35780;&#20272;&#39044;&#31639;&#30340;&#38382;&#39064;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#35299;&#20915;&#24037;&#19994;&#30028;&#30340;&#25968;&#20540;&#20248;&#21270;&#38382;&#39064;&#20013;&#23588;&#20026;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#30446;&#26631;&#20989;&#25968;&#30340;&#35780;&#20272;&#36890;&#24120;&#20381;&#36182;&#32791;&#26102;&#30340;&#27169;&#25311;&#25110;&#29289;&#29702;&#23454;&#39564;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24037;&#19994;&#38382;&#39064;&#28041;&#21450;&#22823;&#37327;&#21442;&#25968;&#65292;&#36825;&#32473;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#20854;&#24615;&#33021;&#22312;&#32500;&#24230;&#36229;&#36807;15&#20010;&#21464;&#37327;&#26102;&#24120;&#24120;&#19979;&#38477;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#21738;&#31181;&#31639;&#27861;&#22312;&#21738;&#31181;&#20248;&#21270;&#22330;&#26223;&#20013;&#34920;&#29616;&#26368;&#22909;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;5&#31181;&#26368;&#26032;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#19982;&#20256;&#32479;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;CMA-ES&#31639;&#27861;&#22312;COCA&#29615;&#22659;&#19979;24&#20010;BBOB&#20989;&#25968;&#19978;&#30340;&#24615;&#33021;&#65292;&#22312;&#32500;&#24230;&#20174;10&#21040;60&#20010;&#21464;&#37327;&#19981;&#26029;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is a class of black-box, surrogate-based heuristics that can efficiently optimize problems that are expensive to evaluate, and hence admit only small evaluation budgets. BO is particularly popular for solving numerical optimization problems in industry, where the evaluation of objective functions often relies on time-consuming simulations or physical experiments. However, many industrial problems depend on a large number of parameters. This poses a challenge for BO algorithms, whose performance is often reported to suffer when the dimension grows beyond 15 variables. Although many new algorithms have been proposed to address this problem, it is not well understood which one is the best for which optimization scenario.  In this work, we compare five state-of-the-art high-dimensional BO algorithms, with vanilla BO and CMA-ES on the 24 BBOB functions of the COCO environment at increasing dimensionality, ranging from 10 to 60 variables. Our results confirm the su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LATTICE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#32858;&#31867;&#32467;&#26500;&#30340;&#28508;&#22312;bandit&#38382;&#39064;&#65292;&#24182;&#22312;&#26368;&#23567;&#21270;&#36951;&#25022;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2301.07040</link><description>&lt;p&gt;
&#20855;&#26377;&#32858;&#31867;&#32467;&#26500;&#30340;&#28508;&#22312;bandit&#38382;&#39064;&#30340;&#26368;&#20248;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Algorithms for Latent Bandits with Cluster Structure. (arXiv:2301.07040v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LATTICE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#32858;&#31867;&#32467;&#26500;&#30340;&#28508;&#22312;bandit&#38382;&#39064;&#65292;&#24182;&#22312;&#26368;&#23567;&#21270;&#36951;&#25022;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#32858;&#31867;&#32467;&#26500;&#30340;&#28508;&#22312;bandit&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#26377;&#22810;&#20010;&#29992;&#25143;&#65292;&#27599;&#20010;&#29992;&#25143;&#37117;&#26377;&#19968;&#20010;&#30456;&#20851;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36825;&#20123;&#29992;&#25143;&#34987;&#20998;&#25104;&#8220;&#28508;&#22312;&#8221;&#31751;&#65292;&#20351;&#24471;&#21516;&#19968;&#31751;&#20869;&#30340;&#29992;&#25143;&#30340;&#24179;&#22343;&#22870;&#21169;&#21521;&#37327;&#30456;&#21516;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#29992;&#25143;&#65292;&#25289;&#21160;&#19968;&#20010;&#25163;&#33218;&#24182;&#35266;&#23519;&#30456;&#24212;&#30340;&#22122;&#22768;&#22870;&#21169;&#12290;&#29992;&#25143;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#12290;&#36825;&#20010;&#38382;&#39064;&#23545;&#20110;&#23454;&#38469;&#30340;&#25512;&#33616;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#65292;&#24182;&#19988;&#26368;&#36817;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27599;&#20010;&#29992;&#25143;&#37117;&#29420;&#31435;&#34892;&#21160;&#65292;&#20182;&#20204;&#23558;&#19981;&#24471;&#19981;&#29420;&#31435;&#25506;&#32034;&#27599;&#20010;&#25163;&#33218;&#65292;&#24182;&#19988;&#19981;&#21487;&#36991;&#20813;&#22320;&#20135;&#29983;$\Omega(\sqrt{\mathsf{MNT}})$&#30340;&#36951;&#25022;&#65292;&#20854;&#20013;$\mathsf{M}$&#21644;$\mathsf{N}$&#20998;&#21035;&#26159;&#25163;&#33218;&#21644;&#29992;&#25143;&#30340;&#25968;&#37327;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LATTICE (&#36890;&#36807;&#30697;&#38453;&#34917;&#20840;&#23454;&#29616;&#30340;&#28508;&#22312;bandit&#38382;&#39064;)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#28508;&#22312;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#25552;&#20379;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of latent bandits with cluster structure where there are multiple users, each with an associated multi-armed bandit problem. These users are grouped into \emph{latent} clusters such that the mean reward vectors of users within the same cluster are identical. At each round, a user, selected uniformly at random, pulls an arm and observes a corresponding noisy reward. The goal of the users is to maximize their cumulative rewards. This problem is central to practical recommendation systems and has received wide attention of late \cite{gentile2014online, maillard2014latent}. Now, if each user acts independently, then they would have to explore each arm independently and a regret of $\Omega(\sqrt{\mathsf{MNT}})$ is unavoidable, where $\mathsf{M}, \mathsf{N}$ are the number of arms and users, respectively. Instead, we propose LATTICE (Latent bAndiTs via maTrIx ComplEtion) which allows exploitation of the latent cluster structure to provide the minimax optimal regret of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#21487;&#38752;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24773;&#22659;&#65292;&#21487;&#25193;&#23637;&#20026;&#21442;&#25968;&#21270;&#20989;&#25968;&#30340;PI&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2210.04318</link><description>&lt;p&gt;
&#20351;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Prediction intervals for neural network models using weighted asymmetric loss functions. (arXiv:2210.04318v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#21487;&#38752;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24773;&#22659;&#65292;&#21487;&#25193;&#23637;&#20026;&#21442;&#25968;&#21270;&#20989;&#25968;&#30340;PI&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#36817;&#20284;&#21644;&#39044;&#27979;&#36235;&#21183;&#30340;&#39044;&#27979;&#21306;&#38388;&#65288;PIs&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#26469;&#20272;&#35745;PI&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#65292;&#26435;&#37325;&#30001;&#21306;&#38388;&#23485;&#24230;&#30830;&#23450;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#26041;&#27861;&#30340;&#31616;&#27905;&#25968;&#23398;&#35777;&#26126;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#21040;&#20026;&#21442;&#25968;&#21270;&#20989;&#25968;&#25512;&#23548;PI&#65292;&#24182;&#35770;&#35777;&#20102;&#35813;&#26041;&#27861;&#20026;&#39044;&#27979;&#30456;&#20851;&#21464;&#37327;&#30340;PI&#32780;&#26377;&#25928;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#39044;&#27979;&#20219;&#21153;&#19978;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24773;&#22659;&#19979;&#21487;&#20197;&#20135;&#29983;&#21487;&#38752;&#30340;PI&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple and efficient approach to generate prediction intervals (PIs) for approximated and forecasted trends. Our method leverages a weighted asymmetric loss function to estimate the lower and upper bounds of the PIs, with the weights determined by the interval width. We provide a concise mathematical proof of the method, show how it can be extended to derive PIs for parametrised functions and argue why the method works for predicting PIs of dependent variables. The presented tests of the method on a real-world forecasting task using a neural network-based model show that it can produce reliable PIs in complex machine learning scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39640;&#32500;&#22122;&#22768;&#19979;&#30340;&#27969;&#24418;&#23494;&#24230;&#21644;&#20960;&#20309;&#36827;&#34892;&#31283;&#20581;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#37325;&#38543;&#26426;&#32553;&#25918;&#39640;&#26031;&#26680;&#36827;&#34892;&#26631;&#20934;&#21270;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#22122;&#22768;&#23545;&#20256;&#32479;&#26631;&#20934;&#21270;&#26041;&#27861;&#30340;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.08004</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#37325;&#38543;&#26426;&#32553;&#25918;&#26041;&#27861;&#23545;&#27969;&#24418;&#23494;&#24230;&#21644;&#20960;&#20309;&#30340;&#31283;&#20581;&#25512;&#26029; (arXiv:2209.08004v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Robust Inference of Manifold Density and Geometry by Doubly Stochastic Scaling. (arXiv:2209.08004v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39640;&#32500;&#22122;&#22768;&#19979;&#30340;&#27969;&#24418;&#23494;&#24230;&#21644;&#20960;&#20309;&#36827;&#34892;&#31283;&#20581;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#37325;&#38543;&#26426;&#32553;&#25918;&#39640;&#26031;&#26680;&#36827;&#34892;&#26631;&#20934;&#21270;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#22122;&#22768;&#23545;&#20256;&#32479;&#26631;&#20934;&#21270;&#26041;&#27861;&#30340;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#26680;&#21450;&#20854;&#20256;&#32479;&#30340;&#26631;&#20934;&#21270;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;&#34892;&#38543;&#26426;&#21270;&#65289;&#26159;&#35780;&#20272;&#25968;&#25454;&#28857;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#22122;&#22768;&#19979;&#65292;&#23427;&#20204;&#21487;&#33021;&#19981;&#20934;&#30830;&#65292;&#29305;&#21035;&#26159;&#24403;&#22122;&#22768;&#30340;&#24133;&#24230;&#22312;&#25968;&#25454;&#20013;&#21464;&#21270;&#36739;&#22823;&#26102;&#65292;&#20363;&#22914;&#22312;&#24322;&#26041;&#24046;&#24615;&#25110;&#24322;&#24120;&#20540;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26356;&#31283;&#20581;&#30340;&#26367;&#20195;&#26041;&#26696;--&#39640;&#26031;&#26680;&#30340;&#21452;&#37325;&#38543;&#26426;&#26631;&#20934;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;&#20174;&#39640;&#32500;&#31354;&#38388;&#20013;&#23884;&#20837;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#26410;&#30693;&#23494;&#24230;&#20013;&#37319;&#26679;&#30340;&#28857;&#65292;&#24182;&#19988;&#21487;&#33021;&#21463;&#21040;&#21487;&#33021;&#24378;&#28872;&#30340;&#12289;&#38750;&#21516;&#20998;&#24067;&#30340;&#12289;&#20122;&#39640;&#26031;&#22122;&#22768;&#30340;&#27745;&#26579;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21452;&#37325;&#38543;&#26426;&#20146;&#21644;&#30697;&#38453;&#21450;&#20854;&#32553;&#25918;&#22240;&#23376;&#22312;&#26576;&#20123;&#31181;&#32676;&#24418;&#24335;&#38468;&#36817;&#38598;&#20013;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#26377;&#38480;&#26679;&#26412;&#27010;&#29575;&#35823;&#24046;&#30028;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#24320;&#21457;&#20102;&#20960;&#31181;&#22312;&#19968;&#33324;&#39640;&#32500;&#22122;&#22768;&#19979;&#30340;&#31283;&#20581;&#25512;&#26029;&#24037;&#20855;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#31283;&#20581;&#23494;&#24230;...
&lt;/p&gt;
&lt;p&gt;
The Gaussian kernel and its traditional normalizations (e.g., row-stochastic) are popular approaches for assessing similarities between data points. Yet, they can be inaccurate under high-dimensional noise, especially if the noise magnitude varies considerably across the data, e.g., under heteroskedasticity or outliers. In this work, we investigate a more robust alternative -- the doubly stochastic normalization of the Gaussian kernel. We consider a setting where points are sampled from an unknown density on a low-dimensional manifold embedded in high-dimensional space and corrupted by possibly strong, non-identically distributed, sub-Gaussian noise. We establish that the doubly stochastic affinity matrix and its scaling factors concentrate around certain population forms, and provide corresponding finite-sample probabilistic error bounds. We then utilize these results to develop several tools for robust inference under general high-dimensional noise. First, we derive a robust density 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#20114;&#24335;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#20026;&#26679;&#26412;&#39640;&#25928;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2112.13487</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#20915;&#31574;&#21046;&#23450;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Statistical Complexity of Interactive Decision Making. (arXiv:2112.13487v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.13487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#20114;&#24335;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#20026;&#26679;&#26412;&#39640;&#25928;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#23398;&#20064;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#25552;&#20379;&#26679;&#26412;&#39640;&#25928;&#12289;&#33258;&#36866;&#24212;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#30340;&#36951;&#25022;&#12290;&#36825;&#20010;&#38382;&#39064;&#31867;&#20284;&#20110;&#32463;&#20856;&#30340;&#26368;&#20248;&#65288;&#30417;&#30563;&#65289;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#37027;&#37324;&#26377;&#30528;&#34987;&#24191;&#27867;&#35748;&#21487;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65288;&#20363;&#22914;VC&#32500;&#21644;Rademacher&#22797;&#26434;&#24230;&#65289;&#26469;&#25511;&#21046;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;&#24615;&#36136;&#65292;&#34920;&#24449;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#20250;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#32467;&#26524;&#25552;&#20379;&#20102;&#19968;&#31181;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#21363;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#65292;&#35813;&#24230;&#37327;&#34987;&#35777;&#26126;&#26159;&#26679;&#26412;&#39640;&#25928;&#30340;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#65306;1. &#20219;&#20309;&#20132;&#20114;&#24335;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#30340;&#26368;&#20248;&#36951;&#25022;&#30340;&#19979;&#30028;&#65292;&#30830;&#31435;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#20316;&#20026;&#19968;&#20010;&#22522;&#26412;&#38480;&#21046;&#65307;2. &#19968;&#31181;&#32479;&#35745;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#33258;&#36866;&#24212;&#24615;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental challenge in interactive learning and decision making, ranging from bandit problems to reinforcement learning, is to provide sample-efficient, adaptive learning algorithms that achieve near-optimal regret. This question is analogous to the classical problem of optimal (supervised) statistical learning, where there are well-known complexity measures (e.g., VC dimension and Rademacher complexity) that govern the statistical complexity of learning. However, characterizing the statistical complexity of interactive learning is substantially more challenging due to the adaptive nature of the problem. The main result of this work provides a complexity measure, the Decision-Estimation Coefficient, that is proven to be both necessary and sufficient for sample-efficient interactive learning. In particular, we provide:  1. a lower bound on the optimal regret for any interactive decision making problem, establishing the Decision-Estimation Coefficient as a fundamental limit.  2. a un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#20013;&#30340;&#20004;&#31181;&#38543;&#26426;&#31639;&#27861;&#65292;GLM-TSL&#21644;GLM-FPL&#12290;GLM-TSL&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65292;GLM-FPL&#21017;&#23558;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#25311;&#21512;&#21040;&#36807;&#21435;&#22870;&#21169;&#30340;&#38543;&#26426;&#25200;&#21160;&#21382;&#21490;&#20013;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20004;&#31181;&#31639;&#27861;&#24182;&#24471;&#20986;&#20102;&#23427;&#20204;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#27492;&#21069;&#30340;&#24037;&#20316;&#20013;&#30340;&#36951;&#25022;&#19978;&#30028;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#24182;&#19988;&#23545;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#38382;&#39064;&#65292;GLM-FPL&#26159;&#39318;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#22312;&#36923;&#36753;&#36172;&#33218;&#38382;&#39064;&#21644;&#31070;&#32463;&#32593;&#32476;&#36172;&#33218;&#38382;&#39064;&#19978;&#23545;&#36825;&#20004;&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#38543;&#26426;&#21270;&#22312;&#25506;&#32034;&#20013;&#30340;&#20316;&#29992;&#65292;&#36229;&#36234;&#20102;&#20165;&#20165;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/1906.08947</link><description>&lt;p&gt;
&#22312;&#24191;&#20041;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#20013;&#30340;&#38543;&#26426;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Randomized Exploration in Generalized Linear Bandits. (arXiv:1906.08947v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1906.08947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#20013;&#30340;&#20004;&#31181;&#38543;&#26426;&#31639;&#27861;&#65292;GLM-TSL&#21644;GLM-FPL&#12290;GLM-TSL&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65292;GLM-FPL&#21017;&#23558;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#25311;&#21512;&#21040;&#36807;&#21435;&#22870;&#21169;&#30340;&#38543;&#26426;&#25200;&#21160;&#21382;&#21490;&#20013;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20004;&#31181;&#31639;&#27861;&#24182;&#24471;&#20986;&#20102;&#23427;&#20204;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#27492;&#21069;&#30340;&#24037;&#20316;&#20013;&#30340;&#36951;&#25022;&#19978;&#30028;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#24182;&#19988;&#23545;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#38382;&#39064;&#65292;GLM-FPL&#26159;&#39318;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#22312;&#36923;&#36753;&#36172;&#33218;&#38382;&#39064;&#21644;&#31070;&#32463;&#32593;&#32476;&#36172;&#33218;&#38382;&#39064;&#19978;&#23545;&#36825;&#20004;&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#38543;&#26426;&#21270;&#22312;&#25506;&#32034;&#20013;&#30340;&#20316;&#29992;&#65292;&#36229;&#36234;&#20102;&#20165;&#20165;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#24191;&#20041;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#30340;&#38543;&#26426;&#31639;&#27861;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;GLM-TSL&#20174;&#21518;&#39564;&#20998;&#24067;&#30340;&#25289;&#26222;&#25289;&#26031;&#25311;&#21512;&#20013;&#37319;&#26679;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;(GLM)&#12290;&#31532;&#20108;&#31181;&#31639;&#27861;GLM-FPL&#23558;&#19968;&#20010;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#25311;&#21512;&#21040;&#36807;&#21435;&#22870;&#21169;&#30340;&#38543;&#26426;&#25200;&#21160;&#21382;&#21490;&#20013;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20004;&#31181;&#31639;&#27861;&#65292;&#24182;&#24471;&#20986;&#20102;&#23427;&#20204;&#22312;n&#36718;&#20013;&#36951;&#25022;&#19978;&#30028;$\tilde{O}(d \sqrt{n \log K})$&#65292;&#20854;&#20013;$d$&#26159;&#29305;&#24449;&#30340;&#25968;&#37327;&#65292;$K$&#26159;&#33218;&#30340;&#25968;&#37327;&#12290;&#21069;&#32773;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#32780;&#21518;&#32773;&#26159;&#38750;&#32447;&#24615;&#27169;&#22411;&#20013;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#22312;&#36923;&#36753;&#36172;&#33218;&#38382;&#39064;&#20013;&#23545;GLM-TSL&#21644;GLM-FPL&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#23558;GLM-FPL&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#36172;&#33218;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#25506;&#32034;&#20013;&#38543;&#26426;&#21270;&#30340;&#20316;&#29992;&#65292;&#19981;&#20165;&#20165;&#26159;&#21518;&#39564;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study two randomized algorithms for generalized linear bandits. The first, GLM-TSL, samples a generalized linear model (GLM) from the Laplace approximation to the posterior distribution. The second, GLM-FPL, fits a GLM to a randomly perturbed history of past rewards. We analyze both algorithms and derive $\tilde{O}(d \sqrt{n \log K})$ upper bounds on their $n$-round regret, where $d$ is the number of features and $K$ is the number of arms. The former improves on prior work while the latter is the first for Gaussian noise perturbations in non-linear models. We empirically evaluate both GLM-TSL and GLM-FPL in logistic bandits, and apply GLM-FPL to neural network bandits. Our work showcases the role of randomization, beyond posterior sampling, in exploration.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20110;&#20854;&#24178;&#25200;&#21382;&#21490;&#30340;&#32447;&#24615;&#27169;&#22411;&#19978;&#36873;&#25321;&#20272;&#35745;&#22870;&#21169;&#26368;&#39640;&#30340;&#33218;&#65292;&#29992;&#20110;&#22312;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#26368;&#23567;&#21270;&#32047;&#31215;&#36951;&#25022;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#31639;&#27861;&#36951;&#25022;&#30340;&#36739;&#22909;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/1903.09132</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#30340;&#24178;&#25200;&#21382;&#21490;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Perturbed-History Exploration in Stochastic Linear Bandits. (arXiv:1903.09132v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1903.09132
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20110;&#20854;&#24178;&#25200;&#21382;&#21490;&#30340;&#32447;&#24615;&#27169;&#22411;&#19978;&#36873;&#25321;&#20272;&#35745;&#22870;&#21169;&#26368;&#39640;&#30340;&#33218;&#65292;&#29992;&#20110;&#22312;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#26368;&#23567;&#21270;&#32047;&#31215;&#36951;&#25022;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#31639;&#27861;&#36951;&#25022;&#30340;&#36739;&#22909;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#26368;&#23567;&#21270;&#32047;&#31215;&#36951;&#25022;&#12290;&#35813;&#31639;&#27861;&#22312;&#35757;&#32451;&#20110;&#20854;&#24178;&#25200;&#21382;&#21490;&#30340;&#32447;&#24615;&#27169;&#22411;&#19978;&#36873;&#25321;&#20272;&#35745;&#22870;&#21169;&#26368;&#39640;&#30340;&#33218;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#30340;&#24178;&#25200;&#21382;&#21490;&#25506;&#32034;&#65288;LinPHE&#65289;&#12290;&#25152;&#35859;&#24178;&#25200;&#21382;&#21490;&#26159;&#25351;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#21644;&#38543;&#26426;&#29983;&#25104;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20266;&#22870;&#21169;&#30340;&#28151;&#21512;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#23545;&#20110;LinPHE&#30340;$n$&#36718;&#36951;&#25022;&#65292;&#20854;&#20013;$d$&#26159;&#29305;&#24449;&#25968;&#37327;&#65292;&#26377;&#19968;&#20010;$\tilde{O}(d \sqrt{n})$&#30340;&#38388;&#38553;&#33258;&#30001;&#30028;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#27493;&#39588;&#26159;&#20851;&#20110;&#20271;&#21162;&#21033;&#38543;&#26426;&#21464;&#37327;&#30340;&#21152;&#26435;&#21644;&#30340;&#26032;&#30340;&#38598;&#20013;&#21644;&#21453;&#38598;&#20013;&#36793;&#30028;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#35774;&#35745;&#30340;&#26222;&#36941;&#24615;&#65292;&#25105;&#20204;&#23558;LinPHE&#25512;&#24191;&#21040;&#19968;&#20010;&#36923;&#36753;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new online algorithm for cumulative regret minimization in a stochastic linear bandit. The algorithm pulls the arm with the highest estimated reward in a linear model trained on its perturbed history. Therefore, we call it perturbed-history exploration in a linear bandit (LinPHE). The perturbed history is a mixture of observed rewards and randomly generated i.i.d. pseudo-rewards. We derive a $\tilde{O}(d \sqrt{n})$ gap-free bound on the $n$-round regret of LinPHE, where $d$ is the number of features. The key steps in our analysis are new concentration and anti-concentration bounds on the weighted sum of Bernoulli random variables. To show the generality of our design, we generalize LinPHE to a logistic model. We evaluate our algorithms empirically and show that they are practical.
&lt;/p&gt;</description></item></channel></rss>