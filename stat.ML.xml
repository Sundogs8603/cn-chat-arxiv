<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#24230;&#37327;&#33021;&#22815;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#38405;&#35835;&#36895;&#24230;&#65292;&#20026;&#26410;&#26469;&#25972;&#21512;LLMs&#21644;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.15822</link><description>&lt;p&gt;
&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Computational Sentence-level Metrics Predicting Human Sentence Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#24230;&#37327;&#33021;&#22815;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#38405;&#35835;&#36895;&#24230;&#65292;&#20026;&#26410;&#26469;&#25972;&#21512;LLMs&#21644;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#24515;&#29702;&#35821;&#35328;&#23398;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#21333;&#35789;&#22788;&#29702;&#19978;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#12290;&#24320;&#21457;&#30340;&#24230;&#37327;&#21253;&#25324;&#21477;&#23376;&#24847;&#22806;&#24615;&#21644;&#21477;&#23376;&#30456;&#20851;&#24615;&#65292;&#28982;&#21518;&#32463;&#36807;&#27979;&#35797;&#21644;&#27604;&#36739;&#20197;&#39564;&#35777;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;&#20154;&#31867;&#22914;&#20309;&#36328;&#35821;&#35328;&#25972;&#20307;&#29702;&#35299;&#21477;&#23376;&#12290;&#36825;&#20123;&#24230;&#37327;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#38405;&#35835;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#35745;&#31639;&#30340;&#21477;&#23376;&#32423;&#24230;&#37327;&#22312;&#39044;&#27979;&#21644;&#38416;&#26126;&#35835;&#32773;&#22312;&#29702;&#35299;&#25972;&#20307;&#21477;&#23376;&#26102;&#36935;&#21040;&#30340;&#22788;&#29702;&#22256;&#38590;&#26041;&#38754;&#24322;&#24120;&#26377;&#25928;&#65292;&#21487;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#12290;&#23427;&#20204;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#20026;&#26410;&#26469;&#22312;&#25972;&#21512;LLMs&#21644;&#35748;&#30693;&#31185;&#23398;&#26041;&#38754;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15822v1 Announce Type: new  Abstract: The majority of research in computational psycholinguistics has concentrated on the processing of words. This study introduces innovative methods for computing sentence-level metrics using multilingual large language models. The metrics developed sentence surprisal and sentence relevance and then are tested and compared to validate whether they can predict how humans comprehend sentences as a whole across languages. These metrics offer significant interpretability and achieve high accuracy in predicting human sentence reading speeds. Our results indicate that these computational sentence-level metrics are exceptionally effective at predicting and elucidating the processing difficulties encountered by readers in comprehending sentences as a whole across a variety of languages. Their impressive performance and generalization capabilities provide a promising avenue for future research in integrating LLMs and cognitive science.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36890;&#29992;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#25512;&#26029;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#25968;&#25454;&#29615;&#22659;&#20013;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#26102;&#30340;&#35745;&#31639;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03007</link><description>&lt;p&gt;
&#36890;&#29992;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Scalable Bayesian inference for the generalized linear mixed model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03007
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36890;&#29992;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#25512;&#26029;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#25968;&#25454;&#29615;&#22659;&#20013;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#26102;&#30340;&#35745;&#31639;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;&#65288;GLMM&#65289;&#26159;&#22788;&#29702;&#30456;&#20851;&#25968;&#25454;&#30340;&#19968;&#31181;&#27969;&#34892;&#32479;&#35745;&#26041;&#27861;&#65292;&#22312;&#21253;&#25324;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#31561;&#22823;&#25968;&#25454;&#24120;&#35265;&#30340;&#24212;&#29992;&#39046;&#22495;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#38024;&#23545;GLMM&#30340;&#21487;&#25193;&#23637;&#32479;&#35745;&#25512;&#26029;&#65292;&#25105;&#20204;&#23558;&#32479;&#35745;&#25512;&#26029;&#23450;&#20041;&#20026;&#65306;&#65288;i&#65289;&#23545;&#24635;&#20307;&#21442;&#25968;&#30340;&#20272;&#35745;&#20197;&#21450;&#65288;ii&#65289;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#31185;&#23398;&#20551;&#35774;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23398;&#20064;&#31639;&#27861;&#25797;&#38271;&#21487;&#25193;&#23637;&#30340;&#32479;&#35745;&#20272;&#35745;&#65292;&#20294;&#24456;&#23569;&#21253;&#25324;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36125;&#21494;&#26031;&#25512;&#26029;&#25552;&#20379;&#23436;&#25972;&#30340;&#32479;&#35745;&#25512;&#26029;&#65292;&#22240;&#20026;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33258;&#21160;&#26469;&#33258;&#21518;&#39564;&#20998;&#24067;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21253;&#25324;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#22312;&#20869;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#31639;&#27861;&#22312;&#22823;&#25968;&#25454;&#29615;&#22659;&#20013;&#21464;&#24471;&#38590;&#20197;&#35745;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#35745;&#25512;&#26029;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03007v1 Announce Type: cross  Abstract: The generalized linear mixed model (GLMM) is a popular statistical approach for handling correlated data, and is used extensively in applications areas where big data is common, including biomedical data settings. The focus of this paper is scalable statistical inference for the GLMM, where we define statistical inference as: (i) estimation of population parameters, and (ii) evaluation of scientific hypotheses in the presence of uncertainty. Artificial intelligence (AI) learning algorithms excel at scalable statistical estimation, but rarely include uncertainty quantification. In contrast, Bayesian inference provides full statistical inference, since uncertainty quantification results automatically from the posterior distribution. Unfortunately, Bayesian inference algorithms, including Markov Chain Monte Carlo (MCMC), become computationally intractable in big data settings. In this paper, we introduce a statistical inference algorithm 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#22312;&#37327;&#23376;&#20648;&#22791;&#35745;&#31639;&#20013;&#22238;&#22768;&#24577;&#24615;&#36136;&#30340;&#19981;&#21516;&#23618;&#27425;&#65292;&#21253;&#25324;&#38750;&#24179;&#31283;&#24615;ESP&#21644;&#23376;&#31995;&#32479;&#20855;&#26377;ESP&#30340;&#23376;&#31354;&#38388;/&#23376;&#38598;ESP&#12290;&#36827;&#34892;&#20102;&#25968;&#20540;&#28436;&#31034;&#21644;&#35760;&#24518;&#23481;&#37327;&#35745;&#31639;&#20197;&#39564;&#35777;&#36825;&#20123;&#23450;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.02686</link><description>&lt;p&gt;
&#37327;&#23376;&#20648;&#22791;&#35745;&#31639;&#20013;&#30340;&#22238;&#22768;&#24577;&#24615;&#36136;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Hierarchy of the echo state property in quantum reservoir computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02686
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#22312;&#37327;&#23376;&#20648;&#22791;&#35745;&#31639;&#20013;&#22238;&#22768;&#24577;&#24615;&#36136;&#30340;&#19981;&#21516;&#23618;&#27425;&#65292;&#21253;&#25324;&#38750;&#24179;&#31283;&#24615;ESP&#21644;&#23376;&#31995;&#32479;&#20855;&#26377;ESP&#30340;&#23376;&#31354;&#38388;/&#23376;&#38598;ESP&#12290;&#36827;&#34892;&#20102;&#25968;&#20540;&#28436;&#31034;&#21644;&#35760;&#24518;&#23481;&#37327;&#35745;&#31639;&#20197;&#39564;&#35777;&#36825;&#20123;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#24577;&#24615;&#36136;&#65288;ESP&#65289;&#20195;&#34920;&#20102;&#20648;&#22791;&#35745;&#31639;&#65288;RC&#65289;&#26694;&#26550;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#21021;&#22987;&#29366;&#24577;&#21644;&#36828;&#26399;&#36755;&#20837;&#19981;&#21152;&#27495;&#35270;&#26469;&#30830;&#20445;&#20648;&#33988;&#32593;&#32476;&#30340;&#20165;&#36755;&#20986;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ESP&#23450;&#20041;&#24182;&#26410;&#25551;&#36848;&#21487;&#33021;&#28436;&#21464;&#32479;&#35745;&#23646;&#24615;&#30340;&#38750;&#24179;&#31283;&#31995;&#32479;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31867;&#26032;&#30340;ESP&#65306;\textit{&#38750;&#24179;&#31283;ESP}&#65292;&#29992;&#20110;&#28508;&#22312;&#38750;&#24179;&#31283;&#31995;&#32479;&#65292;&#21644;\textit{&#23376;&#31354;&#38388;/&#23376;&#38598;ESP}&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;ESP&#30340;&#23376;&#31995;&#32479;&#30340;&#31995;&#32479;&#12290;&#26681;&#25454;&#36825;&#20123;&#23450;&#20041;&#65292;&#25105;&#20204;&#22312;&#37327;&#23376;&#20648;&#22791;&#35745;&#31639;&#65288;QRC&#65289;&#26694;&#26550;&#20013;&#25968;&#20540;&#28436;&#31034;&#20102;&#38750;&#24179;&#31283;ESP&#19982;&#20856;&#22411;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#21644;&#20351;&#29992;&#38750;&#32447;&#24615;&#33258;&#22238;&#24402;&#31227;&#21160;&#24179;&#22343;&#65288;NARMA&#65289;&#20219;&#21153;&#30340;&#36755;&#20837;&#32534;&#30721;&#26041;&#27861;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#35745;&#31639;&#32447;&#24615;/&#38750;&#32447;&#24615;&#35760;&#24518;&#23481;&#37327;&#26469;&#30830;&#35748;&#36825;&#31181;&#23545;&#24212;&#20851;&#31995;&#65292;&#20197;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02686v1 Announce Type: cross  Abstract: The echo state property (ESP) represents a fundamental concept in the reservoir computing (RC) framework that ensures output-only training of reservoir networks by being agnostic to the initial states and far past inputs. However, the traditional definition of ESP does not describe possible non-stationary systems in which statistical properties evolve. To address this issue, we introduce two new categories of ESP: \textit{non-stationary ESP}, designed for potentially non-stationary systems, and \textit{subspace/subset ESP}, designed for systems whose subsystems have ESP. Following the definitions, we numerically demonstrate the correspondence between non-stationary ESP in the quantum reservoir computer (QRC) framework with typical Hamiltonian dynamics and input encoding methods using non-linear autoregressive moving-average (NARMA) tasks. We also confirm the correspondence by computing linear/non-linear memory capacities that quantify 
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65288;GEnBP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#39640;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.08193</link><description>&lt;p&gt;
&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#30340;&#39640;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08193
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65288;GEnBP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#39640;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#25512;&#26029;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#65288;GEnBP&#65289;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#65288;GaBP&#65289;&#26041;&#27861;&#30340;&#32467;&#21512;&#12290;GEnBP&#36890;&#36807;&#22312;&#22270;&#27169;&#22411;&#32467;&#26500;&#20013;&#20256;&#36882;&#20302;&#31209;&#26412;&#22320;&#20449;&#24687;&#26469;&#26356;&#26032;&#38598;&#25104;&#27169;&#22411;&#12290;&#36825;&#31181;&#32452;&#21512;&#32487;&#25215;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26377;&#21033;&#29305;&#24615;&#12290;&#38598;&#25104;&#25216;&#26415;&#20351;&#24471;GEnBP&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#12289;&#22024;&#26434;&#30340;&#40657;&#31665;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#22270;&#27169;&#22411;&#32467;&#26500;&#20013;&#20351;&#29992;&#26412;&#22320;&#20449;&#24687;&#30830;&#20445;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#24182;&#33021;&#39640;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;&#24403;&#38598;&#25104;&#22823;&#23567;&#36828;&#23567;&#20110;&#25512;&#26029;&#32500;&#24230;&#26102;&#65292;GEnBP&#29305;&#21035;&#26377;&#20248;&#21183;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#31354;&#26102;&#24314;&#27169;&#12289;&#22270;&#20687;&#22788;&#29702;&#21644;&#29289;&#29702;&#27169;&#22411;&#21453;&#28436;&#31561;&#39046;&#22495;&#32463;&#24120;&#20986;&#29616;&#12290;GEnBP&#21487;&#20197;&#24212;&#29992;&#20110;&#19968;&#33324;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient inference in high-dimensional models remains a central challenge in machine learning. This paper introduces the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing low-rank local messages in a graphical model structure. This combination inherits favourable qualities from each method. Ensemble techniques allow GEnBP to handle high-dimensional states, parameters and intricate, noisy, black-box generation processes. The use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures. GEnBP is particularly advantageous when the ensemble size is considerably smaller than the inference dimension. This scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion. GEnBP can be applied to general problem s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36890;&#36807;&#36890;&#20449;&#32593;&#32476;&#36830;&#25509;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;&#20998;&#24067;&#24335;(&#38750;)&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#24314;&#31435;&#20102;&#39057;&#29575;&#29305;&#24615;&#65292;&#25506;&#35752;&#20102;&#22312;&#36866;&#24403;&#20551;&#35774;&#19979;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#21442;&#25968;&#25928;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#36890;&#20449;&#22270;&#35774;&#35745;&#21644;&#22823;&#23567;&#23545;&#21518;&#39564;&#25910;&#32553;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2311.08214</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;(&#38750;)&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#39057;&#29575;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Frequentist Guarantees of Distributed (Non)-Bayesian Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36890;&#36807;&#36890;&#20449;&#32593;&#32476;&#36830;&#25509;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;&#20998;&#24067;&#24335;(&#38750;)&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#24314;&#31435;&#20102;&#39057;&#29575;&#29305;&#24615;&#65292;&#25506;&#35752;&#20102;&#22312;&#36866;&#24403;&#20551;&#35774;&#19979;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#21442;&#25968;&#25928;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#36890;&#20449;&#22270;&#35774;&#35745;&#21644;&#22823;&#23567;&#23545;&#21518;&#39564;&#25910;&#32553;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20998;&#26512;&#22823;&#22411;&#20998;&#25955;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#25512;&#21160;&#65292;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#25512;&#26029;&#24050;&#25104;&#20026;&#36328;&#22810;&#20010;&#39046;&#22495;&#65288;&#21253;&#25324;&#32479;&#35745;&#23398;&#12289;&#30005;&#27668;&#24037;&#31243;&#21644;&#32463;&#27982;&#23398;&#65289;&#30340;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#38024;&#23545;&#36890;&#36807;&#36890;&#20449;&#32593;&#32476;&#36830;&#25509;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;&#20998;&#24067;&#24335;(&#38750;)&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#24314;&#31435;&#20102;&#39057;&#29575;&#29305;&#24615;&#65292;&#22914;&#21518;&#39564;&#19968;&#33268;&#24615;&#12289;&#28176;&#36817;&#27491;&#24577;&#24615;&#21644;&#21518;&#39564;&#25910;&#32553;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36890;&#20449;&#22270;&#19978;&#30340;&#36866;&#24403;&#20551;&#35774;&#19979;&#65292;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#25512;&#26029;&#20445;&#30041;&#20102;&#21442;&#25968;&#25928;&#29575;&#65292;&#21516;&#26102;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#38754;&#22686;&#24378;&#20102;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#30740;&#31350;&#35774;&#35745;&#21644;&#36890;&#20449;&#22270;&#30340;&#22823;&#23567;&#22914;&#20309;&#24433;&#21709;&#21518;&#39564;&#25910;&#32553;&#29575;&#26469;&#25506;&#35752;&#20102;&#32479;&#35745;&#25928;&#29575;&#21644;&#36890;&#20449;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#26102;&#21464;&#22270;&#65292;&#24182;&#23558;&#32467;&#26524;&#24212;&#29992;&#20110;&#25351;&#25968;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08214v2 Announce Type: replace-cross  Abstract: Motivated by the need to analyze large, decentralized datasets, distributed Bayesian inference has become a critical research area across multiple fields, including statistics, electrical engineering, and economics. This paper establishes Frequentist properties, such as posterior consistency, asymptotic normality, and posterior contraction rates, for the distributed (non-)Bayes Inference problem among agents connected via a communication network. Our results show that, under appropriate assumptions on the communication graph, distributed Bayesian inference retains parametric efficiency while enhancing robustness in uncertainty quantification. We also explore the trade-off between statistical efficiency and communication efficiency by examining how the design and size of the communication graph impact the posterior contraction rate. Furthermore, We extend our analysis to time-varying graphs and apply our results to exponential f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#22686;&#24378;&#37319;&#26679;&#12289;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#20027;&#21160;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#21183;&#33021;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#37327;&#23376;&#31934;&#24230;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21450;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2401.16487</link><description>&lt;p&gt;
&#27963;&#24615;&#23398;&#20064;&#29627;&#23572;&#20857;&#26364;&#37319;&#26679;&#22120;&#21644;&#20855;&#26377;&#37327;&#23376;&#21147;&#23398;&#31934;&#24230;&#30340;&#21183;&#33021;
&lt;/p&gt;
&lt;p&gt;
Active learning of Boltzmann samplers and potential energies with quantum mechanical accuracy. (arXiv:2401.16487v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16487
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#22686;&#24378;&#37319;&#26679;&#12289;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#20027;&#21160;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#21183;&#33021;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#37327;&#23376;&#31934;&#24230;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21450;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29289;&#29702;&#23398;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#26469;&#35828;&#65292;&#25552;&#21462;&#20998;&#23376;&#31995;&#32479;&#30456;&#20851;&#33258;&#30001;&#33021;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#19968;&#33268;&#32479;&#35745;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#21487;&#20197;&#24110;&#21161;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#65292;&#20294;&#23545;&#20110;&#38656;&#35201;&#37327;&#23376;&#31934;&#24230;&#30340;&#31995;&#32479;&#32780;&#35328;&#65292;&#35745;&#31639;&#20195;&#20215;&#24456;&#39640;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#32467;&#21512;&#22686;&#24378;&#37319;&#26679;&#12289;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#20027;&#21160;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#21183;&#33021;&#65288;MLP&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26694;&#26550;&#65292;&#20351;&#24471;&#27599;&#20010;&#29366;&#24577;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#27491;&#21017;&#21270;&#27969;&#65288;NF&#65289;&#21644;&#19968;&#20010;MLP&#12290;&#25105;&#20204;&#24182;&#34892;&#27169;&#25311;&#22810;&#20010;&#39532;&#23572;&#31185;&#22827;&#38142;&#30452;&#21040;&#25910;&#25947;&#65292;&#20351;&#29992;&#39640;&#25928;&#30340;&#33021;&#37327;&#35780;&#20272;&#20174;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#35745;&#31639;NF&#29983;&#25104;&#30340;&#37197;&#32622;&#23376;&#38598;&#30340;&#33021;&#37327;&#65292;&#29992;MLP&#39044;&#27979;&#21097;&#20313;&#37197;&#32622;&#30340;&#33021;&#37327;&#65292;&#24182;&#20351;&#29992;DFT&#35745;&#31639;&#24471;&#21040;&#30340;&#33021;&#37327;&#23545;MLP&#36827;&#34892;&#20027;&#21160;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting consistent statistics between relevant free-energy minima of a molecular system is essential for physics, chemistry and biology. Molecular dynamics (MD) simulations can aid in this task but are computationally expensive, especially for systems that require quantum accuracy. To overcome this challenge, we develop an approach combining enhanced sampling with deep generative models and active learning of a machine learning potential (MLP). We introduce an adaptive Markov chain Monte Carlo framework that enables the training of one Normalizing Flow (NF) and one MLP per state. We simulate several Markov chains in parallel until they reach convergence, sampling the Boltzmann distribution with an efficient use of energy evaluations. At each iteration, we compute the energy of a subset of the NF-generated configurations using Density Functional Theory (DFT), we predict the remaining configuration's energy with the MLP and actively train the MLP using the DFT-computed energies. Lever
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#65292;&#24179;&#34913;&#32676;&#20307;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;BGATE&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22788;&#29702;&#22312;&#32676;&#20307;&#38388;&#30340;&#25928;&#24212;&#24046;&#24322;&#65292;&#35813;&#21442;&#25968;&#22522;&#20110;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#31163;&#25955;&#22788;&#29702;&#36827;&#34892;&#20272;&#35745;&#12290;&#36890;&#36807;&#27604;&#36739;&#20004;&#20010;BGATE&#30340;&#24046;&#24322;&#65292;&#33021;&#26356;&#22909;&#22320;&#20998;&#26512;&#22788;&#29702;&#30340;&#24322;&#36136;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08290</link><description>&lt;p&gt;
&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#20013;&#20171;&#25928;&#24212;&#12290; (arXiv:2401.08290v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
Causal Machine Learning for Moderation Effects. (arXiv:2401.08290v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#65292;&#24179;&#34913;&#32676;&#20307;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;BGATE&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22788;&#29702;&#22312;&#32676;&#20307;&#38388;&#30340;&#25928;&#24212;&#24046;&#24322;&#65292;&#35813;&#21442;&#25968;&#22522;&#20110;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#31163;&#25955;&#22788;&#29702;&#36827;&#34892;&#20272;&#35745;&#12290;&#36890;&#36807;&#27604;&#36739;&#20004;&#20010;BGATE&#30340;&#24046;&#24322;&#65292;&#33021;&#26356;&#22909;&#22320;&#20998;&#26512;&#22788;&#29702;&#30340;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20219;&#20309;&#20915;&#31574;&#32773;&#26469;&#35828;&#65292;&#20102;&#35299;&#20915;&#31574;&#65288;&#22788;&#29702;&#65289;&#23545;&#25972;&#20307;&#21644;&#23376;&#32676;&#30340;&#24433;&#21709;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#12290;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26368;&#36817;&#25552;&#20379;&#20102;&#29992;&#20110;&#20272;&#35745;&#32676;&#20307;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;GATE&#65289;&#30340;&#24037;&#20855;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22788;&#29702;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32771;&#34385;&#20854;&#20182;&#21327;&#21464;&#37327;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#35299;&#37322;&#32676;&#20307;&#38388;&#22788;&#29702;&#25928;&#24212;&#24046;&#24322;&#30340;&#38590;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21442;&#25968;&#65292;&#21363;&#24179;&#34913;&#32676;&#20307;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;BGATE&#65289;&#65292;&#23427;&#34913;&#37327;&#20102;&#20855;&#26377;&#29305;&#23450;&#20998;&#24067;&#30340;&#20808;&#39564;&#30830;&#23450;&#21327;&#21464;&#37327;&#30340;GATE&#12290;&#36890;&#36807;&#27604;&#36739;&#20004;&#20010;BGATE&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#26377;&#24847;&#20041;&#22320;&#20998;&#26512;&#24322;&#36136;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#27604;&#36739;&#20004;&#20010;GATE&#12290;&#36825;&#20010;&#21442;&#25968;&#30340;&#20272;&#35745;&#31574;&#30053;&#26159;&#22522;&#20110;&#26080;&#28151;&#28102;&#35774;&#32622;&#20013;&#31163;&#25955;&#22788;&#29702;&#30340;&#21452;&#37325;/&#21435;&#20559;&#26426;&#22120;&#23398;&#20064;&#65292;&#35813;&#20272;&#35745;&#37327;&#22312;&#26631;&#20934;&#26465;&#20214;&#19979;&#34920;&#29616;&#20026;$\sqrt{N}$&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#28155;&#21152;&#39069;&#22806;&#30340;&#26631;&#35782;
&lt;/p&gt;
&lt;p&gt;
It is valuable for any decision maker to know the impact of decisions (treatments) on average and for subgroups. The causal machine learning literature has recently provided tools for estimating group average treatment effects (GATE) to understand treatment heterogeneity better. This paper addresses the challenge of interpreting such differences in treatment effects between groups while accounting for variations in other covariates. We propose a new parameter, the balanced group average treatment effect (BGATE), which measures a GATE with a specific distribution of a priori-determined covariates. By taking the difference of two BGATEs, we can analyse heterogeneity more meaningfully than by comparing two GATEs. The estimation strategy for this parameter is based on double/debiased machine learning for discrete treatments in an unconfoundedness setting, and the estimator is shown to be $\sqrt{N}$-consistent and asymptotically normal under standard conditions. Adding additional identifyin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22312;&#31232;&#26377;&#31867;&#27010;&#29575;&#36235;&#36817;&#20110;&#38646;&#30340;&#24773;&#20917;&#19979;&#30340;&#26032;&#36129;&#29486;&#65292;&#20998;&#21035;&#26159;&#19968;&#31181;&#38750;&#28176;&#36817;&#24555;&#36895;&#29575;&#27010;&#29575;&#30028;&#38480;&#21644;&#19968;&#31181;&#19968;&#33268;&#19978;&#30028;&#20272;&#35745;&#26041;&#27861;&#65292;&#36825;&#20123;&#21457;&#29616;&#20026;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#31867;&#21035;&#21152;&#26435;&#25552;&#20379;&#20102;&#26356;&#28165;&#26224;&#30340;&#29702;&#35299;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.14826</link><description>&lt;p&gt;
&#38024;&#23545;&#19981;&#24179;&#34913;&#20998;&#31867;&#30340;&#23574;&#38160;&#35823;&#24046;&#30028;&#65306;&#23569;&#25968;&#31867;&#20013;&#26377;&#22810;&#23569;&#26679;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
Sharp error bounds for imbalanced classification: how many examples in the minority class?. (arXiv:2310.14826v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22312;&#31232;&#26377;&#31867;&#27010;&#29575;&#36235;&#36817;&#20110;&#38646;&#30340;&#24773;&#20917;&#19979;&#30340;&#26032;&#36129;&#29486;&#65292;&#20998;&#21035;&#26159;&#19968;&#31181;&#38750;&#28176;&#36817;&#24555;&#36895;&#29575;&#27010;&#29575;&#30028;&#38480;&#21644;&#19968;&#31181;&#19968;&#33268;&#19978;&#30028;&#20272;&#35745;&#26041;&#27861;&#65292;&#36825;&#20123;&#21457;&#29616;&#20026;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#31867;&#21035;&#21152;&#26435;&#25552;&#20379;&#20102;&#26356;&#28165;&#26224;&#30340;&#29702;&#35299;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#19981;&#24179;&#34913;&#20998;&#31867;&#25968;&#25454;&#26102;&#65292;&#37325;&#26032;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#26159;&#19968;&#31181;&#26631;&#20934;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#39118;&#38505;&#24230;&#37327;&#20013;&#24179;&#34913;&#30495;&#27491;&#30340;&#27491;&#20363;&#29575;&#21644;&#30495;&#27491;&#30340;&#36127;&#20363;&#29575;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#26377;&#37325;&#35201;&#30340;&#29702;&#35770;&#24037;&#20316;&#65292;&#20294;&#29616;&#26377;&#30340;&#32467;&#26524;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#19981;&#24179;&#34913;&#20998;&#31867;&#26694;&#26550;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#30456;&#23545;&#20110;&#25972;&#20010;&#26679;&#26412;&#22823;&#23567;&#26469;&#35828;&#19968;&#20010;&#31867;&#30340;&#21487;&#24573;&#30053;&#30340;&#22823;&#23567;&#20197;&#21450;&#38656;&#35201;&#36890;&#36807;&#36235;&#36817;&#20110;&#38646;&#30340;&#27010;&#29575;&#26469;&#37325;&#26032;&#35843;&#25972;&#39118;&#38505;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#31232;&#26377;&#31867;&#27010;&#29575;&#36235;&#36817;&#20110;&#38646;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#36129;&#29486;&#65306;&#65288;1&#65289;&#29992;&#20110;&#32422;&#26463;&#24179;&#34913;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38750;&#28176;&#36817;&#24555;&#36895;&#29575;&#27010;&#29575;&#30028;&#38480;&#65292;&#20197;&#21450;&#65288;2&#65289;&#29992;&#20110;&#24179;&#34913;&#26368;&#36817;&#37051;&#20272;&#35745;&#30340;&#19968;&#33268;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26356;&#28165;&#26970;&#22320;&#35828;&#26126;&#20102;&#31867;&#21035;&#21152;&#26435;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#30410;&#22788;&#65292;&#20026;&#36825;&#20010;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
When dealing with imbalanced classification data, reweighting the loss function is a standard procedure allowing to equilibrate between the true positive and true negative rates within the risk measure. Despite significant theoretical work in this area, existing results do not adequately address a main challenge within the imbalanced classification framework, which is the negligible size of one class in relation to the full sample size and the need to rescale the risk function by a probability tending to zero. To address this gap, we present two novel contributions in the setting where the rare class probability approaches zero: (1) a non asymptotic fast rate probability bound for constrained balanced empirical risk minimization, and (2) a consistent upper bound for balanced nearest neighbors estimates. Our findings provide a clearer understanding of the benefits of class-weighting in realistic settings, opening new avenues for further research in this field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#21644;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#30340;&#38598;&#25104;&#23454;&#29616;&#23545;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#30340;&#36924;&#36817;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31934;&#30830;&#24615;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10745</link><description>&lt;p&gt;
Mori-Zwanzig&#28508;&#21464;&#31354;&#38388;Koopman&#38381;&#21253;&#29992;&#20110;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder. (arXiv:2310.10745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#21644;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#30340;&#38598;&#25104;&#23454;&#29616;&#23545;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#30340;&#36924;&#36817;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31934;&#30830;&#24615;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#20840;&#23616;&#32447;&#24615;&#21270;&#65292;&#20351;&#20854;&#25104;&#20026;&#31616;&#21270;&#22797;&#26434;&#21160;&#21147;&#23398;&#29702;&#35299;&#30340;&#23453;&#36149;&#26041;&#27861;&#12290;&#34429;&#28982;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;&#36924;&#36817;&#26377;&#38480;Koopman&#31639;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#21508;&#31181;&#25361;&#25112;&#65292;&#20363;&#22914;&#36873;&#25321;&#21512;&#36866;&#30340;&#21487;&#35266;&#23519;&#37327;&#12289;&#38477;&#32500;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#25552;&#21462;&#20851;&#38190;&#21487;&#35266;&#23519;&#37327;&#26469;&#36924;&#36817;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#38598;&#25104;&#38750;&#39532;&#23572;&#21487;&#22827;&#26657;&#27491;&#26426;&#21046;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#21464;&#27969;&#24418;&#20013;&#20135;&#29983;&#20102;&#21160;&#21147;&#23398;&#30340;&#23553;&#38381;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31934;&#30830;&#24615;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The Koopman operator presents an attractive approach to achieve global linearization of nonlinear systems, making it a valuable method for simplifying the understanding of complex dynamics. While data-driven methodologies have exhibited promise in approximating finite Koopman operators, they grapple with various challenges, such as the judicious selection of observables, dimensionality reduction, and the ability to predict complex system behaviours accurately. This study presents a novel approach termed Mori-Zwanzig autoencoder (MZ-AE) to robustly approximate the Koopman operator in low-dimensional spaces. The proposed method leverages a nonlinear autoencoder to extract key observables for approximating a finite invariant Koopman subspace and integrates a non-Markovian correction mechanism using the Mori-Zwanzig formalism. Consequently, this approach yields a closed representation of dynamics within the latent manifold of the nonlinear autoencoder, thereby enhancing the precision and s
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#20851;&#20110;&#37327;&#23376;&#31995;&#32479;&#23398;&#20064;&#24615;&#36136;&#20197;&#21450;&#36890;&#36807;&#37327;&#23376;&#35745;&#31639;&#22788;&#29702;&#32463;&#20856;&#25110;&#37327;&#23376;&#25968;&#25454;&#30340;&#38382;&#39064;&#26085;&#30410;&#27963;&#36291;&#12290;&#36825;&#31687;&#25991;&#31456;&#22238;&#39038;&#20102;&#37327;&#23376;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#22797;&#26434;&#24615;&#12289;&#22797;&#21046;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;&#37327;&#23376;&#27979;&#37327;&#30772;&#22351;&#24615;&#23548;&#33268;&#22797;&#21046;&#22797;&#26434;&#24615;&#65292;&#38480;&#21046;&#20102;&#20174;&#37327;&#23376;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.11617</link><description>&lt;p&gt;
&#37327;&#23376;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Statistical Complexity of Quantum Learning. (arXiv:2309.11617v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11617
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20851;&#20110;&#37327;&#23376;&#31995;&#32479;&#23398;&#20064;&#24615;&#36136;&#20197;&#21450;&#36890;&#36807;&#37327;&#23376;&#35745;&#31639;&#22788;&#29702;&#32463;&#20856;&#25110;&#37327;&#23376;&#25968;&#25454;&#30340;&#38382;&#39064;&#26085;&#30410;&#27963;&#36291;&#12290;&#36825;&#31687;&#25991;&#31456;&#22238;&#39038;&#20102;&#37327;&#23376;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#22797;&#26434;&#24615;&#12289;&#22797;&#21046;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;&#37327;&#23376;&#27979;&#37327;&#30772;&#22351;&#24615;&#23548;&#33268;&#22797;&#21046;&#22797;&#26434;&#24615;&#65292;&#38480;&#21046;&#20102;&#20174;&#37327;&#23376;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22312;&#20351;&#29992;&#25968;&#25454;&#23398;&#20064;&#20851;&#20110;&#37327;&#23376;&#31995;&#32479;&#30340;&#24615;&#36136;&#25110;&#36890;&#36807;&#37327;&#23376;&#35745;&#31639;&#22788;&#29702;&#32463;&#20856;&#25110;&#37327;&#23376;&#25968;&#25454;&#30340;&#38382;&#39064;&#19978;&#20986;&#29616;&#20102;&#30456;&#24403;&#22823;&#30340;&#27963;&#36291;&#24230;&#12290;&#19982;&#32463;&#20856;&#23398;&#20064;&#31867;&#20284;&#65292;&#37327;&#23376;&#23398;&#20064;&#38382;&#39064;&#28041;&#21450;&#21040;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#26410;&#30693;&#30340;&#35774;&#32622;&#65292;&#23398;&#20064;&#31639;&#27861;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#20165;&#26377;&#25968;&#25454;&#21644;&#21487;&#33021;&#30340;&#36741;&#21161;&#20449;&#24687;&#65288;&#27604;&#22914;&#19987;&#23478;&#30693;&#35782;&#65289;&#30340;&#24773;&#20917;&#19979;&#20445;&#35777;&#28385;&#24847;&#30340;&#20934;&#30830;&#24230;&#27700;&#24179;&#12290;&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#20449;&#24687;&#35770;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#22797;&#26434;&#24615;&#12289;&#22797;&#21046;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#65292;&#22238;&#39038;&#20102;&#37327;&#23376;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#12290;&#22797;&#21046;&#22797;&#26434;&#24615;&#28304;&#20110;&#37327;&#23376;&#27979;&#37327;&#30340;&#30772;&#22351;&#24615;&#65292;&#36825;&#31181;&#27979;&#37327;&#20250;&#19981;&#21487;&#36870;&#22320;&#25913;&#21464;&#24453;&#22788;&#29702;&#30340;&#29366;&#24577;&#65292;&#38480;&#21046;&#20102;&#33021;&#20174;&#37327;&#23376;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;&#20363;&#22914;&#65292;&#22312;&#37327;&#23376;&#31995;&#32479;&#20013;&#65292;&#19982;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#19981;&#21516;&#65292;&#36890;&#24120;&#19981;&#21487;&#33021;&#21516;&#26102;&#35780;&#20272;&#35757;&#32451;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen significant activity on the problem of using data for the purpose of learning properties of quantum systems or of processing classical or quantum data via quantum computing. As in classical learning, quantum learning problems involve settings in which the mechanism generating the data is unknown, and the main goal of a learning algorithm is to ensure satisfactory accuracy levels when only given access to data and, possibly, side information such as expert knowledge. This article reviews the complexity of quantum learning using information-theoretic techniques by focusing on data complexity, copy complexity, and model complexity. Copy complexity arises from the destructive nature of quantum measurements, which irreversibly alter the state to be processed, limiting the information that can be extracted about quantum data. For example, in a quantum system, unlike in classical machine learning, it is generally not possible to evaluate the training loss simultaneously
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#19981;&#21516;&#32423;&#21035;&#33258;&#36866;&#24212;&#24615;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#22810;&#31181;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#20998;&#26512;&#20013;&#30452;&#25509;&#24212;&#29992;&#20110;&#23567;&#25439;&#22833;&#30028;&#12290;&#21516;&#26102;&#65292;&#23427;&#19982;&#23545;&#25239;&#24615;/&#38543;&#26426;&#20984;&#20248;&#21270;&#21644;&#21338;&#24328;&#35770;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.08360</link><description>&lt;p&gt;
&#20855;&#26377;&#36880;&#28176;&#21464;&#21270;&#30340;&#36890;&#29992;&#22312;&#32447;&#23398;&#20064;&#65306;&#19968;&#31181;&#22810;&#23618;&#22312;&#32447;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Online Learning with Gradual Variations: A Multi-layer Online Ensemble Approach. (arXiv:2307.08360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08360
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#19981;&#21516;&#32423;&#21035;&#33258;&#36866;&#24212;&#24615;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#22810;&#31181;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#20998;&#26512;&#20013;&#30452;&#25509;&#24212;&#29992;&#20110;&#23567;&#25439;&#22833;&#30028;&#12290;&#21516;&#26102;&#65292;&#23427;&#19982;&#23545;&#25239;&#24615;/&#38543;&#26426;&#20984;&#20248;&#21270;&#21644;&#21338;&#24328;&#35770;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#19981;&#21516;&#32423;&#21035;&#33258;&#36866;&#24212;&#24615;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26041;&#27861;&#12290;&#22312;&#26356;&#39640;&#32423;&#21035;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20855;&#20307;&#31867;&#22411;&#21644;&#26354;&#29575;&#19981;&#30693;&#24773;&#65292;&#32780;&#22312;&#26356;&#20302;&#32423;&#21035;&#19978;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#29615;&#22659;&#30340;&#33391;&#22909;&#24615;&#36136;&#24182;&#33719;&#24471;&#38382;&#39064;&#30456;&#20851;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#24378;&#20984;&#12289;&#25351;&#25968;&#20985;&#21644;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#20998;&#21035;&#33719;&#24471;&#20102;$O(\ln V_T)$&#12289;$O(d \ln V_T)$&#21644;$\hat{O}(\sqrt{V_T})$&#30340;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;$d$&#26159;&#32500;&#24230;&#65292;$V_T$&#34920;&#31034;&#38382;&#39064;&#30456;&#20851;&#30340;&#26799;&#24230;&#21464;&#21270;&#65292;$\hat{O}(\cdot)$&#34920;&#31034;&#22312;$V_T$&#19978;&#30465;&#30053;&#23545;&#25968;&#22240;&#23376;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20855;&#26377;&#24191;&#27867;&#30340;&#24433;&#21709;&#21644;&#24212;&#29992;&#12290;&#23427;&#19981;&#20165;&#20445;&#35777;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#36824;&#30452;&#25509;&#23548;&#20986;&#20102;&#20998;&#26512;&#20013;&#30340;&#23567;&#25439;&#22833;&#30028;&#12290;&#27492;&#22806;&#65292;&#23427;&#19982;&#23545;&#25239;&#24615;/&#38543;&#26426;&#20984;&#20248;&#21270;&#21644;&#21338;&#24328;&#35770;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#65292;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#20854;&#23454;&#38469;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an online convex optimization method with two different levels of adaptivity. On a higher level, our method is agnostic to the specific type and curvature of the loss functions, while at a lower level, it can exploit the niceness of the environments and attain problem-dependent guarantees. To be specific, we obtain $\mathcal{O}(\ln V_T)$, $\mathcal{O}(d \ln V_T)$ and $\hat{\mathcal{O}}(\sqrt{V_T})$ regret bounds for strongly convex, exp-concave and convex loss functions, respectively, where $d$ is the dimension, $V_T$ denotes problem-dependent gradient variations and $\hat{\mathcal{O}}(\cdot)$-notation omits logarithmic factors on $V_T$. Our result finds broad implications and applications. It not only safeguards the worst-case guarantees, but also implies the small-loss bounds in analysis directly. Besides, it draws deep connections with adversarial/stochastic convex optimization and game theory, further validating its practical potential. Our method is based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26469;&#25551;&#36848;&#38750;&#36127;&#36229;&#39532;&#27663;&#36807;&#31243;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#20010;&#26032;&#30340;&#26497;&#22823;&#19981;&#31561;&#24335;&#65292;&#36866;&#29992;&#20110;&#38750;&#21487;&#31215;&#24773;&#20917;&#65292;&#24182;&#35828;&#26126;&#20102;&#28151;&#21512;&#26041;&#27861;&#30340;&#25193;&#23637;&#20197;&#21450;&#35813;&#29702;&#35770;&#22312;&#39034;&#24207;&#32479;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.01163</link><description>&lt;p&gt;
&#38750;&#21487;&#31215;&#38750;&#36127;&#36229;&#39532;&#27663;&#36807;&#31243;&#30340;&#25193;&#23637;&#32500;&#23572;&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
The extended Ville's inequality for nonintegrable nonnegative supermartingales. (arXiv:2304.01163v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26469;&#25551;&#36848;&#38750;&#36127;&#36229;&#39532;&#27663;&#36807;&#31243;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#20010;&#26032;&#30340;&#26497;&#22823;&#19981;&#31561;&#24335;&#65292;&#36866;&#29992;&#20110;&#38750;&#21487;&#31215;&#24773;&#20917;&#65292;&#24182;&#35828;&#26126;&#20102;&#28151;&#21512;&#26041;&#27861;&#30340;&#25193;&#23637;&#20197;&#21450;&#35813;&#29702;&#35770;&#22312;&#39034;&#24207;&#32479;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312; Robbins &#30340;&#21021;&#22987;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#20005;&#23494;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#36127;&#36229;&#39532;&#27663;&#36807;&#31243;&#30340;&#25193;&#23637;&#29702;&#35770;&#65292;&#19981;&#38656;&#35201;&#21487;&#31215;&#24615;&#25110;&#26377;&#38480;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102; Robbins &#39044;&#31034;&#30340;&#19968;&#20010;&#20851;&#38190;&#26497;&#22823;&#19981;&#31561;&#24335;&#65292;&#31216;&#20026;&#25193;&#23637;&#32500;&#23572;&#19981;&#31561;&#24335;&#65292;&#23427;&#21152;&#24378;&#20102;&#32463;&#20856;&#30340;&#32500;&#23572;&#19981;&#31561;&#24335;&#65288;&#36866;&#29992;&#20110;&#21487;&#31215;&#38750;&#36127;&#36229;&#39532;&#27663;&#36807;&#31243;&#65289;&#65292;&#24182;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#38750;&#21487;&#31215;&#35774;&#32622;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#28151;&#21512;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#25105;&#20204;&#25193;&#23637;&#30340;&#38750;&#36127;&#36229;&#39532;&#27663;&#36807;&#31243;&#30340; $\sigma$- &#26377;&#38480;&#28151;&#21512;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#29702;&#35770;&#22312;&#39034;&#24207;&#32479;&#35745;&#20013;&#30340;&#19968;&#20123;&#24212;&#29992;&#65292;&#22914;&#22312;&#25512;&#23548;&#38750;&#21442;&#25968;&#32622;&#20449;&#24207;&#21015;&#21644;&#65288;&#25193;&#23637;&#65289;e-&#36807;&#31243;&#20013;&#20351;&#29992;&#19981;&#36866;&#24403;&#28151;&#21512;&#65288;&#20808;&#39564;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following initial work by Robbins, we rigorously present an extended theory of nonnegative supermartingales, requiring neither integrability nor finiteness. In particular, we derive a key maximal inequality foreshadowed by Robbins, which we call the extended Ville's inequality, that strengthens the classical Ville's inequality (for integrable nonnegative supermartingales), and also applies to our nonintegrable setting. We derive an extension of the method of mixtures, which applies to $\sigma$-finite mixtures of our extended nonnegative supermartingales. We present some implications of our theory for sequential statistics, such as the use of improper mixtures (priors) in deriving nonparametric confidence sequences and (extended) e-processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17963</link><description>&lt;p&gt;
&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States. (arXiv:2303.17963v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25511;&#21046;&#24037;&#31243;&#26041;&#27861;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#36776;&#35782;&#26041;&#27861;&#25104;&#20026;&#29289;&#29702;&#24314;&#27169;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#29366;&#24577;&#27979;&#37327;&#30340;&#21487;&#29992;&#24615;&#65292;&#32780;&#22797;&#26434;&#31995;&#32479;&#30340;&#29366;&#24577;&#36890;&#24120;&#19981;&#26159;&#30452;&#25509;&#21487;&#27979;&#37327;&#30340;&#12290;&#22240;&#27492;&#65292;&#21487;&#33021;&#38656;&#35201;&#21516;&#26102;&#20272;&#35745;&#21160;&#21147;&#23398;&#21644;&#28508;&#22312;&#29366;&#24577;&#65292;&#20174;&#32780;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#22320;&#35774;&#35745;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#25511;&#21046;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#30340;&#26410;&#30693;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#36755;&#20837;&#36712;&#36857;&#12290;&#23545;&#32467;&#26524;&#36755;&#20837;&#36712;&#36857;&#36827;&#34892;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As control engineering methods are applied to increasingly complex systems, data-driven approaches for system identification appear as a promising alternative to physics-based modeling. While many of these approaches rely on the availability of state measurements, the states of a complex system are often not directly measurable. It may then be necessary to jointly estimate the dynamics and a latent state, making it considerably more challenging to design controllers with performance guarantees. This paper proposes a novel method for the computation of an optimal input trajectory for unknown nonlinear systems with latent states. Probabilistic performance guarantees are derived for the resulting input trajectory, and an approach to validate the performance of arbitrary control laws is presented. The effectiveness of the proposed method is demonstrated in a numerical simulation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20598;&#28982;&#24615;&#21644;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#23558;&#20854;&#20998;&#31867;&#20026;&#25968;&#25454;&#20998;&#24067;&#20013;&#22266;&#26377;&#30340;&#27495;&#35270;&#21644;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#20915;&#31574;&#23548;&#33268;&#30340;&#27495;&#35270;&#12290;&#36890;&#36807;&#37327;&#21270;&#20598;&#28982;&#24615;&#27495;&#35270;&#30340;&#24615;&#33021;&#38480;&#21046;&#21644;&#21051;&#30011;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#25581;&#31034;&#20102;&#20844;&#24179;&#24178;&#39044;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#30740;&#31350;&#36824;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#65292;&#24182;&#25506;&#31350;&#20102;&#22312;&#23384;&#22312;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#30340;&#20844;&#24179;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2301.11781</link><description>&lt;p&gt;
&#20598;&#28982;&#24615;&#21644;&#35748;&#30693;&#24615;&#27495;&#35270;&#65306;&#20844;&#24179;&#24178;&#39044;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions. (arXiv:2301.11781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11781
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20598;&#28982;&#24615;&#21644;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#23558;&#20854;&#20998;&#31867;&#20026;&#25968;&#25454;&#20998;&#24067;&#20013;&#22266;&#26377;&#30340;&#27495;&#35270;&#21644;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#20915;&#31574;&#23548;&#33268;&#30340;&#27495;&#35270;&#12290;&#36890;&#36807;&#37327;&#21270;&#20598;&#28982;&#24615;&#27495;&#35270;&#30340;&#24615;&#33021;&#38480;&#21046;&#21644;&#21051;&#30011;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#25581;&#31034;&#20102;&#20844;&#24179;&#24178;&#39044;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#30740;&#31350;&#36824;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#65292;&#24182;&#25506;&#31350;&#20102;&#22312;&#23384;&#22312;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#30340;&#20844;&#24179;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26576;&#20123;&#20154;&#32676;&#20013;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#65292;&#21407;&#22240;&#26159;&#22312;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#20570;&#20986;&#30340;&#36873;&#25321;&#21644;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#23558;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#27495;&#35270;&#26469;&#28304;&#20998;&#20026;&#20004;&#31867;&#65306;&#20598;&#28982;&#24615;&#27495;&#35270;&#65292;&#21363;&#25968;&#25454;&#20998;&#24067;&#20013;&#22266;&#26377;&#30340;&#27495;&#35270;&#65292;&#21644;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#21363;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#20570;&#20986;&#30340;&#20915;&#31574;&#23548;&#33268;&#30340;&#27495;&#35270;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#23450;&#22312;&#23436;&#20840;&#20102;&#35299;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20844;&#24179;&#32422;&#26463;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#38480;&#21046;&#26469;&#37327;&#21270;&#20598;&#28982;&#24615;&#27495;&#35270;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#24067;&#33713;&#20811;&#38886;&#23572;&#23545;&#27604;&#32479;&#35745;&#23454;&#39564;&#30340;&#32467;&#26524;&#26469;&#21051;&#30011;&#20598;&#28982;&#24615;&#27495;&#35270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35748;&#30693;&#24615;&#27495;&#35270;&#23450;&#20041;&#20026;&#22312;&#24212;&#29992;&#20844;&#24179;&#32422;&#26463;&#26102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19982;&#20598;&#28982;&#24615;&#27495;&#35270;&#25152;&#38480;&#23450;&#30340;&#30028;&#38480;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#30340;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#65292;&#24182;&#35843;&#26597;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#30340;&#20844;&#24179;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models can underperform on certain population groups due to choices made during model development and bias inherent in the data. We categorize sources of discrimination in the ML pipeline into two classes: aleatoric discrimination, which is inherent in the data distribution, and epistemic discrimination, which is due to decisions made during model development. We quantify aleatoric discrimination by determining the performance limits of a model under fairness constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize aleatoric discrimination by applying Blackwell's results on comparing statistical experiments. We then quantify epistemic discrimination as the gap between a model's accuracy when fairness constraints are applied and the limit posed by aleatoric discrimination. We apply this approach to benchmark existing fairness interventions and investigate fairness risks in data with missing values. Our results indicate th
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#21644;&#40654;&#26364;&#27969;&#24418;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.14598</link><description>&lt;p&gt;
&#30830;&#20999;&#30340;&#27969;&#24418;&#39640;&#26031;&#21464;&#20998;&#36125;&#21494;&#26031;
&lt;/p&gt;
&lt;p&gt;
Exact Manifold Gaussian Variational Bayes. (arXiv:2210.14598v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14598
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#21644;&#40654;&#26364;&#27969;&#24418;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#27169;&#22411;&#20013;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#65292;&#20854;&#20013;&#21464;&#20998;&#31354;&#38388;&#26159;&#19968;&#20010;&#40654;&#26364;&#27969;&#24418;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#20197;&#38544;&#24335;&#28385;&#36275;&#21464;&#20998;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#27491;&#23450;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#30830;&#20999;&#27969;&#24418;&#39640;&#26031;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;EMGVB&#65289;&#25552;&#20379;&#20102;&#31934;&#30830;&#20294;&#31616;&#21333;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#24182;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;&#30001;&#20110;&#20854;&#40657;&#30418;&#24615;&#36136;&#65292;EMGVB&#25104;&#20026;&#22797;&#26434;&#27169;&#22411;&#20013;&#21363;&#25554;&#21363;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#32479;&#35745;&#12289;&#35745;&#37327;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#20351;&#29992;&#20116;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#21487;&#34892;&#24615;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#24182;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an optimization algorithm for Variational Inference (VI) in complex models. Our approach relies on natural gradient updates where the variational space is a Riemann manifold. We develop an efficient algorithm for Gaussian Variational Inference that implicitly satisfies the positive definite constraint on the variational covariance matrix. Our Exact manifold Gaussian Variational Bayes (EMGVB) provides exact but simple update rules and is straightforward to implement. Due to its black-box nature, EMGVB stands as a ready-to-use solution for VI in complex models. Over five datasets, we empirically validate our feasible approach on different statistical, econometric, and deep learning models, discussing its performance with respect to baseline methods.
&lt;/p&gt;</description></item></channel></rss>