<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#31526;&#21512;&#20915;&#31574;&#29702;&#35770;&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#19981;&#23436;&#32654;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20135;&#29983;&#23433;&#20840;&#30340;&#33258;&#20027;&#20915;&#31574;&#12290;&#35813;&#29702;&#35770;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21487;&#20197;&#22312;&#27809;&#26377;&#23545;&#19990;&#30028;&#27169;&#22411;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20855;&#26377;&#20302;&#39118;&#38505;&#30340;&#32479;&#35745;&#20445;&#35777;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2310.05921</link><description>&lt;p&gt;
&#31526;&#21512;&#20915;&#31574;&#29702;&#35770;: &#36890;&#36807;&#19981;&#23436;&#32654;&#30340;&#39044;&#27979;&#20135;&#29983;&#23433;&#20840;&#30340;&#33258;&#20027;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions. (arXiv:2310.05921v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05921
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#20915;&#31574;&#29702;&#35770;&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#19981;&#23436;&#32654;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20135;&#29983;&#23433;&#20840;&#30340;&#33258;&#20027;&#20915;&#31574;&#12290;&#35813;&#29702;&#35770;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21487;&#20197;&#22312;&#27809;&#26377;&#23545;&#19990;&#30028;&#27169;&#22411;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20855;&#26377;&#20302;&#39118;&#38505;&#30340;&#32479;&#35745;&#20445;&#35777;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31526;&#21512;&#20915;&#31574;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#19981;&#23436;&#32654;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#23433;&#20840;&#30340;&#33258;&#20027;&#20915;&#31574;&#12290;&#36825;&#31181;&#20915;&#31574;&#30340;&#20363;&#23376;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#20174;&#20381;&#36182;&#20110;&#34892;&#20154;&#39044;&#27979;&#30340;&#26426;&#22120;&#20154;&#35268;&#21010;&#31639;&#27861;&#65292;&#21040;&#26657;&#20934;&#33258;&#21160;&#21270;&#21046;&#36896;&#20197;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#38169;&#35823;&#29575;&#65292;&#20877;&#21040;&#22312;&#36816;&#34892;&#26102;&#36873;&#25321;&#20449;&#20219;&#21517;&#20041;&#31574;&#30053;&#36824;&#26159;&#20999;&#25442;&#21040;&#23433;&#20840;&#22791;&#20221;&#31574;&#30053;&#12290;&#25105;&#20204;&#31639;&#27861;&#20135;&#29983;&#30340;&#20915;&#31574;&#22312;&#32479;&#35745;&#20445;&#35777;&#30340;&#24773;&#20917;&#19979;&#26159;&#23433;&#20840;&#30340;&#65292;&#26080;&#38656;&#23545;&#19990;&#30028;&#27169;&#22411;&#20316;&#20986;&#20219;&#20309;&#20551;&#35774;&#65307;&#35266;&#27979;&#25968;&#25454;&#21487;&#20197;&#19981;&#28385;&#36275;&#29420;&#31435;&#21516;&#20998;&#24067;(I.I.D.)&#30340;&#26465;&#20214;&#65292;&#29978;&#33267;&#21487;&#33021;&#26159;&#23545;&#25239;&#24615;&#30340;&#12290;&#35813;&#29702;&#35770;&#23558;&#31526;&#21512;&#39044;&#27979;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#30452;&#25509;&#26657;&#20934;&#20915;&#31574;&#65292;&#32780;&#19981;&#38656;&#35201;&#26500;&#24314;&#39044;&#27979;&#38598;&#21512;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22260;&#32469;&#20154;&#31867;&#36827;&#34892;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#12289;&#33258;&#21160;&#32929;&#31080;&#20132;&#26131;&#21644;&#26426;&#22120;&#20154;&#21046;&#36896;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Conformal Decision Theory, a framework for producing safe autonomous decisions despite imperfect machine learning predictions. Examples of such decisions are ubiquitous, from robot planning algorithms that rely on pedestrian predictions, to calibrating autonomous manufacturing to exhibit high throughput and low error, to the choice of trusting a nominal policy versus switching to a safe backup policy at run-time. The decisions produced by our algorithms are safe in the sense that they come with provable statistical guarantees of having low risk without any assumptions on the world model whatsoever; the observations need not be I.I.D. and can even be adversarial. The theory extends results from conformal prediction to calibrate decisions directly, without requiring the construction of prediction sets. Experiments demonstrate the utility of our approach in robot motion planning around humans, automated stock trading, and robot manufacturin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;"grokking"&#29616;&#35937;&#24402;&#22240;&#20110;&#21387;&#32553;&#65292;&#24182;&#25552;&#20986;&#32447;&#24615;&#26144;&#23556;&#25968;&#65288;LMN&#65289;&#20316;&#20026;&#34913;&#37327;&#31070;&#32463;&#32593;&#32476;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#12290;LMN&#33021;&#22815;&#26356;&#22909;&#22320;&#25551;&#36848;&#32593;&#32476;&#30340;&#21387;&#32553;&#36807;&#31243;&#65292;&#24182;&#23637;&#29616;XOR&#32593;&#32476;&#22312;&#36890;&#29992;&#21270;&#35299;&#20915;&#26041;&#26696;&#38388;&#20999;&#25442;&#30340;&#26377;&#36259;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2310.05918</link><description>&lt;p&gt;
Grokking&#20316;&#20026;&#21387;&#32553;&#65306;&#19968;&#31181;&#38750;&#32447;&#24615;&#22797;&#26434;&#24615;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Grokking as Compression: A Nonlinear Complexity Perspective. (arXiv:2310.05918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;"grokking"&#29616;&#35937;&#24402;&#22240;&#20110;&#21387;&#32553;&#65292;&#24182;&#25552;&#20986;&#32447;&#24615;&#26144;&#23556;&#25968;&#65288;LMN&#65289;&#20316;&#20026;&#34913;&#37327;&#31070;&#32463;&#32593;&#32476;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#12290;LMN&#33021;&#22815;&#26356;&#22909;&#22320;&#25551;&#36848;&#32593;&#32476;&#30340;&#21387;&#32553;&#36807;&#31243;&#65292;&#24182;&#23637;&#29616;XOR&#32593;&#32476;&#22312;&#36890;&#29992;&#21270;&#35299;&#20915;&#26041;&#26696;&#38388;&#20999;&#25442;&#30340;&#26377;&#36259;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#36831;&#32531;&#36890;&#29992;&#21270;&#65288;generalization&#65289;&#30340;&#29616;&#35937;&#8220;grokking&#8221;&#24402;&#22240;&#20110;&#21387;&#32553;&#12290;&#20026;&#20102;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#32447;&#24615;&#26144;&#23556;&#25968;&#65288;LMN&#65289;&#26469;&#27979;&#37327;&#32593;&#32476;&#22797;&#26434;&#24230;&#65292;&#36825;&#26159;ReLU&#32593;&#32476;&#32447;&#24615;&#21306;&#22495;&#25968;&#30340;&#24191;&#20041;&#29256;&#26412;&#12290;LMN&#21487;&#20197;&#24456;&#22909;&#22320;&#34920;&#24449;&#32593;&#32476;&#22312;&#36890;&#29992;&#21270;&#20043;&#21069;&#30340;&#21387;&#32553;&#36807;&#31243;&#12290;&#34429;&#28982;$L_2$&#33539;&#25968;&#19968;&#30452;&#26159;&#25551;&#36848;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#24120;&#29992;&#36873;&#25321;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#29702;&#30001;&#25903;&#25345;&#20351;&#29992;LMN&#65306;&#65288;1&#65289;LMN&#33021;&#22815;&#33258;&#28982;&#22320;&#35299;&#37322;&#20026;&#20449;&#24687;/&#35745;&#31639;&#65292;&#32780;$L_2$&#21017;&#19981;&#33021;&#12290;&#65288;2&#65289;&#22312;&#21387;&#32553;&#38454;&#27573;&#65292;LMN&#19982;&#27979;&#35797;&#35823;&#24046;&#20855;&#26377;&#32447;&#24615;&#20851;&#31995;&#65292;&#32780;$L_2$&#21017;&#20197;&#19968;&#31181;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#26041;&#24335;&#19982;&#27979;&#35797;&#35823;&#24046;&#30456;&#20851;&#12290;&#65288;3&#65289;LMN&#36824;&#25581;&#31034;&#20102;XOR&#32593;&#32476;&#22312;&#20004;&#31181;&#36890;&#29992;&#21270;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#20999;&#25442;&#30340;&#26377;&#36259;&#29616;&#35937;&#65292;&#32780;$L_2$&#21017;&#27809;&#26377;&#12290;&#38500;&#20102;&#35299;&#37322;&#8220;grokking&#8221;&#29616;&#35937;&#22806;&#65292;&#25105;&#20204;&#36824;&#35748;&#20026;LMN&#26159;Kolmogorov&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#29256;&#26412;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#32773;&#65292;&#22240;&#20026;&#23427;&#26174;&#24335;&#22320;&#32771;&#34385;&#20102;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We attribute grokking, the phenomenon where generalization is much delayed after memorization, to compression. To do so, we define linear mapping number (LMN) to measure network complexity, which is a generalized version of linear region number for ReLU networks. LMN can nicely characterize neural network compression before generalization. Although the $L_2$ norm has been a popular choice for characterizing model complexity, we argue in favor of LMN for a number of reasons: (1) LMN can be naturally interpreted as information/computation, while $L_2$ cannot. (2) In the compression phase, LMN has linear relations with test losses, while $L_2$ is correlated with test losses in a complicated nonlinear way. (3) LMN also reveals an intriguing phenomenon of the XOR network switching between two generalization solutions, while $L_2$ does not. Besides explaining grokking, we argue that LMN is a promising candidate as the neural network version of the Kolmogorov complexity since it explicitly co
&lt;/p&gt;</description></item><item><title>Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2310.05898</link><description>&lt;p&gt;
&#29422;&#23376;&#31192;&#23494;&#22320;&#35299;&#20915;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#65306;&#27491;&#22914;&#26446;&#38597;&#26222;&#35834;&#22827;&#25152;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05898
&lt;/p&gt;
&lt;p&gt;
Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;Lion&#65288;&#36827;&#21270;&#30340;&#31526;&#21495;&#21160;&#37327;&#65289;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#23427;&#22312;&#35757;&#32451;&#25928;&#26524;&#19978;&#19982;AdamW&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#27491;&#22914;&#25105;&#20204;&#21487;&#20197;&#20174;&#38543;&#26426;&#25628;&#32034;&#31243;&#24207;&#30340;&#32467;&#26524;&#20013;&#26399;&#24453;&#30340;&#65292;Lion&#38598;&#25104;&#20102;&#20960;&#20010;&#29616;&#26377;&#31639;&#27861;&#30340;&#20803;&#32032;&#65292;&#21253;&#25324;&#31526;&#21495;&#21160;&#37327;&#12289;&#29420;&#31435;&#30340;&#26435;&#37325;&#34928;&#20943;&#12289;Polak&#21644;Nesterov&#21160;&#37327;&#65292;&#20294;&#21448;&#19981;&#23646;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#29702;&#35770;&#22522;&#30784;&#20248;&#21270;&#22120;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;Lion&#20316;&#20026;&#24191;&#27867;&#20219;&#21153;&#30340;&#36890;&#29992;&#20248;&#21270;&#22120;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#36825;&#31181;&#32570;&#20047;&#29702;&#35770;&#30340;&#26126;&#30830;&#24615;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#21644;&#25193;&#23637;Lion&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#24320;Lion&#30340;&#31070;&#31192;&#38754;&#32433;&#12290;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;$f(x)$&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#38750;&#24179;&#31283;&#30456;&#20851;&#25968;&#25454;&#19978;&#24314;&#31435;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.05892</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#30456;&#20851;&#25968;&#25454;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
A Generalization Bound of Deep Neural Networks for Dependent Data. (arXiv:2310.05892v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05892
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#38750;&#24179;&#31283;&#30456;&#20851;&#25968;&#25454;&#19978;&#24314;&#31435;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30028;&#38480;&#35201;&#27714;&#25968;&#25454;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#32780;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#22914;&#36827;&#21270;&#29983;&#29289;&#23398;&#12289;&#20256;&#26579;&#30149;&#27969;&#34892;&#30149;&#23398;&#21644;&#32929;&#20215;&#39044;&#27979;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#24179;&#31283; $\phi$-&#28151;&#21512;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing generalization bounds for deep neural networks require data to be independent and identically distributed (iid). This assumption may not hold in real-life applications such as evolutionary biology, infectious disease epidemiology, and stock price prediction. This work establishes a generalization bound of feed-forward neural networks for non-stationary $\phi$-mixing data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;WSINDy&#36827;&#34892;&#31895;&#31890;&#21270;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;WSINDy&#22312;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#20013;&#30340;&#31895;&#31890;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#35782;&#21035;&#36817;&#20284;&#23545;&#31216;&#24615;&#21644;&#22788;&#29702;&#22806;&#37096;&#25200;&#21160;&#65292;WSINDy&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#38477;&#32500;&#30340;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#30456;&#20851;&#33258;&#30001;&#24230;&#30340;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.05879</link><description>&lt;p&gt;
&#20351;&#29992;WSINDy&#36827;&#34892;&#31895;&#31890;&#21270;&#21704;&#23494;&#39039;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Coarse-Graining Hamiltonian Systems Using WSINDy. (arXiv:2310.05879v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;WSINDy&#36827;&#34892;&#31895;&#31890;&#21270;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;WSINDy&#22312;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#20013;&#30340;&#31895;&#31890;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#35782;&#21035;&#36817;&#20284;&#23545;&#31216;&#24615;&#21644;&#22788;&#29702;&#22806;&#37096;&#25200;&#21160;&#65292;WSINDy&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#38477;&#32500;&#30340;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#30456;&#20851;&#33258;&#30001;&#24230;&#30340;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#30340;&#32972;&#26223;&#19979;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#24369;&#24418;&#24577;&#31232;&#30095;&#35782;&#21035;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31639;&#27861;(WSINDy)&#20855;&#26377;&#31895;&#31890;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#33021;&#21147;&#25193;&#23637;&#21040;&#20855;&#26377;&#36817;&#20284;&#23545;&#31216;&#24615;&#30340;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#30340;&#31895;&#31890;&#21270;&#38382;&#39064;&#19978;&#12290;&#36825;&#31181;&#36817;&#20284;&#23545;&#31216;&#24615;&#36890;&#24120;&#23548;&#33268;&#23384;&#22312;&#19968;&#20010;&#38477;&#32500;&#30340;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#30456;&#20851;&#33258;&#30001;&#24230;&#30340;&#21160;&#21147;&#23398;&#12290;&#23548;&#20986;&#36825;&#26679;&#30340;&#38477;&#32500;&#31995;&#32479;&#65292;&#25110;&#32773;&#36890;&#36807;&#25968;&#20540;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#36817;&#20284;&#65292;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;WSINDy&#21487;&#20197;&#25104;&#21151;&#22320;&#22312;&#23545;&#31216;&#19981;&#31934;&#30830;&#24615;&#21644;&#22806;&#37096;&#22122;&#22768;&#30340;&#24433;&#21709;&#19979;&#35782;&#21035;&#20986;&#36825;&#20010;&#38477;&#32500;&#30340;&#21704;&#23494;&#39039;&#31995;&#32479;&#12290;&#36825;&#22312;&#19968;&#37096;&#20998;&#26159;&#22240;&#20026;&#36825;&#26679;&#30340;&#31995;&#32479;&#22914;&#20309;&#34987;&#35299;&#26512;&#22320;&#23548;&#20986;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;WSINDy&#33258;&#28982;&#22320;&#20445;&#30041;&#20102;&#21704;&#23494;&#39039;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Weak-form Sparse Identification of Nonlinear Dynamics algorithm (WSINDy) has been demonstrated to offer coarse-graining capabilities in the context of interacting particle systems ( https://doi.org/10.1016/j.physd.2022.133406 ). In this work we extend this capability to the problem of coarse-graining Hamiltonian dynamics which possess approximate symmetries. Such approximate symmetries often lead to the existence of a Hamiltonian system of reduced dimension that may be used to efficiently capture the dynamics of the relevant degrees of freedom. Deriving such reduced systems, or approximating them numerically, is an ongoing challenge. We demonstrate that WSINDy can successfully identify this reduced Hamiltonian system in the presence of large perturbations imparted from both the inexact nature of the symmetry and extrinsic noise. This is significant in part due to the nontrivial means by which such systems are derived analytically. WSINDy naturally preserves the Hamiltonian structur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GNNSync&#30340;&#22522;&#20110;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#35282;&#24230;&#21516;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35282;&#24230;&#21516;&#27493;&#38382;&#39064;&#22312;&#39640;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#26356;&#22909;&#22320;&#32534;&#30721;&#21516;&#27493;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2310.05842</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#35282;&#24230;&#21516;&#27493;&#38382;&#39064;&#30340;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Robust Angular Synchronization via Directed Graph Neural Networks. (arXiv:2310.05842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GNNSync&#30340;&#22522;&#20110;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#35282;&#24230;&#21516;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35282;&#24230;&#21516;&#27493;&#38382;&#39064;&#22312;&#39640;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#26356;&#22909;&#22320;&#32534;&#30721;&#21516;&#27493;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35282;&#24230;&#21516;&#27493;&#38382;&#39064;&#26088;&#22312;&#36890;&#36807;$m$&#20010;&#20559;&#31227;&#37327;$\theta_i-\theta_j \;\mbox{mod} \; 2\pi$&#30340;&#22122;&#22768;&#27979;&#37327;&#20934;&#30830;&#20272;&#35745;&#65288;&#26368;&#22810;&#19968;&#20010;&#24120;&#25968;&#30456;&#20301;&#20559;&#31227;&#65289;&#19968;&#32452;&#26410;&#30693;&#35282;&#24230;$\theta_1, \dots, \theta_n\in[0, 2\pi)$. &#24212;&#29992;&#21253;&#25324;&#20256;&#24863;&#22120;&#32593;&#32476;&#23450;&#20301;&#12289;&#30456;&#20301;&#24674;&#22797;&#21644;&#20998;&#24067;&#24335;&#26102;&#38047;&#21516;&#27493;&#12290;&#35813;&#38382;&#39064;&#30340;&#24322;&#26500;&#25193;&#23637;&#65288;&#31216;&#20026;$k$-&#21516;&#27493;&#65289;&#26159;&#21516;&#26102;&#20272;&#35745;$k$&#32452;&#35282;&#24230;&#65292;&#32473;&#23450;&#27599;&#20010;&#32452;&#30340;&#26410;&#30693;&#32452;&#20998;&#37197;&#30340;&#22122;&#22768;&#35266;&#23519;&#20540;&#12290;&#29616;&#26377;&#30340;&#35282;&#24230;&#21516;&#27493;&#26041;&#27861;&#22312;&#39640;&#22122;&#22768;&#29615;&#22659;&#19979;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#36825;&#22312;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#35282;&#24230;&#21516;&#27493;&#38382;&#39064;&#21450;&#20854;&#24322;&#26500;&#25193;&#23637;&#65292;&#25552;&#20986;&#20102;GNNSync&#65292;&#36825;&#26159;&#19968;&#20010;&#29702;&#35770;&#25903;&#25745;&#30340;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#32534;&#30721;&#35282;&#24230;&#21516;&#27493;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
The angular synchronization problem aims to accurately estimate (up to a constant additive phase) a set of unknown angles $\theta_1, \dots, \theta_n\in[0, 2\pi)$ from $m$ noisy measurements of their offsets $\theta_i-\theta_j \;\mbox{mod} \; 2\pi.$ Applications include, for example, sensor network localization, phase retrieval, and distributed clock synchronization. An extension of the problem to the heterogeneous setting (dubbed $k$-synchronization) is to estimate $k$ groups of angles simultaneously, given noisy observations (with unknown group assignment) from each group. Existing methods for angular synchronization usually perform poorly in high-noise regimes, which are common in applications. In this paper, we leverage neural networks for the angular synchronization problem, and its heterogeneous extension, by proposing GNNSync, a theoretically-grounded end-to-end trainable framework using directed graph neural networks. In addition, new loss functions are devised to encode synchro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#26680;&#35780;&#20998;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#21327;&#26041;&#24046;&#20998;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#37327;&#30340;&#26080;&#20559;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#24212;&#29992;&#22312;&#25193;&#25955;&#27169;&#22411;&#19978;&#21457;&#29616;&#23569;&#25968;&#32676;&#20307;&#30340;&#27169;&#24335;&#22349;&#32553;&#26159;&#19968;&#31181;&#19982;&#36807;&#25311;&#21512;&#30456;&#21453;&#30340;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#26041;&#24046;&#21644;&#39044;&#27979;&#26680;&#29109;&#26159;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#34892;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.05833</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#26680;&#35780;&#20998;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#21327;&#26041;&#24046;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models. (arXiv:2310.05833v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#26680;&#35780;&#20998;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#21327;&#26041;&#24046;&#20998;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#37327;&#30340;&#26080;&#20559;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#24212;&#29992;&#22312;&#25193;&#25955;&#27169;&#22411;&#19978;&#21457;&#29616;&#23569;&#25968;&#32676;&#20307;&#30340;&#27169;&#24335;&#22349;&#32553;&#26159;&#19968;&#31181;&#19982;&#36807;&#25311;&#21512;&#30456;&#21453;&#30340;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#26041;&#24046;&#21644;&#39044;&#27979;&#26680;&#29109;&#26159;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#34892;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#23578;&#19981;&#23384;&#22312;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#27867;&#21270;&#34892;&#20026;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#38382;&#39064;&#36890;&#24120;&#20197;&#19968;&#31181;&#29305;&#23450;&#20219;&#21153;&#30340;&#20020;&#26102;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#12290;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#26041;&#27861;&#19981;&#33021;&#24212;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#29992;&#20110;&#26680;&#35780;&#20998;&#21450;&#20854;&#30456;&#20851;&#29109;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#21327;&#26041;&#24046;&#20998;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27599;&#20010;&#37327;&#30340;&#26080;&#20559;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#65292;&#21482;&#38656;&#35201;&#29983;&#25104;&#26679;&#26412;&#32780;&#19981;&#38656;&#35201;&#24213;&#23618;&#27169;&#22411;&#26412;&#36523;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#23569;&#25968;&#32676;&#20307;&#30340;&#27169;&#24335;&#22349;&#32553;&#26159;&#19968;&#31181;&#19982;&#36807;&#25311;&#21512;&#30456;&#21453;&#30340;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26041;&#24046;&#21644;&#39044;&#27979;&#26680;&#29109;&#26159;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#34892;&#24230;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#26679;&#26412;&#29983;&#25104;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models, like large language models, are becoming increasingly relevant in our daily lives, yet a theoretical framework to assess their generalization behavior and uncertainty does not exist. Particularly, the problem of uncertainty estimation is commonly solved in an ad-hoc manner and task dependent. For example, natural language approaches cannot be transferred to image generation. In this paper we introduce the first bias-variance-covariance decomposition for kernel scores and their associated entropy. We propose unbiased and consistent estimators for each quantity which only require generated samples but not the underlying model itself. As an application, we offer a generalization evaluation of diffusion models and discover how mode collapse of minority groups is a contrary phenomenon to overfitting. Further, we demonstrate that variance and predictive kernel entropy are viable measures of uncertainty for image, audio, and language generation. Specifically, our approach f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#20984;&#38750;&#20984;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#27491;&#21017;&#21270;&#65307;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24369;&#20984;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#23545;&#25239;&#24615;&#26041;&#27861;&#30340;&#25968;&#20540;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05812</link><description>&lt;p&gt;
&#21487;&#35777;&#25910;&#25947;&#30340;&#25968;&#25454;&#39537;&#21160;&#20984;&#38750;&#20984;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Provably Convergent Data-Driven Convex-Nonconvex Regularization. (arXiv:2310.05812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#20984;&#38750;&#20984;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#27491;&#21017;&#21270;&#65307;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24369;&#20984;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#23545;&#25239;&#24615;&#26041;&#27861;&#30340;&#25968;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;&#22120;&#26159;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#26032;&#20852;&#33539;&#24335;&#12290;&#36825;&#23548;&#33268;&#20102;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#26080;&#27861;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20984;&#38750;&#20984;&#65288;CNC&#65289;&#26694;&#26550;&#20013;&#20986;&#29616;&#20102;&#33391;&#23450;&#20041;&#24615;&#21644;&#25910;&#25947;&#24615;&#27491;&#21017;&#21270;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24369;&#20984;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65288;IWCNN&#65289;&#26500;&#24314;&#65292;&#23558;&#23398;&#20064;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#36866;&#24212;&#21040;CNC&#26694;&#26550;&#20013;&#12290;&#20174;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#20043;&#21069;&#23545;&#25239;&#24615;&#26041;&#27861;&#30340;&#25968;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emerging new paradigm for solving inverse problems is via the use of deep learning to learn a regularizer from data. This leads to high-quality results, but often at the cost of provable guarantees. In this work, we show how well-posedness and convergent regularization arises within the convex-nonconvex (CNC) framework for inverse problems. We introduce a novel input weakly convex neural network (IWCNN) construction to adapt the method of learned adversarial regularization to the CNC framework. Empirically we show that our method overcomes numerical issues of previous adversarial methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#21516;&#26102;&#26041;&#31243;&#27169;&#22411;&#21644;&#25511;&#21046;&#20989;&#25968;&#19982;&#20998;&#24067;&#27010;&#25324;&#30340;&#26032;&#36830;&#25509;&#65292;&#35299;&#20915;&#20102;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#24773;&#20917;&#19979;&#65292;&#38024;&#23545;&#19981;&#21516;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05805</link><description>&lt;p&gt;
&#25552;&#21319;&#25511;&#21046;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Boosted Control Functions. (arXiv:2310.05805v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#21516;&#26102;&#26041;&#31243;&#27169;&#22411;&#21644;&#25511;&#21046;&#20989;&#25968;&#19982;&#20998;&#24067;&#27010;&#25324;&#30340;&#26032;&#36830;&#25509;&#65292;&#35299;&#20915;&#20102;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#24773;&#20917;&#19979;&#65292;&#38024;&#23545;&#19981;&#21516;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20026;&#20174;&#22823;&#37327;&#30340;&#21327;&#21464;&#37327;&#20013;&#20934;&#30830;&#39044;&#27979;&#30446;&#26631;&#25968;&#37327;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#27979;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#38544;&#34255;&#28151;&#28102;&#30340;&#24773;&#20917;&#19979;&#12290;&#34429;&#28982;&#23545;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65288;&#20363;&#22914;&#20202;&#22120;&#21464;&#37327;&#65289;&#24050;&#32463;&#23545;&#38544;&#34255;&#28151;&#28102;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#39044;&#27979;&#20219;&#21153;&#26469;&#35828;&#24182;&#38750;&#22914;&#27492;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#35299;&#20915;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#30340;&#24773;&#20917;&#19979;&#65292;&#38024;&#23545;&#19981;&#21516;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#24067;&#27010;&#25324;&#39046;&#22495;&#65292;&#20197;&#21450;&#35745;&#37327;&#32463;&#27982;&#23398;&#20013;&#30340;&#21516;&#26102;&#26041;&#31243;&#27169;&#22411;&#21644;&#25511;&#21046;&#20989;&#25968;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#30340;&#26680;&#24515;&#26159;&#25551;&#36848;&#22312;&#19968;&#32452;&#20998;&#24067;&#36716;&#21464;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#20998;&#24067;&#27010;&#25324;&#21516;&#26102;&#26041;&#31243;&#27169;&#22411;&#65288;SIMDGs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning methods and the availability of large-scale data opened the door to accurately predict target quantities from large sets of covariates. However, existing prediction methods can perform poorly when the training and testing data are different, especially in the presence of hidden confounding. While hidden confounding is well studied for causal effect estimation (e.g., instrumental variables), this is not the case for prediction tasks. This work aims to bridge this gap by addressing predictions under different training and testing distributions in the presence of unobserved confounding. In particular, we establish a novel connection between the field of distribution generalization from machine learning, and simultaneous equation models and control function from econometrics. Central to our contribution are simultaneous equation models for distribution generalization (SIMDGs) which describe the data-generating process under a set of distributional shifts. Within thi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#26377;&#38480;&#24773;&#20917;&#19979;&#65292;&#23545;&#39640;&#32500;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#24418;&#29366;&#36317;&#31163;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#23545;&#24418;&#29366;&#36317;&#31163;&#26631;&#20934;&#20272;&#35745;&#22120;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#19978;&#19979;&#30028;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#25361;&#25112;&#24615;&#36136;&#12290;&#20026;&#20102;&#20811;&#26381;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#30697;&#27861;&#20272;&#35745;&#22120;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39640;&#32500;&#35774;&#32622;&#19979;&#30456;&#23545;&#20110;&#26631;&#20934;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05742</link><description>&lt;p&gt;
&#26377;&#38480;&#37319;&#26679;&#19979;&#31070;&#32463;&#34920;&#31034;&#30340;&#24418;&#29366;&#36317;&#31163;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimating Shape Distances on Neural Representations with Limited Samples. (arXiv:2310.05742v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#26377;&#38480;&#24773;&#20917;&#19979;&#65292;&#23545;&#39640;&#32500;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#24418;&#29366;&#36317;&#31163;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#23545;&#24418;&#29366;&#36317;&#31163;&#26631;&#20934;&#20272;&#35745;&#22120;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#19978;&#19979;&#30028;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#25361;&#25112;&#24615;&#36136;&#12290;&#20026;&#20102;&#20811;&#26381;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#30697;&#27861;&#20272;&#35745;&#22120;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39640;&#32500;&#35774;&#32622;&#19979;&#30456;&#23545;&#20110;&#26631;&#20934;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#31185;&#23398;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#34913;&#37327;&#39640;&#32500;&#32593;&#32476;&#34920;&#31034;&#20043;&#38388;&#30340;&#20960;&#20309;&#30456;&#20284;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#23545;&#23427;&#20204;&#30340;&#32479;&#35745;&#25928;&#29575;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#65292;&#25110;&#32773;&#23545;&#25968;&#25454;&#26377;&#38480;&#24773;&#20917;&#19979;&#30340;&#20272;&#35745;&#22120;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#37327;&#21270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26631;&#20934;&#24418;&#29366;&#36317;&#31163;&#20272;&#35745;&#22120;&#65288;&#30001;Williams et al. (2021)&#25552;&#20986;&#65289;&#30340;&#26368;&#22351;&#24773;&#20917;&#25910;&#25947;&#19978;&#19979;&#30028;&#12290;&#36825;&#20123;&#30028;&#38480;&#25581;&#31034;&#20102;&#22312;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#36825;&#20010;&#38382;&#39064;&#30340;&#25361;&#25112;&#24615;&#36136;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#30697;&#27861;&#20272;&#35745;&#22120;&#65292;&#20855;&#26377;&#21487;&#35843;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#20272;&#35745;&#22120;&#22312;&#27169;&#25311;&#21644;&#31070;&#32463;&#25968;&#25454;&#19978;&#30456;&#23545;&#20110;&#26631;&#20934;&#20272;&#35745;&#22120;&#22312;&#39640;&#32500;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20026;&#39640;&#32500;&#24418;&#29366;&#20998;&#26512;&#22880;&#23450;&#20102;&#20005;&#26684;&#30340;&#32479;&#35745;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring geometric similarity between high-dimensional network representations is a topic of longstanding interest to neuroscience and deep learning. Although many methods have been proposed, only a few works have rigorously analyzed their statistical efficiency or quantified estimator uncertainty in data-limited regimes. Here, we derive upper and lower bounds on the worst-case convergence of standard estimators of shape distance$\unicode{x2014}$a measure of representational dissimilarity proposed by Williams et al. (2021). These bounds reveal the challenging nature of the problem in high-dimensional feature spaces. To overcome these challenges, we introduce a new method-of-moments estimator with a tunable bias-variance tradeoff. We show that this estimator achieves superior performance to standard estimators in simulation and on neural data, particularly in high-dimensional settings. Thus, we lay the foundation for a rigorous statistical theory for high-dimensional shape analysis, an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#39564;&#20559;&#24046;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#22312;&#28385;&#36275;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#22522;&#20110;&#20559;&#24046;&#20998;&#25968;&#30340;&#20462;&#25913;&#35268;&#21017;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05725</link><description>&lt;p&gt;
&#21518;&#39564;&#20559;&#24046;&#35780;&#20998;&#23545;&#20844;&#24179;&#20998;&#31867;&#26368;&#20248;
&lt;/p&gt;
&lt;p&gt;
Post-hoc Bias Scoring Is Optimal For Fair Classification. (arXiv:2310.05725v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#39564;&#20559;&#24046;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#22312;&#28385;&#36275;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#22522;&#20110;&#20559;&#24046;&#20998;&#25968;&#30340;&#20462;&#25913;&#35268;&#21017;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22312;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#26159;&#20154;&#21475;&#32479;&#35745;&#23398;&#20844;&#24179;&#24615;&#65288;DP&#65289;&#65292;&#26426;&#20250;&#22343;&#31561;&#65288;EOp&#65289;&#25110;&#31561;&#27010;&#29575;&#65288;EO&#65289;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#36125;&#21494;&#26031;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#26126;&#30830;&#29305;&#24449;&#21270;&#65292;&#32467;&#26524;&#26159;&#19981;&#21463;&#32422;&#26463;&#20998;&#31867;&#22120;&#30340;&#31616;&#21333;&#20462;&#25913;&#35268;&#21017;&#12290;&#21363;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#32423;&#21035;&#30340;&#20559;&#24046;&#24230;&#37327;&#65292;&#31216;&#20026;&#20559;&#24046;&#20998;&#25968;&#65292;&#32780;&#20462;&#25913;&#35268;&#21017;&#21017;&#26159;&#22312;&#26377;&#38480;&#37327;&#30340;&#20559;&#24046;&#20998;&#25968;&#20043;&#19978;&#30340;&#31616;&#21333;&#32447;&#24615;&#35268;&#21017;&#12290;&#22522;&#20110;&#36825;&#20010;&#29305;&#24449;&#21270;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21518;&#39564;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36866;&#24212;&#20844;&#24179;&#24615;&#32422;&#26463;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;DP&#21644;EOp&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#20462;&#25913;&#35268;&#21017;&#26159;&#22522;&#20110;&#21333;&#20010;&#20559;&#24046;&#20998;&#25968;&#30340;&#38408;&#20540;&#36873;&#25321;&#65292;&#32780;&#22312;EO&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#38656;&#35201;&#35843;&#25972;&#20855;&#26377;2&#20010;&#21442;&#25968;&#30340;&#32447;&#24615;&#20462;&#25913;&#35268;&#21017;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#29992;&#20110;&#21253;&#21547;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#30340;&#22797;&#21512;&#32676;&#20307;&#20844;&#24179;&#24615;&#26631;&#20934;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a binary classification problem under group fairness constraints, which can be one of Demographic Parity (DP), Equalized Opportunity (EOp), or Equalized Odds (EO). We propose an explicit characterization of Bayes optimal classifier under the fairness constraints, which turns out to be a simple modification rule of the unconstrained classifier. Namely, we introduce a novel instance-level measure of bias, which we call bias score, and the modification rule is a simple linear rule on top of the finite amount of bias scores. Based on this characterization, we develop a post-hoc approach that allows us to adapt to fairness constraints while maintaining high accuracy. In the case of DP and EOp constraints, the modification rule is thresholding a single bias score, while in the case of EO constraints we are required to fit a linear modification rule with 2 parameters. The method can also be applied for composite group-fairness criteria, such as ones involving several sensitive att
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#34701;&#21512;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#40784;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#24182;&#20801;&#35768;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#21387;&#32553;Transformer&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.05719</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#22120;&#21512;&#24182;Transformer
&lt;/p&gt;
&lt;p&gt;
Transformer Fusion with Optimal Transport. (arXiv:2310.05719v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#34701;&#21512;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#40784;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#24182;&#20801;&#35768;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#21387;&#32553;Transformer&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#26159;&#19968;&#31181;&#23558;&#22810;&#20010;&#29420;&#31435;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21512;&#24182;&#20197;&#32467;&#21512;&#23427;&#20204;&#30340;&#33021;&#21147;&#30340;&#25216;&#26415;&#12290;&#36807;&#21435;&#30340;&#23581;&#35797;&#20165;&#38480;&#20110;&#20840;&#36830;&#25509;&#12289;&#21367;&#31215;&#21644;&#27531;&#24046;&#32593;&#32476;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#34701;&#21512;&#20004;&#20010;&#25110;&#22810;&#20010;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#65292;&#20197;&#65288;&#36719;&#65289;&#23545;&#40784;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#19968;&#31181;&#23618;&#23545;&#40784;&#30340;&#25277;&#35937;&#26041;&#27861;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20219;&#24847;&#26550;&#26500;&#65292;&#20363;&#22914;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#28040;&#34701;&#30740;&#31350;&#35752;&#35770;&#20102;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#26550;&#26500;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65288;&#24322;&#26500;&#34701;&#21512;&#65289;&#65292;&#20026;Transformer&#30340;&#21387;&#32553;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;Vision Transformer&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Fusion is a technique for merging multiple independently-trained neural networks in order to combine their capabilities. Past attempts have been restricted to the case of fully-connected, convolutional, and residual networks. In this paper, we present a systematic approach for fusing two or more transformer-based networks exploiting Optimal Transport to (soft-)align the various architectural components. We flesh out an abstraction for layer alignment, that can generalize to arbitrary architectures -- in principle -and we apply this to the key ingredients of Transformers such as multi-head self-attention, layer-normalization, and residual connections, and we discuss how to handle them via various ablation studies. Furthermore, our method allows the fusion of models of different sizes (heterogeneous fusion), providing a new and efficient way for compression of Transformers. The proposed approach is evaluated on both image classification tasks via Vision Transformer and natural language
&lt;/p&gt;</description></item><item><title>&#22810;&#27493;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#30446;&#26631;&#26469;&#35757;&#32451;&#19968;&#27493;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#36712;&#36857;&#38271;&#24230;&#22686;&#38271;&#26102;&#19968;&#27493;&#39044;&#27979;&#35823;&#24046;&#30340;&#32047;&#31215;&#38382;&#39064;&#65292;&#24182;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.05672</link><description>&lt;p&gt;
&#22810;&#27493;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-timestep models for Model-based Reinforcement Learning. (arXiv:2310.05672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05672
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27493;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#30446;&#26631;&#26469;&#35757;&#32451;&#19968;&#27493;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#36712;&#36857;&#38271;&#24230;&#22686;&#38271;&#26102;&#19968;&#27493;&#39044;&#27979;&#35823;&#24046;&#30340;&#32047;&#31215;&#38382;&#39064;&#65292;&#24182;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#20381;&#36182;&#20110;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#27169;&#25311;&#36712;&#36857;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#30340;&#22686;&#38271;&#65292;&#19968;&#27493;&#39044;&#27979;&#35823;&#24046;&#30340;&#32047;&#31215;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#30446;&#26631;&#26469;&#35757;&#32451;&#19968;&#27493;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#21508;&#31181;&#26410;&#26469;&#26102;&#38388;&#27573;&#19978;&#30340;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65288;&#20363;&#22914;&#65292;&#36127;&#23545;&#25968;&#20284;&#28982;&#65289;&#30340;&#21152;&#26435;&#21644;&#12290;&#25105;&#20204;&#25506;&#32034;&#21644;&#27979;&#35797;&#20102;&#19968;&#31995;&#21015;&#26435;&#37325;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#25351;&#25968;&#34928;&#20943;&#26435;&#37325;&#23548;&#33268;&#27169;&#22411;&#22312;&#38271;&#26102;&#38388;&#27573;&#30340;R2&#24471;&#20998;&#26174;&#33879;&#25552;&#39640;&#12290;&#24403;&#27169;&#22411;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#36825;&#31181;&#25913;&#36827;&#23588;&#20026;&#26126;&#26174;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#32431;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#36845;&#20195;&#25209;&#37327;RL&#22330;&#26223;&#20013;&#20351;&#29992;&#36719;&#20214;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;SAC&#65289;&#20195;&#29702;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#22810;&#27493;&#27169;&#22411;&#20248;&#20110;&#25110;&#19982;&#26631;&#20934;&#30340;&#19968;&#27493;&#27169;&#22411;&#30456;&#21305;&#37197;&#12290;&#36825;&#22312;&#32771;&#34385;&#29615;&#22659;&#30340;&#22122;&#22768;&#21464;&#20307;&#20013;&#23588;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
In model-based reinforcement learning (MBRL), most algorithms rely on simulating trajectories from one-step dynamics models learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as length of the trajectory grows. In this paper we tackle this issue by using a multi-timestep objective to train one-step models. Our objective is a weighted sum of a loss function (e.g., negative log-likelihood) at various future horizons. We explore and test a range of weights profiles. We find that exponentially decaying weights lead to models that significantly improve the long-horizon R2 score. This improvement is particularly noticeable when the models were evaluated on noisy data. Finally, using a soft actor-critic (SAC) agent in pure batch reinforcement learning (RL) and iterated batch RL scenarios, we found that our multi-timestep models outperform or match standard one-step models. This was especially evident in a noisy variant of the considered envi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21487;&#36870;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#31185;&#22827;&#38142;&#65292;&#21363;&#8220;&#22240;&#26524;Zig-Zag&#37319;&#26679;&#22120;&#8221;&#65292;&#29992;&#20110;&#25512;&#26029;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#37327;&#21464;&#37327;&#65292;&#35813;&#37319;&#26679;&#22120;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#28151;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05655</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#37327;&#36827;&#34892;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#65306;&#22312;DAG&#30340;Markov&#31561;&#20215;&#31867;&#19978;&#37319;&#26679;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Causal structure learning with momentum: Sampling distributions over Markov Equivalence Classes of DAGs. (arXiv:2310.05655v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21487;&#36870;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#31185;&#22827;&#38142;&#65292;&#21363;&#8220;&#22240;&#26524;Zig-Zag&#37319;&#26679;&#22120;&#8221;&#65292;&#29992;&#20110;&#25512;&#26029;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#37327;&#21464;&#37327;&#65292;&#35813;&#37319;&#26679;&#22120;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#28151;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#26029;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#65288;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;DAG&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#21487;&#36870;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#31185;&#22827;&#38142;&#65292;&#21363;&#8220;&#22240;&#26524;Zig-Zag&#37319;&#26679;&#22120;&#8221;&#65292;&#35813;&#37319;&#26679;&#22120;&#38024;&#23545;&#19968;&#31867;&#35266;&#27979;&#31561;&#20215;&#65288;Markov&#31561;&#20215;&#65289;DAG&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#20123;&#31867;&#21035;&#20197;&#23436;&#25104;&#30340;&#37096;&#20998;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;CPDAG&#65289;&#34920;&#31034;&#12290;&#38750;&#21487;&#36870;&#39532;&#23572;&#31185;&#22827;&#38142;&#20381;&#36182;&#20110;Chickering&#30340;&#36138;&#23146;&#31561;&#20215;&#25628;&#32034;&#65288;GES&#65289;&#20013;&#20351;&#29992;&#30340;&#25805;&#20316;&#31526;&#65292;&#24182;&#19988;&#20855;&#26377;&#19968;&#20010;&#21160;&#37327;&#21464;&#37327;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#28151;&#21512;&#24615;&#33021;&#12290;&#21487;&#33021;&#30340;&#30446;&#26631;&#20998;&#24067;&#21253;&#25324;&#22522;&#20110;DAG&#20808;&#39564;&#21644;Markov&#31561;&#20215;&#20284;&#28982;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23454;&#29616;&#65292;&#20854;&#20013;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#21015;&#20030;&#12289;&#35745;&#25968;&#12289;&#22343;&#21248;&#37319;&#26679;&#21644;&#24212;&#29992;GES&#25805;&#20316;&#31526;&#30340;&#21487;&#33021;&#31227;&#21160;&#65292;&#25152;&#26377;&#36825;&#20123;&#31639;&#27861;&#37117;&#26174;&#33879;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of inferring a Bayesian network structure (directed acyclic graph, DAG for short), we devise a non-reversible continuous time Markov chain, the "Causal Zig-Zag sampler", that targets a probability distribution over classes of observationally equivalent (Markov equivalent) DAGs. The classes are represented as completed partially directed acyclic graphs (CPDAGs). The non-reversible Markov chain relies on the operators used in Chickering's Greedy Equivalence Search (GES) and is endowed with a momentum variable, which improves mixing significantly as we show empirically. The possible target distributions include posterior distributions based on a prior over DAGs and a Markov equivalent likelihood. We offer an efficient implementation wherein we develop new algorithms for listing, counting, uniformly sampling, and applying possible moves of the GES operators, all of which significantly improve upon the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#20803;&#32467;&#26524;&#25552;&#21319;&#24314;&#27169;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#38646;&#32467;&#26524;&#26679;&#26412;&#30340;&#20449;&#24687;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#12290; (arXiv:2310.05549v1 [stat.ML])</title><link>http://arxiv.org/abs/2310.05549</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#20108;&#20803;&#32467;&#26524;&#25552;&#21319;&#24314;&#27169;&#36716;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Transformation Approach for Uplift Modeling with Binary Outcome. (arXiv:2310.05549v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#20803;&#32467;&#26524;&#25552;&#21319;&#24314;&#27169;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#38646;&#32467;&#26524;&#26679;&#26412;&#30340;&#20449;&#24687;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#12290; (arXiv:2310.05549v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#24314;&#27169;&#22312;&#24066;&#22330;&#33829;&#38144;&#21644;&#23458;&#25143;&#20445;&#30041;&#31561;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#26377;&#25928;&#24212;&#29992;&#65292;&#29992;&#20110;&#38024;&#23545;&#37027;&#20123;&#30001;&#20110;&#27963;&#21160;&#25110;&#27835;&#30103;&#26356;&#26377;&#21487;&#33021;&#20135;&#29983;&#21453;&#24212;&#30340;&#23458;&#25143;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#20803;&#32467;&#26524;&#36716;&#25442;&#26041;&#27861;&#65292;&#35299;&#38145;&#20102;&#38646;&#32467;&#26524;&#26679;&#26412;&#30340;&#20840;&#37096;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uplift modeling has been used effectively in fields such as marketing and customer retention, to target those customers who are more likely to respond due to the campaign or treatment. Essentially, it is a machine learning technique that predicts the gain from performing some action with respect to not taking it. A popular class of uplift models is the transformation approach that redefines the target variable with the original treatment indicator. These transformation approaches only need to train and predict the difference in outcomes directly. The main drawback of these approaches is that in general it does not use the information in the treatment indicator beyond the construction of the transformed outcome and usually is not efficient. In this paper, we design a novel transformed outcome for the case of the binary target variable and unlock the full value of the samples with zero outcome. From a practical perspective, our new approach is flexible and easy to use. Experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20855;&#26377;&#37325;&#22797;&#36793;&#30340;&#26080;&#38480;&#26102;&#38388;&#24207;&#21015;&#22270;&#25237;&#23556;&#21040;&#26377;&#38480;&#36793;&#38469;&#22270;&#19978;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20197;&#24448;&#26080;&#27861;&#22788;&#29702;&#30340;$m$-&#20998;&#31163;&#26597;&#35810;&#20219;&#21153;&#65292;&#24182;&#20026;&#26102;&#38388;&#24207;&#21015;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#22240;&#26524;&#25928;&#26524;&#20272;&#35745;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2310.05526</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#35770;&#23558;&#26080;&#38480;&#26102;&#38388;&#24207;&#21015;&#22270;&#25237;&#23556;&#21040;&#26377;&#38480;&#36793;&#38469;&#22270;
&lt;/p&gt;
&lt;p&gt;
Projecting infinite time series graphs to finite marginal graphs using number theory. (arXiv:2310.05526v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20855;&#26377;&#37325;&#22797;&#36793;&#30340;&#26080;&#38480;&#26102;&#38388;&#24207;&#21015;&#22270;&#25237;&#23556;&#21040;&#26377;&#38480;&#36793;&#38469;&#22270;&#19978;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20197;&#24448;&#26080;&#27861;&#22788;&#29702;&#30340;$m$-&#20998;&#31163;&#26597;&#35810;&#20219;&#21153;&#65292;&#24182;&#20026;&#26102;&#38388;&#24207;&#21015;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#22240;&#26524;&#25928;&#26524;&#20272;&#35745;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#24037;&#20316;&#23558;&#22240;&#26524;&#22270;&#27169;&#22411;&#26694;&#26550;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20854;&#20013;&#24456;&#22810;&#24037;&#20316;&#20351;&#29992;&#20102;&#26102;&#38388;&#20998;&#36776;&#30340;&#22240;&#26524;&#22270;&#65292;&#36825;&#20123;&#22270;&#22312;&#36807;&#21435;&#21644;&#26410;&#26469;&#37117;&#24310;&#23637;&#21040;&#26080;&#38480;&#36828;&#65292;&#24182;&#19988;&#36793;&#22312;&#26102;&#38388;&#19978;&#26159;&#37325;&#22797;&#30340;&#65292;&#20307;&#29616;&#20102;&#31283;&#24577;&#22240;&#26524;&#20851;&#31995;&#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22240;&#26524;&#22270;&#27169;&#22411;&#26694;&#26550;&#30340;&#22823;&#22810;&#25968;&#32467;&#26524;&#21644;&#31639;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;&#26080;&#38480;&#22270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#20855;&#26377;&#37325;&#22797;&#36793;&#30340;&#26080;&#38480;&#26102;&#38388;&#24207;&#21015;&#22270;&#25237;&#23556;&#21040;&#26377;&#38480;&#26102;&#38388;&#31383;&#21475;&#19978;&#30340;&#36793;&#38469;&#22270;&#27169;&#22411;&#12290;&#36825;&#20123;&#26377;&#38480;&#36793;&#38469;&#22270;&#23545;&#20110;&#19982;&#26080;&#38480;&#22270;&#30456;&#20851;&#30340;$m$-&#20998;&#31163;&#26597;&#35810;&#25552;&#20379;&#20102;&#31572;&#26696;&#65292;&#36825;&#26159;&#20197;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#36793;&#38469;&#22270;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#22240;&#26524;&#25928;&#26524;&#20272;&#35745;&#26159;&#26377;&#29992;&#30340;&#65292;&#21487;&#20197;&#23558;&#20026;&#26377;&#38480;&#22270;&#24320;&#21457;&#30340;&#32467;&#26524;&#24212;&#29992;&#21040;&#26080;&#38480;&#22270;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a growing number of method and application works have adapted and applied the causal-graphical-model framework to time series data. Many of these works employ time-resolved causal graphs that extend infinitely into the past and future and whose edges are repetitive in time, thereby reflecting the assumption of stationary causal relationships. However, most results and algorithms from the causal-graphical-model framework are not designed for infinite graphs. In this work, we develop a method for projecting infinite time series graphs with repetitive edges to marginal graphical models on a finite time window. These finite marginal graphs provide the answers to $m$-separation queries with respect to the infinite graph, a task that was previously unresolved. Moreover, we argue that these marginal graphs are useful for causal discovery and causal effect estimation in time series, effectively enabling to apply results developed for finite graphs to the infinite graphs. The p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#35270;&#35282;&#30340;&#32852;&#37030;&#24179;&#22343;&#26041;&#27861;&#22312;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.05495</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#32852;&#37030;&#24179;&#22343;&#22312;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Neural Tangent Kernel View on Federated Averaging for Deep Linear Neural Network. (arXiv:2310.05495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#35270;&#35282;&#30340;&#32852;&#37030;&#24179;&#22343;&#26041;&#27861;&#22312;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#26469;&#33258;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#12290;&#22914;&#20170;&#65292;&#30001;&#20110;&#20854;&#21331;&#36234;&#24615;&#33021;&#65292;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;FedAvg&#20013;&#30340;&#39318;&#36873;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#26159;&#38750;&#20984;&#30340;&#29978;&#33267;&#26159;&#38750;&#20809;&#28369;&#30340;&#12290;&#27492;&#22806;&#65292;FedAvg&#24635;&#26159;&#28041;&#21450;&#22810;&#20010;&#23458;&#25143;&#31471;&#21644;&#26412;&#22320;&#26356;&#26032;&#65292;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#26356;&#26032;&#26041;&#21521;&#12290;&#36825;&#20123;&#23646;&#24615;&#32473;&#20998;&#26512;FedAvg&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#26368;&#36817;&#65292;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#29702;&#35770;&#24050;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#29702;&#35299;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#38750;&#20984;&#38382;&#39064;&#20013;&#30340;&#19968;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#26159;&#29702;&#35770;&#23398;&#31185;&#20013;&#30340;&#32463;&#20856;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#30340;&#20844;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#19978;&#65292;&#23545;&#20110;FedAvg&#30340;&#25910;&#25947;&#24615;&#30446;&#21069;&#36824;&#27809;&#26377;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated averaging (FedAvg) is a widely employed paradigm for collaboratively training models from distributed clients without sharing data. Nowadays, the neural network has achieved remarkable success due to its extraordinary performance, which makes it a preferred choice as the model in FedAvg. However, the optimization problem of the neural network is often non-convex even non-smooth. Furthermore, FedAvg always involves multiple clients and local updates, which results in an inaccurate updating direction. These properties bring difficulties in analyzing the convergence of FedAvg in training neural networks. Recently, neural tangent kernel (NTK) theory has been proposed towards understanding the convergence of first-order methods in tackling the non-convex problem of neural networks. The deep linear neural network is a classical model in theoretical subject due to its simple formulation. Nevertheless, there exists no theoretical result for the convergence of FedAvg in training the d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#31215;&#20998;&#35757;&#32451;&#30340;&#26102;&#31354;&#22810;&#27169;&#24577;&#21327;&#21464;&#28145;&#24230;&#20869;&#26680;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#25968;&#21305;&#37197;&#26041;&#27861;&#21644;&#21435;&#22122;&#20998;&#25968;&#21305;&#37197;&#26041;&#27861;&#35299;&#20915;&#20102;&#35757;&#32451;&#22256;&#38590;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.05485</link><description>&lt;p&gt;
&#26080;&#31215;&#20998;&#35757;&#32451;&#30340;&#26102;&#31354;&#22810;&#27169;&#24577;&#21327;&#21464;&#28145;&#24230;&#20869;&#26680;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Integration-free Training for Spatio-temporal Multimodal Covariate Deep Kernel Point Processes. (arXiv:2310.05485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#31215;&#20998;&#35757;&#32451;&#30340;&#26102;&#31354;&#22810;&#27169;&#24577;&#21327;&#21464;&#28145;&#24230;&#20869;&#26680;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#25968;&#21305;&#37197;&#26041;&#27861;&#21644;&#21435;&#22122;&#20998;&#25968;&#21305;&#37197;&#26041;&#27861;&#35299;&#20915;&#20102;&#35757;&#32451;&#22256;&#38590;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#26102;&#31354;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;Deep Kernel Mixture Point Processes (DKMPP)&#65292;&#23427;&#34701;&#21512;&#20102;&#22810;&#27169;&#24577;&#21327;&#21464;&#20449;&#24687;&#12290;DKMPP&#26159;Deep Mixture Point Processes (DMPP)&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#20351;&#29992;&#26356;&#28789;&#27963;&#30340;&#28145;&#24230;&#20869;&#26680;&#24314;&#27169;&#20107;&#20214;&#21644;&#21327;&#21464;&#25968;&#25454;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;DKMPP&#30340;&#38750;&#21487;&#31215;&#28145;&#24230;&#20869;&#26680;&#36896;&#25104;&#30340;&#35757;&#32451;&#22256;&#38590;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#20998;&#25968;&#21305;&#37197;&#30340;&#26080;&#31215;&#20998;&#26041;&#27861;&#65292;&#24182;&#36827;&#19968;&#27493;&#36890;&#36807;&#37319;&#29992;&#21487;&#25193;&#23637;&#30340;&#21435;&#22122;&#20998;&#25968;&#21305;&#37197;&#26041;&#27861;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DKMPP&#21450;&#20854;&#30456;&#24212;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#20272;&#35745;&#22120;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#34701;&#21512;&#21327;&#21464;&#20449;&#24687;&#12289;&#21033;&#29992;&#28145;&#24230;&#20869;&#26680;&#21644;&#37319;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#20272;&#35745;&#22120;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a novel deep spatio-temporal point process model, Deep Kernel Mixture Point Processes (DKMPP), that incorporates multimodal covariate information. DKMPP is an enhanced version of Deep Mixture Point Processes (DMPP), which uses a more flexible deep kernel to model complex relationships between events and covariate data, improving the model's expressiveness. To address the intractable training procedure of DKMPP due to the non-integrable deep kernel, we utilize an integration-free method based on score matching, and further improve efficiency by adopting a scalable denoising score matching method. Our experiments demonstrate that DKMPP and its corresponding score-based estimators outperform baseline models, showcasing the advantages of incorporating covariate information, utilizing a deep kernel, and employing score-based estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;EIF+&#21644;ExIFFI&#20004;&#31181;&#25913;&#36827;&#20102;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#25512;&#24191;&#33021;&#21147;&#21644;&#35299;&#37322;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.05468</link><description>&lt;p&gt;
ExIFFI&#21644;EIF+&#65306;&#35299;&#37322;&#24615;&#21644;&#22686;&#24378;&#30340;&#25512;&#24191;&#33021;&#21147;&#20197;&#25193;&#23637;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
ExIFFI and EIF+: Interpretability and Enhanced Generalizability to Extend the Extended Isolation Forest. (arXiv:2310.05468v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;EIF+&#21644;ExIFFI&#20004;&#31181;&#25913;&#36827;&#20102;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#25512;&#24191;&#33021;&#21147;&#21644;&#35299;&#37322;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#28041;&#21450;&#22312;&#22797;&#26434;&#25968;&#25454;&#38598;&#21644;&#31995;&#32479;&#20013;&#35782;&#21035;&#24322;&#24120;&#34892;&#20026;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;DSS&#65289;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20165;&#20165;&#23450;&#20301;&#24322;&#24120;&#24448;&#24448;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#19981;&#36275;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#29992;&#25143;&#36890;&#24120;&#38656;&#35201;&#20102;&#35299;&#39044;&#27979;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#20197;&#20415;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24182;&#22686;&#24378;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#24615;&#36136;&#65292;&#21019;&#24314;&#21487;&#35299;&#37322;&#30340;&#24037;&#20855;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EIF+&#65292;&#36825;&#26159;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;&#65288;EIF&#65289;&#30340;&#22686;&#24378;&#21464;&#20307;&#65292;&#26088;&#22312;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ExIFFI&#65292;&#19968;&#31181;&#23558;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;&#19982;&#35299;&#37322;&#24615;&#21151;&#33021;&#65288;&#29305;&#24449;&#25490;&#21517;&#65289;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#25552;&#20379;&#20102;&#20197;&#23396;&#31435;&#22522;&#20110;&#26041;&#27861;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection, an essential unsupervised machine learning task, involves identifying unusual behaviors within complex datasets and systems. While Machine Learning algorithms and decision support systems (DSSs) offer effective solutions for this task, simply pinpointing anomalies often falls short in real-world applications. Users of these systems often require insight into the underlying reasons behind predictions to facilitate Root Cause Analysis and foster trust in the model. However, due to the unsupervised nature of anomaly detection, creating interpretable tools is challenging. This work introduces EIF+, an enhanced variant of Extended Isolation Forest (EIF), designed to enhance generalization capabilities. Additionally, we present ExIFFI, a novel approach that equips Extended Isolation Forest with interpretability features, specifically feature rankings. Experimental results provide a comprehensive comparative analysis of Isolation-based approaches for Anomaly Detection, incl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;Spike-and-Slab&#20808;&#39564;&#30340;&#26041;&#31243;&#24335;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#22238;&#24402;&#21644;&#36125;&#21494;&#26031;&#31232;&#30095;&#20998;&#24067;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#39640;&#25928;&#30340;&#21518;&#39564;&#25512;&#26029;&#21644;&#20989;&#25968;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.05387</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;Spike-and-Slab&#20808;&#39564;&#21644;&#39640;&#25928;&#26680;&#20989;&#25968;&#30340;&#26041;&#31243;&#24335;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Equation Discovery with Bayesian Spike-and-Slab Priors and Efficient Kernels. (arXiv:2310.05387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;Spike-and-Slab&#20808;&#39564;&#30340;&#26041;&#31243;&#24335;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#22238;&#24402;&#21644;&#36125;&#21494;&#26031;&#31232;&#30095;&#20998;&#24067;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#39640;&#25928;&#30340;&#21518;&#39564;&#25512;&#26029;&#21644;&#20989;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#23545;&#20110;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#25104;&#21151;&#26696;&#20363;&#65292;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#30528;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#38543;&#22788;&#21487;&#35265;&#12290;&#27492;&#22806;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#32570;&#20047;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;/&#25110;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;Spike-and-Slab&#20808;&#39564;&#65288;KBASS&#65289;&#30340;&#26032;&#22411;&#26041;&#31243;&#24335;&#21457;&#29616;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26680;&#22238;&#24402;&#26469;&#20272;&#35745;&#30446;&#26631;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#34920;&#36798;&#21147;&#65292;&#24182;&#19988;&#23545;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#26356;&#21152;&#31283;&#20581;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#36125;&#21494;&#26031;Spike-and-Slab&#20808;&#39564;&#32467;&#21512;&#20351;&#29992;&#65292;&#21518;&#32773;&#26159;&#19968;&#31181;&#29702;&#24819;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#20998;&#24067;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#31639;&#23376;&#36873;&#25321;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#26399;&#26395;&#20256;&#25773;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EP-EM&#65289;&#31639;&#27861;&#30340;&#26377;&#25928;&#21518;&#39564;&#25512;&#26029;&#21644;&#20989;&#25968;&#20272;&#35745;&#26041;&#27861;&#12290;&#20026;&#20102;&#20811;&#26381;&#26680;&#22238;&#24402;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering governing equations from data is important to many scientific and engineering applications. Despite promising successes, existing methods are still challenged by data sparsity as well as noise issues, both of which are ubiquitous in practice. Moreover, state-of-the-art methods lack uncertainty quantification and/or are costly in training. To overcome these limitations, we propose a novel equation discovery method based on Kernel learning and BAyesian Spike-and-Slab priors (KBASS). We use kernel regression to estimate the target function, which is flexible, expressive, and more robust to data sparsity and noises. We combine it with a Bayesian spike-and-slab prior -- an ideal Bayesian sparse distribution -- for effective operator selection and uncertainty quantification. We develop an expectation propagation expectation-maximization (EP-EM) algorithm for efficient posterior inference and function estimation. To overcome the computational challenge of kernel regression, we pla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#32943;&#23450;&#30340;&#31572;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#21253;&#25324;&#26368;&#22823;&#21106;&#21644;&#26368;&#23567;&#21106;&#12289;&#26368;&#22823;$k$&#32422;&#26463;&#38382;&#39064;&#12289;&#26368;&#22823;&#26435;&#37325;&#20108;&#20998;&#22270;&#21305;&#37197;&#21644;&#26053;&#34892;&#21830;&#38382;&#39064;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#32452;&#21512;&#38382;&#39064;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.05309</link><description>&lt;p&gt;
&#20248;&#21270;&#32452;&#21512;&#38382;&#39064;&#30340;&#35299;&#37319;&#26679;&#22120;&#65306;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#26799;&#24230;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods. (arXiv:2310.05309v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#32943;&#23450;&#30340;&#31572;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#21253;&#25324;&#26368;&#22823;&#21106;&#21644;&#26368;&#23567;&#21106;&#12289;&#26368;&#22823;$k$&#32422;&#26463;&#38382;&#39064;&#12289;&#26368;&#22823;&#26435;&#37325;&#20108;&#20998;&#22270;&#21305;&#37197;&#21644;&#26053;&#34892;&#21830;&#38382;&#39064;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#32452;&#21512;&#38382;&#39064;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#32452;&#21512;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#29992;&#20316;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#22120;&#65292;&#28982;&#21518;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#31561;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#36880;&#27493;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#31215;&#26497;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#21253;&#25324;&#26368;&#22823;&#21106;&#21644;&#26368;&#23567;&#21106;&#12289;&#26368;&#22823;$k$&#32422;&#26463;&#38382;&#39064;&#12289;&#26368;&#22823;&#26435;&#37325;&#20108;&#20998;&#22270;&#21305;&#37197;&#21644;&#26053;&#34892;&#21830;&#38382;&#39064;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Reinforcement Learning methods have empirically shown great promise in tackling challenging combinatorial problems. In those methods a deep neural network is used as a solution generator which is then trained by gradient-based methods (e.g., policy gradient) to successively obtain better solution distributions. In this work we introduce a novel theoretical framework for analyzing the effectiveness of such methods. We ask whether there exist generative models that (i) are expressive enough to generate approximately optimal solutions; (ii) have a tractable, i.e, polynomial in the size of the input, number of parameters; (iii) their optimization landscape is benign in the sense that it does not contain sub-optimal stationary points. Our main contribution is a positive answer to this question. Our result holds for a broad class of combinatorial problems including Max- and Min-Cut, Max-$k$-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem. A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#65292;&#24182;&#32473;&#20986;&#20102;&#25915;&#20987;&#21487;&#33021;&#24615;&#30340;&#26465;&#20214;&#12290;&#19982;&#20197;&#24448;&#23545;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#29702;&#35299;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#29305;&#23450;CMAB&#23454;&#20363;&#30340;&#25915;&#20987;&#21487;&#33021;&#24615;&#36824;&#21462;&#20915;&#20110;&#21457;&#21733;&#23454;&#20363;&#26159;&#21542;&#34987;&#23545;&#25163;&#30693;&#26195;&#12290;&#36825;&#34920;&#26126;&#22312;&#23454;&#36341;&#20013;&#23545;CMAB&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23545;&#25163;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#26080;&#27861;&#20102;&#35299;&#29615;&#22659;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.05308</link><description>&lt;p&gt;
&#23545;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks on Combinatorial Multi-Armed Bandits. (arXiv:2310.05308v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#65292;&#24182;&#32473;&#20986;&#20102;&#25915;&#20987;&#21487;&#33021;&#24615;&#30340;&#26465;&#20214;&#12290;&#19982;&#20197;&#24448;&#23545;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#29702;&#35299;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#29305;&#23450;CMAB&#23454;&#20363;&#30340;&#25915;&#20987;&#21487;&#33021;&#24615;&#36824;&#21462;&#20915;&#20110;&#21457;&#21733;&#23454;&#20363;&#26159;&#21542;&#34987;&#23545;&#25163;&#30693;&#26195;&#12290;&#36825;&#34920;&#26126;&#22312;&#23454;&#36341;&#20013;&#23545;CMAB&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23545;&#25163;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#26080;&#27861;&#20102;&#35299;&#29615;&#22659;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;CMAB&#65289;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#20102;CMAB&#25915;&#20987;&#21487;&#33021;&#24615;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#21462;&#20915;&#20110;&#30456;&#24212;CMAB&#23454;&#20363;&#30340;&#20869;&#22312;&#29305;&#24615;&#65292;&#22914;&#36229;&#33218;&#30340;&#22870;&#21169;&#20998;&#24067;&#21644;&#22522;&#26412;&#33218;&#30340;&#32467;&#26524;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#21487;&#25915;&#20987;CMAB&#23454;&#20363;&#30340;&#25915;&#20987;&#31639;&#27861;&#12290;&#19982;&#20197;&#24448;&#23545;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#29702;&#35299;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#20107;&#23454;&#65292;&#21363;&#29305;&#23450;CMAB&#23454;&#20363;&#30340;&#25915;&#20987;&#21487;&#33021;&#24615;&#36824;&#21462;&#20915;&#20110;&#21457;&#21733;&#23454;&#20363;&#26159;&#21542;&#34987;&#23545;&#25163;&#30693;&#26195;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;CMAB&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#23454;&#36341;&#20013;&#24456;&#22256;&#38590;&#65292;&#24182;&#19988;&#19981;&#23384;&#22312;&#36866;&#29992;&#20110;&#20219;&#20309;CMAB&#23454;&#20363;&#30340;&#36890;&#29992;&#25915;&#20987;&#31574;&#30053;&#65292;&#22240;&#20026;&#29615;&#22659;&#23545;&#20110;&#23545;&#25163;&#26469;&#35828;&#22823;&#37096;&#20998;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#23454;&#38469;CMAB&#24212;&#29992;&#65288;&#21253;&#25324;&#27010;&#29575;&#26368;&#22823;&#35206;&#30422;&#38382;&#39064;&#12289;&#22312;&#32447;&#26368;&#23567;&#29983;&#25104;&#26641;&#38382;&#39064;&#65289;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study reward poisoning attacks on Combinatorial Multi-armed Bandits (CMAB). We first provide a sufficient and necessary condition for the attackability of CMAB, which depends on the intrinsic properties of the corresponding CMAB instance such as the reward distributions of super arms and outcome distributions of base arms. Additionally, we devise an attack algorithm for attackable CMAB instances. Contrary to prior understanding of multi-armed bandits, our work reveals a surprising fact that the attackability of a specific CMAB instance also depends on whether the bandit instance is known or unknown to the adversary. This finding indicates that adversarial attacks on CMAB are difficult in practice and a general attack strategy for any CMAB instance does not exist since the environment is mostly unknown to the adversary. We validate our theoretical findings via extensive experiments on real-world CMAB applications including probabilistic maximum covering problem, online minimum spanni
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30697;&#38453;&#24418;&#24335;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20854;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.05288</link><description>&lt;p&gt;
&#24102;&#26377;&#24322;&#24120;&#20540;&#30340;&#19977;&#20803;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Clustering Three-Way Data with Outliers. (arXiv:2310.05288v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05288
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30697;&#38453;&#24418;&#24335;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20854;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#21464;&#37327;&#20998;&#24067;&#26159;&#27169;&#22411;&#32858;&#31867;&#39046;&#22495;&#30340;&#26368;&#26032;&#28155;&#21152;&#65292;&#20174;&#32780;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#65288;&#22914;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#65289;&#30340;&#30697;&#38453;&#24418;&#24335;&#25968;&#25454;&#12290;&#30001;&#20110;&#20854;&#26368;&#36817;&#30340;&#20986;&#29616;&#65292;&#20851;&#20110;&#30697;&#38453;&#21464;&#37327;&#25968;&#25454;&#30340;&#25991;&#29486;&#26377;&#38480;&#65292;&#23545;&#20110;&#22788;&#29702;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#24322;&#24120;&#20540;&#30340;&#25991;&#29486;&#26356;&#23569;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30697;&#38453;&#21464;&#37327;&#27491;&#24577;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#23376;&#38598;&#23545;&#25968;&#20284;&#28982;&#30340;&#20998;&#24067;&#65292;&#23558;OCLUST&#31639;&#27861;&#25193;&#23637;&#21040;&#30697;&#38453;&#21464;&#37327;&#27491;&#24577;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#36845;&#20195;&#26041;&#27861;&#26816;&#27979;&#21644;&#21098;&#35009;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix-variate distributions are a recent addition to the model-based clustering field, thereby making it possible to analyze data in matrix form with complex structure such as images and time series. Due to its recent appearance, there is limited literature on matrix-variate data, with even less on dealing with outliers in these models. An approach for clustering matrix-variate normal data with outliers is discussed. The approach, which uses the distribution of subset log-likelihoods, extends the OCLUST algorithm to matrix-variate normal data and uses an iterative approach to detect and trim outliers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31616;&#21270;GNN&#24615;&#33021;&#30340;&#20302;&#31209;&#20869;&#26680;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#20256;&#32479;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#22312;&#35889;&#22495;&#20013;&#21462;&#20195;&#36807;&#20110;&#22797;&#26434;&#30340;GNN&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22270;&#31867;&#22411;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05250</link><description>&lt;p&gt;
&#29992;&#20302;&#31209;&#20869;&#26680;&#27169;&#22411;&#31616;&#21270;GNN&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Simplifying GNN Performance with Low Rank Kernel Models. (arXiv:2310.05250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31616;&#21270;GNN&#24615;&#33021;&#30340;&#20302;&#31209;&#20869;&#26680;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#20256;&#32479;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#22312;&#35889;&#22495;&#20013;&#21462;&#20195;&#36807;&#20110;&#22797;&#26434;&#30340;GNN&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22270;&#31867;&#22411;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#36817;&#30340;&#35889;GNN&#26041;&#27861;&#23545;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65288;SSNC&#65289;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#35768;&#22810;&#24403;&#21069;&#30340;GNN&#26550;&#26500;&#21487;&#33021;&#36807;&#20110;&#31934;&#32454;&#35774;&#35745;&#12290;&#30456;&#21453;&#65292;&#31616;&#21333;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#35889;&#22495;&#20013;&#24212;&#29992;&#65292;&#21487;&#20197;&#21462;&#20195;&#35768;&#22810;&#21463;&#28145;&#24230;&#23398;&#20064;&#21551;&#21457;&#30340;GNN&#35774;&#35745;&#12290;&#36825;&#20123;&#20256;&#32479;&#25216;&#26415;&#20284;&#20046;&#38750;&#24120;&#36866;&#21512;&#21508;&#31181;&#22270;&#31867;&#22411;&#65292;&#22312;&#35768;&#22810;&#24120;&#35265;&#30340;SSNC&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26368;&#36817;&#22312;GNN&#26041;&#27861;&#26041;&#38754;&#30340;&#24615;&#33021;&#25913;&#36827;&#21487;&#33021;&#37096;&#20998;&#24402;&#22240;&#20110;&#35780;&#20272;&#24815;&#20363;&#30340;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#19982;GNN&#35889;&#36807;&#28388;&#25216;&#26415;&#30456;&#20851;&#30340;&#21508;&#31181;&#36229;&#21442;&#25968;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit recent spectral GNN approaches to semi-supervised node classification (SSNC). We posit that many of the current GNN architectures may be over-engineered. Instead, simpler, traditional methods from nonparametric estimation, applied in the spectral domain, could replace many deep-learning inspired GNN designs. These conventional techniques appear to be well suited for a variety of graph types reaching state-of-the-art performance on many of the common SSNC benchmarks. Additionally, we show that recent performance improvements in GNN approaches may be partially attributed to shifts in evaluation conventions. Lastly, an ablative study is conducted on the various hyperparameters associated with GNN spectral filtering techniques. Code available at: https://github.com/lucianoAvinas/lowrank-gnn-kernels
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Transformers&#20869;&#37096;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#19968;&#23618;Transformer&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#20855;&#26377;&#24179;&#34913;&#29305;&#24449;&#30340;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#20445;&#35777;&#65292;&#19988;&#39044;&#27979;&#35823;&#24046;&#25509;&#36817;&#38646;&#12290;</title><link>http://arxiv.org/abs/2310.05249</link><description>&lt;p&gt;
Transformers&#20869;&#37096;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-Context Convergence of Transformers. (arXiv:2310.05249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Transformers&#20869;&#37096;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#19968;&#23618;Transformer&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#20855;&#26377;&#24179;&#34913;&#29305;&#24449;&#30340;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#20445;&#35777;&#65292;&#19988;&#39044;&#27979;&#35823;&#24046;&#25509;&#36817;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;Transformers&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#38761;&#21629;&#24615;&#36827;&#23637;&#65292;&#20854;&#20013;&#19968;&#20010;&#26174;&#33879;&#30340;&#21457;&#29616;&#26159;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#23450;&#20219;&#21153;&#30340;&#25552;&#31034;&#32780;&#26080;&#38656;&#21442;&#25968;&#24494;&#35843;&#65292;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20174;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#12290;&#36825;&#20063;&#21551;&#21457;&#20102;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#30446;&#26631;&#26159;&#29702;&#35299;Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#26426;&#21046;&#65292;&#28982;&#32780;&#36825;&#20123;&#30740;&#31350;&#20165;&#20851;&#27880;&#32447;&#24615;Transformer&#12290;&#26412;&#25991;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#19968;&#23618;Transformer&#30340;softmax attention&#26426;&#21046;&#65292;&#39318;&#27425;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#32447;&#24615;&#20989;&#25968;&#31867;&#26041;&#38754;&#30740;&#31350;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#25968;&#25454;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#26631;&#35760;&#20174;&#19968;&#32452;&#24179;&#34913;&#25110;&#19981;&#24179;&#34913;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#38543;&#26426;&#37319;&#26679;&#12290;&#23545;&#20110;&#20855;&#26377;&#24179;&#34913;&#29305;&#24449;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#20004;&#20010;&#38454;&#27573;&#19978;&#36827;&#34892;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#20445;&#35777;&#65292;&#19988;&#39044;&#27979;&#35823;&#24046;&#25509;&#36817;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have recently revolutionized many domains in modern machine learning and one salient discovery is their remarkable in-context learning capability, where models can solve an unseen task by utilizing task-specific prompts without further parameters fine-tuning. This also inspired recent theoretical studies aiming to understand the in-context learning mechanism of transformers, which however focused only on linear transformers. In this work, we take the first step toward studying the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. We consider a structured data model, where each token is randomly sampled from a set of feature vectors in either balanced or imbalanced fashion. For data with balanced features, we establish the finite-time convergence guarantee with near-zero prediction error by navigating our analysis over two phases of the training dynamics of the attention map
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Orlicz&#21518;&#24724;&#26041;&#27861;&#65292;&#29992;&#20110;&#19968;&#33268;&#22320;&#30028;&#23450;&#38543;&#26426;&#21464;&#37327;&#30340;&#32479;&#35745;&#37327;&#19978;&#19979;&#30028;&#65292;&#36890;&#36807;&#28789;&#27963;&#35780;&#20272;&#38543;&#26426;&#21464;&#37327;&#30340;&#23614;&#34892;&#20026;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#27492;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#33268;&#24615;&#35780;&#20272;&#65292;&#24182;&#24471;&#21040;&#20102;&#23558;&#20854;&#19982;&#21457;&#25955;&#39118;&#38505;&#24230;&#37327;&#31561;&#25928;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2310.05168</link><description>&lt;p&gt;
Orlicz&#21518;&#24724;&#32479;&#19968;&#36793;&#30028;&#38543;&#26426;&#21464;&#37327;&#32479;&#35745;&#30340;&#26041;&#27861;&#21450;&#20854;&#22312;&#29615;&#22659;&#25351;&#26631;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Orlicz regrets to consistently bound statistics of random variables with an application to environmental indicators. (arXiv:2310.05168v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Orlicz&#21518;&#24724;&#26041;&#27861;&#65292;&#29992;&#20110;&#19968;&#33268;&#22320;&#30028;&#23450;&#38543;&#26426;&#21464;&#37327;&#30340;&#32479;&#35745;&#37327;&#19978;&#19979;&#30028;&#65292;&#36890;&#36807;&#28789;&#27963;&#35780;&#20272;&#38543;&#26426;&#21464;&#37327;&#30340;&#23614;&#34892;&#20026;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#27492;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#33268;&#24615;&#35780;&#20272;&#65292;&#24182;&#24471;&#21040;&#20102;&#23558;&#20854;&#19982;&#21457;&#25955;&#39118;&#38505;&#24230;&#37327;&#31561;&#25928;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;&#21464;&#37327;&#30340;&#32479;&#35745;&#35780;&#20272;&#26159;&#35774;&#35745;&#26356;&#22909;&#30340;&#29615;&#22659;&#31649;&#29702;&#21644;&#24674;&#22797;&#26041;&#26696;&#30340;&#20027;&#35201;&#35758;&#39064;&#12290;&#23545;&#20110;&#27700;&#36136;&#25351;&#26631;&#12289;&#27946;&#28061;&#21644;&#24178;&#26097;&#27700;&#20301;&#31561;&#36825;&#20123;&#21464;&#37327;&#30340;&#19978;&#19979;&#30028;&#20272;&#35745;&#37117;&#24456;&#37325;&#35201;&#65292;&#24212;&#24403;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#23398;&#26694;&#26550;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Orlicz&#21518;&#24724;&#26041;&#27861;&#65292;&#29992;&#20110;&#19968;&#33268;&#22320;&#30028;&#23450;&#38543;&#26426;&#21464;&#37327;&#30340;&#32479;&#35745;&#37327;&#19978;&#19979;&#30028;&#12290;&#36825;&#37324;&#30340;&#19968;&#33268;&#24615;&#25351;&#30340;&#26159;&#19978;&#30028;&#21644;&#19979;&#30028;&#20351;&#29992;&#30456;&#21516;&#30340;&#31995;&#25968;&#21644;&#21442;&#25968;&#20540;&#36827;&#34892;&#35780;&#20272;&#65292;&#19982;&#36804;&#20170;&#20026;&#27490;&#25552;&#20986;&#30340;&#26576;&#20123;&#39118;&#38505;&#24230;&#37327;&#19981;&#21516;&#12290;Orlicz&#21518;&#24724;&#33021;&#22815;&#26681;&#25454;&#38543;&#26426;&#21464;&#37327;&#30340;&#23614;&#34892;&#20026;&#28789;&#27963;&#22320;&#35780;&#20272;&#20854;&#32479;&#35745;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#26126;&#30830;&#22320;&#23558;Orlicz&#21518;&#24724;&#19982;&#21457;&#25955;&#39118;&#38505;&#24230;&#37327;&#32852;&#31995;&#36215;&#26469;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#20204;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#23558;Orlicz&#21518;&#24724;&#21644;&#21457;&#25955;&#39118;&#38505;&#24230;&#37327;&#31561;&#25928;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating environmental variables that vary stochastically is the principal topic for designing better environmental management and restoration schemes. Both the upper and lower estimates of these variables, such as water quality indices and flood and drought water levels, are important and should be consistently evaluated within a unified mathematical framework. We propose a novel pair of Orlicz regrets to consistently bound the statistics of random variables both from below and above. Here, consistency indicates that the upper and lower bounds are evaluated with common coefficients and parameter values being different from some of the risk measures proposed thus far. Orlicz regrets can flexibly evaluate the statistics of random variables based on their tail behavior. The explicit linkage between Orlicz regrets and divergence risk measures was exploited to better comprehend them. We obtain sufficient conditions to pose the Orlicz regrets as well as divergence risk measures, and furth
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20462;&#27491;&#30340;&#26399;&#26395;&#25913;&#21892;&#37319;&#38598;&#20989;&#25968;&#65292;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#35299;&#20915;&#20102;&#23545;&#20110;&#26377;&#22122;&#22768;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#24573;&#30053;&#20505;&#36873;&#35299;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05166</link><description>&lt;p&gt;
&#19968;&#20010;&#22312;&#26377;&#22122;&#22768;&#35266;&#27979;&#19979;&#20462;&#27491;&#30340;&#26399;&#26395;&#25913;&#21892;&#37319;&#38598;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Corrected Expected Improvement Acquisition Function Under Noisy Observations. (arXiv:2310.05166v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05166
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20462;&#27491;&#30340;&#26399;&#26395;&#25913;&#21892;&#37319;&#38598;&#20989;&#25968;&#65292;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#35299;&#20915;&#20102;&#23545;&#20110;&#26377;&#22122;&#22768;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#24573;&#30053;&#20505;&#36873;&#35299;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#26368;&#22823;&#21270;&#26399;&#26395;&#25913;&#21892;(EI)&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#26368;&#24120;&#29992;&#30340;&#31574;&#30053;&#20043;&#19968;&#65292;&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#22788;&#29702;&#22122;&#22768;&#35266;&#27979;&#30340;&#33021;&#21147;&#32780;&#24191;&#27867;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#65292;&#25913;&#21892;&#20989;&#25968;&#36890;&#24120;&#20351;&#29992;&#26368;&#20339;&#21518;&#39564;&#22343;&#20540;&#20316;&#20026;&#26368;&#20339;&#20505;&#36873;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#35299;&#26512;&#30340;EI&#31867;&#22411;&#26041;&#27861;&#20013;&#65292;&#24120;&#24120;&#24573;&#30053;&#19982;&#20505;&#36873;&#35299;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#22312;&#26080;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#24418;&#24335;&#30340;&#37319;&#38598;&#20989;&#25968;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#26377;&#22122;&#22768;&#35266;&#27979;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;EI&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#26031;&#36807;&#31243;(GP)&#27169;&#22411;&#25552;&#20379;&#30340;&#21327;&#26041;&#24046;&#20449;&#24687;&#32435;&#20837;&#20854;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#20013;&#12290;&#36825;&#20010;&#37319;&#38598;&#20989;&#25968;&#19982;&#32463;&#20856;&#30340;&#26080;&#22122;&#22768;&#32467;&#26524;&#30456;&#21563;&#21512;&#65292;&#25105;&#20204;&#35748;&#20026;&#23427;&#24212;&#35813;&#21462;&#20195;&#36125;&#21494;&#26031;&#20248;&#21270;&#36719;&#20214;&#21253;&#12289;&#25945;&#31243;&#21644;&#25945;&#26448;&#20013;&#30340;&#37027;&#20010;&#20844;&#24335;&#12290;&#36825;&#20010;&#25913;&#36827;&#30340;&#37319;&#38598;&#20989;&#25968;&#20026;&#26377;&#22122;&#22768;&#21644;&#26080;&#22122;&#22768;&#30340;&#35299;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential maximization of expected improvement (EI) is one of the most widely used policies in Bayesian optimization because of its simplicity and ability to handle noisy observations. In particular, the improvement function often uses the best posterior mean as the best incumbent in noisy settings. However, the uncertainty associated with the incumbent solution is often neglected in many analytic EI-type methods: a closed-form acquisition function is derived in the noise-free setting, but then applied to the setting with noisy observations. To address this limitation, we propose a modification of EI that corrects its closed-form expression by incorporating the covariance information provided by the Gaussian Process (GP) model. This acquisition function specializes to the classical noise-free result, and we argue should replace that formula in Bayesian optimization software packages, tutorials, and textbooks. This enhanced acquisition provides good generality for noisy and noiseless s
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21387;&#32553;&#22312;&#32447;Sinkhorn&#31639;&#27861;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#23427;&#25552;&#20986;&#20102;&#22788;&#29702;&#36830;&#32493;&#25968;&#25454;&#27969;&#30340;&#38543;&#26426;&#29256;&#26412;&#65292;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2310.05019</link><description>&lt;p&gt;
&#21387;&#32553;&#22312;&#32447;Sinkhorn&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Compressed online Sinkhorn. (arXiv:2310.05019v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21387;&#32553;&#22312;&#32447;Sinkhorn&#31639;&#27861;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#23427;&#25552;&#20986;&#20102;&#22788;&#29702;&#36830;&#32493;&#25968;&#25454;&#27969;&#30340;&#38543;&#26426;&#29256;&#26412;&#65292;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#31185;&#23398;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#36317;&#31163;&#30340;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#29109;&#27491;&#21017;&#21270;OT&#36317;&#31163;&#65292;&#25104;&#20026;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23427;&#20204;&#30340;&#20351;&#29992;&#20027;&#35201;&#26159;&#30001;&#20110;Sinkhorn&#31639;&#27861;&#31561;&#39640;&#25928;&#31639;&#27861;&#30340;&#21487;&#29992;&#24615;&#12290;Sinkhorn&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#23427;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#20174;&#27010;&#29575;&#20998;&#24067;&#20013;&#25277;&#21462;&#22823;&#37327;&#25968;&#25454;&#65292;&#28982;&#21518;&#23558;Sinkhorn&#31639;&#27861;&#24212;&#29992;&#20110;&#31163;&#25955;&#27010;&#29575;&#27979;&#24230;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#26377;&#19968;&#20123;&#30740;&#31350;&#24320;&#21457;&#20102;&#30452;&#25509;&#22788;&#29702;&#36830;&#32493;&#25968;&#25454;&#27969;&#30340;Sinkhorn&#30340;&#38543;&#26426;&#29256;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;[Mensch&#21644;Peyr\'e, 2020]&#26368;&#36817;&#24341;&#20837;&#30340;&#22312;&#32447;Sinkhorn&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#25105;&#20204;&#25913;&#36827;&#20102;&#22312;&#32447;Sinkhorn&#31639;&#27861;&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#26032;&#24471;&#21040;&#30340;&#25910;&#25947;&#36895;&#24230;&#22312;&#26576;&#20123;&#21442;&#25968;&#19979;&#27604;&#20808;&#21069;&#30340;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of optimal transport (OT) distances, and in particular entropic-regularised OT distances, is an increasingly popular evaluation metric in many areas of machine learning and data science. Their use has largely been driven by the availability of efficient algorithms such as the Sinkhorn algorithm. One of the drawbacks of the Sinkhorn algorithm for large-scale data processing is that it is a two-phase method, where one first draws a large stream of data from the probability distributions, before applying the Sinkhorn algorithm to the discrete probability measures. More recently, there have been several works developing stochastic versions of Sinkhorn that directly handle continuous streams of data. In this work, we revisit the recently introduced online Sinkhorn algorithm of [Mensch and Peyr\'e, 2020]. Our contributions are twofold: We improve the convergence analysis for the online Sinkhorn algorithm, the new rate that we obtain is faster than the previous rate under certain para
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#36866;&#24212;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#30340;&#20449;&#21495;&#24674;&#22797;&#65292;&#36890;&#36807;&#20248;&#21270;&#37319;&#26679;&#20998;&#24067;&#21644;&#26032;&#30340;&#29702;&#35770;&#24674;&#22797;&#20445;&#35777;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#27979;&#37327;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.04984</link><description>&lt;p&gt;
&#27169;&#22411;&#36866;&#24212;&#30340;&#20613;&#31435;&#21494;&#37319;&#26679;&#29992;&#20110;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Model-adapted Fourier sampling for generative compressed sensing. (arXiv:2310.04984v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04984
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#36866;&#24212;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#30340;&#20449;&#21495;&#24674;&#22797;&#65292;&#36890;&#36807;&#20248;&#21270;&#37319;&#26679;&#20998;&#24067;&#21644;&#26032;&#30340;&#29702;&#35770;&#24674;&#22797;&#20445;&#35777;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#27979;&#37327;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#27979;&#37327;&#30697;&#38453;&#26159;&#20174;&#19968;&#20010;&#37193;&#30697;&#38453;&#20013;&#38543;&#26426;&#23376;&#37319;&#26679;&#24471;&#21040;&#26102;&#30340;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65288;&#31163;&#25955;&#20613;&#31435;&#21494;&#21464;&#25442;&#26159;&#37325;&#35201;&#30340;&#29305;&#27530;&#24773;&#20917;&#65289;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#27599;&#20010;&#20613;&#31435;&#21494;&#21521;&#37327;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#21462;&#20540;&#33539;&#22260;&#23545;&#40784;&#26102;&#65292;&#21482;&#38656;&#35201;$O(kdn\|\boldsymbol{\alpha}\|_{\infty}^{2})$&#20010;&#22343;&#21248;&#38543;&#26426;&#20613;&#31435;&#21494;&#27979;&#37327;&#23601;&#36275;&#20197;&#24674;&#22797;&#36755;&#20986;&#20449;&#21495;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#36866;&#24212;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#21482;&#38656;&#35201;$O(kd\|\boldsymbol{\alpha}\|_{2}^{2})$&#20010;&#27979;&#37327;&#12290;&#36825;&#26159;&#36890;&#36807;&#20197;&#19979;&#27493;&#39588;&#23454;&#29616;&#30340;&#65306;&#65288;1&#65289;&#25105;&#20204;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#38750;&#22343;&#21248;&#38543;&#26426;&#37319;&#26679;&#20998;&#24067;  &#30340;&#26032;&#30340;&#29702;&#35770;&#24674;&#22797;&#20445;&#35777;&#65292;&#28982;&#21518;&#65288;2&#65289;&#20248;&#21270;&#37319;&#26679;&#20998;&#24067;&#20197;&#26368;&#23567;&#21270;&#36825;&#20123;&#20445;&#35777;&#25152;&#38656;&#30340;&#27979;&#37327;&#25968;&#37327;&#12290;&#36825;&#31181;&#25216;&#26415;&#21457;&#23637;&#25552;&#20379;&#20102;&#36866;&#29992;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study generative compressed sensing when the measurement matrix is randomly subsampled from a unitary matrix (with the DFT as an important special case). It was recently shown that $\textit{O}(kdn\| \boldsymbol{\alpha}\|_{\infty}^{2})$ uniformly random Fourier measurements are sufficient to recover signals in the range of a neural network $G:\mathbb{R}^k \to \mathbb{R}^n$ of depth $d$, where each component of the so-called local coherence vector $\boldsymbol{\alpha}$ quantifies the alignment of a corresponding Fourier vector with the range of $G$. We construct a model-adapted sampling strategy with an improved sample complexity of $\textit{O}(kd\| \boldsymbol{\alpha}\|_{2}^{2})$ measurements. This is enabled by: (1) new theoretical recovery guarantees that we develop for nonuniformly random sampling distributions and then (2) optimizing the sampling distribution to minimize the number of measurements needed for these guarantees. This development offers a sample complexity applicable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#40065;&#26834;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26032;&#22411;M&#20272;&#35745;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#19968;&#31867;&#38750;&#20984;&#20989;&#25968;&#65292;&#29992;&#20110;&#20943;&#24369;&#21463;&#24322;&#24120;&#20540;&#27745;&#26579;&#30340;&#35266;&#27979;&#20540;&#65292;&#24471;&#21040;&#20102;&#30456;&#24212;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#31639;&#27861;&#35774;&#35745;&#21644;&#25910;&#25947;&#24615;&#20998;&#26512;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24674;&#22797;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.04953</link><description>&lt;p&gt;
&#40065;&#26834;&#30697;&#38453;&#34917;&#20840;&#30340;&#26032;&#22411;M&#20272;&#35745;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Robust matrix completion via Novel M-estimator Functions. (arXiv:2310.04953v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#40065;&#26834;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26032;&#22411;M&#20272;&#35745;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#19968;&#31867;&#38750;&#20984;&#20989;&#25968;&#65292;&#29992;&#20110;&#20943;&#24369;&#21463;&#24322;&#24120;&#20540;&#27745;&#26579;&#30340;&#35266;&#27979;&#20540;&#65292;&#24471;&#21040;&#20102;&#30456;&#24212;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#31639;&#27861;&#35774;&#35745;&#21644;&#25910;&#25947;&#24615;&#20998;&#26512;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24674;&#22797;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Welsch&#21644;Cauchy&#31561;M&#20272;&#35745;&#22120;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23545;&#25239;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#23427;&#20204;&#20063;&#20250;&#20943;&#24369;&#26410;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29983;&#25104;&#19968;&#31867;&#38750;&#20984;&#20989;&#25968;&#30340;&#26694;&#26550;&#65292;&#36825;&#20123;&#20989;&#25968;&#21482;&#20250;&#20943;&#24369;&#21463;&#24322;&#24120;&#20540;&#27745;&#26579;&#30340;&#35266;&#27979;&#20540;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;Welsch&#65292;Cauchy&#21644;$\ell_p$-norm&#20989;&#25968;&#65292;&#29983;&#25104;&#30456;&#24212;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#38024;&#23545;&#40065;&#26834;&#30697;&#38453;&#34917;&#20840;&#24212;&#29992;&#20102;&#22522;&#20110;&#36825;&#20123;&#20989;&#25968;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#23545;&#20854;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24674;&#22797;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
M-estmators including the Welsch and Cauchy have been widely adopted for robustness against outliers, but they also down-weigh the uncontaminated data. To address this issue, we devise a framework to generate a class of nonconvex functions which only down-weigh outlier-corrupted observations. Our framework is then applied to the Welsch, Cauchy and $\ell_p$-norm functions to produce the corresponding robust loss functions. Targeting on the application of robust matrix completion, efficient algorithms based on these functions are developed and their convergence is analyzed. Finally, extensive numerical results demonstrate that the proposed methods are superior to the competitors in terms of recovery accuracy and runtime.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;PAC-Bayesian&#29702;&#35770;&#20026;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#32479;&#35745;&#20445;&#35777;&#65292;&#21253;&#25324;&#23545;&#21518;&#39564;&#20998;&#24067;&#12289;&#37325;&#26500;&#25439;&#22833;&#21644;&#36755;&#20837;&#19982;&#29983;&#25104;&#20998;&#24067;&#20043;&#38388;&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.04935</link><description>&lt;p&gt;
&#20351;&#29992;PAC-Bayesian&#29702;&#35770;&#32473;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Statistical Guarantees for Variational Autoencoders using PAC-Bayesian Theory. (arXiv:2310.04935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04935
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;PAC-Bayesian&#29702;&#35770;&#20026;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#32479;&#35745;&#20445;&#35777;&#65292;&#21253;&#25324;&#23545;&#21518;&#39564;&#20998;&#24067;&#12289;&#37325;&#26500;&#25439;&#22833;&#21644;&#36755;&#20837;&#19982;&#29983;&#25104;&#20998;&#24067;&#20043;&#38388;&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#23427;&#20204;&#30340;&#38382;&#19990;&#20197;&#26469;&#65292;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20851;&#20110;&#23427;&#20204;&#30340;&#29702;&#35770;&#24615;&#36136;&#20173;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#12290;&#26412;&#25991;&#21033;&#29992;PAC-Bayesian&#29702;&#35770;&#20026;VAEs&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22522;&#20110;&#29420;&#31435;&#26679;&#26412;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#39318;&#20010;PAC-Bayesian&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#20026;VAE&#30340;&#37325;&#26500;&#25439;&#22833;&#25552;&#20379;&#20102;&#27867;&#21270;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36755;&#20837;&#20998;&#24067;&#19982;VAE&#29983;&#25104;&#27169;&#22411;&#23450;&#20041;&#30340;&#20998;&#24067;&#20043;&#38388;&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36755;&#20837;&#20998;&#24067;&#19982;VAE&#29983;&#25104;&#27169;&#22411;&#23450;&#20041;&#30340;&#20998;&#24067;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their inception, Variational Autoencoders (VAEs) have become central in machine learning. Despite their widespread use, numerous questions regarding their theoretical properties remain open. Using PAC-Bayesian theory, this work develops statistical guarantees for VAEs. First, we derive the first PAC-Bayesian bound for posterior distributions conditioned on individual samples from the data-generating distribution. Then, we utilize this result to develop generalization guarantees for the VAE's reconstruction loss, as well as upper bounds on the distance between the input and the regenerated distributions. More importantly, we provide upper bounds on the Wasserstein distance between the input distribution and the distribution defined by the VAE's generative model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30340;&#21019;&#26032;&#26159;&#36890;&#36807;&#31454;&#20105;&#21644;&#21078;&#26512;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#35270;&#35273;&#32593;&#32476;&#26356;&#23481;&#26131;&#35299;&#37322;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#28145;&#24230;&#32593;&#32476;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04929</link><description>&lt;p&gt;
DISCOVER: &#36890;&#36807;&#31454;&#20105;&#21644;&#21078;&#26512;&#20351;&#35270;&#35273;&#32593;&#32476;&#21487;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DISCOVER: Making Vision Networks Interpretable via Competition and Dissection. (arXiv:2310.04929v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04929
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30340;&#21019;&#26032;&#26159;&#36890;&#36807;&#31454;&#20105;&#21644;&#21078;&#26512;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#35270;&#35273;&#32593;&#32476;&#26356;&#23481;&#26131;&#35299;&#37322;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#28145;&#24230;&#32593;&#32476;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#32593;&#32476;&#38750;&#24120;&#22797;&#26434;&#65292;&#20854;&#25512;&#29702;&#32467;&#26524;&#24456;&#38590;&#35299;&#37322;&#12290;&#36825;&#23545;&#20110;&#36879;&#26126;&#22320;&#23558;&#20854;&#37096;&#32626;&#22312;&#23433;&#20840;&#20851;&#38190;&#25110;&#20559;&#35265;&#24863;&#30693;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#23545;&#21518;&#26399;&#21487;&#35299;&#37322;&#24615;&#29305;&#21035;&#26159;&#32593;&#32476;&#21078;&#26512;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#24471;&#22312;&#35757;&#32451;&#20110;&#35270;&#35273;&#20219;&#21153;&#30340;&#32593;&#32476;&#20013;&#65292;&#21457;&#29616;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#20010;&#20307;&#21151;&#33021;&#26356;&#23481;&#26131;&#65307;&#36890;&#36807;&#29983;&#25104;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#21457;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20197;&#19979;&#20004;&#20010;&#26041;&#38754;&#65306;(i)&#26368;&#26032;&#30340;&#22810;&#27169;&#24577;&#35270;&#35273;-&#25991;&#26412;&#27169;&#22411;&#30340;&#36827;&#23637;&#21644;(ii)&#22522;&#20110;&#32447;&#24615;&#21333;&#20803;&#20043;&#38388;&#30340;&#26032;&#27010;&#24565;-&#38543;&#26426;&#23616;&#37096;&#31454;&#20105;&#30340;&#32593;&#32476;&#23618;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#36755;&#20837;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#23618;&#31070;&#32463;&#20803;&#34987;&#28608;&#27963;&#65292;&#23548;&#33268;&#26497;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65288;&#20165;&#32422;&#20026;4%&#65289;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25512;&#26029;&#20986;&#20102;&#65288;&#31232;&#30095;&#30340;&#65289;&#31070;&#32463;&#20803;&#28608;&#27963;&#27169;&#24335;&#65292;&#20351;&#24471;&#31070;&#32463;&#20803;&#33021;&#22815;&#28608;&#27963;/&#19987;&#38376;&#21270;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Modern deep networks are highly complex and their inferential outcome very hard to interpret. This is a serious obstacle to their transparent deployment in safety-critical or bias-aware applications. This work contributes to post-hoc interpretability, and specifically Network Dissection. Our goal is to present a framework that makes it easier to discover the individual functionality of each neuron in a network trained on a vision task; discovery is performed in terms of textual description generation. To achieve this objective, we leverage: (i) recent advances in multimodal vision-text models and (ii) network layers founded upon the novel concept of stochastic local competition between linear units. In this setting, only a small subset of layer neurons are activated for a given input, leading to extremely high activation sparsity (as low as only $\approx 4\%$). Crucially, our proposed method infers (sparse) neuron activation patterns that enables the neurons to activate/specialize to i
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#39044;&#27979;&#20989;&#25968;&#30340;Knockoff&#32479;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#23545;&#35823;&#21457;&#29616;&#29575;&#36827;&#34892;&#25511;&#21046;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#21040;&#39044;&#27979;&#21464;&#37327;&#21644;&#32467;&#26524;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.04919</link><description>&lt;p&gt;
&#26465;&#20214;&#39044;&#27979;&#20989;&#25968;&#65306;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#27169;&#22411;&#30340;&#25511;&#21046;&#35823;&#21457;&#29616;&#29575;&#30340;&#26032;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
The Conditional Prediction Function: A Novel Technique to Control False Discovery Rate for Complex Models. (arXiv:2310.04919v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04919
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#39044;&#27979;&#20989;&#25968;&#30340;Knockoff&#32479;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#23545;&#35823;&#21457;&#29616;&#29575;&#36827;&#34892;&#25511;&#21046;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#21040;&#39044;&#27979;&#21464;&#37327;&#21644;&#32467;&#26524;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#31185;&#23398;&#30740;&#31350;&#20013;&#65292;&#30446;&#26631;&#36890;&#24120;&#26159;&#22312;&#22823;&#37327;&#21487;&#33021;&#30340;&#39044;&#27979;&#22240;&#32032;&#20013;&#30830;&#23450;&#19982;&#32467;&#26524;&#30456;&#20851;&#30340;&#21464;&#37327;&#12290;&#36825;&#20010;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#20197;&#25511;&#21046;&#35823;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#30340;&#26041;&#24335;&#26469;&#36873;&#25321;&#21464;&#37327;&#26469;&#23454;&#29616;&#65292;&#21363;&#22312;&#25152;&#36873;&#25321;&#30340;&#21464;&#37327;&#20013;&#65292;&#26080;&#20851;&#30340;&#39044;&#27979;&#22240;&#32032;&#30340;&#27604;&#20363;&#12290;Knockoff&#28388;&#27874;&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;FDR&#25511;&#21046;&#12290;&#29616;&#26377;&#30340;Knockoff&#32479;&#35745;&#26041;&#27861;&#32463;&#24120;&#20351;&#29992;&#32447;&#24615;&#27169;&#22411;&#26469;&#35780;&#20272;&#29305;&#24449;&#21644;&#21709;&#24212;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20294;&#26159;&#22312;&#32447;&#24615;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#24120;&#24120;&#34987;&#36829;&#21453;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#30495;&#27491;&#39044;&#27979;&#21464;&#37327;&#30340;&#26816;&#27979;&#33021;&#21147;&#36739;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#39044;&#27979;&#20989;&#25968;&#65288;CPF&#65289;&#30340;Knockoff&#32479;&#35745;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#37197;&#23545;&#20351;&#29992;&#12290;CPF&#32479;&#35745;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#39044;&#27979;&#21464;&#37327;&#21644;&#32467;&#26524;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21516;&#26102;&#36824;&#32771;&#34385;&#20102;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern scientific research, the objective is often to identify which variables are associated with an outcome among a large class of potential predictors. This goal can be achieved by selecting variables in a manner that controls the the false discovery rate (FDR), the proportion of irrelevant predictors among the selections. Knockoff filtering is a cutting-edge approach to variable selection that provides FDR control. Existing knockoff statistics frequently employ linear models to assess relationships between features and the response, but the linearity assumption is often violated in real world applications. This may result in poor power to detect truly prognostic variables. We introduce a knockoff statistic based on the conditional prediction function (CPF), which can pair with state-of-art machine learning predictive models, such as deep neural networks. The CPF statistics can capture the nonlinear relationships between predictors and outcomes while also accounting for correlati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#28436;&#21270;&#26041;&#31243;&#35299;&#20915;&#30340;&#31070;&#32463;Galerkin&#26041;&#26696;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#31232;&#30095;&#32593;&#32476;&#21442;&#25968;&#30340;&#26356;&#26032;&#26469;&#36991;&#20813;&#22312;&#26102;&#38388;&#19978;&#36807;&#25311;&#21512;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.04867</link><description>&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;&#31232;&#30095;&#31070;&#32463;Galerkin&#26041;&#26696;&#29992;&#20110;&#27714;&#35299;&#21547;&#26377;&#28145;&#24230;&#32593;&#32476;&#30340;&#28436;&#21270;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks. (arXiv:2310.04867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#28436;&#21270;&#26041;&#31243;&#35299;&#20915;&#30340;&#31070;&#32463;Galerkin&#26041;&#26696;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#31232;&#30095;&#32593;&#32476;&#21442;&#25968;&#30340;&#26356;&#26032;&#26469;&#36991;&#20813;&#22312;&#26102;&#38388;&#19978;&#36807;&#25311;&#21512;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#19978;&#39034;&#24207;&#22320;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#35299;&#20915;&#21547;&#26377;&#26102;&#38388;&#20381;&#36182;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#21487;&#20197;&#24110;&#21161;&#20445;&#25345;&#22240;&#26524;&#24615;&#21644;&#20854;&#20182;&#29289;&#29702;&#23646;&#24615;&#65307;&#28982;&#32780;&#65292;&#26102;&#38388;&#19978;&#30340;&#39034;&#24207;&#35757;&#32451;&#22312;&#25968;&#20540;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35757;&#32451;&#35823;&#24046;&#20250;&#38543;&#30528;&#26102;&#38388;&#24555;&#36895;&#31215;&#32047;&#21644;&#25918;&#22823;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31070;&#32463;Galerkin&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#26356;&#26032;&#38543;&#26426;&#31232;&#30095;&#30340;&#32593;&#32476;&#21442;&#25968;&#23376;&#38598;&#12290;&#38543;&#26426;&#21270;&#36991;&#20813;&#20102;&#22312;&#26102;&#38388;&#19978;&#36807;&#24230;&#25311;&#21512;&#65292;&#24182;&#24110;&#21161;&#38450;&#27490;&#35823;&#24046;&#22312;&#26102;&#38388;&#19978;&#30340;&#24555;&#36895;&#32047;&#31215;&#65292;&#36825;&#21463;&#21040;dropout&#30340;&#21551;&#21457;&#65292;dropout&#35299;&#20915;&#20102;&#30001;&#20110;&#31070;&#32463;&#20803;&#21327;&#21516;&#36866;&#24212;&#32780;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#26356;&#26032;&#30340;&#31232;&#30095;&#24615;&#38477;&#20302;&#20102;&#35757;&#32451;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#19981;&#20250;&#20007;&#22833;&#34920;&#36798;&#33021;&#21147;&#65292;&#22240;&#20026;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#35768;&#22810;&#32593;&#32476;&#21442;&#25968;&#26159;&#22810;&#20313;&#30340;&#12290;&#22312;&#24191;&#27867;&#30340;&#28436;&#21270;&#26041;&#31243;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#37319;&#29992;&#20102;&#38543;&#26426;&#31232;&#30095;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training neural networks sequentially in time to approximate solution fields of time-dependent partial differential equations can be beneficial for preserving causality and other physics properties; however, the sequential-in-time training is numerically challenging because training errors quickly accumulate and amplify over time. This work introduces Neural Galerkin schemes that update randomized sparse subsets of network parameters at each time step. The randomization avoids overfitting locally in time and so helps prevent the error from accumulating quickly over the sequential-in-time training, which is motivated by dropout that addresses a similar issue of overfitting due to neuron co-adaptation. The sparsity of the update reduces the computational costs of training without losing expressiveness because many of the network parameters are redundant locally at each time step. In numerical experiments with a wide range of evolution equations, the proposed scheme with randomized sparse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#35299;transformer&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#35821;&#20041;&#29702;&#35299;&#20013;&#30340;&#38544;&#21547;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.04861</link><description>&lt;p&gt;
&#36890;&#36807;&#21306;&#20998;&#20301;&#32622;&#21644;&#19978;&#19979;&#25991;&#26469;&#25581;&#31034;Transformers&#20013;&#30340;&#38544;&#34255;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
Uncovering hidden geometry in Transformers via disentangling position and context. (arXiv:2310.04861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#35299;transformer&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#35821;&#20041;&#29702;&#35299;&#20013;&#30340;&#38544;&#21547;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24191;&#27867;&#29992;&#20110;&#20174;&#36755;&#20837;&#20196;&#29260;&#20013;&#25552;&#21462;&#22797;&#26434;&#30340;&#35821;&#20041;&#24847;&#20041;&#65292;&#28982;&#32780;&#23427;&#20204;&#36890;&#24120;&#20316;&#20026;&#40657;&#30418;&#27169;&#22411;&#36816;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#20449;&#24687;&#20016;&#23500;&#30340;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#22909;&#30340;transformer&#30340;&#38544;&#34255;&#29366;&#24577;&#65288;&#25110;&#23884;&#20837;&#65289;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#32452;&#20214;&#12290;&#23545;&#20110;&#20219;&#20309;&#23618;&#65292;&#36755;&#20837;&#24207;&#21015;&#26679;&#26412;&#30340;&#23884;&#20837;&#21521;&#37327;&#30001;&#19968;&#20010;&#24352;&#37327;&#34920;&#31034; $\boldsymbol{h} \in \mathbb{R}^{C \times T \times d}$&#12290;&#32473;&#23450;&#22312;&#24207;&#21015;&#65288;&#25110;&#19978;&#19979;&#25991;&#65289; $c \le C$ &#30340;&#20301;&#32622; $t \le T$ &#22788;&#30340;&#23884;&#20837;&#21521;&#37327; $\boldsymbol{h}_{c,t} \in \mathbb{R}^d$&#65292;&#25552;&#21462;&#22343;&#20540;&#25928;&#26524;&#24471;&#21040;&#20998;&#35299;&#24418;&#24335; \[ \boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t + \mathbf{ctx}_c + \mathbf{resid}_{c,t} \] &#20854;&#20013; $\boldsymbol{\mu}$ &#26159;&#20840;&#23616;&#22343;&#20540;&#21521;&#37327;&#65292;$\mathbf{pos}_t$ &#21644; $\mathbf{ctx}_c$ &#20998;&#21035;&#26159;&#36328;&#19978;&#19979;&#25991;&#21644;&#36328;&#20301;&#32622;&#30340;&#22343;&#20540;&#21521;&#37327;&#65292;$\mathbf{resid}_{c,t}$ &#26159;&#27531;&#20313;&#21521;&#37327;&#12290;&#38024;&#23545;&#27969;&#34892;&#30340;transformer&#26550;&#26500;&#21644;&#22810;&#26679;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#32463;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Transformers are widely used to extract complex semantic meanings from input tokens, yet they usually operate as black-box models. In this paper, we present a simple yet informative decomposition of hidden states (or embeddings) of trained transformers into interpretable components. For any layer, embedding vectors of input sequence samples are represented by a tensor $\boldsymbol{h} \in \mathbb{R}^{C \times T \times d}$. Given embedding vector $\boldsymbol{h}_{c,t} \in \mathbb{R}^d$ at sequence position $t \le T$ in a sequence (or context) $c \le C$, extracting the mean effects yields the decomposition \[ \boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t + \mathbf{ctx}_c + \mathbf{resid}_{c,t} \] where $\boldsymbol{\mu}$ is the global mean vector, $\mathbf{pos}_t$ and $\mathbf{ctx}_c$ are the mean vectors across contexts and across positions respectively, and $\mathbf{resid}_{c,t}$ is the residual vector. For popular transformer architectures and diverse text datasets, empirica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#33945;&#29305;&#21345;&#32599;&#26426;&#21046;&#65292;&#31216;&#20026;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#65292;&#36890;&#36807;&#25913;&#36827;&#22270;&#30340;&#37319;&#26679;&#65292;&#25552;&#39640;&#20102;&#32479;&#35745;&#20272;&#35745;&#22120;&#30340;&#38598;&#20013;&#24230;&#12290;&#35813;&#26426;&#21046;&#22312;&#20272;&#35745;&#22270;&#20869;&#26680;&#12289;PageRank&#21521;&#37327;&#21644;&#22270;&#24418;&#27987;&#24230;&#31561;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04859</link><description>&lt;p&gt;
&#36890;&#29992;&#22270;&#38543;&#26426;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Universal Graph Random Features. (arXiv:2310.04859v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#33945;&#29305;&#21345;&#32599;&#26426;&#21046;&#65292;&#31216;&#20026;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#65292;&#36890;&#36807;&#25913;&#36827;&#22270;&#30340;&#37319;&#26679;&#65292;&#25552;&#39640;&#20102;&#32479;&#35745;&#20272;&#35745;&#22120;&#30340;&#38598;&#20013;&#24230;&#12290;&#35813;&#26426;&#21046;&#22312;&#20272;&#35745;&#22270;&#20869;&#26680;&#12289;PageRank&#21521;&#37327;&#21644;&#22270;&#24418;&#27987;&#24230;&#31561;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20934;&#33945;&#29305;&#21345;&#32599;&#26426;&#21046;&#65292;&#31216;&#20026;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#65292;&#20197;&#25913;&#36827;&#22522;&#20110;&#22270;&#30340;&#37319;&#26679;&#12290;&#36890;&#36807;&#22312;&#30456;&#20114;&#20316;&#29992;&#38598;&#21512;&#30340;&#36712;&#36857;&#20043;&#38388;&#24341;&#20837;&#30456;&#20851;&#24615;&#65292;&#20351;&#23427;&#20204;&#30340;&#36793;&#38469;&#36716;&#31227;&#27010;&#29575;&#20445;&#25345;&#19981;&#21464;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#25506;&#32034;&#22270;&#24418;&#65292;&#25552;&#39640;&#32479;&#35745;&#20272;&#35745;&#22120;&#30340;&#38598;&#20013;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#30340;&#26080;&#20559;&#24615;&#12290;&#35813;&#26426;&#21046;&#21487;&#20197;&#36731;&#26494;&#22320;&#23454;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20272;&#35745;&#22270;&#20869;&#26680;&#12289;PageRank&#21521;&#37327;&#21644;&#22270;&#24418;&#27987;&#24230;&#31561;&#21508;&#31181;&#24773;&#20917;&#19979;&#65292;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#35780;&#20272;&#21644;&#40065;&#26834;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#26159;&#31532;&#19968;&#20010;&#22312;&#22270;&#19978;&#30456;&#20851;&#27493;&#34892;&#32773;&#26041;&#21521;&#36827;&#34892;&#20005;&#26684;&#30740;&#31350;&#30340;&#20934;&#33945;&#29305;&#21345;&#32599;&#26041;&#26696;&#65292;&#20026;&#36825;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#26032;&#20852;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel quasi-Monte Carlo mechanism to improve graph-based sampling, coined repelling random walks. By inducing correlations between the trajectories of an interacting ensemble such that their marginal transition probabilities are unmodified, we are able to explore the graph more efficiently, improving the concentration of statistical estimators whilst leaving them unbiased. The mechanism has a trivial drop-in implementation. We showcase the effectiveness of repelling random walks in a range of settings including estimation of graph kernels, the PageRank vector and graphlet concentrations. We provide detailed experimental evaluation and robust theoretical guarantees. To our knowledge, repelling random walks constitute the first rigorously studied quasi-Monte Carlo scheme correlating the directions of walkers on a graph, inviting new research in this exciting nascent domain.
&lt;/p&gt;</description></item><item><title>&#25269;&#21046;&#38543;&#26426;&#28216;&#36208;&#26159;&#19968;&#31181;&#26032;&#30340;&#20934;&#33945;&#29305;&#21345;&#32599;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#22270;&#19978;&#30340;&#34892;&#36208;&#32773;&#20043;&#38388;&#24341;&#20837;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#25506;&#32034;&#22270;&#24182;&#25552;&#39640;&#32479;&#35745;&#20272;&#35745;&#30340;&#38598;&#20013;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#26080;&#20559;&#24615;&#12290;&#27492;&#26426;&#21046;&#22312;&#20272;&#35745;&#22270;&#26680;&#12289;PageRank&#21521;&#37327;&#21644;&#22270;&#20803;&#27987;&#24230;&#31561;&#22810;&#20010;&#39046;&#22495;&#37117;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#35780;&#20272;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.04854</link><description>&lt;p&gt;
&#25269;&#21046;&#38543;&#26426;&#28216;&#36208;
&lt;/p&gt;
&lt;p&gt;
Repelling Random Walks. (arXiv:2310.04854v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04854
&lt;/p&gt;
&lt;p&gt;
&#25269;&#21046;&#38543;&#26426;&#28216;&#36208;&#26159;&#19968;&#31181;&#26032;&#30340;&#20934;&#33945;&#29305;&#21345;&#32599;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#22270;&#19978;&#30340;&#34892;&#36208;&#32773;&#20043;&#38388;&#24341;&#20837;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#25506;&#32034;&#22270;&#24182;&#25552;&#39640;&#32479;&#35745;&#20272;&#35745;&#30340;&#38598;&#20013;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#26080;&#20559;&#24615;&#12290;&#27492;&#26426;&#21046;&#22312;&#20272;&#35745;&#22270;&#26680;&#12289;PageRank&#21521;&#37327;&#21644;&#22270;&#20803;&#27987;&#24230;&#31561;&#22810;&#20010;&#39046;&#22495;&#37117;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#35780;&#20272;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#33945;&#29305;&#21345;&#32599;&#26426;&#21046;&#26469;&#25913;&#36827;&#22522;&#20110;&#22270;&#30340;&#25277;&#26679;&#65292;&#31216;&#20026;&#25269;&#21046;&#38543;&#26426;&#28216;&#36208;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#30456;&#20114;&#20316;&#29992;&#30340;&#38598;&#21512;&#20013;&#30340;&#36712;&#36857;&#20043;&#38388;&#24341;&#20837;&#30456;&#20851;&#24615;&#65292;&#20351;&#23427;&#20204;&#30340;&#36793;&#38469;&#36716;&#31227;&#27010;&#29575;&#20445;&#25345;&#19981;&#21464;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#22270;&#65292;&#25552;&#39640;&#32479;&#35745;&#20272;&#35745;&#22120;&#30340;&#38598;&#20013;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#26080;&#20559;&#24615;&#12290;&#36825;&#20010;&#26426;&#21046;&#26377;&#19968;&#20010;&#31616;&#21333;&#30340;&#25554;&#20837;&#23454;&#29616;&#26041;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25269;&#21046;&#38543;&#26426;&#28216;&#36208;&#22312;&#19968;&#31995;&#21015;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22270;&#26680;&#30340;&#20272;&#35745;&#12289;PageRank&#21521;&#37327;&#21644;&#22270;&#20803;&#27987;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#35780;&#20272;&#21644;&#31283;&#20581;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25269;&#21046;&#38543;&#26426;&#28216;&#36208;&#26159;&#39318;&#20010;&#22312;&#22270;&#19978;&#30456;&#20851;&#34892;&#36208;&#26041;&#21521;&#30340;&#20934;&#33945;&#29305;&#21345;&#32599;&#26041;&#26696;&#36827;&#34892;&#20102;&#20005;&#35880;&#30740;&#31350;&#65292;&#20026;&#36825;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#26032;&#20852;&#39046;&#22495;&#24320;&#23637;&#26032;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22865;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel quasi-Monte Carlo mechanism to improve graph-based sampling, coined repelling random walks. By inducing correlations between the trajectories of an interacting ensemble such that their marginal transition probabilities are unmodified, we are able to explore the graph more efficiently, improving the concentration of statistical estimators whilst leaving them unbiased. The mechanism has a trivial drop-in implementation. We showcase the effectiveness of repelling random walks in a range of settings including estimation of graph kernels, the PageRank vector and graphlet concentrations. We provide detailed experimental evaluation and robust theoretical guarantees. To our knowledge, repelling random walks constitute the first rigorously studied quasi-Monte Carlo scheme correlating the directions of walkers on a graph, inviting new research in this exciting nascent domain.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23376;&#31354;&#38388;&#35782;&#21035;&#29702;&#35770;&#30340;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22495;&#20043;&#38388;&#30340;&#20559;&#31227;&#23545;&#19981;&#21464;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#20102;&#28304;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#12290;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#28789;&#27963;&#65292;&#19981;&#38656;&#35201;&#28385;&#36275;&#20005;&#26684;&#30340;&#20551;&#35774;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2310.04723</link><description>&lt;p&gt;
&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#23376;&#31354;&#38388;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Subspace Identification for Multi-Source Domain Adaptation. (arXiv:2310.04723v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23376;&#31354;&#38388;&#35782;&#21035;&#29702;&#35770;&#30340;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22495;&#20043;&#38388;&#30340;&#20559;&#31227;&#23545;&#19981;&#21464;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#20102;&#28304;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#12290;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#28789;&#27963;&#65292;&#19981;&#38656;&#35201;&#28385;&#36275;&#20005;&#26684;&#30340;&#20551;&#35774;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#26041;&#27861;&#26088;&#22312;&#23558;&#22810;&#20010;&#26377;&#26631;&#31614;&#30340;&#28304;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19968;&#20010;&#26080;&#26631;&#31614;&#30340;&#30446;&#26631;&#22495;&#20013;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#22495;&#20043;&#38388;&#26045;&#21152;&#26368;&#23567;&#30340;&#21464;&#21270;&#26469;&#23454;&#29616;&#30446;&#26631;&#32852;&#21512;&#20998;&#24067;&#30340;&#21487;&#36776;&#35782;&#24615;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#20005;&#26684;&#30340;&#26465;&#20214;&#65292;&#22914;&#36275;&#22815;&#25968;&#37327;&#30340;&#22495;&#12289;&#28508;&#22312;&#21464;&#37327;&#30340;&#21333;&#35843;&#21464;&#25442;&#21644;&#19981;&#21464;&#30340;&#26631;&#31614;&#20998;&#24067;&#12290;&#36825;&#20123;&#35201;&#27714;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#38590;&#28385;&#36275;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#36825;&#20123;&#20005;&#26684;&#20551;&#35774;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#31354;&#38388;&#35782;&#21035;&#29702;&#35770;&#65292;&#23427;&#22312;&#20851;&#20110;&#22495;&#25968;&#37327;&#21644;&#21464;&#25442;&#29305;&#24615;&#26041;&#38754;&#20855;&#26377;&#36739;&#23485;&#26494;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#32780;&#36890;&#36807;&#26368;&#23567;&#21270;&#22495;&#20043;&#38388;&#30340;&#20559;&#31227;&#23545;&#19981;&#21464;&#21464;&#37327;&#30340;&#24433;&#21709;&#26469;&#20419;&#36827;&#22495;&#33258;&#36866;&#24212;&#12290;&#22522;&#20110;&#36825;&#20010;&#29702;&#35770;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#30340;&#23376;&#31354;&#38388;&#35782;&#21035;&#20445;&#35777;&#65288;SIG&#65289;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-source domain adaptation (MSDA) methods aim to transfer knowledge from multiple labeled source domains to an unlabeled target domain. Although current methods achieve target joint distribution identifiability by enforcing minimal changes across domains, they often necessitate stringent conditions, such as an adequate number of domains, monotonic transformation of latent variables, and invariant label distributions. These requirements are challenging to satisfy in real-world applications. To mitigate the need for these strict assumptions, we propose a subspace identification theory that guarantees the disentanglement of domain-invariant and domain-specific variables under less restrictive constraints regarding domain numbers and transformation properties, thereby facilitating domain adaptation by minimizing the impact of domain shifts on invariant variables. Based on this theory, we develop a Subspace Identification Guarantee (SIG) model that leverages variational inference. Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#24615;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;TAB&#27169;&#22411;&#65292;&#36890;&#36807;&#34913;&#37327;&#30446;&#26631;&#19982;&#28304;&#22238;&#24402;&#20989;&#25968;&#20043;&#38388;&#30340;&#27169;&#31946;&#24230;&#27700;&#24179;&#26469;&#25913;&#21892;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#36991;&#20813;&#36127;&#36801;&#31227;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;TAB&#27169;&#22411;&#22312;&#38750;&#21442;&#25968;&#20998;&#31867;&#21644;&#36923;&#36753;&#22238;&#24402;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04606</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#21487;&#38752;&#28304;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Transfer Learning with Unreliable Source Data. (arXiv:2310.04606v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#24615;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;TAB&#27169;&#22411;&#65292;&#36890;&#36807;&#34913;&#37327;&#30446;&#26631;&#19982;&#28304;&#22238;&#24402;&#20989;&#25968;&#20043;&#38388;&#30340;&#27169;&#31946;&#24230;&#27700;&#24179;&#26469;&#25913;&#21892;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#36991;&#20813;&#36127;&#36801;&#31227;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;TAB&#27169;&#22411;&#22312;&#38750;&#21442;&#25968;&#20998;&#31867;&#21644;&#36923;&#36753;&#22238;&#24402;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#40065;&#26834;&#24615;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#27169;&#31946;&#24615;&#21644;&#30446;&#26631;&#19982;&#28304;&#20998;&#24067;&#20043;&#38388;&#30340;&#24369;&#21487;&#36716;&#31227;&#20449;&#21495;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#65292;&#31216;&#20026;&#8220;&#27169;&#31946;&#24230;&#27700;&#24179;&#8221;&#65292;&#29992;&#20110;&#34913;&#37327;&#30446;&#26631;&#19982;&#28304;&#22238;&#24402;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#19968;&#33324;&#23450;&#29702;&#65292;&#35828;&#26126;&#20102;&#36825;&#20010;&#26032;&#37327;&#19982;&#23398;&#20064;&#30340;&#36801;&#31227;&#24615;&#22312;&#39118;&#38505;&#25913;&#21892;&#26041;&#38754;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#36793;&#30028;&#21608;&#22260;&#36716;&#31227;&#8221;(Transfer Around Boundary, TAB)&#27169;&#22411;&#36890;&#36807;&#22312;&#30446;&#26631;&#25968;&#25454;&#21644;&#28304;&#25968;&#25454;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#24179;&#34913;&#30340;&#38408;&#20540;&#65292;&#26082;&#39640;&#25928;&#21448;&#40065;&#26834;&#65292;&#33021;&#22815;&#25913;&#21892;&#20998;&#31867;&#24182;&#36991;&#20813;&#36127;&#36801;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;TAB&#27169;&#22411;&#22312;&#38750;&#21442;&#25968;&#20998;&#31867;&#21644;&#36923;&#36753;&#22238;&#24402;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#36798;&#21040;&#20102;&#26368;&#20248;&#30340;&#19978;&#30028;&#65292;&#21482;&#26377;&#23545;&#25968;&#22240;&#23376;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#20223;&#30495;&#30740;&#31350;&#36827;&#19968;&#27493;&#25903;&#25345;&#20102;TAB&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses challenges in robust transfer learning stemming from ambiguity in Bayes classifiers and weak transferable signals between the target and source distribution. We introduce a novel quantity called the ''ambiguity level'' that measures the discrepancy between the target and source regression functions, propose a simple transfer learning procedure, and establish a general theorem that shows how this new quantity is related to the transferability of learning in terms of risk improvements. Our proposed ''Transfer Around Boundary'' (TAB) model, with a threshold balancing the performance of target and source data, is shown to be both efficient and robust, improving classification while avoiding negative transfer. Moreover, we demonstrate the effectiveness of the TAB model on non-parametric classification and logistic regression tasks, achieving upper bounds which are optimal up to logarithmic factors. Simulation studies lend further support to the effectiveness of TAB. We 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21452;&#37325;&#40065;&#26834;&#30340;&#20272;&#35745;&#22120;TNDDR&#65292;&#29992;&#20110;&#22312;&#38452;&#24615;&#27979;&#35797;&#35774;&#35745;&#19979;&#20272;&#35745;COVID-19&#30123;&#33495;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#36741;&#21161;&#20989;&#25968;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.04578</link><description>&lt;p&gt;
TNDDR: &#39640;&#25928;&#19988;&#21452;&#37325;&#40065;&#26834;&#30340;COVID-19&#30123;&#33495;&#26377;&#25928;&#24615;&#20272;&#35745;&#22312;&#38452;&#24615;&#27979;&#35797;&#35774;&#35745;&#19979;
&lt;/p&gt;
&lt;p&gt;
TNDDR: Efficient and doubly robust estimation of COVID-19 vaccine effectiveness under the test-negative design. (arXiv:2310.04578v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04578
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21452;&#37325;&#40065;&#26834;&#30340;&#20272;&#35745;&#22120;TNDDR&#65292;&#29992;&#20110;&#22312;&#38452;&#24615;&#27979;&#35797;&#35774;&#35745;&#19979;&#20272;&#35745;COVID-19&#30123;&#33495;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#36741;&#21161;&#20989;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38452;&#24615;&#27979;&#35797;&#35774;&#35745;&#65288;TND&#65289;&#24120;&#29992;&#20110;&#30417;&#27979;&#23395;&#33410;&#24615;&#27969;&#24863;&#30123;&#33495;&#26377;&#25928;&#24615;&#65288;VE&#65289;&#65292;&#20294;&#26368;&#36817;&#24050;&#25104;&#20026;COVID-19&#30123;&#33495;&#30417;&#27979;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#30001;&#20110;&#32467;&#26524;&#30456;&#20851;&#25277;&#26679;&#65292;&#23427;&#23481;&#26131;&#21463;&#21040;&#36873;&#25321;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;TND&#19979;&#22240;&#26524;&#21442;&#25968;&#30340;&#21487;&#37492;&#21035;&#24615;&#21644;&#20272;&#35745;&#38382;&#39064;&#65292;&#20294;&#23578;&#26410;&#30740;&#31350;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#22312;&#26080;&#28151;&#26434;&#24615;&#20551;&#35774;&#19979;&#30340;&#25928;&#29575;&#36793;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TNDDR&#65288;TND&#21452;&#37325;&#40065;&#26834;&#65289;&#30340;&#19968;&#27493;&#21452;&#37325;&#40065;&#26834;&#21644;&#23616;&#37096;&#39640;&#25928;&#20272;&#35745;&#22120;,&#23427;&#21033;&#29992;&#26679;&#26412;&#20998;&#21106;&#65292;&#24182;&#21487;&#20197;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#20272;&#35745;&#36741;&#21161;&#20989;&#25968;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#32467;&#26524;&#36793;&#38469;&#26399;&#26395;&#30340;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;&#65288;EIF&#65289;&#65292;&#25506;&#32034;&#20102;von Mises&#23637;&#24320;&#65292;&#24182;&#24314;&#31435;&#20102;TNDDR&#30340;n&#30340;&#24179;&#26041;&#26681;&#19968;&#33268;&#24615;&#12289;&#28176;&#36817;&#27491;&#24577;&#24615;&#21644;&#21452;&#37325;&#40065;&#26834;&#24615;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the test-negative design (TND), which is routinely used for monitoring seasonal flu vaccine effectiveness (VE), has recently become integral to COVID-19 vaccine surveillance, it is susceptible to selection bias due to outcome-dependent sampling. Some studies have addressed the identifiability and estimation of causal parameters under the TND, but efficiency bounds for nonparametric estimators of the target parameter under the unconfoundedness assumption have not yet been investigated. We propose a one-step doubly robust and locally efficient estimator called TNDDR (TND doubly robust), which utilizes sample splitting and can incorporate machine learning techniques to estimate the nuisance functions. We derive the efficient influence function (EIF) for the marginal expectation of the outcome under a vaccination intervention, explore the von Mises expansion, and establish the conditions for $\sqrt{n}-$consistency, asymptotic normality and double robustness of TNDDR. The proposed TND
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#21387;&#21147;&#27979;&#35797;&#20013;&#24212;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#38477;&#32500;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#33258;&#32534;&#30721;&#22120;&#25193;&#23637;&#20102;&#39118;&#38505;&#22240;&#32032;&#30340;&#33539;&#22260;&#65292;&#24182;&#25552;&#20379;&#20102;&#28508;&#22312;&#22240;&#23376;&#30340;&#35299;&#37322;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#20854;&#20182;&#38656;&#35201;&#38477;&#32500;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#39046;&#22495;&#20063;&#26377;&#29992;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.04511</link><description>&lt;p&gt;
&#39118;&#38505;&#22240;&#32032;&#32858;&#21512;&#21644;&#21387;&#21147;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Risk factor aggregation and stress testing. (arXiv:2310.04511v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#21387;&#21147;&#27979;&#35797;&#20013;&#24212;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#38477;&#32500;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#33258;&#32534;&#30721;&#22120;&#25193;&#23637;&#20102;&#39118;&#38505;&#22240;&#32032;&#30340;&#33539;&#22260;&#65292;&#24182;&#25552;&#20379;&#20102;&#28508;&#22312;&#22240;&#23376;&#30340;&#35299;&#37322;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#20854;&#20182;&#38656;&#35201;&#38477;&#32500;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#39046;&#22495;&#20063;&#26377;&#29992;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#21147;&#27979;&#35797;&#26159;&#23558;&#19981;&#21033;&#30340;&#37329;&#34701;&#25110;&#23439;&#35266;&#32463;&#27982;&#24773;&#26223;&#24212;&#29992;&#20110;&#25237;&#36164;&#32452;&#21512;&#12290;&#20026;&#27492;&#65292;&#36890;&#36807;&#22240;&#23376;&#27169;&#22411;&#36890;&#24120;&#23558;&#37329;&#34701;&#25110;&#23439;&#35266;&#32463;&#27982;&#39118;&#38505;&#22240;&#32032;&#19982;&#36164;&#20135;&#25910;&#30410;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#37319;&#29992;&#38477;&#32500;&#25216;&#26415;&#65292;&#21363;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#33258;&#32534;&#30721;&#22120;&#65292;&#25193;&#23637;&#20102;&#39118;&#38505;&#22240;&#32032;&#30340;&#33539;&#22260;&#12290;&#36825;&#23548;&#33268;&#20102;&#32858;&#21512;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#21253;&#25324;&#20840;&#29699;&#22240;&#23376;&#12289;&#20195;&#34920;&#24191;&#27867;&#22320;&#29702;&#21306;&#22495;&#30340;&#22240;&#23376;&#20197;&#21450;&#20195;&#34920;&#21608;&#26399;&#24615;&#21644;&#38450;&#24481;&#24615;&#34892;&#19994;&#30340;&#22240;&#23376;&#12290;&#30001;&#20110;&#36866;&#24212;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#33258;&#32534;&#30721;&#22120;&#23545;&#28508;&#22312;&#22240;&#23376;&#36827;&#34892;&#20102;&#35299;&#37322;&#65292;&#22240;&#27492;&#35813;&#26041;&#27861;&#22312;&#20854;&#20182;&#38656;&#35201;&#38477;&#32500;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#39046;&#22495;&#20063;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stress testing refers to the application of adverse financial or macroeconomic scenarios to a portfolio. For this purpose, financial or macroeconomic risk factors are linked with asset returns, typically via a factor model. We expand the range of risk factors by adapting dimension-reduction techniques from unsupervised learning, namely PCA and autoencoders. This results in aggregated risk factors, encompassing a global factor, factors representing broad geographical regions, and factors specific to cyclical and defensive industries. As the adapted PCA and autoencoders provide an interpretation of the latent factors, this methodology is also valuable in other areas where dimension-reduction and explainability are crucial.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#29420;&#31435;&#38477;&#32500;&#21644;&#21516;&#26102;&#38477;&#32500;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#32447;&#24615;&#27169;&#22411;&#35780;&#20272;&#20102;&#20854;&#30456;&#23545;&#20934;&#30830;&#24615;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.04458</link><description>&lt;p&gt;
&#21516;&#26102;&#38477;&#32500;&#65306;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simultaneous Dimensionality Reduction: A Data Efficient Approach for Multimodal Representations Learning. (arXiv:2310.04458v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04458
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#29420;&#31435;&#38477;&#32500;&#21644;&#21516;&#26102;&#38477;&#32500;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#32447;&#24615;&#27169;&#22411;&#35780;&#20272;&#20102;&#20854;&#30456;&#23545;&#20934;&#30830;&#24615;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#38477;&#32500;&#26041;&#27861;&#65306;&#29420;&#31435;&#38477;&#32500;(IDR)&#21644;&#21516;&#26102;&#38477;&#32500;(SDR)&#12290;&#22312;IDR&#26041;&#27861;&#20013;&#65292;&#27599;&#20010;&#27169;&#24577;&#37117;&#34987;&#29420;&#31435;&#21387;&#32553;&#65292;&#21147;&#22270;&#20445;&#30041;&#27599;&#20010;&#27169;&#24577;&#20869;&#30340;&#23613;&#21487;&#33021;&#22810;&#30340;&#21464;&#21270;&#12290;&#30456;&#21453;&#65292;&#22312;SDR&#20013;&#65292;&#21516;&#26102;&#21387;&#32553;&#27169;&#24577;&#20197;&#26368;&#22823;&#21270;&#20943;&#23569;&#25551;&#36848;&#20043;&#38388;&#30340;&#21327;&#21464;&#24615;&#65292;&#21516;&#26102;&#23545;&#20445;&#30041;&#21333;&#20010;&#21464;&#21270;&#30340;&#31243;&#24230;&#19981;&#22826;&#20851;&#27880;&#12290;&#20856;&#22411;&#30340;&#20363;&#23376;&#21253;&#25324;&#20559;&#26368;&#23567;&#20108;&#20056;&#27861;&#21644;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#12290;&#34429;&#28982;&#36825;&#20123;&#38477;&#32500;&#26041;&#27861;&#26159;&#32479;&#35745;&#23398;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#30340;&#30456;&#23545;&#31934;&#24230;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#35201;&#27714;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#32447;&#24615;&#27169;&#22411;&#26469;&#21512;&#25104;&#20855;&#26377;&#24050;&#30693;&#26041;&#24046;&#21644;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21327;&#26041;&#24046;&#30340;&#37325;&#26500;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore two primary classes of approaches to dimensionality reduction (DR): Independent Dimensionality Reduction (IDR) and Simultaneous Dimensionality Reduction (SDR). In IDR methods, of which Principal Components Analysis is a paradigmatic example, each modality is compressed independently, striving to retain as much variation within each modality as possible. In contrast, in SDR, one simultaneously compresses the modalities to maximize the covariation between the reduced descriptions while paying less attention to how much individual variation is preserved. Paradigmatic examples include Partial Least Squares and Canonical Correlations Analysis. Even though these DR methods are a staple of statistics, their relative accuracy and data set size requirements are poorly understood. We introduce a generative linear model to synthesize multimodal data with known variance and covariance structures to examine these questions. We assess the accuracy of the reconstruction of the covariance s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25193;&#25955;&#27169;&#22411;&#20026;&#28789;&#24863;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#24182;&#21487;&#22312;&#25968;&#37327;&#30456;&#21516;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#19979;&#19982;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#27604;&#36739;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#36890;&#36807;&#25512;&#23548;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.04417</link><description>&lt;p&gt;
&#25193;&#25955;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Random Feature Model. (arXiv:2310.04417v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25193;&#25955;&#27169;&#22411;&#20026;&#28789;&#24863;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#24182;&#21487;&#22312;&#25968;&#37327;&#30456;&#21516;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#19979;&#19982;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#27604;&#36739;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#36890;&#36807;&#25512;&#23548;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#24050;&#25104;&#21151;&#29992;&#20110;&#29983;&#25104;&#20174;&#22122;&#22768;&#20013;&#20135;&#29983;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#38590;&#20197;&#35299;&#37322;&#65292;&#32570;&#20047;&#29702;&#35770;&#20381;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#22312;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#25193;&#25955;&#27169;&#22411;&#21551;&#21457;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#26082;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#33021;&#32473;&#20986;&#19982;&#20855;&#26377;&#30456;&#21516;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#21033;&#29992;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#23548;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#26102;&#23578;MNIST&#25968;&#25454;&#38598;&#21644;&#20048;&#22120;&#38899;&#39057;&#25968;&#25454;&#19978;&#29983;&#25104;&#26679;&#26412;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models have been successfully used to generate data from noise. However, most diffusion models are computationally expensive and difficult to interpret with a lack of theoretical justification. Random feature models on the other hand have gained popularity due to their interpretability but their application to complex machine learning tasks remains limited. In this work, we present a diffusion model-inspired deep random feature model that is interpretable and gives comparable numerical results to a fully connected neural network having the same number of trainable parameters. Specifically, we extend existing results for random features and derive generalization bounds between the distribution of sampled data and the true distribution using properties of score matching. We validate our findings by generating samples on the fashion MNIST dataset and instrumental audio data.
&lt;/p&gt;</description></item><item><title>MoatPlus&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#20215;&#26684;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#21644;&#21382;&#21490;&#20215;&#26684;&#36235;&#21183;&#29983;&#25104;&#19978;&#38480;&#20215;&#26684;&#36793;&#30028;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#24066;&#22330;&#20013;&#30340;&#25968;&#25454;&#36136;&#37327;&#21644;&#38169;&#35823;&#20215;&#26684;&#21457;&#24067;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04367</link><description>&lt;p&gt;
&#19968;&#20010;&#22823;&#35268;&#27169;&#24066;&#22330;&#20215;&#26684;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Marketplace Price Anomaly Detection System at Scale. (arXiv:2310.04367v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04367
&lt;/p&gt;
&lt;p&gt;
MoatPlus&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#20215;&#26684;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#21644;&#21382;&#21490;&#20215;&#26684;&#36235;&#21183;&#29983;&#25104;&#19978;&#38480;&#20215;&#26684;&#36793;&#30028;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#24066;&#22330;&#20013;&#30340;&#25968;&#25454;&#36136;&#37327;&#21644;&#38169;&#35823;&#20215;&#26684;&#21457;&#24067;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24066;&#22330;&#27599;&#22825;&#22312;&#24179;&#21488;&#19978;&#25191;&#34892;&#22823;&#37327;&#30340;&#20215;&#26684;&#26356;&#26032;&#65292;&#36825;&#20123;&#26356;&#26032;&#30001;&#20010;&#20307;&#24066;&#22330;&#21334;&#23478;&#21457;&#36215;&#12290;&#36825;&#31181;&#20215;&#26684;&#27665;&#20027;&#21270;&#38543;&#30528;&#25968;&#25454;&#36136;&#37327;&#30340;&#25361;&#25112;&#32780;&#22686;&#21152;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#22312;&#32447;&#38646;&#21806;&#21830;&#65292;&#32570;&#20047;&#38598;&#20013;&#30340;&#38450;&#25252;&#25514;&#26045;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#38169;&#35823;&#20215;&#26684;&#22312;&#32593;&#31449;&#19978;&#21457;&#24067;&#65292;&#20174;&#32780;&#32473;&#39038;&#23458;&#20307;&#39564;&#24102;&#26469;&#24046;&#35780;&#21644;&#28508;&#22312;&#30340;&#25910;&#20837;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MoatPlus&#65288;&#20351;&#29992;&#26641;&#12289;&#22522;&#20110;&#37051;&#36817;&#24230;&#30340;&#26631;&#31614;&#20197;&#21450;&#26080;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#30340;&#33945;&#38754;&#26368;&#20248;&#38170;&#28857;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#19981;&#26029;&#22686;&#38271;&#30340;&#24066;&#22330;&#24179;&#21488;&#30340;&#21487;&#25193;&#23637;&#20215;&#26684;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#12290;&#30446;&#26631;&#26159;&#21033;&#29992;&#37051;&#36817;&#24230;&#21644;&#21382;&#21490;&#20215;&#26684;&#36235;&#21183;&#30340;&#26080;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#26469;&#29983;&#25104;&#19978;&#38480;&#20215;&#26684;&#36793;&#30028;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#38598;&#21512;&#26469;&#26816;&#27979;&#22522;&#20110;&#20215;&#26684;&#30340;&#29305;&#24449;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#65292;&#25490;&#38500;&#24322;&#24120;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20248;&#21270;&#30340;&#21152;&#26435;&#26041;&#26696;&#26469;&#26500;&#24314;&#23454;&#26102;&#23450;&#20215;&#31649;&#36947;&#20013;&#21487;&#38752;&#30340;&#20215;&#26684;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online marketplaces execute large volume of price updates that are initiated by individual marketplace sellers each day on the platform. This price democratization comes with increasing challenges with data quality. Lack of centralized guardrails that are available for a traditional online retailer causes a higher likelihood for inaccurate prices to get published on the website, leading to poor customer experience and potential for revenue loss. We present MoatPlus (Masked Optimal Anchors using Trees, Proximity-based Labeling and Unsupervised Statistical-features), a scalable price anomaly detection framework for a growing marketplace platform. The goal is to leverage proximity and historical price trends from unsupervised statistical features to generate an upper price bound. We build an ensemble of models to detect irregularities in price-based features, exclude irregular features and use optimized weighting scheme to build a reliable price bound in real-time pricing pipeline. We obs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26367;&#25442;&#20010;&#20307;&#30340;&#25935;&#24863;&#29305;&#24449;&#20026;&#32858;&#31867;&#30340;&#24179;&#22343;&#20540;&#26469;&#22686;&#24378;&#38544;&#31169;&#12290;&#36890;&#36807;&#23545;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#35757;&#32451;&#27169;&#22411;&#30340;&#31934;&#30830;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#32452;&#25104;&#37096;&#20998;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#22312;&#26576;&#20123;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.04015</link><description>&lt;p&gt;
&#36890;&#36807;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#23398;&#20064;&#65306;&#23545;&#27169;&#22411;&#27867;&#21270;&#30340;&#31934;&#30830;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization. (arXiv:2310.04015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26367;&#25442;&#20010;&#20307;&#30340;&#25935;&#24863;&#29305;&#24449;&#20026;&#32858;&#31867;&#30340;&#24179;&#22343;&#20540;&#26469;&#22686;&#24378;&#38544;&#31169;&#12290;&#36890;&#36807;&#23545;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#35757;&#32451;&#27169;&#22411;&#30340;&#31934;&#30830;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#32452;&#25104;&#37096;&#20998;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#22312;&#26576;&#20123;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#30830;&#20445;&#29992;&#25143;&#25968;&#25454;&#30340;&#20445;&#25252;&#20173;&#28982;&#26159;&#36825;&#20123;&#23398;&#20064;&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#22686;&#24378;&#38544;&#31169;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#21311;&#21517;&#25968;&#25454;&#32780;&#19981;&#26159;&#20010;&#20307;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#8221;&#30340;&#33258;&#28982;&#25216;&#26415;&#65292;&#23427;&#28041;&#21450;&#23558;&#20010;&#20307;&#30340;&#25935;&#24863;&#29305;&#24449;&#26367;&#25442;&#20026;&#32858;&#31867;&#30340;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#24433;&#21709;&#20854;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#20010;&#28176;&#36817;&#24773;&#20917;&#65292;&#21363;&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#19982;&#29305;&#24449;&#32500;&#24230;&#25104;&#27604;&#20363;&#22686;&#38271;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#20984;&#39640;&#26031;&#26497;&#23567;&#21270;&#26497;&#22823;&#23450;&#29702;&#65288;Convex Gaussian Minimax Theorem&#65292;CGMT&#65289;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29702;&#35770;&#19978;&#29702;&#35299;&#19981;&#21516;&#27169;&#22411;&#32452;&#25104;&#37096;&#20998;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20123;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While personalized recommendations systems have become increasingly popular, ensuring user data protection remains a paramount concern in the development of these learning systems. A common approach to enhancing privacy involves training models using anonymous data rather than individual data. In this paper, we explore a natural technique called \emph{look-alike clustering}, which involves replacing sensitive features of individuals with the cluster's average values. We provide a precise analysis of how training models using anonymous cluster centers affects their generalization capabilities. We focus on an asymptotic regime where the size of the training set grows in proportion to the features dimension. Our analysis is based on the Convex Gaussian Minimax Theorem (CGMT) and allows us to theoretically understand the role of different model components on the generalization error. In addition, we demonstrate that in certain high-dimensional regimes, training over anonymous cluster cente
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36864;&#28779;&#26041;&#27861;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#12290;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#23545;&#20272;&#35745;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#36864;&#28779;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#22120;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20351;&#29992;&#20960;&#20309;&#36335;&#24452;&#21487;&#20197;&#38477;&#20302;&#20272;&#35745;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.03902</link><description>&lt;p&gt;
&#36890;&#36807;&#36864;&#28779;&#26469;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#21487;&#35777;&#26126;&#30340;&#30410;&#22788;&#65306;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65292;&#20197;&#21450;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
Provable benefits of annealing for estimating normalizing constants: Importance Sampling, Noise-Contrastive Estimation, and beyond. (arXiv:2310.03902v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03902
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36864;&#28779;&#26041;&#27861;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#12290;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#23545;&#20272;&#35745;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#36864;&#28779;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#22120;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20351;&#29992;&#20960;&#20309;&#36335;&#24452;&#21487;&#20197;&#38477;&#20302;&#20272;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#23637;&#20102;&#20960;&#31181;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#26469;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#65288;&#37197;&#20998;&#20989;&#25968;&#65289;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#36864;&#28779;&#30340;&#24605;&#24819;&#12290;&#21363;&#20174;&#21487;&#35745;&#31639;&#30340;&#8220;&#25552;&#35758;&#8221;&#20998;&#24067;&#21644;&#26410;&#24402;&#19968;&#21270;&#30340;&#8220;&#30446;&#26631;&#8221;&#20998;&#24067;&#20043;&#38388;&#30340;&#36335;&#24452;&#36880;&#27493;&#37319;&#26679;&#12290;&#36825;&#20123;&#23478;&#26063;&#20013;&#30340;&#37325;&#35201;&#20272;&#35745;&#22120;&#21253;&#25324;&#36864;&#28779;&#37325;&#35201;&#24615;&#25277;&#26679;&#21644;&#36864;&#28779;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#65306;&#20351;&#29992;&#21738;&#20010;&#20272;&#35745;&#22120;&#12289;&#20351;&#29992;&#21738;&#20010;&#20998;&#24067;&#36335;&#24452;&#20197;&#21450;&#26159;&#21542;&#20351;&#29992;&#20998;&#24067;&#36335;&#24452;&#65307;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#23545;&#20110;&#21738;&#20123;&#36873;&#25321;&#26159;&#26377;&#25928;&#30340;&#36824;&#27809;&#26377;&#26126;&#30830;&#30340;&#29702;&#35770;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20135;&#29983;&#30340;&#28176;&#36817;&#20272;&#35745;&#35823;&#24046;&#26469;&#35780;&#20272;&#27599;&#20010;&#35774;&#35745;&#36873;&#25321;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;NCE&#27604;&#37325;&#35201;&#24615;&#25277;&#26679;&#20272;&#35745;&#22120;&#26356;&#26377;&#25928;&#65292;&#20294;&#22312;&#26080;&#38480;&#23567;&#30340;&#36335;&#24452;&#27493;&#38271;&#30340;&#26497;&#38480;&#19979;&#65292;&#24046;&#24322;&#28040;&#22833;&#20102;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#20960;&#20309;&#36335;&#24452;&#23558;&#20272;&#35745;&#35823;&#24046;&#20174;&#25351;&#25968;&#32423;&#38477;&#20302;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Recent research has developed several Monte Carlo methods for estimating the normalization constant (partition function) based on the idea of annealing. This means sampling successively from a path of distributions that interpolate between a tractable "proposal" distribution and the unnormalized "target" distribution. Prominent estimators in this family include annealed importance sampling and annealed noise-contrastive estimation (NCE). Such methods hinge on a number of design choices: which estimator to use, which path of distributions to use and whether to use a path at all; so far, there is no definitive theory on which choices are efficient. Here, we evaluate each design choice by the asymptotic estimation error it produces. First, we show that using NCE is more efficient than the importance sampling estimator, but in the limit of infinitesimal path steps, the difference vanishes. Second, we find that using the geometric path brings down the estimation error from an exponential to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38750;&#32447;&#24615;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#23454;&#29616;&#32479;&#19968;&#30340;&#20449;&#21495;&#24674;&#22797;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#21644;&#21487;&#33021;&#38750;&#36830;&#32493;&#25110;&#26410;&#30693;&#30340;&#35266;&#27979;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#20013;&#25152;&#26377;&#21487;&#33021;&#30340;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2310.03758</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#32479;&#19968;&#20449;&#21495;&#24674;&#22797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing. (arXiv:2310.03758v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03758
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38750;&#32447;&#24615;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#23454;&#29616;&#32479;&#19968;&#30340;&#20449;&#21495;&#24674;&#22797;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#21644;&#21487;&#33021;&#38750;&#36830;&#32493;&#25110;&#26410;&#30693;&#30340;&#35266;&#27979;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#20013;&#25152;&#26377;&#21487;&#33021;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#20351;&#29992;&#29983;&#25104;&#20808;&#39564;&#20174;m&#20010;&#27979;&#37327;&#20013;&#65288;m&#8810;n&#65289;&#24674;&#22797;&#19968;&#20010;&#20449;&#21495;x&#8727;&#8712;Rn&#65292;&#20854;&#20013;G&#36890;&#24120;&#26159;&#19968;&#20010;L-Lipschitz&#36830;&#32493;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;B2k(r)&#34920;&#31034;Rk&#20013;&#30340;&#21322;&#24452;&#20026;r&#30340;&#8467;2&#29699;&#12290;&#22312;&#38750;&#32447;&#24615;&#27979;&#37327;&#19979;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#32467;&#26524;&#26159;&#38750;&#22343;&#21248;&#30340;&#65292;&#21363;&#23427;&#20204;&#23545;&#20110;&#22266;&#23450;&#30340;x&#8727;&#20855;&#26377;&#39640;&#27010;&#29575;&#65292;&#32780;&#19981;&#26159;&#23545;&#20110;&#25152;&#26377;&#30340;x&#8727;&#21516;&#26102;&#25104;&#31435;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#25512;&#23548;&#38750;&#32447;&#24615;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#30340;&#22343;&#21248;&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#21644;&#21487;&#33021;&#38750;&#36830;&#32493;&#25110;&#26410;&#30693;&#30340;&#35266;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20102;1&#20301;/&#22343;&#21248;&#37327;&#21270;&#35266;&#27979;&#21644;&#21333;&#32034;&#24341;&#27169;&#22411;&#20316;&#20026;&#35268;&#33539;&#31034;&#20363;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#24863;&#30693;&#38598;&#21512;&#30340;&#21333;&#20010;&#23454;&#29616;&#21644;&#24191;&#20041;Lasso&#65292;&#25152;&#26377;&#30340;x&#8727;&#8712;G(B2k(r))&#21487;&#20197;&#24674;&#22797;&#21040;&#19968;&#20010;el
&lt;/p&gt;
&lt;p&gt;
In generative compressed sensing (GCS), we want to recover a signal $\mathbf{x}^* \in \mathbb{R}^n$ from $m$ measurements ($m\ll n$) using a generative prior $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$, where $G$ is typically an $L$-Lipschitz continuous generative model and $\mathbb{B}_2^k(r)$ represents the radius-$r$ $\ell_2$-ball in $\mathbb{R}^k$. Under nonlinear measurements, most prior results are non-uniform, i.e., they hold with high probability for a fixed $\mathbf{x}^*$ rather than for all $\mathbf{x}^*$ simultaneously. In this paper, we build a unified framework to derive uniform recovery guarantees for nonlinear GCS where the observation model is nonlinear and possibly discontinuous or unknown. Our framework accommodates GCS with 1-bit/uniformly quantized observations and single index models as canonical examples. Specifically, using a single realization of the sensing ensemble and generalized Lasso, {\em all} $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$ can be recovered up to an $\el
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#26041;&#27861;&#65292;&#20998;&#21035;&#36890;&#36807;&#26367;&#25442;Lai&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.03722</link><description>&lt;p&gt;
&#26410;&#30693;&#26041;&#24046;&#19979;&#30340;&#39640;&#26031;&#22343;&#20540;&#30340;&#20219;&#24847;&#26377;&#25928;T&#26816;&#39564;&#21644;&#32622;&#20449;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance. (arXiv:2310.03722v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#26041;&#27861;&#65292;&#20998;&#21035;&#36890;&#36807;&#26367;&#25442;Lai&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1976&#24180;&#65292;Lai&#26500;&#36896;&#20102;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#22343;&#20540;$\mu$&#30340;&#39640;&#26031;&#20998;&#24067;&#30340;&#32622;&#20449;&#24207;&#21015;&#65292;&#35813;&#20998;&#24067;&#30340;&#26041;&#24046;$\sigma$&#26159;&#26410;&#30693;&#30340;&#12290;&#20182;&#20351;&#29992;&#20102;&#20851;&#20110;$\sigma$&#30340;&#19981;&#36866;&#24403;&#65288;&#21491;Haar&#65289;&#28151;&#21512;&#21644;&#20851;&#20110;$\mu$&#30340;&#19981;&#36866;&#24403;&#65288;&#24179;&#22374;&#65289;&#28151;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#20182;&#26500;&#24314;&#30340;&#32454;&#33410;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#24191;&#20041;&#30340;&#19981;&#21487;&#31215;&#20998;&#38789;&#21644;&#25193;&#23637;&#30340;&#32500;&#23572;&#19981;&#31561;&#24335;&#12290;&#23613;&#31649;&#36825;&#30830;&#23454;&#20135;&#29983;&#20102;&#19968;&#20010;&#39034;&#24207;T&#26816;&#39564;&#65292;&#20294;&#30001;&#20110;&#20182;&#30340;&#38789;&#19981;&#21487;&#31215;&#20998;&#65292;&#23427;&#24182;&#27809;&#26377;&#20135;&#29983;&#19968;&#20010;&#8220;e-process&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#30456;&#21516;&#30340;&#35774;&#32622;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#65306;&#19968;&#20010;&#26159;&#22312;&#32553;&#20943;&#28388;&#27874;&#22120;&#20013;&#30340;&#27979;&#35797;&#38789;&#65292;&#21478;&#19968;&#20010;&#26159;&#22312;&#35268;&#33539;&#25968;&#25454;&#28388;&#27874;&#22120;&#20013;&#30340;&#8220;e-process&#8221;&#12290;&#36825;&#20123;&#20998;&#21035;&#26159;&#36890;&#36807;&#23558;Lai&#30340;&#24179;&#22374;&#28151;&#21512;&#26367;&#25442;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#24182;&#23558;&#23545;$\sigma$&#30340;&#21491;Haar&#28151;&#21512;&#26367;&#25442;&#20026;&#22312;&#38646;&#31354;&#38388;&#19979;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#23601;&#20687;&#22312;&#36890;&#29992;&#25512;&#26029;&#20013;&#19968;&#26679;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$ of a Gaussian distribution with unknown variance $\sigma$. Curiously, he employed both an improper (right Haar) mixture over $\sigma$ and an improper (flat) mixture over $\mu$. Here, we elaborate carefully on the details of his construction, which use generalized nonintegrable martingales and an extended Ville's inequality. While this does yield a sequential t-test, it does not yield an ``e-process'' (due to the nonintegrability of his martingale). In this paper, we develop two new e-processes and confidence sequences for the same setting: one is a test martingale in a reduced filtration, while the other is an e-process in the canonical data filtration. These are respectively obtained by swapping Lai's flat mixture for a Gaussian mixture, and swapping the right Haar mixture over $\sigma$ with the maximum likelihood estimate under the null, as done in universal inference. We also analyze the width of resulting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24369;&#20998;&#24067;&#19981;&#21464;&#24615;&#36827;&#34892;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#34701;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#30340;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#31283;&#23450;&#30340;&#21464;&#37327;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.02854</link><description>&lt;p&gt;
&#36890;&#36807;&#24369;&#20998;&#24067;&#19981;&#21464;&#24615;&#23454;&#29616;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Domain Causal Representation Learning via Weak Distributional Invariances. (arXiv:2310.02854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24369;&#20998;&#24067;&#19981;&#21464;&#24615;&#36827;&#34892;&#22810;&#39046;&#22495;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#34701;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#30340;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#31283;&#23450;&#30340;&#21464;&#37327;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#24050;&#25104;&#20026;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#26680;&#24515;&#12290;&#29305;&#21035;&#26159;&#65292;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#20026;&#23637;&#31034;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#23545;&#20110;&#26631;&#20934;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#20248;&#21183;&#25552;&#20379;&#20102;&#33258;&#28982;&#26426;&#20250;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36807;&#20110;&#31616;&#21270;&#25968;&#25454;&#30340;&#20551;&#35774;&#65292;&#23427;&#20204;&#24448;&#24448;&#19981;&#33021;&#36866;&#29992;&#20110;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#65307;&#20363;&#22914;&#65292;&#27599;&#20010;&#39046;&#22495;&#37117;&#26469;&#33258;&#19981;&#21516;&#30340;&#21333;&#33410;&#28857;&#23436;&#32654;&#24178;&#39044;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#24182;&#21033;&#29992;&#20197;&#19979;&#35266;&#23519;&#32467;&#26524;&#65306;&#22312;&#22810;&#39046;&#22495;&#25968;&#25454;&#20013;&#65292;&#24448;&#24448;&#23384;&#22312;&#19968;&#37096;&#20998;&#28508;&#21464;&#37327;&#30340;&#26576;&#20123;&#20998;&#24067;&#23646;&#24615;&#65288;&#20363;&#22914;&#25903;&#25345;&#24230;&#12289;&#26041;&#24046;&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#20445;&#25345;&#31283;&#23450;&#65307;&#24403;&#27599;&#20010;&#39046;&#22495;&#26469;&#33258;&#22810;&#33410;&#28857;&#19981;&#23436;&#32654;&#24178;&#39044;&#26102;&#65292;&#36825;&#20010;&#23646;&#24615;&#25104;&#31435;&#12290;&#21033;&#29992;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#34701;&#20837;&#36825;&#31181;&#19981;&#21464;&#24615;&#30340;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#31283;&#23450;&#30340;&#21464;&#37327;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal representation learning has emerged as the center of action in causal machine learning research. In particular, multi-domain datasets present a natural opportunity for showcasing the advantages of causal representation learning over standard unsupervised representation learning. While recent works have taken crucial steps towards learning causal representations, they often lack applicability to multi-domain datasets due to over-simplifying assumptions about the data; e.g. each domain comes from a different single-node perfect intervention. In this work, we relax these assumptions and capitalize on the following observation: there often exists a subset of latents whose certain distributional properties (e.g., support, variance) remain stable across domains; this property holds when, for example, each domain comes from a multi-node imperfect intervention. Leveraging this observation, we show that autoencoders that incorporate such invariances can provably identify the stable set o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#22312;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#19979;&#32473;&#20986;&#20102;&#19981;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.01769</link><description>&lt;p&gt;
&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#20943;&#32531;&#30697;&#38453;&#24863;&#30693;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#65306;&#23545;&#31216;&#24615;&#21644;&#21021;&#22987;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization. (arXiv:2310.01769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#22312;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#19979;&#32473;&#20986;&#20102;&#19981;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#38416;&#36848;&#20102;&#36807;&#21442;&#25968;&#21270;&#22914;&#20309;&#25913;&#21464;&#26799;&#24230;&#19979;&#38477;&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#22312;&#23545;&#31216;&#35774;&#32622;&#20013;&#65292;&#36890;&#36807;&#23545;&#31216;&#21442;&#25968;&#21270;&#23398;&#20064;&#26410;&#30693;&#30340;&#21322;&#27491;&#23450;&#30697;&#38453;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65288;$k&gt;r$&#65289;&#38543;&#26426;&#21021;&#22987;&#21270;&#26799;&#24230;&#19979;&#38477;&#30340;&#26032;&#22411;$\Omega (1/T^2)$&#19979;&#30028;&#65292;&#19982;&#31934;&#30830;&#21442;&#25968;&#21270;&#24773;&#20917;&#65288;$k=r$&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;$\exp (-\Omega (T))$&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#23545;&#31216;&#35774;&#32622;&#65292;&#20854;&#20013;$M^* \in \mathbb{R}^{n_1 \times n_2}$&#26159;&#26410;&#30693;&#30697;&#38453;&#65292;&#37319;&#29992;&#38750;&#23545;&#31216;&#21442;&#25968;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper rigorously shows how over-parameterization changes the convergence behaviors of gradient descent (GD) for the matrix sensing problem, where the goal is to recover an unknown low-rank ground-truth matrix from near-isotropic linear measurements. First, we consider the symmetric setting with the symmetric parameterization where $M^* \in \mathbb{R}^{n \times n}$ is a positive semi-definite unknown matrix of rank $r \ll n$, and one uses a symmetric parameterization $XX^\top$ to learn $M^*$. Here $X \in \mathbb{R}^{n \times k}$ with $k &gt; r$ is the factor matrix. We give a novel $\Omega (1/T^2)$ lower bound of randomly initialized GD for the over-parameterized case ($k &gt;r$) where $T$ is the number of iterations. This is in stark contrast to the exact-parameterization scenario ($k=r$) where the convergence rate is $\exp (-\Omega (T))$. Next, we study asymmetric setting where $M^* \in \mathbb{R}^{n_1 \times n_2}$ is the unknown matrix of rank $r \ll \min\{n_1,n_2\}$, and one uses an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#28857;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24067;&#26391;&#36816;&#21160;&#20808;&#39564;&#21644;&#29305;&#24449;&#21521;&#37327;&#24341;&#23548;&#21464;&#37327;&#65292;&#25512;&#23548;&#20102;&#39057;&#29575;&#27966;&#21487;&#20449;&#21306;&#38388;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#38480;&#21046;&#65292;&#24182;&#22312;&#36275;&#22815;&#22810;&#30340;&#24341;&#23548;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#31934;&#30830;&#23450;&#20041;&#20102;&#28176;&#36817;&#39057;&#29575;&#27966;&#35206;&#30422;&#65292;&#20174;&#32780;&#25512;&#26029;&#20986;&#36825;&#20010;&#21464;&#20998;&#26041;&#27861;&#30340;&#21487;&#20449;&#21306;&#38388;&#20309;&#26102;&#20445;&#23432;&#65292;&#20309;&#26102;&#36807;&#20110;&#33258;&#20449;/&#35823;&#23548;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#32467;&#26524;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#20854;&#20182;&#24120;&#35265;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00097</link><description>&lt;p&gt;
&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#28857;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#19982;&#24067;&#26391;&#36816;&#21160;&#20808;&#39564;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Pointwise uncertainty quantification for sparse variational Gaussian process regression with a Brownian motion prior. (arXiv:2310.00097v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#28857;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24067;&#26391;&#36816;&#21160;&#20808;&#39564;&#21644;&#29305;&#24449;&#21521;&#37327;&#24341;&#23548;&#21464;&#37327;&#65292;&#25512;&#23548;&#20102;&#39057;&#29575;&#27966;&#21487;&#20449;&#21306;&#38388;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#38480;&#21046;&#65292;&#24182;&#22312;&#36275;&#22815;&#22810;&#30340;&#24341;&#23548;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#31934;&#30830;&#23450;&#20041;&#20102;&#28176;&#36817;&#39057;&#29575;&#27966;&#35206;&#30422;&#65292;&#20174;&#32780;&#25512;&#26029;&#20986;&#36825;&#20010;&#21464;&#20998;&#26041;&#27861;&#30340;&#21487;&#20449;&#21306;&#38388;&#20309;&#26102;&#20445;&#23432;&#65292;&#20309;&#26102;&#36807;&#20110;&#33258;&#20449;/&#35823;&#23548;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#32467;&#26524;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#20854;&#20182;&#24120;&#35265;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#21521;&#37327;&#24341;&#23548;&#21464;&#37327;&#30340;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#30340;&#28857;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#23545;&#20110;&#20855;&#26377;&#37325;&#26631;&#23450;&#24067;&#26391;&#36816;&#21160;&#20808;&#39564;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#28857;&#21270;&#21487;&#20449;&#21306;&#38388;&#30340;&#39057;&#29575;&#27966;&#22823;&#23567;&#21644;&#35206;&#30422;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#38480;&#21046;&#12290;&#36890;&#36807;&#20805;&#20998;&#30340;&#24341;&#23548;&#21464;&#37327;&#65292;&#25105;&#20204;&#31934;&#30830;&#22320;&#25551;&#36848;&#20102;&#28176;&#36817;&#39057;&#29575;&#27966;&#35206;&#30422;&#65292;&#25512;&#26029;&#20102;&#36825;&#20010;&#21464;&#20998;&#26041;&#27861;&#30340;&#21487;&#20449;&#21306;&#38388;&#20309;&#26102;&#20445;&#23432;&#65292;&#20309;&#26102;&#36807;&#20110;&#33258;&#20449;/&#35823;&#23548;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#20854;&#20182;&#24120;&#35265;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study pointwise estimation and uncertainty quantification for a sparse variational Gaussian process method with eigenvector inducing variables. For a rescaled Brownian motion prior, we derive theoretical guarantees and limitations for the frequentist size and coverage of pointwise credible sets. For sufficiently many inducing variables, we precisely characterize the asymptotic frequentist coverage, deducing when credible sets from this variational method are conservative and when overconfident/misleading. We numerically illustrate the applicability of our results and discuss connections with other common Gaussian process priors.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#32447;&#24615;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#12289;LightGBM&#21644;MLP&#31070;&#32463;&#32593;&#32476;&#20116;&#31181;&#27169;&#22411;&#22312;&#20304;&#27835;&#20122;&#24030;&#39044;&#27979;&#27700;&#36136;pH&#20540;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;LightGBM&#34920;&#29616;&#26368;&#22909;&#12290;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#20248;&#21183;&#26174;&#33879;&#65292;&#32780;MLP&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#24449;&#32553;&#25918;&#25935;&#24863;&#12290;&#21516;&#26102;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#19982;&#21407;&#30740;&#31350;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2309.16951</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#27700;&#36136;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Water quality prediction using machine learning and neural network approaches. (arXiv:2309.16951v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#32447;&#24615;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#12289;LightGBM&#21644;MLP&#31070;&#32463;&#32593;&#32476;&#20116;&#31181;&#27169;&#22411;&#22312;&#20304;&#27835;&#20122;&#24030;&#39044;&#27979;&#27700;&#36136;pH&#20540;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;LightGBM&#34920;&#29616;&#26368;&#22909;&#12290;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#20248;&#21183;&#26174;&#33879;&#65292;&#32780;MLP&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#24449;&#32553;&#25918;&#25935;&#24863;&#12290;&#21516;&#26102;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#19982;&#21407;&#30740;&#31350;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#36164;&#28304;&#26159;&#20154;&#31867;&#29983;&#35745;&#21644;&#32463;&#27982;&#36827;&#27493;&#30340;&#22522;&#30784;&#65292;&#19982;&#20844;&#20849;&#20581;&#24247;&#21644;&#29615;&#22659;&#31119;&#31049;&#26377;&#30528;&#20869;&#22312;&#30340;&#32852;&#31995;&#12290;&#20934;&#30830;&#39044;&#27979;&#27700;&#36136;&#26159;&#25913;&#21892;&#27700;&#36164;&#28304;&#31649;&#29702;&#21644;&#23545;&#25239;&#27745;&#26579;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#22810;&#31181;&#24615;&#33021;&#25351;&#26631;&#65292;&#35780;&#20272;&#20102;&#20116;&#31181;&#19981;&#21516;&#27169;&#22411;&#65288;&#32447;&#24615;&#22238;&#24402;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;XGBoost&#65292;LightGBM&#21644;MLP&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#32654;&#22269;&#20304;&#27835;&#20122;&#24030;&#39044;&#27979;pH&#20540;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;LightGBM&#22312;&#25152;&#26377;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#31934;&#24230;&#12290;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#20984;&#26174;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;MLP&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#23545;&#29305;&#24449;&#32553;&#25918;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#36824;&#35814;&#32454;&#38416;&#36848;&#24182;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#31354;&#38388;&#32771;&#34385;&#22240;&#32032;&#26041;&#38754;&#19982;&#21407;&#30740;&#31350;&#30456;&#27604;&#25152;&#21462;&#24471;&#30340;&#20248;&#36234;&#24615;&#33021;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Water resources serve as the cornerstone of human livelihoods and economic progress, with intrinsic links to both public health and environmental well-being. The accurate prediction of water quality stands as a pivotal factor in enhancing water resource management and combating pollution. This research, employing diverse performance metrics, assesses the efficacy of five distinct models, namely, linear regression, Random Forest, XGBoost, LightGBM, and MLP neural network, in forecasting pH values within Georgia, USA. Concurrently, LightGBM attains the highest average precision among all models examined. Tree-based models underscore their supremacy in addressing regression challenges. Furthermore, the performance of MLP neural network is sensitive to feature scaling. Additionally, we expound upon and dissect the reasons behind the superior precision of the machine learning models when they are compared to the original study, which factors in time dependencies and spatial considerations. 
&lt;/p&gt;</description></item><item><title>TomOpt&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#20248;&#21270;&#23431;&#23449;&#23556;&#32447;&#956;&#23376;&#26029;&#23618;&#25195;&#25551;&#35774;&#35745;&#20013;&#30340;&#24494;&#31890;&#25506;&#27979;&#22120;&#30340;&#20960;&#20309;&#24067;&#23616;&#21644;&#35268;&#26684;&#12290;&#23427;&#21033;&#29992;&#21487;&#24494;&#20998;&#32534;&#31243;&#27169;&#25311;&#956;&#23376;&#19982;&#25506;&#27979;&#22120;&#21644;&#25195;&#25551;&#20307;&#31215;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#36890;&#36807;&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#20248;&#21270;&#24490;&#29615;&#36827;&#34892;&#25512;&#26029;&#24863;&#30693;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.14027</link><description>&lt;p&gt;
TomOpt&#65306;&#22312;&#23431;&#23449;&#23556;&#32447;&#956;&#23376;&#26029;&#23618;&#25195;&#25551;&#20013;&#38754;&#21521;&#20219;&#21153;&#21644;&#32422;&#26463;&#24863;&#30693;&#35774;&#35745;&#30340;&#24494;&#31890;&#25506;&#27979;&#22120;&#30340;&#24046;&#20998;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
TomOpt: Differential optimisation for task- and constraint-aware design of particle detectors in the context of muon tomography. (arXiv:2309.14027v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14027
&lt;/p&gt;
&lt;p&gt;
TomOpt&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#20248;&#21270;&#23431;&#23449;&#23556;&#32447;&#956;&#23376;&#26029;&#23618;&#25195;&#25551;&#35774;&#35745;&#20013;&#30340;&#24494;&#31890;&#25506;&#27979;&#22120;&#30340;&#20960;&#20309;&#24067;&#23616;&#21644;&#35268;&#26684;&#12290;&#23427;&#21033;&#29992;&#21487;&#24494;&#20998;&#32534;&#31243;&#27169;&#25311;&#956;&#23376;&#19982;&#25506;&#27979;&#22120;&#21644;&#25195;&#25551;&#20307;&#31215;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#36890;&#36807;&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#20248;&#21270;&#24490;&#29615;&#36827;&#34892;&#25512;&#26029;&#24863;&#30693;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#21517;&#20026;TomOpt&#30340;&#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#20248;&#21270;&#20960;&#20309;&#24067;&#23616;&#21644;&#25506;&#27979;&#22120;&#35268;&#26684;&#65292;&#20197;&#36827;&#34892;&#23431;&#23449;&#23556;&#32447;&#956;&#23376;&#30340;&#25955;&#23556;&#26029;&#23618;&#25195;&#25551;&#35774;&#35745;&#12290;&#35813;&#36719;&#20214;&#21033;&#29992;&#21487;&#24494;&#20998;&#32534;&#31243;&#26469;&#27169;&#25311;&#956;&#23376;&#19982;&#25506;&#27979;&#22120;&#21644;&#25195;&#25551;&#20307;&#31215;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25512;&#26029;&#20307;&#31215;&#23646;&#24615;&#65292;&#24182;&#36827;&#34892;&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#20248;&#21270;&#24490;&#29615;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#39318;&#27425;&#28436;&#31034;&#20102;&#31890;&#23376;&#29289;&#29702;&#20202;&#22120;&#30340;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#21644;&#25512;&#26029;&#24863;&#30693;&#20248;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#36719;&#20214;&#22312;&#30456;&#20851;&#22522;&#20934;&#22330;&#26223;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a software package, TomOpt, developed to optimise the geometrical layout and specifications of detectors designed for tomography by scattering of cosmic-ray muons. The software exploits differentiable programming for the modeling of muon interactions with detectors and scanned volumes, the inference of volume properties, and the optimisation cycle performing the loss minimisation. In doing so, we provide the first demonstration of end-to-end-differentiable and inference-aware optimisation of particle physics instruments. We study the performance of the software on a relevant benchmark scenarios and discuss its potential applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#29366;&#24577;&#36873;&#25321;&#26368;&#20248;&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#36817;&#20284;&#21644;&#20272;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#37325;&#26032;&#24179;&#34913;&#25104;&#26412;&#19979;&#20999;&#25442;&#25237;&#36164;&#32452;&#21512;&#27169;&#22411;&#26102;&#65292;&#20351;&#29992;&#23439;&#35266;&#32463;&#27982;&#20449;&#24687;&#30340;&#24615;&#33021;&#20248;&#20110;&#20107;&#21518;&#36873;&#25321;&#26368;&#20339;&#25237;&#36164;&#32452;&#21512;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.04754</link><description>&lt;p&gt;
&#21160;&#20316;&#29366;&#24577;&#30456;&#20851;&#30340;&#21160;&#24577;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Action-State Dependent Dynamic Model Selection. (arXiv:2307.04754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#29366;&#24577;&#36873;&#25321;&#26368;&#20248;&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#36817;&#20284;&#21644;&#20272;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#37325;&#26032;&#24179;&#34913;&#25104;&#26412;&#19979;&#20999;&#25442;&#25237;&#36164;&#32452;&#21512;&#27169;&#22411;&#26102;&#65292;&#20351;&#29992;&#23439;&#35266;&#32463;&#27982;&#20449;&#24687;&#30340;&#24615;&#33021;&#20248;&#20110;&#20107;&#21518;&#36873;&#25321;&#26368;&#20339;&#25237;&#36164;&#32452;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19990;&#30028;&#30340;&#26576;&#20123;&#29366;&#24577;&#19979;&#65292;&#22810;&#20010;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#21487;&#33021;&#21482;&#22312;&#20854;&#20013;&#26576;&#20123;&#29366;&#24577;&#19979;&#34920;&#29616;&#26368;&#20339;&#12290;&#32780;&#22312;&#27169;&#22411;&#20043;&#38388;&#30340;&#20999;&#25442;&#20063;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23547;&#25214;&#19968;&#31181;&#33021;&#22815;&#21160;&#24577;&#36873;&#25321;&#27169;&#22411;&#30340;&#36807;&#31243;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#22797;&#26434;&#30340;&#20272;&#35745;&#38382;&#39064;&#21644;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#12290;&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#20174;&#25968;&#25454;&#20013;&#36817;&#20284;&#21644;&#20272;&#35745;&#36825;&#20010;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#19968;&#33268;&#22320;&#20272;&#35745;&#20986;&#26681;&#25454;&#19968;&#32452;&#21327;&#21464;&#37327;&#36873;&#25321;&#19981;&#21516;&#27169;&#22411;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#20855;&#20307;&#24212;&#29992;&#26041;&#38754;&#65292;&#20363;&#22914;&#22312;&#37325;&#26032;&#24179;&#34913;&#25104;&#26412;&#19979;&#20999;&#25442;&#19981;&#21516;&#25237;&#36164;&#32452;&#21512;&#27169;&#22411;&#65292;&#20351;&#29992;&#23439;&#35266;&#32463;&#27982;&#20449;&#24687;&#36827;&#34892;&#20915;&#31574;&#12290;&#36890;&#36807;&#19968;&#32452;&#23439;&#35266;&#32463;&#27982;&#21464;&#37327;&#21644;&#20215;&#26684;&#25968;&#25454;&#65292;&#32463;&#39564;&#24212;&#29992;&#20110;&#19978;&#36848;&#25237;&#36164;&#32452;&#21512;&#38382;&#39064;&#34920;&#29616;&#20986;&#27604;&#20107;&#21518;&#36873;&#25321;&#26368;&#20339;&#25237;&#36164;&#32452;&#21512;&#27169;&#22411;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A model among many may only be best under certain states of the world. Switching from a model to another can also be costly. Finding a procedure to dynamically choose a model in these circumstances requires to solve a complex estimation procedure and a dynamic programming problem. A Reinforcement learning algorithm is used to approximate and estimate from the data the optimal solution to this dynamic programming problem. The algorithm is shown to consistently estimate the optimal policy that may choose different models based on a set of covariates. A typical example is the one of switching between different portfolio models under rebalancing costs, using macroeconomic information. Using a set of macroeconomic variables and price data, an empirical application to the aforementioned portfolio problem shows superior performance to choosing the best portfolio model with hindsight.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.06599</link><description>&lt;p&gt;
&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;(Variational Imbalanced Regression)
&lt;/p&gt;
&lt;p&gt;
Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#26102;&#65292;&#29616;&#26377;&#30340;&#22238;&#24402;&#27169;&#22411;&#24448;&#24448;&#22312;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#65292;&#23427;&#19981;&#20165;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#33258;&#28982;&#22320;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#19982;&#20856;&#22411;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20551;&#35774;I.I.D.&#34920;&#31034;&#65288;&#25968;&#25454;&#28857;&#30340;&#34920;&#31034;&#19981;&#30452;&#25509;&#21463;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;VIR&#20511;&#29992;&#20855;&#26377;&#31867;&#20284;&#22238;&#24402;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#35745;&#31639;&#28508;&#22312;&#34920;&#31034;&#30340;&#21464;&#20998;&#20998;&#24067;&#65307;&#27492;&#22806;&#65292;&#19981;&#21516;&#20110;&#20135;&#29983;&#28857;&#20272;&#35745;&#30340;&#30830;&#23450;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292; VIR&#39044;&#27979;&#25972;&#20010;&#27491;&#24577;&#21453;-&#20285;&#29595;&#20998;&#24067;&#24182;&#35843;&#33410;&#30456;&#20851;&#32852;&#30340;&#20849;&#36717;&#20998;&#24067;&#65292;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#26045;&#21152;&#27010;&#29575;&#37325;&#26032;&#21152;&#26435;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#40657;&#30418;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.19187</link><description>&lt;p&gt;
&#29983;&#25104;&#21487;&#20449;&#30340;&#25991;&#26412;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models. (arXiv:2305.19187v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#40657;&#30418;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#19987;&#38376;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#30740;&#31350;&#20063;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25991;&#29486;&#36890;&#24120;&#20551;&#23450;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#30333;&#30418;&#35775;&#38382;&#65292;&#36825;&#35201;&#20040;&#26159;&#30001;&#20110;&#26368;&#26032;&#30340;LLMs&#30340;&#23553;&#38381;&#28304;&#20195;&#30721;&#30340;&#24615;&#36136;&#65292;&#35201;&#20040;&#26159;&#30001;&#20110;&#35745;&#31639;&#38480;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#40657;&#30418;LLMs&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#21306;&#20998;&#20102;&#20004;&#31181;&#23494;&#20999;&#30456;&#20851;&#30340;&#27010;&#24565;: &#21482;&#19982;&#36755;&#20837;&#26377;&#20851;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#8221;&#21644;&#36824;&#19982;&#29983;&#25104;&#30340;&#22238;&#22797;&#26377;&#20851;&#30340;&#8220;&#32622;&#20449;&#24230;&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#20960;&#20010;&#32622;&#20449;&#24230;/&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#8220;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#8221;&#65292;&#20854;&#20013;&#19981;&#21487;&#38752;&#30340;&#32467;&#26524;&#21487;&#20197;&#34987;&#24573;&#30053;&#25110;&#32773;&#31227;&#20132;&#32473;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or due to computational constraints. In this work, we investigate uncertainty quantification in NLG for $\textit{black-box}$ LLMs. We first differentiate two closely-related notions: $\textit{uncertainty}$, which depends only on the input, and $\textit{confidence}$, which additionally depends on the generated response. We then propose and compare several confidence/uncertainty metrics, applying them to $\textit{selective NLG}$, where unreliable results could either be ignored or yielded for further 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#24378;&#20581;Lipschitz&#36172;&#24466;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#22312;&#24378;&#25932;&#25163;&#24773;&#20917;&#19979;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.18543</link><description>&lt;p&gt;
&#38024;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#20581;Lipschitz&#36172;&#24466;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Lipschitz Bandits to Adversarial Corruptions. (arXiv:2305.18543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#24378;&#20581;Lipschitz&#36172;&#24466;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#22312;&#24378;&#25932;&#25163;&#24773;&#20917;&#19979;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lipschitz&#36172;&#24466;&#31639;&#27861;&#26159;&#19968;&#31181;&#22788;&#29702;&#23450;&#20041;&#22312;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;&#36830;&#32493;&#33218;&#38598;&#30340;&#38543;&#26426;&#36172;&#24466;&#31639;&#27861;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#22870;&#21169;&#20989;&#25968;&#21463;&#21040;Lipschitz&#32422;&#26463;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;Lipschitz&#36172;&#24466;&#38382;&#39064;&#65292;&#21363;&#22312;&#23545;&#25239;&#24615;&#30772;&#22351;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#36866;&#24212;&#25932;&#25163;&#23558;&#38543;&#26426;&#22870;&#21169;&#25439;&#22351;&#21040;&#24635;&#39044;&#31639; $C$&#12290; &#39044;&#31639;&#36890;&#36807;&#26102;&#38388;&#36328;&#24230; $T$ &#20013;&#30340;&#30772;&#22351;&#27700;&#24179;&#20043;&#21644;&#26469;&#34913;&#37327;&#12290; &#25105;&#20204;&#32771;&#34385;&#24369;&#21644;&#24378;&#25932;&#25163;&#65292;&#20854;&#20013;&#24369;&#25932;&#25163;&#22312;&#25915;&#20987;&#20043;&#21069;&#19981;&#30693;&#36947;&#24403;&#21069;&#30340;&#34892;&#21160;&#65292;&#32780;&#24378;&#25932;&#25163;&#21487;&#20197;&#35266;&#23519;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#34892;&#24378;&#20581;Lipschitz&#36172;&#24466;&#31639;&#27861;&#65292;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#25932;&#25163;&#19979;&#65292;&#29978;&#33267;&#22312;&#25439;&#22351;&#24635;&#39044;&#31639; $C$ &#26410;&#21521;&#20195;&#29702;&#25259;&#38706;&#30340;&#24773;&#20917;&#19979;&#65292;&#22343;&#33021;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;&#25105;&#20204;&#22312;&#27599;&#31181;&#31867;&#22411;&#30340;&#25932;&#25163;&#19979;&#25552;&#20379;&#19979;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24378;&#31867;&#22411;&#19979;&#26159;&#26368;&#20248;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#20197;&#35828;&#26126;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lipschitz bandit is a variant of stochastic bandits that deals with a continuous arm set defined on a metric space, where the reward function is subject to a Lipschitz constraint. In this paper, we introduce a new problem of Lipschitz bandits in the presence of adversarial corruptions where an adaptive adversary corrupts the stochastic rewards up to a total budget $C$. The budget is measured by the sum of corruption levels across the time horizon $T$. We consider both weak and strong adversaries, where the weak adversary is unaware of the current action before the attack, while the strong one can observe it. Our work presents the first line of robust Lipschitz bandit algorithms that can achieve sub-linear regret under both types of adversary, even when the total budget of corruption $C$ is unrevealed to the agent. We provide a lower bound under each type of adversary, and show that our algorithm is optimal under the strong case. Finally, we conduct experiments to illustrate the effecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GFlowNets&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.17010</link><description>&lt;p&gt;
&#21033;&#29992;GFlowNets&#35299;&#20915;&#22270;&#24418;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Let the Flows Tell: Solving Graph Combinatorial Optimization Problems with GFlowNets. (arXiv:2305.17010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GFlowNets&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#26159;NP&#38590;&#39064;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#31934;&#30830;&#31639;&#27861;&#65292;&#36825;&#20351;&#23427;&#20204;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#29702;&#24819;&#39046;&#22495;&#12290;&#36825;&#20123;&#38382;&#39064;&#20013;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#38480;&#21046;&#21487;&#33021;&#20250;&#30452;&#25509;&#38459;&#30861;&#20248;&#21270;&#25110;&#37319;&#26679;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;GFlowNets&#26368;&#36817;&#34987;&#21457;&#29616;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#39034;&#24207;&#22320;&#20174;&#22797;&#21512;&#38750;&#35268;&#33539;&#21270;&#23494;&#24230;&#20013;&#26377;&#25928;&#22320;&#37319;&#26679;&#65292;&#24182;&#20855;&#26377;&#22312;CO&#20013;&#20998;&#25674;&#27492;&#31867;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#36807;&#31243;&#20197;&#21450;&#29983;&#25104;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#20505;&#36873;&#39033;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#25552;&#20986;&#35757;&#32451;&#26377;&#26465;&#20214;&#30340;GFlowNets&#20174;&#35299;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#31574;&#30053;&#12290;&#36824;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#25216;&#26415;&#26469;&#21463;&#30410;&#20110;&#36828;&#31243;&#20449;&#29992;&#20998;&#37197;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20351;&#29992;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#19981;&#21516;CO&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GFlowNet&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combinatorial optimization (CO) problems are often NP-hard and thus out of reach for exact algorithms, making them a tempting domain to apply machine learning methods. The highly structured constraints in these problems can hinder either optimization or sampling directly in the solution space. On the other hand, GFlowNets have recently emerged as a powerful machinery to efficiently sample from composite unnormalized densities sequentially and have the potential to amortize such solution-searching processes in CO, as well as generate diverse solution candidates. In this paper, we design Markov decision processes (MDPs) for different combinatorial problems and propose to train conditional GFlowNets to sample from the solution space. Efficient training techniques are also developed to benefit long-range credit assignment. Through extensive experiments on a variety of different CO tasks with synthetic and realistic data, we demonstrate that GFlowNet policies can efficiently find high-quali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26680;&#26041;&#27861;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#23545;&#22810;&#32452;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#21518;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#37117;&#26159;&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13202</link><description>&lt;p&gt;
&#26680;&#26041;&#27861;&#22312;&#31639;&#23376;&#23398;&#20064;&#20013;&#34920;&#29616;&#31454;&#20105;&#21147;
&lt;/p&gt;
&lt;p&gt;
Kernel Methods are Competitive for Operator Learning. (arXiv:2304.13202v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26680;&#26041;&#27861;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#23545;&#22810;&#32452;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#21518;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#37117;&#26159;&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26680;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#20808;&#39564;&#35823;&#24046;&#20998;&#26512;&#21644;&#19982;&#27969;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;&#22914;Deep Operator Net&#65288;DeepONet&#65289;[Lu et al.]&#21644;Fourier&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;[Li et al.]&#65289;&#30340;&#20840;&#38754;&#25968;&#23383;&#27604;&#36739;&#12290;&#25105;&#20204;&#32771;&#34385;&#30446;&#26631;&#31639;&#23376;$\mathcal{G}^\dagger:\mathcal{U}\to\mathcal{V}$&#30340;&#36755;&#20837;/&#36755;&#20986;&#31354;&#38388;&#26159;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#30340;&#24773;&#20917;&#65292;&#25968;&#25454;&#20197;&#36755;&#20837;/&#36755;&#20986;&#20989;&#25968;&#30340;&#37096;&#20998;&#35266;&#27979;$\varphi(v_i),\phi(u_i)$&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#20854;&#20013;$v_i=\mathcal{G}^\dagger(u_i)$&#65288;$i=1,\ldots,N$&#65289;&#65292;&#27979;&#37327;&#31639;&#23376;$\varphi:\mathcal{V}\to\mathbb{R}^m$&#21644;$\phi:\mathcal{U}\to\mathbb{R}^n$&#26159;&#32447;&#24615;&#30340;&#12290;&#22312;&#20889;$\psi:\mathbb{R}^n\to\mathcal{U}$&#21644;$\chi:\mathbb{R}^m\to\mathcal{V}$&#20316;&#20026;&#19982;$\phi$&#21644;$\varphi$&#30456;&#20851;&#30340;&#26368;&#20339;&#24674;&#22797;&#26144;&#23556;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;$\bar{f}$ &#26680;&#26144;&#23556; $L^2(\mathcal{U},\mathbb{R}^n)$ &#23450;&#20041;&#19968;&#20010;$k$ &#31867;&#22411;&#30340;&#26368;&#23567;&#20108;&#20056;&#27169;&#22411;&#65292; &#28982;&#21518;&#29992; $\bar{\mathcal{G}}=\chi\circ\bar{f}\circ\psi$ &#26469;&#36817;&#20284;$\mathcal{G}^\dagger$&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#28041;&#21450;&#22810;&#20010;&#20363;&#23376;&#65292;&#21253;&#25324;&#24120;&#35265;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31639;&#23376;&#36817;&#20284;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#26680;&#26041;&#27861;&#37117;&#26159;&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general kernel-based framework for learning operators between Banach spaces along with a priori error analysis and comprehensive numerical comparisons with popular neural net (NN) approaches such as Deep Operator Net (DeepONet) [Lu et al.] and Fourier Neural Operator (FNO) [Li et al.]. We consider the setting where the input/output spaces of target operator $\mathcal{G}^\dagger\,:\, \mathcal{U}\to \mathcal{V}$ are reproducing kernel Hilbert spaces (RKHS), the data comes in the form of partial observations $\phi(u_i), \varphi(v_i)$ of input/output functions $v_i=\mathcal{G}^\dagger(u_i)$ ($i=1,\ldots,N$), and the measurement operators $\phi\,:\, \mathcal{U}\to \mathbb{R}^n$ and $\varphi\,:\, \mathcal{V} \to \mathbb{R}^m$ are linear. Writing $\psi\,:\, \mathbb{R}^n \to \mathcal{U}$ and $\chi\,:\, \mathbb{R}^m \to \mathcal{V}$ for the optimal recovery maps associated with $\phi$ and $\varphi$, we approximate $\mathcal{G}^\dagger$ with $\bar{\mathcal{G}}=\chi \circ \bar{f} \ci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#22270;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22788;&#29702;&#20855;&#26377;&#20849;&#21516;&#39030;&#28857;&#38598;&#30340;&#22810;&#20010;&#22270;&#65292;&#26377;&#30528;&#38750;&#24120;&#29702;&#24819;&#30340;&#8220;&#21327;&#21516;&#25928;&#24212;&#8221;&#65292;&#21363;&#39030;&#28857;&#20998;&#31867;&#20934;&#30830;&#24230;&#24635;&#26159;&#21463;&#30410;&#20110;&#39069;&#22806;&#30340;&#22270;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#20854;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18051</link><description>&lt;p&gt;
&#22522;&#20110;&#32534;&#30721;&#22120;&#23884;&#20837;&#30340;&#21327;&#21516;&#22270;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Synergistic Graph Fusion via Encoder Embedding. (arXiv:2303.18051v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#22270;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22788;&#29702;&#20855;&#26377;&#20849;&#21516;&#39030;&#28857;&#38598;&#30340;&#22810;&#20010;&#22270;&#65292;&#26377;&#30528;&#38750;&#24120;&#29702;&#24819;&#30340;&#8220;&#21327;&#21516;&#25928;&#24212;&#8221;&#65292;&#21363;&#39030;&#28857;&#20998;&#31867;&#20934;&#30830;&#24230;&#24635;&#26159;&#21463;&#30410;&#20110;&#39069;&#22806;&#30340;&#22270;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#20854;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#34701;&#21512;&#32534;&#30721;&#22120;&#23884;&#20837;&#30340;&#22810;&#22270;&#23884;&#20837;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#22788;&#29702;&#20855;&#26377;&#20849;&#21516;&#39030;&#28857;&#38598;&#30340;&#22810;&#20010;&#22270;&#12290;&#22312;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#21497;&#20294;&#38750;&#24120;&#29702;&#24819;&#30340;&#8220;&#21327;&#21516;&#25928;&#24212;&#8221;&#65306;&#23545;&#20110;&#36275;&#22815;&#22823;&#30340;&#39030;&#28857;&#25968;&#65292;&#20998;&#31867;&#20934;&#30830;&#24230;&#24635;&#26159;&#21463;&#30410;&#20110;&#39069;&#22806;&#30340;&#22270;&#12290;&#25105;&#20204;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#19979;&#25552;&#20379;&#20102;&#36825;&#31181;&#25928;&#24212;&#30340;&#25968;&#23398;&#35777;&#26126;&#65292;&#24182;&#30830;&#23450;&#20102;&#28176;&#36817;&#23436;&#32654;&#20998;&#31867;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#20013;&#22987;&#32456;&#20248;&#20110;&#26368;&#36817;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel approach to multi-graph embedding called graph fusion encoder embedding. The method is designed to work with multiple graphs that share a common vertex set. Under the supervised learning setting, we show that the resulting embedding exhibits a surprising yet highly desirable "synergistic effect": for sufficiently large vertex size, the vertex classification accuracy always benefits from additional graphs. We provide a mathematical proof of this effect under the stochastic block model, and identify the necessary and sufficient condition for asymptotically perfect classification. The simulations and real data experiments confirm the superiority of the proposed method, which consistently outperforms recent benchmark methods in classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;</title><link>http://arxiv.org/abs/2303.16372</link><description>&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#30340;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#26102;&#31169;&#26377;&#23398;&#20064;&#31639;&#27861;&#30340;&#35821;&#20041;&#20445;&#35777;&#24378;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23548;&#20986;&#38750;&#28176;&#36827;&#37327;&#32423;&#19979;&#30028;&#26469;&#30740;&#31350;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21644;&#24230;&#37327;&#38544;&#31169;&#65288;mDP&#65289;&#30340;&#23398;&#20064;&#22120;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#23545;mDP&#30340;&#20998;&#26512;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#23545;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;DP-SGD&#21644;Projected Noisy SGD&#36827;&#34892;&#20102;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#25193;&#23637;&#38544;&#31169;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#36136;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#19979;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#31867;&#21035;$F$&#30456;&#27604;&#31867;&#21035;$G$&#26356;&#8220;&#31616;&#21333;&#8221;&#26102;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#22120;&#23545;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#22312;$\textbf{y}$&#30340;&#20559;&#31227;&#36828;&#23567;&#20110;$\textbf{x}$&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;ERM&#30340;&#34892;&#20026;&#19982;&#27491;&#20132;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13934</link><description>&lt;p&gt;
&#24322;&#36136;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#32479;&#35745;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Statistical Learning under Heterogenous Distribution Shift. (arXiv:2302.13934v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#36136;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#19979;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#31867;&#21035;$F$&#30456;&#27604;&#31867;&#21035;$G$&#26356;&#8220;&#31616;&#21333;&#8221;&#26102;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#22120;&#23545;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#22312;$\textbf{y}$&#30340;&#20559;&#31227;&#36828;&#23567;&#20110;$\textbf{x}$&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;ERM&#30340;&#34892;&#20026;&#19982;&#27491;&#20132;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#38543;&#26426;&#21464;&#37327;&#23545;$(\mathbf{x},\mathbf{y})$&#20013;&#39044;&#27979;&#30446;&#26631;$\mathbf{z}$, &#20854;&#20013;&#30495;&#23454;&#30340;&#39044;&#27979;&#22120;&#26159;&#21152;&#27861;&#30340;$\mathbb{E}[\mathbf{z} \mid \mathbf{x},\mathbf{y}] = f_\star(\mathbf{x}) +g_{\star}(\mathbf{y})$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32473;&#23450;&#35757;&#32451;&#20998;&#24067;&#19978;&#25311;&#21512;&#30340;&#20989;&#25968;$f+g$, $f \in F$&#21644;$g \in G$&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#22312;&#34920;&#29616;&#19978;&#30340;&#24046;&#24322;&#65292;&#20294;&#22312;&#27979;&#35797;&#20998;&#24067;&#19978;&#24471;&#21040;&#35780;&#20272;&#26102;&#20250;&#26174;&#31034;&#20986;&#21327;&#21464;&#37327;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#31867;&#21035;$F$&#27604;$G$&#26356;&#8220;&#31616;&#21333;&#8221;&#65288;&#20363;&#22914;&#65292;&#20197;&#24230;&#37327;&#29109;&#20026;&#34913;&#37327;&#26631;&#20934;&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#22120;&#23545;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#30340;&#25239;&#24178;&#25200;&#33021;&#21147;&#26356;&#24378;&#65292;&#20854;&#20013;$\textbf{y}$&#30340;&#20559;&#31227;&#35201;&#36828;&#23567;&#20110;$\textbf{x}$&#30340;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ERM&#30340;&#34892;&#20026;&#19982;&#27491;&#20132;&#26426;&#22120;&#23398;&#20064;$\textbf{ qualitatively similarly}$&#65306;ERM&#24674;&#22797;&#39044;&#27979;&#22120;&#20013;&#30340;$f$&#25104;&#20998;&#30340;&#36895;&#29575;&#20165;&#23545;&#20110;&#31867;&#21035;$G$&#30340;&#22797;&#26434;&#24615;&#20855;&#26377;&#36739;&#20302;&#38454;&#30340;&#20381;&#36182;&#24615;&#65292;&#35843;&#25972;&#21518;...
&lt;/p&gt;
&lt;p&gt;
This paper studies the prediction of a target $\mathbf{z}$ from a pair of random variables $(\mathbf{x},\mathbf{y})$, where the ground-truth predictor is additive $\mathbb{E}[\mathbf{z} \mid \mathbf{x},\mathbf{y}] = f_\star(\mathbf{x}) +g_{\star}(\mathbf{y})$. We study the performance of empirical risk minimization (ERM) over functions $f+g$, $f \in F$ and $g \in G$, fit on a given training distribution, but evaluated on a test distribution which exhibits covariate shift. We show that, when the class $F$ is "simpler" than $G$ (measured, e.g., in terms of its metric entropy), our predictor is more resilient to $\textbf{heterogenous covariate shifts}$ in which the shift in $\mathbf{x}$ is much greater than that in $\mathbf{y}$. Our analysis proceeds by demonstrating that ERM behaves $\textbf{qualitatively similarly to orthogonal machine learning}$: the rate at which ERM recovers the $f$-component of the predictor has only a lower-order dependence on the complexity of the class $G$, adjus
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDG&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#34920;&#31034;&#21644;&#29983;&#25104;&#22240;&#26524;&#35299;&#32544;&#32467;&#26524;&#12290;&#36890;&#36807;&#25506;&#32034;&#29305;&#23450;&#27169;&#22411;&#19979;&#23454;&#29616;CDG&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#22312;&#32534;&#30721;&#22120;&#20013;&#21152;&#20837;&#30417;&#30563;&#27491;&#21017;&#21270;&#26159;&#19981;&#22815;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#24230;&#37327;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#22240;&#26524;&#35299;&#32544;&#31243;&#24230;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.11737</link><description>&lt;p&gt;
&#22240;&#26524;&#35299;&#32544;&#30340;&#29983;&#25104;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Causally Disentangled Generative Variational AutoEncoder. (arXiv:2302.11737v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDG&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#34920;&#31034;&#21644;&#29983;&#25104;&#22240;&#26524;&#35299;&#32544;&#32467;&#26524;&#12290;&#36890;&#36807;&#25506;&#32034;&#29305;&#23450;&#27169;&#22411;&#19979;&#23454;&#29616;CDG&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#22312;&#32534;&#30721;&#22120;&#20013;&#21152;&#20837;&#30417;&#30563;&#27491;&#21017;&#21270;&#26159;&#19981;&#22815;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#24230;&#37327;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#22240;&#26524;&#35299;&#32544;&#31243;&#24230;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#20351;&#20854;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#34920;&#31034;&#21644;&#29983;&#25104;&#22240;&#26524;&#35299;&#32544;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#22240;&#26524;&#35299;&#32544;&#29983;&#25104;&#65288;CDG&#65289;&#12290;CDG&#26159;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#22240;&#26524;&#35299;&#32544;&#34920;&#31034;&#20934;&#30830;&#22320;&#35299;&#30721;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#20165;&#22312;&#32534;&#30721;&#22120;&#20013;&#21152;&#20837;&#30417;&#30563;&#27491;&#21017;&#21270;&#26159;&#26080;&#27861;&#23454;&#29616;&#20855;&#26377;CDG&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#65292;&#21363;&#20351;&#23545;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#20063;&#26159;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#29305;&#23450;&#27169;&#22411;&#20013;&#23454;&#29616;CDG&#25152;&#38656;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22240;&#26524;&#35299;&#32544;&#31243;&#24230;&#30340;&#36890;&#29992;&#24230;&#37327;&#12290;&#26469;&#33258;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#32467;&#26524;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new supervised learning technique for the Variational AutoEncoder (VAE) that allows it to learn a causally disentangled representation and generate causally disentangled outcomes simultaneously. We call this approach Causally Disentangled Generation (CDG). CDG is a generative model that accurately decodes an output based on a causally disentangled representation. Our research demonstrates that adding supervised regularization to the encoder alone is insufficient for achieving a generative model with CDG, even for a simple task. Therefore, we explore the necessary and sufficient conditions for achieving CDG within a specific model. Additionally, we introduce a universal metric for evaluating the causal disentanglement of a generative model. Empirical results from both image and tabular datasets support our findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#21270;&#30340;&#20302;&#31209;&#22810;&#20803;&#22238;&#24402;&#65292;&#36890;&#36807;&#37319;&#29992;&#22343;&#21248;&#37327;&#21270;&#19982;&#38543;&#26426;&#25238;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#32422;&#26463;Lasso&#21644;&#27491;&#21017;&#21270;Lasso&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#26368;&#20248;&#29575;&#30340;&#20272;&#35745;&#65292;&#21516;&#26102;&#37327;&#21270;&#20165;&#23545;&#20056;&#27861;&#22240;&#23376;&#30053;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.11197</link><description>&lt;p&gt;
&#37327;&#23376;&#21270;&#30340;&#20302;&#31209;&#22810;&#20803;&#22238;&#24402;&#19982;&#38543;&#26426;&#25238;&#21160;
&lt;/p&gt;
&lt;p&gt;
Quantized Low-Rank Multivariate Regression with Random Dithering. (arXiv:2302.11197v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#21270;&#30340;&#20302;&#31209;&#22810;&#20803;&#22238;&#24402;&#65292;&#36890;&#36807;&#37319;&#29992;&#22343;&#21248;&#37327;&#21270;&#19982;&#38543;&#26426;&#25238;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#32422;&#26463;Lasso&#21644;&#27491;&#21017;&#21270;Lasso&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#26368;&#20248;&#29575;&#30340;&#20272;&#35745;&#65292;&#21516;&#26102;&#37327;&#21270;&#20165;&#23545;&#20056;&#27861;&#22240;&#23376;&#30053;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#22810;&#20803;&#22238;&#24402;&#65288;LRMR&#65289;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#39640;&#24230;&#30456;&#20851;&#30340;&#20219;&#21153;&#20316;&#20026;&#20855;&#26377;&#20302;&#31209;&#20808;&#39564;&#30340;&#22810;&#21709;&#24212;&#22238;&#24402;&#38382;&#39064;&#36827;&#34892;&#32452;&#21512;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#21270;&#30340;LRMR&#65292;&#36825;&#26159;&#19968;&#31181;&#23454;&#38469;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#21709;&#24212;&#21644;/&#25110;&#21327;&#21464;&#37327;&#34987;&#31163;&#25955;&#21270;&#20026;&#26377;&#38480;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20272;&#35745;&#22522;&#30784;&#31995;&#25968;&#30697;&#38453;&#12290;&#20026;&#20102;&#20351;&#33021;&#22815;&#23454;&#29616;&#20219;&#24847;&#23567;&#35823;&#24046;&#30340;&#19968;&#33268;&#20272;&#35745;&#22120;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22343;&#21248;&#37327;&#21270;&#19982;&#38543;&#26426;&#25238;&#21160;&#65292;&#21363;&#22312;&#37327;&#21270;&#20043;&#21069;&#21521;&#25968;&#25454;&#28155;&#21152;&#36866;&#24403;&#30340;&#38543;&#26426;&#22122;&#22768;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21709;&#24212;&#20351;&#29992;&#22343;&#21248;&#25238;&#21160;&#65292;&#21327;&#21464;&#37327;&#20351;&#29992;&#19977;&#35282;&#25238;&#21160;&#12290;&#22522;&#20110;&#37327;&#21270;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;Lasso&#21644;&#27491;&#21017;&#21270;Lasso&#20272;&#35745;&#22120;&#65292;&#24182;&#25512;&#23548;&#20102;&#38750;&#28176;&#36817;&#24615;&#35823;&#24046;&#30028;&#12290;&#36890;&#36807;&#25238;&#21160;&#30340;&#24110;&#21161;&#65292;&#20272;&#35745;&#22120;&#23454;&#29616;&#20102;&#26368;&#23567;&#26368;&#20248;&#29575;&#65292;&#32780;&#37327;&#21270;&#20165;&#30053;&#24494;&#24694;&#21270;&#20102;&#20056;&#27861;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank multivariate regression (LRMR) is an important statistical learning model that combines highly correlated tasks as a multiresponse regression problem with low-rank priori on the coefficient matrix. In this paper, we study quantized LRMR, a practical setting where the responses and/or the covariates are discretized to finite precision. We focus on the estimation of the underlying coefficient matrix. To make consistent estimator that could achieve arbitrarily small error possible, we employ uniform quantization with random dithering, i.e., we add appropriate random noise to the data before quantization. Specifically, uniform dither and triangular dither are used for responses and covariates, respectively. Based on the quantized data, we propose the constrained Lasso and regularized Lasso estimators, and derive the non-asymptotic error bounds. With the aid of dithering, the estimators achieve minimax optimal rate, while quantization only slightly worsens the multiplicative factor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.04054</link><description>&lt;p&gt;
&#36861;&#27714;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#25512;&#29702;&#22797;&#29616;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#8212;&#8212;&#21363;&#22312;&#22797;&#21046;&#30340;&#27169;&#22411;&#35757;&#32451;&#36816;&#34892;&#20013;&#35266;&#23519;&#21040;&#30340;&#35780;&#20272;&#20998;&#25968;&#30340;&#19968;&#33268;&#24615;&#8212;&#8212;&#21463;&#21040;&#20960;&#31181;&#38750;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#27979;&#37327;&#22122;&#22768;&#12290;&#30446;&#21069;&#30340;&#36235;&#21183;&#26159;&#21435;&#38500;&#22122;&#22768;&#65292;&#20197;&#24378;&#21046;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#24573;&#30053;&#20102;&#23454;&#29616;&#23618;&#38754;&#22266;&#26377;&#30340;&#38750;&#30830;&#23450;&#24615;&#20197;&#21450;&#31639;&#27861;&#22122;&#22768;&#22240;&#32032;&#21644;&#25968;&#25454;&#29305;&#24615;&#20043;&#38388;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;&#36825;&#38480;&#21046;&#20102;&#20174;&#36825;&#20123;&#23454;&#39564;&#20013;&#21487;&#20197;&#24471;&#20986;&#30340;&#32467;&#35770;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23558;&#20960;&#20010;&#26041;&#24046;&#26469;&#28304;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982;&#25968;&#25454;&#29305;&#24615;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#26174;&#33879;&#24615;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#20013;&#65292;&#20197;&#26399;&#20174;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#23450;&#23454;&#20363;&#24471;&#20986;&#25512;&#29702;&#32467;&#35770;, &#32780;&#38750;&#21435;&#38500;&#22122;&#22768;&#12290;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#29992;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#24335;&#26469;&#32771;&#34385;&#31639;&#27861;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#28304;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#21508;&#20010;&#26041;&#24046;&#26469;&#28304;&#23545;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
&lt;/p&gt;</description></item><item><title>Simplex&#38543;&#26426;&#29305;&#24449;&#65288;SimRFs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#29305;&#24449;&#26426;&#21046;&#65292;&#36890;&#36807;&#20960;&#20309;&#30456;&#20851;&#24615;&#26469;&#26080;&#20559;&#20272;&#35745;softmax&#21644;&#39640;&#26031;&#26680;&#12290;&#22312;&#26435;&#37325;&#26080;&#20851;&#30340;&#20960;&#20309;&#30456;&#20851;&#27491;&#38543;&#26426;&#29305;&#24449;&#26426;&#21046;&#31867;&#20013;&#65292;SimRFs&#25552;&#20379;&#20102;&#26368;&#23567;&#21487;&#33021;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#39069;&#22806;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#26368;&#20934;&#30830;&#30340;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;SimRFs&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2301.13856</link><description>&lt;p&gt;
Simplex&#38543;&#26426;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Simplex Random Features. (arXiv:2301.13856v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13856
&lt;/p&gt;
&lt;p&gt;
Simplex&#38543;&#26426;&#29305;&#24449;&#65288;SimRFs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#29305;&#24449;&#26426;&#21046;&#65292;&#36890;&#36807;&#20960;&#20309;&#30456;&#20851;&#24615;&#26469;&#26080;&#20559;&#20272;&#35745;softmax&#21644;&#39640;&#26031;&#26680;&#12290;&#22312;&#26435;&#37325;&#26080;&#20851;&#30340;&#20960;&#20309;&#30456;&#20851;&#27491;&#38543;&#26426;&#29305;&#24449;&#26426;&#21046;&#31867;&#20013;&#65292;SimRFs&#25552;&#20379;&#20102;&#26368;&#23567;&#21487;&#33021;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#39069;&#22806;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#26368;&#20934;&#30830;&#30340;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;SimRFs&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Simplex&#38543;&#26426;&#29305;&#24449;&#65288;SimRFs&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#21521;&#37327;&#30340;&#20960;&#20309;&#30456;&#20851;&#24615;&#26469;&#26080;&#20559;&#20272;&#35745;softmax&#21644;&#39640;&#26031;&#26680;&#30340;&#26032;&#38543;&#26426;&#29305;&#24449;&#65288;RF&#65289;&#26426;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26080;&#20559;&#20272;&#35745;&#36825;&#20123;&#26680;&#30340;&#26435;&#37325;&#26080;&#20851;&#30340;&#20960;&#20309;&#30456;&#20851;&#27491;&#38543;&#26426;&#29305;&#24449;&#65288;PRF&#65289;&#26426;&#21046;&#31867;&#20013;&#65292;SimRFs&#25552;&#20379;&#20102;&#26368;&#23567;&#21487;&#33021;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#22312;&#27809;&#26377;&#35266;&#23519;&#21040;&#39069;&#22806;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#26368;&#20934;&#30830;&#30340;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#25104;&#26412;&#26356;&#39640;&#30340;SimRFs+&#21464;&#31181;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26356;&#24191;&#27867;&#30340;&#26435;&#37325;&#30456;&#20851;&#20960;&#20309;&#32806;&#21512;&#26041;&#26696;&#26063;&#20013;&#65288;&#20801;&#35768;&#38543;&#26426;&#21521;&#37327;&#26041;&#21521;&#21644;&#33539;&#25968;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65289;&#65292;&#23427;&#26159;&#28176;&#36817;&#20248;&#21270;&#30340;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SimRFs&#22312;&#21253;&#25324;&#36880;&#28857;&#26680;&#20272;&#35745;&#12289;&#38750;&#21442;&#25968;&#20998;&#31867;&#21644;&#21487;&#25193;&#23637;&#30340;Transformer&#20013;&#25552;&#20379;&#30340;&#19968;&#33268;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Simplex Random Features (SimRFs), a new random feature (RF) mechanism for unbiased approximation of the softmax and Gaussian kernels by geometrical correlation of random projection vectors. We prove that SimRFs provide the smallest possible mean square error (MSE) on unbiased estimates of these kernels among the class of weight-independent geometrically-coupled positive random feature (PRF) mechanisms, substantially outperforming the previously most accurate Orthogonal Random Features at no observable extra cost. We present a more computationally expensive SimRFs+ variant, which we prove is asymptotically optimal in the broader family of weight-dependent geometrical coupling schemes (which permit correlations between random vector directions and norms). In extensive empirical studies, we show consistent gains provided by SimRFs in settings including pointwise kernel estimation, nonparametric classification and scalable Transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#20984;&#20108;&#32858;&#31867;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#26080;&#38656;&#35843;&#21442;&#30340;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#38754;&#23545;&#37325;&#23614;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.03122</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#20984;&#20108;&#32858;&#31867;&#26041;&#27861;&#20013;&#20351;&#29992;&#20102;&#26080;&#38656;&#35843;&#21442;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust convex biclustering with a tuning-free method. (arXiv:2212.03122v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#20984;&#20108;&#32858;&#31867;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#26080;&#38656;&#35843;&#21442;&#30340;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#38754;&#23545;&#37325;&#23614;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#32858;&#31867;&#24191;&#27867;&#24212;&#29992;&#20110;&#22522;&#22240;&#20449;&#24687;&#20998;&#26512;&#12289;&#25991;&#26412;&#25366;&#25496;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#39046;&#22495;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#21457;&#29616;&#26679;&#26412;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#23616;&#37096;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20108;&#32858;&#31867;&#31639;&#27861;&#22312;&#38754;&#23545;&#37325;&#23614;&#25968;&#25454;&#26102;&#23481;&#26131;&#23849;&#28291;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#20984;&#20108;&#32858;&#31867;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;Huber&#25439;&#22833;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#26032;&#24341;&#20837;&#30340;&#40065;&#26834;&#21270;&#21442;&#25968;&#20250;&#22686;&#21152;&#36873;&#25321;&#26368;&#20339;&#21442;&#25968;&#30340;&#39069;&#22806;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#30340;&#40065;&#26834;&#21270;&#21442;&#25968;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#27169;&#25311;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38754;&#23545;&#37325;&#23614;&#22122;&#22768;&#26102;&#27604;&#20256;&#32479;&#30340;&#20108;&#32858;&#31867;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#30495;&#23454;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#31034;&#20363;&#12290;R&#21253;RcvxBiclustr&#21487;&#22312;https://github.com/YifanChen3/RcvxBiclustr &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biclustering is widely used in different kinds of fields including gene information analysis, text mining, and recommendation system by effectively discovering the local correlation between samples and features. However, many biclustering algorithms will collapse when facing heavy-tailed data. In this paper, we propose a robust version of convex biclustering algorithm with Huber loss. Yet, the newly introduced robustification parameter brings an extra burden to selecting the optimal parameters. Therefore, we propose a tuning-free method for automatically selecting the optimal robustification parameter with high efficiency. The simulation study demonstrates the more fabulous performance of our proposed method than traditional biclustering methods when encountering heavy-tailed noise. A real-life biomedical application is also presented. The R package RcvxBiclustr is available at https://github.com/YifanChen3/RcvxBiclustr.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FaiREE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2211.15072</link><description>&lt;p&gt;
FaiREE&#65306;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#20445;&#35777;&#30340;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FaiREE: Fair Classification with Finite-Sample and Distribution-Free Guarantee. (arXiv:2211.15072v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FaiREE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#21644;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20844;&#24179;&#20998;&#31867;&#26041;&#27861;&#30340;&#20844;&#24179;&#20445;&#35777;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#26679;&#26412;&#37327;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#36829;&#21453;&#20844;&#24179;&#24615;&#65292;&#32780;&#36825;&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FaiREE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#29702;&#35770;&#20445;&#35777;&#19979;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#12290;FaiREE&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#26426;&#20250;&#24179;&#31561;&#65292;&#24179;&#34913;&#20960;&#29575;&#65292;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#31561;&#65289;&#24182;&#23454;&#29616;&#26368;&#20339;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#29702;&#35770;&#20445;&#35777;&#36827;&#19968;&#27493;&#24471;&#21040;&#20102;&#23545;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#23454;&#39564;&#25903;&#25345;&#12290;FaiREE&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic fairness plays an increasingly critical role in machine learning research. Several group fairness notions and algorithms have been proposed. However, the fairness guarantee of existing fair classification methods mainly depends on specific data distributional assumptions, often requiring large sample sizes, and fairness could be violated when there is a modest number of samples, which is often the case in practice. In this paper, we propose FaiREE, a fair classification algorithm that can satisfy group fairness constraints with finite-sample and distribution-free theoretical guarantees. FaiREE can be adapted to satisfy various group fairness notions (e.g., Equality of Opportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimal accuracy. These theoretical guarantees are further supported by experiments on both synthetic and real data. FaiREE is shown to have favorable performance over state-of-the-art algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Softmax&#32452;&#20214;&#20013;&#24341;&#20837;&#26799;&#24230;&#34928;&#20943;&#36229;&#21442;&#25968;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#27867;&#21270;&#24615;&#33021;&#19982;&#26799;&#24230;&#34928;&#20943;&#29575;&#26174;&#33879;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#36739;&#23567;&#30340;&#26799;&#24230;&#34928;&#20943;&#30340;&#20248;&#21270;&#26041;&#27861;&#31867;&#20284;&#20110;&#35838;&#31243;&#23398;&#20064;&#24207;&#21015;&#65292;&#20351;&#24471;&#22256;&#38590;&#26679;&#26412;&#22312;&#26131;&#26679;&#26412;&#30830;&#20449;&#20043;&#21518;&#24471;&#21040;&#20851;&#27880;&#12290;&#22823;&#36793;&#38469;Softmax&#20250;&#24433;&#21709;&#23616;&#37096;Lipschitz&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2210.17145</link><description>&lt;p&gt;
&#22823;&#36793;&#38469;Softmax&#20013;&#30340;&#27010;&#29575;&#30456;&#20851;&#26799;&#24230;&#34928;&#20943;
&lt;/p&gt;
&lt;p&gt;
Probability-Dependent Gradient Decay in Large Margin Softmax. (arXiv:2210.17145v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Softmax&#32452;&#20214;&#20013;&#24341;&#20837;&#26799;&#24230;&#34928;&#20943;&#36229;&#21442;&#25968;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#27867;&#21270;&#24615;&#33021;&#19982;&#26799;&#24230;&#34928;&#20943;&#29575;&#26174;&#33879;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#36739;&#23567;&#30340;&#26799;&#24230;&#34928;&#20943;&#30340;&#20248;&#21270;&#26041;&#27861;&#31867;&#20284;&#20110;&#35838;&#31243;&#23398;&#20064;&#24207;&#21015;&#65292;&#20351;&#24471;&#22256;&#38590;&#26679;&#26412;&#22312;&#26131;&#26679;&#26412;&#30830;&#20449;&#20043;&#21518;&#24471;&#21040;&#20851;&#27880;&#12290;&#22823;&#36793;&#38469;Softmax&#20250;&#24433;&#21709;&#23616;&#37096;Lipschitz&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;Softmax&#24050;&#32463;&#25104;&#20026;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#24120;&#35265;&#30340;&#32452;&#20214;&#12290;&#26412;&#25991;&#22312;Softmax&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26799;&#24230;&#34928;&#20943;&#36229;&#21442;&#25968;&#65292;&#20197;&#25511;&#21046;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#27010;&#29575;&#30456;&#20851;&#26799;&#24230;&#34928;&#20943;&#29575;&#12290;&#36890;&#36807;&#23545;&#22522;&#20110;MNIST&#12289;CIFAR-10/100&#21644;SVHN&#30340;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#27867;&#21270;&#24615;&#33021;&#19982;&#26799;&#24230;&#34928;&#20943;&#29575;&#26174;&#33879;&#30456;&#20851;&#65292;&#21363;&#38543;&#30528;&#32622;&#20449;&#27010;&#29575;&#30340;&#19978;&#21319;&#65292;&#26799;&#24230;&#20250;&#21576;&#20984;&#20989;&#25968;&#25110;&#20985;&#20989;&#25968;&#36882;&#20943;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#36739;&#23567;&#30340;&#26799;&#24230;&#34928;&#20943;&#30340;&#20248;&#21270;&#26041;&#27861;&#31867;&#20284;&#20110;&#35838;&#31243;&#23398;&#20064;&#24207;&#21015;&#65292;&#21363;&#22312;&#26131;&#26679;&#26412;&#36275;&#22815;&#30830;&#20449;&#20043;&#21518;&#65292;&#25165;&#20250;&#20851;&#27880;&#22256;&#38590;&#26679;&#26412;&#65292;&#24182;&#19988;&#23545;&#20110;&#26679;&#26412;&#20043;&#38388;&#30340;&#31867;&#20869;&#36317;&#31163;&#36739;&#22823;&#30340;&#24773;&#20917;&#20250;&#33719;&#24471;&#26356;&#39640;&#30340;&#26799;&#24230;&#20197;&#20943;&#23567;&#36317;&#31163;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;&#35777;&#25454;&#35777;&#26126;&#22823;&#36793;&#38469;Softmax&#23558;&#24433;&#21709;&#23616;&#37096;Lipschitz&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, Softmax has become a common component in neural network frameworks. In this paper, a gradient decay hyperparameter is introduced in Softmax to control the probability-dependent gradient decay rate during training. By following the theoretical analysis and empirical results of a variety of model architectures trained on MNIST, CIFAR-10/100 and SVHN, we find that the generalization performance depends significantly on the gradient decay rate as the confidence probability rises, i.e., the gradient decreases convexly or concavely as the sample probability increases. Moreover, optimization with the small gradient decay shows a similar curriculum learning sequence where hard samples are in the spotlight only after easy samples are convinced sufficiently, and well-separated samples gain a higher gradient to reduce intra-class distance. Based on the analysis results, we can provide evidence that the large margin Softmax will affect the local Lipschitz constraint of the l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#28508;&#22312;&#34920;&#31034;&#30340;&#35889;&#20998;&#26512;&#21457;&#29616;&#65292;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#20250;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20559;&#21521;&#32534;&#30721;&#36739;&#20302;&#26377;&#25928;&#31209;&#30340;&#34920;&#31034;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#31209;&#27491;&#21017;&#21270;&#39044;&#35757;&#32451;&#26377;&#20559;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05248</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#20302;&#31209;&#27491;&#21017;&#21270;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-supervised debiasing using low rank regularization. (arXiv:2210.05248v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#28508;&#22312;&#34920;&#31034;&#30340;&#35889;&#20998;&#26512;&#21457;&#29616;&#65292;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#20250;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20559;&#21521;&#32534;&#30721;&#36739;&#20302;&#26377;&#25928;&#31209;&#30340;&#34920;&#31034;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#36890;&#36807;&#31209;&#27491;&#21017;&#21270;&#39044;&#35757;&#32451;&#26377;&#20559;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#30456;&#20851;&#24615;&#21487;&#33021;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24378;&#20559;&#35265;&#65292;&#24433;&#21709;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21435;&#20559;&#26041;&#27861;&#35201;&#27714;&#23545;&#34394;&#20551;&#23646;&#24615;&#25110;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#23436;&#20840;&#30417;&#30563;&#65292;&#20294;&#22914;&#20309;&#20165;&#36890;&#36807;&#26377;&#38480;&#30340;&#27880;&#37322;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#21435;&#20559;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#28508;&#22312;&#34920;&#31034;&#36827;&#34892;&#35889;&#20998;&#26512;&#30740;&#31350;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65306;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#20351;&#31070;&#32463;&#32593;&#32476;&#24402;&#32435;&#22320;&#20559;&#21521;&#32534;&#30721;&#36739;&#20302;&#26377;&#25928;&#31209;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31209;&#27491;&#21017;&#21270;&#21487;&#20197;&#25918;&#22823;&#36825;&#31181;&#20559;&#24046;&#65292;&#20197;&#40723;&#21169;&#39640;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#30340;&#21435;&#20559;&#26694;&#26550;&#65292;&#21487;&#33021;&#19982;&#26080;&#26631;&#31614;&#26679;&#26412;&#20860;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#31209;&#27491;&#21017;&#21270;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#39044;&#35757;&#32451;&#19968;&#20010;&#26377;&#20559;&#32534;&#30721;&#22120;&#65292;&#20316;&#20026;&#35821;&#20041;&#29942;&#39048;&#26469;&#24378;&#21046;&#32534;&#30721;&#22120;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spurious correlations can cause strong biases in deep neural networks, impairing generalization ability. While most existing debiasing methods require full supervision on either spurious attributes or target labels, training a debiased model from a limited amount of both annotations is still an open question. To address this issue, we investigate an interesting phenomenon using the spectral analysis of latent representations: spuriously correlated attributes make neural networks inductively biased towards encoding lower effective rank representations. We also show that a rank regularization can amplify this bias in a way that encourages highly correlated features. Leveraging these findings, we propose a self-supervised debiasing framework potentially compatible with unlabeled samples. Specifically, we first pretrain a biased encoder in a self-supervised manner with the rank regularization, serving as a semantic bottleneck to enforce the encoder to learn the spuriously correlated attrib
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;softmax&#28201;&#24230;&#21442;&#25968;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#23545;&#40784;&#25299;&#25169;&#35774;&#35745;&#12290;&#37319;&#29992;&#36825;&#31181;&#35774;&#35745;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38646;-shot&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.02127</link><description>&lt;p&gt;
&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#25299;&#25169;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Design of the topology for contrastive visual-textual alignment. (arXiv:2209.02127v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02127
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;softmax&#28201;&#24230;&#21442;&#25968;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#23545;&#40784;&#25299;&#25169;&#35774;&#35745;&#12290;&#37319;&#29992;&#36825;&#31181;&#35774;&#35745;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38646;-shot&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20313;&#24358;&#30456;&#20284;&#24230;&#26159;&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#23398;&#20064;&#20013;&#27979;&#37327;&#29305;&#24449;&#34920;&#31034;&#20043;&#38388;&#36317;&#31163;&#30340;&#24120;&#35265;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23398;&#20064;&#22823;&#35268;&#27169;&#22024;&#26434;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#38656;&#35201;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;softmax&#28201;&#24230;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;softmax&#28201;&#24230;&#22312;&#23884;&#20837;&#31354;&#38388;&#25299;&#25169;&#23646;&#24615;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;softmax&#28201;&#24230;&#26159;&#23545;&#22024;&#26434;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#20851;&#38190;&#26426;&#21046;&#12290;&#23427;&#20316;&#20026;&#36317;&#31163;&#33539;&#22260;&#30340;&#32553;&#25918;&#22240;&#23376;&#65288;&#20363;&#22914;&#65292;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#33539;&#22260;&#26159;[-1, 1]&#65289;&#65292;&#20854;&#23398;&#21040;&#30340;&#20540;&#34920;&#26126;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22122;&#38899;&#27700;&#24179;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#23545;&#40784;&#25299;&#25169;&#30340;&#26367;&#20195;&#35774;&#35745;&#12290;&#25105;&#20204;&#21033;&#29992;Transformer&#26550;&#26500;&#20013;&#30340;&#22810;&#20010;&#31867;&#26631;&#35760;&#65292;&#23558;&#29305;&#24449;&#34920;&#31034;&#26144;&#23556;&#21040;&#20855;&#26377;&#36127;&#20869;&#31215;&#20316;&#20026;&#36317;&#31163;&#20989;&#25968;&#30340;&#26012;&#38754;&#27969;&#24418;&#19978;&#12290;&#36890;&#36807;&#36825;&#31181;&#37197;&#32622;&#65292;&#25105;&#20204;&#22823;&#22823;&#25552;&#39640;&#20102;&#38646;-shot
&lt;/p&gt;
&lt;p&gt;
Cosine similarity is the common choice for measuring the distance between the feature representations in contrastive visual-textual alignment learning. However, empirically a learnable softmax temperature parameter is required when learning on large-scale noisy training data. In this work, we first discuss the role of softmax temperature from the embedding space's topological properties. We argue that the softmax temperature is the key mechanism for contrastive learning on noisy training data. It acts as a scaling factor of the distance range (e.g. [-1, 1] for the cosine similarity), and its learned value indicates the level of noise in the training data. Then, we propose an alternative design of the topology for the embedding alignment. We make use of multiple class tokens in the transformer architecture; then map the feature representations onto an oblique manifold endowed with the negative inner product as the distance function. With this configuration, we largely improve the zero-s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#26080;&#35770;&#39044;&#35757;&#32451;&#37319;&#29992;&#20309;&#31181;&#21327;&#35758;&#65292;&#32447;&#24615;&#39044;&#27979;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#21463;&#20854;&#22522;&#30784;&#34920;&#31034;&#40065;&#26834;&#24615;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25439;&#22833;&#19978;&#30028;&#21644;&#40065;&#26834;&#20998;&#31867;&#20934;&#21017;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.03835</link><description>&lt;p&gt;
&#20851;&#20110;&#20174;&#39044;&#35757;&#32451;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
On Transfer of Adversarial Robustness from Pretraining to Downstream Tasks. (arXiv:2208.03835v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#26080;&#35770;&#39044;&#35757;&#32451;&#37319;&#29992;&#20309;&#31181;&#21327;&#35758;&#65292;&#32447;&#24615;&#39044;&#27979;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#21463;&#20854;&#22522;&#30784;&#34920;&#31034;&#40065;&#26834;&#24615;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25439;&#22833;&#19978;&#30028;&#21644;&#40065;&#26834;&#20998;&#31867;&#20934;&#21017;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35757;&#32451;&#26041;&#26696;&#30340;&#27969;&#34892;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#34429;&#28982;&#23454;&#36341;&#20013;&#24050;&#32463;&#35777;&#26126;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#20174;&#39044;&#35757;&#32451;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#23646;&#24615;&#30340;&#36716;&#31227;&#20173;&#28982;&#19981;&#22815;&#29702;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32447;&#24615;&#39044;&#27979;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#21487;&#20197;&#30001;&#20854;&#22522;&#30784;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#38480;&#21046;&#65292;&#32780;&#19981;&#31649;&#39044;&#35757;&#32451;&#20351;&#29992;&#30340;&#21327;&#35758;&#22914;&#20309;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;(i)&#19968;&#20010;&#22312;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#20013;&#37117;&#25104;&#31435;&#30340;&#25439;&#22833;&#19978;&#30028;&#65292;&#20197;&#21450;(ii)&#29305;&#23450;&#20110;&#40065;&#26834;&#20998;&#31867;&#30340;&#20934;&#21017;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#22914;&#20309;&#29992;&#20110;&#26657;&#20934;&#19979;&#28216;&#40065;&#26834;&#24615;&#30340;&#26399;&#26395;&#65292;&#20197;&#21450;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#26368;&#20248;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#29992;&#36884;&#12290;&#32508;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#34920;&#24449;&#35201;&#27714;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large-scale training regimes have gained popularity, the use of pretrained models for downstream tasks has become common practice in machine learning. While pretraining has been shown to enhance the performance of models in practice, the transfer of robustness properties from pretraining to downstream tasks remains poorly understood. In this study, we demonstrate that the robustness of a linear predictor on downstream tasks can be constrained by the robustness of its underlying representation, regardless of the protocol used for pretraining. We prove (i) a bound on the loss that holds independent of any downstream task, as well as (ii) a criterion for robust classification in particular. We validate our theoretical results in practical applications, show how our results can be used for calibrating expectations of downstream robustness, and when our results are useful for optimal transfer learning. Taken together, our results offer an initial step towards characterizing the requireme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AEnbMIMOCQR&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#38598;&#25104;&#30340;&#26041;&#24335;&#65292;&#22312;&#19981;&#38656;&#35201;&#25968;&#25454;&#25286;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#20998;&#24067;&#26080;&#20851;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#27493;&#40077;&#22411;&#39044;&#27979;&#21306;&#38388;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#24322;&#26041;&#24046;&#24615;&#65292;&#24182;&#23545;&#20998;&#24067;&#36716;&#21464;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.14219</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27493;&#40077;&#22411;&#33258;&#36866;&#24212;&#24322;&#26041;&#24046;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A general framework for multi-step ahead adaptive conformal heteroscedastic time series forecasting. (arXiv:2207.14219v7 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AEnbMIMOCQR&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#38598;&#25104;&#30340;&#26041;&#24335;&#65292;&#22312;&#19981;&#38656;&#35201;&#25968;&#25454;&#25286;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#20998;&#24067;&#26080;&#20851;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#27493;&#40077;&#22411;&#39044;&#27979;&#21306;&#38388;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#24322;&#26041;&#24046;&#24615;&#65292;&#24182;&#23545;&#20998;&#24067;&#36716;&#21464;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#26080;&#20851;&#31639;&#27861;&#65292;&#21517;&#20026;&#33258;&#36866;&#24212;&#38598;&#25104;&#25209;&#37327;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#40077;&#22411;&#20998;&#20301;&#25968;&#22238;&#24402;&#65288;AEnbMIMOCQR&#65289;&#65292;&#20351;&#24471;&#39044;&#27979;&#32773;&#33021;&#22815;&#20197;&#20998;&#24067;&#26080;&#20851;&#30340;&#26041;&#24335;&#29983;&#25104;&#22266;&#23450;&#39044;&#35774;&#22833;&#37197;&#29575;&#30340;&#22810;&#27493;&#40077;&#22411;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#40077;&#22411;&#39044;&#27979;&#21407;&#29702;&#65292;&#20294;&#19981;&#38656;&#35201;&#25968;&#25454;&#25286;&#20998;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#25968;&#25454;&#19981;&#21487;&#20114;&#25442;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#25552;&#20379;&#25509;&#36817;&#31934;&#30830;&#30340;&#35206;&#30422;&#29575;&#12290;&#27492;&#22806;&#65292;&#25152;&#24471;&#21040;&#30340;&#39044;&#27979;&#21306;&#38388;&#22312;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#32463;&#39564;&#35777;&#26126;&#26377;&#25928;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#24322;&#26041;&#24046;&#24615;&#12290;AEnbMIMOCQR&#34987;&#35774;&#35745;&#25104;&#23545;&#20998;&#24067;&#36716;&#21464;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#20854;&#39044;&#27979;&#21306;&#38388;&#22312;&#26080;&#38480;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#25345;&#21487;&#38752;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#19981;&#20999;&#23454;&#38469;&#30340;&#20005;&#26684;&#20551;&#35774;&#12290;&#36890;&#36807;&#31995;&#32479;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#40077;&#22411;&#39044;&#27979;&#20013;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel model-agnostic algorithm called adaptive ensemble batch multi-input multi-output conformalized quantile regression (AEnbMIMOCQR} that enables forecasters to generate multi-step ahead prediction intervals for a fixed pre-specified miscoverage rate in a distribution-free manner. Our method is grounded on conformal prediction principles, however, it does not require data splitting and provides close to exact coverage even when the data is not exchangeable. Moreover, the resulting prediction intervals, besides being empirically valid along the forecast horizon, do not neglect heteroscedasticity. AEnbMIMOCQR is designed to be robust to distribution shifts, which means that its prediction intervals remain reliable over an unlimited period of time, without entailing retraining or imposing unrealistic strict assumptions on the data-generating process. Through methodically experimentation, we demonstrate that our approach outperforms other competitive methods on bo
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23558;&#20004;&#20010;&#38598;&#21512;&#36716;&#21270;&#20026;&#32447;&#24615;&#21487;&#20998;&#30340;&#38598;&#21512;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2205.11716</link><description>&lt;p&gt;
&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20351;&#25968;&#25454;&#32447;&#24615;&#21487;&#20998;
&lt;/p&gt;
&lt;p&gt;
Randomly Initialized One-Layer Neural Networks Make Data Linearly Separable. (arXiv:2205.11716v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11716
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23558;&#20004;&#20010;&#38598;&#21512;&#36716;&#21270;&#20026;&#32447;&#24615;&#21487;&#20998;&#30340;&#38598;&#21512;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#23558;&#20004;&#20010;&#20219;&#24847;&#38598;&#21512;&#26144;&#23556;&#20026;&#20004;&#20010;&#32447;&#24615;&#21487;&#20998;&#38598;&#21512;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#30456;&#27604;&#23436;&#20840;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#19978;&#30340;&#21560;&#24341;&#21147;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22312;&#20110;&#24314;&#31435;&#20102;&#22312;&#36275;&#22815;&#23485;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#33021;&#22815;&#23558;&#20004;&#20010;&#38598;&#21512;&#36716;&#21270;&#20026;&#32447;&#24615;&#21487;&#20998;&#30340;&#38598;&#21512;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#24517;&#35201;&#23485;&#24230;&#30340;&#31934;&#30830;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#21021;&#22987;&#30028;&#38480;&#22312;&#36755;&#20837;&#32500;&#24230;&#19978;&#21576;&#25351;&#25968;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#21442;&#25968;&#19978;&#21576;&#22810;&#39033;&#24335;&#20381;&#36182;&#20851;&#31995;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#30028;&#38480;&#19982;&#36755;&#20837;&#32500;&#24230;&#26080;&#20851;&#65292;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#32500;&#24230;&#28798;&#38590;&#12290;&#25105;&#20204;&#35777;&#26126;&#20013;&#20351;&#29992;&#30340;&#20027;&#35201;&#24037;&#20855;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20960;&#20309;&#21407;&#29702;&#21644;&#38543;&#26426;&#38598;&#20013;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural networks have demonstrated remarkable capabilities in mapping two arbitrary sets to two linearly separable sets. The prospect of achieving this with randomly initialized neural networks is particularly appealing due to the computational efficiency compared to fully trained networks. This paper contributes by establishing that, given sufficient width, a randomly initialized one-layer neural network can, with high probability, transform two sets into two linearly separable sets without any training. Moreover, we furnish precise bounds on the necessary width of the neural network for this phenomenon to occur. Our initial bound exhibits exponential dependence on the input dimension while maintaining polynomial dependence on all other parameters. In contrast, our second bound is independent of input dimension, effectively surmounting the curse of dimensionality. The main tools used in our proof heavily relies on a fusion of geometric principles and concentration of random m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23547;&#25214;&#31574;&#30053;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#23433;&#20840;&#21306;&#22495;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#20934;&#21017;&#36924;&#36817;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#20986;&#36867;&#36920;&#27010;&#29575;&#21644;&#23433;&#20840;&#21306;&#22495;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2202.11593</link><description>&lt;p&gt;
&#23547;&#25214;&#31574;&#30053;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#23433;&#20840;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Finding Safe Zones of policies Markov Decision Processes. (arXiv:2202.11593v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.11593
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23547;&#25214;&#31574;&#30053;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#23433;&#20840;&#21306;&#22495;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#20934;&#21017;&#36924;&#36817;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#20986;&#36867;&#36920;&#27010;&#29575;&#21644;&#23433;&#20840;&#21306;&#22495;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#23433;&#20840;&#21306;&#22495;&#65292;&#21363;&#29366;&#24577;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#22823;&#22810;&#25968;&#31574;&#30053;&#30340;&#36712;&#36857;&#37117;&#34987;&#38480;&#21046;&#22312;&#35813;&#23376;&#38598;&#20869;&#12290;&#23433;&#20840;&#21306;&#22495;&#30340;&#36136;&#37327;&#30001;&#29366;&#24577;&#25968;&#21644;&#36867;&#36920;&#27010;&#29575;&#21442;&#25968;&#21270;&#65292;&#21363;&#38543;&#26426;&#36712;&#36857;&#31163;&#24320;&#23376;&#38598;&#30340;&#27010;&#29575;&#12290;&#24403;&#23433;&#20840;&#21306;&#22495;&#20855;&#26377;&#23569;&#37327;&#30340;&#29366;&#24577;&#21644;&#36739;&#20302;&#30340;&#36867;&#36920;&#27010;&#29575;&#26102;&#65292;&#23588;&#20854;&#26377;&#36259;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23547;&#25214;&#26368;&#20248;&#23433;&#20840;&#21306;&#22495;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#33324;&#24773;&#20917;&#19979;&#35813;&#38382;&#39064;&#35745;&#31639;&#19978;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#20010;&#21452;&#20934;&#21017;&#36924;&#36817;&#23398;&#20064;&#31639;&#27861;&#65292;&#20934;&#30830;&#24230;&#36817;&#20284;&#20026;$2$&#20493;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#36867;&#36920;&#27010;&#29575;&#21644;&#23433;&#20840;&#21306;&#22495;&#22823;&#23567;&#65292;&#24182;&#19988;&#20351;&#29992;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a policy of a Markov Decision Process, we define a SafeZone as a subset of states, such that most of the policy's trajectories are confined to this subset. The quality of a SafeZone is parameterized by the number of states and the escape probability, i.e., the probability that a random trajectory will leave the subset. SafeZones are especially interesting when they have a small number of states and low escape probability. We study the complexity of finding optimal SafeZones, and show that in general, the problem is computationally hard. Our main result is a bi-criteria approximation learning algorithm with a factor of almost $2$ approximation for both the escape probability and SafeZone size, using a polynomial size sample complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20132;&#21449;&#39564;&#35777;&#20013;Cox&#27169;&#22411;&#27979;&#35797;&#35823;&#24046;&#30340;&#32622;&#20449;&#21306;&#38388;&#38382;&#39064;&#65292;&#21457;&#29616;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#20302;&#20272;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2201.10770</link><description>&lt;p&gt;
&#20132;&#21449;&#39564;&#35777;&#20013;Cox&#27169;&#22411;&#27979;&#35797;&#35823;&#24046;&#30340;&#32622;&#20449;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Confidence intervals for the Cox model test error from cross-validation. (arXiv:2201.10770v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.10770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20132;&#21449;&#39564;&#35777;&#20013;Cox&#27169;&#22411;&#27979;&#35797;&#35823;&#24046;&#30340;&#32622;&#20449;&#21306;&#38388;&#38382;&#39064;&#65292;&#21457;&#29616;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#20302;&#20272;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#39564;&#35777;&#26159;&#32479;&#35745;&#23398;&#20064;&#20013;&#29992;&#20110;&#20272;&#35745;&#27169;&#22411;&#27979;&#35797;&#35823;&#24046;&#30340;&#26368;&#24120;&#29992;&#25216;&#26415;&#20043;&#19968;&#65292;&#20294;&#20854;&#34892;&#20026;&#20173;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20132;&#21449;&#39564;&#35777;&#20272;&#35745;&#24471;&#21040;&#30340;&#26631;&#20934;&#27979;&#35797;&#35823;&#24046;&#30340;&#32622;&#20449;&#21306;&#38388;&#21487;&#33021;&#20302;&#20110;&#21517;&#20041;&#27700;&#24179;&#12290;&#36825;&#31181;&#29616;&#35937;&#26159;&#22240;&#20026;&#22312;&#20132;&#21449;&#39564;&#35777;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;&#26679;&#26412;&#37117;&#21516;&#26102;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#23548;&#33268;&#20272;&#35745;&#30340;&#35823;&#24046;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#22914;&#26524;&#19981;&#32771;&#34385;&#36825;&#31181;&#30456;&#20851;&#24615;&#65292;&#20272;&#35745;&#30340;&#26041;&#24046;&#20250;&#23567;&#20110;&#24212;&#26377;&#30340;&#20540;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26469;&#20272;&#35745;&#39044;&#27979;&#35823;&#24046;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20351;&#29992;&#26631;&#20934;&#20132;&#21449;&#39564;&#35777;&#24471;&#21040;&#30340;&#32622;&#20449;&#21306;&#38388;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;&#26412;&#30740;&#31350;&#23558;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#24605;&#24819;&#25512;&#24191;&#21040;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#35813;&#35774;&#32622;&#19979;&#30340;&#21508;&#31181;&#27979;&#35797;&#35823;&#24046;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-validation (CV) is one of the most widely used techniques in statistical learning for estimating the test error of a model, but its behavior is not yet fully understood. It has been shown that standard confidence intervals for test error using estimates from CV may have coverage below nominal levels. This phenomenon occurs because each sample is used in both the training and testing procedures during CV and as a result, the CV estimates of the errors become correlated. Without accounting for this correlation, the estimate of the variance is smaller than it should be. One way to mitigate this issue is by estimating the mean squared error of the prediction error instead using nested CV. This approach has been shown to achieve superior coverage compared to intervals derived from standard CV. In this work, we generalize the nested CV idea to the Cox proportional hazards model and explore various choices of test error for this setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#38544;&#34255;&#23618;&#23454;&#29616;&#23545;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65288;FDNN&#21644;FBNN&#65289;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#24471;&#21040;&#26356;&#21152;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2107.14151</link><description>&lt;p&gt;
&#29616;&#20195;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65306;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Modern Non-Linear Function-on-Function Regression. (arXiv:2107.14151v1 [stat.ME] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#38544;&#34255;&#23618;&#23454;&#29616;&#23545;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65288;FDNN&#21644;FBNN&#65289;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#24471;&#21040;&#26356;&#21152;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#31867;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#30001;&#36830;&#32493;&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#38544;&#34255;&#23618;&#65292;&#31216;&#20026;&#36830;&#32493;&#38544;&#34255;&#23618;&#65292;&#29992;&#20110;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65306;&#21151;&#33021;&#30452;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FDNN&#65289;&#21644;&#21151;&#33021;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65288;FBNN&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#26159;&#19987;&#38376;&#35774;&#35745;&#26469;&#21033;&#29992;&#21151;&#33021;&#25968;&#25454;&#22266;&#26377;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#21151;&#33021;&#39044;&#27979;&#21464;&#37327;&#21644;&#21151;&#33021;&#21709;&#24212;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#27714;&#35299;&#20989;&#25968;&#26799;&#24230;&#24182;&#23454;&#26045;&#27491;&#21017;&#21270;&#25216;&#26415;&#36827;&#34892;&#27169;&#22411;&#25311;&#21512;&#65292;&#24471;&#21040;&#26356;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#21151;&#33021;&#27169;&#22411;&#26041;&#38754;&#30340;&#24378;&#22823;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of non-linear function-on-function regression models for functional data using neural networks. We propose a framework using a hidden layer consisting of continuous neurons, called a continuous hidden layer, for functional response modeling and give two model fitting strategies, Functional Direct Neural Network (FDNN) and Functional Basis Neural Network (FBNN). Both are designed explicitly to exploit the structure inherent in functional data and capture the complex relations existing between the functional predictors and the functional response. We fit these models by deriving functional gradients and implement regularization techniques for more parsimonious results. We demonstrate the power and flexibility of our proposed method in handling complex functional models through extensive simulation studies as well as real data examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26410;&#26631;&#35760;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;UPCA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#25968;&#20960;&#20309;&#35777;&#26126;&#20102;&#20854;&#26159;&#19968;&#20010;&#33391;&#23450;&#20041;&#30340;&#20195;&#25968;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#27969;&#31243;&#26469;&#24212;&#23545;&#34987;&#32622;&#25442;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#26080;&#26631;&#35760;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2101.09446</link><description>&lt;p&gt;
&#26410;&#26631;&#35760;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#30697;&#38453;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Unlabeled Principal Component Analysis and Matrix Completion. (arXiv:2101.09446v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.09446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26410;&#26631;&#35760;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;UPCA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#25968;&#20960;&#20309;&#35777;&#26126;&#20102;&#20854;&#26159;&#19968;&#20010;&#33391;&#23450;&#20041;&#30340;&#20195;&#25968;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#27969;&#31243;&#26469;&#24212;&#23545;&#34987;&#32622;&#25442;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#26080;&#26631;&#35760;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26410;&#26631;&#35760;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;UPCA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#34987;&#32622;&#25442;&#25439;&#22351;&#30340;&#25968;&#25454;&#30697;&#38453;&#20013;&#25552;&#21462;&#40065;&#26834;&#30340;&#20027;&#25104;&#20998;&#12290;&#21033;&#29992;&#20195;&#25968;&#20960;&#20309;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;UPCA&#26159;&#19968;&#20010;&#33391;&#23450;&#20041;&#30340;&#20195;&#25968;&#38382;&#39064;&#65292;&#21363;&#19982;&#32473;&#23450;&#25968;&#25454;&#30456;&#19968;&#33268;&#30340;&#26368;&#23567;&#31209;&#30697;&#38453;&#21482;&#26377;&#20316;&#20026;&#22810;&#39033;&#24335;&#26041;&#31243;&#32452;&#30340;&#21807;&#19968;&#35299;&#30340;&#23454;&#38469;&#30697;&#38453;&#30340;&#34892;&#32622;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#23454;&#38469;&#30456;&#20851;&#24773;&#20917;&#30340;&#39640;&#25928;&#20004;&#38454;&#27573;&#31639;&#27861;&#27969;&#31243;&#29992;&#20110;UPCA&#65292;&#20854;&#20013;&#21482;&#26377;&#19968;&#37096;&#20998;&#25968;&#25454;&#34987;&#32622;&#25442;&#12290;&#38454;&#27573;I&#21033;&#29992;&#40065;&#26834;&#30340;&#24322;&#24120;&#20540;&#20027;&#25104;&#20998;&#20998;&#26512;&#26041;&#27861;&#26469;&#20272;&#35745;&#23454;&#38469;&#30697;&#38453;&#30340;&#21015;&#31354;&#38388;&#12290;&#22312;&#20855;&#22791;&#21015;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#38454;&#27573;II&#24212;&#29992;&#26368;&#36817;&#30340;&#26080;&#26631;&#35760;&#24863;&#30693;&#26041;&#27861;&#26469;&#24674;&#22797;&#34987;&#32622;&#25442;&#30340;&#25968;&#25454;&#12290;&#22312;UPCA&#30340;&#22522;&#30784;&#19978;&#20801;&#35768;&#20986;&#29616;&#32570;&#22833;&#26465;&#30446;&#21644;&#32622;&#25442;&#23548;&#33268;&#20102;&#26080;&#26631;&#35760;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#27492;&#25512;&#23548;&#20102;&#29702;&#35770;&#21644;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce robust principal component analysis from a data matrix in which the entries of its columns have been corrupted by permutations, termed Unlabeled Principal Component Analysis (UPCA). Using algebraic geometry, we establish that UPCA is a well-defined algebraic problem in the sense that the only matrices of minimal rank that agree with the given data are row-permutations of the ground-truth matrix, arising as the unique solutions of a polynomial system of equations. Further, we propose an efficient two-stage algorithmic pipeline for UPCA suitable for the practically relevant case where only a fraction of the data have been permuted. Stage-I employs outlier-robust PCA methods to estimate the ground-truth column-space. Equipped with the column-space, Stage-II applies recent methods for unlabeled sensing to restore the permuted data. Allowing for missing entries on top of permutations in UPCA leads to the problem of unlabeled matrix completion, for which we derive theory and alg
&lt;/p&gt;</description></item><item><title>CrossQ&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#36816;&#29992;&#25209;&#24402;&#19968;&#21270;&#21644;&#21024;&#38500;&#30446;&#26631;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#23454;&#26045;&#31616;&#21333;&#12290;</title><link>http://arxiv.org/abs/1902.05605</link><description>&lt;p&gt;
CrossQ: &#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;&#21644;&#31616;&#27905;&#24615;&#30340;&#25209;&#24402;&#19968;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity. (arXiv:1902.05605v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.05605
&lt;/p&gt;
&lt;p&gt;
CrossQ&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#36816;&#29992;&#25209;&#24402;&#19968;&#21270;&#21644;&#21024;&#38500;&#30446;&#26631;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#23454;&#26045;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26679;&#26412;&#25928;&#29575;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#31639;&#27861;&#65292;&#22914;REDQ&#21644;DroQ&#65292;&#36890;&#36807;&#23558;&#25209;&#27425;&#26631;&#20934;&#21270;&#30340;&#26356;&#26032;&#25968;&#25454;&#65288;UTD&#65289;&#27604;&#29575;&#22686;&#21152;&#21040;&#27599;&#20010;&#29615;&#22659;&#26679;&#26412;&#19978;&#30340;20&#20010;&#26799;&#24230;&#26356;&#26032;&#27493;&#39588;&#65292;&#25913;&#21892;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#20250;&#24102;&#26469;&#22823;&#24133;&#22686;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#35745;&#31639;&#36127;&#25285;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CrossQ&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#23427;&#24039;&#22937;&#22320;&#36816;&#29992;&#25209;&#24402;&#19968;&#21270;&#65292;&#24182;&#21435;&#38500;&#20102;&#30446;&#26631;&#32593;&#32476;&#65292;&#20197;&#22312;&#20445;&#25345;&#20302;UTD&#27604;&#29575;&#20026;1&#30340;&#21516;&#26102;&#36229;&#36234;&#30446;&#21069;&#30340;&#26368;&#26032;&#26679;&#26412;&#25928;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CrossQ&#19981;&#20381;&#36182;&#20110;&#24403;&#21069;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#39640;&#32423;&#20559;&#24046;&#32553;&#20943;&#26041;&#26696;&#12290;CrossQ&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#26368;&#20808;&#36827;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#65288;2&#65289;&#19982;REDQ&#21644;DroQ&#30456;&#27604;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#65288;3&#65289;&#23454;&#26045;&#31616;&#21333;&#65292;&#20165;&#38656;&#35201;&#22312;SAC&#20043;&#19978;&#28155;&#21152;&#20960;&#34892;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce Cross$Q$: a lightweight algorithm that makes careful use of Batch Normalization and removes target networks to surpass the state-of-the-art in sample efficiency while maintaining a low UTD ratio of $1$. Notably, Cross$Q$ does not rely on advanced bias-reduction schemes used in current methods. Cross$Q$'s contributions are thus threefold: (1) state-of-the-art sample efficiency, (2) substantial reduction in computational cost compared to REDQ and DroQ, and (3) ease of implementation, requiring just a few lines of code on top of SAC.
&lt;/p&gt;</description></item></channel></rss>