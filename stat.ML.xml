<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>SteinGen&#26159;&#19968;&#31181;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;Stein&#26041;&#27861;&#21644;MCMC&#21160;&#21147;&#23398;&#65292;&#36866;&#29992;&#20110;&#21482;&#26377;&#19968;&#27425;&#35266;&#23519;&#21040;&#30340;&#22270;&#24418;&#65292;&#36991;&#20813;&#20102;&#21442;&#25968;&#20272;&#35745;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.18578</link><description>&lt;p&gt;
SteinGen: &#29983;&#25104;&#24544;&#23454;&#21644;&#22810;&#26679;&#21270;&#30340;&#22270;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
SteinGen: Generating Fidelitous and Diverse Graph Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18578
&lt;/p&gt;
&lt;p&gt;
SteinGen&#26159;&#19968;&#31181;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;Stein&#26041;&#27861;&#21644;MCMC&#21160;&#21147;&#23398;&#65292;&#36866;&#29992;&#20110;&#21482;&#26377;&#19968;&#27425;&#35266;&#23519;&#21040;&#30340;&#22270;&#24418;&#65292;&#36991;&#20813;&#20102;&#21442;&#25968;&#20272;&#35745;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20445;&#30041;&#29305;&#24449;&#32467;&#26500;&#24182;&#20419;&#36827;&#26679;&#26412;&#22810;&#26679;&#24615;&#30340;&#22270;&#24418;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#22270;&#24418;&#35266;&#23519;&#25968;&#37327;&#36739;&#23569;&#26102;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20165;&#20174;&#19968;&#20010;&#35266;&#23519;&#21040;&#30340;&#22270;&#24418;&#29983;&#25104;&#22270;&#24418;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#22270;&#24418;&#30340;&#35774;&#32622;&#20013;&#20197;&#25351;&#25968;&#38543;&#26426;&#22270;&#24418;&#27169;&#22411;&#30340;&#24418;&#24335;&#34920;&#36798;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#29983;&#25104;&#36807;&#31243;SteinGen&#32467;&#21512;&#20102;Stein&#26041;&#27861;&#21644;&#22522;&#20110;MCMC&#30340;&#39532;&#23572;&#21487;&#22827;&#21160;&#21147;&#23398;&#30340;&#24605;&#24819;&#65292;&#35813;&#21160;&#21147;&#23398;&#22522;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;Stein&#31639;&#23376;&#12290;SteinGen&#20351;&#29992;&#19982;e&#30456;&#20851;&#32852;&#30340;Glauber&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18578v1 Announce Type: cross  Abstract: Generating graphs that preserve characteristic structures while promoting sample diversity can be challenging, especially when the number of graph observations is small. Here, we tackle the problem of graph generation from only one observed graph. The classical approach of graph generation from parametric models relies on the estimation of parameters, which can be inconsistent or expensive to compute due to intractable normalisation constants. Generative modelling based on machine learning techniques to generate high-quality graph samples avoids parameter estimation but usually requires abundant training samples. Our proposed generating procedure, SteinGen, which is phrased in the setting of graphs as realisations of exponential random graph models, combines ideas from Stein's method and MCMC by employing Markovian dynamics which are based on a Stein operator for the target model. SteinGen uses the Glauber dynamics associated with an e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20256;&#36798;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.15263</link><description>&lt;p&gt;
&#32852;&#37030;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65306;&#32479;&#35745;&#32858;&#21512;&#26041;&#27861;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20256;&#36798;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#31181;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#21644;&#20943;&#23569;&#19982;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#25972;&#21512;&#25110;&#34701;&#21512;&#20998;&#24067;&#24335;&#30830;&#23450;&#24615;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#65307;&#28982;&#32780;&#65292;&#29616;&#20195;&#30830;&#23450;&#24615;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#36890;&#24120;&#26657;&#20934;&#19981;&#20339;&#65292;&#32570;&#20047;&#22312;&#39044;&#27979;&#20013;&#20256;&#36798;&#19968;&#31181;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;&#36965;&#24863;&#24179;&#21488;&#21644;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#26159;&#29702;&#24819;&#30340;&#12290;&#30456;&#21453;&#65292;&#36125;&#21494;&#26031;DL&#27169;&#22411;&#36890;&#24120;&#26657;&#20934;&#33391;&#22909;&#65292;&#33021;&#22815;&#37327;&#21270;&#21644;&#20256;&#36798;&#19968;&#31181;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#20197;&#21450;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22240;&#20026;&#36125;&#21494;&#26031;DL&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#30001;&#27010;&#29575;&#20998;&#24067;&#23450;&#20041;&#65292;&#25152;&#20197;&#31616;&#21333;&#24212;&#29992;&#32858;&#21512;&#26041;&#27861;&#26159;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15263v1 Announce Type: new  Abstract: Federated learning (FL) is an approach to training machine learning models that takes advantage of multiple distributed datasets while maintaining data privacy and reducing communication costs associated with sharing local datasets. Aggregation strategies have been developed to pool or fuse the weights and biases of distributed deterministic models; however, modern deterministic deep learning (DL) models are often poorly calibrated and lack the ability to communicate a measure of epistemic uncertainty in prediction, which is desirable for remote sensing platforms and safety-critical applications. Conversely, Bayesian DL models are often well calibrated and capable of quantifying and communicating a measure of epistemic uncertainty along with a competitive prediction accuracy. Unfortunately, because the weights and biases in Bayesian DL models are defined by a probability distribution, simple application of the aggregation methods associa
&lt;/p&gt;</description></item><item><title>KCUSUM&#31639;&#27861;&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#25193;&#23637;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#23481;&#37327;&#25968;&#25454;&#24773;&#26223;&#19979;&#23454;&#26102;&#26816;&#27979;&#31361;&#21464;&#21464;&#21270;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#26356;&#28789;&#27963;&#22320;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.10291</link><description>&lt;p&gt;
&#20351;&#29992;KCUSUM&#31639;&#27861;&#35780;&#20272;&#23454;&#26102;&#33258;&#36866;&#24212;&#37319;&#26679;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Evaluation of Real-time Adaptive Sampling Change Point Detection Algorithm using KCUSUM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10291
&lt;/p&gt;
&lt;p&gt;
KCUSUM&#31639;&#27861;&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#25193;&#23637;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#23481;&#37327;&#25968;&#25454;&#24773;&#26223;&#19979;&#23454;&#26102;&#26816;&#27979;&#31361;&#21464;&#21464;&#21270;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#26356;&#28789;&#27963;&#22320;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31185;&#23398;&#27169;&#25311;&#25968;&#25454;&#27969;&#20013;&#23454;&#26102;&#26816;&#27979;&#31361;&#21464;&#21464;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#35201;&#27714;&#37096;&#32626;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#26680;&#30340;&#32047;&#31215;&#21644;&#65288;KCUSUM&#65289;&#31639;&#27861;&#65292;&#19968;&#31181;&#20256;&#32479;&#32047;&#31215;&#21644;&#65288;CUSUM&#65289;&#26041;&#27861;&#30340;&#38750;&#21442;&#25968;&#25193;&#23637;&#65292;&#20197;&#20854;&#22312;&#36739;&#23569;&#38480;&#21046;&#26465;&#20214;&#19979;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10291v1 Announce Type: new  Abstract: Detecting abrupt changes in real-time data streams from scientific simulations presents a challenging task, demanding the deployment of accurate and efficient algorithms. Identifying change points in live data stream involves continuous scrutiny of incoming observations for deviations in their statistical characteristics, particularly in high-volume data scenarios. Maintaining a balance between sudden change detection and minimizing false alarms is vital. Many existing algorithms for this purpose rely on known probability distributions, limiting their feasibility. In this study, we introduce the Kernel-based Cumulative Sum (KCUSUM) algorithm, a non-parametric extension of the traditional Cumulative Sum (CUSUM) method, which has gained prominence for its efficacy in online change point detection under less restrictive conditions. KCUSUM splits itself by comparing incoming samples directly with reference samples and computes a statistic gr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#26469;&#31283;&#23450;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#26469;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07087</link><description>&lt;p&gt;
&#33258;&#25105;&#32416;&#27491;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Self-Correcting Self-Consuming Loops for Generative Model Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#26469;&#31283;&#23450;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#26469;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#36136;&#37327;&#36234;&#26469;&#36234;&#39640;&#20197;&#21450;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#22312;&#20154;&#24037;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#23613;&#31649;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#34920;&#24449;&#23398;&#20064;&#30340;&#25104;&#21151;&#26696;&#20363;&#26377;&#24456;&#22810;&#65292;&#20294;&#26159;&#22312;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#20013;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#20250;&#20135;&#29983;"&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;"&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#29978;&#33267;&#23849;&#28291;&#65292;&#38500;&#38750;&#28385;&#36275;&#26576;&#20123;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#31283;&#23450;&#33258;&#25105;&#28040;&#32791;&#30340;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#23558;&#25968;&#25454;&#28857;&#26144;&#23556;&#20026;&#26356;&#26377;&#21487;&#33021;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#20351;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#30340;&#31283;&#23450;&#24615;&#21576;&#25351;&#25968;&#22686;&#21152;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#65292;&#23427;&#20381;&#36182;&#20110;&#19987;&#23478;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#32534;&#31243;&#22312;&#27169;&#25311;&#22120;&#20013;&#30340;&#29289;&#29702;&#23450;&#24459;&#65289;&#65292;&#24182;&#19988;&#26088;&#22312;&#33258;&#21160;&#19988;&#22823;&#35268;&#27169;&#22320;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#33258;&#25105;&#32416;&#27491;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#22312;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates "self-consuming loops" which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consum
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.04520</link><description>&lt;p&gt;
&#20851;&#20110;&#29616;&#20195;Hopfield&#27169;&#22411;&#35745;&#31639;&#38480;&#21046;&#30340;&#19968;&#20010;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04520
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;&#27169;&#24335;&#30340;&#33539;&#25968;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#25928;&#29575;&#36827;&#34892;&#30456;&#21464;&#34892;&#20026;&#30340;&#21051;&#30011;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#36755;&#20837;&#26597;&#35810;&#27169;&#24335;&#21644;&#35760;&#24518;&#27169;&#24335;&#30340;&#33539;&#25968;&#30340;&#19978;&#30028;&#26631;&#20934;&#12290;&#20165;&#22312;&#36825;&#20010;&#26631;&#20934;&#20043;&#19979;&#65292;&#20551;&#35774;&#28385;&#36275;Strong Exponential Time Hypothesis (SETH)&#65292;&#23384;&#22312;&#23376;&#20108;&#27425;&#65288;&#39640;&#25928;&#65289;&#21464;&#20307;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24403;&#26377;&#25928;&#26631;&#20934;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26377;&#25928;&#26500;&#36896;&#30340;&#27491;&#24335;&#31034;&#20363;&#12290;&#36825;&#21253;&#25324;&#19968;&#20010;&#35745;&#31639;&#26102;&#38388;&#30340;&#19979;&#30028;&#23548;&#20986;&#65292;&#19982;$\Max\{$&#23384;&#20648;&#30340;&#35760;&#24518;&#27169;&#24335;&#25968;&#37327;&#65292;&#36755;&#20837;&#26597;&#35810;&#24207;&#21015;&#30340;&#38271;&#24230;$\}$&#32447;&#24615;&#32553;&#25918;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#22270;&#20687;&#24674;&#22797;&#26694;&#26550;&#65292;&#31216;&#20026;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;SNORE&#65289;&#12290;&#35813;&#26694;&#26550;&#22312;&#24688;&#24403;&#22122;&#22768;&#27700;&#24179;&#30340;&#22270;&#20687;&#19978;&#24212;&#29992;&#21435;&#22122;&#22120;&#65292;&#24182;&#22522;&#20110;&#38543;&#26426;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#35299;&#20915;&#30149;&#24577;&#36870;&#38382;&#39064;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SNORE&#22312;&#21435;&#27169;&#31946;&#21644;&#20462;&#22797;&#20219;&#21153;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01779</link><description>&lt;p&gt;
&#24102;&#26377;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#30340;&#21363;&#25554;&#21363;&#29992;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play image restoration with Stochastic deNOising REgularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#22270;&#20687;&#24674;&#22797;&#26694;&#26550;&#65292;&#31216;&#20026;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;SNORE&#65289;&#12290;&#35813;&#26694;&#26550;&#22312;&#24688;&#24403;&#22122;&#22768;&#27700;&#24179;&#30340;&#22270;&#20687;&#19978;&#24212;&#29992;&#21435;&#22122;&#22120;&#65292;&#24182;&#22522;&#20110;&#38543;&#26426;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#35299;&#20915;&#30149;&#24577;&#36870;&#38382;&#39064;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SNORE&#22312;&#21435;&#27169;&#31946;&#21644;&#20462;&#22797;&#20219;&#21153;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#25554;&#21363;&#29992;&#65288;PnP&#65289;&#31639;&#27861;&#26159;&#19968;&#31867;&#36845;&#20195;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#29289;&#29702;&#27169;&#22411;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#22270;&#20687;&#21453;&#28436;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22270;&#20687;&#24674;&#22797;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#36234;&#26469;&#36234;&#23569;&#22122;&#38899;&#30340;&#22270;&#20687;&#19978;&#30340;&#19968;&#31181;&#38750;&#26631;&#20934;&#30340;&#21435;&#22122;&#22120;&#20351;&#29992;&#26041;&#27861;&#65292;&#36825;&#19982;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#30340;&#26368;&#26032;&#31639;&#27861;&#30456;&#30683;&#30462;&#65292;&#22312;&#36825;&#20123;&#31639;&#27861;&#20013;&#65292;&#21435;&#22122;&#22120;&#20165;&#24212;&#29992;&#20110;&#37325;&#26032;&#21152;&#22122;&#30340;&#22270;&#20687;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PnP&#26694;&#26550;&#65292;&#31216;&#20026;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;SNORE&#65289;&#65292;&#23427;&#20165;&#22312;&#22122;&#22768;&#27700;&#24179;&#36866;&#24403;&#30340;&#22270;&#20687;&#19978;&#24212;&#29992;&#21435;&#22122;&#22120;&#12290;&#23427;&#22522;&#20110;&#26174;&#24335;&#30340;&#38543;&#26426;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#31181;&#35299;&#20915;&#30149;&#24577;&#36870;&#38382;&#39064;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#31639;&#27861;&#21450;&#20854;&#36864;&#28779;&#25193;&#23637;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#22312;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;SNORE&#22312;&#21435;&#27169;&#31946;&#21644;&#20462;&#22797;&#20219;&#21153;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play (PnP) algorithms are a class of iterative algorithms that address image inverse problems by combining a physical model and a deep neural network for regularization. Even if they produce impressive image restoration results, these algorithms rely on a non-standard use of a denoiser on images that are less and less noisy along the iterations, which contrasts with recent algorithms based on Diffusion Models (DM), where the denoiser is applied only on re-noised images. We propose a new PnP framework, called Stochastic deNOising REgularization (SNORE), which applies the denoiser only on images with noise of the adequate level. It is based on an explicit stochastic regularization, which leads to a stochastic gradient descent algorithm to solve ill-posed inverse problems. A convergence analysis of this algorithm and its annealing extension is provided. Experimentally, we prove that SNORE is competitive with respect to state-of-the-art methods on deblurring and inpainting tasks, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#21521;&#24322;&#24615;&#25193;&#25955;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20302;&#32500;&#27969;&#24418;&#20013;&#30340;&#28508;&#22312;&#28436;&#21270;&#21160;&#24577;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#26089;&#26399;&#35686;&#25253;&#20449;&#21495;&#26469;&#26816;&#27979;&#22797;&#26434;&#31995;&#32479;&#25110;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21160;&#21147;&#23398;&#36716;&#21464;&#65292;&#24182;&#22312;&#30495;&#23454;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.03842</link><description>&lt;p&gt;
&#38544;&#24615;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#30340;&#36716;&#21521;&#39044;&#35686;
&lt;/p&gt;
&lt;p&gt;
Early warning via transitions in latent stochastic dynamical systems. (arXiv:2309.03842v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#21521;&#24322;&#24615;&#25193;&#25955;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20302;&#32500;&#27969;&#24418;&#20013;&#30340;&#28508;&#22312;&#28436;&#21270;&#21160;&#24577;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#26089;&#26399;&#35686;&#25253;&#20449;&#21495;&#26469;&#26816;&#27979;&#22797;&#26434;&#31995;&#32479;&#25110;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21160;&#21147;&#23398;&#36716;&#21464;&#65292;&#24182;&#22312;&#30495;&#23454;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22914;&#22522;&#22240;&#31361;&#21464;&#12289;&#33041;&#30142;&#30149;&#12289;&#33258;&#28982;&#28798;&#23475;&#12289;&#37329;&#34701;&#21361;&#26426;&#21644;&#24037;&#31243;&#21487;&#38752;&#24615;&#65292;&#23545;&#22797;&#26434;&#31995;&#32479;&#25110;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21160;&#21147;&#23398;&#36716;&#21464;&#36827;&#34892;&#26089;&#26399;&#35686;&#25253;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#26377;&#25928;&#25552;&#21462;&#26089;&#26399;&#35686;&#25253;&#20449;&#21495;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#23450;&#21521;&#24322;&#24615;&#25193;&#25955;&#22270;&#65292;&#23427;&#25429;&#25417;&#20102;&#20302;&#32500;&#27969;&#24418;&#20013;&#30340;&#28508;&#22312;&#28436;&#21270;&#21160;&#24577;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#25105;&#20204;&#25104;&#21151;&#25214;&#21040;&#20102;&#36866;&#24403;&#30340;&#26377;&#25928;&#22352;&#26631;&#65292;&#24182;&#25512;&#23548;&#20986;&#33021;&#22815;&#26816;&#27979;&#29366;&#24577;&#36716;&#21464;&#20013;&#20020;&#30028;&#28857;&#30340;&#26089;&#26399;&#35686;&#25253;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#28508;&#22312;&#21160;&#24577;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#32852;&#31995;&#36215;&#26469;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#23494;&#24230;&#21644;&#36716;&#21464;&#27010;&#29575;&#31561;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31532;&#20108;&#20010;&#22352;&#26631;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#20013;&#20445;&#25345;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early warnings for dynamical transitions in complex systems or high-dimensional observation data are essential in many real world applications, such as gene mutation, brain diseases, natural disasters, financial crises, and engineering reliability. To effectively extract early warning signals, we develop a novel approach: the directed anisotropic diffusion map that captures the latent evolutionary dynamics in low-dimensional manifold. Applying the methodology to authentic electroencephalogram (EEG) data, we successfully find the appropriate effective coordinates, and derive early warning signals capable of detecting the tipping point during the state transition. Our method bridges the latent dynamics with the original dataset. The framework is validated to be accurate and effective through numerical experiments, in terms of density and transition probability. It is shown that the second coordinate holds meaningful information for critical transition in various evaluation metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#30340;&#24213;&#23618;&#32553;&#25918;&#23450;&#24459;&#21644;&#26222;&#36866;&#32479;&#35745;&#32467;&#26500;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#31867;&#27604;&#20026;&#29289;&#29702;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#29305;&#24449;-&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20540;&#32479;&#35745;&#37327;&#30340;&#35268;&#24459;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26080;&#20851;&#38543;&#26426;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#38271;&#31243;&#30456;&#20851;&#24615;&#23436;&#20840;&#24674;&#22797;&#32553;&#25918;&#34892;&#20026;&#12290;&#21516;&#26102;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37117;&#23646;&#20110;&#28151;&#27788;&#31995;&#32479;&#65292;&#24182;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#21363;&#21487;&#20307;&#29616;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#32479;&#35745;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.14975</link><description>&lt;p&gt;
&#22797;&#26434;&#25968;&#25454;&#38598;&#30340;&#24213;&#23618;&#32553;&#25918;&#23450;&#24459;&#21644;&#26222;&#36866;&#32479;&#35745;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets. (arXiv:2306.14975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#30340;&#24213;&#23618;&#32553;&#25918;&#23450;&#24459;&#21644;&#26222;&#36866;&#32479;&#35745;&#32467;&#26500;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#31867;&#27604;&#20026;&#29289;&#29702;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#29305;&#24449;-&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20540;&#32479;&#35745;&#37327;&#30340;&#35268;&#24459;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26080;&#20851;&#38543;&#26426;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#38271;&#31243;&#30456;&#20851;&#24615;&#23436;&#20840;&#24674;&#22797;&#32553;&#25918;&#34892;&#20026;&#12290;&#21516;&#26102;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37117;&#23646;&#20110;&#28151;&#27788;&#31995;&#32479;&#65292;&#24182;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#21363;&#21487;&#20307;&#29616;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#32479;&#35745;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#25968;&#25454;&#38598;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#20013;&#37117;&#20986;&#29616;&#30340;&#26222;&#36941;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#31867;&#27604;&#20026;&#29289;&#29702;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#24037;&#20855;&#25581;&#31034;&#20854;&#24213;&#23618;&#32467;&#26500;&#12290;&#25105;&#20204;&#37325;&#28857;&#20998;&#26512;&#20102;&#29305;&#24449;-&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#20998;&#26512;&#20102;&#20854;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20540;&#32479;&#35745;&#37327;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#32467;&#26524;&#26159;&#65306;(i) &#22823;&#37096;&#20998;&#29305;&#24449;&#20540;&#21576;&#29616;&#30340;&#24130;&#24459;&#32553;&#25918;&#22312;&#26080;&#30456;&#20851;&#38543;&#26426;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;(ii) &#36890;&#36807;&#31616;&#21333;&#22320;&#24341;&#20837;&#38271;&#31243;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#23436;&#20840;&#24674;&#22797;&#36825;&#31181;&#32553;&#25918;&#34892;&#20026;&#21040;&#21512;&#25104;&#25968;&#25454;&#20013;&#65292;(iii) &#20174;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#35282;&#24230;&#30475;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#23646;&#20110;&#21516;&#19968;&#20010;&#26222;&#36866;&#24615;&#31867;&#21035;&#65292;&#37117;&#26159;&#28151;&#27788;&#31995;&#32479;&#32780;&#38750;&#21487;&#31215;&#31995;&#32479;&#65292;(iv) &#39044;&#26399;&#30340;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#32479;&#35745;&#34892;&#20026;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23601;&#24050;&#32463;&#22312;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20013;&#24471;&#21040;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study universal traits which emerge both in real-world complex datasets, as well as in artificially generated ones. Our approach is to analogize data to a physical system and employ tools from statistical physics and Random Matrix Theory (RMT) to reveal their underlying structure. We focus on the feature-feature covariance matrix, analyzing both its local and global eigenvalue statistics. Our main observations are: (i) The power-law scalings that the bulk of its eigenvalues exhibit are vastly different for uncorrelated random data compared to real-world data, (ii) this scaling behavior can be completely recovered by introducing long range correlations in a simple way to the synthetic data, (iii) both generated and real-world datasets lie in the same universality class from the RMT perspective, as chaotic rather than integrable systems, (iv) the expected RMT statistical behavior already manifests for empirical covariance matrices at dataset sizes significantly smaller than those conv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807; MACE &#31639;&#27861;&#65292;&#20197;&#38543;&#26426;&#26862;&#26519;&#21644;&#21463;&#38480;&#23725;&#22238;&#24402;&#20248;&#21270;&#32452;&#21512;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#31243;&#24230;&#30340;&#21487;&#39044;&#27979;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#27979;&#31639;&#27861;&#21644;&#39044;&#27979;&#22120;&#38598;&#65292;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.05568</link><description>&lt;p&gt;
&#26368;&#22823;&#26426;&#22120;&#23398;&#20064;&#32452;&#21512;&#30340;&#26500;&#24314;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Maximally Machine-Learnable Portfolios. (arXiv:2306.05568v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807; MACE &#31639;&#27861;&#65292;&#20197;&#38543;&#26426;&#26862;&#26519;&#21644;&#21463;&#38480;&#23725;&#22238;&#24402;&#20248;&#21270;&#32452;&#21512;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#31243;&#24230;&#30340;&#21487;&#39044;&#27979;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#27979;&#31639;&#27861;&#21644;&#39044;&#27979;&#22120;&#38598;&#65292;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32929;&#31080;&#22238;&#25253;&#65292;&#20219;&#20309;&#24418;&#24335;&#30340;&#21487;&#39044;&#27979;&#24615;&#37117;&#21487;&#20197;&#22686;&#24378;&#35843;&#25972;&#39118;&#38505;&#21518;&#30340;&#30408;&#21033;&#33021;&#21147;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20248;&#21270;&#32452;&#21512;&#26435;&#37325;&#65292;&#20197;&#20351;&#24471;&#21512;&#25104;&#35777;&#21048;&#26368;&#22823;&#31243;&#24230;&#30340;&#21487;&#39044;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MACE&#65292;Alternating Conditional Expectations&#30340;&#22810;&#20803;&#25193;&#23637;&#65292;&#36890;&#36807;&#22312;&#26041;&#31243;&#30340;&#19968;&#20391;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#21644;&#21463;&#38480;&#23725;&#22238;&#24402;&#22312;&#21478;&#19968;&#20391;&#23454;&#29616;&#20102;&#19978;&#36848;&#30446;&#26631;&#12290;&#30456;&#36739;&#20110;Lo&#21644;MacKinlay&#30340;&#26368;&#22823;&#21487;&#39044;&#27979;&#32452;&#21512;&#26041;&#27861;&#65292;&#26412;&#25991;&#26377;&#20004;&#20010;&#20851;&#38190;&#25913;&#36827;&#12290;&#31532;&#19968;&#65292;&#23427;&#36866;&#29992;&#20110;&#20219;&#20309;&#65288;&#38750;&#32447;&#24615;&#65289;&#39044;&#27979;&#31639;&#27861;&#21644;&#39044;&#27979;&#22120;&#38598;&#12290;&#31532;&#20108;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#32452;&#21512;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#26085;&#39057;&#21644;&#26376;&#39057;&#30340;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#22312;&#20351;&#29992;&#24456;&#23569;&#30340;&#26465;&#20214;&#20449;&#24687;&#26102;&#65292;&#21487;&#39044;&#27979;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#26174;&#33879;&#22686;&#21152;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21487;&#39044;&#27979;&#24615;&#22312;&#22909;&#26102;&#21644;&#22351;&#26102;&#37117;&#23384;&#22312;&#65292;&#24182;&#19988;MACE&#25104;&#21151;&#22320;&#23548;&#33322;&#20102;&#20004;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
When it comes to stock returns, any form of predictability can bolster risk-adjusted profitability. We develop a collaborative machine learning algorithm that optimizes portfolio weights so that the resulting synthetic security is maximally predictable. Precisely, we introduce MACE, a multivariate extension of Alternating Conditional Expectations that achieves the aforementioned goal by wielding a Random Forest on one side of the equation, and a constrained Ridge Regression on the other. There are two key improvements with respect to Lo and MacKinlay's original maximally predictable portfolio approach. First, it accommodates for any (nonlinear) forecasting algorithm and predictor set. Second, it handles large portfolios. We conduct exercises at the daily and monthly frequency and report significant increases in predictability and profitability using very little conditioning information. Interestingly, predictability is found in bad as well as good times, and MACE successfully navigates
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;DC&#35268;&#21010;&#31639;&#27861;&#26469;&#35299;&#20915;&#23376;&#27169;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#24615;&#36136;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#20840;&#38754;&#65292;&#21516;&#26102;&#22312;&#35821;&#38899;&#29305;&#24449;&#36873;&#25321;&#21644;&#25991;&#26723;&#25688;&#35201;&#31561;&#24212;&#29992;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11046</link><description>&lt;p&gt;
DC&#35268;&#21010;&#31639;&#27861;&#22312;&#23376;&#27169;&#26368;&#23567;&#21270;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Difference of Submodular Minimization via DC Programming. (arXiv:2305.11046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;DC&#35268;&#21010;&#31639;&#27861;&#26469;&#35299;&#20915;&#23376;&#27169;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#24615;&#36136;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#20840;&#38754;&#65292;&#21516;&#26102;&#22312;&#35821;&#38899;&#29305;&#24449;&#36873;&#25321;&#21644;&#25991;&#26723;&#25688;&#35201;&#31561;&#24212;&#29992;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#26368;&#23567;&#21270;&#20004;&#20010;&#23376;&#27169;&#65288;DS&#65289;&#20989;&#25968;&#30340;&#24046;&#24322;&#26159;&#19968;&#20010;&#33258;&#28982;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#20154;&#30693;&#36947;DS&#38382;&#39064;&#21487;&#20197;&#31561;&#20215;&#22320;&#36716;&#21270;&#20026;&#20004;&#20010;&#20984;&#65288;DC&#65289;&#20989;&#25968;&#30340;&#24046;&#24322;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#12290;&#23545;&#20110;DC&#38382;&#39064;&#65292;&#19968;&#20010;&#32463;&#20856;&#30340;&#31639;&#27861;&#21483;&#20570;DC&#31639;&#27861;&#65288;DCA&#65289;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;DCA&#21450;&#20854;&#23436;&#25972;&#24418;&#24335;&#65288;CDCA&#65289;&#30340;&#21464;&#20307;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23545;&#24212;&#20110;DS&#26368;&#23567;&#21270;&#30340;DC&#31243;&#24207;&#20013;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;DCA&#30340;&#29616;&#26377;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;DS&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#36136;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;DCA&#32467;&#26524;&#19982;&#29616;&#26377;&#30340;DS&#31639;&#27861;&#28385;&#36275;&#30456;&#21516;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#23436;&#25972;&#30340;&#25910;&#25947;&#24615;&#36136;&#25551;&#36848;&#12290;&#23545;&#20110;CDCA&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26356;&#24378;&#30340;&#23616;&#37096;&#26368;&#23567;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#25968;&#23383;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20004;&#20010;&#24212;&#29992;&#8212;&#8212;&#35821;&#38899;&#35821;&#26009;&#24211;&#36873;&#25321;&#29305;&#24449;&#20248;&#21270;&#21644;&#25991;&#26723;&#25688;&#35201;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimizing the difference of two submodular (DS) functions is a problem that naturally occurs in various machine learning problems. Although it is well known that a DS problem can be equivalently formulated as the minimization of the difference of two convex (DC) functions, existing algorithms do not fully exploit this connection. A classical algorithm for DC problems is called the DC algorithm (DCA). We introduce variants of DCA and its complete form (CDCA) that we apply to the DC program corresponding to DS minimization. We extend existing convergence properties of DCA, and connect them to convergence properties on the DS problem. Our results on DCA match the theoretical guarantees satisfied by existing DS algorithms, while providing a more complete characterization of convergence properties. In the case of CDCA, we obtain a stronger local minimality guarantee. Our numerical results show that our proposed algorithms outperform existing baselines on two applications: speech corpus sel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20998;&#24067;&#21452;&#20998;&#22359;&#28151;&#21512;&#25104;&#21592;&#27169;&#22411;&#65288;BiMMDF&#65289;&#65292;&#21487;&#29992;&#20110;&#37325;&#21472;&#21452;&#20998;&#22359;&#21152;&#26435;&#32593;&#32476;&#30340;&#31038;&#21306;&#21457;&#29616;&#65292;&#24182;&#21487;&#20197;&#27169;&#25311;&#37325;&#21472;&#21452;&#20998;&#22359;&#31526;&#21495;&#32593;&#32476;&#12290;&#35813;&#27169;&#22411;&#30340;&#20272;&#35745;&#20855;&#26377;&#19968;&#33268;&#24615;&#20445;&#35777;&#21644;&#29702;&#35770;&#20998;&#31163;&#26465;&#20214;&#65292;&#24182;&#21487;&#25552;&#39640;&#22312;&#21512;&#25104;&#32593;&#32476;&#21644;&#29616;&#23454;&#32593;&#32476;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.00912</link><description>&lt;p&gt;
&#26080;&#20998;&#24067;&#21452;&#20998;&#22359;&#28151;&#21512;&#25104;&#21592;&#27169;&#22411;&#65306;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#37325;&#21472;&#21452;&#20998;&#22359;&#21152;&#26435;&#32593;&#32476;&#31038;&#21306;&#21457;&#29616;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bipartite Mixed Membership Distribution-Free Model. A novel model for community detection in overlapping bipartite weighted networks. (arXiv:2211.00912v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00912
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20998;&#24067;&#21452;&#20998;&#22359;&#28151;&#21512;&#25104;&#21592;&#27169;&#22411;&#65288;BiMMDF&#65289;&#65292;&#21487;&#29992;&#20110;&#37325;&#21472;&#21452;&#20998;&#22359;&#21152;&#26435;&#32593;&#32476;&#30340;&#31038;&#21306;&#21457;&#29616;&#65292;&#24182;&#21487;&#20197;&#27169;&#25311;&#37325;&#21472;&#21452;&#20998;&#22359;&#31526;&#21495;&#32593;&#32476;&#12290;&#35813;&#27169;&#22411;&#30340;&#20272;&#35745;&#20855;&#26377;&#19968;&#33268;&#24615;&#20445;&#35777;&#21644;&#29702;&#35770;&#20998;&#31163;&#26465;&#20214;&#65292;&#24182;&#21487;&#25552;&#39640;&#22312;&#21512;&#25104;&#32593;&#32476;&#21644;&#29616;&#23454;&#32593;&#32476;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#37325;&#21472;&#21333;&#20998;&#22359;&#26080;&#26435;&#37325;&#32593;&#32476;&#30340;&#28151;&#21512;&#25104;&#21592;&#24314;&#27169;&#21644;&#20272;&#35745;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#27169;&#22411;&#36866;&#29992;&#20110;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#65292;&#21363;&#37325;&#21472;&#21452;&#20998;&#22359;&#21152;&#26435;&#32593;&#32476;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21363;&#26080;&#20998;&#24067;&#21452;&#20998;&#22359;&#28151;&#21512;&#25104;&#21592;&#27169;&#22411;&#65288;BiMMDF&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20801;&#35768;&#37051;&#25509;&#30697;&#38453;&#36981;&#24490;&#20219;&#20309;&#20998;&#24067;&#65292;&#21482;&#35201;&#20854;&#26399;&#26395;&#20855;&#26377;&#19982;&#33410;&#28857;&#25104;&#21592;&#26377;&#20851;&#30340;&#22359;&#32467;&#26500;&#21363;&#21487;&#12290;&#29305;&#21035;&#22320;&#65292;BiMMDF&#21487;&#20197;&#27169;&#25311;&#37325;&#21472;&#21452;&#20998;&#22359;&#31526;&#21495;&#32593;&#32476;&#65292;&#24182;&#19988;&#26159;&#35768;&#22810;&#20808;&#21069;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#21253;&#25324;&#27969;&#34892;&#30340;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#12290;&#25105;&#20204;&#24212;&#29992;&#20855;&#26377;&#19968;&#33268;&#20272;&#35745;&#29702;&#35770;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#26469;&#25311;&#21512;BiMMDF&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#19981;&#21516;&#20998;&#24067;&#30340;BiMMDF&#30340;&#20998;&#31163;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#31232;&#30095;&#32593;&#32476;&#30340;&#32570;&#22833;&#36793;&#32536;&#12290;BiMMDF&#30340;&#20248;&#21183;&#22312;&#24191;&#27867;&#30340;&#21512;&#25104;&#32593;&#32476;&#21644;&#29616;&#23454;&#32593;&#32476;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling and estimating mixed memberships for overlapping unipartite un-weighted networks has been well studied in recent years. However, to our knowledge, there is no model for a more general case, the overlapping bipartite weighted networks. To close this gap, we introduce a novel model, the Bipartite Mixed Membership Distribution-Free (BiMMDF) model. Our model allows an adjacency matrix to follow any distribution as long as its expectation has a block structure related to node membership. In particular, BiMMDF can model overlapping bipartite signed networks and it is an extension of many previous models, including the popular mixed membership stochastic blcokmodels. An efficient algorithm with a theoretical guarantee of consistent estimation is applied to fit BiMMDF. We then obtain the separation conditions of BiMMDF for different distributions. Furthermore, we also consider missing edges for sparse networks. The advantage of BiMMDF is demonstrated in extensive synthetic networks an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25104;&#21592;&#26080;&#20998;&#24067;&#27169;&#22411;&#65292;&#29992;&#20110;&#37325;&#21472;&#21152;&#26435;&#32593;&#32476;&#30340;&#31038;&#32676;&#26816;&#27979;&#65292;&#25903;&#25345;&#33410;&#28857;&#25152;&#23646;&#22810;&#20010;&#31038;&#32676;&#21644;&#26377;&#38480;&#23454;&#25968;&#26435;&#20540;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#20043;&#21069;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;&#20855;&#26377;&#28508;&#22312;&#31038;&#32676;&#32467;&#26500;&#30340;&#37325;&#21472;&#31526;&#21495;&#32593;&#32476;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#25928;&#35889;&#31639;&#27861;&#20272;&#35745;&#27169;&#22411;&#19979;&#30340;&#31038;&#32676;&#25104;&#21592;&#36164;&#26684;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#31946;&#21152;&#26435;&#27169;&#22359;&#24230;&#26469;&#35780;&#20272;&#37325;&#21472;&#21152;&#26435;&#32593;&#32476;&#30340;&#31038;&#32676;&#26816;&#27979;&#36136;&#37327;&#24182;&#30830;&#23450;&#21152;&#26435;&#32593;&#32476;&#31038;&#32676;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2112.04389</link><description>&lt;p&gt;
&#28151;&#21512;&#25104;&#21592;&#26080;&#20998;&#24067;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixed Membership Distribution-Free Model. (arXiv:2112.04389v4 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.04389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25104;&#21592;&#26080;&#20998;&#24067;&#27169;&#22411;&#65292;&#29992;&#20110;&#37325;&#21472;&#21152;&#26435;&#32593;&#32476;&#30340;&#31038;&#32676;&#26816;&#27979;&#65292;&#25903;&#25345;&#33410;&#28857;&#25152;&#23646;&#22810;&#20010;&#31038;&#32676;&#21644;&#26377;&#38480;&#23454;&#25968;&#26435;&#20540;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#20043;&#21069;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;&#20855;&#26377;&#28508;&#22312;&#31038;&#32676;&#32467;&#26500;&#30340;&#37325;&#21472;&#31526;&#21495;&#32593;&#32476;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#25928;&#35889;&#31639;&#27861;&#20272;&#35745;&#27169;&#22411;&#19979;&#30340;&#31038;&#32676;&#25104;&#21592;&#36164;&#26684;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#31946;&#21152;&#26435;&#27169;&#22359;&#24230;&#26469;&#35780;&#20272;&#37325;&#21472;&#21152;&#26435;&#32593;&#32476;&#30340;&#31038;&#32676;&#26816;&#27979;&#36136;&#37327;&#24182;&#30830;&#23450;&#21152;&#26435;&#32593;&#32476;&#31038;&#32676;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#20855;&#26377;&#37325;&#21472;&#21152;&#26435;&#32593;&#32476;&#20013;&#36827;&#34892;&#31038;&#32676;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#33410;&#28857;&#21487;&#20197;&#23646;&#20110;&#22810;&#20010;&#31038;&#32676;&#65292;&#36793;&#26435;&#21487;&#20197;&#26159;&#26377;&#38480;&#23454;&#25968;&#12290;&#20026;&#20102;&#23545;&#36825;&#26679;&#30340;&#22797;&#26434;&#32593;&#32476;&#36827;&#34892;&#24314;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#8212;&#8212;&#28151;&#21512;&#25104;&#21592;&#26080;&#20998;&#24067;&#65288;MMDF&#65289;&#27169;&#22411;&#12290;MMDF&#27809;&#26377;&#23545;&#36793;&#26435;&#30340;&#20998;&#24067;&#32422;&#26463;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20123;&#20808;&#21069;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21253;&#25324;&#33879;&#21517;&#30340;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#20855;&#26377;&#28508;&#22312;&#31038;&#32676;&#32467;&#26500;&#30340;&#37325;&#21472;&#31526;&#21495;&#32593;&#32476;&#20063;&#21487;&#20197;&#20174;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#25910;&#25947;&#29575;&#29702;&#35770;&#20445;&#35777;&#30340;&#39640;&#25928;&#35889;&#31639;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#19979;&#30340;&#31038;&#32676;&#25104;&#21592;&#36164;&#26684;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#27169;&#31946;&#21152;&#26435;&#27169;&#22359;&#24230;&#26469;&#35780;&#20272;&#20855;&#26377;&#27491;&#36127;&#36793;&#26435;&#30340;&#37325;&#21472;&#21152;&#26435;&#32593;&#32476;&#30340;&#31038;&#32676;&#26816;&#27979;&#36136;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#25105;&#20204;&#30340;fuzzy weighted modularity&#26469;&#30830;&#23450;&#21152;&#26435;&#32593;&#32476;&#31038;&#32676;&#25968;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of community detection in overlapping weighted networks, where nodes can belong to multiple communities and edge weights can be finite real numbers. To model such complex networks, we propose a general framework - the mixed membership distribution-free (MMDF) model. MMDF has no distribution constraints of edge weights and can be viewed as generalizations of some previous models, including the well-known mixed membership stochastic blockmodels. Especially, overlapping signed networks with latent community structures can also be generated from our model. We use an efficient spectral algorithm with a theoretical guarantee of convergence rate to estimate community memberships under the model. We also propose fuzzy weighted modularity to evaluate the quality of community detection for overlapping weighted networks with positive and negative edge weights. We then provide a method to determine the number of communities for weighted networks by taking advantage of our f
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#21098;&#24322;&#24120;&#20540;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21024;&#38500;&#26368;&#19981;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#28857;&#65292;&#28982;&#21518;&#29992;&#31526;&#21512;&#21442;&#32771;&#20998;&#24067;&#30340;&#23545;&#25968;&#20284;&#28982;&#24230;&#36827;&#34892;&#20462;&#21098;&#65292;&#20174;&#32780;&#22266;&#26377;&#20272;&#35745;&#24322;&#24120;&#20540;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/1907.01136</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#27169;&#22411;&#30340;&#32858;&#31867;&#20013;&#24322;&#24120;&#20540;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Finding Outliers in Gaussian Model-Based Clustering. (arXiv:1907.01136v4 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1907.01136
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#21098;&#24322;&#24120;&#20540;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21024;&#38500;&#26368;&#19981;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#28857;&#65292;&#28982;&#21518;&#29992;&#31526;&#21512;&#21442;&#32771;&#20998;&#24067;&#30340;&#23545;&#25968;&#20284;&#28982;&#24230;&#36827;&#34892;&#20462;&#21098;&#65292;&#20174;&#32780;&#22266;&#26377;&#20272;&#35745;&#24322;&#24120;&#20540;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#20998;&#31867;&#25110;&#32858;&#31867;&#24120;&#24120;&#21463;&#21040;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30417;&#30563;&#20998;&#31867;&#20013;&#22788;&#29702;&#24322;&#24120;&#20540;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#30446;&#21069;&#65292;&#24322;&#24120;&#20540;&#31639;&#27861;&#21487;&#20998;&#20026;&#20004;&#22823;&#31867;&#65306;&#24322;&#24120;&#28857;&#21253;&#21547;&#26041;&#27861;&#21644;&#20462;&#21098;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39044;&#20808;&#25351;&#23450;&#35201;&#21024;&#38500;&#30340;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#12290;&#26412;&#25991;&#21033;&#29992;&#26679;&#26412;&#39532;&#27663;&#36317;&#31163;&#30340;&#36125;&#22612;&#20998;&#24067;&#23548;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#20998;&#24067;&#65292;&#29992;&#20110;&#26377;&#38480;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#23376;&#38598;&#30340;&#23545;&#25968;&#20284;&#28982;&#24230;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21024;&#38500;&#26368;&#19981;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#28857;&#65292;&#21363;&#21028;&#23450;&#20026;&#24322;&#24120;&#20540;&#65292;&#30452;&#21040;&#23545;&#25968;&#20284;&#28982;&#24230;&#31526;&#21512;&#21442;&#32771;&#20998;&#24067;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#22266;&#26377;&#20272;&#35745;&#24322;&#24120;&#20540;&#25968;&#37327;&#30340;&#20462;&#21098;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised classification, or clustering, is a problem often plagued by outliers, yet there is a paucity of work on handling outliers in unsupervised classification. Outlier algorithms tend to fall into two broad categories: outlier inclusion methods and trimming methods, which often require pre-specification of the number of points to remove. The fact that sample Mahalanobis distance is beta-distributed is used to derive an approximate distribution for the log-likelihoods of subset finite Gaussian mixture models. An algorithm is proposed that removes the least likely points, which are deemed outliers, until the log-likelihoods adhere to the reference distribution. This results in a trimming method which inherently estimates the number of outliers present.
&lt;/p&gt;</description></item></channel></rss>