<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;</title><link>https://rss.arxiv.org/abs/2402.00957</link><description>&lt;p&gt;
&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Credal Learning Theory
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#20026;&#20174;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#20013;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#30340;&#39118;&#38505;&#25552;&#20379;&#29702;&#35770;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#21464;&#21270;&#65292;&#23548;&#33268;&#39046;&#22495;&#36866;&#24212;/&#27867;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#8220;&#20449;&#20219;&#8221;&#23398;&#20064;&#29702;&#35770;&#30340;&#22522;&#30784;&#65292;&#20351;&#29992;&#27010;&#29575;&#30340;&#20984;&#38598;&#65288;&#20449;&#20219;&#38598;&#65289;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26679;&#30340;&#20449;&#20219;&#38598;&#21487;&#20197;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#23545;&#20110;&#26377;&#38480;&#20551;&#35774;&#31354;&#38388;&#65288;&#26080;&#35770;&#26159;&#21542;&#21487;&#23454;&#29616;&#65289;&#21644;&#26080;&#38480;&#27169;&#22411;&#31354;&#38388;&#65292;&#25512;&#23548;&#20986;&#30028;&#38480;&#65292;&#36825;&#30452;&#25509;&#25512;&#24191;&#20102;&#32463;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learnt from a (single) training set, assumed to issue from an unknown probability distribution. In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues. In this paper we lay the foundations for a `credal' theory of learning, using convex sets of probabilities (credal sets) to model the variability in the data-generating distribution. Such credal sets, we argue, may be inferred from a finite sample of training sets. Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not) as well as infinite model spaces, which directly generalize classical results.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;</title><link>https://arxiv.org/abs/2403.16369</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21464;&#24615;&#23398;&#20064;&#22522;&#20110;&#21160;&#20316;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Action-based Representations Using Invariance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20351;&#29992;&#39640;&#32500;&#24230;&#35266;&#27979;&#24517;&#39035;&#33021;&#22815;&#22312;&#35768;&#22810;&#22806;&#28304;&#24615;&#24178;&#25200;&#20013;&#35782;&#21035;&#30456;&#20851;&#29366;&#24577;&#29305;&#24449;&#12290;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21487;&#25511;&#24615;&#30340;&#34920;&#31034;&#36890;&#36807;&#30830;&#23450;&#24433;&#21709;&#20195;&#29702;&#25511;&#21046;&#30340;&#22240;&#32032;&#26469;&#35782;&#21035;&#36825;&#20123;&#29366;&#24577;&#20803;&#32032;&#12290;&#34429;&#28982;&#35832;&#22914;&#36870;&#21160;&#21147;&#23398;&#21644;&#20114;&#20449;&#24687;&#31561;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#26377;&#38480;&#25968;&#37327;&#30340;&#26102;&#38388;&#27493;&#30340;&#21487;&#25511;&#24615;&#65292;&#20294;&#25429;&#33719;&#38271;&#26102;&#38388;&#20803;&#32032;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30701;&#35270;&#30340;&#21487;&#25511;&#24615;&#21487;&#20197;&#25429;&#25417;&#20195;&#29702;&#21363;&#23558;&#25758;&#21521;&#22681;&#22721;&#30340;&#30636;&#38388;&#65292;&#20294;&#19981;&#33021;&#22312;&#20195;&#29702;&#36824;&#26377;&#19968;&#23450;&#36317;&#31163;&#20043;&#26102;&#25429;&#25417;&#22681;&#22721;&#30340;&#25511;&#21046;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#27169;&#25311;&#19981;&#21464;&#37327;&#20551;&#24230;&#37327;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#23398;&#20064;&#20102;&#19968;&#20010;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16369v1 Announce Type: cross  Abstract: Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors. A representation that captures controllability identifies these state elements by determining what affects agent control. While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem. Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away. To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint. By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts dist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#30340;&#25910;&#32553;&#24615;&#36136;, &#24182;&#32473;&#20986;&#20102;&#36755;&#20986;&#20998;&#24067;&#19982;&#36755;&#20837;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#19978;&#30028;&#65292;&#36825;&#23545;&#20110;&#30028;&#23450;&#26497;&#23567;&#26497;&#22823;&#20272;&#35745;&#39118;&#38505;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>https://arxiv.org/abs/2210.13386</link><description>&lt;p&gt;
&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#30340;&#25910;&#32553;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Contraction of Locally Differentially Private Mechanisms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.13386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#30340;&#25910;&#32553;&#24615;&#36136;, &#24182;&#32473;&#20986;&#20102;&#36755;&#20986;&#20998;&#24067;&#19982;&#36755;&#20837;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#19978;&#30028;&#65292;&#36825;&#23545;&#20110;&#30028;&#23450;&#26497;&#23567;&#26497;&#22823;&#20272;&#35745;&#39118;&#38505;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#30340;&#25910;&#32553;&#24615;&#36136;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#36755;&#20837;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#36755;&#20986;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#19978;&#30028;&#65292;&#29992;&#20110;&#24230;&#37327;&#1013;-&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;K&#30340;&#36755;&#20986;&#20998;&#24067;PK&#21644;QK&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#36755;&#20837;&#20998;&#24067;P&#21644;Q&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#20027;&#35201;&#25216;&#26415;&#32467;&#26524;&#32473;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#967;^2-&#36317;&#31163;&#967;^2(PK}&#8741;QK)&#30340;&#23574;&#38160;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#19982;&#967;^2(P&#8741;Q)&#21644;&#1013;&#26377;&#20851;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#32467;&#26524;&#23545;&#19968;&#22823;&#31867;&#36317;&#31163;&#30340;&#19978;&#30028;&#25104;&#31435;&#65292;&#21253;&#25324;KL-&#36317;&#31163;&#21644;&#24179;&#26041;Hellinger&#36317;&#31163;&#12290;&#31532;&#20108;&#20010;&#20027;&#35201;&#25216;&#26415;&#32467;&#26524;&#32473;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#967;^2(PK&#8741;QK)&#30340;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#19982;&#24635;&#21464;&#24046;&#36317;&#31163;TV(P,Q)&#21644;&#1013;&#26377;&#20851;&#12290;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#19978;&#30028;&#24314;&#31435;&#20102;van Trees&#19981;&#31561;&#24335;&#12289;Le Cam&#27663;&#19981;&#31561;&#24335;&#12289;Assouad&#19981;&#31561;&#24335;&#21644;&#20114;&#20449;&#24687;&#26041;&#27861;&#30340;&#23616;&#37096;&#38544;&#31169;&#29256;&#26412;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#30028;&#23450;&#26497;&#23567;&#26497;&#22823;&#20272;&#35745;&#39118;&#38505;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the contraction properties of locally differentially private mechanisms. More specifically, we derive tight upper bounds on the divergence between $PK$ and $QK$ output distributions of an $\epsilon$-LDP mechanism $K$ in terms of a divergence between the corresponding input distributions $P$ and $Q$, respectively. Our first main technical result presents a sharp upper bound on the $\chi^2$-divergence $\chi^2(PK}\|QK)$ in terms of $\chi^2(P\|Q)$ and $\varepsilon$. We also show that the same result holds for a large family of divergences, including KL-divergence and squared Hellinger distance. The second main technical result gives an upper bound on $\chi^2(PK\|QK)$ in terms of total variation distance $\mathsf{TV}(P, Q)$ and $\epsilon$. We then utilize these bounds to establish locally private versions of the van Trees inequality, Le Cam's, Assouad's, and the mutual information methods, which are powerful tools for bounding minimax estimation risks. These results are shown
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20998;&#26512;&#30340;&#27169;&#22411;&#36827;&#34892;&#21051;&#30011;&#12290;</title><link>http://arxiv.org/abs/2401.08788</link><description>&lt;p&gt;
&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Differential Feature Under-reporting on Algorithmic Fairness. (arXiv:2401.08788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20998;&#26512;&#30340;&#27169;&#22411;&#36827;&#34892;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#37096;&#38376;&#30340;&#39044;&#27979;&#39118;&#38505;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#26356;&#23436;&#25972;&#30340;&#34892;&#25919;&#25968;&#25454;&#26469;&#24320;&#21457;&#65292;&#36825;&#20123;&#25968;&#25454;&#23545;&#20110;&#26356;&#22823;&#31243;&#24230;&#20381;&#36182;&#20844;&#20849;&#26381;&#21153;&#30340;&#20122;&#32676;&#20307;&#26356;&#20026;&#23436;&#25972;&#12290;&#20363;&#22914;&#65292;&#22312;&#32654;&#22269;&#65292;&#23545;&#20110;&#30001;&#21307;&#30103;&#34917;&#21161;&#21644;&#21307;&#30103;&#20445;&#38505;&#25903;&#25345;&#30340;&#20010;&#20154;&#65292;&#25919;&#24220;&#26426;&#26500;&#24120;&#24120;&#21487;&#20197;&#33719;&#24471;&#26377;&#20851;&#21307;&#30103;&#20445;&#20581;&#21033;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#23545;&#20110;&#31169;&#20154;&#20445;&#38505;&#30340;&#20154;&#21017;&#27809;&#26377;&#12290;&#23545;&#20844;&#20849;&#37096;&#38376;&#31639;&#27861;&#30340;&#25209;&#35780;&#25351;&#20986;&#65292;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#23548;&#33268;&#31639;&#27861;&#20915;&#31574;&#20013;&#30340;&#19981;&#20844;&#24179;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25968;&#25454;&#20559;&#35265;&#22312;&#25216;&#26415;&#35270;&#35282;&#19979;&#20173;&#28982;&#30740;&#31350;&#19981;&#36275;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#28155;&#21152;&#29305;&#24449;&#22122;&#22768;&#21644;&#26126;&#30830;&#26631;&#35760;&#20026;&#32570;&#22833;&#30340;&#29305;&#24449;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#32570;&#22833;&#25351;&#26631;&#30340;&#25968;&#25454;&#32570;&#22833;&#24773;&#20917;&#65288;&#21363;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#65289;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20998;&#26512;&#30340;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#29305;&#24449;&#26410;&#25253;&#21578;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive risk models in the public sector are commonly developed using administrative data that is more complete for subpopulations that more greatly rely on public services. In the United States, for instance, information on health care utilization is routinely available to government agencies for individuals supported by Medicaid and Medicare, but not for the privately insured. Critiques of public sector algorithms have identified such differential feature under-reporting as a driver of disparities in algorithmic decision-making. Yet this form of data bias remains understudied from a technical viewpoint. While prior work has examined the fairness impacts of additive feature noise and features that are clearly marked as missing, the setting of data missingness absent indicators (i.e. differential feature under-reporting) has been lacking in research attention. In this work, we present an analytically tractable model of differential feature under-reporting which we then use to charac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21435;&#27542;&#27665;&#21270;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#19977;&#20010;&#24314;&#35758;&#65306;&#25913;&#21464;&#22522;&#26412;&#36947;&#24503;&#21746;&#23398;&#20026;&#36798;&#23572;&#29595;&#21746;&#23398;&#65292;&#20801;&#35768;&#22810;&#20803;&#20027;&#20041;&#30340;&#35770;&#35777;&#20256;&#32479;&#23384;&#22312;&#20110;&#23545;&#40784;&#25216;&#26415;&#20013;&#65292;&#20197;&#21450;&#23558;&#20215;&#20540;&#35748;&#35782;&#35770;&#25193;&#23637;&#21040;&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2309.05030</link><description>&lt;p&gt;
&#21435;&#27542;&#27665;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#65306;&#23041;&#33394;&#36798;&#23572;&#29595;&#12289;&#35770;&#35777;&#21644;&#33402;&#26415;&#34920;&#36798;
&lt;/p&gt;
&lt;p&gt;
Decolonial AI Alignment: Vi\'{s}esadharma, Argument, and Artistic Expression. (arXiv:2309.05030v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21435;&#27542;&#27665;&#21270;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#19977;&#20010;&#24314;&#35758;&#65306;&#25913;&#21464;&#22522;&#26412;&#36947;&#24503;&#21746;&#23398;&#20026;&#36798;&#23572;&#29595;&#21746;&#23398;&#65292;&#20801;&#35768;&#22810;&#20803;&#20027;&#20041;&#30340;&#35770;&#35777;&#20256;&#32479;&#23384;&#22312;&#20110;&#23545;&#40784;&#25216;&#26415;&#20013;&#65292;&#20197;&#21450;&#23558;&#20215;&#20540;&#35748;&#35782;&#35770;&#25193;&#23637;&#21040;&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#38416;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#27542;&#27665;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#24456;&#23569;&#28041;&#21450;&#21040;&#23545;&#40784;&#65306;&#21363;&#22522;&#20110;&#32454;&#33268;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34892;&#20026;&#19982;&#26399;&#26395;&#20540;&#19968;&#33268;&#12290;&#38500;&#20102;&#20854;&#20182;&#23454;&#36341;&#65292;&#27542;&#27665;&#20027;&#20041;&#36824;&#26377;&#19968;&#37096;&#20998;&#26159;&#25913;&#21464;&#34987;&#27542;&#27665;&#27665;&#26063;&#30340;&#20449;&#20208;&#21644;&#20215;&#20540;&#35266;&#30340;&#21382;&#21490;&#65307;&#32780;&#24403;&#21069;&#30340;LLM&#23545;&#40784;&#23454;&#36341;&#27491;&#26159;&#36825;&#19968;&#21382;&#21490;&#30340;&#22797;&#21046;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#19977;&#20010;&#25552;&#35758;&#23545;AI&#23545;&#40784;&#36827;&#34892;&#21435;&#27542;&#27665;&#21270;&#65306;&#65288;a&#65289;&#23558;&#22522;&#26412;&#36947;&#24503;&#21746;&#23398;&#20174;&#35199;&#26041;&#21746;&#23398;&#36716;&#21464;&#20026;&#36798;&#23572;&#29595;&#21746;&#23398;&#65292;&#65288;b&#65289;&#22312;&#23545;&#40784;&#25216;&#26415;&#20013;&#20801;&#35768;&#35770;&#35777;&#21644;&#22810;&#20803;&#20027;&#20041;&#30340;&#20256;&#32479;&#65292;&#20197;&#21450;&#65288;c&#65289;&#23558;&#20215;&#20540;&#30340;&#35748;&#35782;&#35770;&#25193;&#23637;&#21040;&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#20196;&#25110;&#21629;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has explicated the coloniality of artificial intelligence (AI) development and deployment. One process that that work has not engaged with much is alignment: the tuning of large language model (LLM) behavior to be in line with desired values based on fine-grained human feedback. In addition to other practices, colonialism has a history of altering the beliefs and values of colonized peoples; this history is recapitulated in current LLM alignment practices. We suggest that AI alignment be decolonialized using three proposals: (a) changing the base moral philosophy from Western philosophy to dharma, (b) permitting traditions of argument and pluralism in alignment technologies, and (c) expanding the epistemology of values beyond instructions or commandments given in natural language.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#22312;&#20998;&#24067;&#23618;&#38754;&#19978;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#27745;&#26579;&#27169;&#22411;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#36125;&#21494;&#26031;&#39118;&#38505;&#30340;&#21464;&#21270;&#23637;&#31034;&#20102;&#36825;&#20123;&#27745;&#26579;&#23545;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#21644;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.08643</link><description>&lt;p&gt;
&#19968;&#20010;&#23398;&#20064;&#21463;&#21040;&#27745;&#26579;&#30340;&#36890;&#29992;&#26694;&#26550;&#65306;&#26631;&#31614;&#22122;&#22768;&#12289;&#23646;&#24615;&#22122;&#22768;&#31561;&#31561;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Learning under Corruption: Label Noise, Attribute Noise, and Beyond. (arXiv:2307.08643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#22312;&#20998;&#24067;&#23618;&#38754;&#19978;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#27745;&#26579;&#27169;&#22411;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#36125;&#21494;&#26031;&#39118;&#38505;&#30340;&#21464;&#21270;&#23637;&#31034;&#20102;&#36825;&#20123;&#27745;&#26579;&#23545;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#21644;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#27745;&#26579;&#29616;&#35937;&#24456;&#24120;&#35265;&#65292;&#24182;&#19988;&#24050;&#32463;&#22312;&#19981;&#21516;&#30340;&#27745;&#26579;&#27169;&#22411;&#19979;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#20102;&#35299;&#26377;&#38480;&#65292;&#32570;&#20047;&#23545;&#27745;&#26579;&#21450;&#20854;&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#26680;&#30340;&#19968;&#33324;&#24615;&#21644;&#35814;&#23613;&#30340;&#26694;&#26550;&#65292;&#22312;&#20998;&#24067;&#23618;&#38754;&#19978;&#27491;&#24335;&#20998;&#26512;&#20102;&#27745;&#26579;&#27169;&#22411;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#26631;&#31614;&#21644;&#23646;&#24615;&#19978;&#23384;&#22312;&#30340;&#22797;&#26434;&#32852;&#21512;&#21644;&#20381;&#36182;&#24615;&#27745;&#26579;&#65292;&#36825;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#24456;&#23569;&#35302;&#21450;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#36125;&#21494;&#26031;&#39118;&#38505;&#21464;&#21270;&#26469;&#23637;&#31034;&#36825;&#20123;&#27745;&#26579;&#22914;&#20309;&#24433;&#21709;&#26631;&#20934;&#30340;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#23545;&#20110;&#8220;&#26356;&#22797;&#26434;&#8221;&#27745;&#26579;&#23545;&#23398;&#20064;&#38382;&#39064;&#24433;&#21709;&#30340;&#23450;&#24615;&#27934;&#23519;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#23450;&#37327;&#27604;&#36739;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#35813;&#26694;&#26550;&#30340;&#24212;&#29992;&#21253;&#25324;&#27745;&#26579;&#26657;&#27491;&#23398;&#20064;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#23376;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Corruption is frequently observed in collected data and has been extensively studied in machine learning under different corruption models. Despite this, there remains a limited understanding of how these models relate such that a unified view of corruptions and their consequences on learning is still lacking. In this work, we formally analyze corruption models at the distribution level through a general, exhaustive framework based on Markov kernels. We highlight the existence of intricate joint and dependent corruptions on both labels and attributes, which are rarely touched by existing research. Further, we show how these corruptions affect standard supervised learning by analyzing the resulting changes in Bayes Risk. Our findings offer qualitative insights into the consequences of "more complex" corruptions on the learning problem, and provide a foundation for future quantitative comparisons. Applications of the framework include corruption-corrected learning, a subcase of which we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#29616;&#20302;&#36951;&#25022;&#29575;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.07465</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#24179;&#31283;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#40657;&#30418;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning. (arXiv:2306.07465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#29616;&#20302;&#36951;&#25022;&#29575;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#24179;&#31283;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23398;&#20064;&#22343;&#34913;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#21306;&#21035;&#20110;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#24102;&#26377;&#36172;&#24466;&#21453;&#39304;&#30340;&#28216;&#25103;&#65292;&#20854;&#20013;&#21363;&#20351;&#24453;&#27979;&#35797;&#30340;&#24046;&#36317;&#24456;&#23567;&#65292;&#27979;&#35797;&#19968;&#20010;&#22343;&#34913;&#20063;&#21487;&#33021;&#23548;&#33268;&#22823;&#37327;&#30340;&#36951;&#25022;&#65292;&#24182;&#19988;&#22312;&#38745;&#24577;&#28216;&#25103;&#20013;&#23384;&#22312;&#22810;&#20010;&#26368;&#20248;&#35299;&#65288;&#22343;&#34913;&#65289;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#38590;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#22914;&#19968;&#33324;&#21644;&#21338;&#24328;&#12289;&#28508;&#22312;&#21338;&#24328;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#21482;&#35201;&#22312;&#38745;&#24577;&#29615;&#22659;&#19979;&#37197;&#22791;&#36866;&#24403;&#30340;&#23398;&#20064;&#21644;&#27979;&#35797;&#31070;&#35861;&#12290;&#24403;&#38750;&#24179;&#31283;&#31243;&#24230;&#65288;&#36890;&#36807;&#24635;&#21464;&#21270;&#37327; $\Delta$ &#27979;&#37327;&#65289;&#24050;&#30693;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616; $\widetilde{O}\left(\Delta^{1/4}T^{3/4}\right)$ &#30340;&#36951;&#25022;&#65292;&#24403; $\Delta$ &#26410;&#30693;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616; $\widetilde{O}\left(\Delta^{1/5}T^{4/5}\right)$ &#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate learning the equilibria in non-stationary multi-agent systems and address the challenges that differentiate multi-agent learning from single-agent learning. Specifically, we focus on games with bandit feedback, where testing an equilibrium can result in substantial regret even when the gap to be tested is small, and the existence of multiple optimal solutions (equilibria) in stationary games poses extra challenges. To overcome these obstacles, we propose a versatile black-box approach applicable to a broad spectrum of problems, such as general-sum games, potential games, and Markov games, when equipped with appropriate learning and testing oracles for stationary environments. Our algorithms can achieve $\widetilde{O}\left(\Delta^{1/4}T^{3/4}\right)$ regret when the degree of nonstationarity, as measured by total variation $\Delta$, is known, and $\widetilde{O}\left(\Delta^{1/5}T^{4/5}\right)$ regret when $\Delta$ is unknown, where $T$ is the number of rounds. Meanwhile, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#22343;&#20540;&#20989;&#25968;&#20272;&#35745;&#65292;&#35777;&#26126;&#20102;&#31232;&#30095;&#24809;&#32602;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#22312;&#35768;&#22810;&#38750;&#32447;&#24615;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#36798;&#21040;&#20102;&#26497;&#23567;&#19979;&#30028;&#30340;&#26368;&#20248;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.02546</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Adaptive deep learning for nonlinear time series models. (arXiv:2207.02546v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#22343;&#20540;&#20989;&#25968;&#20272;&#35745;&#65292;&#35777;&#26126;&#20102;&#31232;&#30095;&#24809;&#32602;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#22312;&#35768;&#22810;&#38750;&#32447;&#24615;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#36798;&#21040;&#20102;&#26497;&#23567;&#19979;&#30028;&#30340;&#26368;&#20248;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#21442;&#25968;&#33258;&#36866;&#24212;&#20272;&#35745;&#29702;&#35770;&#65292;&#29992;&#20110;&#38750;&#24179;&#31283;&#21644;&#38750;&#32447;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#22343;&#20540;&#20989;&#25968;&#20272;&#35745;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#65292;&#21363;&#38750;&#24809;&#32602;&#21644;&#31232;&#30095;&#24809;&#32602;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#20026;&#19968;&#33324;&#30340;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#24314;&#31435;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20272;&#35745;&#23646;&#20110;&#24191;&#27867;&#31867;&#21035;&#30340;&#38750;&#32447;&#24615;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;&#21253;&#25324;&#38750;&#32447;&#24615;&#24191;&#20041;&#21487;&#21152;&#33258;&#22238;&#24402;&#12289;&#21333;&#25351;&#25968;&#21644;&#38408;&#20540;&#33258;&#22238;&#24402;&#27169;&#22411;&#65289;&#22343;&#20540;&#20989;&#25968;&#30340;&#26497;&#23567;&#19979;&#30028;&#12290;&#22312;&#36825;&#20123;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31232;&#30095;&#24809;&#32602;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#22312;&#35768;&#22810;&#38750;&#32447;&#24615;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#24182;&#36798;&#21040;&#26497;&#23567;&#19979;&#30028;&#30340;&#26368;&#20248;&#36895;&#29575;&#65292;&#20165;&#26377;&#22810;&#23545;&#25968;&#22240;&#23376;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#20272;&#35745;&#20855;&#26377;&#20869;&#22312;&#20302;&#32500;&#32467;&#26500;&#21644;&#19981;&#36830;&#32493;&#25110;&#31895;&#31961;&#22343;&#20540;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop a general theory for adaptive nonparametric estimation of the mean function of a non-stationary and nonlinear time series model using deep neural networks (DNNs). We first consider two types of DNN estimators, non-penalized and sparse-penalized DNN estimators, and establish their generalization error bounds for general non-stationary time series. We then derive minimax lower bounds for estimating mean functions belonging to a wide class of nonlinear autoregressive (AR) models that include nonlinear generalized additive AR, single index, and threshold AR models. Building upon the results, we show that the sparse-penalized DNN estimator is adaptive and attains the minimax optimal rates up to a poly-logarithmic factor for many nonlinear AR models. Through numerical simulations, we demonstrate the usefulness of the DNN methods for estimating nonlinear AR models with intrinsic low-dimensional structures and discontinuous or rough mean functions, which is consistent
&lt;/p&gt;</description></item></channel></rss>