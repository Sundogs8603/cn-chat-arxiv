<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;G-LowTESTR&#31639;&#27861;&#26469;&#23454;&#29616;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2311.01771</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Generalized Low-Rank Tensor Contextual Bandits. (arXiv:2311.01771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;G-LowTESTR&#31639;&#27861;&#26469;&#23454;&#29616;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#19968;&#31181;&#26032;&#39062;&#30340;&#36172;&#21338;&#31639;&#27861;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#22810;&#32500;&#25968;&#25454;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#22266;&#26377;&#38750;&#32447;&#24615;&#29305;&#24615;&#65292;&#25552;&#20379;&#39640;&#21487;&#29992;&#21644;&#36127;&#36131;&#20219;&#30340;&#20915;&#31574;&#26381;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#21160;&#20316;&#30001;&#19977;&#20010;&#29305;&#24449;&#21521;&#37327;&#32452;&#25104;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#24352;&#37327;&#34920;&#31034;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#22870;&#21169;&#26159;&#36890;&#36807;&#23558;&#21160;&#20316;&#30340;&#29305;&#24449;&#24352;&#37327;&#19982;&#19968;&#20010;&#22266;&#23450;&#20294;&#26410;&#30693;&#30340;&#21442;&#25968;&#24352;&#37327;&#30340;&#20869;&#31215;&#24212;&#29992;&#20110;&#24191;&#20041;&#32447;&#24615;&#20989;&#25968;&#26469;&#30830;&#23450;&#30340;&#65292;&#32780;&#36825;&#20010;&#21442;&#25968;&#24352;&#37327;&#20855;&#26377;&#36739;&#20302;&#30340;&#31649;&#29366;&#31209;&#12290;&#20026;&#20102;&#23454;&#29616;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#25506;&#32034;&#23376;&#31354;&#38388;&#28982;&#21518;&#32454;&#21270;&#8221;&#30340;&#26032;&#31639;&#27861;&#65288;G-LowTESTR&#65289;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#25910;&#38598;&#21407;&#22987;&#25968;&#25454;&#65292;&#20197;&#25506;&#32034;&#23884;&#20837;&#22312;&#20915;&#31574;&#24773;&#22659;&#20013;&#30340;&#26412;&#36136;&#20302;&#31209;&#24352;&#37327;&#23376;&#31354;&#38388;&#20449;&#24687;&#65292;&#28982;&#21518;&#23558;&#21407;&#22987;&#27010;&#29575;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to build a novel bandits algorithm that is capable of fully harnessing the power of multi-dimensional data and the inherent non-linearity of reward functions to provide high-usable and accountable decision-making services. To this end, we introduce a generalized low-rank tensor contextual bandits model in which an action is formed from three feature vectors, and thus can be represented by a tensor. In this formulation, the reward is determined through a generalized linear function applied to the inner product of the action's feature tensor and a fixed but unknown parameter tensor with a low tubal rank. To effectively achieve the trade-off between exploration and exploitation, we introduce a novel algorithm called "Generalized Low-Rank Tensor Exploration Subspace then Refine" (G-LowTESTR). This algorithm first collects raw data to explore the intrinsic low-rank tensor subspace information embedded in the decision-making scenario, and then converts the original prob
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#39044;&#21518;&#21327;&#21464;&#37327;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21327;&#21464;&#37327;&#35843;&#25972;&#21644;&#21382;&#21490;&#23545;&#29031;&#20449;&#24687;&#21033;&#29992;&#30340;&#31574;&#30053;&#65292;&#22312;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#29992;&#20110;&#26377;&#25928;&#21644;&#24555;&#36895;&#20915;&#31574;&#12290;&#36890;&#36807;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26500;&#24314;&#25968;&#23383;&#23402;&#29983;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#21382;&#21490;&#23545;&#29031;&#25968;&#25454;&#20135;&#29983;&#25968;&#23383;&#23402;&#29983;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#32780;&#36827;&#34892;&#21333;&#19968;&#21327;&#21464;&#37327;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2310.18027</link><description>&lt;p&gt;
&#20855;&#26377;&#21472;&#21152;&#28151;&#21512;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#39044;&#21518;&#21327;&#21464;&#37327;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayesian Prognostic Covariate Adjustment With Additive Mixture Priors. (arXiv:2310.18027v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18027
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#39044;&#21518;&#21327;&#21464;&#37327;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21327;&#21464;&#37327;&#35843;&#25972;&#21644;&#21382;&#21490;&#23545;&#29031;&#20449;&#24687;&#21033;&#29992;&#30340;&#31574;&#30053;&#65292;&#22312;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#29992;&#20110;&#26377;&#25928;&#21644;&#24555;&#36895;&#20915;&#31574;&#12290;&#36890;&#36807;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26500;&#24314;&#25968;&#23383;&#23402;&#29983;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#21382;&#21490;&#23545;&#29031;&#25968;&#25454;&#20135;&#29983;&#25968;&#23383;&#23402;&#29983;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#32780;&#36827;&#34892;&#21333;&#19968;&#21327;&#21464;&#37327;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCTs&#65289;&#20013;&#36827;&#34892;&#26377;&#25928;&#21644;&#24555;&#36895;&#30340;&#20915;&#31574;&#38656;&#35201;&#26080;&#20559;&#21644;&#20934;&#30830;&#30340;&#27835;&#30103;&#25928;&#26524;&#25512;&#26029;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#35201;&#27714;&#65292;&#26377;&#20004;&#31181;&#31574;&#30053;&#65306;&#35843;&#25972;&#19982;&#32467;&#26524;&#39640;&#24230;&#30456;&#20851;&#30340;&#21327;&#21464;&#37327;&#65292;&#20197;&#21450;&#36890;&#36807;&#36125;&#21494;&#26031;&#23450;&#29702;&#21033;&#29992;&#21382;&#21490;&#23545;&#29031;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#39044;&#21518;&#21327;&#21464;&#37327;&#35843;&#25972;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;PROCOVA&#65292;&#23558;&#36825;&#20004;&#31181;&#31574;&#30053;&#32467;&#21512;&#36215;&#26469;&#12290;&#21327;&#21464;&#37327;&#35843;&#25972;&#22522;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#65292;&#26500;&#24314;&#20102;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#21442;&#19982;&#32773;&#30340;&#25968;&#23383;&#23402;&#29983;&#29983;&#25104;&#22120;&#65288;DTG&#65289;&#12290;DTG&#36890;&#36807;&#21382;&#21490;&#23545;&#29031;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#27599;&#20010;&#21442;&#19982;&#32773;&#30340;&#23545;&#29031;&#32467;&#26524;&#20135;&#29983;&#20102;&#19968;&#20010;&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#27010;&#29575;&#20998;&#24067;&#12290;DT&#20998;&#24067;&#30340;&#26399;&#26395;&#23450;&#20041;&#20102;&#29992;&#20110;&#35843;&#25972;&#30340;&#21333;&#19968;&#21327;&#21464;&#37327;&#12290;&#21382;&#21490;&#23545;&#29031;&#20449;&#24687;&#36890;&#36807;&#20855;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#21472;&#21152;&#28151;&#21512;&#20808;&#39564;&#36827;&#34892;&#21033;&#29992;&#65306;&#22522;&#20110;&#20808;&#39564;&#20449;&#24687;&#30830;&#23450;&#30340;&#19968;&#20010;&#20449;&#24687;&#20808;&#39564;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective and rapid decision-making from randomized controlled trials (RCTs) requires unbiased and precise treatment effect inferences. Two strategies to address this requirement are to adjust for covariates that are highly correlated with the outcome, and to leverage historical control information via Bayes' theorem. We propose a new Bayesian prognostic covariate adjustment methodology, referred to as Bayesian PROCOVA, that combines these two strategies. Covariate adjustment is based on generative artificial intelligence (AI) algorithms that construct a digital twin generator (DTG) for RCT participants. The DTG is trained on historical control data and yields a digital twin (DT) probability distribution for each participant's control outcome. The expectation of the DT distribution defines the single covariate for adjustment. Historical control information are leveraged via an additive mixture prior with two components: an informative prior probability distribution specified based on h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24320;&#25918;&#33719;&#21462;&#30340;&#21355;&#26143;&#22270;&#20687;&#21644;GEDI&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#27861;&#22269;&#30340;&#22823;&#33539;&#22260;&#20869;&#20272;&#35745;&#20102;&#26862;&#26519;&#39640;&#24230;&#21644;&#29983;&#29289;&#37327;&#65292;&#24182;&#29983;&#25104;&#20102;2020&#24180;&#30340;&#39640;&#20998;&#36776;&#29575;&#22320;&#22270;&#12290;</title><link>http://arxiv.org/abs/2310.14662</link><description>&lt;p&gt;
&#20351;&#29992;&#24320;&#25918;&#33719;&#21462;&#30340;&#22810;&#20256;&#24863;&#22120;&#21355;&#26143;&#22270;&#20687;&#21644;GEDI&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#20272;&#35745;&#26862;&#26519;&#39640;&#24230;&#21644;&#29983;&#29289;&#37327;&#65306;&#27861;&#22269;&#22478;&#24066;&#22320;&#21306;&#30340;&#39640;&#20998;&#36776;&#29575;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Estimation of forest height and biomass from open-access multi-sensor satellite imagery and GEDI Lidar data: high-resolution maps of metropolitan France. (arXiv:2310.14662v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24320;&#25918;&#33719;&#21462;&#30340;&#21355;&#26143;&#22270;&#20687;&#21644;GEDI&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#27861;&#22269;&#30340;&#22823;&#33539;&#22260;&#20869;&#20272;&#35745;&#20102;&#26862;&#26519;&#39640;&#24230;&#21644;&#29983;&#29289;&#37327;&#65292;&#24182;&#29983;&#25104;&#20102;2020&#24180;&#30340;&#39640;&#20998;&#36776;&#29575;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26144;&#23556;&#26862;&#26519;&#36164;&#28304;&#21644;&#30899;&#20648;&#37327;&#23545;&#20110;&#25913;&#21892;&#26862;&#26519;&#31649;&#29702;&#24182;&#23454;&#29616;&#30899;&#20648;&#23384;&#21644;&#29615;&#22659;&#20445;&#25252;&#30446;&#26631;&#38750;&#24120;&#37325;&#35201;&#12290;&#33322;&#22825;&#36965;&#24863;&#26041;&#27861;&#22312;&#25552;&#20379;&#22823;&#33539;&#22260;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#37325;&#22797;&#35266;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#26469;&#25903;&#25345;&#26862;&#26519;&#39640;&#24230;&#30417;&#27979;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20808;&#21069;&#24320;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#27861;&#22269;&#22269;&#23478;&#33539;&#22260;&#31561;&#26356;&#22823;&#27604;&#20363;&#19978;&#25512;&#24191;&#20102;&#35813;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;GEDI&#28608;&#20809;&#38647;&#36798;&#20219;&#21153;&#20316;&#20026;&#21442;&#32771;&#39640;&#24230;&#25968;&#25454;&#65292;&#21033;&#29992;Sentinel-1&#12289;Sentinel-2&#21644;ALOS-2 PALSA-2&#21355;&#26143;&#22270;&#20687;&#20272;&#35745;&#26862;&#26519;&#39640;&#24230;&#24182;&#20135;&#29983;2020&#24180;&#30340;&#27861;&#22269;&#22320;&#22270;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36890;&#29992;&#26041;&#31243;&#23558;&#39640;&#24230;&#22270;&#36716;&#21270;&#20026;&#20307;&#31215;&#21644;&#22320;&#19978;&#29983;&#29289;&#37327;&#65288;AGB&#65289;&#12290;&#21033;&#29992;ALS&#25968;&#25454;&#30340;&#26412;&#22320;&#22320;&#22270;&#39564;&#35777;&#20102;&#39640;&#24230;&#22270;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mapping forest resources and carbon is important for improving forest management and meeting the objectives of storing carbon and preserving the environment. Spaceborne remote sensing approaches have considerable potential to support forest height monitoring by providing repeated observations at high spatial resolution over large areas. This study uses a machine learning approach that was previously developed to produce local maps of forest parameters (basal area, height, diameter, etc.). The aim of this paper is to present the extension of the approach to much larger scales such as the French national coverage. We used the GEDI Lidar mission as reference height data, and the satellite images from Sentinel-1, Sentinel-2 and ALOS-2 PALSA-2 to estimate forest height and produce a map of France for the year 2020. The height map is then derived into volume and aboveground biomass (AGB) using allometric equations. The validation of the height map with local maps from ALS data shows an accur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#37319;&#26679;&#22120;&#25351;&#23450;&#38544;&#21547;&#20998;&#24067;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36817;&#20284;&#22797;&#26434;&#30340;&#22810;&#23792;&#21644;&#30456;&#20851;&#21518;&#39564;&#20998;&#24067;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#32447;&#24615;&#21270;&#30340;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#39069;&#22806;&#30340;&#32593;&#32476;&#21644;&#19981;&#31283;&#23450;&#23545;&#25239;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#26550;&#26500;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#25968;&#30334;&#19975;&#20010;&#28508;&#21464;&#37327;&#30340;&#38544;&#21547;&#20998;&#24067;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#22823;&#22411;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#23618;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#23545;&#20110;&#32593;&#32476;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.06643</link><description>&lt;p&gt;
&#39640;&#32500;&#21518;&#39564;&#25512;&#26029;&#30340;&#38544;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Implicit Variational Inference for High-Dimensional Posteriors. (arXiv:2310.06643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#37319;&#26679;&#22120;&#25351;&#23450;&#38544;&#21547;&#20998;&#24067;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36817;&#20284;&#22797;&#26434;&#30340;&#22810;&#23792;&#21644;&#30456;&#20851;&#21518;&#39564;&#20998;&#24067;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#32447;&#24615;&#21270;&#30340;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#39069;&#22806;&#30340;&#32593;&#32476;&#21644;&#19981;&#31283;&#23450;&#23545;&#25239;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#26550;&#26500;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#25968;&#30334;&#19975;&#20010;&#28508;&#21464;&#37327;&#30340;&#38544;&#21547;&#20998;&#24067;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#22823;&#22411;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#23618;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#23545;&#20110;&#32593;&#32476;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#20998;&#25512;&#26029;&#20013;&#65292;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#22909;&#22788;&#22312;&#20110;&#20934;&#30830;&#25429;&#25417;&#30495;&#23454;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25351;&#23450;&#38544;&#21547;&#20998;&#24067;&#30340;&#31070;&#32463;&#37319;&#26679;&#22120;&#65292;&#36825;&#23545;&#20110;&#36817;&#20284;&#39640;&#32500;&#31354;&#38388;&#20013;&#22797;&#26434;&#22810;&#23792;&#21644;&#30456;&#20851;&#21518;&#39564;&#20998;&#24067;&#38750;&#24120;&#36866;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#21270;&#31070;&#32463;&#37319;&#26679;&#22120;&#24341;&#20837;&#26032;&#30340;&#32422;&#26463;&#65292;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#37492;&#21035;&#22120;&#32593;&#32476;&#21644;&#19981;&#31283;&#23450;&#30340;&#23545;&#25239;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#26550;&#26500;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#25968;&#30334;&#19975;&#20010;&#28508;&#21464;&#37327;&#30340;&#38544;&#21547;&#20998;&#24067;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#25968;&#20540;&#36817;&#20284;&#26469;&#35299;&#20915;&#35745;&#31639;&#19978;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#22823;&#22411;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#24674;&#22797;&#23618;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#26159;&#32593;&#32476;&#24615;&#33021;&#20851;&#38190;&#20294;&#33261;&#21517;&#26157;&#33879;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In variational inference, the benefits of Bayesian models rely on accurately capturing the true posterior distribution. We propose using neural samplers that specify implicit distributions, which are well-suited for approximating complex multimodal and correlated posteriors in high-dimensional spaces. Our approach advances inference using implicit distributions by introducing novel bounds that come about by locally linearising the neural sampler. This is distinct from existing methods that rely on additional discriminator networks and unstable adversarial objectives. Furthermore, we present a new sampler architecture that, for the first time, enables implicit distributions over millions of latent variables, addressing computational concerns by using differentiable numerical approximations. Our empirical analysis indicates our method is capable of recovering correlations across layers in large Bayesian neural networks, a property that is crucial for a network's performance but notorious
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#21644;&#35821;&#35328;&#25551;&#36848;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#32780;&#19988;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.05061</link><description>&lt;p&gt;
&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language. (arXiv:2308.05061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#21644;&#35821;&#35328;&#25551;&#36848;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#32780;&#19988;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#22312;&#25512;&#29702;&#38454;&#27573;&#20174;&#25552;&#31034;&#25968;&#25454;&#20013;&#23398;&#20064;&#36816;&#31639;&#31526;&#30340;&#26174;&#33879;&#28508;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#27169;&#22411;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#24573;&#35270;&#36816;&#31639;&#31526;&#30340;&#23453;&#36149;&#30340;&#20154;&#31867;&#27934;&#23519;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;&#36716;&#21270;&#20026;&#19968;&#31181;&#22810;&#27169;&#24335;&#33539;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#8220;&#26631;&#39064;&#8221;&#26469;&#25972;&#21512;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#26041;&#31243;&#24335;&#34920;&#36798;&#30340;&#36816;&#31639;&#31526;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36941;&#24615;&#65292;&#32780;&#19988;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#24182;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#22810;&#27169;&#24335;&#19978;&#19979;&#25991;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#8220;ICON-LM&#8221;&#65292;&#22522;&#20110;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of "captions" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as "ICON-LM", based on a language-model-like architecture. We demonstrate the viability of "ICO
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27867;&#21270;&#24615;&#30340;&#22330;&#35770;&#24418;&#24335;&#20307;&#31995;&#65292;&#29992;&#20110;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#38480;&#23485;&#38544;&#34255;&#23618;&#30340;&#26497;&#38480;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#38416;&#26126;&#20102;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#23545;&#32593;&#32476;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.16695</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36125;&#21494;&#26031;&#25512;&#29702;&#20013;&#30340;&#25968;&#25454;&#21464;&#24322;&#24615;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A theory of data variability in Neural Network Bayesian inference. (arXiv:2307.16695v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27867;&#21270;&#24615;&#30340;&#22330;&#35770;&#24418;&#24335;&#20307;&#31995;&#65292;&#29992;&#20110;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#38480;&#23485;&#38544;&#34255;&#23618;&#30340;&#26497;&#38480;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#38416;&#26126;&#20102;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#23545;&#32593;&#32476;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#26680;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#36890;&#36807;&#20351;&#29992;&#26680;&#21644;&#25512;&#29702;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#38480;&#23485;&#38544;&#34255;&#23618;&#30340;&#26497;&#38480;&#24773;&#20917;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#22312;&#36825;&#20010;&#26497;&#38480;&#30340;&#22522;&#30784;&#19978;&#24314;&#31435;&#20102;&#19968;&#20010;&#22330;&#35770;&#24418;&#24335;&#20307;&#31995;&#65292;&#28085;&#30422;&#20102;&#26080;&#38480;&#23485;&#32593;&#32476;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35745;&#31639;&#20102;&#20855;&#26377;&#24322;&#36136;&#26465;&#30446;&#30340;&#26680;&#30697;&#38453;&#30340;&#32447;&#24615;&#12289;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#19982;&#30446;&#21069;&#20351;&#29992;&#30340;&#35889;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#36755;&#20837;&#25968;&#25454;&#30340;&#32479;&#35745;&#29305;&#24615;&#25512;&#23548;&#20986;&#27867;&#21270;&#29305;&#24615;&#65292;&#38416;&#26126;&#20102;&#36755;&#20837;&#32500;&#24230;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20197;&#21450;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#34920;&#26126;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#23548;&#33268;&#20102;&#19968;&#31181;&#38750;&#39640;&#26031;&#20316;&#29992;&#65292;&#31867;&#20284;&#20110;($\varphi^3+\varphi^4$)-&#29702;&#35770;&#12290;&#22312;&#19968;&#20010;&#21512;&#25104;&#20219;&#21153;&#21644;MNIST&#19978;&#20351;&#29992;&#25105;&#20204;&#30340;&#24418;&#24335;&#20307;&#31995;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#22343;&#21248;&#30340;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference and kernel methods are well established in machine learning. The neural network Gaussian process in particular provides a concept to investigate neural networks in the limit of infinitely wide hidden layers by using kernel and inference methods. Here we build upon this limit and provide a field-theoretic formalism which covers the generalization properties of infinitely wide networks. We systematically compute generalization properties of linear, non-linear, and deep non-linear networks for kernel matrices with heterogeneous entries. In contrast to currently employed spectral methods we derive the generalization properties from the statistical properties of the input, elucidating the interplay of input dimensionality, size of the training data set, and variability of the data. We show that data variability leads to a non-Gaussian action reminiscent of a ($\varphi^3+\varphi^4$)-theory. Using our formalism on a synthetic task and on MNIST we obtain a homogeneous kernel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#36719;&#24178;&#39044;&#20013;&#30830;&#20445;&#22240;&#26524;&#20998;&#35299;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#32534;&#30721;&#21464;&#20998;&#36125;&#21494;&#26031;&#31639;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#19968;&#33324;&#21270;&#30340;&#24544;&#35802;&#24615;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#26524;&#21464;&#37327;&#65292;&#20173;&#28982;&#21487;&#20197;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#39044;&#27979;&#26410;&#35265;&#32452;&#21512;&#30340;&#24178;&#39044;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06250</link><description>&lt;p&gt;
&#20174;&#36719;&#24178;&#39044;&#20013;&#30830;&#20445;&#22240;&#26524;&#20998;&#35299;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability Guarantees for Causal Disentanglement from Soft Interventions. (arXiv:2307.06250v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#36719;&#24178;&#39044;&#20013;&#30830;&#20445;&#22240;&#26524;&#20998;&#35299;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#32534;&#30721;&#21464;&#20998;&#36125;&#21494;&#26031;&#31639;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#19968;&#33324;&#21270;&#30340;&#24544;&#35802;&#24615;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#26524;&#21464;&#37327;&#65292;&#20173;&#28982;&#21487;&#20197;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#39044;&#27979;&#26410;&#35265;&#32452;&#21512;&#30340;&#24178;&#39044;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#20998;&#35299;&#26088;&#22312;&#36890;&#36807;&#28508;&#22312;&#21464;&#37327;&#30340;&#30456;&#20851;&#24615;&#25581;&#31034;&#25968;&#25454;&#30340;&#34920;&#24449;&#65292;&#20854;&#36890;&#36807;&#22240;&#26524;&#27169;&#22411;&#30456;&#20114;&#20851;&#32852;&#12290;&#22914;&#26524;&#35299;&#37322;&#25968;&#25454;&#30340;&#28508;&#22312;&#27169;&#22411;&#26159;&#21807;&#19968;&#30340;&#65292;&#37027;&#20040;&#36825;&#31181;&#34920;&#31034;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#24403;&#23384;&#22312;&#19981;&#37197;&#23545;&#30340;&#35266;&#27979;&#21644;&#24178;&#39044;&#25968;&#25454;&#26102;&#30340;&#24773;&#20917;&#65292;&#27599;&#20010;&#24178;&#39044;&#37117;&#20250;&#25913;&#21464;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#26426;&#21046;&#12290;&#24403;&#22240;&#26524;&#21464;&#37327;&#23436;&#20840;&#35266;&#27979;&#21040;&#26102;&#65292;&#22312;&#35802;&#23454;&#24615;&#20551;&#35774;&#19979;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#32479;&#35745;&#19968;&#33268;&#30340;&#31639;&#27861;&#26469;&#35782;&#21035;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#26524;&#21464;&#37327;&#65292;&#22312;&#32473;&#23450;&#19968;&#33324;&#21270;&#30340;&#24544;&#35802;&#24615;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#21487;&#35782;&#21035;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20445;&#35777;&#20102;&#25105;&#20204;&#21487;&#20197;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#39044;&#27979;&#26410;&#35265;&#32452;&#21512;&#30340;&#24178;&#39044;&#25928;&#26524;&#65292;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#32534;&#30721;&#21464;&#20998;&#36125;&#21494;&#26031;&#31639;&#27861;&#21644;ap&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#22240;&#26524;&#20998;&#35299;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal disentanglement aims to uncover a representation of data using latent variables that are interrelated through a causal model. Such a representation is identifiable if the latent model that explains the data is unique. In this paper, we focus on the scenario where unpaired observational and interventional data are available, with each intervention changing the mechanism of a latent variable. When the causal variables are fully observed, statistically consistent algorithms have been developed to identify the causal model under faithfulness assumptions. We here show that identifiability can still be achieved with unobserved causal variables, given a generalized notion of faithfulness. Our results guarantee that we can recover the latent causal model up to an equivalence class and predict the effect of unseen combinations of interventions, in the limit of infinite data. We implement our causal disentanglement framework by developing an autoencoding variational Bayes algorithm and ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;(IFM)&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#20197;&#23454;&#29616;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#12290;</title><link>http://arxiv.org/abs/2306.04027</link><description>&lt;p&gt;
&#22240;&#23376;&#22270;&#27169;&#22411;&#35270;&#35282;&#19979;&#30340;&#24178;&#39044;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Intervention Generalization: A View from Factor Graph Models. (arXiv:2306.04027v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;(IFM)&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#20197;&#23454;&#29616;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#30340;&#19968;&#20010;&#30446;&#26631;&#26159;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21644;&#35266;&#23519;&#25968;&#25454;&#25512;&#24191;&#21040;&#26032;&#30340;&#26465;&#20214;&#12290;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#25552;&#20379;&#36275;&#22815;&#22810;&#30340;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35770;&#19978;&#21487;&#33021;&#26368;&#32456;&#23398;&#20064;&#20174;&#26032;&#30340;&#23454;&#39564;&#26465;&#20214;&#21040;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#30340;&#26144;&#23556;&#65292;&#20294;&#26159;&#22788;&#29702;&#22823;&#37327;&#21487;&#33021;&#30340;&#24178;&#39044;&#32452;&#21512;&#31354;&#38388;&#24456;&#22256;&#38590;&#12290;&#22312;&#20856;&#22411;&#30340;&#31232;&#30095;&#23454;&#39564;&#35774;&#35745;&#19979;&#65292;&#22914;&#26524;&#19981;&#20381;&#36182;&#20110;&#37325;&#30340;&#35268;&#21017;&#21270;&#25110;&#20808;&#39564;&#20998;&#24067;&#65292;&#36825;&#31181;&#26144;&#23556;&#26159;&#19981;&#36866;&#24403;&#30340;&#12290;&#36825;&#26679;&#30340;&#20551;&#35774;&#21487;&#33021;&#26159;&#21487;&#38752;&#30340;&#65292;&#20063;&#21487;&#33021;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#24456;&#38590;&#36777;&#25252;&#25110;&#27979;&#35797;&#12290;&#26412;&#25991;&#20174;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#35821;&#35328;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#22914;&#20309;&#20445;&#35777;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#12290;&#20551;&#35774;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#26159;&#23427;&#24456;&#26041;&#20415;&#22320;&#22788;&#29702;&#20102;&#22823;&#37327;&#21487;&#33021;&#30340;&#24178;&#39044;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the goals of causal inference is to generalize from past experiments and observational data to novel conditions. While it is in principle possible to eventually learn a mapping from a novel experimental condition to an outcome of interest, provided a sufficient variety of experiments is available in the training data, coping with a large combinatorial space of possible interventions is hard. Under a typical sparse experimental design, this mapping is ill-posed without relying on heavy regularization or prior distributions. Such assumptions may or may not be reliable, and can be hard to defend or test. In this paper, we take a close look at how to warrant a leap from past experiments to novel conditions based on minimal assumptions about the factorization of the distribution of the manipulated system, communicated in the well-understood language of factor graph models. A postulated $\textit{interventional factor model}$ (IFM) may not always be informative, but it conveniently abs
&lt;/p&gt;</description></item><item><title>&#20998;&#25955;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;&#65292;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#21644;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>http://arxiv.org/abs/2306.02913</link><description>&lt;p&gt;
&#20998;&#25955;&#21270;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;
&lt;/p&gt;
&lt;p&gt;
Decentralized SGD and Average-direction SAM are Asymptotically Equivalent. (arXiv:2306.02913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02913
&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;&#65292;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#21644;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;D-SGD&#65289;&#20801;&#35768;&#22312;&#27809;&#26377;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#25511;&#21046;&#19979;&#65292;&#22823;&#37327;&#35774;&#22791;&#21516;&#26102;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#29702;&#35770;&#35748;&#20026;&#65292;&#20998;&#25955;&#21270;&#19981;&#21487;&#36991;&#20813;&#22320;&#21066;&#24369;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25361;&#25112;&#20256;&#32479;&#20449;&#24565;&#65292;&#25552;&#20986;&#20102;&#23436;&#20840;&#26032;&#30340;&#35282;&#24230;&#26469;&#29702;&#35299;&#20998;&#25955;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#38750;&#20984;&#38750;-$\beta$-&#24179;&#28369;&#35774;&#32622;&#19979;&#65292;D-SGD&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#20102;&#24179;&#22343;&#26041;&#21521;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#31639;&#27861;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#36825;&#31181;&#24778;&#20154;&#30340;&#28176;&#36817;&#31561;&#20215;&#25581;&#31034;&#20102;&#20869;&#22312;&#30340;&#27491;&#21017;&#21270;-&#20248;&#21270;&#26435;&#34913;&#20197;&#21450;&#20998;&#25955;&#21270;&#30340;&#19977;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;D-SGD&#20013;&#23384;&#22312;&#19968;&#20010;&#33258;&#30001;&#30340;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26426;&#21046;&#65292;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#20272;&#35745;&#65307;&#65288;2&#65289;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#65307;&#65288;3&#65289;D-SGD&#30340;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#19981;&#20250;&#38543;&#30528;&#24635;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#36825;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;WCAM&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#65292;&#30830;&#23450;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#38416;&#26126;&#32553;&#25918;&#22914;&#20309;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14979</link><description>&lt;p&gt;
&#23610;&#24230;&#24456;&#37325;&#35201;&#65306;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;&#35299;&#37322;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scale Matters: Attribution Meets the Wavelet Domain to Explain Model Sensitivity to Image Corruptions. (arXiv:2305.14979v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;WCAM&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#65292;&#30830;&#23450;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#38416;&#26126;&#32553;&#25918;&#22914;&#20309;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#30001;&#20110;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#23646;&#24615;&#26041;&#27861;&#23545;&#20110;&#35299;&#37322;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#26159;&#26080;&#25928;&#30340;&#65292;&#32780;&#24378;&#20581;&#24615;&#39046;&#22495;&#30340;&#25991;&#29486;&#20165;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#20687;&#25439;&#22351;&#30340;&#24773;&#20917;&#19979;&#65292;&#23457;&#26597;&#27169;&#22411;&#30340;&#34892;&#20026;&#33021;&#21147;&#23545;&#20110;&#25552;&#39640;&#29992;&#25143;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Wavelet sCale Attribution Method (WCAM)&#65292;&#23427;&#26159;&#20174;&#20687;&#32032;&#22495;&#21040;&#31354;&#38388;&#23610;&#24230;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;&#30340;&#27010;&#25324;&#12290;&#22312;&#31354;&#38388;&#23610;&#24230;&#22495;&#20013;&#36827;&#34892;&#23646;&#24615;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#20851;&#27880;&#28857;&#21644;&#23610;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;WCAM&#35299;&#37322;&#20102;&#27169;&#22411;&#22312;&#22270;&#20687;&#30772;&#22351;&#19979;&#30340;&#22833;&#25928;&#65292;&#30830;&#23450;&#20102;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#35299;&#37322;&#20102;&#22914;&#20309;&#36890;&#36807;&#32553;&#25918;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have shown remarkable performance in computer vision, but their deployment in real-world scenarios is challenging due to their sensitivity to image corruptions. Existing attribution methods are uninformative for explaining the sensitivity to image corruptions, while the literature on robustness only provides model-based explanations. However, the ability to scrutinize models' behavior under image corruptions is crucial to increase the user's trust. Towards this end, we introduce the Wavelet sCale Attribution Method (WCAM), a generalization of attribution from the pixel domain to the space-scale domain. Attribution in the space-scale domain reveals where and on what scales the model focuses. We show that the WCAM explains models' failures under image corruptions, identifies sufficient information for prediction, and explains how zoom-in increases accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#25351;&#30340;&#26159;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#39044;&#27979;&#26102;&#30340;&#39044;&#27979;&#24615;&#33021;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2305.10564</link><description>&lt;p&gt;
&#23545;&#25918;&#24323;&#20998;&#31867;&#22120;&#36827;&#34892;&#21453;&#20107;&#23454;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Counterfactually Comparing Abstaining Classifiers. (arXiv:2305.10564v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#25351;&#30340;&#26159;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#39044;&#27979;&#26102;&#30340;&#39044;&#27979;&#24615;&#33021;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#24323;&#20998;&#31867;&#22120;&#21487;&#20197;&#36873;&#25321;&#22312;&#19981;&#30830;&#23450;&#26102;&#25918;&#24323;&#23545;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#38382;&#39064;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20445;&#30041;&#19981;&#30830;&#23450;&#30340;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#40657;&#30418;&#25918;&#24323;&#20998;&#31867;&#22120;&#26102;&#65292;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#32771;&#34385;&#20998;&#31867;&#22120;&#22312;&#23427;&#30340;&#25918;&#24323;&#39044;&#27979;&#19978;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#24403;&#25918;&#23556;&#31185;&#21307;&#29983;&#19981;&#30830;&#23450;&#20854;&#35786;&#26029;&#25110;&#24403;&#39550;&#39542;&#21592;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#19981;&#27880;&#24847;&#26102;&#65292;&#36825;&#20123;&#32570;&#22833;&#30340;&#39044;&#27979;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#22260;&#32469;&#30528;&#23450;&#20041;&#19968;&#20010;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#21363;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#30340;&#24773;&#20917;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#25351;&#23450;&#20102;&#26465;&#20214;... (&#27492;&#22788;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;
Abstaining classifiers have the option to abstain from making predictions on inputs that they are unsure about. These classifiers are becoming increasingly popular in high-stake decision-making problems, as they can withhold uncertain predictions to improve their reliability and safety. When evaluating black-box abstaining classifier(s), however, we lack a principled approach that accounts for what the classifier would have predicted on its abstentions. These missing predictions are crucial when, e.g., a radiologist is unsure of their diagnosis or when a driver is inattentive in a self-driving car. In this paper, we introduce a novel approach and perspective to the problem of evaluating and comparing abstaining classifiers by treating abstentions as missing data. Our evaluation approach is centered around defining the counterfactual score of an abstaining classifier, defined as the expected performance of the classifier had it not been allowed to abstain. We specify the conditions unde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26799;&#24230;&#21098;&#20999;&#30340;&#25910;&#25947;&#20445;&#35777;&#26426;&#21046;&#65292;&#19981;&#20877;&#38656;&#35201;&#29305;&#23450;&#30340;&#38408;&#20540;&#21644;&#24378;&#22122;&#22768;&#20551;&#35774;&#65292;&#21516;&#26102;&#21487;&#20197;&#29420;&#31435;&#20110;&#27493;&#38271;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25910;&#25947;&#30340;&#33258;&#30001;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.01588</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26799;&#24230;&#21098;&#20999;&#65306;&#38543;&#26426;&#20559;&#24046;&#21644;&#32039;&#23494;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees. (arXiv:2305.01588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26799;&#24230;&#21098;&#20999;&#30340;&#25910;&#25947;&#20445;&#35777;&#26426;&#21046;&#65292;&#19981;&#20877;&#38656;&#35201;&#29305;&#23450;&#30340;&#38408;&#20540;&#21644;&#24378;&#22122;&#22768;&#20551;&#35774;&#65292;&#21516;&#26102;&#21487;&#20197;&#29420;&#31435;&#20110;&#27493;&#38271;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25910;&#25947;&#30340;&#33258;&#30001;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#21098;&#20999;&#26159;&#26631;&#20934;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#30340;&#19968;&#31181;&#27969;&#34892;&#20462;&#25913;&#26041;&#27861;&#65292;&#27599;&#27425;&#36845;&#20195;&#23558;&#26799;&#24230;&#33539;&#25968;&#38480;&#21046;&#22312;&#26576;&#20010;&#20540;c&gt;0&#12290;&#23427;&#34987;&#24191;&#27867;&#29992;&#20110;&#31283;&#23450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;( Goodfellow et al., 2016 )&#25110;&#24378;&#21046;&#23454;&#26045;&#24046;&#20998;&#38544;&#31169;( Abadi et al., 2016 )&#12290;&#23613;&#31649;&#21098;&#20999;&#26426;&#21046;&#21463;&#27426;&#36814;&#19988;&#31616;&#21333;&#65292;&#20294;&#20854;&#25910;&#25947;&#20445;&#35777;&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#30340;$c$&#20540;&#21644;&#24378;&#22122;&#22768;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#25910;&#25947;&#20445;&#35777;&#65292;&#26174;&#31034;&#20102;&#23545;&#20219;&#24847;&#21098;&#36753;&#38408;&#20540;&#30340;&#31934;&#30830;&#20381;&#36182;&#65292;&#24182;&#19988;&#34920;&#26126;&#25105;&#20204;&#30340;&#20445;&#35777;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#37117;&#26159;&#32039;&#23494;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;(i)&#23545;&#20110;&#30830;&#23450;&#24615;&#30340;&#26799;&#24230;&#19979;&#38477;&#65292;&#21098;&#36753;&#38408;&#20540;&#20165;&#24433;&#21709;&#25910;&#25947;&#30340;&#39640;&#38454;&#39033;&#65292;(ii)&#22312;&#38543;&#26426;&#35774;&#32622;&#20013;&#65292;&#21363;&#20351;&#23545;&#20110;&#20219;&#24847;&#23567;&#30340;&#27493;&#38271;&#65292;&#20063;&#19981;&#33021;&#20445;&#35777;&#25910;&#25947;&#21040;&#30495;&#27491;&#30340;&#26368;&#20248;&#35299;&#22312;&#26631;&#20934;&#30340;&#22122;&#22768;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#29305;&#23450;&#30340;&#38543;&#26426;&#22122;&#22768;&#20551;&#35774;&#65292;&#22312;&#27492;&#20551;&#35774;&#19979;&#65292;&#25910;&#25947;&#26159;&#20445;&#35777;&#30340;&#65292;&#21098;&#20999;&#38408;&#20540;$c$&#21487;&#20197;&#29420;&#31435;&#20110;&#27493;&#38271;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient clipping is a popular modification to standard (stochastic) gradient descent, at every iteration limiting the gradient norm to a certain value $c &gt;0$. It is widely used for example for stabilizing the training of deep learning models (Goodfellow et al., 2016), or for enforcing differential privacy (Abadi et al., 2016). Despite popularity and simplicity of the clipping mechanism, its convergence guarantees often require specific values of $c$ and strong noise assumptions.  In this paper, we give convergence guarantees that show precise dependence on arbitrary clipping thresholds $c$ and show that our guarantees are tight with both deterministic and stochastic gradients. In particular, we show that (i) for deterministic gradient descent, the clipping threshold only affects the higher-order terms of convergence, (ii) in the stochastic setting convergence to the true optimum cannot be guaranteed under the standard noise assumption, even under arbitrary small step-sizes. We give ma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#33539;&#25968;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;&#20559;&#24046;&#39033;&#30340;&#33539;&#25968;&#36827;&#34892;&#24809;&#32602;&#21487;&#20197;&#23454;&#29616;&#31232;&#30095;&#20272;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.01353</link><description>&lt;p&gt;
&#23545;&#27491;&#21017;&#21270;&#20013;&#30340;&#20559;&#24046;&#36827;&#34892;&#24809;&#32602;&#23558;&#20351;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Penalising the biases in norm regularisation enforces sparsity. (arXiv:2303.01353v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#33539;&#25968;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;&#20559;&#24046;&#39033;&#30340;&#33539;&#25968;&#36827;&#34892;&#24809;&#32602;&#21487;&#20197;&#23454;&#29616;&#31232;&#30095;&#20272;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#36890;&#36807;&#25511;&#21046;&#21442;&#25968;&#30340;&#33539;&#25968;&#24448;&#24448;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#33539;&#25968;&#21644;&#25152;&#24471;&#20272;&#35745;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#22312;&#29702;&#35770;&#19978;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#21333;&#19968;&#38544;&#34255;&#23618;&#21644;&#19968;&#32500;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#20989;&#25968;&#25152;&#38656;&#30340;&#21442;&#25968;&#33539;&#25968;&#30001;&#20854;&#20108;&#38454;&#23548;&#25968;&#30340;&#24635;&#21464;&#24046;&#21152;&#26435;&#24471;&#21040;&#65292;&#20854;&#20013;&#25152;&#21152;&#26435;&#30340;&#22240;&#23376;&#20026;$\sqrt{1+x^2}$&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#19981;&#23545;&#20559;&#24046;&#39033;&#30340;&#33539;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#26102;&#65292;&#36825;&#20010;&#21152;&#26435;&#22240;&#23376;&#20250;&#28040;&#22833;&#12290;&#36825;&#20010;&#39069;&#22806;&#30340;&#21152;&#26435;&#22240;&#23376;&#30340;&#23384;&#22312;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#34987;&#35777;&#26126;&#21487;&#20197;&#24378;&#21046;&#23454;&#29616;&#26368;&#23567;&#33539;&#25968;&#20869;&#25554;&#22120;&#30340;&#21807;&#19968;&#24615;&#21644;&#31232;&#30095;&#24615;&#65288;&#22312;&#25296;&#28857;&#25968;&#37327;&#19978;&#65289;&#12290;&#30456;&#21453;&#65292;&#30465;&#30053;&#20559;&#24046;&#30340;&#33539;&#25968;&#21017;&#20250;&#23548;&#33268;&#38750;&#31232;&#30095;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#27491;&#21017;&#21270;&#20013;&#23545;&#20559;&#24046;&#39033;&#36827;&#34892;&#24809;&#32602;&#65292;&#26080;&#35770;&#26159;&#26174;&#24335;&#36824;&#26159;&#38544;&#24335;&#22320;&#65292;&#37117;&#20250;&#23548;&#33268;&#31232;&#30095;&#20272;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling the parameters' norm often yields good generalisation when training neural networks. Beyond simple intuitions, the relation between regularising parameters' norm and obtained estimators remains theoretically misunderstood. For one hidden ReLU layer networks with unidimensional data, this work shows the parameters' norm required to represent a function is given by the total variation of its second derivative, weighted by a $\sqrt{1+x^2}$ factor. Notably, this weighting factor disappears when the norm of bias terms is not regularised. The presence of this additional weighting factor is of utmost significance as it is shown to enforce the uniqueness and sparsity (in the number of kinks) of the minimal norm interpolator. Conversely, omitting the bias' norm allows for non-sparse solutions. Penalising the bias terms in the regularisation, either explicitly or implicitly, thus leads to sparse estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#30340;&#26680;CUSUM&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#25935;&#24863;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#20998;&#26512;&#65292;&#24182;&#24314;&#31435;&#20102;&#26368;&#20248;&#31383;&#21475;&#38271;&#24230;&#65292;&#24341;&#20837;&#20102;&#36882;&#24402;&#35745;&#31639;&#31243;&#24207;&#26469;&#30830;&#20445;&#35745;&#31639;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#24658;&#23450;&#12290;</title><link>http://arxiv.org/abs/2211.15070</link><description>&lt;p&gt;
&#22312;&#32447;&#26680;CUSUM&#26041;&#27861;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Kernel CUSUM for Change-Point Detection. (arXiv:2211.15070v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#30340;&#26680;CUSUM&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#25935;&#24863;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#20998;&#26512;&#65292;&#24182;&#24314;&#31435;&#20102;&#26368;&#20248;&#31383;&#21475;&#38271;&#24230;&#65292;&#24341;&#20837;&#20102;&#36882;&#24402;&#35745;&#31639;&#31243;&#24207;&#26469;&#30830;&#20445;&#35745;&#31639;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#24658;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#26680;Cumulative Sum (CUSUM)&#26041;&#27861;&#65292;&#29992;&#20110;&#21464;&#28857;&#26816;&#27979;&#65292;&#21033;&#29992;&#26680;&#32479;&#35745;&#37327;&#38598;&#21512;&#20013;&#30340;&#26368;&#22823;&#20540;&#26469;&#32771;&#34385;&#26410;&#30693;&#30340;&#21464;&#28857;&#20301;&#32622;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;Scan-B&#32479;&#35745;&#37327;&#65292;&#21363;&#23545;&#24212;&#20110;&#38750;&#21442;&#25968;Shewhart&#22270;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#23567;&#21464;&#21270;&#20855;&#26377;&#26356;&#39640;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#30340;&#20934;&#30830;&#20998;&#26512;&#36817;&#20284;&#20540;&#65306;&#24179;&#22343;&#36816;&#34892;&#38271;&#24230;&#65288;ARL&#65289;&#21644;&#39044;&#26399;&#26816;&#27979;&#24310;&#36831;&#65288;EDD&#65289;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#24314;&#31435;&#19968;&#20010;&#19982;ARL&#23545;&#25968;&#21516;&#38454;&#30340;&#26368;&#20248;&#31383;&#21475;&#38271;&#24230;&#65292;&#20197;&#30830;&#20445;&#30456;&#23545;&#20110;&#20855;&#26377;&#26080;&#38480;&#20869;&#23384;&#30340;&#29702;&#35770;&#27169;&#22411;&#33021;&#22815;&#20445;&#25345;&#26368;&#23567;&#21151;&#29575;&#25439;&#22833;&#12290;&#36825;&#31867;&#20284;&#20110;&#21442;&#25968;&#21464;&#28857;&#26816;&#27979;&#25991;&#29486;&#20013;&#30340;&#31383;&#21475;&#38480;&#21046;&#24191;&#20041;&#20284;&#28982;&#27604;&#65288;GLR&#65289;&#36807;&#31243;&#30340;&#32463;&#20856;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36882;&#24402;&#35745;&#31639;&#31243;&#24207;&#65292;&#29992;&#20110;&#26816;&#27979;&#32479;&#35745;&#37327;&#65292;&#20197;&#30830;&#20445;&#35745;&#31639;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#24658;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient online kernel Cumulative Sum (CUSUM) method for change-point detection that utilizes the maximum over a set of kernel statistics to account for the unknown change-point location. Our approach exhibits increased sensitivity to small changes compared to existing methods, such as the Scan-B statistic, which corresponds to a non-parametric Shewhart chart-type procedure. We provide accurate analytic approximations for two key performance metrics: the Average Run Length (ARL) and Expected Detection Delay (EDD), which enable us to establish an optimal window length on the order of the logarithm of ARL to ensure minimal power loss relative to an oracle procedure with infinite memory. Such a finding parallels the classic result for window-limited Generalized Likelihood Ratio (GLR) procedure in parametric change-point detection literature. Moreover, we introduce a recursive calculation procedure for detection statistics to ensure constant computational and memory complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31574;&#30053;&#30340;&#37327;&#21270;&#38598;&#21453;&#28436;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#24314;&#27169;&#21644;&#36880;&#27493;&#19981;&#30830;&#23450;&#24615;&#20943;&#23569;&#21407;&#29702;&#65292;&#39034;&#24207;&#36873;&#25321;&#35780;&#20272;&#20989;&#25968;&#30340;&#28857;&#65292;&#20174;&#32780;&#26377;&#25928;&#36817;&#20284;&#24863;&#20852;&#36259;&#30340;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2211.01008</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#24207;&#36143;&#35774;&#35745;&#30340;&#35745;&#31639;&#26426;&#23454;&#39564;&#37327;&#21270;&#38598;&#21453;&#28436;
&lt;/p&gt;
&lt;p&gt;
Bayesian sequential design of computer experiments for quantile set inversion. (arXiv:2211.01008v2 [stat.ML] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31574;&#30053;&#30340;&#37327;&#21270;&#38598;&#21453;&#28436;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#24314;&#27169;&#21644;&#36880;&#27493;&#19981;&#30830;&#23450;&#24615;&#20943;&#23569;&#21407;&#29702;&#65292;&#39034;&#24207;&#36873;&#25321;&#35780;&#20272;&#20989;&#25968;&#30340;&#28857;&#65292;&#20174;&#32780;&#26377;&#25928;&#36817;&#20284;&#24863;&#20852;&#36259;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#26410;&#30693;&#30340;&#22810;&#20803;&#20989;&#25968;&#65292;&#23427;&#20195;&#34920;&#30528;&#19968;&#20010;&#31995;&#32479;&#65292;&#22914;&#19968;&#20010;&#22797;&#26434;&#30340;&#25968;&#20540;&#27169;&#25311;&#22120;&#65292;&#21516;&#26102;&#20855;&#26377;&#30830;&#23450;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20272;&#35745;&#30830;&#23450;&#24615;&#36755;&#20837;&#38598;&#65292;&#36825;&#20123;&#36755;&#20837;&#23548;&#33268;&#30340;&#36755;&#20986;&#65288;&#23601;&#19981;&#30830;&#23450;&#24615;&#36755;&#20837;&#30340;&#20998;&#24067;&#32780;&#35328;&#65289;&#23646;&#20110;&#32473;&#23450;&#38598;&#21512;&#30340;&#27010;&#29575;&#23567;&#20110;&#32473;&#23450;&#38408;&#20540;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#37327;&#21270;&#38598;&#21453;&#28436;&#65288;QSI&#65289;&#65292;&#20363;&#22914;&#22312;&#31283;&#20581;&#65288;&#22522;&#20110;&#21487;&#38752;&#24615;&#65289;&#20248;&#21270;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#24403;&#23547;&#25214;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#19988;&#20855;&#26377;&#36275;&#22815;&#22823;&#27010;&#29575;&#30340;&#35299;&#38598;&#26102;&#20250;&#21457;&#29983;&#12290;&#20026;&#20102;&#35299;&#20915;QSI&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#24314;&#27169;&#21644;&#36880;&#27493;&#19981;&#30830;&#23450;&#24615;&#20943;&#23569;&#65288;SUR&#65289;&#21407;&#29702;&#30340;&#36125;&#21494;&#26031;&#31574;&#30053;&#65292;&#20197;&#39034;&#24207;&#36873;&#25321;&#24212;&#35813;&#35780;&#20272;&#20989;&#25968;&#30340;&#28857;&#65292;&#20197;&#20415;&#39640;&#25928;&#36817;&#20284;&#24863;&#20852;&#36259;&#30340;&#38598;&#21512;&#12290;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;SUR&#31574;&#30053;&#30340;&#24615;&#33021;&#21644;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
We consider an unknown multivariate function representing a system-such as a complex numerical simulator-taking both deterministic and uncertain inputs. Our objective is to estimate the set of deterministic inputs leading to outputs whose probability (with respect to the distribution of the uncertain inputs) of belonging to a given set is less than a given threshold. This problem, which we call Quantile Set Inversion (QSI), occurs for instance in the context of robust (reliability-based) optimization problems, when looking for the set of solutions that satisfy the constraints with sufficiently large probability. To solve the QSI problem, we propose a Bayesian strategy based on Gaussian process modeling and the Stepwise Uncertainty Reduction (SUR) principle, to sequentially choose the points at which the function should be evaluated to efficiently approximate the set of interest. We illustrate the performance and interest of the proposed SUR strategy through several numerical experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32467;&#21512;&#27969;&#24418;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;&#65292;&#21033;&#29992;&#27969;&#24418;&#26679;&#26412;&#28857;&#20113;&#23450;&#20041;&#22270;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#26597;&#35810;&#28857;&#22312;&#27969;&#24418;&#19978;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#65292;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.10962</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#39640;&#26031;&#36807;&#31243;&#30340;&#27969;&#24418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimization on Manifolds via Graph Gaussian Processes. (arXiv:2210.10962v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32467;&#21512;&#27969;&#24418;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;&#65292;&#21033;&#29992;&#27969;&#24418;&#26679;&#26412;&#28857;&#20113;&#23450;&#20041;&#22270;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#26597;&#35810;&#28857;&#22312;&#27969;&#24418;&#19978;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#65292;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#19982;&#39640;&#26031;&#36807;&#31243;&#19978;&#38480;&#32622;&#20449;&#24230;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#20248;&#21270;&#27969;&#24418;&#19978;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#22312;&#26080;&#27861;&#33719;&#24471;&#23436;&#25972;&#27969;&#24418;&#34920;&#31034;&#19988;&#26597;&#35810;&#30446;&#26631;&#26114;&#36149;&#30340;&#24212;&#29992;&#22330;&#26223;&#32780;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#20381;&#38752;&#27969;&#24418;&#26679;&#26412;&#28857;&#20113;&#26469;&#23450;&#20041;&#29992;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#22270;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#12290;&#20351;&#29992;&#20808;&#21069;&#25152;&#26377;&#26597;&#35810;&#30340;&#21518;&#39564;&#20998;&#24067;&#36880;&#27493;&#36873;&#25321;&#26597;&#35810;&#28857;&#12290;&#25105;&#20204;&#22312;&#26597;&#35810;&#27425;&#25968;&#21644;&#28857;&#20113;&#22823;&#23567;&#26041;&#38754;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#38480;&#12290;&#25968;&#20540;&#23454;&#39564;&#34917;&#20805;&#20102;&#29702;&#35770;&#65292;&#24182;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper integrates manifold learning techniques within a \emph{Gaussian process upper confidence bound} algorithm to optimize an objective function on a manifold. Our approach is motivated by applications where a full representation of the manifold is not available and querying the objective is expensive. We rely on a point cloud of manifold samples to define a graph Gaussian process surrogate model for the objective. Query points are sequentially chosen using the posterior distribution of the surrogate model given all previous queries. We establish regret bounds in terms of the number of queries and the size of the point cloud. Several numerical examples complement the theory and illustrate the performance of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#24207;&#36143;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#24207;&#36143;&#25512;&#26029;&#31243;&#24207;&#26469;&#20272;&#35745;&#39044;&#27979;&#24471;&#20998;&#30340;&#26102;&#21464;&#24046;&#24322;&#65292;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#23545;&#39044;&#27979;&#21644;&#32467;&#26524;&#29983;&#25104;&#26041;&#24335;&#30340;&#19981;&#21487;&#39564;&#35777;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2110.00115</link><description>&lt;p&gt;
&#27604;&#36739;&#24207;&#36143;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Comparing Sequential Forecasters. (arXiv:2110.00115v5 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#24207;&#36143;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#24207;&#36143;&#25512;&#26029;&#31243;&#24207;&#26469;&#20272;&#35745;&#39044;&#27979;&#24471;&#20998;&#30340;&#26102;&#21464;&#24046;&#24322;&#65292;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#23545;&#39044;&#27979;&#21644;&#32467;&#26524;&#29983;&#25104;&#26041;&#24335;&#30340;&#19981;&#21487;&#39564;&#35777;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#20004;&#20010;&#39044;&#27979;&#22120;&#65292;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#23545;&#19968;&#31995;&#21015;&#20107;&#20214;&#36827;&#34892;&#21333;&#27425;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#30456;&#23545;&#22522;&#30784;&#30340;&#38382;&#39064;&#65306;&#22312;&#19981;&#20551;&#35774;&#39044;&#27979;&#21644;&#32467;&#26524;&#29983;&#25104;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22914;&#20309;&#27604;&#36739;&#36825;&#20123;&#39044;&#27979;&#22120;&#65292;&#26080;&#35770;&#26159;&#22312;&#32447;&#36824;&#26159;&#20107;&#21518;&#27604;&#36739;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#29992;&#20110;&#20272;&#35745;&#26102;&#21464;&#39044;&#27979;&#24471;&#20998;&#24046;&#24322;&#30340;&#26032;&#22411;&#24207;&#36143;&#25512;&#26029;&#31243;&#24207;&#65292;&#23545;&#36825;&#20010;&#38382;&#39064;&#32473;&#20986;&#20102;&#20005;&#26684;&#30340;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#32622;&#20449;&#24207;&#21015;&#65288;CS&#65289;&#65292;&#23427;&#26159;&#19968;&#31995;&#21015;&#32622;&#20449;&#21306;&#38388;&#65292;&#21487;&#20197;&#36830;&#32493;&#30417;&#27979;&#24182;&#22312;&#20219;&#24847;&#25968;&#25454;&#20381;&#36182;&#20572;&#26102;&#65288;&#8220;anytime-valid&#8221;&#65289;&#19979;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#32622;&#20449;&#24207;&#21015;&#30340;&#23485;&#24230;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#36866;&#24212;&#20102;&#24471;&#20998;&#24046;&#24322;&#30340;&#24213;&#23618;&#26041;&#24046;&#12290;&#23427;&#20204;&#30340;&#26500;&#24314;&#22522;&#20110;&#21338;&#24328;&#35770;&#32479;&#35745;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#29992;&#20110;&#39034;&#24207;&#26816;&#39564;&#24369;&#38646;&#20551;&#35774;&#30340;e&#36807;&#31243;&#21644;p&#36807;&#31243;&#65292;&#21363;&#19968;&#20010;&#39044;&#27979;&#22120;&#24179;&#22343;&#34920;&#29616;&#26159;&#21542;&#20248;&#20110;&#21478;&#19968;&#20010;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider two forecasters, each making a single prediction for a sequence of events over time. We ask a relatively basic question: how might we compare these forecasters, either online or post-hoc, while avoiding unverifiable assumptions on how the forecasts and outcomes were generated? In this paper, we present a rigorous answer to this question by designing novel sequential inference procedures for estimating the time-varying difference in forecast scores. To do this, we employ confidence sequences (CS), which are sequences of confidence intervals that can be continuously monitored and are valid at arbitrary data-dependent stopping times ("anytime-valid"). The widths of our CSs are adaptive to the underlying variance of the score differences. Underlying their construction is a game-theoretic statistical framework, in which we further identify e-processes and p-processes for sequentially testing a weak null hypothesis -- whether one forecaster outperforms another on average (rather tha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;Frank-Wolfe&#31639;&#27861;&#21464;&#20307;&#65292;&#21033;&#29992;&#24191;&#20041;&#33258;&#21327;&#35843;&#20989;&#25968;&#30340;&#29305;&#24615;&#65292;&#22312;&#19981;&#38656;&#35201;&#20351;&#29992;&#20108;&#38454;&#20449;&#24687;&#25110;&#20272;&#35745;&#23616;&#37096;&#24179;&#28369;&#24230;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;$\mathcal{O}(1/t)$&#30340;&#25910;&#25947;&#36895;&#24230;&#36798;&#21040;&#20102;&#20248;&#21270;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2105.13913</link><description>&lt;p&gt;
&#21482;&#38656;&#31616;&#21333;&#27493;&#39588;&#65306;Frank-Wolfe&#31639;&#27861;&#21644;&#24191;&#20041;&#33258;&#21327;&#35843;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Simple steps are all you need: Frank-Wolfe and generalized self-concordant functions. (arXiv:2105.13913v6 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.13913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;Frank-Wolfe&#31639;&#27861;&#21464;&#20307;&#65292;&#21033;&#29992;&#24191;&#20041;&#33258;&#21327;&#35843;&#20989;&#25968;&#30340;&#29305;&#24615;&#65292;&#22312;&#19981;&#38656;&#35201;&#20351;&#29992;&#20108;&#38454;&#20449;&#24687;&#25110;&#20272;&#35745;&#23616;&#37096;&#24179;&#28369;&#24230;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;$\mathcal{O}(1/t)$&#30340;&#25910;&#25947;&#36895;&#24230;&#36798;&#21040;&#20102;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#33258;&#21327;&#35843;&#26159;&#35768;&#22810;&#37325;&#35201;&#23398;&#20064;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;Frank-Wolfe&#21464;&#20307;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#35813;&#21464;&#20307;&#20351;&#29992;&#20102;&#24320;&#29615;&#27493;&#38271;&#31574;&#30053;$\gamma_t=2/(t+2)$&#65292;&#23545;&#20110;&#36825;&#31867;&#20989;&#25968;&#22312;&#21407;&#22987;&#38388;&#38553;&#21644;Frank-Wolfe&#38388;&#38553;&#26041;&#38754;&#33719;&#24471;&#20102;$\mathcal{O}(1/t)$&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$t$&#26159;&#36845;&#20195;&#27425;&#25968;&#12290;&#36825;&#36991;&#20813;&#20102;&#20351;&#29992;&#20108;&#38454;&#20449;&#24687;&#25110;&#38656;&#35201;&#20272;&#35745;&#20808;&#21069;&#24037;&#20316;&#30340;&#23616;&#37096;&#24179;&#28369;&#24230;&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19981;&#21516;&#24120;&#35265;&#24773;&#20917;&#19979;&#30340;&#25913;&#36827;&#25910;&#25947;&#36895;&#24230;&#65292;&#20363;&#22914;&#65292;&#24403;&#25152;&#32771;&#34385;&#30340;&#21487;&#34892;&#22495;&#26159;&#22343;&#21248;&#20984;&#30340;&#25110;&#32773;&#26159;&#22810;&#38754;&#20307;&#30340;&#26102;&#20505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized self-concordance is a key property present in the objective function of many important learning problems. We establish the convergence rate of a simple Frank-Wolfe variant that uses the open-loop step size strategy $\gamma_t = 2/(t+2)$, obtaining a $\mathcal{O}(1/t)$ convergence rate for this class of functions in terms of primal gap and Frank-Wolfe gap, where $t$ is the iteration count. This avoids the use of second-order information or the need to estimate local smoothness parameters of previous work. We also show improved convergence rates for various common cases, e.g., when the feasible region under consideration is uniformly convex or polyhedral.
&lt;/p&gt;</description></item></channel></rss>