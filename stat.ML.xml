<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#23558;&#26222;&#36890;GANs&#19982;&#27700;&#26031;&#22374;&#36317;&#31163;&#32852;&#31995;&#36215;&#26469;&#65292;&#25193;&#23637;&#29616;&#26377;&#27700;&#26031;&#22374;GANs&#32467;&#26524;&#21040;&#26222;&#36890;GANs&#65292;&#33719;&#24471;&#20102;&#26222;&#36890;GANs&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.15312</link><description>&lt;p&gt;
&#27700;&#26031;&#22374;&#35270;&#35282;&#19979;&#30340;&#26222;&#36890; GANs
&lt;/p&gt;
&lt;p&gt;
A Wasserstein perspective of Vanilla GANs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15312
&lt;/p&gt;
&lt;p&gt;
&#23558;&#26222;&#36890;GANs&#19982;&#27700;&#26031;&#22374;&#36317;&#31163;&#32852;&#31995;&#36215;&#26469;&#65292;&#25193;&#23637;&#29616;&#26377;&#27700;&#26031;&#22374;GANs&#32467;&#26524;&#21040;&#26222;&#36890;GANs&#65292;&#33719;&#24471;&#20102;&#26222;&#36890;GANs&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#30340;&#23454;&#35777;&#25104;&#21151;&#24341;&#36215;&#20102;&#23545;&#29702;&#35770;&#30740;&#31350;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#12290;&#32479;&#35745;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#27700;&#26031;&#22374;GANs&#21450;&#20854;&#25193;&#23637;&#19978;&#65292;&#29305;&#21035;&#26159;&#20801;&#35768;&#20855;&#26377;&#33391;&#22909;&#30340;&#38477;&#32500;&#29305;&#24615;&#12290;&#23545;&#20110;&#26222;&#36890;GANs&#65292;&#21363;&#21407;&#22987;&#20248;&#21270;&#38382;&#39064;&#65292;&#32479;&#35745;&#32467;&#26524;&#20173;&#28982;&#30456;&#24403;&#26377;&#38480;&#65292;&#38656;&#35201;&#20551;&#35774;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#21644;&#28508;&#31354;&#38388;&#19982;&#21608;&#22260;&#31354;&#38388;&#30340;&#32500;&#24230;&#30456;&#31561;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#26222;&#36890;GANs&#19982;&#27700;&#26031;&#22374;&#36317;&#31163;&#32852;&#31995;&#36215;&#26469;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#29616;&#26377;&#30340;&#27700;&#26031;&#22374;GANs&#32467;&#26524;&#21487;&#20197;&#25193;&#23637;&#21040;&#26222;&#36890;GANs&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#27700;&#26031;&#22374;&#36317;&#31163;&#20013;&#33719;&#24471;&#20102;&#26222;&#36890;GANs&#30340;&#31070;&#35861;&#19981;&#31561;&#24335;&#12290;&#36825;&#20010;&#31070;&#35861;&#19981;&#31561;&#24335;&#30340;&#20551;&#35774;&#26088;&#22312;&#30001;&#23454;&#36341;&#20013;&#24120;&#29992;&#30340;&#32593;&#32476;&#26550;&#26500;&#28385;&#36275;&#65292;&#22914;&#21069;&#39304;ReLU&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15312v1 Announce Type: cross  Abstract: The empirical success of Generative Adversarial Networks (GANs) caused an increasing interest in theoretical research. The statistical literature is mainly focused on Wasserstein GANs and generalizations thereof, which especially allow for good dimension reduction properties. Statistical results for Vanilla GANs, the original optimization problem, are still rather limited and require assumptions such as smooth activation functions and equal dimensions of the latent space and the ambient space. To bridge this gap, we draw a connection from Vanilla GANs to the Wasserstein distance. By doing so, existing results for Wasserstein GANs can be extended to Vanilla GANs. In particular, we obtain an oracle inequality for Vanilla GANs in Wasserstein distance. The assumptions of this oracle inequality are designed to be satisfied by network architectures commonly used in practice, such as feedforward ReLU networks. By providing a quantitative resu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20256;&#36798;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.15263</link><description>&lt;p&gt;
&#32852;&#37030;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65306;&#32479;&#35745;&#32858;&#21512;&#26041;&#27861;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20256;&#36798;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#31181;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#21644;&#20943;&#23569;&#19982;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#25972;&#21512;&#25110;&#34701;&#21512;&#20998;&#24067;&#24335;&#30830;&#23450;&#24615;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#65307;&#28982;&#32780;&#65292;&#29616;&#20195;&#30830;&#23450;&#24615;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#36890;&#24120;&#26657;&#20934;&#19981;&#20339;&#65292;&#32570;&#20047;&#22312;&#39044;&#27979;&#20013;&#20256;&#36798;&#19968;&#31181;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;&#36965;&#24863;&#24179;&#21488;&#21644;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#26159;&#29702;&#24819;&#30340;&#12290;&#30456;&#21453;&#65292;&#36125;&#21494;&#26031;DL&#27169;&#22411;&#36890;&#24120;&#26657;&#20934;&#33391;&#22909;&#65292;&#33021;&#22815;&#37327;&#21270;&#21644;&#20256;&#36798;&#19968;&#31181;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#20197;&#21450;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22240;&#20026;&#36125;&#21494;&#26031;DL&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#30001;&#27010;&#29575;&#20998;&#24067;&#23450;&#20041;&#65292;&#25152;&#20197;&#31616;&#21333;&#24212;&#29992;&#32858;&#21512;&#26041;&#27861;&#26159;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15263v1 Announce Type: new  Abstract: Federated learning (FL) is an approach to training machine learning models that takes advantage of multiple distributed datasets while maintaining data privacy and reducing communication costs associated with sharing local datasets. Aggregation strategies have been developed to pool or fuse the weights and biases of distributed deterministic models; however, modern deterministic deep learning (DL) models are often poorly calibrated and lack the ability to communicate a measure of epistemic uncertainty in prediction, which is desirable for remote sensing platforms and safety-critical applications. Conversely, Bayesian DL models are often well calibrated and capable of quantifying and communicating a measure of epistemic uncertainty along with a competitive prediction accuracy. Unfortunately, because the weights and biases in Bayesian DL models are defined by a probability distribution, simple application of the aggregation methods associa
&lt;/p&gt;</description></item><item><title>&#21452;&#20132;&#21449;&#22266;&#23450;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#38024;&#23545;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#39044;&#26399;&#26465;&#20214;&#21327;&#26041;&#24046;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#25286;&#20998;&#35757;&#32451;&#25968;&#25454;&#24182;&#22312;&#29420;&#31435;&#26679;&#26412;&#19978;&#19979;&#35843;nuisance&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#32467;&#26500;&#26080;&#20851;&#30340;&#38169;&#35823;&#20998;&#26512;&#20197;&#21450;&#26356;&#24378;&#20551;&#35774;&#30340;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#26356;&#31934;&#30830;&#30340;DCDR&#20272;&#35745;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.15175</link><description>&lt;p&gt;
&#21452;&#20132;&#21449;&#22266;&#23450;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#65306;&#36229;&#36234;&#20018;&#34892;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Double Cross-fit Doubly Robust Estimators: Beyond Series Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15175
&lt;/p&gt;
&lt;p&gt;
&#21452;&#20132;&#21449;&#22266;&#23450;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#38024;&#23545;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#39044;&#26399;&#26465;&#20214;&#21327;&#26041;&#24046;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#25286;&#20998;&#35757;&#32451;&#25968;&#25454;&#24182;&#22312;&#29420;&#31435;&#26679;&#26412;&#19978;&#19979;&#35843;nuisance&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#32467;&#26500;&#26080;&#20851;&#30340;&#38169;&#35823;&#20998;&#26512;&#20197;&#21450;&#26356;&#24378;&#20551;&#35774;&#30340;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#26356;&#31934;&#30830;&#30340;DCDR&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#36328;&#25311;&#21512;&#20132;&#21449;&#30340;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#22240;&#20854;&#33391;&#22909;&#30340;&#32467;&#26500;&#26080;&#20851;&#38169;&#35823;&#20445;&#35777;&#32780;&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#24403;&#23384;&#22312;&#39069;&#22806;&#32467;&#26500;&#65292;&#20363;&#22914;H\"{o}lder&#24179;&#28369;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#29420;&#31435;&#26679;&#26412;&#19978;&#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#25286;&#20998;&#21644;&#19979;&#35843;nuisance&#20989;&#25968;&#20272;&#35745;&#22120;&#26469;&#26500;&#24314;&#26356;&#31934;&#30830;&#30340;&#8220;&#21452;&#20132;&#21449;&#22266;&#23450;&#21452;&#31283;&#20581;&#8221;&#65288;DCDR&#65289;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#26399;&#26465;&#20214;&#21327;&#26041;&#24046;&#30340;DCDR&#20272;&#35745;&#22120;&#65292;&#22312;&#22240;&#26524;&#25512;&#26029;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#20013;&#26159;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#20989;&#25968;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#31995;&#21015;&#36880;&#28176;&#26356;&#24378;&#20551;&#35774;&#30340;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;DCDR&#20272;&#35745;&#22120;&#25552;&#20379;&#26080;&#38656;&#23545;nuisance&#20989;&#25968;&#25110;&#23427;&#20204;&#30340;&#20272;&#35745;&#22120;&#20570;&#20986;&#20551;&#35774;&#30340;&#32467;&#26500;&#26080;&#20851;&#38169;&#35823;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#20551;&#35774;nuisance&#20989;&#25968;&#26159;H\"{o}lder&#24179;&#28369;&#65292;&#20294;&#19981;&#20551;&#35774;&#30693;&#26195;&#30495;&#23454;&#24179;&#28369;&#32423;&#21035;&#25110;&#21327;&#21464;&#37327;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15175v1 Announce Type: cross  Abstract: Doubly robust estimators with cross-fitting have gained popularity in causal inference due to their favorable structure-agnostic error guarantees. However, when additional structure, such as H\"{o}lder smoothness, is available then more accurate "double cross-fit doubly robust" (DCDR) estimators can be constructed by splitting the training data and undersmoothing nuisance function estimators on independent samples. We study a DCDR estimator of the Expected Conditional Covariance, a functional of interest in causal inference and conditional independence testing, and derive a series of increasingly powerful results with progressively stronger assumptions. We first provide a structure-agnostic error analysis for the DCDR estimator with no assumptions on the nuisance functions or their estimators. Then, assuming the nuisance functions are H\"{o}lder smooth, but without assuming knowledge of the true smoothness level or the covariate densit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#21270;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30452;&#26041;&#22270;&#30340;&#32622;&#25442;&#19981;&#21464;&#32593;&#32476;HistNetQ&#65292;&#22312;&#37327;&#21270;&#27604;&#36187;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.15123</link><description>&lt;p&gt;
&#22522;&#20110;&#30452;&#26041;&#22270;&#30340;&#32622;&#25442;&#19981;&#21464;&#32593;&#32476;&#30340;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantification using Permutation-Invariant Networks based on Histograms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#21270;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30452;&#26041;&#22270;&#30340;&#32622;&#25442;&#19981;&#21464;&#32593;&#32476;HistNetQ&#65292;&#22312;&#37327;&#21270;&#27604;&#36187;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#65292;&#20063;&#31216;&#20026;&#31867;&#21035;&#26222;&#36941;&#24615;&#20272;&#35745;&#65292;&#26159;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#27169;&#22411;&#34987;&#35757;&#32451;&#29992;&#26469;&#39044;&#27979;&#32473;&#23450;&#26679;&#26412;&#38598;&#20013;&#27599;&#20010;&#31867;&#30340;&#26222;&#36941;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#21270;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#21487;&#20197;&#24212;&#29992;&#23545;&#31216;&#30417;&#30563;&#26041;&#27861;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#20998;&#31867;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#30340;&#38656;&#27714;&#65292;&#30452;&#25509;&#35299;&#20915;&#37327;&#21270;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#20026;&#38598;&#21512;&#22788;&#29702;&#35774;&#35745;&#30340;&#29616;&#26377;&#32622;&#25442;&#19981;&#21464;&#23618;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#37327;&#21270;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HistNetQ&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#23427;&#20381;&#36182;&#20110;&#22522;&#20110;&#30452;&#26041;&#22270;&#30340;&#32622;&#25442;&#19981;&#21464;&#34920;&#31034;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#37327;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#36804;&#20170;&#20026;&#27490;&#21807;&#19968;&#30340;&#37327;&#21270;&#31454;&#36187;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HistNetQ&#32988;&#36807;&#20854;&#20182;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15123v1 Announce Type: new  Abstract: Quantification, also known as class prevalence estimation, is the supervised learning task in which a model is trained to predict the prevalence of each class in a given bag of examples. This paper investigates the application of deep neural networks to tasks of quantification in scenarios where it is possible to apply a symmetric supervised approach that eliminates the need for classification as an intermediary step, directly addressing the quantification problem. Additionally, it discusses existing permutation-invariant layers designed for set processing and assesses their suitability for quantification. In light of our analysis, we propose HistNetQ, a novel neural architecture that relies on a permutation-invariant representation based on histograms that is specially suited for quantification problems. Our experiments carried out in the only quantification competition held to date, show that HistNetQ outperforms other deep neural arch
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;GroupSort&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#20272;&#35745;&#24182;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#24555;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15108</link><description>&lt;p&gt;
&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;GroupSort&#31070;&#32463;&#32593;&#32476;&#30340;&#22238;&#24402;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Regression based on Wasserstein distance and GroupSort Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15108
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;GroupSort&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26356;&#31934;&#30830;&#30340;&#20272;&#35745;&#24182;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#24555;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;Wasserstein&#20027;&#21160;&#22238;&#24402;&#27169;&#22411;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#21407;&#21017;&#65292;&#29992;&#20110;&#34913;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#20195;&#34920;&#24615;&#12290;&#20351;&#29992;GroupSort&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;Wasserstein&#36317;&#31163;&#12290;&#36825;&#20123;&#32593;&#32476;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#21487;&#20197;&#37327;&#21270;&#38169;&#35823;&#24182;&#26126;&#30830;&#20854;&#22823;&#23567;&#21644;&#28145;&#24230;&#12290;&#27492;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#20102;&#21478;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#23545;&#24322;&#24120;&#20540;&#26356;&#20855;&#23481;&#24525;&#24615;&#65292;&#20197;&#23436;&#25104;&#26597;&#35810;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#19982;&#20854;&#20182;&#32463;&#20856;&#21644;&#26368;&#36817;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#36825;&#31181;&#20195;&#34920;&#24615;-&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#30456;&#20851;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#25972;&#20010;&#26597;&#35810;&#36807;&#31243;&#20013;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;Wasserstein&#20027;&#21160;&#22238;&#24402;&#36890;&#24120;&#33021;&#22815;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20272;&#35745;&#65292;&#24182;&#19988;&#24448;&#24448;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#24555;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15108v1 Announce Type: new  Abstract: This paper addresses a new active learning strategy for regression problems. The presented Wasserstein active regression model is based on the principles of distribution-matching to measure the representativeness of the labeled dataset. The Wasserstein distance is computed using GroupSort Neural Networks. The use of such networks provides theoretical foundations giving a way to quantify errors with explicit bounds for their size and depth. This solution is combined with another uncertainty-based approach that is more outlier-tolerant to complete the query strategy. Finally, this method is compared with other classical and recent solutions. The study empirically shows the pertinence of such a representativity-uncertainty approach, which provides good estimation all along the query procedure. Moreover, the Wasserstein active regression often achieves more precise estimations and tends to improve accuracy faster than other models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20984;&#32452;&#21512;&#30340;&#26041;&#27861;&#20272;&#35745;&#39640;&#32500;&#31354;&#38388;&#20013;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#22810;&#32500;&#22343;&#20540;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#26435;&#37325;&#30830;&#23450;&#31574;&#30053;&#65306;&#19968;&#31181;&#36890;&#36807;&#27979;&#35797;&#31243;&#24207;&#35782;&#21035;&#20302;&#26041;&#24046;&#30340;&#30456;&#37051;&#22343;&#20540;&#65292;&#25552;&#20986;&#20102;&#23553;&#38381;&#24418;&#24335;&#25554;&#34917;&#20844;&#24335;&#65307;&#21478;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#20108;&#27425;&#39118;&#38505;&#30340;&#19978;&#32622;&#20449;&#30028;&#30830;&#23450;&#26435;&#37325;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#24471;&#20986;&#26041;&#27861;&#23545;&#32463;&#39564;&#22343;&#20540;&#30340;&#20108;&#27425;&#39118;&#38505;&#25913;&#36827;&#65292;&#22312;&#32500;&#24230;&#28176;&#36817;&#30340;&#35282;&#24230;&#19978;&#28176;&#36817;&#22320;&#25509;&#36817; Oracle&#65288;Minimax&#65289;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.15038</link><description>&lt;p&gt;
&#39640;&#32500;&#24773;&#20917;&#19979;&#22810;&#20010;&#22343;&#20540;&#21521;&#37327;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimation of multiple mean vectors in high dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15038
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20984;&#32452;&#21512;&#30340;&#26041;&#27861;&#20272;&#35745;&#39640;&#32500;&#31354;&#38388;&#20013;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#22810;&#32500;&#22343;&#20540;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#26435;&#37325;&#30830;&#23450;&#31574;&#30053;&#65306;&#19968;&#31181;&#36890;&#36807;&#27979;&#35797;&#31243;&#24207;&#35782;&#21035;&#20302;&#26041;&#24046;&#30340;&#30456;&#37051;&#22343;&#20540;&#65292;&#25552;&#20986;&#20102;&#23553;&#38381;&#24418;&#24335;&#25554;&#34917;&#20844;&#24335;&#65307;&#21478;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#20108;&#27425;&#39118;&#38505;&#30340;&#19978;&#32622;&#20449;&#30028;&#30830;&#23450;&#26435;&#37325;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#24471;&#20986;&#26041;&#27861;&#23545;&#32463;&#39564;&#22343;&#20540;&#30340;&#20108;&#27425;&#39118;&#38505;&#25913;&#36827;&#65292;&#22312;&#32500;&#24230;&#28176;&#36817;&#30340;&#35282;&#24230;&#19978;&#28176;&#36817;&#22320;&#25509;&#36817; Oracle&#65288;Minimax&#65289;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33268;&#21147;&#20110;&#22522;&#20110;&#29420;&#31435;&#26679;&#26412;&#22312;&#19968;&#20010;&#20849;&#21516;&#31354;&#38388;&#20013;&#20272;&#35745;&#26469;&#33258;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#22810;&#32500;&#22343;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#23545;&#36825;&#20123;&#26679;&#26412;&#23548;&#20986;&#30340;&#32463;&#39564;&#22343;&#20540;&#36827;&#34892;&#20984;&#32452;&#21512;&#26469;&#24418;&#25104;&#20272;&#35745;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#25214;&#21040;&#36866;&#24403;&#30340;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#20984;&#32452;&#21512;&#26435;&#37325;&#65306;&#31532;&#19968;&#31181;&#21033;&#29992;&#27979;&#35797;&#31243;&#24207;&#26469;&#35782;&#21035;&#20855;&#26377;&#20302;&#26041;&#24046;&#30340;&#30456;&#37051;&#22343;&#20540;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#20851;&#20110;&#26435;&#37325;&#30340;&#23553;&#38381;&#24418;&#24335;&#25554;&#34917;&#20844;&#24335;&#65307;&#31532;&#20108;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#20108;&#27425;&#39118;&#38505;&#30340;&#19978;&#32622;&#20449;&#21306;&#38388;&#26469;&#30830;&#23450;&#26435;&#37325;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#32463;&#39564;&#22343;&#20540;&#25552;&#20379;&#30340;&#20108;&#27425;&#39118;&#38505;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#38598;&#20013;&#22312;&#32500;&#24230;&#28176;&#36817;&#30340;&#35282;&#24230;&#19978;&#65292;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#30340;&#26377;&#25928;&#32500;&#24230;&#22686;&#21152;&#26102;&#28176;&#36817;&#22320;&#25509;&#36817;&#20110;&#19968;&#20010; Oracle&#65288;Minimax&#65289;&#25913;&#36827;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22343;&#20540;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15038v1 Announce Type: cross  Abstract: We endeavour to estimate numerous multi-dimensional means of various probability distributions on a common space based on independent samples. Our approach involves forming estimators through convex combinations of empirical means derived from these samples. We introduce two strategies to find appropriate data-dependent convex combination weights: a first one employing a testing procedure to identify neighbouring means with low variance, which results in a closed-form plug-in formula for the weights, and a second one determining weights via minimization of an upper confidence bound on the quadratic risk.Through theoretical analysis, we evaluate the improvement in quadratic risk offered by our methods compared to the empirical means. Our analysis focuses on a dimensional asymptotics perspective, showing that our methods asymptotically approach an oracle (minimax) improvement as the effective dimension of the data increases.We demonstrat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#29289;&#29702;&#20449;&#24687;&#25351;&#23548;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#31283;&#20581;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.15025</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#31283;&#20581;&#31526;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Conformal Prediction under Distribution Shift via Physics-Informed Structural Causal Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15025
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#29289;&#29702;&#20449;&#24687;&#25351;&#23548;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#31283;&#20581;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21487;&#38752;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290; &#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#36890;&#36807;&#39044;&#27979;&#27979;&#35797;&#36755;&#20837;&#30340;&#19968;&#20010;&#38598;&#21512;&#26469;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65292;&#24076;&#26395;&#35813;&#38598;&#21512;&#20197;&#33267;&#23569;$(1-\alpha)$&#30340;&#32622;&#20449;&#24230;&#35206;&#30422;&#30495;&#23454;&#26631;&#31614;&#12290; &#21363;&#20351;&#22312;&#26657;&#20934;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36793;&#32536;&#20998;&#24067; $P_X$ &#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#35206;&#30422;&#20063;&#21487;&#20197;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290; &#20294;&#26159;&#65292;&#23454;&#36341;&#20013;&#24456;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#65292;&#24403;&#26657;&#20934;&#21644;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#26465;&#20214;&#20998;&#24067; $P_{Y|X}$ &#19981;&#21516;&#26102;&#65292;&#35206;&#30422;&#23601;&#26080;&#27861;&#20445;&#35777;&#65292;&#22312;&#25152;&#26377;&#21487;&#33021;&#30340;&#32622;&#20449;&#27700;&#24179;&#19979;&#27979;&#37327;&#21644;&#26368;&#23567;&#21270;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#35206;&#30422;&#25439;&#22833;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#26657;&#20934;&#21644;&#27979;&#35797;&#31526;&#21512;&#20998;&#25968;&#30340;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#20197;&#21450;Wasserstein&#36317;&#31163;&#22312;&#21508;&#20010;&#27700;&#24179;&#19978;&#19978;&#30028;&#35206;&#30422;&#24046;&#24322;&#12290; &#21463;&#29289;&#29702;&#23398;&#22312;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#19981;&#21464;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#20449;&#24687;&#25351;&#23548;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15025v1 Announce Type: new  Abstract: Uncertainty is critical to reliable decision-making with machine learning. Conformal prediction (CP) handles uncertainty by predicting a set on a test input, hoping the set to cover the true label with at least $(1-\alpha)$ confidence. This coverage can be guaranteed on test data even if the marginal distributions $P_X$ differ between calibration and test datasets. However, as it is common in practice, when the conditional distribution $P_{Y|X}$ is different on calibration and test data, the coverage is not guaranteed and it is essential to measure and minimize the coverage loss under distributional shift at \textit{all} possible confidence levels. To address these issues, we upper bound the coverage difference at all levels using the cumulative density functions of calibration and test conformal scores and Wasserstein distance. Inspired by the invariance of physics across data distributions, we propose a physics-informed structural caus
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22810;&#28304;&#29615;&#22659;&#20013;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#26631;&#20934;K&#25240;&#20132;&#21449;&#39564;&#35777;&#21644;&#30041;&#20986;&#28304;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#26356;&#20840;&#38754;&#21644;&#30495;&#23454;&#30340;&#31934;&#30830;&#24230;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;</title><link>https://arxiv.org/abs/2403.15012</link><description>&lt;p&gt;
&#20020;&#24202;&#26426;&#22120;&#23398;&#20064;&#20013;&#22810;&#28304;&#20132;&#21449;&#39564;&#35777;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Empirical investigation of multi-source cross-validation in clinical machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22810;&#28304;&#29615;&#22659;&#20013;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#26631;&#20934;K&#25240;&#20132;&#21449;&#39564;&#35777;&#21644;&#30041;&#20986;&#28304;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#26356;&#20840;&#38754;&#21644;&#30495;&#23454;&#30340;&#31934;&#30830;&#24230;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#26159;&#22312;&#26469;&#33258;&#21333;&#19968;&#26469;&#28304;&#65288;&#22914;&#21307;&#38498;&#65289;&#30340;&#24739;&#32773;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#12290;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#21487;&#36890;&#36807;&#37325;&#22797;&#38543;&#26426;&#25286;&#20998;&#25968;&#25454;&#26469;&#20272;&#35745;&#36825;&#20123;&#27169;&#22411;&#22312;&#26469;&#33258;&#21516;&#19968;&#26469;&#28304;&#30340;&#26032;&#24739;&#32773;&#19978;&#30340;&#31934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#19982;&#37096;&#32626;&#27169;&#22411;&#21040;&#25968;&#25454;&#38598;&#20013;&#26410;&#20195;&#34920;&#30340;&#28304;&#22836;&#65288;&#22914;&#26032;&#21307;&#38498;&#65289;&#33719;&#24471;&#30340;&#31934;&#30830;&#24230;&#30456;&#27604;&#65292;&#36825;&#20123;&#20272;&#35745;&#24448;&#24448;&#36807;&#20110;&#20048;&#35266;&#12290;&#22810;&#28304;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#19981;&#26029;&#22686;&#21152;&#20026;&#36890;&#36807;&#22522;&#20110;&#28304;&#22836;&#30340;&#20132;&#21449;&#39564;&#35777;&#35774;&#35745;&#33719;&#24471;&#26356;&#20840;&#38754;&#21644;&#30495;&#23454;&#30340;&#39044;&#26399;&#31934;&#30830;&#24230;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15012v1 Announce Type: new  Abstract: Traditionally, machine learning-based clinical prediction models have been trained and evaluated on patient data from a single source, such as a hospital. Cross-validation methods can be used to estimate the accuracy of such models on new patients originating from the same source, by repeated random splitting of the data. However, such estimates tend to be highly overoptimistic when compared to accuracy obtained from deploying models to sources not represented in the dataset, such as a new hospital. The increasing availability of multi-source medical datasets provides new opportunities for obtaining more comprehensive and realistic evaluations of expected accuracy through source-level cross-validation designs.   In this study, we present a systematic empirical evaluation of standard K-fold cross-validation and leave-source-out cross-validation methods in a multi-source setting. We consider the task of electrocardiogram based cardiovascul
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#22810;&#27169;&#24577;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#24182;&#23581;&#35797;&#23558;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#25552;&#39640;&#24739;&#32773;&#21307;&#30103;&#21382;&#21490;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14926</link><description>&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#22810;&#27169;&#24577;&#20998;&#26512;&#19978;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning on Multimodal Analysis of Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14926
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#22810;&#27169;&#24577;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#24182;&#23581;&#35797;&#23558;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#25552;&#39640;&#24739;&#32773;&#21307;&#30103;&#21382;&#21490;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#31995;&#32479;&#21253;&#21547;&#22823;&#37327;&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#65292;&#21253;&#25324;&#32467;&#26500;&#21270;&#25968;&#25454;&#22914;&#20020;&#24202;&#32534;&#30721;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#22914;&#20020;&#24202;&#31508;&#35760;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#38024;&#23545;EHR&#30340;&#30740;&#31350;&#20256;&#32479;&#19978;&#35201;&#20040;&#38598;&#20013;&#20110;&#20010;&#21035;&#27169;&#24577;&#65292;&#35201;&#20040;&#20197;&#19968;&#31181;&#30456;&#24403;&#31895;&#31961;&#30340;&#26041;&#24335;&#21512;&#24182;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#23558;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#35270;&#20026;&#21333;&#29420;&#23454;&#20307;&#65292;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#22266;&#26377;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20004;&#20010;&#37325;&#35201;&#30340;&#27169;&#24577;&#21253;&#21547;&#20020;&#24202;&#30456;&#20851;&#12289;&#23494;&#20999;&#30456;&#20851;&#21644;&#20114;&#34917;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;&#36825;&#20004;&#31181;&#25968;&#25454;&#27169;&#24577;&#21487;&#20197;&#25429;&#25417;&#21040;&#24739;&#32773;&#21307;&#30103;&#21382;&#21490;&#30340;&#26356;&#23436;&#25972;&#30011;&#38754;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#22312;&#35270;&#35273;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#22810;&#27169;&#24577;EHR&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#22312;&#29702;&#35770;&#29702;&#35299;&#26041;&#38754;&#65292;&#20854;&#28508;&#21147;&#20173;&#26410;&#20805;&#20998;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14926v1 Announce Type: cross  Abstract: Electronic health record (EHR) systems contain a wealth of multimodal clinical data including structured data like clinical codes and unstructured data such as clinical notes. However, many existing EHR-focused studies has traditionally either concentrated on an individual modality or merged different modalities in a rather rudimentary fashion. This approach often results in the perception of structured and unstructured data as separate entities, neglecting the inherent synergy between them. Specifically, the two important modalities contain clinically relevant, inextricably linked and complementary health information. A more complete picture of a patient's medical history is captured by the joint analysis of the two modalities of data. Despite the great success of multimodal contrastive learning on vision-language, its potential remains under-explored in the realm of multimodal EHR, particularly in terms of its theoretical understandi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#27169;&#22411;&#20013;&#21033;&#29992;&#36741;&#21161;&#20449;&#24687;&#36827;&#34892;&#26377;&#22122;&#22768;&#30697;&#38453;&#34917;&#20840;&#30340;&#32479;&#35745;&#25512;&#26029;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14899</link><description>&lt;p&gt;
&#24102;&#26377;&#36741;&#21161;&#20449;&#24687;&#30340;&#26377;&#22122;&#22768;&#30697;&#38453;&#34917;&#20840;&#30340;&#32479;&#35745;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Statistical Inference For Noisy Matrix Completion Incorporating Auxiliary Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#27169;&#22411;&#20013;&#21033;&#29992;&#36741;&#21161;&#20449;&#24687;&#36827;&#34892;&#26377;&#22122;&#22768;&#30697;&#38453;&#34917;&#20840;&#30340;&#32479;&#35745;&#25512;&#26029;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#27169;&#22411;&#20013;&#20351;&#29992;&#36741;&#21161;&#21327;&#21464;&#37327;&#36827;&#34892;&#26377;&#22122;&#22768;&#30697;&#38453;&#34917;&#20840;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#20004;&#20010;&#37096;&#20998;&#12290;&#19968;&#20010;&#37096;&#20998;&#26159;&#30001;&#26410;&#35266;&#23519;&#21040;&#30340;&#28508;&#22312;&#22240;&#32032;&#35825;&#23548;&#30340;&#20302;&#31209;&#30697;&#38453;&#65307;&#21478;&#19968;&#20010;&#37096;&#20998;&#36890;&#36807;&#30001;&#39640;&#32500;&#21015;&#21521;&#37327;&#32452;&#25104;&#30340;&#31995;&#25968;&#30697;&#38453;&#26469;&#27169;&#25311;&#35266;&#23519;&#21040;&#30340;&#21327;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#36923;&#36753;&#22238;&#24402;&#23545;&#21709;&#24212;&#30340;&#35266;&#27979;&#27169;&#24335;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20801;&#35768;&#20854;&#27010;&#29575;&#38543;&#30528;&#26679;&#26412;&#37327;&#30340;&#22686;&#21152;&#32780;&#36235;&#20110;&#38646;&#12290;&#25105;&#20204;&#22312;&#32771;&#34385;&#30340;&#32972;&#26223;&#19979;&#24212;&#29992;&#20102;&#36845;&#20195;&#26368;&#23567;&#20108;&#20056;&#65288;LS&#65289;&#20272;&#35745;&#26041;&#27861;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36845;&#20195;LS&#26041;&#27861;&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#25512;&#23548;&#20986;&#25152;&#24471;&#20272;&#35745;&#37327;&#30340;&#32479;&#35745;&#24615;&#36136;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#23569;&#37327;&#36845;&#20195;&#65292;&#20197;&#21450;&#24471;&#21040;&#30340;&#36880;&#20010;&#26465;&#30446;&#30340;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#37327;&#21644;&#31995;&#25968;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14899v1 Announce Type: cross  Abstract: This paper investigates statistical inference for noisy matrix completion in a semi-supervised model when auxiliary covariates are available. The model consists of two parts. One part is a low-rank matrix induced by unobserved latent factors; the other part models the effects of the observed covariates through a coefficient matrix which is composed of high-dimensional column vectors. We model the observational pattern of the responses through a logistic regression of the covariates, and allow its probability to go to zero as the sample size increases. We apply an iterative least squares (LS) estimation approach in our considered context. The iterative LS methods in general enjoy a low computational cost, but deriving the statistical properties of the resulting estimators is a challenging task. We show that our method only needs a few iterations, and the resulting entry-wise estimators of the low-rank matrix and the coefficient matrix a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#22312;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#24212;&#29992;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.14830</link><description>&lt;p&gt;
&#28145;&#24230;&#32858;&#31867;&#35780;&#20272;&#65306;&#22914;&#20309;&#39564;&#35777;&#20869;&#37096;&#32858;&#31867;&#26377;&#25928;&#24615;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Clustering Evaluation: How to Validate Internal Clustering Validation Measures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#22312;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#24212;&#29992;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14830v1 &#36890;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#28145;&#24230;&#32858;&#31867;&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#22797;&#26434;&#12289;&#39640;&#32500;&#25968;&#25454;&#36827;&#34892;&#21010;&#20998;&#30340;&#26041;&#27861;&#65292;&#23427;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#35780;&#20272;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#32858;&#31867;&#39564;&#35777;&#26041;&#27861;&#65292;&#35774;&#35745;&#29992;&#20110;&#20302;&#32500;&#31354;&#38388;&#65292;&#23545;&#20110;&#28041;&#21450;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#36739;&#20302;&#32500;&#23884;&#20837;&#31354;&#38388;&#21518;&#20877;&#36827;&#34892;&#21010;&#20998;&#30340;&#28145;&#24230;&#32858;&#31867;&#26469;&#35828;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#35770;&#25991;&#30830;&#23450;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;1&#65289;&#22312;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#21407;&#22987;&#25968;&#25454;&#26102;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;2&#65289;&#30001;&#20110;&#19981;&#21516;&#32858;&#31867;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#21442;&#25968;&#35774;&#32622;&#30340;&#21464;&#21270;&#32780;&#23548;&#33268;&#19981;&#21516;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#32858;&#31867;&#32467;&#26524;&#26080;&#27861;&#21487;&#38752;&#27604;&#36739;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#24378;&#35843;&#22312;&#21407;&#22987;&#25968;&#25454;&#21644;&#23884;&#20837;&#25968;&#25454;&#19978;&#20351;&#29992;&#20869;&#37096;&#39564;&#35777;&#26041;&#27861;&#21487;&#33021;&#20986;&#29616;&#30340;&#26080;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#24212;&#29992;&#28145;&#24230;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14830v1 Announce Type: cross  Abstract: Deep clustering, a method for partitioning complex, high-dimensional data using deep neural networks, presents unique evaluation challenges. Traditional clustering validation measures, designed for low-dimensional spaces, are problematic for deep clustering, which involves projecting data into lower-dimensional embeddings before partitioning. Two key issues are identified: 1) the curse of dimensionality when applying these measures to raw data, and 2) the unreliable comparison of clustering results across different embedding spaces stemming from variations in training procedures and parameter settings in different clustering models. This paper addresses these challenges in evaluating clustering quality in deep learning. We present a theoretical framework to highlight ineffectiveness arising from using internal validation measures on raw and embedded data and propose a systematic approach to applying clustering validity indices in deep 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;P\'olya-Gamma&#38543;&#26426;&#21464;&#37327;&#21046;&#23450;VGPMIL&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#19982;&#21407;&#22987;VGPMIL&#30456;&#21516;&#30340;&#21464;&#20998;&#21518;&#39564;&#36817;&#20284;&#65292;&#36825;&#26159;&#21452;&#26354;&#27491;&#21106;&#20998;&#24067;&#25152;&#25215;&#35748;&#30340;&#20004;&#31181;&#34920;&#31034;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14829</link><description>&lt;p&gt;
&#36923;&#36753;&#20989;&#25968;&#30340;&#21452;&#26354;&#27491;&#21106;&#34920;&#31034;&#65306;&#22312;CT&#39045;&#20869;&#20986;&#34880;&#26816;&#27979;&#20013;&#30340;&#27010;&#29575;&#22810;&#31034;&#20363;&#23398;&#20064;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Secant representation of the logistic function: Application to probabilistic Multiple Instance Learning for CT intracranial hemorrhage detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14829
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;P\'olya-Gamma&#38543;&#26426;&#21464;&#37327;&#21046;&#23450;VGPMIL&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#19982;&#21407;&#22987;VGPMIL&#30456;&#21516;&#30340;&#21464;&#20998;&#21518;&#39564;&#36817;&#20284;&#65292;&#36825;&#26159;&#21452;&#26354;&#27491;&#21106;&#20998;&#24067;&#25152;&#25215;&#35748;&#30340;&#20004;&#31181;&#34920;&#31034;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31034;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26159;&#19968;&#31181;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#31185;&#23398;&#39046;&#22495;&#30340;&#24369;&#30417;&#30563;&#33539;&#24335;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21307;&#23398;&#25104;&#20687;&#12290;&#27010;&#29575;MIL&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#65292;&#30001;&#20110;&#20854;&#39640;&#34920;&#36798;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#24050;&#32463;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#26368;&#25104;&#21151;&#30340;&#22522;&#20110;GP&#30340;MIL&#26041;&#27861;&#20043;&#19968;&#65292;VGPMIL&#65292;&#20351;&#29992;&#21464;&#20998;&#36793;&#30028;&#22788;&#29702;&#36923;&#36753;&#20989;&#25968;&#30340;&#19981;&#21487;&#35299;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;P\'olya-Gamma&#38543;&#26426;&#21464;&#37327;&#26469;&#21046;&#23450;VGPMIL&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#19982;&#21407;&#22987;VGPMIL&#30456;&#21516;&#30340;&#21464;&#20998;&#21518;&#39564;&#36817;&#20284;&#65292;&#36825;&#26159;&#21452;&#26354;&#27491;&#21106;&#20998;&#24067;&#25152;&#25215;&#35748;&#30340;&#20004;&#31181;&#34920;&#31034;&#30340;&#32467;&#26524;&#12290;&#36825;&#23548;&#33268;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22522;&#20110;GP&#30340;MIL&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#21033;&#29992;&#38500;&#21452;&#26354;&#27491;&#21106;&#20197;&#22806;&#30340;&#20998;&#24067;&#65292;&#21487;&#20197;&#37319;&#21462;&#19981;&#21516;&#24418;&#24335;&#12290;&#20351;&#29992;Gamma&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14829v1 Announce Type: new  Abstract: Multiple Instance Learning (MIL) is a weakly supervised paradigm that has been successfully applied to many different scientific areas and is particularly well suited to medical imaging. Probabilistic MIL methods, and more specifically Gaussian Processes (GPs), have achieved excellent results due to their high expressiveness and uncertainty quantification capabilities. One of the most successful GP-based MIL methods, VGPMIL, resorts to a variational bound to handle the intractability of the logistic function. Here, we formulate VGPMIL using P\'olya-Gamma random variables. This approach yields the same variational posterior approximations as the original VGPMIL, which is a consequence of the two representations that the Hyperbolic Secant distribution admits. This leads us to propose a general GP-based MIL method that takes different forms by simply leveraging distributions other than the Hyperbolic Secant one. Using the Gamma distribution
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;Sinkhorn&#19981;&#30830;&#23450;&#24615;&#38598;&#35299;&#20915;&#38750;&#20984;&#40065;&#26834;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#30830;&#20999;&#30340;&#28151;&#21512;&#25972;&#25968;&#25351;&#25968;&#38181;&#37325;&#26500;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20248;&#20110;&#30446;&#21069;&#25991;&#29486;&#20013;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20984;&#36924;&#36817;&#12290;</title><link>https://arxiv.org/abs/2403.14822</link><description>&lt;p&gt;
&#20351;&#29992;Sinkhorn&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#38750;&#20984;&#40065;&#26834;&#20551;&#35774;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Non-Convex Robust Hypothesis Testing using Sinkhorn Uncertainty Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14822
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;Sinkhorn&#19981;&#30830;&#23450;&#24615;&#38598;&#35299;&#20915;&#38750;&#20984;&#40065;&#26834;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#30830;&#20999;&#30340;&#28151;&#21512;&#25972;&#25968;&#25351;&#25968;&#38181;&#37325;&#26500;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20248;&#20110;&#30446;&#21069;&#25991;&#29486;&#20013;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20984;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#38750;&#20984;&#40065;&#26834;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#23547;&#25214;&#26368;&#20248;&#25506;&#27979;&#22120;&#65292;&#20197;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#31867;&#22411;I&#21644;&#31867;&#22411;II&#39118;&#38505;&#20989;&#25968;&#30340;&#26368;&#22823;&#20540;&#12290;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;&#38598;&#22260;&#32469;&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#26679;&#26412;&#24471;&#20986;&#30340;&#32463;&#39564;&#20998;&#24067;&#26500;&#24314;&#12290;&#30001;&#20110;&#30446;&#26631;&#28041;&#21450;&#38750;&#20984;&#12289;&#38750;&#24179;&#28369;&#30340;&#27010;&#29575;&#20989;&#25968;&#65292;&#36890;&#24120;&#38590;&#20197;&#20248;&#21270;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#36817;&#20284;&#32780;&#38750;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30830;&#20999;&#30340;&#28151;&#21512;&#25972;&#25968;&#25351;&#25968;&#38181;&#37325;&#26500;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36866;&#37327;&#30340;&#36755;&#20837;&#25968;&#25454;&#19979;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#36924;&#36817;&#65292;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#24403;&#21069;&#25991;&#29486;&#20013;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#40065;&#26834;&#20551;&#35774;&#26816;&#39564;&#19982;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14822v1 Announce Type: cross  Abstract: We present a new framework to address the non-convex robust hypothesis testing problem, wherein the goal is to seek the optimal detector that minimizes the maximum of worst-case type-I and type-II risk functions. The distributional uncertainty sets are constructed to center around the empirical distribution derived from samples based on Sinkhorn discrepancy. Given that the objective involves non-convex, non-smooth probabilistic functions that are often intractable to optimize, existing methods resort to approximations rather than exact solutions. To tackle the challenge, we introduce an exact mixed-integer exponential conic reformulation of the problem, which can be solved into a global optimum with a moderate amount of input data. Subsequently, we propose a convex approximation, demonstrating its superiority over current state-of-the-art methodologies in literature. Furthermore, we establish connections between robust hypothesis testi
&lt;/p&gt;</description></item><item><title>&#23558;&#38477;&#32500;&#38382;&#39064;&#24314;&#27169;&#20026;&#26426;&#26800;/&#29289;&#29702;&#27169;&#22411;&#65292;&#24341;&#20837;&#26354;&#29575;&#22686;&#24378;&#21147;&#30340;&#26354;&#29575;&#22686;&#24378;&#27969;&#24418;&#23884;&#20837;&#19982;&#23398;&#20064;&#65288;CAMEL&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;&#25968;&#25454;&#38598;&#30340;n&#32500;&#27969;&#24418;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.14813</link><description>&lt;p&gt;
&#26354;&#29575;&#22686;&#24378;&#27969;&#24418;&#23884;&#20837;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curvature Augmented Manifold Embedding and Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14813
&lt;/p&gt;
&lt;p&gt;
&#23558;&#38477;&#32500;&#38382;&#39064;&#24314;&#27169;&#20026;&#26426;&#26800;/&#29289;&#29702;&#27169;&#22411;&#65292;&#24341;&#20837;&#26354;&#29575;&#22686;&#24378;&#21147;&#30340;&#26354;&#29575;&#22686;&#24378;&#27969;&#24418;&#23884;&#20837;&#19982;&#23398;&#20064;&#65288;CAMEL&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;&#25968;&#25454;&#38598;&#30340;n&#32500;&#27969;&#24418;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#34920;&#20110;arXiv:2403.14813v1&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38477;&#32500;&#65288;DR&#65289;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#26354;&#29575;&#22686;&#24378;&#27969;&#24418;&#23884;&#20837;&#19982;&#23398;&#20064;&#65288;CAMEL&#65289;&#12290;&#20854;&#20851;&#38190;&#21019;&#26032;&#36129;&#29486;&#22312;&#20110;&#23558;&#38477;&#32500;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#26426;&#26800;/&#29289;&#29702;&#27169;&#22411;&#65292;&#20854;&#20013;&#33410;&#28857;&#65288;&#25968;&#25454;&#28857;&#65289;&#20043;&#38388;&#30340;&#21147;&#22330;&#34987;&#29992;&#26469;&#25214;&#21040;&#25968;&#25454;&#38598;&#30340;n&#32500;&#27969;&#24418;&#34920;&#31034;&#12290;&#19982;&#35768;&#22810;&#29616;&#26377;&#30340;&#21560;&#24341;-&#25490;&#26021;&#21147;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#30340;&#19968;&#20010;&#29420;&#29305;&#36129;&#29486;&#26159;&#21253;&#21547;&#20102;&#19968;&#20010;&#38750;&#25104;&#23545;&#21147;&#12290;&#24341;&#20837;&#21644;&#35752;&#35770;&#20102;&#19968;&#31181;&#26032;&#30340;&#21147;&#22330;&#27169;&#22411;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#26230;&#26684;&#31890;&#23376;&#29289;&#29702;&#23398;&#20013;&#30340;&#22810;&#20307;&#21183;&#21644;&#25299;&#25169;&#23398;&#20013;&#30340;&#40654;&#26364;&#26354;&#29575;&#12290;CAMEL&#20013;&#21253;&#21547;&#20102;&#19968;&#31181;&#26354;&#29575;&#22686;&#24378;&#21147;&#12290;&#20854;&#27425;&#65292;&#25552;&#20379;&#20102;CAMEL&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#12289;&#30417;&#30563;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;/&#24230;&#37327;&#23398;&#20064;&#21644;&#36870;&#21521;&#23398;&#20064;&#30340;&#20844;&#24335;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#19982;&#29616;&#26377;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#23558;CAMEL&#24212;&#29992;&#20110;&#35768;&#22810;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14813v1 Announce Type: cross  Abstract: A new dimensional reduction (DR) and data visualization method, Curvature-Augmented Manifold Embedding and Learning (CAMEL), is proposed. The key novel contribution is to formulate the DR problem as a mechanistic/physics model, where the force field among nodes (data points) is used to find an n-dimensional manifold representation of the data sets. Compared with many existing attractive-repulsive force-based methods, one unique contribution of the proposed method is to include a non-pairwise force. A new force field model is introduced and discussed, inspired by the multi-body potential in lattice-particle physics and Riemann curvature in topology. A curvature-augmented force is included in CAMEL. Following this, CAMEL formulation for unsupervised learning, supervised learning, semi-supervised learning/metric learning, and inverse learning are provided. Next, CAMEL is applied to many benchmark datasets by comparing existing models, suc
&lt;/p&gt;</description></item><item><title>&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25918;&#23485;&#25110;&#29978;&#33267;&#22312;&#25490;&#38500;&#25152;&#26377;&#30456;&#20851;&#39118;&#38505;&#22240;&#32032;&#34987;&#35266;&#27979;&#21040;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#32473;&#20986;&#23545;&#39640;&#39118;&#38505;&#20010;&#20307;&#20998;&#37197;&#29575;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.14713</link><description>&lt;p&gt;
&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#22240;&#32032;&#19979;&#23457;&#35745;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Auditing Fairness under Unobserved Confounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14713
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25918;&#23485;&#25110;&#29978;&#33267;&#22312;&#25490;&#38500;&#25152;&#26377;&#30456;&#20851;&#39118;&#38505;&#22240;&#32032;&#34987;&#35266;&#27979;&#21040;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#32473;&#20986;&#23545;&#39640;&#39118;&#38505;&#20010;&#20307;&#20998;&#37197;&#29575;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#36328;&#36234;&#20154;&#21475;&#32479;&#35745;&#32447;&#23384;&#22312;&#19981;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#20844;&#24179;&#24615;&#21487;&#33021;&#38590;&#20197;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;&#22914;&#26524;&#25105;&#20204;&#23545;&#20844;&#24179;&#24615;&#30340;&#29702;&#35299;&#20381;&#36182;&#20110;&#38590;&#20197;&#34913;&#37327;&#30340;&#39118;&#38505;&#31561;&#35266;&#24565;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#37027;&#20123;&#27809;&#26377;&#20854;&#27835;&#30103;&#23601;&#20250;&#27515;&#20129;&#30340;&#20154;&#24179;&#31561;&#33719;&#24471;&#27835;&#30103;&#65289;&#12290;&#23457;&#35745;&#36825;&#31181;&#19981;&#20844;&#24179;&#24615;&#38656;&#35201;&#20934;&#30830;&#27979;&#37327;&#20010;&#20307;&#39118;&#38505;&#65292;&#32780;&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#65292;&#38590;&#20197;&#20272;&#35745;&#12290;&#22312;&#36825;&#20123;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#32032;&#8220;&#35299;&#37322;&#8221;&#26126;&#26174;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#33021;&#20302;&#20272;&#25110;&#39640;&#20272;&#19981;&#20844;&#24179;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25918;&#23485;&#25110;&#65288;&#20196;&#20154;&#24778;&#35766;&#22320;&#65289;&#29978;&#33267;&#22312;&#25490;&#38500;&#25152;&#26377;&#30456;&#20851;&#39118;&#38505;&#22240;&#32032;&#34987;&#35266;&#27979;&#21040;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#23545;&#39640;&#39118;&#38505;&#20010;&#20307;&#30340;&#20998;&#37197;&#29575;&#32473;&#20986;&#20449;&#24687;&#20016;&#23500;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22312;&#35768;&#22810;&#23454;&#38469;&#29615;&#22659;&#20013;&#65288;&#20363;&#22914;&#24341;&#20837;&#26032;&#22411;&#27835;&#30103;&#65289;&#25105;&#20204;&#25317;&#26377;&#22312;&#20219;&#20309;&#20998;&#37197;&#20043;&#21069;&#30340;&#25968;&#25454;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14713v1 Announce Type: cross  Abstract: A fundamental problem in decision-making systems is the presence of inequity across demographic lines. However, inequity can be difficult to quantify, particularly if our notion of equity relies on hard-to-measure notions like risk (e.g., equal access to treatment for those who would die without it). Auditing such inequity requires accurate measurements of individual risk, which is difficult to estimate in the realistic setting of unobserved confounding. In the case that these unobservables "explain" an apparent disparity, we may understate or overstate inequity. In this paper, we show that one can still give informative bounds on allocation rates among high-risk individuals, even while relaxing or (surprisingly) even when eliminating the assumption that all relevant risk factors are observed. We utilize the fact that in many real-world settings (e.g., the introduction of a novel treatment) we have data from a period prior to any alloc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.15213</link><description>&lt;p&gt;
&#32479;&#35745;&#26080;&#20559;&#22238;&#24402;&#65306;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#22238;&#24402;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Statistical Agnostic Regression: a machine learning method to validate regression models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#20998;&#26512;&#26159;&#32479;&#35745;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#65292;&#26088;&#22312;&#20272;&#35745;&#22240;&#21464;&#37327;&#65288;&#36890;&#24120;&#31216;&#20026;&#21709;&#24212;&#21464;&#37327;&#65289;&#19982;&#19968;&#20010;&#25110;&#22810;&#20010;&#33258;&#21464;&#37327;&#65288;&#21363;&#35299;&#37322;&#21464;&#37327;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32447;&#24615;&#22238;&#24402;&#26159;&#36804;&#20170;&#20026;&#27490;&#22312;&#39044;&#27979;&#12289;&#39044;&#27979;&#25110;&#22240;&#26524;&#25512;&#26029;&#31561;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#25191;&#34892;&#27492;&#20219;&#21153;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#12290;&#38500;&#20102;&#35299;&#20915;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#30340;&#21508;&#31181;&#20256;&#32479;&#26041;&#27861;&#22806;&#65292;&#22914;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#12289;&#23725;&#22238;&#24402;&#25110;&#22871;&#32034;&#22238;&#24402;&#8212;&#8212;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#26159;&#26356;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#30340;&#22522;&#30784;&#8212;&#8212;&#21518;&#32773;&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#20294;&#27809;&#26377;&#23545;&#32479;&#35745;&#26174;&#33879;&#24615;&#36827;&#34892;&#27491;&#24335;&#23450;&#20041;&#12290;&#26368;&#22810;&#65292;&#22522;&#20110;&#32463;&#39564;&#27979;&#37327;&#65288;&#22914;&#27531;&#24046;&#25110;&#20934;&#30830;&#24230;&#65289;&#36827;&#34892;&#32622;&#25442;&#25110;&#22522;&#20110;&#32463;&#20856;&#20998;&#26512;&#65292;&#20197;&#21453;&#26144;ML&#20272;&#35745;&#23545;&#26816;&#27979;&#30340;&#26356;&#39640;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#23545;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15213v1 Announce Type: cross  Abstract: Regression analysis is a central topic in statistical modeling, aiming to estimate the relationships between a dependent variable, commonly referred to as the response variable, and one or more independent variables, i.e., explanatory variables. Linear regression is by far the most popular method for performing this task in several fields of research, such as prediction, forecasting, or causal inference. Beyond various classical methods to solve linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso regressions - which are often the foundation for more advanced machine learning (ML) techniques - the latter have been successfully applied in this scenario without a formal definition of statistical significance. At most, permutation or classical analyses based on empirical measures (e.g., residuals or accuracy) have been conducted to reflect the greater ability of ML estimations for detection. In this paper, we introd
&lt;/p&gt;</description></item><item><title>&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#25968;&#23398;&#26426;&#36935;&#38656;&#35201;&#22522;&#30784;&#25968;&#23398;&#36827;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25968;&#23383;&#23402;&#29983;&#20174;&#29305;&#23450;&#29616;&#23454;&#20986;&#21457;&#65292;&#38656;&#35201;&#22810;&#23610;&#24230;&#24314;&#27169;&#21644;&#32806;&#21512;&#65292;&#36890;&#36807;&#20256;&#24863;&#22120;&#23558;&#25968;&#25454;&#36755;&#20837;&#65292;&#24110;&#21161;&#20154;&#31867;&#20570;&#20986;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.10326</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#25968;&#23398;&#26426;&#36935;&#65288;MATH-DT&#65289;
&lt;/p&gt;
&lt;p&gt;
Mathematical Opportunities in Digital Twins (MATH-DT)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10326
&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#25968;&#23398;&#26426;&#36935;&#38656;&#35201;&#22522;&#30784;&#25968;&#23398;&#36827;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25968;&#23383;&#23402;&#29983;&#20174;&#29305;&#23450;&#29616;&#23454;&#20986;&#21457;&#65292;&#38656;&#35201;&#22810;&#23610;&#24230;&#24314;&#27169;&#21644;&#32806;&#21512;&#65292;&#36890;&#36807;&#20256;&#24863;&#22120;&#23558;&#25968;&#25454;&#36755;&#20837;&#65292;&#24110;&#21161;&#20154;&#31867;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#25551;&#36848;&#20102;2023&#24180;12&#26376;11&#26085;&#33267;13&#26085;&#22312;&#20052;&#27835;&#26757;&#26862;&#22823;&#23398;&#20030;&#21150;&#30340;&#8220;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#25968;&#23398;&#26426;&#36935;&#8221;&#65288;MATH-DT&#65289;&#30740;&#35752;&#20250;&#30340;&#35752;&#35770;&#12290;&#25253;&#21578;&#25351;&#20986;&#65292;&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#38656;&#35201;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#22522;&#30784;&#25968;&#23398;&#36827;&#23637;&#12290;&#20256;&#32479;&#27169;&#22411;&#22312;&#29983;&#29289;&#23398;&#12289;&#29289;&#29702;&#23398;&#12289;&#24037;&#31243;&#23398;&#25110;&#21307;&#23398;&#20013;&#36215;&#22987;&#20110;&#36890;&#29992;&#29289;&#29702;&#23450;&#24459;&#65288;&#20363;&#22914;&#26041;&#31243;&#65289;&#65292;&#36890;&#24120;&#26159;&#23545;&#29616;&#23454;&#30340;&#31616;&#21270;&#12290;&#25968;&#23383;&#23402;&#29983;&#21017;&#20174;&#29305;&#23450;&#30340;&#29983;&#24577;&#31995;&#32479;&#12289;&#29289;&#20307;&#25110;&#20010;&#20154;&#65288;&#20363;&#22914;&#20010;&#24615;&#21270;&#21307;&#30103;&#65289;&#20316;&#20026;&#29616;&#23454;&#20986;&#21457;&#65292;&#38656;&#35201;&#22810;&#23610;&#24230;&#12289;&#22810;&#29289;&#29702;&#24314;&#27169;&#21644;&#32806;&#21512;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#36807;&#31243;&#22312;&#27169;&#25311;&#21644;&#24314;&#27169;&#27969;&#31243;&#30340;&#20004;&#31471;&#24320;&#22987;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#21487;&#38752;&#24615;&#26631;&#20934;&#21644;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25968;&#23383;&#23402;&#29983;&#24110;&#21161;&#20154;&#31867;&#20026;&#29289;&#29702;&#31995;&#32479;&#20570;&#20986;&#20915;&#31574;&#65292;&#20854;&#36890;&#36807;&#20256;&#24863;&#22120;&#23558;&#25968;&#25454;&#20256;&#36755;&#21040;&#25968;&#23383;&#23402;&#29983;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10326v1 Announce Type: cross  Abstract: The report describes the discussions from the Workshop on Mathematical Opportunities in Digital Twins (MATH-DT) from December 11-13, 2023, George Mason University.   It illustrates that foundational Mathematical advances are required for Digital Twins (DTs) that are different from traditional approaches. A traditional model, in biology, physics, engineering or medicine, starts with a generic physical law (e.g., equations) and is often a simplification of reality. A DT starts with a specific ecosystem, object or person (e.g., personalized care) representing reality, requiring multi -scale, -physics modeling and coupling. Thus, these processes begin at opposite ends of the simulation and modeling pipeline, requiring different reliability criteria and uncertainty assessments. Additionally, unlike existing approaches, a DT assists humans to make decisions for the physical system, which (via sensors) in turn feeds data into the DT, and oper
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.16427</link><description>&lt;p&gt;
&#29420;&#31435;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning to Embed Time Series Patches Independently
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16427
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25513;&#30721;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21551;&#21457;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#39318;&#20808;&#23558;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#22359;&#22788;&#29702;&#24182;&#37096;&#20998;&#25513;&#30422;&#65292;&#28982;&#21518;&#35757;&#32451;Transformer&#27169;&#22411;&#36890;&#36807;&#20174;&#26410;&#25513;&#30422;&#30340;&#22359;&#39044;&#27979;&#34987;&#25513;&#30422;&#22359;&#26469;&#25429;&#25417;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#25429;&#25417;&#36825;&#31181;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21487;&#33021;&#19981;&#26159;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#31574;&#30053;&#65307;&#30456;&#21453;&#65292;&#29420;&#31435;&#23398;&#20064;&#23884;&#20837;&#29255;&#27573;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;1&#65289;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#65292;&#33258;&#21160;&#23558;&#27599;&#20010;&#22359;&#36827;&#34892;&#32534;&#30721;&#32780;&#19981;&#26597;&#30475;&#20854;&#20182;&#22359;&#65292;&#20197;&#21450;2&#65289;&#29420;&#33258;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;&#31616;&#21333;&#22359;&#24335;MLP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#26377;&#25928;&#22320;&#20998;&#23618;&#25429;&#33719;&#30456;&#37051;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16427v2 Announce Type: replace-cross  Abstract: Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train Transformers to capture the dependencies between patches by predicting masked patches from unmasked patches. However, we argue that capturing such patch dependencies might not be an optimal strategy for time series representation learning; rather, learning to embed patches independently results in better time series representations. Specifically, we propose to use 1) the simple patch reconstruction task, which autoencode each patch without looking at other patches, and 2) the simple patch-wise MLP that embeds each patch independently. In addition, we introduce complementary contrastive learning to hierarchically capture adjacent time series information efficiently. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftCLT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#36719;&#23545;&#27604;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#24573;&#30053;&#22266;&#26377;&#30456;&#20851;&#24615;&#25152;&#23548;&#33268;&#30340;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.16424</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#36719;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Soft Contrastive Learning for Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftCLT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#36719;&#23545;&#27604;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#24573;&#30053;&#22266;&#26377;&#30456;&#20851;&#24615;&#25152;&#23548;&#33268;&#30340;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23545;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#34920;&#31034;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30456;&#20284;&#30340;&#23454;&#20363;&#25110;&#30456;&#37051;&#26102;&#38388;&#25139;&#30340;&#20540;&#36827;&#34892;&#23545;&#27604;&#20250;&#24573;&#30053;&#23427;&#20204;&#22266;&#26377;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SoftCLT&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#36719;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#12290;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#20174;&#38646;&#21040;&#19968;&#30340;&#36719;&#36171;&#20540;&#30340;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#23545;&#27604;&#25439;&#22833;&#26469;&#23454;&#29616;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20026;1)&#22522;&#20110;&#25968;&#25454;&#31354;&#38388;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#23450;&#20041;&#20102;&#23454;&#20363;&#32423;&#23545;&#27604;&#25439;&#22833;&#30340;&#36719;&#36171;&#20540;&#65292;&#24182;&#20026;2)&#22522;&#20110;&#26102;&#38388;&#25139;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#20102;&#26102;&#38388;&#32423;&#23545;&#27604;&#25439;&#22833;&#12290;SoftCLT&#26159;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#27809;&#26377;&#36807;&#22810;&#22797;&#26434;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16424v2 Announce Type: replace-cross  Abstract: Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way. However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating the quality of learned representations. To address this issue, we propose SoftCLT, a simple yet effective soft contrastive learning strategy for time series. This is achieved by introducing instance-wise and temporal contrastive loss with soft assignments ranging from zero to one. Specifically, we define soft assignments for 1) instance-wise contrastive loss by the distance between time series on the data space, and 2) temporal contrastive loss by the difference of timestamps. SoftCLT is a plug-and-play method for time series contrastive learning that improves the quality of learned representations without bells and whistles. In experi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#25918;&#26494;&#31561;&#21464;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35299;&#20915;&#31561;&#21464;&#20989;&#25968;&#26080;&#27861;&#22312;&#21333;&#20010;&#25968;&#25454;&#26679;&#26412;&#23618;&#38754;&#25171;&#30772;&#23545;&#31216;&#30340;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#24212;&#29992;&#20110;&#31561;&#21464;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;E-MLP&#65289;&#20013;&#12290;</title><link>https://arxiv.org/abs/2312.09016</link><description>&lt;p&gt;
&#23545;&#31216;&#30772;&#32570;&#21644;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Symmetry Breaking and Equivariant Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09016
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#25918;&#26494;&#31561;&#21464;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35299;&#20915;&#31561;&#21464;&#20989;&#25968;&#26080;&#27861;&#22312;&#21333;&#20010;&#25968;&#25454;&#26679;&#26412;&#23618;&#38754;&#25171;&#30772;&#23545;&#31216;&#30340;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#24212;&#29992;&#20110;&#31561;&#21464;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;E-MLP&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20351;&#29992;&#23545;&#31216;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35774;&#35745;&#20986;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#31216;&#21644;&#31561;&#21464;&#24615;&#30340;&#20851;&#31995;&#24182;&#19981;&#24635;&#26159;&#26174;&#32780;&#26131;&#35265;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#31561;&#21464;&#20989;&#25968;&#20013;&#20986;&#29616;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#23427;&#20204;&#19981;&#33021;&#22312;&#21333;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#23618;&#38754;&#25171;&#30772;&#23545;&#31216;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#8220;&#25918;&#26494;&#31561;&#21464;&#24615;&#8221;&#30340;&#27010;&#24565;&#26469;&#35268;&#36991;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#31181;&#25918;&#26494;&#24341;&#20837;&#31561;&#21464;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;E-MLP&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#27880;&#20837;&#22122;&#22768;&#30340;&#26041;&#27861;&#12290;&#25509;&#30528;&#35752;&#35770;&#20102;&#23545;&#31216;&#30772;&#32570;&#22312;&#29289;&#29702;&#12289;&#22270;&#34920;&#31034;&#23398;&#20064;&#12289;&#32452;&#21512;&#20248;&#21270;&#21644;&#31561;&#21464;&#35299;&#30721;&#31561;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09016v2 Announce Type: replace  Abstract: Using symmetry as an inductive bias in deep learning has been proven to be a principled approach for sample-efficient model design. However, the relationship between symmetry and the imperative for equivariance in neural networks is not always obvious. Here, we analyze a key limitation that arises in equivariant functions: their incapacity to break symmetry at the level of individual data samples. In response, we introduce a novel notion of 'relaxed equivariance' that circumvents this limitation. We further demonstrate how to incorporate this relaxation into equivariant multilayer perceptrons (E-MLPs), offering an alternative to the noise-injection method. The relevance of symmetry breaking is then discussed in various application domains: physics, graph representation learning, combinatorial optimization and equivariant decoding.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2311.10270</link><description>&lt;p&gt;
&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#30340;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multiscale Hodge Scattering Networks for Data Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25955;&#23556;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#27979;&#37327;&#30340;&#20449;&#21495;&#65292;&#31216;&#20026;\emph{&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;}&#65288;MHSNs&#65289;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#22522;&#20110;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#65292;&#21363;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#65292;&#25105;&#20204;&#26368;&#36817;&#20026;&#32473;&#23450;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#20013;&#30340;&#32500;&#24230;$\kappa \in \mathbb{N}$&#25512;&#24191;&#20102;&#22522;&#20110;&#33410;&#28857;&#30340;&#24191;&#20041;&#21704;-&#27779;&#20160;&#21464;&#25442;&#65288;GHWT&#65289;&#21644;&#20998;&#23618;&#22270;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21464;&#25442;&#65288;HGLET&#65289;&#12290;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#37117;&#24418;&#25104;&#20887;&#20313;&#38598;&#21512;&#65288;&#21363;&#35789;&#20856;&#65289;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#21521;&#37327;&#21644;&#32473;&#23450;&#20449;&#21495;&#30340;&#30456;&#24212;&#25193;&#23637;&#31995;&#25968;&#12290;&#25105;&#20204;&#30340;MHSNs&#20351;&#29992;&#31867;&#20284;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20998;&#23618;&#32467;&#26500;&#26469;&#32423;&#32852;&#35789;&#20856;&#31995;&#25968;&#27169;&#30340;&#30697;&#12290;&#25152;&#24471;&#29305;&#24449;&#23545;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#30340;&#37325;&#26032;&#25490;&#24207;&#19981;&#21464;&#65288;&#21363;&#33410;&#28857;&#25490;&#21015;&#30340;&#32622;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10270v2 Announce Type: replace  Abstract: We propose new scattering networks for signals measured on simplicial complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs). Our construction is based on multiscale basis dictionaries on simplicial complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently developed for simplices of dimension $\kappa \in \mathbb{N}$ in a given simplicial complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\kappa$-GHWT and the $\kappa$-HGLET both form redundant sets (i.e., dictionaries) of multiscale basis vectors and the corresponding expansion coefficients of a given signal. Our MHSNs use a layered structure analogous to a convolutional neural network (CNN) to cascade the moments of the modulus of the dictionary coefficients. The resulting features are invariant to reordering of the simplices (i.e., node permutation of the u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#20027;&#37319;&#26679;&#31354;&#38388;&#20013;&#23398;&#20064;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#28210;&#26579;&#31639;&#27861;&#20013;&#30340;&#26041;&#24046;&#32553;&#20943;&#12290;</title><link>https://arxiv.org/abs/1808.07840</link><description>&lt;p&gt;
&#22312;&#20027;&#37319;&#26679;&#31354;&#38388;&#20013;&#23398;&#20064;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Learning to Importance Sample in Primary Sample Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1808.07840
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#20027;&#37319;&#26679;&#31354;&#38388;&#20013;&#23398;&#20064;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#28210;&#26579;&#31639;&#27861;&#20013;&#30340;&#26041;&#24046;&#32553;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#37319;&#26679;&#26159;&#33945;&#29305;&#21345;&#27931;&#28210;&#26579;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#24046;&#32553;&#20943;&#31574;&#30053;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22914;&#20309;&#20174;&#19968;&#32452;&#26679;&#26412;&#34920;&#31034;&#30340;&#30446;&#26631;&#23494;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29616;&#26377;&#30340;&#33945;&#29305;&#21345;&#27931;&#28210;&#26579;&#31639;&#27861;&#35270;&#20026;&#40657;&#21283;&#23376;&#12290;&#22312;&#22330;&#26223;&#30456;&#20851;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23398;&#20064;&#22312;&#28210;&#26579;&#31639;&#27861;&#30340;&#20027;&#37319;&#26679;&#31354;&#38388;&#20013;&#29983;&#25104;&#20855;&#26377;&#30446;&#26631;&#23494;&#24230;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#35774;&#35745;&#29992;&#20110;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#34920;&#31034;&#23454;&#20540;&#38750;&#20307;&#31215;&#20445;&#25345;&#65288;'Real NVP'&#65289;&#21464;&#25442;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;Real NVP&#26469;&#38750;&#32447;&#24615;&#25197;&#26354;&#20027;&#37319;&#26679;&#31354;&#38388;&#24182;&#33719;&#24471;&#26399;&#26395;&#30340;&#23494;&#24230;&#12290;&#27492;&#22806;&#65292;Real NVP&#26377;&#25928;&#22320;&#35745;&#31639;&#20102;&#25197;&#26354;&#30340;&#38597;&#21487;&#27604;&#34892;&#21015;&#24335;&#65292;&#36825;&#26159;&#23454;&#29616;&#31215;&#20998;&#21464;&#25442;&#25152;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1808.07840v2 Announce Type: replace  Abstract: Importance sampling is one of the most widely used variance reduction strategies in Monte Carlo rendering. In this paper, we propose a novel importance sampling technique that uses a neural network to learn how to sample from a desired density represented by a set of samples. Our approach considers an existing Monte Carlo rendering algorithm as a black box. During a scene-dependent training phase, we learn to generate samples with a desired density in the primary sample space of the rendering algorithm using maximum likelihood estimation. We leverage a recent neural network architecture that was designed to represent real-valued non-volume preserving ('Real NVP') transformations in high dimensional spaces. We use Real NVP to non-linearly warp primary sample space and obtain desired densities. In addition, Real NVP efficiently computes the determinant of the Jacobian of the warp, which is required to implement the change of integratio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16512</link><description>&lt;p&gt;
&#20174;&#22797;&#26434;&#21040;&#28165;&#26224;&#65306;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#20998;&#26512;&#34920;&#36798;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#65288;Clifford&#65289;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20351;&#29992;&#26631;&#20934;&#27491;&#21017;&#21270;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#30001;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#32473;&#20986;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#38382;&#39064;&#21487;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#22312;&#20854;&#20013;&#32534;&#30721;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#35813;&#32467;&#26500;&#20197;&#25968;&#25454;&#21521;&#37327;&#29983;&#25104;&#30340;&#19977;&#35282;&#24418;&#21644;&#24179;&#34892;&#20307;&#30340;&#26377;&#31526;&#21495;&#20307;&#31215;&#34920;&#31034;&#12290;&#20984;&#38382;&#39064;&#36890;&#36807;$\ell_1$&#27491;&#21017;&#21270;&#25214;&#21040;&#26679;&#26412;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#20197;&#21457;&#29616;&#20165;&#30456;&#20851;&#30340;&#26964;&#31215;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#23618;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
&lt;/p&gt;</description></item><item><title>CARE&#26041;&#27861;&#36890;&#36807;&#31934;&#30830;&#25351;&#23450;&#32452;&#25104;&#25968;&#25454;&#30340;&#31934;&#30830;&#30697;&#38453;&#65292;&#24182;&#21033;&#29992;&#20854;&#19982;&#22522;&#30784;&#30697;&#38453;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#31232;&#30095;&#22522;&#30784;&#30697;&#38453;&#30340;&#32452;&#25104;&#25968;&#25454;&#20272;&#35745;&#26041;&#27861;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#36275;&#22815;&#39640;&#30340;&#32500;&#24230;&#19979;&#65292;CARE&#20272;&#35745;&#22120;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#39118;&#38505;&#30340;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.06985</link><description>&lt;p&gt;
CARE: &#22823;&#35268;&#27169;&#31934;&#30830;&#30697;&#38453;&#20272;&#35745;&#29992;&#20110;&#32452;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
CARE: Large Precision Matrix Estimation for Compositional Data. (arXiv:2309.06985v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06985
&lt;/p&gt;
&lt;p&gt;
CARE&#26041;&#27861;&#36890;&#36807;&#31934;&#30830;&#25351;&#23450;&#32452;&#25104;&#25968;&#25454;&#30340;&#31934;&#30830;&#30697;&#38453;&#65292;&#24182;&#21033;&#29992;&#20854;&#19982;&#22522;&#30784;&#30697;&#38453;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#31232;&#30095;&#22522;&#30784;&#30697;&#38453;&#30340;&#32452;&#25104;&#25968;&#25454;&#20272;&#35745;&#26041;&#27861;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#36275;&#22815;&#39640;&#30340;&#32500;&#24230;&#19979;&#65292;CARE&#20272;&#35745;&#22120;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#39118;&#38505;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#32452;&#25104;&#25968;&#25454;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#12290;&#31616;&#21333;&#24418;&#24335;&#30340;&#32422;&#26463;&#23545;&#20110;&#25512;&#26029;&#32452;&#25104;&#25968;&#25454;&#20013;&#30340;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65292;&#21363;&#22823;&#35268;&#27169;&#31934;&#30830;&#30697;&#38453;&#25152;&#32534;&#30721;&#30340;&#32452;&#20998;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24102;&#26469;&#20102;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32452;&#25104;&#31934;&#30830;&#30697;&#38453;&#30340;&#31934;&#30830;&#23450;&#20041;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#22522;&#30784;&#23545;&#24212;&#29289;&#32852;&#31995;&#36215;&#26469;&#65292;&#22312;&#36866;&#24403;&#30340;&#31232;&#30095;&#24615;&#20551;&#35774;&#19979;&#24471;&#21040;&#28176;&#36817;&#21487;&#36776;&#35748;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#20272;&#35745;&#31232;&#30095;&#22522;&#30784;&#31934;&#30830;&#30697;&#38453;&#30340;&#32452;&#25104;&#36866;&#24212;&#27491;&#21017;&#21270;&#20272;&#35745;&#65288;CARE&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20272;&#35745;&#22120;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#25903;&#25345;&#24674;&#22797;&#21644;&#25968;&#25454;&#39537;&#21160;&#21442;&#25968;&#35843;&#25972;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#25581;&#31034;&#20102;&#37492;&#23450;&#21644;&#20272;&#35745;&#20043;&#38388;&#30340;&#26377;&#36259;&#26435;&#34913;&#65292;&#20174;&#32780;&#31361;&#26174;&#20102;&#32500;&#24230;&#22312;&#32452;&#25104;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20248;&#21183;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#36275;&#22815;&#39640;&#30340;&#32500;&#24230;&#19979;&#65292;CARE&#20272;&#35745;&#22120;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#39118;&#38505;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional compositional data are prevalent in many applications. The simplex constraint poses intrinsic challenges to inferring the conditional dependence relationships among the components forming a composition, as encoded by a large precision matrix. We introduce a precise specification of the compositional precision matrix and relate it to its basis counterpart, which is shown to be asymptotically identifiable under suitable sparsity assumptions. By exploiting this connection, we propose a composition adaptive regularized estimation (CARE) method for estimating the sparse basis precision matrix. We derive rates of convergence for the estimator and provide theoretical guarantees on support recovery and data-driven parameter tuning. Our theory reveals an intriguing trade-off between identification and estimation, thereby highlighting the blessing of dimensionality in compositional data analysis. In particular, in sufficiently high dimensions, the CARE estimator achieves minimax
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#23725;&#22238;&#24402;&#20013;&#36807;&#25311;&#21512;&#25104;&#26412;&#65292;&#37319;&#29992;&#8220;&#19981;&#21487;&#30693;&#8221;&#30340;&#35266;&#28857;&#65292;&#20197;&#20998;&#26512;&#26679;&#26412;&#37327;&#21644;&#20219;&#21153;&#29305;&#24449;&#32467;&#26500;&#23545;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#36807;&#24230;&#25311;&#21512;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.13185</link><description>&lt;p&gt;
(&#26680;) &#23725;&#22238;&#24402;&#20013;&#36807;&#24230;&#25311;&#21512;&#25104;&#26412;&#30340;&#19981;&#21487;&#30693;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression. (arXiv:2306.13185v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#23725;&#22238;&#24402;&#20013;&#36807;&#25311;&#21512;&#25104;&#26412;&#65292;&#37319;&#29992;&#8220;&#19981;&#21487;&#30693;&#8221;&#30340;&#35266;&#28857;&#65292;&#20197;&#20998;&#26512;&#26679;&#26412;&#37327;&#21644;&#20219;&#21153;&#29305;&#24449;&#32467;&#26500;&#23545;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#36807;&#24230;&#25311;&#21512;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26377;&#22122;&#22768;&#30340;&#26680;&#23725;&#22238;&#24402; (KRR) &#20013;&#36807;&#25311;&#21512;&#30340;&#25104;&#26412;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#25554;&#20540;&#26080;&#23725;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#19982;&#26368;&#20248;&#35843;&#33410;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#20043;&#27604;&#12290;&#25105;&#20204;&#37319;&#29992;&#8220;&#19981;&#21487;&#30693;&#8221;&#30340;&#35266;&#28857;&#65292;&#21363;&#23545;&#20110;&#20219;&#20309;&#30446;&#26631;&#20989;&#25968;&#65292;&#21363;&#20351;&#26679;&#26412;&#37327;&#19981;&#36275;&#20197;&#36798;&#21040;&#19968;&#33268;&#24615;&#25110;&#30446;&#26631;&#20989;&#25968;&#19981;&#22312; RKHS &#20013;&#65292;&#25105;&#20204;&#20063;&#23558;&#25104;&#26412;&#30475;&#20316;&#26679;&#26412;&#37327;&#30340;&#20989;&#25968;&#12290;&#20351;&#29992;&#26368;&#36817;&#25512;&#23548;&#20986;&#30340;&#65288;&#38750;&#20005;&#26684;&#30340;&#65289;&#39118;&#38505;&#35780;&#20272;&#65292;&#20197;&#20219;&#21153;&#29305;&#24449;&#32467;&#26500;&#20026;&#22522;&#30784;&#65292;&#21033;&#29992;&#39640;&#26031;&#26222;&#36866;&#24615;&#20551;&#35774;&#20998;&#26512;&#36807;&#24230;&#25311;&#21512;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#33391;&#24615;&#12289;&#32531;&#21644;&#21644;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#65288;&#21442;&#35265; Mallinar &#31561;&#20154; 2022&#65289;&#30340;&#26356;&#31934;&#32454;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the cost of overfitting in noisy kernel ridge regression (KRR), which we define as the ratio between the test error of the interpolating ridgeless model and the test error of the optimally-tuned model. We take an "agnostic" view in the following sense: we consider the cost as a function of sample size for any target function, even if the sample size is not large enough for consistency or the target is outside the RKHS. We analyze the cost of overfitting under a Gaussian universality ansatz using recently derived (non-rigorous) risk estimates in terms of the task eigenstructure. Our analysis provides a more refined characterization of benign, tempered and catastrophic overfitting (qv Mallinar et al. 2022).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#32791;&#24863;&#30693;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#34920;&#26684;&#22522;&#20934; EC-NAS&#65292;&#35813;&#22522;&#20934;&#36890;&#36807;&#28155;&#21152;&#33021;&#32791;&#21644;&#30899;&#36275;&#36857;&#20449;&#24687;&#65292;&#25903;&#25345;&#35774;&#35745;&#33021;&#25928;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#38477;&#20302;&#24635;&#33021;&#32791;&#12290;</title><link>http://arxiv.org/abs/2210.06015</link><description>&lt;p&gt;
EC-NAS: &#38754;&#21521;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#33021;&#32791;&#24863;&#30693;&#34920;&#26684;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search. (arXiv:2210.06015v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06015
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#32791;&#24863;&#30693;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#34920;&#26684;&#22522;&#20934; EC-NAS&#65292;&#35813;&#22522;&#20934;&#36890;&#36807;&#28155;&#21152;&#33021;&#32791;&#21644;&#30899;&#36275;&#36857;&#20449;&#24687;&#65292;&#25903;&#25345;&#35774;&#35745;&#33021;&#25928;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#38477;&#20302;&#24635;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36873;&#25321;&#12289;&#35757;&#32451;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#33021;&#37327;&#28040;&#32791;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#25991;&#26088;&#22312;&#25903;&#25345;&#35774;&#35745;&#33021;&#25928;&#39640;&#12289;&#35757;&#32451;&#36164;&#28304;&#28040;&#32791;&#36739;&#20302;&#12289;&#36866;&#29992;&#20110;&#23454;&#38469;&#36793;&#32536;/&#31227;&#21160;&#35745;&#31639;&#29615;&#22659;&#24182;&#20855;&#26377;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#33021;&#25928;&#20316;&#20026;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034; (NAS) &#30340;&#19968;&#39033;&#39069;&#22806;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#19981;&#21516;&#26550;&#26500;&#30340;&#33021;&#32791;&#21644;&#30899;&#36275;&#36857;&#20449;&#24687;&#65292;&#25552;&#20379;&#26356;&#26032;&#30340;&#34920;&#26684;&#22522;&#20934; EC-NAS &#20197;&#22312;&#36739;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#35780;&#20272; NAS &#31574;&#30053;&#12290;EC-NAS &#36824;&#21253;&#25324;&#29992;&#20110;&#39044;&#27979;&#33021;&#32791;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#26377;&#21161;&#20110;&#38477;&#20302;&#24635;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy consumption from selecting, training and deploying deep learning models has continued to increase over the past few years. Our goal in this work is to support the design of energy-efficient deep learning models that are easier to train with lower compute resources, practical to deploy in real-world edge/mobile computing settings and environmentally sustainable. Tabular benchmarks for neural architecture search (NAS) allow the evaluation of NAS strategies at lower computational cost by providing pre-computed performance statistics. In this work, we suggest including energy efficiency as an additional performance criterion to NAS and present an updated tabular benchmark by including information on energy consumption and carbon footprint for different architectures. The benchmark called EC-NAS is made available open-source to support energy consumption-aware NAS research. EC-NAS also includes a surrogate model for predicting energy consumption, and helps us reduce the overall energ
&lt;/p&gt;</description></item></channel></rss>