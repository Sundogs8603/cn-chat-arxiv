<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25628;&#32034;&#20840;&#23616;&#26368;&#20248;&#26102;&#25928;&#29575;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08757</link><description>&lt;p&gt;
&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Combinatorial Optimization via Heat Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08757
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25628;&#32034;&#20840;&#23616;&#26368;&#20248;&#26102;&#25928;&#29575;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#28909;&#25193;&#25955;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#12290;&#38024;&#23545;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35775;&#38382;&#35299;&#31354;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#36825;&#19968;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#35299;&#20915;&#19968;&#33324;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#19968;&#31995;&#21015;&#26368;&#20855;&#25361;&#25112;&#24615;&#21644;&#24191;&#27867;&#36935;&#21040;&#30340;&#32452;&#21512;&#20248;&#21270;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08757v1 Announce Type: cross  Abstract: Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature.The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal. To overcome this challenge, diverging from conventional efforts of expanding the solver's search scope, we focus on enabling information to actively propagate to the solver through heat diffusion. By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation. Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems. The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations. Echoing rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#65292;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#21450;&#20854;&#34920;&#31034;&#20013;&#28508;&#22312;&#32467;&#26500;&#65292;&#36890;&#36807;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#29702;&#35770;&#21644;&#21464;&#20998;&#32467;&#26524;&#24471;&#20986;&#20102;&#36866;&#29992;&#20110;&#23454;&#38469;&#20013;&#24120;&#35265;&#26377;&#38480;&#28145;&#24230;&#32593;&#32476;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.08750</link><description>&lt;p&gt;
&#31070;&#32463;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#21644;&#28145;&#24230;&#32593;&#32476;&#30340;&#34920;&#29616;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural reproducing kernel Banach spaces and representer theorems for deep networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#65292;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#21450;&#20854;&#34920;&#31034;&#20013;&#28508;&#22312;&#32467;&#26500;&#65292;&#36890;&#36807;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#29702;&#35770;&#21644;&#21464;&#20998;&#32467;&#26524;&#24471;&#20986;&#20102;&#36866;&#29992;&#20110;&#23454;&#38469;&#20013;&#24120;&#35265;&#26377;&#38480;&#28145;&#24230;&#32593;&#32476;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#30001;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#30340;&#20989;&#25968;&#31354;&#38388;&#26377;&#21161;&#20110;&#29702;&#35299;&#30456;&#24212;&#30340;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#24402;&#32435;&#20559;&#24046;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#65292;&#36825;&#20123;&#31354;&#38388;&#37197;&#22791;&#26377;&#24378;&#21046;&#31232;&#30095;&#24615;&#30340;&#33539;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#21450;&#20854;&#34920;&#31034;&#20013;&#28508;&#22312;&#32467;&#26500;&#12290;&#22522;&#20110;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#29702;&#35770;&#65292;&#32467;&#21512;&#21464;&#20998;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#35777;&#26126;&#22312;&#24212;&#29992;&#20013;&#24120;&#29992;&#30340;&#26377;&#38480;&#26550;&#26500;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25193;&#23637;&#20102;&#27973;&#23618;&#32593;&#32476;&#30340;&#31867;&#20284;&#32467;&#26524;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#26397;&#30528;&#26356;&#23454;&#29992;&#30340;&#26041;&#21521;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08750v1 Announce Type: cross  Abstract: Studying the function spaces defined by neural networks helps to understand the corresponding learning models and their inductive bias. While in some limits neural networks correspond to function spaces that are reproducing kernel Hilbert spaces, these regimes do not capture the properties of the networks used in practice. In contrast, in this paper we show that deep neural networks define suitable reproducing kernel Banach spaces.   These spaces are equipped with norms that enforce a form of sparsity, enabling them to adapt to potential latent structures within the input data and their representations. In particular, leveraging the theory of reproducing kernel Banach spaces, combined with variational results, we derive representer theorems that justify the finite architectures commonly employed in applications. Our study extends analogous results for shallow networks and can be seen as a step towards considering more practically plaus
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#19968;&#23618;Softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#19978;&#25351;&#25968;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#27969;&#65292;&#21457;&#29616;&#22312;&#28176;&#36827;&#26368;&#23567;&#21270;&#25439;&#22833;&#20540;&#26102;&#38544;&#24335;&#26368;&#23567;&#21270;&#20102;&#20851;&#38190;&#21644;&#26597;&#35810;&#26435;&#37325;&#30697;&#38453;&#20056;&#31215;&#30340;&#26680;&#33539;&#25968;&#65292;&#36825;&#31181;&#38544;&#24335;&#27491;&#21017;&#21270;&#21487;&#36890;&#36807;&#19982;&#27880;&#24847;&#21147;&#26435;&#37325;&#30456;&#20851;&#30340;SVM&#38382;&#39064;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.08699</link><description>&lt;p&gt;
&#19968;&#23618;Softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#19978;&#26799;&#24230;&#27969;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Implicit Regularization of Gradient Flow on One-Layer Softmax Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08699
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#19968;&#23618;Softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#19978;&#25351;&#25968;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#27969;&#65292;&#21457;&#29616;&#22312;&#28176;&#36827;&#26368;&#23567;&#21270;&#25439;&#22833;&#20540;&#26102;&#38544;&#24335;&#26368;&#23567;&#21270;&#20102;&#20851;&#38190;&#21644;&#26597;&#35810;&#26435;&#37325;&#30697;&#38453;&#20056;&#31215;&#30340;&#26680;&#33539;&#25968;&#65292;&#36825;&#31181;&#38544;&#24335;&#27491;&#21017;&#21270;&#21487;&#36890;&#36807;&#19982;&#27880;&#24847;&#21147;&#26435;&#37325;&#30456;&#20851;&#30340;SVM&#38382;&#39064;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#23618;Softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#19978;&#25351;&#25968;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#27969;&#65292;&#20854;&#20013;&#20851;&#38190;&#21644;&#26597;&#35810;&#26435;&#37325;&#30697;&#38453;&#26159;&#20998;&#21035;&#35757;&#32451;&#30340;&#12290;&#22312;&#25968;&#25454;&#21487;&#20998;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#26799;&#24230;&#27969;&#36798;&#21040;&#26368;&#23567;&#25439;&#22833;&#20540;&#26102;&#65292;&#23427;&#36827;&#19968;&#27493;&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#20102;&#20851;&#38190;&#21644;&#26597;&#35810;&#26435;&#37325;&#30697;&#38453;&#20056;&#31215;&#30340;&#26680;&#33539;&#25968;&#12290;&#36825;&#31181;&#38544;&#24335;&#27491;&#21017;&#21270;&#21487;&#20197;&#36890;&#36807;&#19982;&#27880;&#24847;&#21147;&#26435;&#37325;&#30456;&#20851;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#38382;&#39064;&#26469;&#25551;&#36848;&#12290;&#36825;&#19968;&#21457;&#29616;&#19982;&#20808;&#21069;&#30340;&#32467;&#26524;&#24418;&#25104;&#23545;&#27604;&#65292;&#20808;&#21069;&#30340;&#32467;&#26524;&#26174;&#31034;&#24403;&#23558;&#20851;&#38190;&#21644;&#26597;&#35810;&#30697;&#38453;&#21512;&#24182;&#20026;&#21333;&#20010;&#26435;&#37325;&#30697;&#38453;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26799;&#24230;&#19979;&#38477;&#20250;&#22312;&#20056;&#31215;&#26435;&#37325;&#30697;&#38453;&#19978;&#23454;&#26045;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#26368;&#23567;&#21270;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#12290;&#23545;&#20110;&#23545;&#35282;&#20851;&#38190;&#21644;&#26597;&#35810;&#30697;&#38453;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#24314;&#31435;&#22312;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#26415;&#21644;&#21033;&#29992;&#19982;&#20998;&#31867;&#20219;&#21153;&#30456;&#20851;&#30340;SVM&#30340;&#36817;&#20284;KKT&#26465;&#20214;&#30340;&#22522;&#30784;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08699v1 Announce Type: cross  Abstract: We study gradient flow on the exponential loss for a classification problem with a one-layer softmax attention model, where the key and query weight matrices are trained separately. Under a separability assumption on the data, we show that when gradient flow achieves the minimal loss value, it further implicitly minimizes the nuclear norm of the product of the key and query weight matrices. Such implicit regularization can be described by a Support Vector Machine (SVM) problem with respect to the attention weights. This finding contrasts with prior results showing that the gradient descent induces an implicit regularization on the Frobenius norm on the product weight matrix when the key and query matrices are combined into a single weight matrix for training. For diagonal key and query matrices, our analysis builds upon the reparameterization technique and exploits approximate KKT conditions of the SVM associated with the classificatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#23545;&#27604;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#25581;&#31034;&#20102;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#36825;&#20123;&#27169;&#22411;&#25509;&#36817;&#20110;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#25110;&#26680;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#23545;&#27604;&#25439;&#22833;&#30340;NTK&#25910;&#25947;&#32467;&#26524;&#65292;&#20026;&#23545;&#27604;&#23398;&#20064;&#19982;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#32852;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.08673</link><description>&lt;p&gt;
&#20309;&#26102;&#33021;&#29992;&#31070;&#32463;&#20999;&#32447;&#26680;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#36817;&#20284;&#23485;&#23545;&#27604;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#23545;&#27604;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#25581;&#31034;&#20102;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#36825;&#20123;&#27169;&#22411;&#25509;&#36817;&#20110;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#25110;&#26680;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#23545;&#27604;&#25439;&#22833;&#30340;NTK&#25910;&#25947;&#32467;&#26524;&#65292;&#20026;&#23545;&#27604;&#23398;&#20064;&#19982;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#32852;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#33539;&#24335;&#65292;&#23545;&#20110;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#38750;&#24120;&#25104;&#21151;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#32771;&#23519;&#20102;&#23545;&#27604;&#25439;&#22833;&#65292;&#22768;&#31216;&#23545;&#27604;&#27169;&#22411;&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#35889;&#23884;&#20837;&#65292;&#32780;&#23569;&#25968;&#24037;&#20316;&#23637;&#31034;&#20102;&#65288;&#23485;&#65289;&#23545;&#27604;&#27169;&#22411;&#19982;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#35757;&#32451;&#22909;&#30340;&#23545;&#27604;&#27169;&#22411;&#26159;&#21542;&#30830;&#23454;&#23545;&#24212;&#20110;&#26680;&#26041;&#27861;&#25110;PCA&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#20004;&#23618;&#23545;&#27604;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#22238;&#31572;&#20102;&#36825;&#20123;&#27169;&#22411;&#20309;&#26102;&#25509;&#36817;PCA&#25110;&#26680;&#26041;&#27861;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#22312;&#21463;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#31561;&#25928;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#26426;&#22120;&#65292;&#24182;&#19988;&#26080;&#31351;&#23485;&#32593;&#32476;&#30340;NTK&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#24658;&#23450;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#27604;&#25439;&#22833;NTK&#30340;&#31532;&#19968;&#20010;&#25910;&#25947;&#32467;&#26524;&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#30011;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08673v1 Announce Type: new  Abstract: Contrastive learning is a paradigm for learning representations from unlabelled data that has been highly successful for image and text data. Several recent works have examined contrastive losses to claim that contrastive models effectively learn spectral embeddings, while few works show relations between (wide) contrastive models and kernel principal component analysis (PCA). However, it is not known if trained contrastive models indeed correspond to kernel methods or PCA. In this work, we analyze the training dynamics of two-layer contrastive models, with non-linear activation, and answer when these models are close to PCA or kernel methods. It is well known in the supervised setting that neural networks are equivalent to neural tangent kernel (NTK) machines, and that the NTK of infinitely wide networks remains constant during training. We provide the first convergence results of NTK for contrastive losses, and present a nuanced pictur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#20174;&#40657;&#31665;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#35299;&#37322;&#12289;&#35777;&#26126;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;DNN&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08652</link><description>&lt;p&gt;
&#20174;&#40657;&#31665;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#35299;&#37322;&#12289;&#35777;&#26126;&#21644;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#20174;&#40657;&#31665;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#35299;&#37322;&#12289;&#35777;&#26126;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;DNN&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26412;&#36523;&#19981;&#20250;&#35745;&#31639;&#25110;&#23637;&#31034;&#32463;&#39564;&#35777;&#26126;&#30340;&#20219;&#21153;&#32622;&#20449;&#24230;&#12290;&#22312;&#20851;&#38190;&#20219;&#21153;&#24212;&#29992;&#20013;&#65292;&#20102;&#35299;&#30456;&#20851;&#30340;DNN&#25512;&#29702;&#21450;&#20854;&#25903;&#25345;&#35777;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;DNNs&#20013;&#25552;&#21462;&#35299;&#37322;&#12289;&#35777;&#26126;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20869;&#23384;&#21644;&#35745;&#31639;&#26041;&#38754;&#37117;&#24456;&#26377;&#25928;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#40657;&#31665;DNN&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#21253;&#25324;&#24322;&#24120;&#26816;&#27979;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DNN&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08652v1 Announce Type: new  Abstract: Deep Neural Networks (DNNs) do not inherently compute or exhibit empirically-justified task confidence. In mission critical applications, it is important to both understand associated DNN reasoning and its supporting evidence. In this paper, we propose a novel Bayesian approach to extract explanations, justifications, and uncertainty estimates from DNNs. Our approach is efficient both in terms of memory and computation, and can be applied to any black box DNN without any retraining, including applications to anomaly detection and out-of-distribution detection tasks. We validate our approach on the CIFAR-10 dataset, and show that it can significantly improve the interpretability and reliability of DNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20004;&#31181;&#26368;&#36817;&#23545;&#40784;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#27867;&#21270;&#29256;&#26412;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.08635</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#20559;&#22909;&#20248;&#21270;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Human Alignment of Large Language Models through Online Preference Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20004;&#31181;&#26368;&#36817;&#23545;&#40784;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#27867;&#21270;&#29256;&#26412;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#23545;&#20110;&#30830;&#20445;&#29992;&#25143;&#20307;&#39564;&#30340;&#26377;&#29992;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#24841;&#24742;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#20154;&#31867;&#23545;&#40784;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20986;&#29616;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20363;&#22914;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12289;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#65288;DPO&#65289;&#21644;&#24207;&#21015;&#20284;&#28982;&#26657;&#20934;&#65288;SLiC&#65289;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#26368;&#36817;&#23545;&#40784;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#21363;&#36523;&#20221;&#31574;&#30053;&#20248;&#21270;&#65288;IPO&#65289;&#21644;&#32435;&#20160;&#38236;&#20687;&#19979;&#38477;&#65288;Nash-MD&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IPO&#30340;&#19968;&#31181;&#27867;&#21270;&#29256;&#26412;&#65292;&#21517;&#20026;IPO-MD&#65292;&#23427;&#21033;&#29992;&#20102;Nash-MD&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#25277;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08635v1 Announce Type: cross  Abstract: Ensuring alignment of language models' outputs with human preferences is critical to guarantee a useful, safe, and pleasant user experience. Thus, human alignment has been extensively studied recently and several methods such as Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation (DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper, our contribution is two-fold. First, we show the equivalence between two recent alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD, that leverages the regularised sampling approach proposed by Nash-MD.   This equivalence may seem surprising at first sight, since IPO is an offline method whereas Nash-MD is an online method using a preference model. However, this equivalence can be proven when we consider the online version of IPO, that is when both generati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20449;&#24230;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#31232;&#32570;&#32780;&#26114;&#36149;&#30340;&#39640;&#20445;&#30495;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08627</link><description>&lt;p&gt;
&#22810;&#20449;&#24230;&#32447;&#24615;&#22238;&#24402;&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#31232;&#32570;&#25968;&#25454;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multifidelity linear regression for scientific machine learning from scarce data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20449;&#24230;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#31232;&#32570;&#32780;&#26114;&#36149;&#30340;&#39640;&#20445;&#30495;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25311;&#21512;&#32473;&#23450;&#21442;&#25968;&#21270;&#27169;&#22411;&#31867;&#30340;&#21442;&#25968;&#26469;&#36866;&#24212;&#25968;&#25454;&#65292;&#20316;&#20026;&#23398;&#20064;&#22797;&#26434;&#24037;&#31243;&#31995;&#32479;&#30340;&#20195;&#29702;&#27169;&#22411;&#30340;&#28508;&#22312;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#29615;&#22659;&#20013;&#65292;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#39640;&#20445;&#30495;&#25968;&#25454;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#29992;&#20110;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#39044;&#31639;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#22810;&#20449;&#24230;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#25968;&#25454;&#30340;&#21508;&#31181;&#20445;&#30495;&#24230;&#21644;&#25104;&#26412;&#21487;&#29992;&#30340;&#31185;&#23398;&#32972;&#26223;&#65307;&#20363;&#22914;&#65292;&#39640;&#20445;&#30495;&#25968;&#25454;&#21487;&#33021;&#30001;&#26114;&#36149;&#30340;&#20840;&#38754;&#35299;&#26512;&#30340;&#29289;&#29702;&#27169;&#25311;&#29983;&#25104;&#65292;&#32780;&#20302;&#20445;&#30495;&#25968;&#25454;&#21487;&#33021;&#26469;&#33258;&#22522;&#20110;&#31616;&#21270;&#30340;&#26356;&#20415;&#23452;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08627v1 Announce Type: cross  Abstract: Machine learning (ML) methods, which fit to data the parameters of a given parameterized model class, have garnered significant interest as potential methods for learning surrogate models for complex engineering systems for which traditional simulation is expensive. However, in many scientific and engineering settings, generating high-fidelity data on which to train ML models is expensive, and the available budget for generating training data is limited. ML models trained on the resulting scarce high-fidelity data have high variance and are sensitive to vagaries of the training data set. We propose a new multifidelity training approach for scientific machine learning that exploits the scientific context where data of varying fidelities and costs are available; for example high-fidelity data may be generated by an expensive fully resolved physics simulation whereas lower-fidelity data may arise from a cheaper model based on simplifying 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;Verifix&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#27714;</title><link>https://arxiv.org/abs/2403.08618</link><description>&lt;p&gt;
Verifix: &#21518;&#35757;&#32451;&#26657;&#27491;&#20197;&#25913;&#21892;&#20855;&#26377;&#32463;&#36807;&#39564;&#35777;&#26679;&#26412;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08618
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;Verifix&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#38169;&#35823;&#65292;&#21363;&#35757;&#32451;&#26679;&#26412;&#20855;&#26377;&#19981;&#27491;&#30830;&#30340;&#26631;&#31614;&#65292;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#38169;&#35823;&#24448;&#24448;&#26469;&#33258;&#38750;&#19987;&#23478;&#26631;&#27880;&#25110;&#25932;&#23545;&#25915;&#20987;&#12290;&#33719;&#21462;&#22823;&#22411;&#12289;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#25104;&#26412;&#39640;&#65292;&#24403;&#26377;&#24178;&#20928;&#30340;&#25968;&#25454;&#38598;&#21487;&#29992;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#23601;&#21464;&#24471;&#35745;&#31639;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Verifix&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#19968;&#20010;&#23567;&#30340;&#12289;&#32463;&#36807;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21333;&#20010;&#26356;&#26032;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#12290;Verifix&#20351;&#29992;SVD&#20272;&#35745;&#24178;&#20928;&#28608;&#27963;&#31354;&#38388;&#65292;&#28982;&#21518;&#23558;&#27169;&#22411;&#30340;&#26435;&#37325;&#25237;&#24433;&#21040;&#36825;&#20010;&#31354;&#38388;&#19978;&#65292;&#20197;&#25233;&#21046;&#23545;&#24212;&#20110;&#25439;&#22351;&#25968;&#25454;&#30340;&#28608;&#27963;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Verifix&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08618v1 Announce Type: cross  Abstract: Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. This corruption often arises from non-expert labeling or adversarial attacks. Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive. To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update. Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data. We demonstrate Verifix's effectiveness 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#33258;&#36866;&#24212;&#27493;&#38271;&#24341;&#20837;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#20174;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25910;&#25947;&#21040;&#27491;&#30830;&#30340;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.08609</link><description>&lt;p&gt;
&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#30340;&#23616;&#37096;&#33258;&#36866;&#24212;&#21644;&#21487;&#25193;&#23637;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#30340;&#25910;&#25947;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Locally Adaptive and Scalable Diffusion-Based Sampling Methods for Deep Bayesian Neural Network Posteriors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#33258;&#36866;&#24212;&#27493;&#38271;&#24341;&#20837;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#20174;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25910;&#25947;&#21040;&#27491;&#30830;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22914;&#21307;&#23398;&#25104;&#20687;&#20013;&#38656;&#35201;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26159;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20174;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#23558;&#26159;&#23558;&#31867;&#20284;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#22120;&#30340;&#33258;&#36866;&#24212;&#27493;&#38271;&#32435;&#20837;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#38656;&#27714;&#12290;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#19968;&#20123;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#23454;&#29616;&#36825;&#19968;&#23646;&#24615;&#30340;&#37319;&#26679;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26159;&#21542;&#30830;&#23454;&#25910;&#25947;&#21040;&#27491;&#30830;&#30340;&#20998;&#24067;&#21602;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08609v1 Announce Type: new  Abstract: Achieving robust uncertainty quantification for deep neural networks represents an important requirement in many real-world applications of deep learning such as medical imaging where it is necessary to assess the reliability of a neural network's prediction. Bayesian neural networks are a promising approach for modeling uncertainties in deep neural networks. Unfortunately, generating samples from the posterior distribution of neural networks is a major challenge. One significant advance in that direction would be the incorporation of adaptive step sizes, similar to modern neural network optimizers, into Monte Carlo Markov chain sampling algorithms without significantly increasing computational demand. Over the past years, several papers have introduced sampling algorithms with claims that they achieve this property. However, do they indeed converge to the correct distribution? In this paper, we demonstrate that these methods can have a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22343;&#22330;&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#37319;&#26679;&#22810;&#20010;&#24369;&#32806;&#21512;&#25968;&#25454;&#28857;&#65292;&#22312;&#25511;&#21046;&#29109;&#25439;&#22833;&#30340;&#21516;&#26102;&#22312;&#20284;&#28982;&#25311;&#21512;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.08362</link><description>&lt;p&gt;
&#22343;&#22330;&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Mean-Field Microcanonical Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08362
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22343;&#22330;&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#37319;&#26679;&#22810;&#20010;&#24369;&#32806;&#21512;&#25968;&#25454;&#28857;&#65292;&#22312;&#25511;&#21046;&#29109;&#25439;&#22833;&#30340;&#21516;&#26102;&#22312;&#20284;&#28982;&#25311;&#21512;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;&#26159;&#19968;&#31181;&#33021;&#37327;&#27169;&#22411;&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#21487;&#23454;&#29616;&#39640;&#32500;&#20998;&#24067;&#30340;&#39640;&#25928;&#37319;&#26679;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23558;&#26679;&#26412;&#20174;&#39640;&#29109;&#20998;&#24067;&#65288;&#22914;&#39640;&#26031;&#30333;&#22122;&#22768;&#65289;&#36716;&#36816;&#33267;&#20302;&#33021;&#21306;&#22495;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#27169;&#22411;&#32622;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#26694;&#26550;&#20013;&#65292;&#26174;&#31034;&#23427;&#36890;&#24120;&#20250;&#30001;&#20110;&#22312;&#19979;&#38477;&#36807;&#31243;&#20013;&#22833;&#21435;&#19981;&#24517;&#35201;&#30340;&#29109;&#32780;&#36807;&#24230;&#25311;&#21512;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22343;&#22330;&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;&#65292;&#21516;&#26102;&#37319;&#26679;&#22810;&#20010;&#24369;&#32806;&#21512;&#25968;&#25454;&#28857;&#65292;&#20801;&#35768;&#26356;&#22909;&#22320;&#25511;&#21046;&#29109;&#25439;&#22833;&#65292;&#21516;&#26102;&#22312;&#20284;&#28982;&#25311;&#21512;&#26041;&#38754;&#20184;&#20986;&#36739;&#23567;&#20195;&#20215;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#32972;&#26223;&#20013;&#65292;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08362v1 Announce Type: cross  Abstract: Microcanonical gradient descent is a sampling procedure for energy-based models allowing for efficient sampling of distributions in high dimension. It works by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region using gradient descent. We put this model in the framework of normalizing flows, showing how it can often overfit by losing an unnecessary amount of entropy in the descent. As a remedy, we propose a mean-field microcanonical gradient descent that samples several weakly coupled data points simultaneously, allowing for better control of the entropy loss while paying little in terms of likelihood fit. We study these models in the context of financial time series, illustrating the improvements on both synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;&#65292;&#24314;&#31435;&#20102;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65292;&#20026;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#35774;&#32622;&#20102;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.08335</link><description>&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Sparsity Principle for Partially Observable Causal Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08335
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;&#65292;&#24314;&#31435;&#20102;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65292;&#20026;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#35774;&#32622;&#20102;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#20174;&#24863;&#30693;&#25968;&#25454;&#20013;&#35782;&#21035;&#39640;&#23618;&#27425;&#30340;&#22240;&#26524;&#21464;&#37327;&#12290;&#26412;&#25991;&#32771;&#34385;&#37096;&#20998;&#35266;&#27979;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#27425;&#27979;&#37327;&#20165;&#25552;&#20379;&#20851;&#20110;&#28508;&#22312;&#22240;&#26524;&#29366;&#24577;&#23376;&#38598;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20174;&#25968;&#25454;&#38598;&#20013;&#19981;&#37197;&#23545;&#35266;&#23519;&#23398;&#20064;&#65292;&#20854;&#20013;&#23384;&#22312;&#23454;&#20363;&#30456;&#20851;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#35813;&#35774;&#32622;&#24314;&#31435;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65306;&#19968;&#20010;&#26159;&#20851;&#20110;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#30340;&#32467;&#26524;&#65292;&#26080;&#38656;&#23545;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#20570;&#21442;&#25968;&#20551;&#35774;&#65292;&#21478;&#19968;&#20010;&#26159;&#23545;&#20855;&#26377;&#39640;&#26031;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#30340;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#30340;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20272;&#35745;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08335v1 Announce Type: cross  Abstract: Causal representation learning aims at identifying high-level causal variables from perceptual data. Most methods assume that all latent causal variables are captured in the high-dimensional observations. We instead consider a partially observed setting, in which each measurement only provides information about a subset of the underlying causal state. Prior work has studied this setting with multiple domains or views, each depending on a fixed subset of latents. Here, we focus on learning from unpaired observations from a dataset with an instance-dependent partial observability pattern. Our main contribution is to establish two identifiability results for this setting: one for linear mixing functions without parametric assumptions on the underlying causal model, and one for piecewise linear mixing functions with Gaussian latent causal variables. Based on these insights, we propose two methods for estimating the underlying causal variab
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Bayesian&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25628;&#32034;&#21306;&#22495;&#38480;&#21046;&#22312;&#36739;&#20302;&#32500;&#24230;&#65292;&#24182;&#21033;&#29992;&#26412;&#22320;GPR&#27169;&#22411;&#65292;&#22312;&#39640;&#32500;&#24230;&#20013;&#25552;&#39640;&#20102;&#25628;&#32034;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08331</link><description>&lt;p&gt;
Bayesian&#20248;&#21270;&#23558;&#25628;&#32034;&#21306;&#22495;&#38480;&#21046;&#22312;&#36739;&#20302;&#32500;&#24230;&#20013;&#65292;&#21033;&#29992;&#26412;&#22320;GPR
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization that Limits Search Region to Lower Dimensions Utilizing Local GPR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08331
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Bayesian&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25628;&#32034;&#21306;&#22495;&#38480;&#21046;&#22312;&#36739;&#20302;&#32500;&#24230;&#65292;&#24182;&#21033;&#29992;&#26412;&#22320;GPR&#27169;&#22411;&#65292;&#22312;&#39640;&#32500;&#24230;&#20013;&#25552;&#39640;&#20102;&#25628;&#32034;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#22495;&#65292;&#21253;&#25324;&#35774;&#35745;&#21644;&#25511;&#21046;&#65292;&#37117;&#38656;&#35201;&#20248;&#21270;&#20135;&#21697;&#21644;&#31995;&#32479;&#29305;&#24615;&#12290; &#24403;&#35266;&#27979;&#25104;&#26412;&#39640;&#26102;&#65292;&#36890;&#24120;&#20351;&#29992;Bayesian&#20248;&#21270;&#65288;BO&#65289;&#65292;&#22240;&#20026;BO&#20174;&#29702;&#35770;&#19978;&#20445;&#35777;&#20102;&#36951;&#25022;&#30340;&#19978;&#38480;&#12290; &#28982;&#32780;&#65292;&#38543;&#30528;&#35201;&#20248;&#21270;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#35745;&#31639;&#25104;&#26412;&#21576;&#25351;&#25968;&#32423;&#22686;&#21152;&#65292;&#23548;&#33268;&#25628;&#32034;&#25928;&#29575;&#38477;&#20302;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25628;&#32034;&#21306;&#22495;&#38480;&#21046;&#22312;&#36739;&#20302;&#32500;&#24230;&#24182;&#21033;&#29992;&#26412;&#22320;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;LGPR&#65289;&#23558;BO&#25193;&#23637;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#26041;&#27861;&#12290; LGPR&#23558;&#20302;&#32500;&#25628;&#32034;&#21306;&#22495;&#35270;&#20026;&#8220;&#26412;&#22320;&#8221;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#37027;&#37324;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290; LGPR&#27169;&#22411;&#26159;&#22312;&#29305;&#23450;&#21306;&#22495;&#30340;&#26412;&#22320;&#25968;&#25454;&#23376;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290; &#36825;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25628;&#32034;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#20102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#30697;&#38453;&#27714;&#36870;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290; &#22312;&#35780;&#20272;20D Ackley&#21644;Rosenbrock&#20989;&#25968;&#26102;&#65292;&#25628;&#32034;&#25928;&#29575;&#19982;&#25110;&#39640;&#20110;&#26082;&#23450;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08331v1 Announce Type: new  Abstract: Optimization of product and system characteristics is required in many fields, including design and control. Bayesian optimization (BO) is often used when there are high observing costs, because BO theoretically guarantees an upper bound on regret. However, computational costs increase exponentially with the number of parameters to be optimized, decreasing search efficiency. We propose a BO that limits the search region to lower dimensions and utilizes local Gaussian process regression (LGPR) to scale the BO to higher dimensions. LGPR treats the low-dimensional search region as "local," improving prediction accuracies there. The LGPR model is trained on a local subset of data specific to that region. This improves prediction accuracy and search efficiency and reduces the time complexity of matrix inversion in the Gaussian process regression. In evaluations with 20D Ackley and Rosenbrock functions, search efficiencies are equal to or high
&lt;/p&gt;</description></item><item><title>&#36816;&#29992;&#23548;&#25968;&#20449;&#24687;&#30340;&#31070;&#32463;&#31639;&#23376;&#21152;&#36895;&#20102;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#35299;&#20915;&#38750;&#32447;&#24615;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.08220</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#39640;&#25928;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65306;&#21033;&#29992;&#23548;&#25968;&#20449;&#24687;&#30340;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08220
&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#23548;&#25968;&#20449;&#24687;&#30340;&#31070;&#32463;&#31639;&#23376;&#21152;&#36895;&#20102;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#35299;&#20915;&#38750;&#32447;&#24615;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#31639;&#23398;&#20064;&#26041;&#27861;&#26469;&#21152;&#36895;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#20197;&#35299;&#20915;&#26080;&#38480;&#32500;&#38750;&#32447;&#24615;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#12290;&#34429;&#28982;&#20960;&#20309;MCMC&#37319;&#29992;&#36866;&#24212;&#21518;&#39564;&#23616;&#37096;&#20960;&#20309;&#30340;&#39640;&#36136;&#37327;&#25552;&#35758;&#65292;&#20294;&#22312;&#21442;&#25968;&#21040;&#21487;&#35266;&#27979;&#65288;PtO&#65289;&#26144;&#23556;&#36890;&#36807;&#26114;&#36149;&#30340;&#27169;&#22411;&#27169;&#25311;&#23450;&#20041;&#26102;&#65292;&#38656;&#35201;&#35745;&#31639;&#23545;&#25968;&#20284;&#28982;&#30340;&#23616;&#37096;&#26799;&#24230;&#21644;Hessian&#20449;&#24687;&#65292;&#36896;&#25104;&#39640;&#25104;&#26412;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#30001;PtO&#26144;&#23556;&#30340;&#31070;&#32463;&#31639;&#23376;&#26367;&#20195;&#39537;&#21160;&#30340;&#24310;&#36831;&#25509;&#21463;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#20854;&#20013;&#25552;&#35758;&#34987;&#35774;&#35745;&#20026;&#21033;&#29992;&#23545;&#25968;&#20284;&#28982;&#21644;&#20854;&#26799;&#24230;&#21644;Hessian&#30340;&#24555;&#36895;&#26367;&#20195;&#20272;&#35745;&#12290;&#20026;&#20102;&#23454;&#29616;&#26174;&#33879;&#21152;&#36895;&#65292;&#26367;&#20195;&#21697;&#38656;&#35201;&#20934;&#30830;&#39044;&#27979;&#21487;&#35266;&#27979;&#21450;&#20854;&#21442;&#25968;&#23548;&#25968;&#65288;&#21487;&#35266;&#27979;&#19982;&#21442;&#25968;&#20043;&#38388;&#30340;&#23548;&#25968;&#65289;&#12290;&#36890;&#36807;&#20256;&#32479;&#30340;&#26041;&#27861;&#23545;&#36825;&#26679;&#30340;&#26367;&#20195;&#21697;&#36827;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08220v1 Announce Type: cross  Abstract: We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse problems. While geometric MCMC employs high-quality proposals that adapt to posterior local geometry, it requires computing local gradient and Hessian information of the log-likelihood, incurring a high cost when the parameter-to-observable (PtO) map is defined through expensive model simulations. We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal is designed to exploit fast surrogate approximations of the log-likelihood and, simultaneously, its gradient and Hessian. To achieve a substantial speedup, the surrogate needs to be accurate in predicting both the observable and its parametric derivative (the derivative of the observable with respect to the parameter). Training such a surrogate via conventional o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#20803;&#23398;&#20064;&#26694;&#26550;&#26469;&#23398;&#20064;&#28151;&#21512;&#28508;&#22312;&#21160;&#24577;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#29289;&#29702;&#24402;&#32435;&#20559;&#24046;&#21644;&#23398;&#20064;&#35782;&#21035;&#31574;&#30053;&#65292;&#22312;&#25429;&#25417;&#26410;&#30693;&#21160;&#24577;&#21644;&#20998;&#36776;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.08194</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#28151;&#21512;&#28508;&#22312;&#21160;&#24577;&#65306;&#19968;&#31181;&#23398;&#20064;&#35782;&#21035;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning of Hybrid Latent Dynamics: A Learn-to-Identify Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08194
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#20803;&#23398;&#20064;&#26694;&#26550;&#26469;&#23398;&#20064;&#28151;&#21512;&#28508;&#22312;&#21160;&#24577;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#29289;&#29702;&#24402;&#32435;&#20559;&#24046;&#21644;&#23398;&#20064;&#35782;&#21035;&#31574;&#30053;&#65292;&#22312;&#25429;&#25417;&#26410;&#30693;&#21160;&#24577;&#21644;&#20998;&#36776;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24212;&#29992;&#36234;&#26469;&#36234;&#38656;&#35201;&#20174;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#28508;&#22312;&#21160;&#24577;&#12290;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#21487;&#36776;&#35782;&#24615;&#25361;&#25112;&#65306;&#35768;&#22810;&#25277;&#35937;&#30340;&#28508;&#22312;&#34920;&#31034;&#21487;&#20197;&#37325;&#26500;&#35266;&#27979;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#20445;&#35777;&#20102;&#23545;&#32479;&#27835;&#21160;&#24577;&#30340;&#20805;&#20998;&#35782;&#21035;&#65311;&#26412;&#25991;&#20174;&#20004;&#20010;&#35282;&#24230;&#25506;&#35752;&#20102;&#36825;&#19968;&#25361;&#25112;&#65306;&#19968;&#26159;&#20351;&#29992;&#29305;&#23450;&#20110;&#25152;&#24314;&#27169;&#25968;&#25454;&#30340;&#29289;&#29702;&#24402;&#32435;&#20559;&#24046;&#65292;&#20108;&#26159;&#20351;&#29992;&#19968;&#20010;&#23398;&#20064;&#35782;&#21035;&#30340;&#31574;&#30053;&#65292;&#23558;&#39044;&#27979;&#30446;&#26631;&#19982;&#29992;&#20110;&#35782;&#21035;&#30340;&#25968;&#25454;&#20998;&#24320;&#12290;&#25105;&#20204;&#23558;&#36825;&#20004;&#31181;&#31574;&#30053;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#20803;&#23398;&#20064;&#28151;&#21512;&#28508;&#22312;&#21160;&#24577;&#26694;&#26550;&#65288;Meta-HyLaD&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#65306;1&#65289;&#19968;&#20010;&#28151;&#21512;&#20808;&#21069;&#29289;&#29702;&#25968;&#23398;&#34920;&#36798;&#24335;&#19982;&#25551;&#36848;&#20854;&#26410;&#30693;&#35823;&#24046;&#30340;&#31070;&#32463;&#20989;&#25968;&#30340;&#28508;&#22312;&#21160;&#24577;&#20989;&#25968;&#65292;&#20197;&#21450;2&#65289;&#19968;&#20010;&#20803;&#23398;&#20064;&#20844;&#24335;&#65292;&#29992;&#20110;&#23398;&#20064;&#20998;&#21035;&#35782;&#21035;&#28151;&#21512;&#21160;&#24577;&#30340;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08194v1 Announce Type: new  Abstract: Modern applications increasingly require unsupervised learning of latent dynamics from high-dimensional time-series. This presents a significant challenge of identifiability: many abstract latent representations may reconstruct observations, yet do they guarantee an adequate identification of the governing dynamics? This paper investigates this challenge from two angles: the use of physics inductive bias specific to the data being modeled, and a learn-to-identify strategy that separates forecasting objectives from the data used for the identification. We combine these two strategies in a novel framework for unsupervised meta-learning of hybrid latent dynamics (Meta-HyLaD) with: 1) a latent dynamic function that hybridize known mathematical expressions of prior physics with neural functions describing its unknown errors, and 2) a meta-learning formulation to learn to separately identify both components of the hybrid dynamics. Through exte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#29305;&#24449;&#23725;&#22238;&#24402;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#21442;&#25968;&#21270;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22914;&#20309;&#36873;&#25321;&#21442;&#25968;&#25968;&#37327;$p$&#30456;&#23545;&#20110;&#26679;&#26412;&#22823;&#23567;$n$&#20197;&#23454;&#29616;&#26368;&#20339;&#27979;&#35797;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.08160</link><description>&lt;p&gt;
&#36229;&#36234;&#32447;&#24615;&#32553;&#25918;&#21306;&#22495;&#30340;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#30340;&#28176;&#36817;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#29305;&#24449;&#23725;&#22238;&#24402;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#21442;&#25968;&#21270;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22914;&#20309;&#36873;&#25321;&#21442;&#25968;&#25968;&#37327;$p$&#30456;&#23545;&#20110;&#26679;&#26412;&#22823;&#23567;$n$&#20197;&#23454;&#29616;&#26368;&#20339;&#27979;&#35797;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26159;&#36890;&#36807;&#20351;&#29992;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#30452;&#21040;&#25509;&#36817;&#35757;&#32451;&#25968;&#25454;&#30340;&#25554;&#20540;&#20026;&#27490;&#12290;&#36890;&#36807;&#21452;&#35895;&#29616;&#35937;&#31561;&#29616;&#35937;&#24050;&#32463;&#34920;&#26126;&#65292;&#21442;&#25968;&#30340;&#25968;&#37327;&#26159;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#19981;&#33391;&#20195;&#29702;&#65292;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#21442;&#25968;&#21270;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#20160;&#20040;&#24433;&#21709;&#65311;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#27867;&#21270;&#22914;&#20309;&#21462;&#20915;&#20110;&#21442;&#25968;&#30340;&#25968;&#37327;$p$&#65311;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#36873;&#25321;$p$&#30456;&#23545;&#20110;&#26679;&#26412;&#22823;&#23567;$n$&#26469;&#23454;&#29616;&#26368;&#20248;&#30340;&#27979;&#35797;&#35823;&#24046;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#29305;&#24449;&#23725;&#22238;&#24402;&#65288;RFRR&#65289;&#30340;&#20363;&#23376;&#12290;&#36825;&#20010;&#27169;&#22411;&#26082;&#21487;&#20197;&#30475;&#20316;&#26159;&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#30340;&#26377;&#38480;&#31209;&#36924;&#36817;&#65292;&#20063;&#21487;&#20197;&#30475;&#20316;&#26159;&#22312;&#25152;&#35859;&#30340;&#25042;&#24816;&#21306;&#22495;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#31616;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;$d$&#32500;&#29699;&#19978;&#22343;&#21248;&#20998;&#24067;&#30340;&#21327;&#21464;&#37327;&#65292;&#24182;&#35745;&#31639;&#23574;&#38160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08160v1 Announce Type: cross  Abstract: Recent advances in machine learning have been achieved by using overparametrized models trained until near interpolation of the training data. It was shown, e.g., through the double descent phenomenon, that the number of parameters is a poor proxy for the model complexity and generalization capabilities. This leaves open the question of understanding the impact of parametrization on the performance of these models. How does model complexity and generalization depend on the number of parameters $p$? How should we choose $p$ relative to the sample size $n$ to achieve optimal test error?   In this paper, we investigate the example of random feature ridge regression (RFRR). This model can be seen either as a finite-rank approximation to kernel ridge regression (KRR), or as a simplified model for neural networks trained in the so-called lazy regime. We consider covariates uniformly distributed on the $d$-dimensional sphere and compute sharp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#26102;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#24615;&#65292;&#21457;&#29616;&#22312;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#22312;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#20445;&#25345;&#36739;&#23567;&#35268;&#33539;&#65292;&#24182;&#19988;&#27839;&#30528;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;KKT&#28857;&#26041;&#21521;&#36817;&#20284;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2403.08121</link><description>&lt;p&gt;
&#26089;&#26399;&#26041;&#21521;&#24615;&#25910;&#25947;&#22312;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#23567;&#21021;&#22987;&#21270;&#26102;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#26102;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#24615;&#65292;&#21457;&#29616;&#22312;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#22312;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#20445;&#25345;&#36739;&#23567;&#35268;&#33539;&#65292;&#24182;&#19988;&#27839;&#30528;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;KKT&#28857;&#26041;&#21521;&#36817;&#20284;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#26102;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#24615;&#65292;&#36825;&#20123;&#32593;&#32476;&#20174;&#23567;&#21021;&#22987;&#21270;&#24320;&#22987;&#12290;&#26412;&#25991;&#32771;&#34385;&#21040;&#20855;&#26377;&#23616;&#37096;Lipschitz&#26799;&#24230;&#21644;&#38454;&#25968;&#20005;&#26684;&#22823;&#20110;&#20004;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25991;&#31456;&#35777;&#26126;&#20102;&#23545;&#20110;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#21270;&#65292;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#20445;&#25345;&#35268;&#33539;&#36739;&#23567;&#65292;&#24182;&#19988;&#22312;Karush-Kuhn-Tucker (KKT)&#28857;&#22788;&#36817;&#20284;&#27839;&#30528;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;&#26041;&#21521;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#24179;&#26041;&#25439;&#22833;&#24182;&#22312;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#19978;&#36827;&#34892;&#21487;&#20998;&#31163;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36824;&#23637;&#31034;&#20102;&#22312;&#25439;&#22833;&#20989;&#25968;&#30340;&#26576;&#20123;&#38797;&#28857;&#38468;&#36817;&#26799;&#24230;&#27969;&#21160;&#21160;&#24577;&#30340;&#31867;&#20284;&#26041;&#21521;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08121v1 Announce Type: new  Abstract: This paper studies the gradient flow dynamics that arise when training deep homogeneous neural networks, starting with small initializations. The present work considers neural networks that are assumed to have locally Lipschitz gradients and an order of homogeneity strictly greater than two. This paper demonstrates that for sufficiently small initializations, during the early stages of training, the weights of the neural network remain small in norm and approximately converge in direction along the Karush-Kuhn-Tucker (KKT) points of the neural correlation function introduced in [1]. Additionally, for square loss and under a separability assumption on the weights of neural networks, a similar directional convergence of gradient flow dynamics is shown near certain saddle points of the loss function.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25351;&#20986;&#22312;&#26500;&#24314;&#22810;&#20445;&#30495;&#24230;&#20195;&#29702;&#27169;&#22411;&#26102;&#65292;&#26377;&#23475;&#25968;&#25454;&#28304;&#30340;&#29305;&#24449;&#21270;&#26377;&#21161;&#20110;&#25351;&#23548;&#20174;&#19994;&#32773;&#22312;&#36873;&#25321;&#26102;&#20309;&#26102;&#24573;&#30053;&#26576;&#20010;&#25968;&#25454;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.08118</link><description>&lt;p&gt;
&#30740;&#31350;&#26500;&#24314;&#22810;&#20445;&#30495;&#24230;&#20195;&#29702;&#27169;&#22411;&#26102;&#26377;&#23475;&#25968;&#25454;&#26469;&#28304;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Characterising harmful data sources when constructing multi-fidelity surrogate models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08118
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25351;&#20986;&#22312;&#26500;&#24314;&#22810;&#20445;&#30495;&#24230;&#20195;&#29702;&#27169;&#22411;&#26102;&#65292;&#26377;&#23475;&#25968;&#25454;&#28304;&#30340;&#29305;&#24449;&#21270;&#26377;&#21161;&#20110;&#25351;&#23548;&#20174;&#19994;&#32773;&#22312;&#36873;&#25321;&#26102;&#20309;&#26102;&#24573;&#30053;&#26576;&#20010;&#25968;&#25454;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24403;&#24212;&#29992;&#20110;&#24037;&#19994;&#35774;&#35745;&#38382;&#39064;&#30340;&#24314;&#27169;&#21644;&#20248;&#21270;&#20013;&#65292;&#20195;&#29702;&#24314;&#27169;&#25216;&#26415;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#24403;&#35780;&#20272;&#29305;&#23450;&#35774;&#35745;&#30340;&#24615;&#33021;&#25104;&#26412;&#24456;&#39640;&#26102;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#20197;&#20195;&#26367;&#21487;&#29992;&#30340;&#39640;&#25104;&#26412;&#26469;&#28304;&#26469;&#26597;&#35810;&#21487;&#20197;&#38477;&#20302;&#24635;&#25104;&#26412;&#12290;&#26500;&#24314;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#21487;&#20197;&#21033;&#29992;&#20854;&#20182;&#20415;&#23452;&#19988;&#19981;&#22826;&#20934;&#30830;&#30340;&#20449;&#24687;&#28304;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20449;&#24687;&#28304;&#30340;&#23384;&#22312;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#26500;&#24314;&#27169;&#22411;&#26102;&#24212;&#35813;&#20351;&#29992;&#21738;&#20123;&#20449;&#24687;&#28304;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#23545;&#26377;&#23475;&#25968;&#25454;&#28304;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#20197;&#25351;&#23548;&#20174;&#19994;&#32773;&#20309;&#26102;&#24573;&#30053;&#26576;&#20010;&#20449;&#24687;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08118v1 Announce Type: cross  Abstract: Surrogate modelling techniques have seen growing attention in recent years when applied to both modelling and optimisation of industrial design problems. These techniques are highly relevant when assessing the performance of a particular design carries a high cost, as the overall cost can be mitigated via the construction of a model to be queried in lieu of the available high-cost source. The construction of these models can sometimes employ other sources of information which are both cheaper and less accurate. The existence of these sources however poses the question of which sources should be used when constructing a model. Recent studies have attempted to characterise harmful data sources to guide practitioners in choosing when to ignore a certain source. These studies have done so in a synthetic setting, characterising sources using a large amount of data that is not available in practice. Some of these studies have also been shown
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#23454;&#29616;&#25968;&#25454;&#30340;&#20302;&#32500;&#23884;&#20837;&#65292;&#23558;&#28909;&#26680;&#20316;&#20026;&#21327;&#26041;&#24046;&#20989;&#25968;&#36827;&#34892;&#35745;&#31639;&#65292;&#25311;&#21512;&#20986;&#23884;&#20837;&#20013;&#30340;&#30452;&#32447;&#36317;&#31163;&#20197;&#27010;&#29575;&#26041;&#24335;&#36817;&#20284;&#25193;&#25955;&#36317;&#31163;&#65292;&#20445;&#30041;&#20102;&#19968;&#20123;&#36739;&#23567;&#23610;&#24230;&#32467;&#26500;&#65292;&#21516;&#26102;&#20855;&#26377;&#23545;&#24322;&#24120;&#20540;&#30340;&#26356;&#24378;&#40065;&#26834;&#24615;</title><link>https://arxiv.org/abs/2403.07929</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#23545;&#28909;&#26680;&#36827;&#34892;&#33609;&#22270;: &#22312;&#20302;&#32500;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#23884;&#20837;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Sketching the Heat Kernel: Using Gaussian Processes to Embed Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07929
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#23454;&#29616;&#25968;&#25454;&#30340;&#20302;&#32500;&#23884;&#20837;&#65292;&#23558;&#28909;&#26680;&#20316;&#20026;&#21327;&#26041;&#24046;&#20989;&#25968;&#36827;&#34892;&#35745;&#31639;&#65292;&#25311;&#21512;&#20986;&#23884;&#20837;&#20013;&#30340;&#30452;&#32447;&#36317;&#31163;&#20197;&#27010;&#29575;&#26041;&#24335;&#36817;&#20284;&#25193;&#25955;&#36317;&#31163;&#65292;&#20445;&#30041;&#20102;&#19968;&#20123;&#36739;&#23567;&#23610;&#24230;&#32467;&#26500;&#65292;&#21516;&#26102;&#20855;&#26377;&#23545;&#24322;&#24120;&#20540;&#30340;&#26356;&#24378;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#38750;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#35745;&#31639;&#20381;&#36182;&#20110;&#25968;&#25454;&#20960;&#20309;&#29305;&#24615;&#30340;&#39640;&#26031;&#36807;&#31243;&#23454;&#29616;&#25968;&#25454;&#22312;&#20302;&#32500;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#23884;&#20837;&#12290;&#27492;&#31867;&#23884;&#20837;&#39318;&#27425;&#20986;&#29616;&#22312;&#65288;Adler&#31561;&#20154;&#65292;2018&#65289;&#20013;&#65292;&#20316;&#20026;&#39640;&#32500;&#36890;&#29992;&#27969;&#24418;&#30340;&#29702;&#35770;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#39640;&#26031;&#36807;&#31243;&#30340;&#21327;&#26041;&#24046;&#20989;&#25968;&#35774;&#32622;&#20026;&#28909;&#26680;&#65292;&#35745;&#31639;&#23884;&#20837;&#30456;&#24403;&#20110;&#33609;&#32472;&#20195;&#34920;&#28909;&#26680;&#30340;&#30697;&#38453;&#12290;Karhunen-Lo\`eve&#23637;&#24320;&#34920;&#26126;&#65292;&#22312;&#23884;&#20837;&#20013;&#30340;&#30452;&#32447;&#36317;&#31163;&#20197;&#27010;&#29575;&#24847;&#20041;&#19978;&#36817;&#20284;&#25193;&#25955;&#36317;&#31163;&#65292;&#36991;&#20813;&#20102;&#23545;&#23574;&#38160;&#25130;&#26029;&#30340;&#38656;&#27714;&#65292;&#24182;&#20445;&#25345;&#20102;&#19968;&#20123;&#36739;&#23567;&#23610;&#24230;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#24322;&#24120;&#20540;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#26469;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07929v1 Announce Type: new  Abstract: This paper introduces a novel, non-deterministic method for embedding data in low-dimensional Euclidean space based on computing realizations of a Gaussian process depending on the geometry of the data. This type of embedding first appeared in (Adler et al, 2018) as a theoretical model for a generic manifold in high dimensions.   In particular, we take the covariance function of the Gaussian process to be the heat kernel, and computing the embedding amounts to sketching a matrix representing the heat kernel. The Karhunen-Lo\`eve expansion reveals that the straight-line distances in the embedding approximate the diffusion distance in a probabilistic sense, avoiding the need for sharp cutoffs and maintaining some of the smaller-scale structure.   Our method demonstrates further advantage in its robustness to outliers. We justify the approach with both theory and experiments.
&lt;/p&gt;</description></item><item><title>ALL0CORE&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#23427;&#22312;&#20445;&#25345;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#30340;&#22522;&#30784;&#19978;&#21033;&#29992;Tucker&#20998;&#35299;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#26680;&#30340;&#24494;&#23567;&#37096;&#20998;&#21363;&#36798;&#21040;&#19982;&#23436;&#25972;Tucker&#20998;&#35299;&#30456;&#21516;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06153</link><description>&lt;p&gt;
ALL0CORE&#24352;&#37327;&#20998;&#35299;&#29992;&#20110;&#31232;&#30095;&#35745;&#25968;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
The ALL0CORE Tensor Decomposition for Sparse Count Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06153
&lt;/p&gt;
&lt;p&gt;
ALL0CORE&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#23427;&#22312;&#20445;&#25345;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#30340;&#22522;&#30784;&#19978;&#21033;&#29992;Tucker&#20998;&#35299;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#26680;&#30340;&#24494;&#23567;&#37096;&#20998;&#21363;&#36798;&#21040;&#19982;&#23436;&#25972;Tucker&#20998;&#35299;&#30456;&#21516;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ALL0CORE&#65292;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#24418;&#24335;&#12290;ALL0CORE&#26159;&#19968;&#31181;Tucker&#20998;&#35299;&#65292;&#20854;&#20013;&#26680;&#24352;&#37327;&#30340;&#38750;&#38646;&#20803;&#32032;&#25968;&#37327;&#65288;&#21363;L0&#33539;&#25968;&#65289;&#34987;&#38480;&#21046;&#20026;&#36828;&#23567;&#20110;&#26680;&#30340;&#22823;&#23567;&#30340;&#39044;&#35774;&#20540;Q&#12290;&#34429;&#28982;&#29992;&#25143;&#35268;&#23450;&#20102;&#24635;&#39044;&#31639;Q&#65292;&#20294;&#38750;&#38646;&#20803;&#32032;&#30340;&#20301;&#32622;&#21644;&#20540;&#26159;&#28508;&#22312;&#21464;&#37327;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20998;&#37197;&#32473;&#26680;&#24352;&#37327;&#30340;&#21508;&#20010;&#37096;&#20998;&#12290;ALL0CORE&#65292;&#21363;&#20998;&#37197;&#30340;L0&#32422;&#26463;&#26680;&#65292;&#22240;&#27492;&#26082;&#20855;&#26377;CP&#20998;&#35299;&#30340;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#65292;&#21448;&#20855;&#26377;Tucker&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ALL0CORE&#36890;&#24120;&#21482;&#38656;&#20351;&#29992;&#26680;&#30340;&#24494;&#23567;&#37096;&#20998;&#65288;&#20363;&#22914;&#65374;1%&#65289;&#21363;&#21487;&#20197;&#19982;&#23436;&#25972;Tucker&#20998;&#35299;&#30456;&#21516;&#30340;&#32467;&#26524;&#65292;&#32780;&#25104;&#26412;&#20165;&#30456;&#24212;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06153v1 Announce Type: cross  Abstract: This paper introduces ALL0CORE, a new form of probabilistic non-negative tensor decomposition. ALL0CORE is a Tucker decomposition where the number of non-zero elements (i.e., the L0-norm) of the core tensor is constrained to a preset value Q much smaller than the size of the core. While the user dictates the total budget Q, the locations and values of the non-zero elements are latent variables and allocated across the core tensor during inference. ALL0CORE -- i.e., allocated L0-constrained core -- thus enjoys both the computational tractability of CP decomposition and the qualitatively appealing latent structure of Tucker. In a suite of real-data experiments, we demonstrate that ALL0CORE typically requires only tiny fractions (e.g.,~1%) of the full core to achieve the same results as full Tucker decomposition at only a correspondingly tiny fraction of the cost.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#39640;&#26031;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#22312;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#35745;&#31639;&#26377;&#25928;&#31639;&#27861;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#22797;&#26434;&#24230;&#26159;&#20805;&#20998;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.05529</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#26031;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Computational Complexity of Learning Gaussian Single-Index Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#39640;&#26031;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#22312;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#35745;&#31639;&#26377;&#25928;&#31639;&#27861;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#22797;&#26434;&#24230;&#26159;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#25351;&#25968;&#27169;&#22411;&#26159;&#20855;&#26377;&#26893;&#20837;&#32467;&#26500;&#30340;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#26631;&#31614;&#20381;&#36182;&#20110;&#36890;&#36807;&#36890;&#29992;&#12289;&#38750;&#32447;&#24615;&#21644;&#28508;&#22312;&#38750;&#30830;&#23450;&#24615;&#36716;&#25442;&#30340;&#36755;&#20837;&#30340;&#26410;&#30693;&#19968;&#32500;&#25237;&#24433;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#32479;&#35745;&#25512;&#26029;&#20219;&#21153;&#31867;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#27169;&#26495;&#65292;&#29992;&#20110;&#30740;&#31350;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#25240;&#34935;&#12290;&#23613;&#31649;&#24674;&#22797;&#38544;&#34255;&#26041;&#21521;&#30340;&#20449;&#24687;&#35770;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#32500;&#24230;$d$&#26159;&#32447;&#24615;&#30340;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#32479;&#35745;&#26597;&#35810;&#65288;SQ&#65289;&#26694;&#26550;&#21644;&#20302;&#38454;&#22810;&#39033;&#24335;&#65288;LDP&#65289;&#26694;&#26550;&#20869;&#65292;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#24517;&#39035;&#38656;&#35201;$\Omega(d^{k^\star/2})$&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;$k^\star$&#26159;&#25105;&#20204;&#26126;&#30830;&#34920;&#24449;&#30340;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#8220;&#29983;&#25104;&#8221;&#25351;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#20351;&#29992;&#37096;&#20998;&#36857;&#30340;&#21305;&#37197;&#19978;&#30028;&#26469;&#35777;&#26126;&#36825;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#20063;&#26159;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05529v1 Announce Type: new  Abstract: Single-Index Models are high-dimensional regression problems with planted structure, whereby labels depend on an unknown one-dimensional projection of the input via a generic, non-linear, and potentially non-deterministic transformation. As such, they encompass a broad class of statistical inference tasks, and provide a rich template to study statistical and computational trade-offs in the high-dimensional regime.   While the information-theoretic sample complexity to recover the hidden direction is linear in the dimension $d$, we show that computationally efficient algorithms, both within the Statistical Query (SQ) and the Low-Degree Polynomial (LDP) framework, necessarily require $\Omega(d^{k^\star/2})$ samples, where $k^\star$ is a "generative" exponent associated with the model that we explicitly characterize. Moreover, we show that this sample complexity is also sufficient, by establishing matching upper bounds using a partial-trace
&lt;/p&gt;</description></item><item><title>Stein Boltzmann&#25277;&#26679;&#65288;SBS&#65289;&#26159;&#19968;&#31181;&#20840;&#23616;&#20248;&#21270;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;Boltzmann&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#30001;Stein Variational Gradient Descent&#31639;&#27861;&#23454;&#29616;&#65292;&#20855;&#26377;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#20989;&#25968;&#19978;&#27604;&#36739;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#65292;&#23588;&#20854;&#36866;&#21512;&#20316;&#20026;&#39640;&#25928;&#20840;&#23616;&#20248;&#21270;&#26041;&#27861;&#30340;&#24310;&#32493;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#26377;&#25928;&#21033;&#29992;&#39044;&#31639;&#12290;</title><link>https://arxiv.org/abs/2402.04689</link><description>&lt;p&gt;
Stein Boltzmann&#25277;&#26679;&#65306;&#19968;&#31181;&#20840;&#23616;&#20248;&#21270;&#30340;&#21464;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stein Boltzmann Sampling: A Variational Approach for Global Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04689
&lt;/p&gt;
&lt;p&gt;
Stein Boltzmann&#25277;&#26679;&#65288;SBS&#65289;&#26159;&#19968;&#31181;&#20840;&#23616;&#20248;&#21270;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;Boltzmann&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#30001;Stein Variational Gradient Descent&#31639;&#27861;&#23454;&#29616;&#65292;&#20855;&#26377;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#20989;&#25968;&#19978;&#27604;&#36739;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#65292;&#23588;&#20854;&#36866;&#21512;&#20316;&#20026;&#39640;&#25928;&#20840;&#23616;&#20248;&#21270;&#26041;&#27861;&#30340;&#24310;&#32493;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#26377;&#25928;&#21033;&#29992;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27969;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;Lipschitz&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#65292;&#31216;&#20026;Stein Boltzmann&#25277;&#26679;&#65288;SBS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;Boltzmann&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#35813;&#20998;&#24067;&#22312;&#20248;&#21270;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#38598;&#21512;&#19978;&#28176;&#36817;&#22343;&#21248;&#12290;&#20505;&#36873;&#35299;&#36890;&#36807;Stein Variational Gradient Descent&#31639;&#27861;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;SBS&#21464;&#20307;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#20989;&#25968;&#19978;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#35774;&#35745;&#12289;&#29702;&#35770;&#32467;&#26524;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;SBS&#29305;&#21035;&#36866;&#21512;&#20316;&#20026;&#39640;&#25928;&#20840;&#23616;&#20248;&#21270;&#26041;&#27861;&#30340;&#24310;&#32493;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#24456;&#22909;&#22320;&#21033;&#29992;&#39044;&#31639;&#30340;&#21516;&#26102;&#20135;&#29983;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new flow-based method for global optimization of Lipschitz functions, called Stein Boltzmann Sampling (SBS). Our method samples from the Boltzmann distribution that becomes asymptotically uniform over the set of the minimizers of the function to be optimized. Candidate solutions are sampled via the \emph{Stein Variational Gradient Descent} algorithm. We prove the asymptotic convergence of our method, introduce two SBS variants, and provide a detailed comparison with several state-of-the-art global optimization algorithms on various benchmark functions. The design of our method, the theoretical results, and our experiments, suggest that SBS is particularly well-suited to be used as a continuation of efficient global optimization methods as it can produce better solutions while making a good use of the budget.
&lt;/p&gt;</description></item><item><title>&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;3D&#21407;&#23376;&#31995;&#32479;&#20013;&#20197;&#21033;&#29992;&#29289;&#29702;&#23545;&#31216;&#24615;&#21644;&#21270;&#23398;&#24615;&#36136;&#31561;&#24402;&#32435;&#20559;&#24046;&#26469;&#23398;&#20064;&#20960;&#20309;&#22270;&#20449;&#24687;&#34920;&#31034;&#32780;&#33879;&#31216;&#12290;</title><link>https://arxiv.org/abs/2312.07511</link><description>&lt;p&gt;
&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;3D&#21407;&#23376;&#31995;&#32479;&#20013;&#30340;&#23454;&#36341;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07511
&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;3D&#21407;&#23376;&#31995;&#32479;&#20013;&#20197;&#21033;&#29992;&#29289;&#29702;&#23545;&#31216;&#24615;&#21644;&#21270;&#23398;&#24615;&#36136;&#31561;&#24402;&#32435;&#20559;&#24046;&#26469;&#23398;&#20064;&#20960;&#20309;&#22270;&#20449;&#24687;&#34920;&#31034;&#32780;&#33879;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39318;&#36873;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#23853;&#38706;&#22836;&#35282;&#65292;&#25903;&#25345;&#20174;&#34507;&#30333;&#32467;&#26500;&#39044;&#27979;&#21040;&#20998;&#23376;&#27169;&#25311;&#21644;&#26448;&#26009;&#29983;&#25104;&#31561;&#24212;&#29992;&#65292;&#20854;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#35832;&#22914;&#29289;&#29702;&#23545;&#31216;&#24615;&#21644;&#21270;&#23398;&#24615;&#36136;&#20043;&#31867;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#23398;&#20064;&#36825;&#20123;&#20960;&#20309;&#22270;&#30340;&#20449;&#24687;&#34920;&#31034;&#12290;&#22312;&#36825;&#31687;&#20027;&#35266;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#32780;&#33258;&#36275;&#22320;&#27010;&#36848;&#20102;&#29992;&#20110;3D&#21407;&#23376;&#31995;&#32479;&#30340;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07511v2 Announce Type: replace-cross  Abstract: Recent advances in computational modelling of atomic systems, spanning molecules, proteins, and materials, represent them as geometric graphs with atoms embedded as nodes in 3D Euclidean space. In these graphs, the geometric attributes transform according to the inherent physical symmetries of 3D atomic systems, including rotations and translations in Euclidean space, as well as node permutations. In recent years, Geometric Graph Neural Networks have emerged as the preferred machine learning architecture powering applications ranging from protein structure prediction to molecular simulations and material generation. Their specificity lies in the inductive biases they leverage - such as physical symmetries and chemical properties - to learn informative representations of these geometric graphs.   In this opinionated paper, we provide a comprehensive and self-contained overview of the field of Geometric GNNs for 3D atomic systems
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20960;&#20309;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#21738;&#31181;&#32467;&#26500;&#23646;&#24615;&#20135;&#29983;&#26368;&#26377;&#25928;&#30340;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;Ricci&#26354;&#29575;&#30340;&#26032;&#22411;&#32467;&#26500;&#32534;&#30721;&#65288;&#23616;&#37096;&#26354;&#29575;&#37197;&#32622;&#65292;LCP&#65289;&#65292;&#35777;&#26126;&#20854;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23558;&#23616;&#37096;&#32467;&#26500;&#32534;&#30721;&#19982;&#20840;&#23616;&#20301;&#32622;&#32534;&#30721;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#21319;&#19979;&#28216;&#24615;&#33021;&#65292;&#25429;&#25417;&#21040;&#20114;&#34917;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2311.14864</link><description>&lt;p&gt;
&#36890;&#36807;&#23616;&#37096;&#26354;&#29575;&#37197;&#32622;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Effective Structural Encodings via Local Curvature Profiles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20960;&#20309;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#21738;&#31181;&#32467;&#26500;&#23646;&#24615;&#20135;&#29983;&#26368;&#26377;&#25928;&#30340;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;Ricci&#26354;&#29575;&#30340;&#26032;&#22411;&#32467;&#26500;&#32534;&#30721;&#65288;&#23616;&#37096;&#26354;&#29575;&#37197;&#32622;&#65292;LCP&#65289;&#65292;&#35777;&#26126;&#20854;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23558;&#23616;&#37096;&#32467;&#26500;&#32534;&#30721;&#19982;&#20840;&#23616;&#20301;&#32622;&#32534;&#30721;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#21319;&#19979;&#28216;&#24615;&#33021;&#65292;&#25429;&#25417;&#21040;&#20114;&#34917;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21644;&#20301;&#32622;&#32534;&#30721;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#24320;&#22987;&#31995;&#32479;&#22320;&#30740;&#31350;&#36825;&#20123;&#26041;&#27861;&#32534;&#30721;&#30340;&#32467;&#26500;&#23646;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#24615;&#33021;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#21738;&#20123;&#32467;&#26500;&#23646;&#24615;&#20135;&#29983;&#26368;&#26377;&#25928;&#30340;&#32534;&#30721;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20960;&#20309;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;Ricci&#26354;&#29575;&#30340;&#26032;&#22411;&#32467;&#26500;&#32534;&#30721;&#65288;&#23616;&#37096;&#26354;&#29575;&#37197;&#32622;&#65292;&#31616;&#31216;LCP&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#32534;&#30721;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#23558;&#23616;&#37096;&#32467;&#26500;&#32534;&#30721;&#65288;&#22914;LCP&#65289;&#19982;&#20840;&#23616;&#20301;&#32622;&#32534;&#30721;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#21319;&#19979;&#28216;&#24615;&#33021;&#65292;&#34920;&#26126;&#23427;&#20204;&#25429;&#25417;&#20102;&#20114;&#34917;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#32534;&#30721;&#19982;&#65288;&#22522;&#20110;&#26354;&#29575;&#30340;&#65289;&#37325;&#36830;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14864v2 Announce Type: replace  Abstract: Structural and Positional Encodings can significantly improve the performance of Graph Neural Networks in downstream tasks. Recent literature has begun to systematically investigate differences in the structural properties that these approaches encode, as well as performance trade-offs between them. However, the question of which structural properties yield the most effective encoding remains open. In this paper, we investigate this question from a geometric perspective. We propose a novel structural encoding based on discrete Ricci curvature (Local Curvature Profiles, short LCP) and show that it significantly outperforms existing encoding approaches. We further show that combining local structural encodings, such as LCP, with global positional encodings improves downstream performance, suggesting that they capture complementary geometric information. Finally, we compare different encoding types with (curvature-based) rewiring techni
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#25511;&#21046;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#29366;&#24577;&#34920;&#31034;&#20989;&#25968;&#21644;&#25511;&#21046;&#22120;&#12290;</title><link>https://arxiv.org/abs/2212.14511</link><description>&lt;p&gt;
&#30452;&#25509;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#33021;&#22815;&#35299;&#20915;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#25511;&#21046;&#38382;&#39064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Direct Latent Model Learning Solve Linear Quadratic Gaussian Control?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.14511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#25511;&#21046;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#29366;&#24577;&#34920;&#31034;&#20989;&#25968;&#21644;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#28508;&#22312;&#39640;&#32500;&#35266;&#27979;&#20013;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#30340;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#25511;&#21046;&#26410;&#30693;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#31995;&#32479;&#12290;&#25105;&#20204;&#37319;&#29992;&#30452;&#25509;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#19982;&#35268;&#21010;&#30452;&#25509;&#30456;&#20851;&#30340;&#25968;&#37327;&#65288;&#20363;&#22914;&#25104;&#26412;&#65289;&#26469;&#23398;&#20064;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#37325;&#24314;&#35266;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#30452;&#35266;&#30340;&#22522;&#20110;&#25104;&#26412;&#39537;&#21160;&#30340;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#65288;LQG&#65289;&#25511;&#21046;&#38382;&#39064;&#65292;&#36825;&#26159;&#26368;&#22522;&#26412;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#25511;&#21046;&#38382;&#39064;&#20043;&#19968;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#29366;&#24577;&#34920;&#31034;&#20989;&#25968;&#21644;&#20351;&#29992;&#30452;&#25509;&#23398;&#20064;&#30340;&#28508;&#22312;&#27169;&#22411;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#25511;&#21046;&#22120;&#30340;&#20445;&#35777;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23613;&#31649;&#20197;&#21069;&#30340;&#30456;&#20851;&#24037;&#20316;&#21462;&#24471;&#20102;&#21508;&#31181;&#32463;&#39564;&#25104;&#21151;&#65292;&#20294;&#22312;&#36825;&#39033;&#24037;&#20316;&#20043;&#21069;&#65292;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#22522;&#20110;&#25104;&#26412;&#39537;&#21160;&#30340;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#26159;&#21542;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.14511v2 Announce Type: replace  Abstract: We study the task of learning state representations from potentially high-dimensional observations, with the goal of controlling an unknown partially observable system. We pursue a direct latent model learning approach, where a dynamic model in some latent state space is learned by predicting quantities directly related to planning (e.g., costs) without reconstructing the observations. In particular, we focus on an intuitive cost-driven state representation learning method for solving Linear Quadratic Gaussian (LQG) control, one of the most fundamental partially observable control problems. As our main results, we establish finite-sample guarantees of finding a near-optimal state representation function and a near-optimal controller using the directly learned latent model. To the best of our knowledge, despite various empirical successes, prior to this work it was unclear if such a cost-driven latent model learner enjoys finite-sampl
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#27491;&#30830;&#26657;&#20934;&#35823;&#24046;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26657;&#20934;&#35823;&#24046;&#19982;&#27491;&#30830;&#24471;&#20998;&#30456;&#20851;&#32852;&#65292;&#25552;&#20379;&#20102;&#26368;&#20339;&#20272;&#35745;&#29305;&#24615;&#30340;&#19978;&#30028;&#65292;&#21487;&#38752;&#37327;&#21270;&#27169;&#22411;&#26657;&#20934;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2203.07835</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#30830;&#24471;&#20998;&#25913;&#36827;&#20998;&#31867;&#21450;&#20854;&#20182;&#20219;&#21153;&#30340;&#26356;&#22909;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Better Uncertainty Calibration via Proper Scores for Classification and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.07835
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#27491;&#30830;&#26657;&#20934;&#35823;&#24046;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26657;&#20934;&#35823;&#24046;&#19982;&#27491;&#30830;&#24471;&#20998;&#30456;&#20851;&#32852;&#65292;&#25552;&#20379;&#20102;&#26368;&#20339;&#20272;&#35745;&#29305;&#24615;&#30340;&#19978;&#30028;&#65292;&#21487;&#38752;&#37327;&#21270;&#27169;&#22411;&#26657;&#20934;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#23545;&#20110;&#25935;&#24863;&#30340;&#29616;&#23454;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20174;&#19994;&#32773;&#36234;&#26469;&#36234;&#20851;&#27880;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;&#26657;&#20934;&#35823;&#24046;&#26088;&#22312;&#37327;&#21270;&#27010;&#29575;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#20294;&#23427;&#20204;&#30340;&#20272;&#35745;&#36890;&#24120;&#26159;&#26377;&#20559;&#24046;&#19988;&#19981;&#19968;&#33268;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27491;&#30830;&#26657;&#20934;&#35823;&#24046;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#27599;&#20010;&#26657;&#20934;&#35823;&#24046;&#19982;&#27491;&#30830;&#24471;&#20998;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#20272;&#35745;&#29305;&#24615;&#30340;&#30456;&#24212;&#19978;&#30028;&#12290;&#36825;&#31181;&#20851;&#31995;&#21487;&#29992;&#20110;&#21487;&#38752;&#22320;&#37327;&#21270;&#27169;&#22411;&#26657;&#20934;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#23637;&#31034;&#20102;&#19982;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#24120;&#29992;&#20272;&#35745;&#22120;&#30340;&#32570;&#38519;&#12290;&#30001;&#20110;&#27491;&#30830;&#24471;&#20998;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#36825;&#20026;&#36229;&#20986;&#20998;&#31867;&#30340;&#37325;&#26032;&#26657;&#20934;&#25552;&#20379;&#20102;&#33258;&#28982;&#24310;&#20280;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.07835v4 Announce Type: replace  Abstract: With model trustworthiness being crucial for sensitive real-world applications, practitioners are putting more and more focus on improving the uncertainty calibration of deep neural networks. Calibration errors are designed to quantify the reliability of probabilistic predictions but their estimators are usually biased and inconsistent. In this work, we introduce the framework of proper calibration errors, which relates every calibration error to a proper score and provides a respective upper bound with optimal estimation properties. This relationship can be used to reliably quantify the model calibration improvement. We theoretically and empirically demonstrate the shortcomings of commonly used estimators compared to our approach. Due to the wide applicability of proper scores, this gives a natural extension of recalibration beyond classification.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20122;&#32447;&#24615;&#26102;&#38388;&#19979;&#30340;&#32456;&#31471;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#25197;&#26354;$1+\epsilon$&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#36890;&#29992;&#19988;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2110.08691</link><description>&lt;p&gt;
&#20122;&#32447;&#24615;&#26102;&#38388;&#19979;&#30340;&#32456;&#31471;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Terminal Embeddings in Sublinear Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.08691
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20122;&#32447;&#24615;&#26102;&#38388;&#19979;&#30340;&#32456;&#31471;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#25197;&#26354;$1+\epsilon$&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#36890;&#29992;&#19988;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65288;Elkin, Filtser, Neiman 2017&#65289;&#24341;&#20837;&#20102;&#20174;&#19968;&#20010;&#20855;&#26377;&#19968;&#32452;&#25351;&#23450;&#32456;&#31471;$T \subset X$&#30340;&#24230;&#37327;&#31354;&#38388;$(X,d_X)$&#21040;&#21478;&#19968;&#20010;$(Y,d_Y)$&#30340;{\it &#32456;&#31471;&#23884;&#20837;}&#30340;&#27010;&#24565;&#12290;&#24403;$X,Y$&#37117;&#26159;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#65292;&#20854;&#20013;$Y$&#26159;$m$&#32500;&#26102;&#65292;&#26368;&#36817;&#65288;Narayanan, Nelson 2019&#65289;&#22312;&#65288;Mahabadi, Makarychev, Makarychev, Razenshteyn 2018&#65289;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#26679;&#19968;&#20010;&#32456;&#31471;&#23884;&#20837;&#23454;&#29616;&#25197;&#26354;$1+\epsilon$&#65292;&#20854;&#20013;$m = O(\epsilon^{-2}\log n)$&#65292;&#20854;&#20013;$n := |T|$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.08691v3 Announce Type: replace-cross  Abstract: Recently (Elkin, Filtser, Neiman 2017) introduced the concept of a {\it terminal embedding} from one metric space $(X,d_X)$ to another $(Y,d_Y)$ with a set of designated terminals $T\subset X$. Such an embedding $f$ is said to have distortion $\rho\ge 1$ if $\rho$ is the smallest value such that there exists a constant $C&gt;0$ satisfying   \begin{equation*}   \forall x\in T\ \forall q\in X,\ C d_X(x, q) \le d_Y(f(x), f(q)) \le C \rho d_X(x, q) .   \end{equation*}   When $X,Y$ are both Euclidean metrics with $Y$ being $m$-dimensional, recently (Narayanan, Nelson 2019), following work of (Mahabadi, Makarychev, Makarychev, Razenshteyn 2018), showed that distortion $1+\epsilon$ is achievable via such a terminal embedding with $m = O(\epsilon^{-2}\log n)$ for $n := |T|$. This generalizes the Johnson-Lindenstrauss lemma, which only preserves distances within $T$ and not to $T$ from the rest of space. The downside of prior work is that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#20998;&#25968;&#20272;&#35745;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23545;&#20998;&#25968;&#20272;&#35745;&#36827;&#34892;&#20998;&#26512;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2401.15604</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20998;&#25968;&#20272;&#35745;&#65306;&#20248;&#21270;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization. (arXiv:2401.15604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#20998;&#25968;&#20272;&#35745;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23545;&#20998;&#25968;&#20272;&#35745;&#36827;&#34892;&#20998;&#26512;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19982;GANs&#30456;&#23218;&#32654;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#20445;&#30495;&#24230;&#65292;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#26469;&#23398;&#20064;&#20998;&#25968;&#20989;&#25968;&#12290;&#23613;&#31649;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#20197;&#21487;&#35777;&#23454;&#30340;&#20934;&#30830;&#24615;&#23398;&#20064;&#20998;&#25968;&#20989;&#25968;&#12290;&#20316;&#20026;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#30340;&#39318;&#35201;&#27493;&#39588;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20998;&#25968;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;&#23398;&#20064;&#36807;&#31243;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#26041;&#38754;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#24418;&#24335;&#26469;&#23558;&#21435;&#22122;&#20998;&#25968;&#21305;&#37197;&#38382;&#39064;&#21046;&#23450;&#20026;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#22238;&#24402;&#38382;&#39064;&#12290;&#19982;&#26631;&#20934;&#30340;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#30456;&#27604;&#65292;&#20998;&#25968;&#21305;&#37197;&#38382;&#39064;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#26080;&#30028;&#36755;&#20837;&#65292;&#21521;&#37327;&#20540;&#36755;&#20986;&#21644;&#39069;&#22806;&#30340;&#26102;&#38388;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a powerful tool rivaling GANs in generating high-quality samples with improved fidelity, flexibility, and robustness. A key component of these models is to learn the score function through score matching. Despite empirical success on various tasks, it remains unclear whether gradient-based algorithms can learn the score function with a provable accuracy. As a first step toward answering this question, this paper establishes a mathematical framework for analyzing score estimation using neural networks trained by gradient descent. Our analysis covers both the optimization and the generalization aspects of the learning procedure. In particular, we propose a parametric form to formulate the denoising score-matching problem as a regression with noisy labels. Compared to the standard supervised learning setup, the score-matching problem introduces distinct challenges, including unbounded input, vector-valued output, and an additional time variable, preventing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#36951;&#25022;&#21040;&#32622;&#20449;&#38598;&#36716;&#25442;&#26041;&#27861;&#25913;&#36827;&#20102;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#20984;&#32622;&#20449;&#38598;&#65292;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#26032;&#30340;&#38789;&#38598;&#20013;&#27493;&#39588;&#30340;&#36951;&#25022;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.18554</link><description>&lt;p&gt;
&#36890;&#36807;&#36951;&#25022;&#21040;&#32622;&#20449;&#38598;&#36716;&#25442;&#25913;&#36827;&#65288;&#22810;&#39033;&#24335;&#65289;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Improved Regret Bounds of (Multinomial) Logistic Bandits via Regret-to-Confidence-Set Conversion. (arXiv:2310.18554v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#36951;&#25022;&#21040;&#32622;&#20449;&#38598;&#36716;&#25442;&#26041;&#27861;&#25913;&#36827;&#20102;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#20984;&#32622;&#20449;&#38598;&#65292;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#26032;&#30340;&#38789;&#38598;&#20013;&#27493;&#39588;&#30340;&#36951;&#25022;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#26159;&#24314;&#27169;&#29992;&#25143;&#36873;&#25321;&#30340;&#26222;&#36941;&#26694;&#26550;&#65292;&#20363;&#22914;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28857;&#20987;&#19982;&#21542;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20808;&#21069;&#30340;&#24037;&#20316;&#24573;&#35270;&#25110;&#24573;&#30053;&#20102;$S \geq \lVert \theta_\star \rVert_2$&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20854;&#20013;$\theta_\star \in \mathbb{R}^d$&#26159;&#26410;&#30693;&#30340;&#21442;&#25968;&#21521;&#37327;&#65292;&#24403;$S$&#36739;&#22823;&#26102;&#65292;&#20363;&#22914;$S \geq d$&#65292;&#36825;&#20250;&#20135;&#29983;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#8220;&#36951;&#25022;&#21040;&#32622;&#20449;&#38598;&#36716;&#25442;&#65288;R2CS&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#25913;&#21892;&#20102;&#23545;$S$&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#23384;&#22312;&#24615;&#30340;&#20984;&#32622;&#20449;&#38598;&#12290;&#20351;&#29992;R2CS&#65292;&#25105;&#20204;&#22312;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#30028;&#38480;&#26041;&#38754;&#33719;&#24471;&#20102;&#20005;&#26684;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#35745;&#31639;&#21487;&#34892;&#24615;&#21644;&#23545;&#20854;&#20182;&#22240;&#32032;&#65288;&#22914;$d$&#21644;$T$&#65289;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26032;&#32622;&#20449;&#38598;&#24212;&#29992;&#20110;&#20855;&#26377;&#26032;&#30340;&#38789;&#38598;&#20013;&#27493;&#39588;&#30340;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#20998;&#26512;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logistic bandit is a ubiquitous framework of modeling users' choices, e.g., click vs. no click for advertisement recommender system. We observe that the prior works overlook or neglect dependencies in $S \geq \lVert \theta_\star \rVert_2$, where $\theta_\star \in \mathbb{R}^d$ is the unknown parameter vector, which is particularly problematic when $S$ is large, e.g., $S \geq d$. In this work, we improve the dependency on $S$ via a novel approach called {\it regret-to-confidence set conversion (R2CS)}, which allows us to construct a convex confidence set based on only the \textit{existence} of an online learning algorithm with a regret guarantee. Using R2CS, we obtain a strict improvement in the regret bound w.r.t. $S$ in logistic bandits while retaining computational feasibility and the dependence on other factors such as $d$ and $T$. We apply our new confidence set to the regret analyses of logistic bandits with a new martingale concentration step that circumvents an additional factor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#27979;&#24230;&#21040;&#27979;&#24230;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#12289;&#30830;&#23450;&#24615;&#33945;&#26684;&#26144;&#23556;&#12289;&#26144;&#23556;&#36328;&#19981;&#21487;&#27604;&#36739;&#31354;&#38388;&#21644;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.09254</link><description>&lt;p&gt;
&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#31354;&#38388;&#20869;&#22806;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Entropic Neural Optimal Transport To Map Within and Across Spaces. (arXiv:2310.09254v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09254
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#27979;&#24230;&#21040;&#27979;&#24230;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#12289;&#30830;&#23450;&#24615;&#33945;&#26684;&#26144;&#23556;&#12289;&#26144;&#23556;&#36328;&#19981;&#21487;&#27604;&#36739;&#31354;&#38388;&#21644;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27979;&#24230;&#21040;&#27979;&#24230;&#30340;&#26144;&#23556;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#23588;&#20854;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#21344;&#25454;&#37325;&#35201;&#22320;&#20301;&#12290;&#36817;&#24180;&#26469;&#65292;&#21463;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21551;&#21457;&#30340;&#25216;&#26415;&#19981;&#26029;&#28044;&#29616;&#12290;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#20123;&#26041;&#27861;&#32479;&#31216;&#20026;"&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;"&#65292;&#23558;&#26368;&#20248;&#20256;&#36755;&#20316;&#20026;&#24402;&#32435;&#20559;&#22909;&#65306;&#36825;&#20123;&#26144;&#23556;&#24212;&#35813;&#38024;&#23545;&#32473;&#23450;&#30340;&#25104;&#26412;&#20989;&#25968;&#26159;&#26368;&#20248;&#30340;&#65292;&#33021;&#20197;&#33410;&#32422;&#30340;&#26041;&#24335;&#65288;&#36890;&#36807;&#26368;&#23567;&#21270;&#20301;&#31227;&#65289;&#22312;&#31354;&#38388;&#20869;&#25110;&#31354;&#38388;&#38388;&#31227;&#21160;&#28857;&#12290;&#36825;&#19968;&#21407;&#21017;&#22312;&#30452;&#35266;&#19978;&#26159;&#21512;&#29702;&#30340;&#65292;&#20294;&#24448;&#24448;&#38754;&#20020;&#20960;&#20010;&#23454;&#38469;&#25361;&#25112;&#65292;&#38656;&#35201;&#35843;&#25972;&#26368;&#20248;&#20256;&#36755;&#24037;&#20855;&#31665;&#65306;&#22788;&#29702;&#20854;&#20182;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#30340;&#25361;&#25112;&#65292;&#30830;&#23450;&#24615;&#29366;&#20917;&#19979;&#30340;&#33945;&#26684;&#26144;&#23556;&#20844;&#24335;&#20250;&#38480;&#21046;&#28789;&#27963;&#24615;&#65292;&#26144;&#23556;&#22312;&#19981;&#21487;&#27604;&#36739;&#30340;&#31354;&#38388;&#20013;&#20250;&#24102;&#26469;&#22810;&#20010;&#25361;&#25112;&#65292;&#26368;&#20248;&#20256;&#36755;&#22266;&#26377;&#30340;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#21487;&#33021;&#23545;&#24322;&#24120;&#25968;&#25454;&#32473;&#20104;&#36807;&#22810;&#30340;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning measure-to-measure mappings is a crucial task in machine learning, featured prominently in generative modeling. Recent years have witnessed a surge of techniques that draw inspiration from optimal transport (OT) theory. Combined with neural network models, these methods collectively known as \textit{Neural OT} use optimal transport as an inductive bias: such mappings should be optimal w.r.t. a given cost function, in the sense that they are able to move points in a thrifty way, within (by minimizing displacements) or across spaces (by being isometric). This principle, while intuitive, is often confronted with several practical challenges that require adapting the OT toolbox: cost functions other than the squared-Euclidean cost can be challenging to handle, the deterministic formulation of Monge maps leaves little flexibility, mapping across incomparable spaces raises multiple challenges, while the mass conservation constraint inherent to OT can provide too much credit to outli
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.02279</link><description>&lt;p&gt;
&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65306;&#23398;&#20064;&#25193;&#25955;&#30340;&#27010;&#29575;&#27969;ODE&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. (arXiv:2310.02279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02279
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CM&#65289;&#21152;&#36895;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#65292;&#20294;&#20197;&#29306;&#29298;&#26679;&#26412;&#36136;&#37327;&#20026;&#20195;&#20215;&#65292;&#32570;&#20047;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#26435;&#34913;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#26159;&#21253;&#25324;CM&#21644;&#22522;&#20110;&#24471;&#20998;&#27169;&#22411;&#22312;&#20869;&#30340;&#27867;&#21270;&#27169;&#22411;&#12290;CTM&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#36755;&#20986;&#24471;&#20998;&#65288;&#21363;&#23545;&#25968;&#23494;&#24230;&#30340;&#26799;&#24230;&#65289;&#65292;&#24182;&#20801;&#35768;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#20219;&#24847;&#21021;&#22987;&#21644;&#26368;&#32456;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#19981;&#21463;&#38480;&#21046;&#30340;&#36941;&#21382;&#27010;&#29575;&#27969;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;CTM&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#26377;&#25928;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#65288;FID 1.73&#65289;&#21644;64X64&#20998;&#36776;&#29575;&#30340;ImageNet&#19978;&#23454;&#29616;&#26032;&#30340;&#26368;&#20808;&#36827;FID&#12290;CTM&#36824;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#21253;&#25324;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#30340;ODE&#35299;&#20013;&#30340;&#38271;&#36339;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE soluti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#24674;&#22797;&#25110;&#36229;&#36234;&#20102;&#24050;&#30693;&#30340;&#36335;&#24452;&#33539;&#25968;&#30028;&#38480;&#65292;&#24182;&#25361;&#25112;&#20102;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#19968;&#20123;&#20855;&#20307;&#25215;&#35834;&#12290;</title><link>http://arxiv.org/abs/2310.01225</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#20195;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65306;&#24433;&#21709;&#12289;&#21069;&#26223;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A path-norm toolkit for modern networks: consequences, promises and challenges. (arXiv:2310.01225v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#24674;&#22797;&#25110;&#36229;&#36234;&#20102;&#24050;&#30693;&#30340;&#36335;&#24452;&#33539;&#25968;&#30028;&#38480;&#65292;&#24182;&#25361;&#25112;&#20102;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#19968;&#20123;&#20855;&#20307;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#33021;&#22815;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#36866;&#29992;&#20110;&#26368;&#24191;&#27867;&#30340;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#21487;&#20197;&#24674;&#22797;&#25110;&#36229;&#36234;&#24050;&#30693;&#30340;&#27492;&#31867;&#33539;&#25968;&#30340;&#26368;&#23574;&#38160;&#30028;&#38480;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#36335;&#24452;&#33539;&#25968;&#36824;&#20139;&#26377;&#36335;&#24452;&#33539;&#25968;&#30340;&#24120;&#35268;&#20248;&#28857;&#65306;&#35745;&#31639;&#31616;&#20415;&#12289;&#23545;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#22312;&#21069;&#39304;&#32593;&#32476;&#19978;&#27604;&#25805;&#20316;&#31526;&#33539;&#25968;&#30340;&#20056;&#31215;&#65288;&#21478;&#19968;&#31181;&#24120;&#29992;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#38160;&#24230;&#12290;&#24037;&#20855;&#21253;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#26131;&#20110;&#23454;&#26045;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#25968;&#20540;&#35780;&#20272;&#22312;ImageNet&#19978;&#23545;ResNet&#30340;&#26368;&#23574;&#38160;&#30028;&#38480;&#26469;&#25361;&#25112;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#20855;&#20307;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the first toolkit around path-norms that is fully able to encompass general DAG ReLU networks with biases, skip connections and max pooling. This toolkit notably allows us to establish generalization bounds for real modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms further enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on feedforward networks compared to the product of operators' norms, another complexity measure most commonly used.  The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.
&lt;/p&gt;</description></item><item><title>DataInf&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#24433;&#21709;&#21147;&#36817;&#20284;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#19978;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.00902</link><description>&lt;p&gt;
DataInf&#65306;&#22312;LLMs&#21644;&#25193;&#25955;&#27169;&#22411;&#20013;&#39640;&#25928;&#20272;&#35745;&#25968;&#25454;&#24433;&#21709;&#21147;
&lt;/p&gt;
&lt;p&gt;
DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models. (arXiv:2310.00902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00902
&lt;/p&gt;
&lt;p&gt;
DataInf&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#24433;&#21709;&#21147;&#36817;&#20284;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#19978;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#21147;&#23545;&#20110;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#25552;&#39640;AI&#31649;&#36947;&#30340;&#36879;&#26126;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#24433;&#21709;&#20989;&#25968;&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#27969;&#34892;&#30340;&#25968;&#25454;&#24402;&#23646;&#26041;&#27861;&#65292;&#20294;&#20854;&#35745;&#31639;&#25104;&#26412;&#20351;&#20854;&#38590;&#20197;&#20351;&#29992;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#26356;&#21152;&#31361;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DataInf&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#24433;&#21709;&#21147;&#36817;&#20284;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#26131;&#20110;&#35745;&#31639;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;DataInf&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#24433;&#21709;&#35745;&#31639;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;DataInf&#29305;&#21035;&#36866;&#29992;&#20110;&#35832;&#22914;LoRA&#30340;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#36890;&#36807;&#31995;&#32479;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DataInf&#33021;&#22815;&#20934;&#30830;&#22320;&#36817;&#20284;&#24433;&#21709;&#20998;&#25968;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#27979;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#38750;&#32447;&#24615;&#27604;&#36739;&#22797;&#26434;&#30340;&#32454;&#32990;&#38388;&#20998;&#23376;&#29305;&#24449;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#26680;&#23884;&#20837;&#30340;&#21464;&#24322;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25581;&#31034;&#32454;&#32990;&#32676;&#20307;&#20013;&#38544;&#34109;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26680;&#27979;&#35797;&#22914;&#20309;&#20811;&#26381;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#30740;&#31350;&#20998;&#21270;&#36870;&#36716;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.08509</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Kernel-Based Testing for Single-Cell Differential Analysis. (arXiv:2307.08509v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#27979;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#38750;&#32447;&#24615;&#27604;&#36739;&#22797;&#26434;&#30340;&#32454;&#32990;&#38388;&#20998;&#23376;&#29305;&#24449;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#26680;&#23884;&#20837;&#30340;&#21464;&#24322;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25581;&#31034;&#32454;&#32990;&#32676;&#20307;&#20013;&#38544;&#34109;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26680;&#27979;&#35797;&#22914;&#20309;&#20811;&#26381;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#30740;&#31350;&#20998;&#21270;&#36870;&#36716;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#32454;&#32990;&#25216;&#26415;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#22240;&#34920;&#36798;&#21644;&#34920;&#35266;&#36951;&#20256;&#20462;&#39280;&#31561;&#20998;&#23376;&#29305;&#24449;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20197;&#25511;&#21046;&#21644;&#24378;&#26377;&#21147;&#30340;&#26041;&#24335;&#27604;&#36739;&#36825;&#20123;&#22797;&#26434;&#20998;&#24067;&#38754;&#20020;&#30528;&#26041;&#27861;&#35770;&#19978;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#26680;&#23884;&#20837;&#30340;&#26680;&#27979;&#35797;&#26694;&#26550;&#26469;&#38750;&#32447;&#24615;&#27604;&#36739;&#32454;&#32990;&#38388;&#22797;&#26434;&#20998;&#23376;&#29305;&#24449;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;&#20801;&#35768;&#23545;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#65292;&#36824;&#33021;&#22312;&#32771;&#34385;&#20102;&#23427;&#20204;&#20043;&#38388;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#36716;&#24405;&#32452;&#25110;&#34920;&#35266;&#32452;&#30340;&#20840;&#23616;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#22120;&#22522;&#20110;&#26680;&#23884;&#20837;&#30340;&#21464;&#24322;&#24615;&#26469;&#21306;&#20998;&#32454;&#32990;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21457;&#29616;&#22312;&#32454;&#32990;&#32676;&#20307;&#20013;&#21407;&#26412;&#26080;&#27861;&#23519;&#35273;&#21040;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26680;&#27979;&#35797;&#26041;&#27861;&#22914;&#20309;&#20811;&#26381;&#19987;&#38376;&#29992;&#20110;&#21333;&#32454;&#32990;&#30340;&#24046;&#24322;&#20998;&#26512;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#26680;&#27979;&#35797;&#24212;&#29992;&#20110;&#30740;&#31350;&#20998;&#21270;&#36870;&#36716;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-cell technologies have provided valuable insights into the distribution of molecular features, such as gene expression and epigenomic modifications. However, comparing these complex distributions in a controlled and powerful manner poses methodological challenges. Here we propose to benefit from the kernel-testing framework to compare the complex cell-wise distributions of molecular features in a non-linear manner based on their kernel embedding. Our framework not only allows for feature-wise analyses but also enables global comparisons of transcriptomes or epigenomes, considering their intricate dependencies. By using a classifier to discriminate cells based on the variability of their embedding, our method uncovers heterogeneities in cell populations that would otherwise go undetected. We show that kernel testing overcomes the limitations of differential analysis methods dedicated to single-cell. Kernel testing is applied to investigate the reversion process of differentiating
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#31561;&#28183;&#24615;&#30340;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#20102;&#26032;&#39062;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65292;&#22312;&#39640;&#32500;&#37319;&#26679;&#20013;&#34920;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02037</link><description>&lt;p&gt;
&#26080;&#31561;&#28183;&#24615;&#30340;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#65306;&#19968;&#31181;&#36870;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach. (arXiv:2307.02037v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#31561;&#28183;&#24615;&#30340;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#20102;&#26032;&#39062;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65292;&#22312;&#39640;&#32500;&#37319;&#26679;&#20013;&#34920;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#36890;&#24120;&#21462;&#20915;&#20110;&#25193;&#25955;&#36335;&#24452;&#19978;&#24471;&#20998;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#37325;&#28857;&#20851;&#27880;&#25193;&#25955;&#27169;&#22411;&#21450;&#20854;&#29983;&#25104;&#39640;&#36136;&#37327;&#25968;&#25454;&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#36890;&#36807;&#36870;&#25193;&#25955;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#23545;&#37319;&#26679;&#25991;&#29486;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#26680;&#30340;&#20998;&#35299;&#23558;&#24471;&#20998;&#20272;&#35745;&#36716;&#21270;&#20026;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#20272;&#35745;&#36741;&#21161;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#36870;&#25193;&#25955;&#36807;&#31243;&#21487;&#20197;&#20135;&#29983;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24635;&#21464;&#24046;&#36317;&#31163;&#19979;&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#31561;&#28183;&#24615;&#20381;&#36182;&#24615;&#30456;&#23545;&#36739;&#20302;&#65292;&#27604;&#20256;&#32479;&#30340;MCMC&#25216;&#26415;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#39640;&#32500;&#37319;&#26679;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficacy of modern generative models is commonly contingent upon the precision of score estimation along the diffusion path, with a focus on diffusion models and their ability to generate high-quality data samples. This study delves into the potentialities of posterior sampling through reverse diffusion. An examination of the sampling literature reveals that score estimation can be transformed into a mean estimation problem via the decomposition of the transition kernel. By estimating the mean of the auxiliary distribution, the reverse diffusion process can give rise to a novel posterior sampling algorithm, which diverges from traditional gradient-based Markov Chain Monte Carlo (MCMC) methods. We provide the convergence analysis in total variation distance and demonstrate that the isoperimetric dependency of the proposed algorithm is comparatively lower than that observed in conventional MCMC techniques, which justifies the superior performance for high dimensional sampling with er
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#22238;&#24402;&#22120;&#26816;&#27979;&#25968;&#20540;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#19982;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#30495;&#27491;&#30340;&#24322;&#24120;&#21644;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.16583</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#24847;&#22238;&#24402;&#27169;&#22411;&#26816;&#27979;&#25968;&#20540;&#25968;&#25454;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting Errors in Numerical Data via any Regression Model. (arXiv:2305.16583v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16583
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#22238;&#24402;&#22120;&#26816;&#27979;&#25968;&#20540;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#19982;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#30495;&#27491;&#30340;&#24322;&#24120;&#21644;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#22256;&#25200;&#30528;&#35768;&#22810;&#25968;&#20540;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#25968;&#25454;&#35760;&#24405;&#30340;&#20540;&#21487;&#33021;&#30001;&#20110;&#38169;&#35823;&#30340;&#20256;&#24863;&#22120;&#12289;&#25968;&#25454;&#36755;&#20837;/&#22788;&#29702;&#38169;&#35823;&#25110;&#19981;&#23436;&#32654;&#30340;&#20154;&#31867;&#20272;&#35745;&#31561;&#21407;&#22240;&#32780;&#26080;&#27861;&#21305;&#37197;&#30495;&#23454;&#30340;&#24213;&#23618;&#20540;&#12290;&#25105;&#20204;&#32771;&#34385;&#20272;&#35745;&#27839;&#25968;&#20540;&#21015;&#21738;&#20123;&#25968;&#25454;&#20540;&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#22238;&#24402;&#22120;&#65288;&#21363;&#22522;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#20854;&#20182;&#21464;&#37327;&#26469;&#39044;&#27979;&#35813;&#21015;&#20540;&#30340;&#32479;&#35745;&#23398;&#25110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21306;&#20998;&#20102;&#30495;&#27491;&#30340;&#24322;&#24120;&#21644;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#65292;&#26465;&#20214;&#26159;&#26377;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#34920;&#26126;&#20854;&#20182;&#26041;&#27861;&#65288;&#22914;&#31526;&#21512;&#24615;&#25512;&#26029;&#65289;&#38590;&#20197;&#26816;&#27979;&#38169;&#35823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35823;&#24046;&#26816;&#27979;&#22522;&#20934;&#65292;&#28041;&#21450; 5 &#20010;&#20855;&#26377;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#38169;&#35823;&#30340;&#22238;&#24402;&#25968;&#25454;&#38598;&#65288;&#23545;&#20110;&#20854;&#20013;&#30340;&#30495;&#23454;&#20540;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noise plagues many numerical datasets, where the recorded values in the data may fail to match the true underlying values due to reasons including: erroneous sensors, data entry/processing mistakes, or imperfect human estimates. Here we consider estimating \emph{which} data values are incorrect along a numerical column. We present a model-agnostic approach that can utilize \emph{any} regressor (i.e.\ statistical or machine learning model) which was fit to predict values in this column based on the other variables in the dataset. By accounting for various uncertainties, our approach distinguishes between genuine anomalies and natural data fluctuations, conditioned on the available information in the dataset. We establish theoretical guarantees for our method and show that other approaches like conformal inference struggle to detect errors. We also contribute a new error detection benchmark involving 5 regression datasets with real-world numerical errors (for which the true values are al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#20808;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#38750;&#31232;&#30095;&#36229;&#21442;&#25968;&#32447;&#24615;&#27169;&#22411;&#12290;&#20174;&#23548;&#20986;&#30340;&#21518;&#39564;&#20998;&#24067;&#25910;&#32553;&#29575;&#21644;&#24320;&#21457;&#30340;&#25130;&#26029;&#39640;&#26031;&#36817;&#20284;&#20004;&#20010;&#26041;&#38754;&#26469;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#35299;&#20915;&#20043;&#21069;&#30340;&#20808;&#39564;&#31232;&#30095;&#24615;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15754</link><description>&lt;p&gt;
&#38750;&#31232;&#30095;&#36229;&#21442;&#25968;&#32447;&#24615;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Bayesian Analysis for Over-parameterized Linear Model without Sparsity. (arXiv:2305.15754v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#20808;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#38750;&#31232;&#30095;&#36229;&#21442;&#25968;&#32447;&#24615;&#27169;&#22411;&#12290;&#20174;&#23548;&#20986;&#30340;&#21518;&#39564;&#20998;&#24067;&#25910;&#32553;&#29575;&#21644;&#24320;&#21457;&#30340;&#25130;&#26029;&#39640;&#26031;&#36817;&#20284;&#20004;&#20010;&#26041;&#38754;&#26469;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#35299;&#20915;&#20043;&#21069;&#30340;&#20808;&#39564;&#31232;&#30095;&#24615;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#20013;&#65292;&#21457;&#23637;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#21253;&#25324;&#35768;&#22810;&#20808;&#39564;&#20998;&#24067;&#65292;&#23427;&#20204;&#23548;&#33268;&#20272;&#35745;&#21442;&#25968;&#30340;&#31232;&#30095;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20808;&#39564;&#22312;&#22788;&#29702;&#25968;&#25454;&#30340;&#35889;&#29305;&#24449;&#21521;&#37327;&#32467;&#26500;&#26041;&#38754;&#26377;&#23616;&#38480;&#24615;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#20998;&#26512;&#26368;&#36817;&#21457;&#23637;&#30340;&#19981;&#20551;&#35774;&#31232;&#30095;&#24615;&#30340;&#39640;&#32500;&#32447;&#24615;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#20381;&#36182;&#20110;&#25968;&#25454;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#20808;&#39564;&#65292;&#20294;&#19981;&#20250;&#24341;&#36215;&#21442;&#25968;&#30340;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23548;&#20986;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#25910;&#32553;&#29575;&#65292;&#24182;&#24320;&#21457;&#20102;&#21518;&#39564;&#20998;&#24067;&#30340;&#25130;&#26029;&#39640;&#26031;&#36817;&#20284;&#12290;&#21069;&#32773;&#35777;&#26126;&#20102;&#21518;&#39564;&#20272;&#35745;&#30340;&#25928;&#29575;&#65292;&#32780;&#21518;&#32773;&#21017;&#20351;&#29992;Bernstein-von Mises&#31867;&#22411;&#26041;&#27861;&#26469;&#37327;&#21270;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#20219;&#20309;&#33021;&#22815;&#22788;&#29702;&#35889;&#29305;&#24449;&#21521;&#37327;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#37117;&#21487;&#20197;&#29992;&#20110;&#38750;&#31232;&#30095;&#36229;&#21442;&#25968;&#32447;&#24615;&#27169;&#22411;&#20998;&#26512;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20808;&#21069;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In high-dimensional Bayesian statistics, several methods have been developed, including many prior distributions that lead to the sparsity of estimated parameters. However, such priors have limitations in handling the spectral eigenvector structure of data, and as a result, they are ill-suited for analyzing over-parameterized models (high-dimensional linear models that do not assume sparsity) that have been developed in recent years. This paper introduces a Bayesian approach that uses a prior dependent on the eigenvectors of data covariance matrices, but does not induce the sparsity of parameters. We also provide contraction rates of derived posterior distributions and develop a truncated Gaussian approximation of the posterior distribution. The former demonstrates the efficiency of posterior estimation, while the latter enables quantification of parameter uncertainty using a Bernstein-von Mises-type approach. These results indicate that any Bayesian method that can handle the spectrum
&lt;/p&gt;</description></item></channel></rss>