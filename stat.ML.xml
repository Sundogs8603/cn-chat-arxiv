<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#20174;&#27835;&#30103;&#20013;&#33719;&#30410;&#30340;&#20154;&#30340;&#38382;&#39064;&#65292;&#22312;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#21644;&#30830;&#23450;&#25130;&#26029;&#20540;&#26102;&#38754;&#20020;&#22810;&#37325;&#27979;&#35797;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#32479;&#19968;&#30340;&#32622;&#20449;&#24102;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20123;&#20010;&#20307;&#30340;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07973</link><description>&lt;p&gt;
&#23545;&#20110;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#20174;&#27835;&#30103;&#20013;&#33719;&#30410;&#30340;&#20154;&#30340;&#32479;&#35745;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Statistical Performance Guarantee for Selecting Those Predicted to Benefit Most from Treatment. (arXiv:2310.07973v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#20174;&#27835;&#30103;&#20013;&#33719;&#30410;&#30340;&#20154;&#30340;&#38382;&#39064;&#65292;&#22312;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#21644;&#30830;&#23450;&#25130;&#26029;&#20540;&#26102;&#38754;&#20020;&#22810;&#37325;&#27979;&#35797;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#32479;&#19968;&#30340;&#32622;&#20449;&#24102;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20123;&#20010;&#20307;&#30340;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#30340;&#23398;&#31185;&#39046;&#22495;&#20013;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#19968;&#32452;&#34987;&#31216;&#20026;&#20363;&#22806;&#21453;&#24212;&#32773;&#30340;&#20010;&#20307;&#65292;&#20182;&#20204;&#26368;&#26377;&#21487;&#33021;&#20174;&#27835;&#30103;&#20013;&#33719;&#30410;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#25110;&#20854;&#20195;&#29702;&#12290;&#28982;&#21518;&#30830;&#23450;&#25152;&#24471;&#27835;&#30103;&#20248;&#20808;&#39034;&#24207;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#20197;&#36873;&#25321;&#37027;&#20123;&#26368;&#26377;&#21487;&#33021;&#20174;&#27835;&#30103;&#20013;&#33719;&#30410;&#30340;&#20154;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#20272;&#35745;&#30340;&#27835;&#30103;&#20248;&#20808;&#39034;&#24207;&#20998;&#25968;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#21644;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#30456;&#21516;&#30340;&#25968;&#25454;&#26082;&#36873;&#25321;&#25130;&#26029;&#20540;&#21448;&#20272;&#35745;&#25152;&#36873;&#20010;&#20307;&#30340;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#20250;&#36935;&#21040;&#22810;&#37325;&#27979;&#35797;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#32622;&#20449;&#24102;&#26469;&#23454;&#39564;&#24615;&#22320;&#35780;&#20272;&#37027;&#20123;&#27835;&#30103;&#20248;&#20808;&#39034;&#24207;&#20998;&#25968;&#33267;&#23569;&#19982;&#20219;&#20309;&#32473;&#23450;&#37327;&#21270;&#20540;&#30456;&#31561;&#30340;&#20010;&#20307;&#30340;&#25490;&#24207;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;GATES&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Across a wide array of disciplines, many researchers use machine learning (ML) algorithms to identify a subgroup of individuals, called exceptional responders, who are likely to be helped by a treatment the most. A common approach consists of two steps. One first estimates the conditional average treatment effect or its proxy using an ML algorithm. They then determine the cutoff of the resulting treatment prioritization score to select those predicted to benefit most from the treatment. Unfortunately, these estimated treatment prioritization scores are often biased and noisy. Furthermore, utilizing the same data to both choose a cutoff value and estimate the average treatment effect among the selected individuals suffer from a multiple testing problem. To address these challenges, we develop a uniform confidence band for experimentally evaluating the sorted average treatment effect (GATES) among the individuals whose treatment prioritization score is at least as high as any given quant
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#21644;&#32447;&#24615;MDPs&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#24573;&#30053;&#26576;&#20123;&#29366;&#24577;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;MDP&#12290;</title><link>http://arxiv.org/abs/2310.07811</link><description>&lt;p&gt;
&#22312;&#32447;RL&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#20013;&#21644;&#32447;&#24615;MDPs&#19968;&#26679;&#23481;&#26131;&#65292;&#21482;&#35201;&#20320;&#23398;&#20250;&#24573;&#30053;&#12290; (arXiv:2310.07811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Online RL in Linearly $q^\pi$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore. (arXiv:2310.07811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#21644;&#32447;&#24615;MDPs&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#24573;&#30053;&#26576;&#20123;&#29366;&#24577;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;MDP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#31163;&#25955;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#65292;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;&#20551;&#35774;&#19979;&#65292;&#20551;&#35774;&#25152;&#26377;&#31574;&#30053;&#30340;&#21160;&#20316;&#20540;&#21487;&#20197;&#34920;&#31034;&#20026;&#29366;&#24577;-&#21160;&#20316;&#29305;&#24449;&#30340;&#32447;&#24615;&#20989;&#25968;&#12290;&#36825;&#20010;&#31867;&#21035;&#34987;&#35748;&#20026;&#27604;&#32447;&#24615;MDPs&#26356;&#19968;&#33324;&#21270;&#65292;&#20854;&#20013;&#36716;&#31227;&#20869;&#26680;&#21644;&#22870;&#21169;&#20989;&#25968;&#34987;&#20551;&#35774;&#20026;&#29305;&#24449;&#21521;&#37327;&#30340;&#32447;&#24615;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#36825;&#20004;&#20010;&#31867;&#21035;&#20043;&#38388;&#30340;&#24046;&#24322;&#26159;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#20013;&#23384;&#22312;&#19968;&#20123;&#29366;&#24577;&#65292;&#22312;&#36825;&#20123;&#29366;&#24577;&#20013;&#65292;&#23545;&#20110;&#20219;&#20309;&#31574;&#30053;&#65292;&#25152;&#26377;&#30340;&#21160;&#20316;&#20540;&#37117;&#36817;&#20284;&#30456;&#31561;&#65292;&#36890;&#36807;&#36339;&#36807;&#36825;&#20123;&#29366;&#24577;&#24182;&#25353;&#29031;&#20219;&#24847;&#22266;&#23450;&#31574;&#30053;&#22312;&#36825;&#20123;&#29366;&#24577;&#20013;&#36827;&#34892;&#36716;&#25442;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;MDP&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31181;&#26032;&#39062;&#65288;&#35745;&#31639;&#25928;&#29575;&#36739;&#20302;&#65289;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#20013;&#65292;&#35813;&#31639;&#27861;&#21516;&#26102;&#23398;&#20064;&#20102;&#24212;&#35813;&#36339;&#36807;&#30340;&#29366;&#24577;&#65292;&#24182;&#36816;&#34892;&#21478;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider online reinforcement learning (RL) in episodic Markov decision processes (MDPs) under the linear $q^\pi$-realizability assumption, where it is assumed that the action-values of all policies can be expressed as linear functions of state-action features. This class is known to be more general than linear MDPs, where the transition kernel and the reward function are assumed to be linear functions of the feature vectors. As our first contribution, we show that the difference between the two classes is the presence of states in linearly $q^\pi$-realizable MDPs where for any policy, all the actions have approximately equal values, and skipping over these states by following an arbitrarily fixed policy in those states transforms the problem to a linear MDP. Based on this observation, we derive a novel (computationally inefficient) learning algorithm for linearly $q^\pi$-realizable MDPs that simultaneously learns what states should be skipped over and runs another learning algorith
&lt;/p&gt;</description></item><item><title>DYGR&#26159;&#19968;&#31181;&#29992;&#20110;&#20851;&#31995;&#25512;&#26029;&#30340;&#21160;&#24577;&#22270;&#20808;&#39564;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39640;&#24230;&#38750;&#23616;&#37096;&#30340;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#20013;&#26500;&#36896;&#24615;&#22320;&#21033;&#29992;&#35823;&#24046;&#25918;&#22823;&#26469;&#29983;&#25104;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#30340;&#33391;&#22909;&#26799;&#24230;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#36866;&#29992;&#20110;&#20855;&#26377;&#20849;&#20139;&#22270;&#25299;&#25169;&#30340;&#8220;&#27973;&#23618;&#8221;&#19968;&#27493;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.06041</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#37051;&#20808;&#39564;&#29992;&#20110;&#20851;&#31995;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
A Dynamical Graph Prior for Relational Inference. (arXiv:2306.06041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06041
&lt;/p&gt;
&lt;p&gt;
DYGR&#26159;&#19968;&#31181;&#29992;&#20110;&#20851;&#31995;&#25512;&#26029;&#30340;&#21160;&#24577;&#22270;&#20808;&#39564;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39640;&#24230;&#38750;&#23616;&#37096;&#30340;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#20013;&#26500;&#36896;&#24615;&#22320;&#21033;&#29992;&#35823;&#24046;&#25918;&#22823;&#26469;&#29983;&#25104;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#30340;&#33391;&#22909;&#26799;&#24230;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#36866;&#29992;&#20110;&#20855;&#26377;&#20849;&#20139;&#22270;&#25299;&#25169;&#30340;&#8220;&#27973;&#23618;&#8221;&#19968;&#27493;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25512;&#26029;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#35782;&#21035;&#37096;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26159;&#22312;&#21487;&#23398;&#20064;&#30340;&#22270;&#19978;&#25311;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26469;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#12290;&#23427;&#20204;&#20351;&#29992;&#19968;&#27493;&#28040;&#24687;&#20256;&#36882; GNN--&#30452;&#35266;&#19978;&#26469;&#35828;&#26159;&#27491;&#30830;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#22810;&#27493;&#25110;&#35889; GNN &#30340;&#38750;&#23616;&#37096;&#24615;&#21487;&#33021;&#20250;&#28151;&#28102;&#30452;&#25509;&#21644;&#38388;&#25509;&#30456;&#20114;&#20316;&#29992;&#12290;&#20294;&#26159;&#8220;&#26377;&#25928;&#8221;&#30340;&#20132;&#20114;&#22270;&#21462;&#20915;&#20110;&#37319;&#26679;&#36895;&#29575;&#65292;&#24456;&#23569;&#23616;&#38480;&#20110;&#30452;&#25509;&#37051;&#23621;&#65292;&#23548;&#33268;&#19968;&#20010;&#27493;&#39588;&#27169;&#22411;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#21160;&#24577;&#22270;&#20808;&#39564;&#8221;(DYGR)&#26469;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#20808;&#39564;&#30340;&#21407;&#22240;&#26159;&#65292;&#19982;&#24050;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#22312;&#39640;&#24230;&#38750;&#23616;&#37096;&#30340;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#20013;&#26500;&#36896;&#24615;&#22320;&#21033;&#29992;&#35823;&#24046;&#25918;&#22823;&#26469;&#29983;&#25104;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#30340;&#33391;&#22909;&#26799;&#24230;&#12290;&#20026;&#20102;&#22788;&#29702;&#38750;&#21807;&#19968;&#24615;&#65292;DYGR &#21516;&#26102;&#36866;&#29992;&#20110;&#20855;&#26377;&#20849;&#20139;&#22270;&#25299;&#25169;&#30340;&#8220;&#27973;&#23618;&#8221;&#19968;&#27493;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126; DYGR &#33021;&#22815;&#37325;&#26032;&#26500;&#24314;&#20132;&#20114;&#32467;&#26500;&#65292;&#21516;&#26102;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational inference aims to identify interactions between parts of a dynamical system from the observed dynamics. Current state-of-the-art methods fit a graph neural network (GNN) on a learnable graph to the dynamics. They use one-step message-passing GNNs -- intuitively the right choice since non-locality of multi-step or spectral GNNs may confuse direct and indirect interactions. But the \textit{effective} interaction graph depends on the sampling rate and it is rarely localized to direct neighbors, leading to local minima for the one-step model. In this work, we propose a \textit{dynamical graph prior} (DYGR) for relational inference. The reason we call it a prior is that, contrary to established practice, it constructively uses error amplification in high-degree non-local polynomial filters to generate good gradients for graph learning. To deal with non-uniqueness, DYGR simultaneously fits a ``shallow'' one-step model with shared graph topology. Experiments show that DYGR reconstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#19988;&#20581;&#22766;&#30340;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20844;&#24179;&#32422;&#26463;&#19979;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#65292;&#24182;&#21487;&#29992;&#20110;&#26435;&#34913;&#20844;&#24179;&#21644;&#26368;&#22823;&#31119;&#21033;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.03625</link><description>&lt;p&gt;
&#20844;&#24179;&#19988;&#20581;&#22766;&#30340;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#25919;&#31574;&#23398;&#20064;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy Learning. (arXiv:2306.03625v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#19988;&#20581;&#22766;&#30340;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20844;&#24179;&#32422;&#26463;&#19979;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#65292;&#24182;&#21487;&#29992;&#20110;&#26435;&#34913;&#20844;&#24179;&#21644;&#26368;&#22823;&#31119;&#21033;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20844;&#24179;&#32422;&#26463;&#26465;&#20214;&#19979;&#38750;&#21442;&#25968;&#20272;&#35745;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#12290;&#22312;&#26631;&#20934;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#24471;&#21040;&#30340;&#20272;&#35745;&#22120;&#20855;&#26377;&#21452;&#37325;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#27492;&#26694;&#26550;&#26469;&#34920;&#24449;&#20844;&#24179;&#21644;&#26368;&#20339;&#25919;&#31574;&#21487;&#23454;&#29616;&#30340;&#26368;&#22823;&#31119;&#21033;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple and general framework for nonparametric estimation of heterogeneous treatment effects under fairness constraints. Under standard regularity conditions, we show that the resulting estimators possess the double robustness property. We use this framework to characterize the trade-off between fairness and the maximum welfare achievable by the optimal policy. We evaluate the methods in a simulation study and illustrate them in a real-world case study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#29420;&#31435;&#33218;&#20998;&#24067;&#20551;&#35774;&#19979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#39640;&#24179;&#22343;&#22870;&#21169;&#30340;&#33218;&#65292;&#36866;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#31561;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.02630</link><description>&lt;p&gt;
&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Covariance Adaptive Best Arm Identification. (arXiv:2306.02630v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#29420;&#31435;&#33218;&#20998;&#24067;&#20551;&#35774;&#19979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#39640;&#24179;&#22343;&#22870;&#21169;&#30340;&#33218;&#65292;&#36866;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#31561;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#27169;&#22411;&#19979;&#65292;&#22522;&#20110;&#22266;&#23450;&#32622;&#20449;&#24230;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#22312;&#32473;&#23450;&#32622;&#20449;&#24230; $\delta$ &#30340;&#24773;&#20917;&#19979;&#65292;&#26088;&#22312;&#20197;&#33267;&#23569; 1 - $\delta$ &#30340;&#27010;&#29575;&#35782;&#21035;&#20986;&#20855;&#26377;&#26368;&#39640;&#24179;&#22343;&#22870;&#21169;&#30340;&#33218;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#33218;&#30340;&#25289;&#21160;&#27425;&#25968;&#12290;&#34429;&#28982;&#25991;&#29486;&#25552;&#20379;&#20102;&#38024;&#23545;&#29420;&#31435;&#33218;&#20998;&#24067;&#20551;&#35774;&#19979;&#35813;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#21152;&#28789;&#27963;&#30340;&#24773;&#24418;&#65292;&#20854;&#20013;&#33218;&#21487;&#20197;&#26159;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#25910;&#30410;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#37319;&#26679;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#23398;&#20064;&#32773;&#20272;&#35745;&#33218;&#20043;&#38388;&#20998;&#24067;&#30340;&#21327;&#26041;&#24046;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#26368;&#20339;&#33218;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#24212;&#33218;&#21327;&#26041;&#24046;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20445;&#35777;&#35777;&#26126;&#20854;&#20855;&#26377;&#23454;&#36136;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of best arm identification in the multi-armed bandit model, under fixed confidence. Given a confidence input $\delta$, the goal is to identify the arm with the highest mean reward with a probability of at least 1 -- $\delta$, while minimizing the number of arm pulls. While the literature provides solutions to this problem under the assumption of independent arms distributions, we propose a more flexible scenario where arms can be dependent and rewards can be sampled simultaneously. This framework allows the learner to estimate the covariance among the arms distributions, enabling a more efficient identification of the best arm. The relaxed setting we propose is relevant in various applications, such as clinical trials, where similarities between patients or drugs suggest underlying correlations in the outcomes. We introduce new algorithms that adapt to the unknown covariance of the arms and demonstrate through theoretical guarantees that substantial improvement 
&lt;/p&gt;</description></item><item><title>LAVA&#26159;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#26469;&#23454;&#29616;&#12290;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2305.00054</link><description>&lt;p&gt;
LAVA: &#26080;&#38656;&#39044;&#23450;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LAVA: Data Valuation without Pre-Specified Learning Algorithms. (arXiv:2305.00054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00054
&lt;/p&gt;
&lt;p&gt;
LAVA&#26159;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#26469;&#23454;&#29616;&#12290;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#38382;&#39064;&#26159;&#22914;&#20309;&#20844;&#24179;&#22320;&#20998;&#37197;&#23398;&#20064;&#31639;&#27861;&#30340;&#39564;&#35777;&#24615;&#33021;&#65292;&#33268;&#20351;&#35745;&#31639;&#24471;&#21040;&#30340;&#25968;&#25454;&#20215;&#20540;&#20381;&#36182;&#20110;&#24213;&#23618;&#23398;&#20064;&#31639;&#27861;&#30340;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;LAVA&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#65292;&#20351;&#20854;&#26080;&#35270;&#19979;&#28216;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#23427;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, data valuation is posed as a problem of equitably splitting the validation performance of a learning algorithm among the training data. As a result, the calculated data values depend on many design choices of the underlying learning algorithm. However, this dependence is undesirable for many use cases of data valuation, such as setting priorities over different data sources in a data acquisition process and informing pricing mechanisms in a data marketplace. In these scenarios, data needs to be valued before the actual analysis and the choice of the learning algorithm is still undetermined then. Another side-effect of the dependence is that to assess the value of individual points, one needs to re-run the learning algorithm with and without a point, which incurs a large computation burden.  This work leapfrogs over the current limits of data valuation methods by introducing a new framework that can value training data in a way that is oblivious to the downstream learning
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13646</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#29992;&#20110;&#24102;&#21327;&#21464;&#20449;&#24687;&#30340;&#38543;&#26426;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Data-driven Piecewise Affine Decision Rules for Stochastic Programming with Covariate Information. (arXiv:2304.13646v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24102;&#21327;&#21464;&#20449;&#24687;&#30340;&#38543;&#26426;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;(PADR)&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#26041;&#27861;&#65292;&#26088;&#22312;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;PADR&#30340;ERM&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#19968;&#33268;&#24615;&#32467;&#26524;&#65292;&#21487;&#29992;&#20110;&#26080;&#32422;&#26463;&#38382;&#39064;&#65292;&#20197;&#21450;&#32422;&#26463;&#38382;&#39064;&#30340;&#28176;&#36817;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;ERM&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#38543;&#26426;&#20027;&#23548;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#27839;&#65288;&#22797;&#21512;&#24378;&#65289;&#26041;&#21521;&#31283;&#23450;&#24615;&#30340;&#28176;&#36817;&#25910;&#25947;&#20197;&#21450;&#22797;&#26434;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;PADR-based ERM&#26041;&#27861;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#19968;&#33268;&#24615;&#20445;&#35777;&#21644;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#12290;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#65292;PADR-based ERM&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Focusing on stochastic programming (SP) with covariate information, this paper proposes an empirical risk minimization (ERM) method embedded within a nonconvex piecewise affine decision rule (PADR), which aims to learn the direct mapping from features to optimal decisions. We establish the nonasymptotic consistency result of our PADR-based ERM model for unconstrained problems and asymptotic consistency result for constrained ones. To solve the nonconvex and nondifferentiable ERM problem, we develop an enhanced stochastic majorization-minimization algorithm and establish the asymptotic convergence to (composite strong) directional stationarity along with complexity analysis. We show that the proposed PADR-based ERM method applies to a broad class of nonconvex SP problems with theoretical consistency guarantees and computational tractability. Our numerical study demonstrates the superior performance of PADR-based ERM methods compared to state-of-the-art approaches under various settings,
&lt;/p&gt;</description></item></channel></rss>