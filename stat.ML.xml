<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#26469;&#25913;&#21892;&#20809;&#35889;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.18306</link><description>&lt;p&gt;
&#30417;&#30563;&#21644;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Supervised and Penalized Baseline Correction. (arXiv:2310.18306v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#26469;&#25913;&#21892;&#20809;&#35889;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#35889;&#27979;&#37327;&#21487;&#20197;&#26174;&#31034;&#30001;&#21560;&#25910;&#21644;&#25955;&#23556;&#25104;&#20998;&#28151;&#21512;&#24341;&#36215;&#30340;&#25197;&#26354;&#20809;&#35889;&#24418;&#29366;&#12290;&#36825;&#20123;&#25197;&#26354;&#65288;&#25110;&#22522;&#32447;&#65289;&#36890;&#24120;&#34920;&#29616;&#20026;&#38750;&#24658;&#23450;&#20559;&#31227;&#25110;&#20302;&#39057;&#25391;&#33633;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#22522;&#32447;&#21487;&#33021;&#23545;&#20998;&#26512;&#21644;&#23450;&#37327;&#32467;&#26524;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22522;&#32447;&#26657;&#27491;&#26159;&#19968;&#20010;&#28085;&#30422;&#20102;&#39044;&#22788;&#29702;&#26041;&#27861;&#30340;&#24635;&#31216;&#65292;&#36890;&#36807;&#33719;&#21462;&#22522;&#32447;&#20809;&#35889;&#65288;&#19981;&#38656;&#35201;&#30340;&#25197;&#26354;&#65289;&#24182;&#36890;&#36807;&#24046;&#24322;&#21270;&#21435;&#38500;&#25197;&#26354;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#21363;&#20351;&#21487;&#29992;&#20998;&#26512;&#29289;&#27987;&#24230;&#25110;&#32773;&#23427;&#20204;&#23545;&#35266;&#23519;&#21040;&#30340;&#20809;&#35889;&#21464;&#24322;&#26377;&#37325;&#35201;&#36129;&#29486;&#65292;&#20063;&#27809;&#26377;&#21033;&#29992;&#23427;&#20204;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#65289;&#24182;&#23545;&#20854;&#36827;&#34892;&#20462;&#25913;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#23558;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#24615;&#33021;&#65292;&#21253;&#25324;&#32463;&#20856;&#21463;&#32602;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectroscopic measurements can show distorted spectra shapes arising from a mixture of absorbing and scattering contributions. These distortions (or baselines) often manifest themselves as non-constant offsets or low-frequency oscillations. As a result, these baselines can adversely affect analytical and quantitative results. Baseline correction is an umbrella term where one applies pre-processing methods to obtain baseline spectra (the unwanted distortions) and then remove the distortions by differencing. However, current state-of-the art baseline correction methods do not utilize analyte concentrations even if they are available, or even if they contribute significantly to the observed spectral variability. We examine a class of state-of-the-art methods (penalized baseline correction) and modify them such that they can accommodate a priori analyte concentration such that prediction can be enhanced. Performance will be access on two near infra-red data sets across both classical penal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#65292;&#24182;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#21644;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#35745;&#31639;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.14421</link><description>&lt;p&gt;
&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#65292;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#65292;&#24182;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#21644;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#35745;&#31639;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#38024;&#23545;&#65288;&#23616;&#37096;&#65289;&#21807;&#19968;&#21487;&#36870;&#20998;&#31867;&#22120;&#12289;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#21644;&#29109;AI&#65288;EAI&#65289;&#20855;&#26377;&#26368;&#23567;&#23545;&#25239;&#36335;&#24452;&#65288;MAP&#65289;&#21644;&#26368;&#23567;&#23545;&#25239;&#36317;&#31163;&#65288;MAD&#65289;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#26126;&#30830;&#30340;&#20998;&#26512;&#35745;&#31639;&#30340;&#31616;&#21333;&#21487;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#12290;&#22312;&#24120;&#35265;&#30340;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#12289;&#25552;&#21319;&#38543;&#26426;&#26862;&#26519;&#12289;GLM&#21644;EAI&#31561;&#21508;&#31867;AI&#24037;&#20855;&#36827;&#34892;MAP&#21644;MAD&#30340;&#23454;&#38469;&#35745;&#31639;&#12289;&#27604;&#36739;&#21644;&#35299;&#37322;&#65292;&#21253;&#25324;&#21452;&#21367;&#29366;&#34746;&#26059;&#32447;&#21450;&#20854;&#25193;&#23637;&#20197;&#21450;&#20004;&#20010;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38382;&#39064;&#65288;&#29992;&#20110;&#20581;&#24247;&#20445;&#38505;&#29702;&#36180;&#39044;&#27979;&#21644;&#24515;&#33039;&#30149;&#21457;&#20316;&#33268;&#27515;&#29575;&#20998;&#31867;&#65289;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;&#23637;&#31034;&#20102;MAP&#22914;&#20309;&#22312;&#39044;&#23450;&#20041;&#30340;&#21487;&#35775;&#38382;&#25511;&#21046;&#21464;&#37327;&#23376;&#38598;&#20013;&#25552;&#20379;&#21807;&#19968;&#30340;&#26368;&#23567;&#24739;&#32773;&#29305;&#23450;&#39118;&#38505;&#32531;&#35299;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simply-verifiable mathematical conditions for existence, uniqueness and explicit analytical computation of minimal adversarial paths (MAP) and minimal adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for generalized linear models (GLM), and for entropic AI (EAI) are formulated and proven. Practical computation of MAP and MAD, their comparison and interpretations for various classes of AI tools (for neuronal networks, boosted random forests, GLM and EAI) are demonstrated on the common synthetic benchmarks: on a double Swiss roll spiral and its extensions, as well as on the two biomedical data problems (for the health insurance claim predictions, and for the heart attack lethality classification). On biomedical applications it is demonstrated how MAP provides unique minimal patient-specific risk-mitigating interventions in the predefined subsets of accessible control variables.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#21306;&#22495;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21271;&#27431;&#22320;&#21306;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.17370</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#21306;&#22495;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based Neural Weather Prediction for Limited Area Modeling. (arXiv:2309.17370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17370
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#21306;&#22495;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21271;&#27431;&#22320;&#21306;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#31934;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22825;&#27668;&#39044;&#25253;&#39046;&#22495;&#30340;&#24212;&#29992;&#20026;&#27169;&#25311;&#22823;&#27668;&#30340;&#21487;&#33021;&#24615;&#24102;&#26469;&#20102;&#26032;&#30340;&#21464;&#38761;&#12290;&#22312;&#27668;&#20505;&#21464;&#21270;&#26102;&#20195;&#65292;&#33719;&#21462;&#20687;&#36825;&#26679;&#30340;&#39640;&#20998;&#36776;&#29575;&#39044;&#25253;&#27169;&#22411;&#30340;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#22825;&#27668;&#39044;&#25253;&#26041;&#27861;&#37117;&#26159;&#38024;&#23545;&#20840;&#29699;&#39044;&#27979;&#65292;&#20294;&#22914;&#20309;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#26377;&#38480;&#21306;&#22495;&#24314;&#27169;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#22522;&#20110;&#22270;&#20687;&#30340;&#31070;&#32463;&#32593;&#32476;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#24212;&#29992;&#20110;&#26377;&#38480;&#21306;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#20998;&#23618;&#27169;&#22411;&#25193;&#23637;&#12290;&#36890;&#36807;&#20351;&#29992;&#21271;&#27431;&#22320;&#21306;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of accurate machine learning methods for weather forecasting is creating radical new possibilities for modeling the atmosphere. In the time of climate change, having access to high-resolution forecasts from models like these is also becoming increasingly vital. While most existing Neural Weather Prediction (NeurWP) methods focus on global forecasting, an important question is how these techniques can be applied to limited area modeling. In this work we adapt the graph-based NeurWP approach to the limited area setting and propose a multi-scale hierarchical model extension. Our approach is validated by experiments with a local model for the Nordic region.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#27748;&#26222;&#26862;&#25277;&#26679;&#25506;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#33218;&#32769;&#34382;&#26426;&#23454;&#20540;&#32452;&#21512;&#32431;&#25506;&#32034;&#38382;&#39064;&#20013;&#21160;&#20316;&#38598;&#21512;&#22823;&#23567;&#20026;&#25351;&#25968;&#32423;&#21035;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.10238</link><description>&lt;p&gt;
Thompson Sampling&#29992;&#20110;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#23454;&#20540;&#32452;&#21512;&#32431;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Thompson Sampling for Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit. (arXiv:2308.10238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10238
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#27748;&#26222;&#26862;&#25277;&#26679;&#25506;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#33218;&#32769;&#34382;&#26426;&#23454;&#20540;&#32452;&#21512;&#32431;&#25506;&#32034;&#38382;&#39064;&#20013;&#21160;&#20316;&#38598;&#21512;&#22823;&#23567;&#20026;&#25351;&#25968;&#32423;&#21035;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#23454;&#20540;&#32452;&#21512;&#32431;&#25506;&#32034;&#65288;R-CPE-MAB&#65289;&#38382;&#39064;&#12290;&#22312;R-CPE-MAB&#20013;&#65292;&#29609;&#23478;&#20174;&#32473;&#23450;&#30340;d&#20010;&#38543;&#26426;&#33218;&#20013;&#36873;&#25321;&#19968;&#20010;&#65292;&#27599;&#20010;&#33218;s&#30340;&#22870;&#21169;&#36981;&#24490;&#26410;&#30693;&#20998;&#24067;&#65292;&#20854;&#24179;&#22343;&#20540;&#20026;&#956;s&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#65292;&#29609;&#23478;&#25289;&#21160;&#19968;&#20010;&#33218;&#24182;&#35266;&#23519;&#20854;&#22870;&#21169;&#12290;&#29609;&#23478;&#30340;&#30446;&#26631;&#26159;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#33218;&#25289;&#21160;&#27425;&#25968;&#26469;&#30830;&#23450;&#26368;&#20248;&#21160;&#20316;&#960;* = argmax&#960;&#8712;A &#956;T&#960;&#65292;&#20854;&#20013;A&#26159;&#26377;&#38480;&#22823;&#23567;&#30340;&#23454;&#20540;&#21160;&#20316;&#38598;&#21512;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20551;&#35774;&#21160;&#20316;&#38598;&#21512;A&#30340;&#22823;&#23567;&#22312;d&#30340;&#22810;&#39033;&#24335;&#32423;&#21035;&#19978;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#27748;&#26222;&#26862;&#25277;&#26679;&#25506;&#32034;&#65288;GenTS-Explore&#65289;&#31639;&#27861;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#35299;&#20915;&#21160;&#20316;&#38598;&#21512;&#22823;&#23567;&#22312;d&#30340;&#25351;&#25968;&#32423;&#21035;&#19978;&#30340;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#30456;&#20851;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the real-valued combinatorial pure exploration of the multi-armed bandit (R-CPE-MAB) problem. In R-CPE-MAB, a player is given $d$ stochastic arms, and the reward of each arm $s\in\{1, \ldots, d\}$ follows an unknown distribution with mean $\mu_s$. In each time step, a player pulls a single arm and observes its reward. The player's goal is to identify the optimal \emph{action} $\boldsymbol{\pi}^{*} = \argmax_{\boldsymbol{\pi} \in \mathcal{A}} \boldsymbol{\mu}^{\top}\boldsymbol{\pi}$ from a finite-sized real-valued \emph{action set} $\mathcal{A}\subset \mathbb{R}^{d}$ with as few arm pulls as possible. Previous methods in the R-CPE-MAB assume that the size of the action set $\mathcal{A}$ is polynomial in $d$. We introduce an algorithm named the Generalized Thompson Sampling Explore (GenTS-Explore) algorithm, which is the first algorithm that can work even when the size of the action set is exponentially large in $d$. We also introduce a novel problem-dependent sample complexity 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#21521;&#37327;&#20540;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#30340;&#23436;&#25972;&#35823;&#24046;&#20998;&#26512;&#65292;&#21253;&#25324;&#22312;&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#19979;&#21521;&#37327;&#20540;RF&#20272;&#35745;&#22120;&#30340;&#24378;&#19968;&#33268;&#24615;&#21644;&#22312;&#33391;&#22909;&#35268;&#23450;&#30340;&#24773;&#20917;&#19979;&#26497;&#23567;&#21270;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.17170</link><description>&lt;p&gt;
&#21521;&#37327;&#20540;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#30340;&#35823;&#24046;&#30028;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Error Bounds for Learning with Vector-Valued Random Features. (arXiv:2305.17170v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#21521;&#37327;&#20540;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#30340;&#23436;&#25972;&#35823;&#24046;&#20998;&#26512;&#65292;&#21253;&#25324;&#22312;&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#19979;&#21521;&#37327;&#20540;RF&#20272;&#35745;&#22120;&#30340;&#24378;&#19968;&#33268;&#24615;&#21644;&#22312;&#33391;&#22909;&#35268;&#23450;&#30340;&#24773;&#20917;&#19979;&#26497;&#23567;&#21270;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#21521;&#37327;&#20540;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#30340;&#23436;&#25972;&#35823;&#24046;&#20998;&#26512;&#12290;&#35813;&#29702;&#35770;&#26159;&#38024;&#23545;&#23436;&#20840;&#36890;&#29992;&#30340;&#26080;&#38480;&#32500;&#24230;&#36755;&#20837;-&#36755;&#20986;&#35774;&#23450;&#20013;&#30340;RF Ridge&#22238;&#24402;&#32780;&#24320;&#21457;&#30340;&#65292;&#20294;&#20173;&#36866;&#29992;&#20110;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#26377;&#38480;&#32500;&#24230;&#20998;&#26512;&#12290;&#19982;&#25991;&#29486;&#20013;&#20854;&#20182;&#31867;&#20284;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#24213;&#23618;&#39118;&#38505;&#20989;&#25968;&#30340;&#30452;&#25509;&#20998;&#26512;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#22522;&#20110;&#38543;&#26426;&#30697;&#38453;&#30340;&#26174;&#24335;RF Ridge&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#20844;&#24335;&#30340;&#20351;&#29992;&#12290;&#36825;&#28040;&#38500;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#20013;&#30340;&#27987;&#24230;&#32467;&#26524;&#25110;&#20854;&#23545;&#38543;&#26426;&#31639;&#23376;&#30340;&#25512;&#24191;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#24314;&#31435;&#30340;&#20027;&#35201;&#32467;&#26524;&#21253;&#25324;&#22312;&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#19979;&#21521;&#37327;&#20540;RF&#20272;&#35745;&#22120;&#30340;&#24378;&#19968;&#33268;&#24615;&#21644;&#22312;&#33391;&#22909;&#35268;&#23450;&#30340;&#24773;&#20917;&#19979;&#26497;&#23567;&#21270;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#23454;&#29616;&#36825;&#20123;&#25910;&#25947;&#36895;&#29575;&#25152;&#38656;&#30340;&#21442;&#25968;&#22797;&#26434;&#24230;(&#38543;&#26426;&#29305;&#24449;&#25968;&#37327;)&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;(&#26631;&#35760;&#25968;&#25454;&#25968;&#37327;)&#19982;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive error analysis of learning with vector-valued random features (RF). The theory is developed for RF ridge regression in a fully general infinite-dimensional input-output setting, but nonetheless applies to and improves existing finite-dimensional analyses. In contrast to comparable work in the literature, the approach proposed here relies on a direct analysis of the underlying risk functional and completely avoids the explicit RF ridge regression solution formula in terms of random matrices. This removes the need for concentration results in random matrix theory or their generalizations to random operators. The main results established in this paper include strong consistency of vector-valued RF estimators under model misspecification and minimax optimal convergence rates in the well-specified setting. The parameter complexity (number of random features) and sample complexity (number of labeled data) required to achieve such rates are comparable with 
&lt;/p&gt;</description></item><item><title>&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11562</link><description>&lt;p&gt;
&#39044;&#27979;&#26159;&#21542;&#38543;&#24847;&#65311;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#35780;&#20272;&#33258;&#27965;&#24615;
&lt;/p&gt;
&lt;p&gt;
Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11562
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290; &#23454;&#35777;&#34920;&#26126;&#65292;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#30340;&#26041;&#24046;&#24046;&#24322;&#38750;&#24120;&#22823;&#65292;&#20197;&#33267;&#20110;&#20915;&#31574;&#23454;&#38469;&#19978;&#26159;&#38543;&#24847;&#30340;&#12290; &#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#20570;&#20986;&#20102;&#22235;&#20010;&#24635;&#20307;&#36129;&#29486;&#65306;&#25105;&#20204;1&#65289;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#33258;&#27965;&#24615;&#65292;&#22312;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#26102;&#20351;&#29992;&#65307; 2&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#24403;&#39044;&#27979;&#26080;&#27861;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#21487;&#20197;&#25918;&#24323;&#20998;&#31867;&#65307; 3&#65289;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26377;&#20851;&#20844;&#24179;&#20998;&#31867;&#20013;&#26041;&#24046;&#65288;&#30456;&#23545;&#20110;&#33258;&#27965;&#24615;&#21644;&#38543;&#24847;&#24615;&#65289;&#20316;&#29992;&#30340;&#26368;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65307; 4&#65289;&#25512;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#21253;&#65292;&#20351;&#32654;&#22269;&#20303;&#25151;&#25269;&#25276;&#36151;&#27454;&#25259;&#38706;&#27861;&#26696;&#65288;HMDA&#65289;&#25968;&#25454;&#38598;&#26131;&#20110;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#12290; &#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#25581;&#31034;&#20102;&#20851;&#20110;&#21487;&#37325;&#22797;&#24615;&#30340;&#20196;&#20154;&#38663;&#24778;&#30340;&#35265;&#35299;&#12290;&#24403;&#32771;&#34385;&#21040;&#26041;&#24046;&#21644;&#38543;&#24847;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#26102;&#65292;&#22823;&#22810;&#25968;&#20844;&#24179;&#20998;&#31867;&#22522;&#20934;&#25509;&#36817;&#20844;&#24179;&#12290; &#20294;&#26159;&#65292;&#19968;&#23567;&#37096;&#20998;&#23454;&#20363;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#38543;&#24847;&#24615;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Sobolev&#21644;Besov&#31354;&#38388;&#20013;&#65292;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20197;&#24590;&#26679;&#30340;&#21442;&#25968;&#25928;&#29575;&#36924;&#36817;&#20989;&#25968;&#65292;&#21253;&#25324;$L_p(\Omega)$&#33539;&#25968;&#19979;&#30340;&#35823;&#24046;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;$1\leq p,q \leq \infty$&#21644;$s&gt;0$&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#26469;&#33719;&#24471;&#23574;&#38160;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2211.14400</link><description>&lt;p&gt;
&#22312;Sobolev&#21644;Besov&#31354;&#38388;&#19978;&#65292;&#20851;&#20110;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20339;&#36924;&#36817;&#36895;&#29575;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces. (arXiv:2211.14400v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14400
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Sobolev&#21644;Besov&#31354;&#38388;&#20013;&#65292;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20197;&#24590;&#26679;&#30340;&#21442;&#25968;&#25928;&#29575;&#36924;&#36817;&#20989;&#25968;&#65292;&#21253;&#25324;$L_p(\Omega)$&#33539;&#25968;&#19979;&#30340;&#35823;&#24046;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;$1\leq p,q \leq \infty$&#21644;$s&gt;0$&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#26469;&#33719;&#24471;&#23574;&#38160;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;Sobolev&#31354;&#38388;$W^s(L_q(\Omega))$&#21644;Besov&#31354;&#38388;$B^s_r(L_q(\Omega))$&#20013;&#20197;$L_p(\Omega)$&#33539;&#25968;&#24230;&#37327;&#35823;&#24046;&#30340;&#21442;&#25968;&#25928;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20110;&#22312;&#31185;&#23398;&#35745;&#31639;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#37325;&#35201;&#65292;&#22312;&#36807;&#21435;&#21482;&#26377;&#24403;$p=q=\infty$&#26102;&#25165;&#23436;&#20840;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#25152;&#26377;$1\leq p,q\leq \infty$&#21644;$s&gt;0$&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#28176;&#36817;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#12290;&#20851;&#38190;&#30340;&#25216;&#26415;&#24037;&#20855;&#26159;&#19968;&#31181;&#26032;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#65292;&#23427;&#25552;&#20379;&#20102;&#31232;&#30095;&#21521;&#37327;&#30340;&#26368;&#20339;&#32534;&#30721;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;$p&gt;q$&#30340;&#38750;&#32447;&#24615;&#21306;&#22495;&#33719;&#24471;&#23574;&#38160;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#30340;$L_p$&#36924;&#36817;&#19979;&#30028;&#25512;&#23548;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Let $\Omega = [0,1]^d$ be the unit cube in $\mathbb{R}^d$. We study the problem of how efficiently, in terms of the number of parameters, deep neural networks with the ReLU activation function can approximate functions in the Sobolev spaces $W^s(L_q(\Omega))$ and Besov spaces $B^s_r(L_q(\Omega))$, with error measured in the $L_p(\Omega)$ norm. This problem is important when studying the application of neural networks in a variety of fields, including scientific computing and signal processing, and has previously been completely solved only when $p=q=\infty$. Our contribution is to provide a complete solution for all $1\leq p,q\leq \infty$ and $s &gt; 0$, including asymptotically matching upper and lower bounds. The key technical tool is a novel bit-extraction technique which gives an optimal encoding of sparse vectors. This enables us to obtain sharp upper bounds in the non-linear regime where $p &gt; q$. We also provide a novel method for deriving $L_p$-approximation lower bounds based upon
&lt;/p&gt;</description></item></channel></rss>