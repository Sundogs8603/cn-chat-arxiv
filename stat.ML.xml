<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#22312;&#31232;&#30095;&#31283;&#20581;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#19968;&#32500;&#23376;&#31354;&#38388;&#65292;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#26494;&#24347;&#26041;&#27861;&#21644;&#26032;&#39062;&#30340;&#25311;&#21512;&#31243;&#24207;&#65292;&#23454;&#29616;&#20102;&#20840;&#23616;&#26368;&#20248;&#30340;&#31283;&#20581;&#31232;&#30095;&#23376;&#31354;&#38388;&#65292;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#25928;&#29575;&#19988;&#21487;&#25193;&#23637;&#24615;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.16712</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31283;&#20581;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Scalable Robust Sparse Principal Component Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#22312;&#31232;&#30095;&#31283;&#20581;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#19968;&#32500;&#23376;&#31354;&#38388;&#65292;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#26494;&#24347;&#26041;&#27861;&#21644;&#26032;&#39062;&#30340;&#25311;&#21512;&#31243;&#24207;&#65292;&#23454;&#29616;&#20102;&#20840;&#23616;&#26368;&#20248;&#30340;&#31283;&#20581;&#31232;&#30095;&#23376;&#31354;&#38388;&#65292;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#25928;&#29575;&#19988;&#21487;&#25193;&#23637;&#24615;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#26469;&#20272;&#35745;&#31232;&#30095;&#31283;&#20581;&#30340;&#19968;&#32500;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#34920;&#31034;&#35823;&#24046;&#21644;l1&#33539;&#25968;&#20934;&#21017;&#19979;&#30340;&#24809;&#32602;&#12290;&#37492;&#20110;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26494;&#24347;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31616;&#21333;&#27604;&#20363;&#21644;&#25490;&#24207;&#25216;&#26415;&#30340;&#26032;&#22411;&#25311;&#21512;&#31243;&#24207;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#23637;&#31034;&#20102;$O(n^2 m \log n)$&#30340;&#26368;&#22351;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#31232;&#30095;&#31283;&#20581;&#23376;&#31354;&#38388;&#30340;&#20840;&#23616;&#26368;&#20248;&#65292;&#20174;&#32780;&#23637;&#31034;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#25928;&#29575;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#25214;&#21040;&#20855;&#26377;&#26368;&#20302;&#19981;&#19968;&#33268;&#24615;&#30340;&#23376;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#22312;&#31232;&#30095;&#24615;&#21644;&#25311;&#21512;&#20043;&#38388;&#26356;&#24179;&#28369;&#30340;&#26435;&#34913;&#12290;&#20854;&#26550;&#26500;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#23545;&#20110;2000x2000&#30340;&#30697;&#38453;&#65292;&#35745;&#31639;&#36895;&#24230;&#30456;&#36739;CPU&#29256;&#26412;&#25552;&#21319;&#20102;16&#20493;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16712v1 Announce Type: new  Abstract: In this work, we propose an optimization framework for estimating a sparse robust one-dimensional subspace. Our objective is to minimize both the representation error and the penalty, in terms of the l1-norm criterion. Given that the problem is NP-hard, we introduce a linear relaxation-based approach. Additionally, we present a novel fitting procedure, utilizing simple ratios and sorting techniques. The proposed algorithm demonstrates a worst-case time complexity of $O(n^2 m \log n)$ and, in certain instances, achieves global optimality for the sparse robust subspace, thereby exhibiting polynomial time efficiency. Compared to extant methodologies, the proposed algorithm finds the subspace with the lowest discordance, offering a smoother trade-off between sparsity and fit. Its architecture affords scalability, evidenced by a 16-fold improvement in computational speeds for matrices of 2000x2000 over CPU version. Furthermore, this method is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#24102;&#26377;&#25104;&#26412;&#20998;&#24067;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;CABAI&#26041;&#27861;&#20197;&#23454;&#29616;&#26368;&#23567;&#26399;&#26395;&#25104;&#26412;&#19979;&#35782;&#21035;&#20986;&#26368;&#22823;&#22870;&#21169;&#33218;&#65292;&#24182;&#35774;&#35745;&#20102;$\mathsf{CTAS}$&#21644;CO&#20004;&#31181;&#31639;&#27861;&#26469;&#36924;&#36817;&#29702;&#35770;&#19979;&#38480;&#24182;&#20248;&#21270;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.16710</link><description>&lt;p&gt;
&#25104;&#26412;&#24847;&#35782;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cost Aware Best Arm Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#24102;&#26377;&#25104;&#26412;&#20998;&#24067;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;CABAI&#26041;&#27861;&#20197;&#23454;&#29616;&#26368;&#23567;&#26399;&#26395;&#25104;&#26412;&#19979;&#35782;&#21035;&#20986;&#26368;&#22823;&#22870;&#21169;&#33218;&#65292;&#24182;&#35774;&#35745;&#20102;$\mathsf{CTAS}$&#21644;CO&#20004;&#31181;&#31639;&#27861;&#26469;&#36924;&#36817;&#29702;&#35770;&#19979;&#38480;&#24182;&#20248;&#21270;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#24102;&#26377;&#21452;&#37325;&#23545;&#35937;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#22870;&#21169;&#22806;&#65292;&#27599;&#20010;&#33218;&#36824;&#19982;&#25104;&#26412;&#20998;&#24067;&#30456;&#20851;&#32852;&#65292;&#30446;&#26631;&#26159;&#20351;&#29992;&#26368;&#23567;&#26399;&#26395;&#25104;&#26412;&#35782;&#21035;&#20986;&#26368;&#22823;&#22870;&#21169;&#33218;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#25104;&#26412;&#24847;&#35782;&#26368;&#20339;&#33218;&#35782;&#21035;&#8221;&#65288;CABAI&#65289;&#65292;&#23427;&#25429;&#25417;&#20102;&#20135;&#21697;&#24320;&#21457;&#27969;&#31243;&#20013;&#27979;&#35797;&#21644;&#23454;&#26045;&#38454;&#27573;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#24182;&#27169;&#25311;&#20102;&#38454;&#27573;&#20043;&#38388;&#30340;&#30446;&#26631;&#36716;&#21464;&#65292;&#21363;&#27979;&#35797;&#30340;&#25104;&#26412;&#21644;&#23454;&#26045;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;CABAI&#25512;&#23548;&#20102;&#19968;&#20010;&#29702;&#35770;&#19979;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\mathsf{CTAS}$&#30340;&#31639;&#27861;&#26469;&#28176;&#36817;&#21305;&#37197;&#23427;&#12290;&#20026;&#20102;&#20943;&#23569;$\mathsf{CTAS}$&#30340;&#35745;&#31639;&#37327;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24179;&#26041;&#26681;&#35268;&#21017;&#30340;&#20302;&#22797;&#26434;&#24230;&#31639;&#27861;&#31216;&#20026;CO&#65292;&#22312;&#31616;&#21270;&#30340;&#21452;&#33218;&#27169;&#22411;&#20013;&#35777;&#26126;&#20102;&#20854;&#26368;&#20248;&#24615;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16710v1 Announce Type: cross  Abstract: In this paper, we study a best arm identification problem with dual objects. In addition to the classic reward, each arm is associated with a cost distribution and the goal is to identify the largest reward arm using the minimum expected cost. We call it \emph{Cost Aware Best Arm Identification} (CABAI), which captures the separation of testing and implementation phases in product development pipelines and models the objective shift between phases, i.e., cost for testing and reward for implementation. We first derive an theoretic lower bound for CABAI and propose an algorithm called $\mathsf{CTAS}$ to match it asymptotically. To reduce the computation of $\mathsf{CTAS}$, we further propose a low-complexity algorithm called CO, based on a square-root rule, which proves optimal in simplified two-armed models and generalizes surprisingly well in numerical experiments. Our results show (i) ignoring the heterogeneous action cost results in 
&lt;/p&gt;</description></item><item><title>&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#26041;&#27861;&#19982;&#20256;&#32479;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26377;&#25152;&#19981;&#21516;&#65292;&#20294;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#34987;&#35270;&#20026;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#29305;&#20363;&#65292;&#36825;&#31181;&#21457;&#29616;&#26550;&#36215;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20026;NCE&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#20248;&#21270;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16688</link><description>&lt;p&gt;
&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#19982;&#23545;&#27604;&#25955;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the connection between Noise-Contrastive Estimation and Contrastive Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16688
&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#26041;&#27861;&#19982;&#20256;&#32479;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26377;&#25152;&#19981;&#21516;&#65292;&#20294;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#34987;&#35270;&#20026;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#29305;&#20363;&#65292;&#36825;&#31181;&#21457;&#29616;&#26550;&#36215;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20026;NCE&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#20248;&#21270;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#38750;&#26631;&#20934;&#27010;&#29575;&#27169;&#22411;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#27604;&#22914;&#33021;&#37327;&#22522;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#38750;&#24120;&#26377;&#25928;&#12290;&#19982;&#20381;&#36182;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;&#23548;&#33268;ML-IS&#65289;&#25110;MCMC&#65288;&#23548;&#33268;&#23545;&#27604;&#25955;&#24230;&#65292;CD&#65289;&#30340;&#32463;&#20856;&#26368;&#22823;&#20284;&#28982;&#65288;ML&#65289;&#20272;&#35745;&#19981;&#21516;&#65292;NCE&#20351;&#29992;&#20195;&#29702;&#26631;&#20934;&#26469;&#36991;&#20813;&#35780;&#20272;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#30340;&#26631;&#20934;&#21270;&#24120;&#25968;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#27010;&#24565;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#25105;&#20204;&#34920;&#26126;&#20004;&#31181;NCE&#26631;&#20934;&#65292;&#25490;&#21517;NCE&#65288;RNCE&#65289;&#21644;&#26465;&#20214;NCE&#65288;CNCE&#65289;&#65292;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;RNCE&#31561;&#21516;&#20110;&#19982;&#26465;&#20214;&#37325;&#35201;&#24615;&#25277;&#26679;&#30456;&#32467;&#21512;&#30340;ML&#20272;&#35745;&#65292;&#32780;RNCE&#21644;CNCE&#37117;&#26159;&#23545;&#27604;&#25955;&#24230;&#30340;&#29305;&#20363;&#12290;&#36825;&#20123;&#21457;&#29616;&#22635;&#34917;&#20102;&#20004;&#31181;&#26041;&#27861;&#31867;&#21035;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#23558;ML-IS&#21644;CD&#25991;&#29486;&#20013;&#30340;&#25216;&#26415;&#24212;&#29992;&#20110;NCE&#65292;&#25552;&#20379;&#20102;&#20960;&#20010;&#26377;&#21033;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16688v1 Announce Type: new  Abstract: Noise-contrastive estimation (NCE) is a popular method for estimating unnormalised probabilistic models, such as energy-based models, which are effective for modelling complex data distributions. Unlike classical maximum likelihood (ML) estimation that relies on importance sampling (resulting in ML-IS) or MCMC (resulting in contrastive divergence, CD), NCE uses a proxy criterion to avoid the need for evaluating an often intractable normalisation constant.   Despite apparent conceptual differences, we show that two NCE criteria, ranking NCE (RNCE) and conditional NCE (CNCE), can be viewed as ML estimation methods. Specifically, RNCE is equivalent to ML estimation combined with conditional importance sampling, and both RNCE and CNCE are special cases of CD. These findings bridge the gap between the two method classes and allow us to apply techniques from the ML-IS and CD literature to NCE, offering several advantageous extensions.
&lt;/p&gt;</description></item><item><title>NIFTy.re&#37325;&#26032;&#26500;&#24314;&#20102;NIFTy&#30340;&#24314;&#27169;&#21407;&#21017;&#21644;&#25512;&#26029;&#31574;&#30053;&#65292;&#36890;&#36807;&#22806;&#21253;&#32321;&#37325;&#24037;&#20316;&#32473;JAX&#65292;&#21152;&#36895;&#20102;&#27169;&#22411;&#30340;&#36895;&#24230;&#65292;&#25552;&#21319;&#20102;&#21487;&#32500;&#25252;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;JAX&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#30340;&#20114;&#25805;&#20316;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16683</link><description>&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#25968;&#20540;&#20449;&#24687;&#22330;&#29702;&#35770;&#65288;NIFTy.re&#65289;&#65306;&#39640;&#26031;&#36807;&#31243;&#21644;&#21464;&#20998;&#25512;&#26029;&#24211;
&lt;/p&gt;
&lt;p&gt;
Re-Envisioning Numerical Information Field Theory (NIFTy.re): A Library for Gaussian Processes and Variational Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16683
&lt;/p&gt;
&lt;p&gt;
NIFTy.re&#37325;&#26032;&#26500;&#24314;&#20102;NIFTy&#30340;&#24314;&#27169;&#21407;&#21017;&#21644;&#25512;&#26029;&#31574;&#30053;&#65292;&#36890;&#36807;&#22806;&#21253;&#32321;&#37325;&#24037;&#20316;&#32473;JAX&#65292;&#21152;&#36895;&#20102;&#27169;&#22411;&#30340;&#36895;&#24230;&#65292;&#25552;&#21319;&#20102;&#21487;&#32500;&#25252;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;JAX&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#30340;&#20114;&#25805;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26500;&#24314;&#27169;&#21407;&#21017;&#12289;&#25193;&#23637;&#25512;&#26029;&#31574;&#30053;&#65292;&#20197;&#21450;&#23558;&#22823;&#37096;&#20998;&#32321;&#37325;&#24037;&#20316;&#22806;&#21253;&#32473;JAX&#65292;&#37325;&#26032;&#21152;&#36895;&#32534;&#20889;&#22312;NIFTy&#20013;&#30340;&#27169;&#22411;&#65292;&#22880;&#23450;&#20102;&#26032;&#31867;&#22411;&#25512;&#29702;&#26426;&#21046;&#30340;&#22522;&#30784;&#65292;&#25552;&#39640;&#20102;&#21487;&#32500;&#25252;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;NIFTy&#19982;JAX&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20043;&#38388;&#30340;&#20114;&#25805;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16683v1 Announce Type: cross  Abstract: Imaging is the process of transforming noisy, incomplete data into a space that humans can interpret. NIFTy is a Bayesian framework for imaging and has already successfully been applied to many fields in astrophysics. Previous design decisions held the performance and the development of methods in NIFTy back. We present a rewrite of NIFTy, coined NIFTy.re, which reworks the modeling principle, extends the inference strategies, and outsources much of the heavy lifting to JAX. The rewrite dramatically accelerates models written in NIFTy, lays the foundation for new types of inference machineries, improves maintainability, and enables interoperability between NIFTy and the JAX machine learning ecosystem.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26465;&#20214;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23545;&#21464;&#37327;&#36827;&#34892;&#24314;&#27169;/&#20272;&#35745;&#65292;&#24182;&#24212;&#29992;Group Lasso&#24809;&#32602;&#36827;&#34892;&#21464;&#37327;&#36873;&#25321;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#21463;&#23457;&#26597;&#29983;&#23384;&#25968;&#25454;&#30340;&#20998;&#26512;&#21644;&#26356;&#26377;&#25928;&#30340;&#20998;&#24067;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.16661</link><description>&lt;p&gt;
&#21463;&#24809;&#32602;&#30340;&#29983;&#25104;&#21464;&#37327;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Penalized Generative Variable Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26465;&#20214;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23545;&#21464;&#37327;&#36827;&#34892;&#24314;&#27169;/&#20272;&#35745;&#65292;&#24182;&#24212;&#29992;Group Lasso&#24809;&#32602;&#36827;&#34892;&#21464;&#37327;&#36873;&#25321;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#21463;&#23457;&#26597;&#29983;&#23384;&#25968;&#25454;&#30340;&#20998;&#26512;&#21644;&#26356;&#26377;&#25928;&#30340;&#20998;&#24067;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#65292;&#21253;&#25324;&#20855;&#26377;&#39640;&#32500;&#39044;&#27979;&#21464;&#37327;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#20998;&#26512;&#20013;&#65292;&#38500;&#20102;&#38656;&#35201;&#20272;&#35745;/&#27169;&#22411;&#26500;&#24314;&#22806;&#65292;&#21487;&#33021;&#36824;&#38656;&#35201;&#36827;&#34892;&#21464;&#37327;&#36873;&#25321;&#12290;&#29616;&#26377;&#22823;&#22810;&#25968;&#32467;&#21512;&#21464;&#37327;&#36873;&#25321;&#30340;&#28145;&#24230;&#32593;&#32476;&#30740;&#31350;&#20165;&#38480;&#20110;&#26041;&#27861;&#21644;&#25968;&#20540;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;&#26465;&#20214;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#24314;&#27169;/&#20272;&#35745;&#12290;&#24212;&#29992;Group Lasso&#24809;&#32602;&#36827;&#34892;&#21464;&#37327;&#36873;&#25321;&#65292;&#21487;&#33021;&#25913;&#21892;&#27169;&#22411;&#20272;&#35745;/&#39044;&#27979;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#31283;&#23450;&#24615;&#31561;&#12290;&#20174;&#29616;&#26377;&#25991;&#29486;&#20013;&#26174;&#33879;&#36827;&#23637;&#65292;&#36824;&#32771;&#34385;&#20102;&#21463;&#23457;&#26597;&#29983;&#23384;&#25968;&#25454;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#21464;&#37327;&#36873;&#25321;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#36924;&#36817;&#35823;&#24046;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#26377;&#25928;&#30340;&#20998;&#24067;&#20272;&#35745;&#12290;&#27169;&#25311;&#21644;&#23454;&#39564;&#25968;&#25454;&#30340;&#20998;&#26512;&#35777;&#26126;&#20102;&#28385;&#24847;&#30340;&#23454;&#36341;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16661v1 Announce Type: new  Abstract: Deep networks are increasingly applied to a wide variety of data, including data with high-dimensional predictors. In such analysis, variable selection can be needed along with estimation/model building. Many of the existing deep network studies that incorporate variable selection have been limited to methodological and numerical developments. In this study, we consider modeling/estimation using the conditional Wasserstein Generative Adversarial networks. Group Lasso penalization is applied for variable selection, which may improve model estimation/prediction, interpretability, stability, etc. Significantly advancing from the existing literature, the analysis of censored survival data is also considered. We establish the convergence rate for variable selection while considering the approximation error, and obtain a more efficient distribution estimation. Simulations and the analysis of real experimental data demonstrate satisfactory prac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20010;&#26631;&#20934;&#36827;&#34892;&#20248;&#21270;&#22120;&#22522;&#20934;&#27979;&#35797;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#27425;&#24207;&#20449;&#24687;&#24182;&#20801;&#35768;&#19981;&#21487;&#27604;&#24615;&#65292;&#36991;&#20813;&#20102;&#32858;&#21512;&#30340;&#32570;&#28857;&#65292;&#21487;&#20197;&#35782;&#21035;&#20135;&#29983;&#20013;&#24515;&#25110;&#31163;&#32676;&#25490;&#24207;&#30340;&#27979;&#35797;&#20989;&#25968;&#65292;&#24182;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.16565</link><description>&lt;p&gt;
&#20248;&#21270;&#22120;&#30340;&#37096;&#20998;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Partial Rankings of Optimizers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20010;&#26631;&#20934;&#36827;&#34892;&#20248;&#21270;&#22120;&#22522;&#20934;&#27979;&#35797;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#27425;&#24207;&#20449;&#24687;&#24182;&#20801;&#35768;&#19981;&#21487;&#27604;&#24615;&#65292;&#36991;&#20813;&#20102;&#32858;&#21512;&#30340;&#32570;&#28857;&#65292;&#21487;&#20197;&#35782;&#21035;&#20135;&#29983;&#20013;&#24515;&#25110;&#31163;&#32676;&#25490;&#24207;&#30340;&#27979;&#35797;&#20989;&#25968;&#65292;&#24182;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26681;&#25454;&#22810;&#20010;&#26631;&#20934;&#22312;&#21508;&#31181;&#27979;&#35797;&#20989;&#25968;&#19978;&#23545;&#20248;&#21270;&#22120;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26694;&#26550;&#12290;&#22522;&#20110;&#26368;&#36817;&#24341;&#20837;&#30340;&#29992;&#20110;&#20559;&#24207;/&#25490;&#24207;&#30340;&#26080;&#38598;&#21512;&#27867;&#20989;&#28145;&#24230;&#20989;&#25968;&#65292;&#23427;&#20805;&#20998;&#21033;&#29992;&#20102;&#27425;&#24207;&#20449;&#24687;&#24182;&#20801;&#35768;&#19981;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25551;&#36848;&#20102;&#25152;&#26377;&#37096;&#20998;&#39034;&#24207;/&#25490;&#24207;&#30340;&#20998;&#24067;&#65292;&#36991;&#20813;&#20102;&#32858;&#21512;&#30340;&#33261;&#21517;&#26157;&#33879;&#30340;&#32570;&#28857;&#12290;&#36825;&#20801;&#35768;&#35782;&#21035;&#20135;&#29983;&#20248;&#21270;&#22120;&#30340;&#20013;&#24515;&#25110;&#31163;&#32676;&#25490;&#24207;&#30340;&#27979;&#35797;&#20989;&#25968;&#65292;&#24182;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16565v1 Announce Type: cross  Abstract: We introduce a framework for benchmarking optimizers according to multiple criteria over various test functions. Based on a recently introduced union-free generic depth function for partial orders/rankings, it fully exploits the ordinal information and allows for incomparability. Our method describes the distribution of all partial orders/rankings, avoiding the notorious shortcomings of aggregation. This permits to identify test functions that produce central or outlying rankings of optimizers and to assess the quality of benchmarking suites.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#39640;&#26031;&#26680;&#30340;&#32463;&#39564;&#22270;&#25289;&#26222;&#25289;&#26031;&#30340;&#35889;&#29305;&#24615;&#65292;&#35777;&#26126;&#20102;&#20854;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#31354;&#38388;&#19982;&#27969;&#24418;&#19978;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#31639;&#23376;&#30340;&#25509;&#36817;&#65292;&#24182;&#23558;&#20854;&#19982;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#30456;&#32852;&#31995;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#21033;&#29992;&#26080;&#38480;&#32500;&#24230;&#20013;&#30340;&#32463;&#39564;&#21327;&#26041;&#24046;&#31639;&#23376;&#32467;&#26524;&#30340;&#26032;&#39062;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2402.16481</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#30340;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#26144;&#23556;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A kernel-based analysis of Laplacian Eigenmaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#39640;&#26031;&#26680;&#30340;&#32463;&#39564;&#22270;&#25289;&#26222;&#25289;&#26031;&#30340;&#35889;&#29305;&#24615;&#65292;&#35777;&#26126;&#20102;&#20854;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#31354;&#38388;&#19982;&#27969;&#24418;&#19978;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#31639;&#23376;&#30340;&#25509;&#36817;&#65292;&#24182;&#23558;&#20854;&#19982;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#30456;&#32852;&#31995;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#21033;&#29992;&#26080;&#38480;&#32500;&#24230;&#20013;&#30340;&#32463;&#39564;&#21327;&#26041;&#24046;&#31639;&#23376;&#32467;&#26524;&#30340;&#26032;&#39062;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#22312;&#38381;&#27969;&#24418;$\mathcal{M}\subseteq \mathbb{R}^p$&#19978;&#22343;&#21248;&#20998;&#24067;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#35266;&#27979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#39640;&#26031;&#26680;&#30340;&#30456;&#20851;&#32463;&#39564;&#22270;&#25289;&#26222;&#25289;&#26031;&#30340;&#35889;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#38750;&#28176;&#36817;&#35823;&#24046;&#30028;&#65292;&#26174;&#31034;&#20102;&#32463;&#39564;&#22270;&#25289;&#26222;&#25289;&#26031;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#31354;&#38388;&#25509;&#36817;&#20110;$\mathcal{M}$&#30340;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#31354;&#38388;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#23558;&#32463;&#39564;&#22270;&#25289;&#26222;&#25289;&#26031;&#19982;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#30456;&#32852;&#31995;&#65292;&#24182;&#32771;&#34385;$\mathcal{M}$&#30340;&#28909;&#26680;&#20316;&#20026;&#20877;&#29983;&#26680;&#29305;&#24449;&#26144;&#23556;&#12290;&#36825;&#23548;&#33268;&#20102;&#26032;&#39062;&#30340;&#35270;&#35282;&#65292;&#24182;&#20801;&#35768;&#21033;&#29992;&#26080;&#38480;&#32500;&#24230;&#20013;&#30340;&#32463;&#39564;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16481v1 Announce Type: cross  Abstract: Given i.i.d. observations uniformly distributed on a closed manifold $\mathcal{M}\subseteq \mathbb{R}^p$, we study the spectral properties of the associated empirical graph Laplacian based on a Gaussian kernel. Our main results are non-asymptotic error bounds, showing that the eigenvalues and eigenspaces of the empirical graph Laplacian are close to the eigenvalues and eigenspaces of the Laplace-Beltrami operator of $\mathcal{M}$. In our analysis, we connect the empirical graph Laplacian to kernel principal component analysis, and consider the heat kernel of $\mathcal{M}$ as reproducing kernel feature map. This leads to novel points of view and allows to leverage results for empirical covariance operators in infinite dimensions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#21464;&#32479;&#35745;&#25439;&#22833;&#35757;&#32451;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#27169;&#24335;&#32570;&#22833;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.16435</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#21464;&#32479;&#35745;&#25439;&#22833;&#35757;&#32451;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Implicit Generative Models via an Invariant Statistical Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16435
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#21464;&#32479;&#35745;&#25439;&#22833;&#35757;&#32451;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#27169;&#24335;&#32570;&#22833;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#23398;&#20064;&#20219;&#24847;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#38656;&#35201;&#36890;&#36807;&#23545;&#25239;&#24615;&#37492;&#21035;&#22120;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#27169;&#24335;&#32570;&#22833;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37492;&#21035;&#22120;&#30340;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#19968;&#32500;&#65288;1D&#65289;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#65292;&#38543;&#21518;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#20197;&#36866;&#24212;&#22810;&#21464;&#37327;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#27169;&#22411;&#26679;&#26412;&#32463;&#36807;&#36866;&#24403;&#36873;&#25321;&#30340;&#21464;&#25442;&#19982;&#22343;&#21248;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#24230;&#37327;&#65307;&#22240;&#27492;&#65292;&#23427;&#23545;&#25968;&#25454;&#30340;&#30495;&#23454;&#20998;&#24067;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#19968;&#32500;&#38543;&#26426;&#21464;&#37327;&#21046;&#23450;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20026;&#36817;&#20284;&#37325;&#21442;&#25968;&#21270;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16435v1 Announce Type: cross  Abstract: Implicit generative models have the capability to learn arbitrary complex data distributions. On the downside, training requires telling apart real data from artificially-generated ones using adversarial discriminators, leading to unstable training and mode-dropping issues. As reported by Zahee et al. (2017), even in the one-dimensional (1D) case, training a generative adversarial network (GAN) is challenging and often suboptimal. In this work, we develop a discriminator-free method for training one-dimensional (1D) generative implicit models and subsequently expand this method to accommodate multivariate cases. Our loss function is a discrepancy measure between a suitably chosen transformation of the model samples and a uniform distribution; hence, it is invariant with respect to the true distribution of the data. We first formulate our method for 1D random variables, providing an effective solution for approximate reparameterization 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31283;&#23450;&#35757;&#32451;&#39640;&#32500;&#21464;&#20998;&#25512;&#26029;&#20013;&#27491;&#35268;&#21270;&#27969;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.16408</link><description>&lt;p&gt;
&#39640;&#32500;&#21464;&#20998;&#25512;&#26029;&#20013;&#27491;&#35268;&#21270;&#27969;&#30340;&#31283;&#23450;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stable Training of Normalizing Flows for High-dimensional Variational Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16408
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31283;&#23450;&#35757;&#32451;&#39640;&#32500;&#21464;&#20998;&#25512;&#26029;&#20013;&#27491;&#35268;&#21270;&#27969;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27491;&#35268;&#21270;&#27969;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#22312;&#21462;&#20195;MCMC&#26041;&#27861;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;&#32806;&#21512;&#23618;&#65288;Real NVPs&#65289;&#30340;&#27491;&#35268;&#21270;&#27969;&#30001;&#20110;&#20854;&#33391;&#22909;&#30340;&#32463;&#39564;&#24615;&#33021;&#32780;&#32463;&#24120;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#35757;&#32451;&#29992;&#20110;&#36924;&#36817;&#39640;&#32500;&#21518;&#39564;&#20998;&#24067;&#30340;&#28145;&#23618;&#27491;&#35268;&#21270;&#27969;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#38543;&#26426;&#26799;&#24230;&#30340;&#39640;&#26041;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20808;&#21069;&#29992;&#20110;&#31283;&#23450;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#24046;&#30340;&#26041;&#27861;&#21487;&#33021;&#19981;&#36275;&#20197;&#23454;&#29616;Real NVPs&#30340;&#31283;&#23450;&#35757;&#32451;&#12290;&#25105;&#20204;&#30830;&#23450;&#38382;&#39064;&#30340;&#26681;&#28304;&#26159;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#26679;&#26412;&#36890;&#24120;&#21576;&#29616;&#24322;&#24120;&#39640;&#30340;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#32452;&#21512;&#65306;&#65288;1&#65289;&#23545;Real NVPs&#20013;&#30340;&#23610;&#24230;&#36827;&#34892;&#36719;&#38408;&#20540;&#22788;&#29702;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23545;&#26679;&#26412;&#36827;&#34892;&#21452;&#23556;&#36719;&#23545;&#25968;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16408v1 Announce Type: cross  Abstract: Variational inference with normalizing flows (NFs) is an increasingly popular alternative to MCMC methods. In particular, NFs based on coupling layers (Real NVPs) are frequently used due to their good empirical performance. In theory, increasing the depth of normalizing flows should lead to more accurate posterior approximations. However, in practice, training deep normalizing flows for approximating high-dimensional posterior distributions is often infeasible due to the high variance of the stochastic gradients. In this work, we show that previous methods for stabilizing the variance of stochastic gradient descent can be insufficient to achieve stable training of Real NVPs. As the source of the problem, we identify that, during training, samples often exhibit unusual high values. As a remedy, we propose a combination of two methods: (1) soft-thresholding of the scale in Real NVPs, and (2) a bijective soft log transformation of the sam
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#20132;&#21449;&#19968;&#33268;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.16388</link><description>&lt;p&gt;
&#20855;&#26377;&#20132;&#21449;&#19968;&#33268;$p$-&#20540;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16388
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#20132;&#21449;&#19968;&#33268;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#38752;&#12289;&#21487;&#20449;&#21644;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#23545;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#35201;&#27714;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25511;&#21046;&#31867;&#22411;I&#38169;&#35823;&#29575;($\alpha$)&#32780;&#21448;&#19981;&#25439;&#23475;&#31995;&#32479;&#30340;&#32479;&#35745;&#21151;&#29575;($1-\beta$)&#21487;&#20197;&#24314;&#31435;&#20449;&#20219;&#65292;&#24182;&#20943;&#23569;&#19982;&#20551;&#21457;&#29616;&#30456;&#20851;&#30340;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#24403;&#21518;&#32493;&#31243;&#24207;&#26114;&#36149;&#26102;&#12290;&#21033;&#29992;&#31526;&#21512;&#39044;&#27979;&#21407;&#21017;&#30340;&#26041;&#27861;&#26377;&#26395;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20026;&#24322;&#24120;&#26816;&#27979;&#25552;&#20379;&#30456;&#24212;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#35813;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026;&#20132;&#21449;&#19968;&#33268;&#24322;&#24120;&#26816;&#27979;&#65292;&#24314;&#31435;&#22312;&#20026;&#39044;&#27979;&#20219;&#21153;&#35774;&#35745;&#30340;&#33879;&#21517;&#20132;&#21449;&#19968;&#33268;&#26041;&#27861;&#20043;&#19978;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#20182;&#22635;&#34917;&#20102;&#22312;&#24402;&#32435;&#19968;&#33268;&#24322;&#24120;&#26816;&#27979;&#29615;&#22659;&#20013;&#25193;&#23637;&#20808;&#21069;&#30740;&#31350;&#30340;&#33258;&#28982;&#30740;&#31350;&#31354;&#30333;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16388v1 Announce Type: cross  Abstract: Given the growing significance of reliable, trustworthy, and explainable machine learning, the requirement of uncertainty quantification for anomaly detection systems has become increasingly important. In this context, effectively controlling Type I error rates ($\alpha$) without compromising the statistical power ($1-\beta$) of these systems can build trust and reduce costs related to false discoveries, particularly when follow-up procedures are expensive. Leveraging the principles of conformal prediction emerges as a promising approach for providing respective statistical guarantees by calibrating a model's uncertainty. This work introduces a novel framework for anomaly detection, termed cross-conformal anomaly detection, building upon well-known cross-conformal methods designed for prediction tasks. With that, it addresses a natural research gap by extending previous works in the context of inductive conformal anomaly detection, rel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#32622;&#25442;&#30340;&#35268;&#33539;&#30456;&#20851;&#24615;&#30446;&#26631;&#23398;&#20064;&#34701;&#21512;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#22810;&#20010;&#35270;&#22270;&#30340;&#19968;&#33268;&#20266;&#26631;&#31614;&#26469;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#26377;&#25928;&#24615;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#36924;&#36817;&#30417;&#30563;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#34920;&#31034;&#65292;&#25552;&#20379;&#20102;&#30001;&#38169;&#35823;&#20266;&#26631;&#31614;&#27880;&#37322;&#24341;&#36215;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.16383</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Self Supervised Correlation-based Permutations for Multi-View Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16383
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#32622;&#25442;&#30340;&#35268;&#33539;&#30456;&#20851;&#24615;&#30446;&#26631;&#23398;&#20064;&#34701;&#21512;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#22810;&#20010;&#35270;&#22270;&#30340;&#19968;&#33268;&#20266;&#26631;&#31614;&#26469;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#26377;&#25928;&#24615;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#36924;&#36817;&#30417;&#30563;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#34920;&#31034;&#65292;&#25552;&#20379;&#20102;&#30001;&#38169;&#35823;&#20266;&#26631;&#31614;&#27880;&#37322;&#24341;&#36215;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#21487;&#20197;&#22686;&#24378;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#65292;&#21253;&#25324;&#32858;&#31867;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#65288;MVC&#65289;&#35299;&#20915;&#26041;&#26696;&#20165;&#38480;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#27425;&#20248;&#30340;&#19988;&#35745;&#31639;&#38656;&#27714;&#39640;&#30340;&#34920;&#31034;&#21644;&#32858;&#31867;&#20004;&#38454;&#27573;&#31243;&#24207;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#30340;&#36890;&#29992;&#25968;&#25454;&#65288;&#22270;&#20687;&#12289;&#34920;&#26684;&#31561;&#65289;&#30340;MVC&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#22522;&#20110;&#26032;&#39062;&#32622;&#25442;&#30340;&#35268;&#33539;&#30456;&#20851;&#24615;&#30446;&#26631;&#26469;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#34701;&#21512;&#25968;&#25454;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#36328;&#22810;&#20010;&#35270;&#22270;&#30340;&#19968;&#33268;&#20266;&#26631;&#31614;&#26469;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#12290;&#25105;&#20204;&#20351;&#29992;&#21313;&#20010;MVC&#22522;&#20934;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#36924;&#36817;&#20102;&#30417;&#30563;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#34920;&#31034;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30001;&#38169;&#35823;&#20266;&#26631;&#31614;&#27880;&#37322;&#24341;&#36215;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16383v1 Announce Type: new  Abstract: Fusing information from different modalities can enhance data analysis tasks, including clustering. However, existing multi-view clustering (MVC) solutions are limited to specific domains or rely on a suboptimal and computationally demanding two-stage procedure of representation and clustering. We propose an end-to-end deep learning-based MVC framework for general data (image, tabular, etc.). Our approach involves learning meaningful fused data representations with a novel permutation-based canonical correlation objective. Concurrently, we learn cluster assignments by identifying consistent pseudo-labels across multiple views. We demonstrate the effectiveness of our model using ten MVC benchmark datasets. Theoretically, we show that our model approximates the supervised linear discrimination analysis (LDA) representation. Additionally, we provide an error bound induced by false-pseudo label annotations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#39304;&#39640;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;</title><link>https://arxiv.org/abs/2402.16359</link><description>&lt;p&gt;
&#21453;&#39304;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Feedback Efficient Online Fine-Tuning of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#39304;&#39640;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#22270;&#20687;&#65292;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#27169;&#25311;&#26368;&#22823;&#21270;&#26576;&#20123;&#23646;&#24615;&#30340;&#20998;&#24067;&#30340;&#37096;&#20998;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#29983;&#25104;&#20855;&#26377;&#39640;&#23457;&#32654;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#25110;&#20855;&#26377;&#39640;&#29983;&#29289;&#27963;&#24615;&#30340;&#20998;&#23376;&#12290;&#33258;&#28982;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#36825;&#35270;&#20026;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#19982;&#26576;&#20123;&#23646;&#24615;&#23545;&#24212;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21363;&#20351;&#21487;&#20197;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#30340;&#22312;&#32447;&#26597;&#35810;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#39640;&#22870;&#21169;&#26679;&#26412;&#20063;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#23427;&#20204;&#22312;&#21021;&#22987;&#20998;&#24067;&#20013;&#30340;&#27010;&#29575;&#21487;&#33021;&#24456;&#20302;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#19981;&#21487;&#34892;&#30340;&#26679;&#26412;&#65292;&#29978;&#33267;&#27809;&#26377;&#23450;&#20041;&#33391;&#22909;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#19981;&#33258;&#28982;&#30340;&#22270;&#20687;&#25110;&#29289;&#29702;&#19978;&#19981;&#21487;&#33021;&#30340;&#20998;&#23376;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#21457;&#29616;&#39640;&#22870;&#21169;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16359v1 Announce Type: cross  Abstract: Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules. However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to fine-tune a diffusion model to maximize a reward function that corresponds to some property. Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). In this work, we propose a novel reinforcement learning procedure that effi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#30340;&#31616;&#21333;&#38543;&#26426;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#30697;&#38453;&#20056;&#27861;&#23454;&#29616;&#39640;&#36136;&#37327;&#36924;&#36817;&#20272;&#35745;&#27010;&#29575;&#21644;&#27169;&#22411;&#25972;&#20307;&#24046;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16326</link><description>&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#30340;&#21487;&#35777;&#23454;&#20934;&#30830;&#24615;&#38543;&#26426;&#25277;&#26679;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Provably Accurate Randomized Sampling Algorithm for Logistic Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16326
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#30340;&#31616;&#21333;&#38543;&#26426;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#30697;&#38453;&#20056;&#27861;&#23454;&#29616;&#39640;&#36136;&#37327;&#36924;&#36817;&#20272;&#35745;&#27010;&#29575;&#21644;&#27169;&#22411;&#25972;&#20307;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#36923;&#36753;&#22238;&#24402;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#20108;&#20998;&#31867;&#20219;&#21153;&#30340;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#12290;&#24403;&#35266;&#27979;&#25968;&#37327;&#36828;&#36828;&#36229;&#36807;&#39044;&#27979;&#21464;&#37327;&#25968;&#37327;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#38543;&#26426;&#25277;&#26679;&#30340;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#31639;&#27861;&#65292;&#20445;&#35777;&#39640;&#36136;&#37327;&#36924;&#36817;&#20272;&#35745;&#27010;&#29575;&#21644;&#27169;&#22411;&#25972;&#20307;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24314;&#31435;&#22312;&#20004;&#20010;&#31616;&#21333;&#30340;&#32467;&#26500;&#26465;&#20214;&#22522;&#30784;&#19978;&#65292;&#36825;&#20004;&#20010;&#26465;&#20214;&#21487;&#24402;&#32467;&#20026;&#38543;&#26426;&#30697;&#38453;&#20056;&#27861;&#65292;&#26159;&#38543;&#26426;&#21270;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#30340;&#22522;&#26412;&#19988;&#28145;&#20837;&#29702;&#35299;&#30340;&#22522;&#20803;&#12290;&#24403;&#21033;&#29992;&#26464;&#26438;&#20998;&#25968;&#23545;&#35266;&#27979;&#36827;&#34892;&#25277;&#26679;&#26102;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36923;&#36753;&#22238;&#24402;&#30340;&#20272;&#35745;&#27010;&#29575;&#23646;&#24615;&#65292;&#24182;&#35777;&#26126;&#20934;&#30830;&#36924;&#36817;&#21487;&#20197;&#36890;&#36807;&#36828;&#23567;&#20110;&#24635;&#35266;&#27979;&#25968;&#30340;&#26679;&#26412;&#23454;&#29616;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16326v1 Announce Type: cross  Abstract: In statistics and machine learning, logistic regression is a widely-used supervised learning technique primarily employed for binary classification tasks. When the number of observations greatly exceeds the number of predictor variables, we present a simple, randomized sampling-based algorithm for logistic regression problem that guarantees high-quality approximations to both the estimated probabilities and the overall discrepancy of the model. Our analysis builds upon two simple structural conditions that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized numerical linear algebra. We analyze the properties of estimated probabilities of logistic regression when leverage scores are used to sample observations, and prove that accurate approximations can be achieved with a sample whose size is much smaller than the total number of observations. To further validate our theoretical findi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#30340;&#32622;&#20449;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#24615;&#22238;&#24402;&#20013;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16300</link><description>&lt;p&gt;
Conformalized Selective Regression
&lt;/p&gt;
&lt;p&gt;
Conformalized Selective Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#30340;&#32622;&#20449;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#24615;&#22238;&#24402;&#20013;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27169;&#22411;&#26159;&#21542;&#24635;&#26159;&#35201;&#25552;&#20379;&#39044;&#27979;&#65311;&#22312;&#36861;&#27714;&#26368;&#22823;&#39044;&#27979;&#24615;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#21487;&#38752;&#24615;&#21644;&#20844;&#24179;&#24615;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#29992;&#12290;&#36873;&#25321;&#24615;&#22238;&#24402;&#65292;&#20063;&#31216;&#20026;&#8220;&#25298;&#32477;&#36873;&#39033;&#8221;&#65292;&#20801;&#35768;&#27169;&#22411;&#22312;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#25918;&#24323;&#39044;&#27979;&#12290;&#23613;&#31649;7&#21313;&#24180;&#21069;&#23601;&#26368;&#21021;&#25552;&#20986;&#20102;&#36873;&#25321;&#24615;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#29992;&#20110;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#20195;&#29702;&#65292;&#23588;&#20854;&#26159;&#26465;&#20214;&#26041;&#24046;&#12290;&#20294;&#36825;&#31181;&#20851;&#27880;&#24573;&#35270;&#20102;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#20026;&#22522;&#20110;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#30340;&#20010;&#21035;&#39044;&#27979;&#25552;&#20379;&#26377;&#26681;&#25454;&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#20415;&#36827;&#34892;&#24688;&#24403;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16300v1 Announce Type: new  Abstract: Should prediction models always deliver a prediction? In the pursuit of maximum predictive performance, critical considerations of reliability and fairness are often overshadowed, particularly when it comes to the role of uncertainty. Selective regression, also known as the "reject option," allows models to abstain from predictions in cases of considerable uncertainty. Initially proposed seven decades ago, approaches to selective regression have mostly focused on distribution-based proxies for measuring uncertainty, particularly conditional variance. However, this focus neglects the significant influence of model-specific biases on a model's performance. In this paper, we propose a novel approach to selective regression by leveraging conformal prediction, which provides grounded confidence measures for individual predictions based on model-specific biases. In addition, we propose a standardized evaluation framework to allow proper compar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#26080;&#20851;&#20844;&#24179;&#23398;&#20064;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;FedFaiREE&#65292;&#36866;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#20855;&#26377;&#23567;&#26679;&#26412;&#30340;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2402.16158</link><description>&lt;p&gt;
&#20998;&#24067;&#26080;&#20851;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#19982;&#23567;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Distribution-Free Fair Federated Learning with Small Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#26080;&#20851;&#20844;&#24179;&#23398;&#20064;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;FedFaiREE&#65292;&#36866;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#20855;&#26377;&#23567;&#26679;&#26412;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32852;&#37030;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#35757;&#32451;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#36328;&#32676;&#20307;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29992;&#20110;&#30830;&#20445;&#20844;&#24179;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#20026;&#38598;&#20013;&#21270;&#25968;&#25454;&#29615;&#22659;&#35774;&#35745;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#26679;&#26412;&#21644;&#20998;&#24067;&#20551;&#35774;&#65292;&#24378;&#35843;&#20102;&#36843;&#20999;&#38656;&#35201;&#38024;&#23545;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#20998;&#24067;&#26080;&#20851;&#20445;&#35777;&#30340;&#21435;&#20013;&#24515;&#21270;&#21644;&#24322;&#26500;&#31995;&#32479;&#36827;&#34892;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#35843;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;FedFaiREE&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#23567;&#26679;&#26412;&#30340;&#20998;&#24067;&#26080;&#20851;&#20844;&#24179;&#23398;&#20064;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20363;&#22914;&#23458;&#25143;&#24322;&#36136;&#24615;&#12289;&#36890;&#20449;&#25104;&#26412;&#21644;&#23567;&#26679;&#26412;&#22823;&#23567;&#12290;&#25105;&#20204;&#20026;bot&#25552;&#20379;&#20005;&#26684;&#30340;&#29702;&#35770;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16158v1 Announce Type: cross  Abstract: As federated learning gains increasing importance in real-world applications due to its capacity for decentralized data training, addressing fairness concerns across demographic groups becomes critically important. However, most existing machine learning algorithms for ensuring fairness are designed for centralized data environments and generally require large-sample and distributional assumptions, underscoring the urgent need for fairness techniques adapted for decentralized and heterogeneous systems with finite-sample and distribution-free guarantees. To address this issue, this paper introduces FedFaiREE, a post-processing algorithm developed specifically for distribution-free fair learning in decentralized settings with small samples. Our approach accounts for unique challenges in decentralized environments, such as client heterogeneity, communication costs, and small sample sizes. We provide rigorous theoretical guarantees for bot
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;VAE&#30340;&#26694;&#26550;&#65292;&#21487;&#32852;&#21512;&#23398;&#20064;&#19968;&#32452;&#30456;&#20851;&#20294;&#24322;&#26500;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#20197;&#21407;&#21017;&#24615;&#26041;&#24335;&#22788;&#29702;&#25552;&#21462;&#20849;&#20139;&#32467;&#26500;&#21644;&#35782;&#21035;&#20010;&#20307;&#29305;&#24615;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.16131</link><description>&lt;p&gt;
&#22522;&#20110;VAE&#30340;&#22810;&#23618;&#31070;&#32463;Granger-&#22240;&#26524;&#36830;&#25509;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16131
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;VAE&#30340;&#26694;&#26550;&#65292;&#21487;&#32852;&#21512;&#23398;&#20064;&#19968;&#32452;&#30456;&#20851;&#20294;&#24322;&#26500;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#20197;&#21407;&#21017;&#24615;&#26041;&#24335;&#22788;&#29702;&#25552;&#21462;&#20849;&#20139;&#32467;&#26500;&#21644;&#35782;&#21035;&#20010;&#20307;&#29305;&#24615;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Granger&#22240;&#26524;&#20851;&#31995;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#29992;&#20110;&#25429;&#25417;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#32452;&#20214;&#20043;&#38388;&#30340;&#20808;&#23548;-&#28382;&#21518;&#20851;&#31995;&#65292;&#29616;&#26377;&#25991;&#29486;&#30340;&#37325;&#28857;&#38598;&#20013;&#22312;&#21333;&#19968;&#21160;&#24577;&#31995;&#32479;&#19978;&#12290;&#22312;&#23439;&#35266;&#32463;&#27982;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#26576;&#20123;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#21487;&#20197;&#35775;&#38382;&#26469;&#33258;&#19968;&#32452;&#30456;&#20851;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#65292;&#24863;&#20852;&#36259;&#30340;&#24314;&#27169;&#20219;&#21153;&#26159;&#25552;&#21462;&#23884;&#20837;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#20849;&#20139;&#20844;&#20849;&#32467;&#26500;&#65292;&#20197;&#21450;&#35782;&#21035;&#21508;&#33258;&#31995;&#32479;&#20013;&#30340;&#29305;&#36136;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#32852;&#21512;&#23398;&#20064;&#20102;&#19968;&#32452;&#30456;&#20851;&#20294;&#24322;&#26500;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#20197;&#21407;&#21017;&#24615;&#26041;&#24335;&#22788;&#29702;&#19978;&#36848;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20960;&#31181;&#21512;&#25104;&#25968;&#25454;&#35774;&#32622;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20026;&#21333;&#20010;&#21160;&#24577;&#31995;&#32479;&#35774;&#35745;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16131v1 Announce Type: new  Abstract: Granger causality has been widely used in various application domains to capture lead-lag relationships amongst the components of complex dynamical systems, and the focus in extant literature has been on a single dynamical system. In certain applications in macroeconomics and neuroscience, one has access to data from a collection of related such systems, wherein the modeling task of interest is to extract the shared common structure that is embedded across them, as well as to identify the idiosyncrasies within individual ones. This paper introduces a Variational Autoencoder (VAE) based framework that jointly learns Granger-causal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems, and handles the aforementioned task in a principled way. The performance of the proposed framework is evaluated on several synthetic data settings and benchmarked against existing approaches designed for individual s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#23558;&#28145;&#39640;&#26031;&#36807;&#31243;&#25193;&#23637;&#21040;&#21253;&#21547;&#26799;&#24230;&#25968;&#25454;&#65292;&#29992;&#20110;&#22810;&#20445;&#30495;&#24314;&#27169;&#65292;&#33021;&#22815;&#25429;&#33719;&#19981;&#21516;&#20445;&#30495;&#25968;&#25454;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#21644;&#36755;&#20837;&#30456;&#20851;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.16059</link><description>&lt;p&gt;
&#26799;&#24230;&#22686;&#24378;&#28145;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#22810;&#20445;&#30495;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Gradient-enhanced deep Gaussian processes for multifidelity modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16059
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23558;&#28145;&#39640;&#26031;&#36807;&#31243;&#25193;&#23637;&#21040;&#21253;&#21547;&#26799;&#24230;&#25968;&#25454;&#65292;&#29992;&#20110;&#22810;&#20445;&#30495;&#24314;&#27169;&#65292;&#33021;&#22815;&#25429;&#33719;&#19981;&#21516;&#20445;&#30495;&#25968;&#25454;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#21644;&#36755;&#20837;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20445;&#30495;&#27169;&#22411;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#29983;&#25104;&#24213;&#23618;&#36807;&#31243;&#30340;&#21333;&#19968;&#36924;&#36817;&#22120;&#12290;&#23494;&#38598;&#30340;&#20302;&#20445;&#30495;&#26679;&#26412;&#29992;&#20110;&#20943;&#23569;&#25554;&#20540;&#35823;&#24046;&#65292;&#31232;&#30095;&#30340;&#39640;&#20445;&#30495;&#26679;&#26412;&#29992;&#20110;&#24357;&#34917;&#20302;&#20445;&#30495;&#26679;&#26412;&#20013;&#30340;&#20559;&#24046;&#25110;&#22122;&#38899;&#12290;&#26799;&#24230;&#22686;&#24378;&#30340;&#28145;&#39640;&#26031;&#36807;&#31243;&#23545;&#22810;&#20445;&#30495;&#24314;&#27169;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#19981;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#20851;&#38190;&#26159;&#33021;&#22815;&#25429;&#33719;&#19981;&#21516;&#20445;&#30495;&#25968;&#25454;&#20043;&#38388;&#38750;&#32447;&#24615;&#21644;&#36755;&#20837;&#30456;&#20851;&#30340;&#20851;&#31995;&#12290;&#35768;&#22810;&#25968;&#25454;&#38598;&#33258;&#28982;&#21253;&#21547;&#26799;&#24230;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#30001;&#19982;&#33258;&#21160;&#24494;&#20998;&#20860;&#23481;&#25110;&#20855;&#26377;&#20849;&#36717;&#35299;&#30340;&#35745;&#31639;&#27169;&#22411;&#29983;&#25104;&#26102;&#12290;&#26412;&#24037;&#20316;&#20027;&#35201;&#26159;&#23558;&#28145;&#39640;&#26031;&#36807;&#31243;&#25193;&#23637;&#20026;&#21253;&#21547;&#26799;&#24230;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20998;&#26512;&#27979;&#35797;&#38382;&#39064;&#21644;&#19968;&#20010;&#29616;&#23454;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#19978;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#36825;&#20004;&#20010;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#27668;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16059v1 Announce Type: cross  Abstract: Multifidelity models integrate data from multiple sources to produce a single approximator for the underlying process. Dense low-fidelity samples are used to reduce interpolation error, while sparse high-fidelity samples are used to compensate for bias or noise in the low-fidelity samples. Deep Gaussian processes (GPs) are attractive for multifidelity modelling as they are non-parametric, robust to overfitting, perform well for small datasets, and, critically, can capture nonlinear and input-dependent relationships between data of different fidelities. Many datasets naturally contain gradient data, especially when they are generated by computational models that are compatible with automatic differentiation or have adjoint solutions. Principally, this work extends deep GPs to incorporate gradient data. We demonstrate this method on an analytical test problem and a realistic partial differential equation problem, where we predict the aer
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#23398;&#20064;&#22312;&#32500;&#24230;N&#20013;&#30340;$\omega(\log \log N)$&#20010;&#21322;&#31354;&#38388;&#29978;&#33267;&#38656;&#35201;&#36229;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#26631;&#20934;&#20551;&#35774;&#65292;&#26174;&#33879;&#32553;&#23567;&#20102;&#36825;&#19968;&#24046;&#36317;</title><link>https://arxiv.org/abs/2402.15995</link><description>&lt;p&gt;
&#25913;&#36827;&#23398;&#20064;&#21322;&#31354;&#38388;&#20132;&#38598;&#30340;&#22256;&#38590;&#24615;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Improved Hardness Results for Learning Intersections of Halfspaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15995
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#23398;&#20064;&#22312;&#32500;&#24230;N&#20013;&#30340;$\omega(\log \log N)$&#20010;&#21322;&#31354;&#38388;&#29978;&#33267;&#38656;&#35201;&#36229;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#26631;&#20934;&#20551;&#35774;&#65292;&#26174;&#33879;&#32553;&#23567;&#20102;&#36825;&#19968;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#27491;&#30830;&#35774;&#32622;&#20013;&#23398;&#20064;&#21322;&#31354;&#38388;&#20132;&#38598;&#30340;&#24369;&#23398;&#20064;&#19979;&#30028;&#65292;&#36825;&#20123;&#19979;&#30028;&#38750;&#24120;&#24378;&#22823;&#65288;&#24182;&#19988;&#20196;&#20154;&#24778;&#35766;&#22320;&#31616;&#21333;&#65289;&#12290;&#20851;&#20110;&#36825;&#20010;&#38382;&#39064;&#30693;&#20043;&#29978;&#23569;&#12290;&#20363;&#22914;&#65292;&#29978;&#33267;&#19981;&#30693;&#36947;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#23398;&#20064;&#20165;&#20004;&#20010;&#21322;&#31354;&#38388;&#30340;&#20132;&#38598;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#33391;&#22909;&#24314;&#31435;&#30340;&#20551;&#35774;&#65288;&#22914;&#36817;&#20284;&#26368;&#22351;&#24773;&#20917;&#30340;&#26684;&#38382;&#39064;&#25110;Feige&#30340;3SAT&#20551;&#35774;&#30340;&#21464;&#20307;&#65289;&#30340;&#19979;&#30028;&#20165;&#23545;&#36229;&#23545;&#25968;&#20010;&#21322;&#31354;&#38388;&#30340;&#20132;&#38598;&#24050;&#30693;&#65288;&#25110;&#32773;&#30001;&#24050;&#26377;&#32467;&#26524;&#26263;&#31034;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15995v1 Announce Type: cross  Abstract: We show strong (and surprisingly simple) lower bounds for weakly learning intersections of halfspaces in the improper setting. Strikingly little is known about this problem. For instance, it is not even known if there is a polynomial-time algorithm for learning the intersection of only two halfspaces. On the other hand, lower bounds based on well-established assumptions (such as approximating worst-case lattice problems or variants of Feige's 3SAT hypothesis) are only known (or are implied by existing results) for the intersection of super-logarithmically many halfspaces [KS09,KS06,DSS16]. With intersections of fewer halfspaces being only ruled out under less standard assumptions [DV21] (such as the existence of local pseudo-random generators with large stretch). We significantly narrow this gap by showing that even learning $\omega(\log \log N)$ halfspaces in dimension $N$ takes super-polynomial time under standard assumptions on wors
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#34920;&#36798;&#24335;&#23548;&#20986;&#23574;&#23792;&#21464;&#25442;&#65292;&#23454;&#29616;&#20102;&#23545;&#21508;&#31181;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#25551;&#36848;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.15984</link><description>&lt;p&gt;
&#29992;&#32479;&#19968;&#30340;&#20613;&#37324;&#21494;&#20999;&#29255;&#26041;&#27861;&#23548;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#28145;&#24230;-2&#31070;&#32463;&#32593;&#32476;&#30340;&#23574;&#23792;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
A unified Fourier slice method to derive ridgelet transform for a variety of depth-2 neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15984
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#34920;&#36798;&#24335;&#23548;&#20986;&#23574;&#23792;&#21464;&#25442;&#65292;&#23454;&#29616;&#20102;&#23545;&#21508;&#31181;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#25551;&#36848;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26102;&#65292;&#30740;&#31350;&#21442;&#25968;&#20998;&#24067;&#27604;&#30740;&#31350;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#26356;&#23481;&#26131;&#12290;&#23574;&#23792;&#21464;&#25442;&#26159;&#19968;&#20010;&#20266;&#36870;&#31639;&#23376;&#65292;&#23558;&#32473;&#23450;&#20989;&#25968; $f$ &#26144;&#23556;&#21040;&#21442;&#25968;&#20998;&#24067; $\gamma$&#65292;&#20351;&#24471;&#32593;&#32476; $\mathtt{NN}[\gamma]$ &#33021;&#22815;&#37325;&#29616; $f$&#65292;&#21363; $\mathtt{NN}[\gamma]=f$&#12290;&#22312;&#27431;&#27663;&#31354;&#38388;&#19978;&#30340;&#28145;&#24230;-2&#20840;&#36830;&#25509;&#32593;&#32476;&#20013;&#65292;&#24050;&#21457;&#29616;&#20102;&#23574;&#23792;&#21464;&#25442;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#25551;&#36848;&#21442;&#25968;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#31181;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23578;&#19981;&#30693;&#36947;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20613;&#37324;&#21494;&#34920;&#36798;&#24335;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#23548;&#21508;&#31181;&#29616;&#20195;&#32593;&#32476;&#30340;&#23574;&#23792;&#21464;&#25442;&#65292;&#20363;&#22914;&#26377;&#38480;&#22495; $\mathbb{F}_p$ &#19978;&#30340;&#32593;&#32476;&#12289;&#25277;&#35937;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388; $\mathcal{H}$ &#19978;&#30340;&#32676;&#21367;&#31215;&#32593;&#32476;&#65292;&#20197;&#21450;&#38750;&#32039;&#33268;&#23545;&#31216;&#30340;&#20840;&#36830;&#25509;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15984v1 Announce Type: new  Abstract: To investigate neural network parameters, it is easier to study the distribution of parameters than to study the parameters in each neuron. The ridgelet transform is a pseudo-inverse operator that maps a given function $f$ to the parameter distribution $\gamma$ so that a network $\mathtt{NN}[\gamma]$ reproduces $f$, i.e. $\mathtt{NN}[\gamma]=f$. For depth-2 fully-connected networks on a Euclidean space, the ridgelet transform has been discovered up to the closed-form expression, thus we could describe how the parameters are distributed. However, for a variety of modern neural network architectures, the closed-form expression has not been known. In this paper, we explain a systematic method using Fourier expressions to derive ridgelet transforms for a variety of modern networks such as networks on finite fields $\mathbb{F}_p$, group convolutional networks on abstract Hilbert space $\mathcal{H}$, fully-connected networks on noncompact symm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#20284;&#28982;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#36125;&#21494;&#26031;&#36793;&#32536;&#20284;&#28982;&#21644;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#26131;&#31232;&#30095;&#21270;&#65292;&#24182;&#37319;&#29992;&#33258;&#21160;&#22885;&#21345;&#22982;&#21059;&#20992;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26435;&#37325;&#21066;&#20943;&#12290;</title><link>https://arxiv.org/abs/2402.15978</link><description>&lt;p&gt;
&#20351;&#29992;&#22885;&#21345;&#22982;&#21059;&#20992;&#21066;&#20943;&#26435;&#37325;&#65306;&#20351;&#29992;&#36793;&#32536;&#20284;&#28982;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural Networks Using the Marginal Likelihood
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#20284;&#28982;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#36125;&#21494;&#26031;&#36793;&#32536;&#20284;&#28982;&#21644;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#26131;&#31232;&#30095;&#21270;&#65292;&#24182;&#37319;&#29992;&#33258;&#21160;&#22885;&#21345;&#22982;&#21059;&#20992;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26435;&#37325;&#21066;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#21270;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#65292;&#21487;&#20197;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#35768;&#22810;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21464;&#24471;&#36807;&#22823;&#20197;&#33267;&#26080;&#27861;&#30452;&#25509;&#37096;&#32626;&#22312;&#28040;&#36153;&#31867;&#30828;&#20214;&#30340;&#26102;&#20195;&#12290;&#34429;&#28982;&#24456;&#22810;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#19981;&#21516;&#30340;&#26435;&#37325;&#21098;&#26525;&#20934;&#21017;&#19978;&#65292;&#20294;&#32593;&#32476;&#30340;&#24635;&#20307;&#31232;&#30095;&#24615;&#65292;&#21363;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21098;&#26525;&#30340;&#33021;&#21147;&#65292;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#36793;&#32536;&#20284;&#28982;&#37327;&#65288;Marginal likelihood&#65289;&#30340;&#31232;&#30095;&#24615;&#65288;SpaM&#65289;&#65292;&#19968;&#20010;&#31232;&#30095;&#21270;&#26694;&#26550;&#65292;&#37325;&#28857;&#24378;&#35843;&#20351;&#29992;&#36125;&#21494;&#26031;&#36793;&#32536;&#20284;&#28982;&#19982;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#26131;&#31232;&#30095;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#33258;&#21160;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#65292;&#36873;&#25321;&#26368;&#24819;&#35201;&#21066;&#20943;&#30340;&#27169;&#22411;&#65292;&#20197;&#20381;&#28982;&#33021;&#22815;&#24456;&#22909;&#22320;&#35299;&#37322;&#25968;&#25454;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#32467;&#26500;&#21270;&#36824;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#31232;&#30095;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#20013;&#20351;&#29992;&#30340;&#39044;&#35745;&#31639;&#21518;&#39564;&#40657;&#22622;&#36817;&#20284;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15978v1 Announce Type: new  Abstract: Neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful AI models are becoming too large to na\"ively deploy on consumer hardware. While much work has focused on different weight pruning criteria, the overall sparsifiability of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked. We present Sparsifiability via the Marginal likelihood (SpaM), a pruning framework that highlights the effectiveness of using the Bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable. Our approach implements an automatic Occam's razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification. In addition, we demonstrate that the pre-computed posterior Hessian approximation used in the Laplace approximation
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#34920;&#26126;&#23545;&#20110;&#20855;&#26377;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#30340;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#65292;&#35774;&#32622;&#19968;&#20010;&#24658;&#23450;&#20294;&#36739;&#22823;&#30340;&#27493;&#38271;&#65292;&#22312;&#21021;&#22987;&#38663;&#33633;&#21518;&#21487;&#20197;&#23454;&#29616;&#36739;&#24555;&#30340;&#25910;&#25947;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#27493;&#39588;&#21518;&#21487;&#20197;&#36798;&#21040;&#21152;&#36895;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#21160;&#37327;&#25110;&#21464;&#27493;&#38271;&#35843;&#24230;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.15926</link><description>&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#30340;&#22823;&#27493;&#26799;&#24230;&#19979;&#38477;&#65306;&#25439;&#22833;&#30340;&#38750;&#21333;&#35843;&#24615;&#25552;&#39640;&#20102;&#20248;&#21270;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15926
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#34920;&#26126;&#23545;&#20110;&#20855;&#26377;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#30340;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#65292;&#35774;&#32622;&#19968;&#20010;&#24658;&#23450;&#20294;&#36739;&#22823;&#30340;&#27493;&#38271;&#65292;&#22312;&#21021;&#22987;&#38663;&#33633;&#21518;&#21487;&#20197;&#23454;&#29616;&#36739;&#24555;&#30340;&#25910;&#25947;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#27493;&#39588;&#21518;&#21487;&#20197;&#36798;&#21040;&#21152;&#36895;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#21160;&#37327;&#25110;&#21464;&#27493;&#38271;&#35843;&#24230;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#19982;&#20855;&#26377;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#30340;&#36923;&#36753;&#22238;&#24402;&#32467;&#21512;&#20351;&#29992;&#30340;&#24658;&#23450;&#27493;&#38271;&#24773;&#20917;&#65292;&#20854;&#20013;&#24658;&#23450;&#27493;&#38271;$\eta$&#38750;&#24120;&#22823;&#65292;&#20197;&#33267;&#20110;&#25439;&#22833;&#22312;&#21021;&#22987;&#38454;&#27573;&#20250;&#38663;&#33633;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GD&#22312;$\mathcal{O}(\eta)$&#27493;&#20869;&#36805;&#36895;&#36864;&#20986;&#36825;&#31181;&#21021;&#22987;&#38663;&#33633;&#38454;&#27573;&#65292;&#24182;&#22312;&#39069;&#22806;&#30340;$t$&#27493;&#20043;&#21518;&#23454;&#29616;&#20102;&#19968;&#20010;$\tilde{\mathcal{O}}(1 / (\eta t) )$&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#65292;&#32473;&#23450;$T$&#27493;&#30340;&#39044;&#31639;&#65292;&#20351;&#29992;&#31215;&#26497;&#30340;&#27493;&#38271;$\eta:= \Theta( T)$&#65292;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#21160;&#37327;&#25110;&#21464;&#27493;&#38271;&#35843;&#24230;&#22120;&#65292;GD&#21487;&#20197;&#23454;&#29616;&#19968;&#20010;$\tilde{\mathcal{O}}(1/T^2)$&#30340;&#21152;&#36895;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#25216;&#26415;&#22810;&#25165;&#22810;&#33402;&#65292;&#36824;&#21487;&#20197;&#22788;&#29702;&#19968;&#33324;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#65288;&#20854;&#20013;&#38656;&#35201;&#25351;&#25968;&#23614;&#37096;&#26469;&#23454;&#29616;$\tilde{\mathcal{O}}(1/T^2)$&#30340;&#21152;&#36895;&#65289;&#12289;&#31070;&#32463;&#20999;&#32447;&#26680;&#21306;&#22495;&#30340;&#38750;&#32447;&#24615;&#39044;&#27979;&#22120;&#65292;&#20197;&#21450;&#20855;&#26377;&#22823;&#27493;&#38271;&#30340;&#22312;&#32447;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15926v1 Announce Type: new  Abstract: We consider gradient descent (GD) with a constant stepsize applied to logistic regression with linearly separable data, where the constant stepsize $\eta$ is so large that the loss initially oscillates. We show that GD exits this initial oscillatory phase rapidly -- in $\mathcal{O}(\eta)$ steps -- and subsequently achieves an $\tilde{\mathcal{O}}(1 / (\eta t) )$ convergence rate after $t$ additional steps. Our results imply that, given a budget of $T$ steps, GD can achieve an accelerated loss of $\tilde{\mathcal{O}}(1/T^2)$ with an aggressive stepsize $\eta:= \Theta( T)$, without any use of momentum or variable stepsize schedulers. Our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\tilde{\mathcal{O}}(1/T^2)$ acceleration), nonlinear predictors in the neural tangent kernel regime, and online stochastic gradient descent (SGD) with a large stepsize, under sui
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;Bayesian&#32479;&#35745;&#23884;&#20837;&#21040;&#26356;&#24191;&#27867;&#30340;&#20915;&#31574;&#26694;&#26550;&#20013;&#65292;&#25552;&#20986;&#20102;&#32479;&#35745;&#28216;&#25103;&#20316;&#20026;&#32479;&#19968;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#39057;&#29575;&#27966;&#21644;&#36125;&#21494;&#26031;&#32479;&#35745;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#23567;&#21518;&#24724;&#20934;&#21017;&#20316;&#20026;&#20915;&#31574;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15892</link><description>&lt;p&gt;
&#32479;&#35745;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Statistical Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;Bayesian&#32479;&#35745;&#23884;&#20837;&#21040;&#26356;&#24191;&#27867;&#30340;&#20915;&#31574;&#26694;&#26550;&#20013;&#65292;&#25552;&#20986;&#20102;&#32479;&#35745;&#28216;&#25103;&#20316;&#20026;&#32479;&#19968;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#39057;&#29575;&#27966;&#21644;&#36125;&#21494;&#26031;&#32479;&#35745;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#23567;&#21518;&#24724;&#20934;&#21017;&#20316;&#20026;&#20915;&#31574;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23545;&#20960;&#31181;&#20856;&#22411;&#30340;&#28216;&#25103;&#36827;&#34892;&#20102;&#25968;&#23398;&#25506;&#32034;&#65292;&#20854;&#20013;&#33258;&#28982;&#28044;&#29616;&#20102;&#32479;&#35745;&#23398;&#21644;&#27010;&#29575;&#35770;&#30340;&#26680;&#24515;&#27010;&#24565;&#12290;&#36825;&#20123;&#28216;&#25103;&#21253;&#25324;&#36153;&#33293;&#23572;&#28216;&#25103;&#21644;&#36125;&#21494;&#26031;&#28216;&#25103;&#65292;&#23427;&#20204;&#20998;&#21035;&#19982;&#39057;&#29575;&#27966;&#32479;&#35745;&#23398;&#21644;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#30456;&#20851;&#12290;&#38543;&#21518;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#19968;&#33324;&#31867;&#22411;&#30340;&#28216;&#25103;&#65292;&#31216;&#20026;&#32479;&#35745;&#28216;&#25103;&#65292;&#22312;&#20854;&#20013;&#21487;&#20197;&#35774;&#32622;&#19968;&#20010;&#36827;&#19968;&#27493;&#30340;&#21442;&#25968;&#65292;&#21363;&#29609;&#23478;&#30340;&#30456;&#23545;&#39118;&#38505;&#21388;&#24694;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36153;&#33293;&#23572;&#28216;&#25103;&#21644;&#36125;&#21494;&#26031;&#28216;&#25103;&#21487;&#20197;&#34987;&#35270;&#20026;&#32479;&#35745;&#28216;&#25103;&#30340;&#26497;&#38480;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#32479;&#35745;&#28216;&#25103;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#39057;&#29575;&#27966;&#21644;&#36125;&#21494;&#26031;&#32479;&#35745;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21746;&#23398;&#26694;&#26550;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#26368;&#23567;&#21518;&#24724;&#20934;&#21017;&#65292;&#20316;&#20026;&#20915;&#31574;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15892v1 Announce Type: cross  Abstract: This work contains the mathematical exploration of a few prototypical games in which central concepts from statistics and probability theory naturally emerge. The first two kinds of games are termed Fisher and Bayesian games, which are connected to Frequentist and Bayesian statistics, respectively. Later, a more general type of game is introduced, termed Statistical game, in which a further parameter, the players' relative risk aversion, can be set. In this work, we show that Fisher and Bayesian games can be viewed as limiting cases of Statistical games. Therefore, Statistical games can be viewed as a unified framework, incorporating both Frequentist and Bayesian statistics. Furthermore, a philosophical framework is (re-)presented -- often referred to as minimax regret criterion -- as a general approach to decision making.   The main motivation for this work was to embed Bayesian statistics into a broader decision-making framework, whe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#32943;&#23450;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23558;&#19968;&#31181;&#24120;&#35265;&#30340;&#23433;&#20840;&#32422;&#26463;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15776</link><description>&lt;p&gt;
&#21463;&#38480;&#21046;MDP&#20013;&#30340;&#30495;&#27491;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Truly No-Regret Learning in Constrained MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#32943;&#23450;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23558;&#19968;&#31181;&#24120;&#35265;&#30340;&#23433;&#20840;&#32422;&#26463;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24314;&#27169;&#23433;&#20840;&#32422;&#26463;&#30340;&#24120;&#35265;&#26041;&#24335;&#12290;&#30446;&#21069;&#29992;&#20110;&#39640;&#25928;&#35299;&#20915;CMDPs&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#12290;&#23545;&#20110;&#36825;&#20123;&#31639;&#27861;&#65292;&#25152;&#26377;&#24403;&#21069;&#24050;&#30693;&#30340;&#21518;&#24724;&#30028;&#37117;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#8212;&#8212;&#21487;&#20197;&#36890;&#36807;&#22312;&#19968;&#20010;&#22238;&#21512;&#20013;&#30340;&#32422;&#26463;&#36829;&#21453;&#26469;&#29992;&#20005;&#26684;&#30340;&#32422;&#26463;&#28385;&#36275;&#22312;&#21478;&#19968;&#20010;&#22238;&#21512;&#20013;&#12290;&#36825;&#20351;&#24471;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#19981;&#23433;&#20840;&#65292;&#22240;&#20026;&#23427;&#20165;&#20445;&#35777;&#26368;&#32456;&#65288;&#28151;&#21512;&#65289;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#20445;&#35777;&#23433;&#20840;&#12290;&#27491;&#22914;Efroni&#31561;&#20154;&#65288;2020&#24180;&#65289;&#25351;&#20986;&#30340;&#65292;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#21487;&#35777;&#26126;&#22320;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#31532;&#19968;&#20010;&#32943;&#23450;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20851;&#20110;&#27491;&#21017;&#21270;&#21407;&#22987;-&#23545;&#20598;&#26041;&#26696;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#24615;&#36890;&#29992;&#21270;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#19978;&#12290;&#22522;&#20110;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21407;&#22987;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15776v1 Announce Type: new  Abstract: Constrained Markov decision processes (CMDPs) are a common way to model safety constraints in reinforcement learning. State-of-the-art methods for efficiently solving CMDPs are based on primal-dual algorithms. For these algorithms, all currently known regret bounds allow for error cancellations -- one can compensate for a constraint violation in one round with a strict constraint satisfaction in another. This makes the online learning process unsafe since it only guarantees safety for the final (mixture) policy but not during learning. As Efroni et al. (2020) pointed out, it is an open question whether primal-dual algorithms can provably achieve sublinear regret if we do not allow error cancellations. In this paper, we give the first affirmative answer. We first generalize a result on last-iterate convergence of regularized primal-dual schemes to CMDPs with multiple constraints. Building upon this insight, we propose a model-based primal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#20027;&#21160;&#22522;&#20110;&#20559;&#22909;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#25968;&#25454;&#26679;&#26412;&#26377;&#25928;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#26597;&#35810;&#29983;&#25104;&#26102;&#38388;&#30701;&#24182;&#21487;&#24182;&#34892;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15757</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Batch Active Learning of Reward Functions from Human Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#20027;&#21160;&#22522;&#20110;&#20559;&#22909;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#25968;&#25454;&#26679;&#26412;&#26377;&#25928;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#26597;&#35810;&#29983;&#25104;&#26102;&#38388;&#30701;&#24182;&#21487;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#29983;&#25104;&#21644;&#26631;&#35760;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24448;&#24448;&#25104;&#26412;&#39640;&#26114;&#12290;&#22522;&#20110;&#20559;&#22909;&#30340;&#23398;&#20064;&#26159;&#19968;&#20010;&#27010;&#24565;&#65292;&#36890;&#36807;&#21521;&#29992;&#25143;&#25552;&#20986;&#20559;&#22909;&#38382;&#39064;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#26631;&#35760;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#32452;&#26032;&#31639;&#27861;&#65292;&#25209;&#37327;&#20027;&#21160;&#22522;&#20110;&#20559;&#22909;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#25968;&#25454;&#26679;&#26412;&#26377;&#25928;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#21516;&#26102;&#20855;&#26377;&#36739;&#30701;&#30340;&#26597;&#35810;&#29983;&#25104;&#26102;&#38388;&#65292;&#24182;&#20445;&#25345;&#21487;&#24182;&#34892;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25209;&#37327;&#29983;&#25104;&#21644;&#20960;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#27169;&#25311;&#20013;&#20171;&#32461;&#20102;&#19968;&#20123;&#26426;&#22120;&#20154;&#23398;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#20165;&#38656;&#35201;&#23569;&#37327;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15757v1 Announce Type: cross  Abstract: Data generation and labeling are often expensive in robot learning. Preference-based learning is a concept that enables reliable labeling by querying users with preference questions. Active querying methods are commonly employed in preference-based learning to generate more informative data at the expense of parallelization and computation time. In this paper, we develop a set of novel algorithms, batch active preference-based learning methods, that enable efficient learning of reward functions using as few data samples as possible while still having short query generation times and also retaining parallelizability. We introduce a method based on determinantal point processes (DPP) for active batch generation and several heuristic-based alternatives. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that ar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#20302;&#31209;&#29615;&#22659;&#20013;&#20855;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#36172;&#24466;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#31574;&#30053;&#35780;&#20272;&#12289;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#24182;&#19988;&#22312;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#31574;&#30053;&#35780;&#20272;&#26041;&#38754;&#30340;&#31639;&#27861;&#20960;&#20046;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.15739</link><description>&lt;p&gt;
&#20302;&#31209;&#36172;&#24466;&#36890;&#36807;&#32039;&#32477;&#23545;&#21040;&#26080;&#31351;&#22855;&#24322;&#23376;&#31354;&#38388;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15739
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#20302;&#31209;&#29615;&#22659;&#20013;&#20855;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#36172;&#24466;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#31574;&#30053;&#35780;&#20272;&#12289;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#24182;&#19988;&#22312;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#31574;&#30053;&#35780;&#20272;&#26041;&#38754;&#30340;&#31639;&#27861;&#20960;&#20046;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#30340;&#24773;&#22659;&#36172;&#24466;&#38382;&#39064;&#65292;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#22914;&#26524;&#36873;&#25321;&#20102;(&#24773;&#22659;&#65292;&#21160;&#20316;)&#23545;$(i,j)\in [m]\times [n]$&#65292;&#23398;&#20064;&#32773;&#20250;&#35266;&#23519;&#19968;&#20010;&#26410;&#30693;&#20302;&#31209;&#22870;&#21169;&#30697;&#38453;&#30340;$(i,j)$-th&#20837;&#21475;&#30340;&#22024;&#26434;&#26679;&#26412;&#12290;&#36830;&#32493;&#30340;&#24773;&#22659;&#20197;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#26041;&#24335;&#38543;&#26426;&#29983;&#25104;&#24182;&#36879;&#38706;&#32473;&#23398;&#20064;&#32773;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#36172;&#24466;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#29992;&#20110;&#31574;&#30053;&#35780;&#20272;&#12289;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;&#23545;&#20110;&#31574;&#30053;&#35780;&#20272;&#21644;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20960;&#20046;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;&#20363;&#22914;&#65292;&#20026;&#20102;&#20197;&#33267;&#23569;$1-\delta$&#30340;&#27010;&#29575;&#36820;&#22238;&#19968;&#20010;$\varepsilon$-&#26368;&#20339;&#31574;&#30053;&#65292;&#36890;&#24120;&#38656;&#35201;&#30340;&#26679;&#26412;&#25968;&#22823;&#33268;&#25353;&#29031;${m+n\over \varepsilon^2}\log(1/\delta)$&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#20139;&#26377;&#30340;&#26497;&#23567;&#26497;&#22823;&#20445;&#35777;&#25353;&#29031;$r^{7/4}(m+n)^{3/4}\sqrt{T}$&#32553;&#25918;&#65292;&#36825;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;&#25152;&#26377;&#25552;&#20986;&#30340;&#31639;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15739v1 Announce Type: new  Abstract: We study contextual bandits with low-rank structure where, in each round, if the (context, arm) pair $(i,j)\in [m]\times [n]$ is selected, the learner observes a noisy sample of the $(i,j)$-th entry of an unknown low-rank reward matrix. Successive contexts are generated randomly in an i.i.d. manner and are revealed to the learner. For such bandits, we present efficient algorithms for policy evaluation, best policy identification and regret minimization. For policy evaluation and best policy identification, we show that our algorithms are nearly minimax optimal. For instance, the number of samples required to return an $\varepsilon$-optimal policy with probability at least $1-\delta$ typically scales as ${m+n\over \varepsilon^2}\log(1/\delta)$. Our regret minimization algorithm enjoys minimax guarantees scaling as $r^{7/4}(m+n)^{3/4}\sqrt{T}$, which improves over existing algorithms. All the proposed algorithms consist of two phases: they
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;PDE&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#22806;&#22495;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15734</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#39640;&#25928;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;PDE&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#22806;&#22495;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#35265;&#35777;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19982;&#29289;&#29702;&#39046;&#22495;&#29305;&#23450;&#27934;&#23519;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#31185;&#23398;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#23494;&#38598;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;PDE&#25968;&#25454;&#12290; &#36825;&#37325;&#26032;&#24341;&#20837;&#20102;&#23545;&#26114;&#36149;&#30340;&#25968;&#20540;PDE&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#65292;&#37096;&#20998;&#21066;&#24369;&#20102;&#36991;&#20813;&#36825;&#20123;&#26114;&#36149;&#27169;&#25311;&#30340;&#21407;&#22987;&#30446;&#26631;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#23547;&#27714;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#29992;&#20110;PDE&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#12290; &#20026;&#20102;&#20943;&#23569;&#23545;&#24102;&#26377;&#27169;&#25311;&#35299;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#37325;&#26500;&#30340;&#20195;&#29702;&#20219;&#21153;&#22312;&#26410;&#26631;&#35760;&#30340;PDE&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#36816;&#31639;&#31526;&#12290; &#20026;&#20102;&#25552;&#39640;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24110;&#21161;&#31070;&#32463;&#36816;&#31639;&#31526;&#28789;&#27963;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#25110;&#35774;&#35745;&#12290; &#22312;&#21508;&#31181;PD&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15734v1 Announce Type: new  Abstract: Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insight for solving scientific problems based on partial differential equations (PDEs). However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive simulations. In this work, seeking data efficiency, we design unsupervised pretraining and in-context learning methods for PDE operator learning. To reduce the need for training data with simulated solutions, we pretrain neural operators on unlabeled PDE data using reconstruction-based proxy tasks. To improve out-of-distribution performance, we further assist neural operators in flexibly leveraging in-context learning methods, without incurring extra training costs or designs. Extensive empirical evaluations on a diverse set of PD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22312;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;KRR&#21487;&#20197;&#36798;&#21040;&#26368;&#23567;&#21270;&#26368;&#20248;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#24449;&#20540;&#30340;&#34928;&#20943;&#21576;&#25351;&#25968;&#24555;&#36895;&#34928;&#20943;&#26102;&#65292;KRR&#23454;&#29616;&#20102;&#35889;&#31934;&#24230;&#12290;&#23545;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#20102;&#23545;&#20598;&#20998;&#26512;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#25193;&#23637;&#30340;&#23545;&#20598;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#36229;&#20986;&#26412;&#24037;&#20316;&#33539;&#22260;&#30340;&#26680;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15718</link><description>&lt;p&gt;
&#22312;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#23545;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#23545;&#20598;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Duality Analysis of Kernel Ridge Regression in the Noiseless Regime
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22312;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;KRR&#21487;&#20197;&#36798;&#21040;&#26368;&#23567;&#21270;&#26368;&#20248;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#24449;&#20540;&#30340;&#34928;&#20943;&#21576;&#25351;&#25968;&#24555;&#36895;&#34928;&#20943;&#26102;&#65292;KRR&#23454;&#29616;&#20102;&#35889;&#31934;&#24230;&#12290;&#23545;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#20102;&#23545;&#20598;&#20998;&#26512;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#25193;&#23637;&#30340;&#23545;&#20598;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#36229;&#20986;&#26412;&#24037;&#20316;&#33539;&#22260;&#30340;&#26680;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22312;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#30340;&#27867;&#21270;&#29305;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#36825;&#23545;&#20110;&#31185;&#23398;&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#25968;&#25454;&#32463;&#24120;&#26159;&#36890;&#36807;&#35745;&#31639;&#26426;&#27169;&#25311;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;KRR&#21487;&#20197;&#36798;&#21040;&#26368;&#23567;&#21270;&#26368;&#20248;&#29575;&#65292;&#36825;&#21462;&#20915;&#20110;&#30456;&#20851;&#26680;&#30340;&#29305;&#24449;&#20540;&#34928;&#20943;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#30456;&#23545;&#24179;&#28369;&#31243;&#24230;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;&#29305;&#24449;&#20540;&#30340;&#34928;&#20943;&#21576;&#25351;&#25968;&#24555;&#36895;&#34928;&#20943;&#26102;&#65292;KRR&#23454;&#29616;&#20102;&#35889;&#31934;&#24230;&#65292;&#21363;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#20219;&#20309;&#22810;&#39033;&#24335;&#12290;&#27492;&#22806;&#65292;&#25968;&#20540;&#23454;&#39564;&#24456;&#22909;&#22320;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#21033;&#29992;&#20102;&#38472;&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24341;&#20837;&#30340;&#23545;&#20598;&#26694;&#26550;&#30340;&#19968;&#31181;&#26032;&#22411;&#25193;&#23637;&#65292;&#36825;&#23545;&#20998;&#26512;&#36229;&#20986;&#26412;&#24037;&#20316;&#33539;&#22260;&#30340;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#21487;&#33021;&#24456;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15718v1 Announce Type: new  Abstract: In this paper, we conduct a comprehensive analysis of generalization properties of Kernel Ridge Regression (KRR) in the noiseless regime, a scenario crucial to scientific computing, where data are often generated via computer simulations. We prove that KRR can attain the minimax optimal rate, which depends on both the eigenvalue decay of the associated kernel and the relative smoothness of target functions. Particularly, when the eigenvalue decays exponentially fast, KRR achieves the spectral accuracy, i.e., a convergence rate faster than any polynomial. Moreover, the numerical experiments well corroborate our theoretical findings. Our proof leverages a novel extension of the duality framework introduced by Chen et al. (2023), which could be useful in analyzing kernel-based methods beyond the scope of this work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#32479;&#35745;&#20998;&#26512;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;Wasserstein&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#20869;&#22312;&#20302;&#32500;&#25968;&#25454;&#30340;&#29305;&#24615;&#19982;&#23616;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.15710</link><description>&lt;p&gt;
&#23545;&#20869;&#22312;&#20302;&#32500;&#25968;&#25454;&#30340;Wasserstein&#33258;&#32534;&#30721;&#22120;&#30340;&#32479;&#35745;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32479;&#35745;&#20998;&#26512;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;Wasserstein&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#20869;&#22312;&#20302;&#32500;&#25968;&#25454;&#30340;&#29305;&#24615;&#19982;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(Variational Autoencoders, VAEs)&#22312;&#30740;&#31350;&#20154;&#21592;&#20013;&#24191;&#21463;&#27426;&#36814;&#65292;&#34987;&#35748;&#20026;&#26159;&#29702;&#35299;&#22522;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#26410;&#30693;&#20998;&#24067;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#36825;&#31181;&#21463;&#27426;&#36814;&#31243;&#24230;&#37096;&#20998;&#28304;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#37096;&#20998;&#28304;&#20110;&#20854;&#33021;&#22815;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;Wasserstein&#33258;&#32534;&#30721;&#22120;(WAEs)&#26159;VAEs&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#26088;&#22312;&#19981;&#20165;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#32780;&#19988;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20854;&#32479;&#35745;&#20445;&#35777;&#30340;&#20998;&#26512;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#30001;&#20110;WAEs&#25152;&#24212;&#29992;&#30340;&#25968;&#25454;&#20998;&#24067;&#65288;&#20363;&#22914;&#33258;&#28982;&#22270;&#20687;&#65289;&#36890;&#24120;&#34987;&#35748;&#20026;&#22312;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20855;&#26377;&#20302;&#32500;&#32467;&#26500;&#65292;&#32780;&#24403;&#21069;&#30340;&#29702;&#35770;&#24182;&#26410;&#20805;&#20998;&#32771;&#34385;&#36825;&#19968;&#28857;&#65292;&#23548;&#33268;&#24050;&#30693;&#30340;&#30028;&#38480;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#24357;&#21512;WAEs&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;WAEs...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15710v1 Announce Type: new  Abstract: Variational Autoencoders (VAEs) have gained significant popularity among researchers as a powerful tool for understanding unknown distributions based on limited samples. This popularity stems partly from their impressive performance and partly from their ability to provide meaningful feature representations in the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to not only improve model efficiency but also interpretability. However, there has been limited focus on analyzing their statistical guarantees. The matter is further complicated by the fact that the data distributions to which WAEs are applied - such as natural images - are often presumed to possess an underlying low-dimensional structure within a high-dimensional feature space, which current theory does not adequately account for, rendering known bounds inefficient. To bridge the gap between the theory and practice of WAEs, in this paper, we show that WAEs 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#33021;&#25214;&#21040;&#19968;&#20010;&#19982;&#26368;&#20248;&#31574;&#30053;&#31454;&#20105;&#30340;&#38543;&#26426;&#31574;&#30053;&#65292;&#20026;&#22312;&#20165;&#26377;&#23569;&#37327;&#26679;&#26412;&#19979;&#36827;&#34892;&#21487;&#38752;&#20915;&#31574;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.15703</link><description>&lt;p&gt;
&#26159;&#21542;&#21487;&#20197;&#20165;&#20973;&#26377;&#38480;&#26679;&#26412;&#36827;&#34892;&#31163;&#32447;&#20915;&#31574;&#65311;&#36890;&#36807;&#20449;&#20219;&#21306;&#22495;&#22686;&#24378;&#22312;&#25968;&#25454;&#31232;&#32570;&#36172;&#21338;&#26426;&#20013;&#21487;&#38752;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Is Offline Decision Making Possible with Only Few Samples? Reliable Decisions in Data-Starved Bandits via Trust Region Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#33021;&#25214;&#21040;&#19968;&#20010;&#19982;&#26368;&#20248;&#31574;&#30053;&#31454;&#20105;&#30340;&#38543;&#26426;&#31574;&#30053;&#65292;&#20026;&#22312;&#20165;&#26377;&#23569;&#37327;&#26679;&#26412;&#19979;&#36827;&#34892;&#21487;&#38752;&#20915;&#31574;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#21482;&#21253;&#21547;&#27599;&#20010;&#33218;&#30340;&#21333;&#20010;&#26679;&#26412;&#25968;&#25454;&#38598;&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#33021;&#20174;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#38382;&#39064;&#20013;&#23398;&#21040;&#20160;&#20040;&#65311;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21363;&#20351;&#22312;&#36825;&#31181;&#25968;&#25454;&#31232;&#32570;&#30340;&#29615;&#22659;&#20013;&#65292;&#20173;&#28982;&#21487;&#33021;&#25214;&#21040;&#19968;&#20010;&#19982;&#26368;&#20248;&#31574;&#30053;&#31454;&#20105;&#30340;&#31574;&#30053;&#12290;&#36825;&#20026;&#22312;&#24517;&#39035;&#20165;&#20381;&#38752;&#23569;&#25968;&#26679;&#26412;&#20570;&#20986;&#20851;&#38190;&#20915;&#31574;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#21487;&#38752;&#30340;&#20915;&#31574;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;\emph{&#38543;&#26426;&#31574;&#30053;&#23545;&#20110;&#31163;&#32447;&#20915;&#31574;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#30830;&#23450;&#24615;&#31574;&#30053;}&#12290;&#19987;&#27880;&#20110;&#31163;&#32447;&#22810;&#33218;&#32769;&#34382;&#26426;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#20449;&#20219;&#21306;&#22495;&#30340;&#38543;&#26426;&#31574;&#30053;&#22686;&#24378;&#65288;TRUST&#65289;&#30340;&#31639;&#27861;&#65292;&#36825;&#19982;&#20027;&#23548;&#24615;&#20215;&#20540;&#20026;&#22522;&#30784;&#30340;&#36739;&#20302;&#32622;&#20449;&#19979;&#30028;&#26041;&#27861;&#26377;&#24456;&#22823;&#19981;&#21516;&#12290;&#20854;&#35774;&#35745;&#24471;&#30410;&#20110;&#23450;&#20301;&#35268;&#24459;&#12289;&#20020;&#30028;&#21322;&#24452;&#21644;&#30456;&#23545;&#24754;&#35266;&#20027;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;L&#30340;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15703v1 Announce Type: cross  Abstract: What can an agent learn in a stochastic Multi-Armed Bandit (MAB) problem from a dataset that contains just a single sample for each arm? Surprisingly, in this work, we demonstrate that even in such a data-starved setting it may still be possible to find a policy competitive with the optimal one. This paves the way to reliable decision-making in settings where critical decisions must be made by relying only on a handful of samples.   Our analysis reveals that \emph{stochastic policies can be substantially better} than deterministic ones for offline decision-making. Focusing on offline multi-armed bandits, we design an algorithm called Trust Region of Uncertainty for Stochastic policy enhancemenT (TRUST) which is quite different from the predominant value-based lower confidence bound approach. Its design is enabled by localization laws, critical radii, and relative pessimism. We prove that its sample complexity is comparable to that of L
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#20132;&#26799;&#24230;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#20419;&#36827;&#29983;&#25104;&#26356;&#21152;&#31616;&#21270;&#30340;&#21152;&#27861;&#35268;&#21017;&#38598;&#21512;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15691</link><description>&lt;p&gt;
&#27491;&#20132;&#26799;&#24230;&#25552;&#21319;&#29992;&#20110;&#31616;&#21270;&#30340;&#21152;&#27861;&#35268;&#21017;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15691
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#20132;&#26799;&#24230;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#20419;&#36827;&#29983;&#25104;&#26356;&#21152;&#31616;&#21270;&#30340;&#21152;&#27861;&#35268;&#21017;&#38598;&#21512;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#35268;&#21017;&#30340;&#26799;&#24230;&#25552;&#21319;&#26159;&#19968;&#31181;&#23398;&#20064;&#28508;&#22312;&#21487;&#35299;&#37322;&#19988;&#20934;&#30830;&#30340;&#27010;&#29575;&#27169;&#22411;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#21487;&#35299;&#37322;&#24615;&#38656;&#35201;&#38480;&#21046;&#29983;&#25104;&#30340;&#35268;&#21017;&#25968;&#37327;&#21644;&#22823;&#23567;&#65292;&#29616;&#26377;&#30340;&#25552;&#21319;&#21464;&#20307;&#24182;&#38750;&#20026;&#27492;&#30446;&#30340;&#32780;&#35774;&#35745;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#30446;&#26631;&#20989;&#25968;&#34913;&#37327;&#20102;&#39118;&#38505;&#26799;&#24230;&#21521;&#37327;&#19982;&#26465;&#20214;&#36755;&#20986;&#21521;&#37327;&#22312;&#24050;&#36873;&#25321;&#26465;&#20214;&#30340;&#27491;&#20132;&#34917;&#19978;&#30340;&#25237;&#24433;&#30340;&#22841;&#35282;&#65292;&#20174;&#32780;&#27491;&#30830;&#36924;&#36817;&#23558;&#39118;&#38505;&#26799;&#24230;&#26412;&#36523;&#28155;&#21152;&#21040;&#27169;&#22411;&#30340;&#29702;&#24819;&#26356;&#26032;&#65292;&#24182;&#20542;&#21521;&#20110;&#21253;&#25324;&#26356;&#19968;&#33324;&#19988;&#26356;&#30701;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15691v1 Announce Type: new  Abstract: Gradient boosting of prediction rules is an efficient approach to learn potentially interpretable yet accurate probabilistic models. However, actual interpretability requires to limit the number and size of the generated rules, and existing boosting variants are not designed for this purpose. Though corrective boosting refits all rule weights in each iteration to minimise prediction risk, the included rule conditions tend to be sub-optimal, because commonly used objective functions fail to anticipate this refitting. Here, we address this issue by a new objective function that measures the angle between the risk gradient vector and the projection of the condition output vector onto the orthogonal complement of the already selected conditions. This approach correctly approximate the ideal update of adding the risk gradient itself to the model and favours the inclusion of more general and thus shorter rules. As we demonstrate using a wide r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#26001;&#28857;&#22122;&#22768;&#23384;&#22312;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#35065;&#34955;&#24335;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#65288;Bagged-DIP&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#19982;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#38598;&#25104;&#65292;&#20197;&#21450;&#36890;&#36807;&#22312;&#36845;&#20195;&#20013;&#20351;&#29992;Newton-Schulz&#31639;&#27861;&#26469;&#20943;&#23569;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.15635</link><description>&lt;p&gt;
&#29992;&#20110;&#22312;&#26001;&#28857;&#22122;&#22768;&#23384;&#22312;&#24773;&#20917;&#19979;&#24674;&#22797;&#22270;&#20687;&#30340;&#35065;&#34955;&#24335;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Bagged Deep Image Prior for Recovering Images in the Presence of Speckle Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15635
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#26001;&#28857;&#22122;&#22768;&#23384;&#22312;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#35065;&#34955;&#24335;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#65288;Bagged-DIP&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#19982;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#38598;&#25104;&#65292;&#20197;&#21450;&#36890;&#36807;&#22312;&#36845;&#20195;&#20013;&#20351;&#29992;Newton-Schulz&#31639;&#27861;&#26469;&#20943;&#23569;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22522;&#20110;&#20284;&#28982;&#30340;&#26041;&#27861;&#22312;&#26001;&#28857;&#65288;&#20056;&#24615;&#65289;&#22122;&#22768;&#24433;&#21709;&#19979;&#20174;&#22810;&#32452;&#27979;&#37327;&#20013;&#24674;&#22797;&#22797;&#26434;&#20540;&#20449;&#21495;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#21253;&#25324;&#24314;&#31435;&#22312;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#20551;&#35774;&#19979;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#31532;&#19968;&#20010;&#29702;&#35770;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#25429;&#25417;&#20102;MSE&#19982;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;&#35266;&#27979;&#27425;&#25968;&#12289;&#20449;&#21495;&#32500;&#24230;&#21644;&#27599;&#27425;&#35266;&#27979;&#30340;&#27979;&#37327;&#27425;&#25968;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#31639;&#27861;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35065;&#34955;&#24335;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#65288;Bagged-DIP&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#19982;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#38598;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;PGD&#30340;&#36845;&#20195;&#20013;&#20351;&#29992;Newton-Schulz&#31639;&#27861;&#35745;&#31639;&#30697;&#38453;&#36870;&#65292;&#20174;&#32780;&#38477;&#20302;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15635v1 Announce Type: cross  Abstract: We investigate both the theoretical and algorithmic aspects of likelihood-based methods for recovering a complex-valued signal from multiple sets of measurements, referred to as looks, affected by speckle (multiplicative) noise. Our theoretical contributions include establishing the first existing theoretical upper bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under the deep image prior hypothesis. Our theoretical results capture the dependence of MSE upon the number of parameters in the deep image prior, the number of looks, the signal dimension, and the number of measurements per look. On the algorithmic side, we introduce the concept of bagged Deep Image Priors (Bagged-DIP) and integrate them with projected gradient descent. Furthermore, we show how employing Newton-Schulz algorithm for calculating matrix inverses within the iterations of PGD reduces the computational complexity of the algorithm. We will 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MissNODAGS&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#24490;&#29615;&#22240;&#26524;&#22270;&#65292;&#36890;&#36807;&#20132;&#26367;&#26367;&#34917;&#32570;&#22833;&#25968;&#25454;&#21644;&#26368;&#22823;&#21270;&#21487;&#35265;&#25968;&#25454;&#37096;&#20998;&#30340;&#39044;&#26399;&#23545;&#25968;&#20284;&#28982;&#26469;&#23398;&#20064;&#22240;&#26524;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.15625</link><description>&lt;p&gt;
&#20174;&#19981;&#23436;&#25972;&#25968;&#25454;&#20013;&#23398;&#20064;&#24490;&#29615;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Cyclic Causal Models from Incomplete Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15625
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MissNODAGS&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#24490;&#29615;&#22240;&#26524;&#22270;&#65292;&#36890;&#36807;&#20132;&#26367;&#26367;&#34917;&#32570;&#22833;&#25968;&#25454;&#21644;&#26368;&#22823;&#21270;&#21487;&#35265;&#25968;&#25454;&#37096;&#20998;&#30340;&#39044;&#26399;&#23545;&#25968;&#20284;&#28982;&#26469;&#23398;&#20064;&#22240;&#26524;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#23398;&#20064;&#26159;&#32479;&#35745;&#23398;&#21644;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#26410;&#35265;&#27835;&#30103;&#23545;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#37117;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#20551;&#35774;&#65306;(i) &#28508;&#22312;&#22270;&#26159;&#26080;&#29615;&#30340;&#65292;(ii) &#21487;&#29992;&#25968;&#25454;&#26159;&#23436;&#25972;&#30340;&#12290;&#36825;&#20123;&#20551;&#35774;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#31995;&#32479;&#21253;&#21547;&#21453;&#39304;&#29615;&#36335;&#65288;&#20363;&#22914;&#29983;&#29289;&#31995;&#32479;&#65289;&#65292;&#23454;&#38469;&#24773;&#20917;&#32463;&#24120;&#28041;&#21450;&#32570;&#22833;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MissNODAGS&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#24490;&#29615;&#22240;&#26524;&#22270;&#12290;&#22312;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#19979;&#65292;MissNODAGS&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#22312;&#26367;&#34917;&#32570;&#22833;&#25968;&#25454;&#19982;&#26368;&#22823;&#21270;&#25968;&#25454;&#21487;&#35265;&#37096;&#20998;&#30340;&#39044;&#26399;&#23545;&#25968;&#20284;&#28982;&#20043;&#38388;&#20132;&#26367;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#36981;&#24490;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26694;&#26550;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15625v1 Announce Type: cross  Abstract: Causal learning is a fundamental problem in statistics and science, offering insights into predicting the effects of unseen treatments on a system. Despite recent advances in this topic, most existing causal discovery algorithms operate under two key assumptions: (i) the underlying graph is acyclic, and (ii) the available data is complete. These assumptions can be problematic as many real-world systems contain feedback loops (e.g., biological systems), and practical scenarios frequently involve missing data. In this work, we propose a novel framework, named MissNODAGS, for learning cyclic causal graphs from partially missing data. Under the additive noise model, MissNODAGS learns the causal graph by alternating between imputing the missing data and maximizing the expected log-likelihood of the visible part of the data in each training step, following the principles of the expectation-maximization (EM) framework. Through synthetic exper
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#19982;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#20108;&#20803;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#25216;&#26415;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#24341;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#38544;&#31169;&#24615;&#33021;&#21644;&#25928;&#29992;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.15603</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20844;&#24179;&#20108;&#20803;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Fair Binary Classifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#19982;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#20108;&#20803;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#25216;&#26415;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#24341;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#38544;&#31169;&#24615;&#33021;&#21644;&#25928;&#29992;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#20108;&#20803;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#32806;&#25216;&#26415;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#20165;&#20855;&#26377;&#20844;&#24179;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#12290;&#35813;&#31639;&#27861;&#25509;&#21463;&#38024;&#23545;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#28385;&#36275;&#32479;&#35745;&#24179;&#34913;&#30340;&#21333;&#19968;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#35813;&#31639;&#27861;&#20197;&#32435;&#20837;&#24046;&#20998;&#38544;&#31169;&#12290;&#26368;&#32456;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20445;&#35777;&#26041;&#38754;&#24471;&#21040;&#20102;&#20005;&#26684;&#26816;&#39564;&#12290;&#23545;Adult&#21644;&#20449;&#29992;&#21345;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20844;&#24179;&#24615;&#20445;&#35777;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30456;&#21516;&#27700;&#24179;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15603v1 Announce Type: new  Abstract: In this work, we investigate binary classification under the constraints of both differential privacy and fairness. We first propose an algorithm based on the decoupling technique for learning a classifier with only fairness guarantee. This algorithm takes in classifiers trained on different demographic groups and generates a single classifier satisfying statistical parity. We then refine this algorithm to incorporate differential privacy. The performance of the final algorithm is rigorously examined in terms of privacy, fairness, and utility guarantees. Empirical evaluations conducted on the Adult and Credit Card datasets illustrate that our algorithm outperforms the state-of-the-art in terms of fairness guarantees, while maintaining the same level of privacy and utility.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#20855;&#26377;&#26497;&#23567;&#22343;&#26041;&#35823;&#24046;&#65292;&#21487;&#20197;&#33719;&#24471;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#31361;&#30772;&#20102;&#20165;&#20570;&#27425;&#39640;&#26031;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.15602</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26497;&#23567;&#21270;&#26368;&#20248;&#24615;&#65306;&#36229;&#36234;&#23494;&#24230;&#19979;&#30028;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Minimax Optimality of Score-based Diffusion Models: Beyond the Density Lower Bound Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#20855;&#26377;&#26497;&#23567;&#22343;&#26041;&#35823;&#24046;&#65292;&#21487;&#20197;&#33719;&#24471;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#31361;&#30772;&#20102;&#20165;&#20570;&#27425;&#39640;&#26031;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#38750;&#21442;&#25968;&#32479;&#35745;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#22823;&#26679;&#26412;&#22330;&#26223;&#19979;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#25277;&#26679;&#30340;&#28176;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#26680;&#30340;&#24471;&#20998;&#20272;&#35745;&#22120;&#21487;&#20197;&#23454;&#29616;&#23545; $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$ &#30340;&#24471;&#20998;&#20989;&#25968;&#30340;&#26368;&#20248;&#22343;&#26041;&#35823;&#24046;&#20026; $\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$&#65292;&#20854;&#20013; $n$ &#21644; $d$ &#20998;&#21035;&#20195;&#34920;&#26679;&#26412;&#22823;&#23567;&#21644;&#32500;&#24230;&#65292;$t$ &#22312;&#19978;&#19979;&#21463;&#21040; $n$ &#30340;&#22810;&#39033;&#24335;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988; $p_0$ &#26159;&#20219;&#24847;&#27425;&#20122;&#39640;&#26031;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#36825;&#23548;&#33268;&#22312;&#20165;&#36827;&#34892;&#27425;&#39640;&#26031;&#20551;&#35774;&#26102;&#65292;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#20026; $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$&#12290;&#22914;&#26524;&#27492;&#22806;&#65292;$p_0$ &#23646;&#20110; $\beta\le 2$ &#30340; $\beta$-Sobolev&#31354;&#38388;&#30340;&#38750;&#21442;&#25968;&#26063;&#65292;&#36890;&#36807;&#37319;&#29992;&#26089;&#20572;&#31574;&#30053;&#65292;&#25105;&#20204;&#24471;&#21040;&#35813;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#30340;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#20026; $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15602v1 Announce Type: cross  Abstract: We study the asymptotic error of score-based diffusion model sampling in large-sample scenarios from a non-parametric statistics perspective. We show that a kernel-based score estimator achieves an optimal mean square error of $\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$ for the score function of $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$, where $n$ and $d$ represent the sample size and the dimension, $t$ is bounded above and below by polynomials of $n$, and $p_0$ is an arbitrary sub-Gaussian distribution. As a consequence, this yields an $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$ upper bound for the total variation error of the distribution of the sample generated by the diffusion model under a mere sub-Gaussian assumption. If in addition, $p_0$ belongs to the nonparametric family of the $\beta$-Sobolev space with $\beta\le 2$, by adopting an early stopping strategy, we obtain that the diffusion
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#25239;&#22122;&#22768;&#30340;&#24418;&#29366;&#24314;&#27169;&#30740;&#31350;&#20013;&#30340;&#24418;&#29366;&#21435;&#22122;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20845;&#31181;&#25200;&#21160;&#24418;&#29366;&#30340;&#22122;&#22768;&#31867;&#22411;&#20197;&#21450;&#29992;&#20110;&#27604;&#36739;&#26041;&#27861;&#22312;&#24418;&#29366;&#21435;&#22122;&#33021;&#21147;&#19978;&#30340;&#23458;&#35266;&#24230;&#37327;&#65292;&#35780;&#20272;&#20102;&#21253;&#25324;&#19968;&#20123;&#29983;&#25104;&#27169;&#22411;&#22312;&#20869;&#30340;&#19971;&#31181;&#33021;&#22815;&#23436;&#25104;&#24418;&#29366;&#21435;&#22122;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15587</link><description>&lt;p&gt;
&#23545;&#25239;&#22122;&#22768;&#30340;&#24418;&#29366;&#24314;&#27169;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Shape Modeling Against Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#25239;&#22122;&#22768;&#30340;&#24418;&#29366;&#24314;&#27169;&#30740;&#31350;&#20013;&#30340;&#24418;&#29366;&#21435;&#22122;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20845;&#31181;&#25200;&#21160;&#24418;&#29366;&#30340;&#22122;&#22768;&#31867;&#22411;&#20197;&#21450;&#29992;&#20110;&#27604;&#36739;&#26041;&#27861;&#22312;&#24418;&#29366;&#21435;&#22122;&#33021;&#21147;&#19978;&#30340;&#23458;&#35266;&#24230;&#37327;&#65292;&#35780;&#20272;&#20102;&#21253;&#25324;&#19968;&#20123;&#29983;&#25104;&#27169;&#22411;&#22312;&#20869;&#30340;&#19971;&#31181;&#33021;&#22815;&#23436;&#25104;&#24418;&#29366;&#21435;&#22122;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#24314;&#27169;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#21307;&#23398;&#24433;&#20687;&#31561;&#39046;&#22495;&#26377;&#35768;&#22810;&#28508;&#22312;&#30340;&#24212;&#29992;&#12290;&#25991;&#29486;&#20013;&#23384;&#22312;&#35768;&#22810;&#24418;&#29366;&#24314;&#27169;&#26041;&#27861;&#65292;&#27599;&#31181;&#26041;&#27861;&#37117;&#20855;&#26377;&#20854;&#20248;&#28857;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24418;&#29366;&#24314;&#27169;&#26041;&#27861;&#22312;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#37096;&#20998;&#25110;&#24322;&#24120;&#20540;&#30340;&#24418;&#29366;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#24418;&#29366;&#21435;&#22122;&#65292;&#36825;&#26159;&#24418;&#29366;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#20301;&#20110;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#21307;&#23398;&#24433;&#20687;&#24212;&#29992;&#30340;&#26680;&#24515;&#20301;&#32622;&#65292;&#22312;&#25991;&#29486;&#20013;&#26410;&#33021;&#33719;&#24471;&#36275;&#22815;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#20197;&#29992;&#26469;&#25200;&#21160;&#24418;&#29366;&#30340;&#20845;&#31181;&#22122;&#22768;&#31867;&#22411;&#65292;&#20197;&#21450;&#22122;&#22768;&#27700;&#24179;&#30340;&#23458;&#35266;&#24230;&#37327;&#21644;&#29992;&#20110;&#27604;&#36739;&#26041;&#27861;&#22312;&#24418;&#29366;&#21435;&#22122;&#33021;&#21147;&#26041;&#38754;&#30340;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35780;&#20272;&#20102;&#19971;&#31181;&#33021;&#22815;&#23436;&#25104;&#27492;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20845;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#21253;&#25324;&#19968;&#20123;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15587v1 Announce Type: cross  Abstract: Shape modeling is a challenging task with many potential applications in computer vision and medical imaging. There are many shape modeling methods in the literature, each with its advantages and applications. However, many shape modeling methods have difficulties handling shapes that have missing pieces or outliers. In this regard, this paper introduces shape denoising, a fundamental problem in shape modeling that lies at the core of many computer vision and medical imaging applications and has not received enough attention in the literature. The paper introduces six types of noise that can be used to perturb shapes as well as an objective measure for the noise level and for comparing methods on their shape denoising capabilities. Finally, the paper evaluates seven methods capable of accomplishing this task, of which six are based on deep learning, including some generative models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#19978;&#28216;&#21644;&#19979;&#28216;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#30340;&#19968;&#27493;&#31574;&#30053;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20559;&#35823;&#65292;&#22312;CEO&#26102;&#38388;&#21033;&#29992;&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#20135;&#29983;&#20102;&#37325;&#35201;&#25928;&#26524;&#65292;&#36866;&#21512;&#24212;&#29992;&#30740;&#31350;&#20154;&#21592;&#12290;</title><link>https://arxiv.org/abs/2402.15585</link><description>&lt;p&gt;
&#20351;&#29992;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#30340;&#21464;&#37327;&#36827;&#34892;&#22238;&#24402;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference for Regression with Variables Generated from Unstructured Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15585
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#19978;&#28216;&#21644;&#19979;&#28216;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#30340;&#19968;&#27493;&#31574;&#30053;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20559;&#35823;&#65292;&#22312;CEO&#26102;&#38388;&#21033;&#29992;&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#20135;&#29983;&#20102;&#37325;&#35201;&#25928;&#26524;&#65292;&#36866;&#21512;&#24212;&#29992;&#30740;&#31350;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20027;&#35201;&#31574;&#30053;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#19978;&#28216;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#20272;&#35745;&#24863;&#20852;&#36259;&#30340;&#28508;&#22312;&#32463;&#27982;&#21464;&#37327;&#12290;&#20854;&#27425;&#65292;&#23558;&#20272;&#35745;&#20540;&#35270;&#20026;&#19979;&#28216;&#35745;&#37327;&#32463;&#27982;&#27169;&#22411;&#20013;&#30340;&#8220;&#25968;&#25454;&#8221;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29702;&#35770;&#35770;&#28857;&#65292;&#35299;&#37322;&#20026;&#20160;&#20040;&#22312;&#23454;&#35777;&#21512;&#29702;&#30340;&#35774;&#32622;&#20013;&#65292;&#36825;&#31181;&#20004;&#27493;&#31574;&#30053;&#20250;&#23548;&#33268;&#20559;&#35823;&#30340;&#25512;&#26029;&#12290;&#26356;&#20855;&#24314;&#35774;&#24615;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#25512;&#26029;&#30340;&#19968;&#27493;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21516;&#26102;&#20351;&#29992;&#19978;&#28216;&#21644;&#19979;&#28216;&#27169;&#22411;&#12290;&#22312;&#27169;&#25311;&#20013;&#65292;&#36825;&#19968;&#27493;&#31574;&#30053;(i) &#26174;&#33879;&#20943;&#23569;&#20102;&#20559;&#35823;&#65307;(ii) &#22312;&#20351;&#29992;CEO&#26102;&#38388;&#21033;&#29992;&#25968;&#25454;&#30340;&#20027;&#35201;&#24212;&#29992;&#20013;&#20135;&#29983;&#20102;&#23450;&#37327;&#37325;&#35201;&#30340;&#25928;&#26524;&#65307;(iii) &#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#34987;&#24212;&#29992;&#30740;&#31350;&#20154;&#21592;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15585v1 Announce Type: new  Abstract: The leading strategy for analyzing unstructured data uses two steps. First, latent variables of economic interest are estimated with an upstream information retrieval model. Second, the estimates are treated as "data" in a downstream econometric model. We establish theoretical arguments for why this two-step strategy leads to biased inference in empirically plausible settings. More constructively, we propose a one-step strategy for valid inference that uses the upstream and downstream models jointly. The one-step strategy (i) substantially reduces bias in simulations; (ii) has quantitatively important effects in a leading application using CEO time-use data; and (iii) can be readily adapted by applied researchers.
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#31995;&#32479;&#22312;&#19968;&#33324;&#35201;&#27714;&#19979;&#36981;&#24490;&#19968;&#31181;&#25200;&#20081;&#29256;&#26412;&#30340;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#65292;&#23398;&#20064;&#36807;&#31243;&#21463;&#21040;&#37327;&#23376;&#31995;&#32479;&#33258;&#32452;&#32455;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.14423</link><description>&lt;p&gt;
&#23431;&#23449;&#20316;&#20026;&#19968;&#20010;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The Universe as a Learning System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14423
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31995;&#32479;&#22312;&#19968;&#33324;&#35201;&#27714;&#19979;&#36981;&#24490;&#19968;&#31181;&#25200;&#20081;&#29256;&#26412;&#30340;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#65292;&#23398;&#20064;&#36807;&#31243;&#21463;&#21040;&#37327;&#23376;&#31995;&#32479;&#33258;&#32452;&#32455;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20854;&#24494;&#35266;&#27700;&#24179;&#19978;&#65292;&#23431;&#23449;&#36981;&#24490;&#37327;&#23376;&#21147;&#23398;&#23450;&#24459;&#12290;&#36890;&#36807;&#20851;&#27880;&#20174;&#37327;&#23376;&#21147;&#23398;&#30340;&#27969;&#20307;&#21147;&#23398;&#34920;&#36848;&#20013;&#36319;&#38543;&#30340;&#31890;&#23376;&#30340;&#37327;&#23376;&#36712;&#36857;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#19968;&#33324;&#35201;&#27714;&#19979;&#65292;&#37327;&#23376;&#31995;&#32479;&#36981;&#24490;&#19968;&#31181;&#25200;&#20081;&#29256;&#26412;&#30340;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#20854;&#20013;&#23398;&#20064;&#30001;&#20110;&#37327;&#23376;&#31995;&#32479;&#30340;&#33258;&#32452;&#32455;&#36807;&#31243;&#32780;&#22833;&#30495;&#12290;&#24403;&#25105;&#20204;&#20551;&#35774;&#32791;&#25955;&#21363;&#37327;&#23376;&#31995;&#32479;&#26159;&#24320;&#25918;&#30340;&#26102;&#65292;&#36825;&#26679;&#30340;&#23398;&#20064;&#36807;&#31243;&#25165;&#26377;&#21487;&#33021;&#12290;&#23398;&#20064;&#21442;&#25968;&#26159;&#36807;&#31243;&#30340;&#26102;&#38388;&#22686;&#37327;&#38500;&#20197;&#37327;&#23376;&#31890;&#23376;&#30340;&#36136;&#37327;&#65292;&#19968;&#20010;&#25705;&#25830;&#21442;&#25968;&#30830;&#23450;&#20102;&#37327;&#23376;&#31995;&#32479;&#30340;&#38750;&#32447;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#23454;&#35777;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14423v1 Announce Type: cross  Abstract: At its microscopic level, the universe follows the laws of quantum mechanics. Focusing on the quantum trajectories of particles as followed from the hydrodynamical formulation of quantum mechanics, we propose that under general requirements, quantum systems follow a disrupted version of the gradient descent model, a basic machine learning algorithm, where the learning is distorted due to the self-organizing process of the quantum system. Such a learning process is possible only when we assume dissipation, i.e., that the quantum system is open. The learning parameter is the time increment of the process over the mass of the quantum particle, and a friction parameter determines the nonlinearity of the quantum system. We then provide an empirical demonstration of the proposed model.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#23567;&#23454;&#20363;&#19978;&#20445;&#35777;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#20063;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#20934;&#30830;&#24230;&#30340;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.14332</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#21040;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#30340;&#23610;&#23544;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Large to Small Datasets: Size Generalization for Clustering Algorithm Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14332
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#23567;&#23454;&#20363;&#19978;&#20445;&#35777;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#20063;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#20934;&#30830;&#24230;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#20013;&#65292;&#25105;&#20204;&#20250;&#24471;&#21040;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#35201;&#26377;&#25928;&#22320;&#36873;&#25321;&#35201;&#20351;&#29992;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26377;&#19968;&#20010;&#26410;&#30693;&#30340;&#22522;&#20934;&#32858;&#31867;&#65292;&#25105;&#20204;&#21482;&#33021;&#36890;&#36807;&#26114;&#36149;&#30340;oracle&#26597;&#35810;&#26469;&#35775;&#38382;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#32858;&#31867;&#31639;&#27861;&#30340;&#36755;&#20986;&#23558;&#19982;&#22522;&#26412;&#20107;&#23454;&#32467;&#26500;&#19978;&#25509;&#36817;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#32858;&#31867;&#31639;&#27861;&#20934;&#30830;&#24615;&#30340;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30830;&#23450;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#25105;&#20204;&#21487;&#20197;&#65288;1&#65289;&#23545;&#22823;&#35268;&#27169;&#32858;&#31867;&#23454;&#20363;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#65288;2&#65289;&#22312;&#36739;&#23567;&#23454;&#20363;&#19978;&#35780;&#20272;&#19968;&#32452;&#20505;&#36873;&#31639;&#27861;&#65292;&#65288;3&#65289;&#20445;&#35777;&#22312;&#23567;&#23454;&#20363;&#19978;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#20026;&#19977;&#31181;&#32463;&#20856;&#32858;&#31867;&#31639;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#23610;&#23544;&#27867;&#21270;&#20445;&#35777;&#65306;&#21333;&#38142;&#25509;&#12289;k-means++&#21644;Gonzalez&#30340;k&#20013;&#24515;&#21551;&#21457;&#24335;&#65288;&#19968;&#31181;&#24179;&#28369;&#30340;&#21464;&#31181;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14332v1 Announce Type: new  Abstract: In clustering algorithm selection, we are given a massive dataset and must efficiently select which clustering algorithm to use. We study this problem in a semi-supervised setting, with an unknown ground-truth clustering that we can only access through expensive oracle queries. Ideally, the clustering algorithm's output will be structurally close to the ground truth. We approach this problem by introducing a notion of size generalization for clustering algorithm accuracy. We identify conditions under which we can (1) subsample the massive clustering instance, (2) evaluate a set of candidate algorithms on the smaller instance, and (3) guarantee that the algorithm with the best accuracy on the small instance will have the best accuracy on the original big instance. We provide theoretical size generalization guarantees for three classic clustering algorithms: single-linkage, k-means++, and (a smoothed variant of) Gonzalez's k-centers heuris
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#21338;&#24328;&#20013;&#36827;&#34892;&#26080;&#36951;&#25022;&#23398;&#20064;&#30340;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#31639;&#27861;&#19982;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.09456</link><description>&lt;p&gt;
&#26410;&#30693;&#21338;&#24328;&#20013;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#29992;&#20110;&#26080;&#36951;&#25022;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimistic Thompson Sampling for No-Regret Learning in Unknown Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#21338;&#24328;&#20013;&#36827;&#34892;&#26080;&#36951;&#25022;&#23398;&#20064;&#30340;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#31639;&#27861;&#19982;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28041;&#21450;&#22810;&#20010;&#20915;&#31574;&#32773;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#21487;&#20197;&#24314;&#27169;&#20026;&#19968;&#20010;&#20855;&#26377;&#37096;&#20998;&#35266;&#27979;&#30340;&#26410;&#30693;&#21338;&#24328;&#12290;&#20026;&#20102;&#35299;&#20915;&#37096;&#20998;&#20449;&#24687;&#21644;&#22810;&#26426;&#26500;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#27748;&#26222;&#26862;&#25277;&#26679;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22914;&#20132;&#36890;&#36335;&#30001;&#21644;&#38647;&#36798;&#24863;&#30693;&#20013;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#19982;&#22522;&#20934;&#31639;&#27861;&#30456;&#27604;&#65292;&#20943;&#23569;&#20102;&#21313;&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23545;&#22870;&#21169;&#32467;&#26500;&#26377;&#19968;&#23450;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36951;&#25022;&#30028;&#38480;&#20165;&#23545;&#24635;&#34892;&#21160;&#31354;&#38388;&#22823;&#23567;&#21576;&#23545;&#25968;&#20381;&#36182;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21644;&#39046;&#22495;&#20869;&#29616;&#26377;&#30340;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#26159;&#19968;&#39033;&#26032;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09456v1 Announce Type: cross  Abstract: Many real-world problems involving multiple decision-makers can be modeled as an unknown game characterized by partial observations. Addressing the challenges posed by partial information and the curse of multi-agency, we developed Thompson sampling-type algorithms, leveraging information about opponent's action and reward structures. Our approach significantly reduces experimental budgets, achieving a more than tenfold reduction compared to baseline algorithms in practical applications like traffic routing and radar sensing. We demonstrate that, under certain assumptions about the reward structure, the regret bound exhibits merely a logarithmic dependence on the total action space size, effectively mitigating the curse of multi-agency. Additionally, this research introduces the Optimism-then-NoRegret framework, a novel contribution that integrates both our proposed methodologies and existing algorithms in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.04875</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Provable Length and Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#26356;&#38271;&#24207;&#21015;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;&#32452;&#21512;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#20196;&#29260;&#32452;&#21512;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#26159;&#37325;&#35201;&#30340;&#38750;&#20998;&#24067;&#21270;&#27867;&#21270;&#24418;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#26550;&#26500;&#20013;&#65292;&#26397;&#30528;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#26681;&#25454;&#26550;&#26500;&#30340;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#30340;&#24517;&#35201;&#24615;&#65292;&#20363;&#22914;&#19982;&#30495;&#23454;&#34920;&#31034;&#20855;&#26377;&#32447;&#24615;&#25110;&#25490;&#21015;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;&#31639;&#27861;&#65288;D-SOBA&#65289;&#65292;&#39318;&#27425;&#38416;&#26126;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#22312;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.03167</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;: &#26080;&#29615;&#31639;&#27861;&#26356;&#26032;&#21644;&#30636;&#24577;&#36845;&#20195;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;&#31639;&#27861;&#65288;D-SOBA&#65289;&#65292;&#39318;&#27425;&#38416;&#26126;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#22312;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21452;&#32423;&#20248;&#21270;&#65288;SBO&#65289;&#22312;&#22788;&#29702;&#23884;&#22871;&#32467;&#26500;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#20351;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;SBO&#65292;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#20316;&#20026;&#26377;&#25928;&#30340;&#33539;&#20363;&#20986;&#29616;&#65292;&#20854;&#20013;&#33410;&#28857;&#19982;&#30452;&#25509;&#30456;&#37051;&#33410;&#28857;&#36827;&#34892;&#36890;&#20449;&#65292;&#26080;&#38656;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#21644;&#22686;&#24378;&#31639;&#27861;&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21435;&#20013;&#24515;&#21270;SBO&#31639;&#27861;&#38754;&#20020;&#25361;&#25112;&#65292;&#21253;&#25324;&#26114;&#36149;&#30340;&#20869;&#37096;&#24490;&#29615;&#26356;&#26032;&#21644;&#23545;&#32593;&#32476;&#25299;&#25169;&#12289;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#23884;&#22871;&#21452;&#32423;&#31639;&#27861;&#32467;&#26500;&#30340;&#24433;&#21709;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;SBO&#65288;D-SOBA&#65289;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#30636;&#24577;&#36845;&#20195;&#22797;&#26434;&#24615;&#65292;&#39318;&#27425;&#28548;&#28165;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. However, current decentralized SBO algorithms face challenges, including expensive inner-loop updates and unclear understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. In this paper, we introduce a single-loop decentralized SBO (D-SOBA) algorithm and establish its transient iteration complexity, which, for the first time, clarifies the joint influence of network topology and data heterogeneity on decentralized bilevel algorithms. D-SOBA achieves the state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and transien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#39564;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#65292;&#20351;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.02229</link><description>&lt;p&gt;
&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Vanilla Bayesian Optimization Performs Great in High Dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#39564;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#65292;&#20351;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#39640;&#32500;&#38382;&#39064;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#36719;&#32907;&#12290;&#21463;&#21040;&#32500;&#24230;&#22122;&#38899;&#30340;&#21050;&#28608;&#65292;&#35768;&#22810;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#23545;&#30446;&#26631;&#24212;&#29992;&#21508;&#31181;&#31616;&#21270;&#20551;&#35774;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#23548;&#33268;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#19981;&#36866;&#29992;&#30340;&#36864;&#21270;&#29616;&#35937;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#29616;&#26377;&#31639;&#27861;&#22914;&#20309;&#36890;&#36807;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#26469;&#24212;&#23545;&#36825;&#20123;&#36864;&#21270;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#20013;&#20856;&#22411;&#20808;&#39564;&#20551;&#35774;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#23545;&#30446;&#26631;&#26045;&#21152;&#32467;&#26500;&#24615;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23558;&#22797;&#26434;&#24615;&#38477;&#20302;&#21040;&#21487;&#31649;&#29702;&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#20462;&#25913;&#26041;&#27861;&#8212;&#8212;&#36890;&#36807;&#32500;&#24230;&#23545;&#39640;&#26031;&#36807;&#31243;&#38271;&#24230;&#20808;&#39564;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#8212;&#8212;&#25581;&#31034;&#20102;&#26631;&#20934;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#26126;&#30830;&#34920;&#26126;&#20854;&#25928;&#26524;&#36828;&#36828;&#36229;&#20986;&#20197;&#24448;&#30340;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional problems have long been considered the Achilles' heel of Bayesian optimization algorithms. Spurred by the curse of dimensionality, a large collection of algorithms aim to make it more performant in this setting, commonly by imposing various simplifying assumptions on the objective. In this paper, we identify the degeneracies that make vanilla Bayesian optimization poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of lowering the model complexity. Moreover, we propose an enhancement to the prior assumptions that are typical to vanilla Bayesian optimization algorithms, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. Our modification - a simple scaling of the Gaussian process lengthscale prior with the dimensionality - reveals that standard Bayesian optimization works drastically better than previously thought in high dimensions, clearly
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.16427</link><description>&lt;p&gt;
&#29420;&#31435;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning to Embed Time Series Patches Independently
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16427
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25513;&#30721;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21551;&#21457;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#39318;&#20808;&#23558;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#22359;&#22788;&#29702;&#24182;&#37096;&#20998;&#25513;&#30422;&#65292;&#28982;&#21518;&#35757;&#32451;Transformer&#27169;&#22411;&#36890;&#36807;&#20174;&#26410;&#25513;&#30422;&#30340;&#22359;&#39044;&#27979;&#34987;&#25513;&#30422;&#22359;&#26469;&#25429;&#25417;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#25429;&#25417;&#36825;&#31181;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21487;&#33021;&#19981;&#26159;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#31574;&#30053;&#65307;&#30456;&#21453;&#65292;&#29420;&#31435;&#23398;&#20064;&#23884;&#20837;&#29255;&#27573;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;1&#65289;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#65292;&#33258;&#21160;&#23558;&#27599;&#20010;&#22359;&#36827;&#34892;&#32534;&#30721;&#32780;&#19981;&#26597;&#30475;&#20854;&#20182;&#22359;&#65292;&#20197;&#21450;2&#65289;&#29420;&#33258;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;&#31616;&#21333;&#22359;&#24335;MLP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#26377;&#25928;&#22320;&#20998;&#23618;&#25429;&#33719;&#30456;&#37051;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16427v2 Announce Type: replace-cross  Abstract: Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train Transformers to capture the dependencies between patches by predicting masked patches from unmasked patches. However, we argue that capturing such patch dependencies might not be an optimal strategy for time series representation learning; rather, learning to embed patches independently results in better time series representations. Specifically, we propose to use 1) the simple patch reconstruction task, which autoencode each patch without looking at other patches, and 2) the simple patch-wise MLP that embeds each patch independently. In addition, we introduce complementary contrastive learning to hierarchically capture adjacent time series information efficiently. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftCLT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#36719;&#23545;&#27604;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#24573;&#30053;&#22266;&#26377;&#30456;&#20851;&#24615;&#25152;&#23548;&#33268;&#30340;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.16424</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#36719;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Soft Contrastive Learning for Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftCLT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#36719;&#23545;&#27604;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#24573;&#30053;&#22266;&#26377;&#30456;&#20851;&#24615;&#25152;&#23548;&#33268;&#30340;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23545;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#34920;&#31034;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30456;&#20284;&#30340;&#23454;&#20363;&#25110;&#30456;&#37051;&#26102;&#38388;&#25139;&#30340;&#20540;&#36827;&#34892;&#23545;&#27604;&#20250;&#24573;&#30053;&#23427;&#20204;&#22266;&#26377;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SoftCLT&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#36719;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#12290;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#20174;&#38646;&#21040;&#19968;&#30340;&#36719;&#36171;&#20540;&#30340;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#23545;&#27604;&#25439;&#22833;&#26469;&#23454;&#29616;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20026;1)&#22522;&#20110;&#25968;&#25454;&#31354;&#38388;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#23450;&#20041;&#20102;&#23454;&#20363;&#32423;&#23545;&#27604;&#25439;&#22833;&#30340;&#36719;&#36171;&#20540;&#65292;&#24182;&#20026;2)&#22522;&#20110;&#26102;&#38388;&#25139;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#20102;&#26102;&#38388;&#32423;&#23545;&#27604;&#25439;&#22833;&#12290;SoftCLT&#26159;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#27809;&#26377;&#36807;&#22810;&#22797;&#26434;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16424v2 Announce Type: replace-cross  Abstract: Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way. However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating the quality of learned representations. To address this issue, we propose SoftCLT, a simple yet effective soft contrastive learning strategy for time series. This is achieved by introducing instance-wise and temporal contrastive loss with soft assignments ranging from zero to one. Specifically, we define soft assignments for 1) instance-wise contrastive loss by the distance between time series on the data space, and 2) temporal contrastive loss by the difference of timestamps. SoftCLT is a plug-and-play method for time series contrastive learning that improves the quality of learned representations without bells and whistles. In experi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;RMIA&#65289;&#65292;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#24314;&#27169;&#21644;&#26356;&#39640;&#30340;&#27979;&#35797;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#38544;&#31169;&#39118;&#38505;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2312.03262</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#39640;&#21151;&#29575;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Low-Cost High-Power Membership Inference Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;RMIA&#65289;&#65292;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#24314;&#27169;&#21644;&#26356;&#39640;&#30340;&#27979;&#35797;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#38544;&#31169;&#39118;&#38505;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#26088;&#22312;&#26816;&#27979;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#20351;&#29992;&#12290;&#26368;&#36817;&#19968;&#20123;&#24378;&#22823;&#30340;&#25915;&#20987;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#34920;&#29616;&#19981;&#19968;&#33268;&#65292;&#20351;&#23427;&#20204;&#23545;&#20110;&#23454;&#38469;&#30340;&#38544;&#31169;&#39118;&#38505;&#35780;&#20272;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;RMIA&#65289;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#27169;&#22411;&#30340;&#24635;&#20307;&#25968;&#25454;&#21644;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20284;&#28982;&#27604;&#26816;&#39564;&#20013;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;&#38646;&#20551;&#35774;&#35774;&#32622;&#65292;&#24182;&#26377;&#25928;&#22320;&#21033;&#29992;&#26469;&#33258;&#24635;&#20307;&#30340;&#21442;&#32771;&#27169;&#22411;&#21644;&#21442;&#32771;&#25968;&#25454;&#26679;&#26412;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#30495;&#27491;&#29575;&#65288;true-positive rate&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#39640;&#30340;&#27979;&#35797;&#33021;&#21147;&#65292;&#25972;&#20010;TPR-FPR&#26354;&#32447;&#37117;&#20855;&#22791;&#36825;&#31181;&#20248;&#21183;&#65292;&#21363;&#20351;&#22312;&#26497;&#20302;&#30340;&#35823;&#25253;&#29575;&#19979;&#65288;&#20302;&#33267;0&#65289;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#35745;&#31639;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03262v2 Announce Type: replace-cross  Abstract: Membership inference attacks (MIA) aim to detect if a particular data point was used in training a machine learning model. Recent strong attacks have high computational costs and inconsistent performance under varying conditions, rendering them unreliable for practical privacy risk assessment. We design a novel, efficient, and robust membership inference attack (RMIA) which accurately differentiates between population data and training data of a model, with minimal computational overhead. We achieve this by a more accurate modeling of the null hypothesis setting in our likelihood ratio tests, and effectively leveraging both reference models and reference data samples from the population. Our algorithm exhibits superior test power (true-positive rate) compared to prior methods, throughout the TPR-FPR curve including at extremely low false-positive rates (as low as 0). Under computation constraints, where only a limited number of
&lt;/p&gt;</description></item><item><title>&#30417;&#25511;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#26159;&#37325;&#35201;&#30340;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#22240;&#26524;&#38236;&#22836;&#23548;&#33322;&#35299;&#20915;&#26377;&#25928;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.11463</link><description>&lt;p&gt;
&#35774;&#35745;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30417;&#25511;&#31574;&#30053;&#65306;&#36890;&#36807;&#22240;&#26524;&#38236;&#22836;&#23548;&#33322;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Designing monitoring strategies for deployed machine learning algorithms: navigating performativity through a causal lens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11463
&lt;/p&gt;
&lt;p&gt;
&#30417;&#25511;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#26159;&#37325;&#35201;&#30340;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#22240;&#26524;&#38236;&#22836;&#23548;&#33322;&#35299;&#20915;&#26377;&#25928;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#31995;&#32479;&#37096;&#32626;&#21518;&#65292;&#30417;&#25511;&#20854;&#24615;&#33021;&#23545;&#20110;&#30830;&#20445;&#31639;&#27861;&#38271;&#26399;&#23433;&#20840;&#26377;&#25928;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;ML&#31639;&#27861;&#19982;&#20854;&#29615;&#22659;&#20114;&#21160;&#26102;&#65292;&#31639;&#27861;&#21487;&#33021;&#24433;&#21709;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#65292;&#24182;&#22312;&#35780;&#20272;&#20854;&#29420;&#31435;&#24615;&#33021;&#26102;&#25104;&#20026;&#20027;&#35201;&#20559;&#35265;&#28304;&#65292;&#36825;&#19968;&#38382;&#39064;&#34987;&#31216;&#20026;&#26377;&#25928;&#24615;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#22312;&#26377;&#25928;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#39564;&#35777;&#27169;&#22411;&#65292;&#20294;&#22312;&#26377;&#25928;&#24615;&#23384;&#22312;&#30340;&#29615;&#22659;&#20013;&#30417;&#25511;&#27169;&#22411;&#30340;&#24037;&#20316;&#21364;&#24456;&#23569;&#12290;&#19982;&#27169;&#22411;&#39564;&#35777;&#35774;&#32622;&#19981;&#21516;&#65292;&#23545;&#20110;&#35201;&#30417;&#25511;&#21738;&#20123;&#24615;&#33021;&#25351;&#26631;&#27809;&#26377;&#24456;&#22810;&#19968;&#33268;&#24615;&#12290;&#19981;&#21516;&#30340;&#30417;&#25511;&#26631;&#20934;&#20250;&#24433;&#21709;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21487;&#36776;&#35782;&#24615;&#25152;&#38656;&#30340;&#20551;&#35774;&#65292;&#20197;&#21450;&#26816;&#27979;&#36895;&#24230;&#12290;&#24403;&#36825;&#19968;&#36873;&#25321;&#36827;&#19968;&#27493;&#19982;&#20351;&#29992;&#35266;&#23519;&#24615;&#19982;&#19981;&#24179;&#31561;&#24615;&#30340;&#20915;&#23450;&#30456;&#32467;&#21512;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11463v2 Announce Type: replace  Abstract: After a machine learning (ML)-based system is deployed, monitoring its performance is important to ensure the safety and effectiveness of the algorithm over time. When an ML algorithm interacts with its environment, the algorithm can affect the data-generating mechanism and be a major source of bias when evaluating its standalone performance, an issue known as performativity. Although prior work has shown how to validate models in the presence of performativity using causal inference techniques, there has been little work on how to monitor models in the presence of performativity. Unlike the setting of model validation, there is much less agreement on which performance metrics to monitor. Different monitoring criteria impact how interpretable the resulting test statistic is, what assumptions are needed for identifiability, and the speed of detection. When this choice is further coupled with the decision to use observational versus in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20132;&#20114;&#26041;&#24335;&#35775;&#38382;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#26465;&#20214;&#20998;&#24067;&#26679;&#26412;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;HMM&#30340;&#39640;&#25928;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#32780;&#32469;&#36807;&#20102;&#20854;&#23494;&#30721;&#23398;&#22256;&#38590;&#24615;&#12290;</title><link>https://arxiv.org/abs/2302.14753</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#26679;&#26412;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Hidden Markov Models Using Conditional Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.14753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20132;&#20114;&#26041;&#24335;&#35775;&#38382;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#26465;&#20214;&#20998;&#24067;&#26679;&#26412;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;HMM&#30340;&#39640;&#25928;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#32780;&#32469;&#36807;&#20102;&#20854;&#23494;&#30721;&#23398;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMM&#65289;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#34429;&#28982;HMM&#26159;&#39034;&#24207;&#21644;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#20294;&#22312;&#26631;&#20934;&#35774;&#32622;&#19979;&#65292;&#21363;&#23545;&#35266;&#27979;&#24207;&#21015;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#26679;&#26412;&#20855;&#26377;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#36215;&#26469;&#26159;&#20855;&#26377;&#23494;&#30721;&#23398;&#22256;&#38590;&#24615;&#30340;&#12290;&#26412;&#25991;&#20559;&#31163;&#20102;&#36825;&#19968;&#35774;&#23450;&#65292;&#32771;&#34385;&#20102;&#19968;&#31181;&#20132;&#20114;&#35775;&#38382;&#27169;&#22411;&#65292;&#22312;&#36825;&#31181;&#27169;&#22411;&#20013;&#65292;&#31639;&#27861;&#21487;&#20197;&#26597;&#35810;HMM&#30340;&#26465;&#20214;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;HMM&#30340;&#20132;&#20114;&#35775;&#38382;&#21487;&#20197;&#23454;&#29616;&#35745;&#31639;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#32780;&#32469;&#36807;&#23494;&#30721;&#23398;&#22256;&#38590;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#23398;&#20064;HMM&#30340;&#39640;&#25928;&#31639;&#27861;&#65306;&#65288;a&#65289;&#19968;&#31181;&#26356;&#23481;&#26131;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#21487;&#20197;&#26597;&#35810;&#20934;&#30830;&#26465;&#20214;&#27010;&#29575;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;&#24182;&#36827;&#34892;&#20102;&#22810;&#39033;&#24335;&#27425;&#26597;&#35810;&#65292;&#20197;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#20013;&#36817;&#20284;&#20219;&#20309;HMM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.14753v2 Announce Type: replace-cross  Abstract: This paper is concerned with the computational complexity of learning the Hidden Markov Model (HMM). Although HMMs are some of the most widely used tools in sequential and time series modeling, they are cryptographically hard to learn in the standard setting where one has access to i.i.d. samples of observation sequences. In this paper, we depart from this setup and consider an interactive access model, in which the algorithm can query for samples from the conditional distributions of the HMMs. We show that interactive access to the HMM enables computationally efficient learning algorithms, thereby bypassing cryptographic hardness. Specifically, we obtain efficient algorithms for learning HMMs in two settings:   (a) An easier setting where we have query access to the exact conditional probabilities. Here our algorithm runs in polynomial time and makes polynomially many queries to approximate any HMM in total variation distance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20351;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#19987;&#27880;&#20110;&#23569;&#25968;&#26679;&#26412;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2301.12334</link><description>&lt;p&gt;
&#19981;&#20559;&#19981;&#20506;&#65306;&#23569;&#25968;&#26063;&#32676;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Don't Play Favorites: Minority Guidance for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.12334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20351;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#19987;&#27880;&#20110;&#23569;&#25968;&#26679;&#26412;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#23569;&#25968;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;&#23569;&#25968;&#26679;&#26412;&#26159;&#20301;&#20110;&#25968;&#25454;&#27969;&#24418;&#20302;&#23494;&#24230;&#21306;&#22495;&#30340;&#23454;&#20363;&#12290;&#29983;&#25104;&#36275;&#22815;&#25968;&#37327;&#30340;&#36825;&#31181;&#23569;&#25968;&#26679;&#26412;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#21253;&#21547;&#25968;&#25454;&#30340;&#19968;&#20123;&#29420;&#29305;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#20284;&#28982;&#24615;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#20256;&#32479;&#29983;&#25104;&#36807;&#31243;&#20027;&#35201;&#20135;&#29983;&#22823;&#22810;&#25968;&#26679;&#26412;&#65288;&#20301;&#20110;&#27969;&#24418;&#39640;&#23494;&#24230;&#21306;&#22495;&#65289;&#65292;&#20351;&#33258;&#36523;&#23545;&#23569;&#25968;&#29983;&#25104;&#20219;&#21153;&#26080;&#25928;&#19988;&#32791;&#26102;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#19987;&#27880;&#20110;&#23569;&#25968;&#26679;&#26412;&#12290;&#39318;&#20808;&#24378;&#35843; Tweedie &#30340;&#38477;&#22122;&#20844;&#24335;&#23545;&#22823;&#22810;&#25968;&#26679;&#26412;&#20135;&#29983;&#26377;&#21033;&#32467;&#26524;&#12290;&#36825;&#19968;&#35266;&#23519;&#28608;&#21169;&#25105;&#20204;&#24341;&#20837;&#25551;&#36848;&#32473;&#23450;&#26679;&#26412;&#29420;&#29305;&#24615;&#30340;&#24230;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#22909;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.12334v2 Announce Type: replace-cross  Abstract: We explore the problem of generating minority samples using diffusion models. The minority samples are instances that lie on low-density regions of a data manifold. Generating a sufficient number of such minority instances is important, since they often contain some unique attributes of the data. However, the conventional generation process of the diffusion models mostly yields majority samples (that lie on high-density regions of the manifold) due to their high likelihoods, making themselves ineffective and time-consuming for the minority generating task. In this work, we present a novel framework that can make the generation process of the diffusion models focus on the minority samples. We first highlight that Tweedie's denoising formula yields favorable results for majority samples. The observation motivates us to introduce a metric that describes the uniqueness of a given sample. To address the inherent preference of the di
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26465;&#20214;&#22810;&#27169;&#24577;&#21028;&#21035;&#65288;CMMD&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#26469;&#39044;&#27979;&#30772;&#20135;&#39118;&#38505;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#30772;&#20135;&#27169;&#22411;&#20013;&#32570;&#23569;MDA&#25991;&#26412;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2211.08405</link><description>&lt;p&gt;
&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#30772;&#20135;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multimodal Generative Models for Bankruptcy Prediction Using Textual Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.08405
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26465;&#20214;&#22810;&#27169;&#24577;&#21028;&#21035;&#65288;CMMD&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#26469;&#39044;&#27979;&#30772;&#20135;&#39118;&#38505;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#30772;&#20135;&#27169;&#22411;&#20013;&#32570;&#23569;MDA&#25991;&#26412;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#36130;&#21153;&#25253;&#21578;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#20363;&#22914;10-K&#34920;&#20013;&#30340;&#8220;&#31649;&#29702;&#35752;&#35770;&#19982;&#20998;&#26512;&#8221;&#65288;MDA&#65289;&#37096;&#20998;&#65292;&#24050;&#34987;&#29992;&#20110;&#25552;&#39640;&#30772;&#20135;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#26080;&#27861;&#20026;&#25152;&#26377;&#19978;&#24066;&#20844;&#21496;&#33719;&#21462;MDA&#37096;&#20998;&#65292;&#36825;&#38480;&#21046;&#20102;MDA&#25968;&#25454;&#22312;&#20256;&#32479;&#30772;&#20135;&#27169;&#22411;&#20013;&#30340;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23436;&#25972;&#25968;&#25454;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#32570;&#23569;MDA&#25968;&#25454;&#30340;&#20004;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#65306;&#65288;i&#65289;&#24182;&#38750;&#25152;&#26377;&#20844;&#21496;&#37117;&#26377;&#20041;&#21153;&#25552;&#20132;MDA&#65292;&#65288;ii&#65289;&#24403;&#29228;&#21462;&#21644;&#25235;&#21462;MDA&#37096;&#20998;&#26102;&#20250;&#20986;&#29616;&#25216;&#26415;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#26465;&#20214;&#22810;&#27169;&#24577;&#21028;&#21035;&#65288;CMMD&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#23884;&#20837;&#20102;&#20250;&#35745;&#12289;&#24066;&#22330;&#21644;&#25991;&#26412;&#25968;&#25454;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;CMMD&#27169;&#22411;&#38656;&#35201;&#19968;&#32452;&#25152;&#26377;&#25968;&#25454;&#27169;&#24577;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;CMMD&#27169;&#22411;&#21482;&#38656;&#35201;&#35775;&#38382;&#20250;&#35745;&#21644;&#24066;&#22330;&#27169;&#24577;&#26469;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.08405v5 Announce Type: replace-cross  Abstract: Textual data from financial filings, e.g., the Management's Discussion &amp; Analysis (MDA) section in Form 10-K, has been used to improve the prediction accuracy of bankruptcy models. In practice, however, we cannot obtain the MDA section for all public companies, which limits the use of MDA data in traditional bankruptcy models, as they need complete data to make predictions. The two main reasons for the lack of MDA are: (i) not all companies are obliged to submit the MDA and (ii) technical problems arise when crawling and scrapping the MDA section. To solve this limitation, this research introduces the Conditional Multimodal Discriminative (CMMD) model that learns multimodal representations that embed information from accounting, market, and textual data modalities. The CMMD model needs a sample with all data modalities for model training. At test time, the CMMD model only needs access to accounting and market modalities to gene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24179;&#31283;&#26465;&#20214;&#20998;&#24067;&#24314;&#27169;&#19982;&#38750;&#24179;&#31283;&#21160;&#24577;&#24314;&#27169;&#35299;&#32806;&#65292;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#19978;&#30340;&#38750;&#24179;&#31283;&#26465;&#20214;&#20998;&#24067;&#65292;&#33021;&#26356;&#22909;&#22320;&#36866;&#24212;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2209.08411</link><description>&lt;p&gt;
DynaConF&#65306;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DynaConF: Dynamic Forecasting of Non-Stationary Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.08411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24179;&#31283;&#26465;&#20214;&#20998;&#24067;&#24314;&#27169;&#19982;&#38750;&#24179;&#31283;&#21160;&#24577;&#24314;&#27169;&#35299;&#32806;&#65292;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#19978;&#30340;&#38750;&#24179;&#31283;&#26465;&#20214;&#20998;&#24067;&#65292;&#33021;&#26356;&#22909;&#22320;&#36866;&#24212;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#24314;&#27169;&#26410;&#26469;&#32473;&#23450;&#36807;&#21435;&#30340;&#26465;&#20214;&#20998;&#24067;&#26159;&#20854;&#26680;&#24515;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#31181;&#26465;&#20214;&#20998;&#24067;&#26159;&#38750;&#24179;&#31283;&#30340;&#26102;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#26469;&#35828;&#65292;&#35201;&#19968;&#33268;&#23398;&#20064;&#21644;&#20934;&#30830;&#39044;&#27979;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#22320;&#23558;&#24179;&#31283;&#26465;&#20214;&#20998;&#24067;&#24314;&#27169;&#19982;&#38750;&#24179;&#31283;&#21160;&#24577;&#24314;&#27169;&#35299;&#32806;&#65292;&#26469;&#23545;&#26102;&#38388;&#19978;&#30340;&#38750;&#24179;&#31283;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36125;&#21494;&#26031;&#21160;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#36866;&#24212;&#26465;&#20214;&#20998;&#24067;&#21464;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;&#20998;&#35299;&#30340;&#36755;&#20986;&#31354;&#38388;&#22788;&#29702;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#28145;&#24230;&#26465;&#20214;&#20998;&#24067;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26356;&#22909;&#22320;&#36866;&#24212;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.08411v3 Announce Type: replace  Abstract: Deep learning has shown impressive results in a variety of time series forecasting tasks, where modeling the conditional distribution of the future given the past is the essence. However, when this conditional distribution is non-stationary, it poses challenges for these models to learn consistently and to predict accurately. In this work, we propose a new method to model non-stationary conditional distributions over time by clearly decoupling stationary conditional distribution modeling from non-stationary dynamics modeling. Our method is based on a Bayesian dynamic model that can adapt to conditional distribution changes and a deep conditional distribution model that handles multivariate time series using a factorized output space. Our experimental results on synthetic and real-world datasets show that our model can adapt to non-stationary time series better than state-of-the-art deep learning solutions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#22411;&#31232;&#30095;&#26080;&#31351;&#38454;VAR&#27169;&#22411;&#65292;&#26082;&#36991;&#20813;&#20102;&#38750;&#21487;&#36776;&#35782;&#24615;&#21644;&#35745;&#31639;&#38590;&#24230;&#65292;&#21448;&#33021;&#20998;&#21035;&#35299;&#37322;VARMA&#31867;&#22411;&#21160;&#24577;&#30340;&#26102;&#38388;&#21644;&#27178;&#25130;&#38754;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2209.01172</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#19988;&#39640;&#25928;&#30340;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#26080;&#31351;&#38454;&#21521;&#37327;&#33258;&#22238;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Interpretable and Efficient Infinite-Order Vector Autoregressive Model for High-Dimensional Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.01172
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#22411;&#31232;&#30095;&#26080;&#31351;&#38454;VAR&#27169;&#22411;&#65292;&#26082;&#36991;&#20813;&#20102;&#38750;&#21487;&#36776;&#35782;&#24615;&#21644;&#35745;&#31639;&#38590;&#24230;&#65292;&#21448;&#33021;&#20998;&#21035;&#35299;&#37322;VARMA&#31867;&#22411;&#21160;&#24577;&#30340;&#26102;&#38388;&#21644;&#27178;&#25130;&#38754;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#30340;&#26080;&#31351;&#38454;&#21521;&#37327;&#33258;&#22238;&#24402;&#65288;VAR&#65289;&#27169;&#22411;&#65292;&#21521;&#37327;&#33258;&#22238;&#24402;&#31227;&#21160;&#24179;&#22343;&#65288;VARMA&#65289;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#27604;&#24191;&#27867;&#20351;&#29992;&#30340;&#26377;&#38480;&#38454;VAR&#27169;&#22411;&#26356;&#20016;&#23500;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#19981;&#21487;&#36776;&#35782;&#24615;&#12289;&#35745;&#31639;&#38590;&#24230;&#21644;&#35299;&#37322;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#23454;&#29992;&#24615;&#38271;&#26399;&#21463;&#21040;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#26080;&#31351;&#38454;VAR&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#19978;&#36848;&#25152;&#26377;&#32570;&#28857;&#65292;&#21516;&#26102;&#32487;&#25215;&#20102;VARMA&#27169;&#22411;&#30340;&#22522;&#26412;&#26102;&#24207;&#27169;&#24335;&#12290;&#20316;&#20026;&#21478;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#25429;&#25417;&#21040;&#30340;VARMA&#31867;&#22411;&#21160;&#24577;&#30340;&#26102;&#38388;&#21644;&#27178;&#25130;&#38754;&#32467;&#26500;&#21487;&#20197;&#20998;&#24320;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#20204;&#30001;&#19981;&#21516;&#30340;&#21442;&#25968;&#38598;&#34920;&#24449;&#12290;&#36825;&#31181;&#20998;&#31163;&#33258;&#28982;&#22320;&#28608;&#21457;&#20102;&#23545;&#30830;&#23450;&#27178;&#25130;&#38754;&#20381;&#36182;&#24615;&#30340;&#21442;&#25968;&#30340;&#31232;&#30095;&#24615;&#20551;&#35774;&#12290;&#32467;&#26524;&#65292;great
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.01172v4 Announce Type: replace-cross  Abstract: As a special infinite-order vector autoregressive (VAR) model, the vector autoregressive moving average (VARMA) model can capture much richer temporal patterns than the widely used finite-order VAR model. However, its practicality has long been hindered by its non-identifiability, computational intractability, and difficulty of interpretation, especially for high-dimensional time series. This paper proposes a novel sparse infinite-order VAR model for high-dimensional time series, which avoids all above drawbacks while inheriting essential temporal patterns of the VARMA model. As another attractive feature, the temporal and cross-sectional structures of the VARMA-type dynamics captured by this model can be interpreted separately, since they are characterized by different sets of parameters. This separation naturally motivates the sparsity assumption on the parameters determining the cross-sectional dependence. As a result, great
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24754;&#35266;&#31639;&#27861; VI-LCB-Game&#65292;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#25214;&#21040;&#20102;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#21152;&#24378;&#20102;&#20808;&#21069;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2206.04044</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-Based Reinforcement Learning for Offline Zero-Sum Markov Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.04044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24754;&#35266;&#31639;&#27861; VI-LCB-Game&#65292;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#25214;&#21040;&#20102;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#21152;&#24378;&#20102;&#20808;&#21069;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24754;&#35266;&#31639;&#27861;&#65292;&#21363;&#20855;&#26377;Bernstein&#39118;&#26684;&#30340;&#19979;&#38480;&#32622;&#20449;&#30028;&#30340;VI-LCB-Game&#31639;&#27861;&#65292;&#21487;&#20197;&#35777;&#26126;&#20197;&#26679;&#26412;&#22797;&#26434;&#24230;&#19981;&#22823;&#20110;$\frac{C_{\mathsf{clipped}}^{\star}S(A+B)}{(1-\gamma)^{3}\varepsilon^{2}}$&#65288;&#24102;&#26377;&#19968;&#20123;&#23545;&#25968;&#22240;&#23376;&#65289;&#25214;&#21040;&#19968;&#20010;$\varepsilon$-&#36817;&#20284;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#22312;&#36825;&#37324;&#65292;$C_{\mathsf{clipped}}^{\star}$ &#26159;&#21453;&#26144;&#21487;&#29992;&#25968;&#25454;&#65288;&#20851;&#20110;&#30446;&#26631;&#25968;&#25454;&#65289;&#30340;&#35206;&#30422;&#29575;&#21644;&#20998;&#24067;&#36716;&#21464;&#30340;&#26576;&#31181;&#21333;&#20391;&#21098;&#20999;&#30340;&#38598;&#20013;&#24230;&#31995;&#25968;&#65292;&#30446;&#26631;&#31934;&#24230;$\varepsilon$ &#21487;&#20197;&#26159;$\big(0,\frac{1}{1-\gamma}\big]$&#33539;&#22260;&#20869;&#30340;&#20219;&#20309;&#20540;&#12290;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#21152;&#24378;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.04044v2 Announce Type: replace  Abstract: This paper makes progress towards learning Nash equilibria in two-player zero-sum Markov games from offline data. Specifically, consider a $\gamma$-discounted infinite-horizon Markov game with $S$ states, where the max-player has $A$ actions and the min-player has $B$ actions. We propose a pessimistic model-based algorithm with Bernstein-style lower confidence bounds -- called VI-LCB-Game -- that provably finds an $\varepsilon$-approximate Nash equilibrium with a sample complexity no larger than $\frac{C_{\mathsf{clipped}}^{\star}S(A+B)}{(1-\gamma)^{3}\varepsilon^{2}}$ (up to some log factor). Here, $C_{\mathsf{clipped}}^{\star}$ is some unilateral clipped concentrability coefficient that reflects the coverage and distribution shift of the available data (vis-\`a-vis the target data), and the target accuracy $\varepsilon$ can be any value within $\big(0,\frac{1}{1-\gamma}\big]$. Our sample complexity bound strengthens prior art by a 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#26080;&#22870;&#21169;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#21040;&#22312;&#32447;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23398;&#20064;&#21160;&#24577;VCG&#26426;&#21046;&#19988;&#20855;&#26377;&#19978;&#30028;&#20026;$\tilde{\mathcal{O}}(T^{2/3})$&#30340;&#36951;&#25022;&#20445;&#35777;&#30340;&#26032;&#39062;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2202.12797</link><description>&lt;p&gt;
&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23398;&#20064;&#21160;&#24577;&#26426;&#21046;&#65306;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Dynamic Mechanisms in Unknown Environments: A Reinforcement Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.12797
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#26080;&#22870;&#21169;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#21040;&#22312;&#32447;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23398;&#20064;&#21160;&#24577;VCG&#26426;&#21046;&#19988;&#20855;&#26377;&#19978;&#30028;&#20026;$\tilde{\mathcal{O}}(T^{2/3})$&#30340;&#36951;&#25022;&#20445;&#35777;&#30340;&#26032;&#39062;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26426;&#21046;&#35774;&#35745;&#30740;&#31350;&#20102;&#26426;&#21046;&#35774;&#35745;&#32773;&#22312;&#26102;&#21464;&#29615;&#22659;&#20013;&#24212;&#35813;&#22914;&#20309;&#22312;&#20195;&#29702;&#20043;&#38388;&#20998;&#37197;&#36164;&#28304;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#38382;&#39064;&#65292;&#21363;&#20195;&#29702;&#26681;&#25454;&#26410;&#30693;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#19982;&#26426;&#21046;&#35774;&#35745;&#32773;&#20114;&#21160;&#65292;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#20195;&#29702;&#30340;&#22870;&#21169;&#21644;&#26426;&#21046;&#35774;&#35745;&#32773;&#30340;&#29366;&#24577;&#26681;&#25454;&#19968;&#20010;&#24102;&#26377;&#26410;&#30693;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#26680;&#30340;&#24773;&#33410;MDP&#28436;&#21270;&#12290;&#25105;&#20204;&#20851;&#27880;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#32447;&#24615;&#20989;&#25968;&#36817;&#20284;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#22810;&#36718;&#20114;&#21160;&#20013;&#24674;&#22797;&#21160;&#24577;Vickrey-Clarke-Grove(VCG)&#26426;&#21046;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#23558;&#26080;&#22870;&#21169;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;(RL)&#32467;&#21512;&#36827;&#26469;&#65292;&#20197;&#24110;&#21161;&#22312;&#20016;&#23500;&#30340;&#31574;&#30053;&#31354;&#38388;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#20174;&#32780;&#20272;&#35745;&#21160;&#24577;VCG&#26426;&#21046;&#20013;&#30340;&#20215;&#26684;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#36951;&#25022;&#19978;&#30028;&#20026;$\tilde{\mathcal{O}}(T^{2/3})$&#65292;&#24182;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.12797v2 Announce Type: replace  Abstract: Dynamic mechanism design studies how mechanism designers should allocate resources among agents in a time-varying environment. We consider the problem where the agents interact with the mechanism designer according to an unknown Markov Decision Process (MDP), where agent rewards and the mechanism designer's state evolve according to an episodic MDP with unknown reward functions and transition kernels. We focus on the online setting with linear function approximation and propose novel learning algorithms to recover the dynamic Vickrey-Clarke-Grove (VCG) mechanism over multiple rounds of interaction. A key contribution of our approach is incorporating reward-free online Reinforcement Learning (RL) to aid exploration over a rich policy space to estimate prices in the dynamic VCG mechanism. We show that the regret of our proposed method is upper bounded by $\tilde{\mathcal{O}}(T^{2/3})$ and further devise a lower bound to show that our a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#22266;&#23450;&#32622;&#20449;&#24230;&#19979;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22312;&#35782;&#21035;&#26368;&#20339;&#33218;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#8220;&#36319;&#36394;&#20572;&#27490;&#8221;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#39640;&#25928;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2106.14077</link><description>&lt;p&gt;
&#22312;&#26368;&#20339;&#33218;&#35782;&#21035;&#20013;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Contextual Information in Best Arm Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.14077
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#22266;&#23450;&#32622;&#20449;&#24230;&#19979;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22312;&#35782;&#21035;&#26368;&#20339;&#33218;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#8220;&#36319;&#36394;&#20572;&#27490;&#8221;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#39640;&#25928;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#24403;&#26377;&#19978;&#19979;&#25991;&#65288;&#21327;&#21464;&#37327;&#65289;&#20449;&#24687;&#21487;&#29992;&#26102;&#30340;&#22266;&#23450;&#32622;&#20449;&#24230;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#34429;&#28982;&#25105;&#20204;&#21487;&#20197;&#22312;&#27599;&#19968;&#36718;&#20013;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20294;&#25105;&#20204;&#23545;&#19978;&#19979;&#25991;&#20998;&#24067;&#30340;&#36793;&#38469;&#21270;&#22343;&#20540;&#37325;&#35270;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#38169;&#35823;&#29575;&#20540;&#30340;&#24773;&#20917;&#19979;&#20197;&#26368;&#23567;&#25968;&#37327;&#30340;&#25277;&#26679;&#35782;&#21035;&#26368;&#20339;&#33218;&#12290;&#25105;&#20204;&#20026;&#35813;&#38382;&#39064;&#23637;&#31034;&#20102;&#29305;&#23450;&#23454;&#20363;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#19979;&#30028;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#36319;&#36394;&#20572;&#27490;&#8221;&#31574;&#30053;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#29256;&#26412;&#65292;&#20854;&#20013;&#33218;&#25277;&#21462;&#30340;&#27604;&#20363;&#36319;&#36394;&#26368;&#20248;&#20998;&#37197;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;&#39044;&#26399;&#30340;&#33218;&#25277;&#21462;&#27425;&#25968;&#28176;&#36817;&#22320;&#19982;&#19979;&#30028;&#21305;&#37197;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;Garivier &amp; Kaufmann&#65288;2016&#65289;&#30340;&#32467;&#26524;&#65292;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#20197;&#29992;&#26469;&#25913;&#21892;&#23545;&#26368;&#20339;&#36793;&#38469;&#21270;&#22343;&#20540;&#22870;&#21169;&#30340;&#35782;&#21035;&#25928;&#29575;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#23454;&#20102; cont
&lt;/p&gt;
&lt;p&gt;
arXiv:2106.14077v3 Announce Type: replace  Abstract: We study the best-arm identification problem with fixed confidence when contextual (covariate) information is available in stochastic bandits. Although we can use contextual information in each round, we are interested in the marginalized mean reward over the contextual distribution. Our goal is to identify the best arm with a minimal number of samplings under a given value of the error rate. We show the instance-specific sample complexity lower bounds for the problem. Then, we propose a context-aware version of the "Track-and-Stop" strategy, wherein the proportion of the arm draws tracks the set of optimal allocations and prove that the expected number of arm draws matches the lower bound asymptotically. We demonstrate that contextual information can be used to improve the efficiency of the identification of the best marginalized mean reward compared with the results of Garivier &amp; Kaufmann (2016). We experimentally confirm that cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#23450;&#22238;&#24402;&#65288;AR&#65289;&#30340;&#21487;&#25193;&#23637;&#20984;&#35268;&#21010;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#22823;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2103.07020</link><description>&lt;p&gt;
&#21033;&#29992;&#20984;&#35268;&#21010;&#30340;&#26368;&#22823;&#32447;&#24615;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Max-Linear Regression by Convex Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2103.07020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#23450;&#22238;&#24402;&#65288;AR&#65289;&#30340;&#21487;&#25193;&#23637;&#20984;&#35268;&#21010;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#22823;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22810;&#20803;&#26368;&#22823;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#38656;&#35201;&#20174;$n$&#20010;&#29420;&#31435;&#26679;&#26412;&#20013;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;$\boldsymbol{\beta}_{1},\dotsc,\boldsymbol{\beta}_{k}\in\mathbb{R}^{p}$&#65292;&#36825;&#20123;&#26679;&#26412;&#26159;&#65288;&#22122;&#22768;&#30340;&#65289;&#35266;&#27979;$y = \max_{1\leq j \leq k} \boldsymbol{\beta}_{j}^{\mathsf{T}} \boldsymbol{x} + \mathrm{noise}$&#12290;&#26368;&#22823;&#32447;&#24615;&#27169;&#22411;&#24191;&#27867;&#22320;&#25512;&#24191;&#20102;&#20256;&#32479;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#24403;&#32447;&#24615;&#27169;&#22411;&#30340;&#25968;&#37327;$k$&#36275;&#22815;&#22823;&#26102;&#65292;&#23427;&#21487;&#20197;&#20197;&#20219;&#24847;&#31934;&#24230;&#36924;&#36817;&#20219;&#20309;&#20984;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#26368;&#22823;&#32447;&#24615;&#27169;&#22411;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#20351;&#24471;&#22238;&#24402;&#21442;&#25968;&#30340;&#20272;&#35745;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25991;&#29486;&#20013;&#27809;&#26377;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20984;&#35268;&#21010;&#31243;&#24207;&#65292;&#21363;&#38170;&#23450;&#22238;&#24402;&#65288;AR&#65289;&#65292;&#20316;&#20026;&#26368;&#22823;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#30340;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2103.07020v2 Announce Type: replace-cross  Abstract: We consider the multivariate max-linear regression problem where the model parameters $\boldsymbol{\beta}_{1},\dotsc,\boldsymbol{\beta}_{k}\in\mathbb{R}^{p}$ need to be estimated from $n$ independent samples of the (noisy) observations $y = \max_{1\leq j \leq k} \boldsymbol{\beta}_{j}^{\mathsf{T}} \boldsymbol{x} + \mathrm{noise}$. The max-linear model vastly generalizes the conventional linear model, and it can approximate any convex function to an arbitrary accuracy when the number of linear models $k$ is large enough. However, the inherent nonlinearity of the max-linear model renders the estimation of the regression parameters computationally challenging. Particularly, no estimator based on convex programming is known in the literature. We formulate and analyze a scalable convex program given by anchored regression (AR) as the estimator for the max-linear regression problem. Under the standard Gaussian observation setting, we
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#20809;&#28369;&#20989;&#25968;&#30340;&#21487;&#36870;&#21464;&#25442;&#34920;&#31034;&#21333;&#35843;&#19977;&#35282;&#24418;&#26144;&#23556;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#24471;&#30456;&#20851;&#30340;&#26080;&#31351;&#32500;&#26368;&#23567;&#21270;&#38382;&#39064;&#20855;&#26377;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;</title><link>https://arxiv.org/abs/2009.10303</link><description>&lt;p&gt;
&#20851;&#20110;&#21333;&#35843;&#19977;&#35282;&#24418;&#36755;&#36816;&#26144;&#23556;&#30340;&#34920;&#31034;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On the representation and learning of monotone triangular transport maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2009.10303
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#20809;&#28369;&#20989;&#25968;&#30340;&#21487;&#36870;&#21464;&#25442;&#34920;&#31034;&#21333;&#35843;&#19977;&#35282;&#24418;&#26144;&#23556;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#24471;&#30456;&#20851;&#30340;&#26080;&#31351;&#32500;&#26368;&#23567;&#21270;&#38382;&#39064;&#20855;&#26377;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#24230;&#30340;&#36755;&#36816;&#25552;&#20379;&#20102;&#23545;&#24314;&#27169;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#30340;&#22810;&#21151;&#33021;&#26041;&#27861;&#65292;&#22312;&#23494;&#24230;&#20272;&#35745;&#12289;&#36125;&#21494;&#26031;&#25512;&#26029;&#12289;&#29983;&#25104;&#24314;&#27169;&#31561;&#26041;&#38754;&#26377;&#24212;&#29992;&#12290;&#21333;&#35843;&#19977;&#35282;&#24418;&#36755;&#36816;&#26144;&#23556;&#8212;&#8212;Knothe-Rosenblatt (KR)&#25490;&#21015;&#30340;&#36817;&#20284;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#26159;&#19968;&#20010;&#32463;&#20856;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26144;&#23556;&#30340;&#34920;&#31034;&#21644;&#21442;&#25968;&#21270;&#23545;&#20854;&#36890;&#29992;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#20197;&#21450;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26144;&#23556;&#25152;&#24341;&#36215;&#30340;&#20248;&#21270;&#38382;&#39064;&#30340;&#24615;&#36136;&#65288;&#20363;&#22914;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65289;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#21333;&#35843;&#19977;&#35282;&#24418;&#26144;&#23556;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20809;&#28369;&#20989;&#25968;&#30340;&#21487;&#36870;&#21464;&#25442;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#21464;&#25442;&#30340;&#26465;&#20214;&#65292;&#20351;&#24471;&#30456;&#20851;&#30340;&#26080;&#31351;&#32500;&#26368;&#23567;&#21270;&#38382;&#39064;&#27809;&#26377;&#34394;&#20551;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#21363;&#25152;&#26377;&#23616;&#37096;&#26497;&#23567;&#20540;&#37117;&#26159;&#20840;&#23616;&#26497;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2009.10303v3 Announce Type: replace-cross  Abstract: Transportation of measure provides a versatile approach for modeling complex probability distributions, with applications in density estimation, Bayesian inference, generative modeling, and beyond. Monotone triangular transport maps$\unicode{x2014}$approximations of the Knothe$\unicode{x2013}$Rosenblatt (KR) rearrangement$\unicode{x2014}$are a canonical choice for these tasks. Yet the representation and parameterization of such maps have a significant impact on their generality and expressiveness, and on properties of the optimization problem that arises in learning a map from data (e.g., via maximum likelihood estimation). We present a general framework for representing monotone triangular maps via invertible transformations of smooth functions. We establish conditions on the transformation such that the associated infinite-dimensional minimization problem has no spurious local minima, i.e., all local minima are global minima;
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#30340;Tanimoto&#26680;&#20844;&#24335;&#65292;&#20801;&#35768;&#34913;&#37327;&#20219;&#24847;&#23454;&#20540;&#20989;&#25968;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20809;&#28369;&#36924;&#36817;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2007.05943</link><description>&lt;p&gt;
&#23558;Tanimoto&#31867;&#22411;&#26680;&#27867;&#21270;&#21040;&#23454;&#20540;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
On the generalization of Tanimoto-type kernels to real valued functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2007.05943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#30340;Tanimoto&#26680;&#20844;&#24335;&#65292;&#20801;&#35768;&#34913;&#37327;&#20219;&#24847;&#23454;&#20540;&#20989;&#25968;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20809;&#28369;&#36924;&#36817;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tanimoto&#26680;&#65288;Jaccard&#25351;&#25968;&#65289;&#26159;&#25551;&#36848;&#20108;&#20540;&#23646;&#24615;&#38598;&#30456;&#20284;&#24615;&#30340;&#30693;&#21517;&#24037;&#20855;&#12290;&#24050;&#23558;&#20854;&#25193;&#23637;&#21040;&#23646;&#24615;&#20026;&#38750;&#36127;&#23454;&#25968;&#20540;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#30340;Tanimoto&#26680;&#20844;&#24335;&#65292;&#20801;&#35768;&#34913;&#37327;&#20219;&#24847;&#23454;&#20540;&#20989;&#25968;&#30340;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#30340;&#38598;&#21512;&#32479;&#19968;&#23646;&#24615;&#34920;&#31034;&#26469;&#26500;&#24314;&#27492;&#25193;&#23637;&#12290;&#22312;&#25512;&#23548;&#26680;&#30340;&#19968;&#33324;&#24418;&#24335;&#21518;&#65292;&#20174;&#26680;&#20989;&#25968;&#20013;&#25552;&#21462;&#20102;&#26174;&#24335;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#19968;&#33324;&#26680;&#21253;&#21547;&#21040;Tanimoto&#26680;&#20013;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#26680;&#20063;&#34920;&#31034;&#20026;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#21830;&#65292;&#24182;&#25552;&#20379;&#20102;&#20809;&#28369;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2007.05943v2 Announce Type: replace  Abstract: The Tanimoto kernel (Jaccard index) is a well known tool to describe the similarity between sets of binary attributes. It has been extended to the case when the attributes are nonnegative real values. This paper introduces a more general Tanimoto kernel formulation which allows to measure the similarity of arbitrary real-valued functions. This extension is constructed by unifying the representation of the attributes via properly chosen sets. After deriving the general form of the kernel, explicit feature representation is extracted from the kernel function, and a simply way of including general kernels into the Tanimoto kernel is shown. Finally, the kernel is also expressed as a quotient of piecewise linear functions, and a smooth approximation is provided.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#24212;&#28857;&#36827;&#34892;&#31232;&#30095;&#27491;&#20132;&#21464;&#20998;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#32039;&#30340;&#36793;&#32536;&#20284;&#28982;&#19979;&#30028;&#21644;&#26032;&#30340;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;</title><link>https://arxiv.org/abs/1910.10596</link><description>&lt;p&gt;
&#31232;&#30095;&#27491;&#20132;&#21464;&#20998;&#25512;&#26029;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Sparse Orthogonal Variational Inference for Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1910.10596
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#24212;&#28857;&#36827;&#34892;&#31232;&#30095;&#27491;&#20132;&#21464;&#20998;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#32039;&#30340;&#36793;&#32536;&#20284;&#28982;&#19979;&#30028;&#21644;&#26032;&#30340;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#21464;&#20998;&#36924;&#36817;&#39640;&#26031;&#36807;&#31243;&#30340;&#35299;&#37322;&#65292;&#20351;&#29992;&#24863;&#24212;&#28857;&#65292;&#36825;&#21487;&#20197;&#23548;&#33268;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#30340;&#31639;&#27861;&#12290;&#23427;&#22522;&#20110;&#23558;&#39640;&#26031;&#36807;&#31243;&#20998;&#35299;&#20026;&#20004;&#20010;&#29420;&#31435;&#36807;&#31243;&#20043;&#21644;&#65306;&#19968;&#20010;&#30001;&#26377;&#38480;&#22522;&#24863;&#24212;&#28857;&#23637;&#24320;&#65292;&#21478;&#19968;&#20010;&#25429;&#33719;&#21097;&#20313;&#21464;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24418;&#24335;&#21487;&#24674;&#22797;&#29616;&#26377;&#36924;&#36817;&#65292;&#24182;&#21516;&#26102;&#20801;&#35768;&#33719;&#24471;&#26356;&#32039;&#30340;&#36793;&#32536;&#20284;&#28982;&#19979;&#30028;&#21644;&#26032;&#30340;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#20960;&#31181;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#25928;&#29575;&#65292;&#20174;&#26631;&#20934;&#22238;&#24402;&#21040;&#22810;&#31867;&#20998;&#31867;&#65292;&#20351;&#29992;(&#28145;&#24230;)&#21367;&#31215;&#39640;&#26031;&#36807;&#31243;&#65292;&#24182;&#22312;CIFAR-10&#19978;&#25253;&#21578;&#20102;&#32431;GP&#27169;&#22411;&#20013;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1910.10596v5 Announce Type: replace-cross  Abstract: We introduce a new interpretation of sparse variational approximations for Gaussian processes using inducing points, which can lead to more scalable algorithms than previous methods. It is based on decomposing a Gaussian process as a sum of two independent processes: one spanned by a finite basis of inducing points and the other capturing the remaining variation. We show that this formulation recovers existing approximations and at the same time allows to obtain tighter lower bounds on the marginal likelihood and new stochastic variational inference algorithms. We demonstrate the efficiency of these algorithms in several Gaussian process models ranging from standard regression to multi-class classification using (deep) convolutional Gaussian processes and report state-of-the-art results on CIFAR-10 among purely GP-based models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#20215;&#21644;&#20108;&#20215;&#25293;&#21334;&#30340;&#32463;&#27982;&#32467;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#26469;&#26377;&#25928;&#35782;&#21035;&#23454;&#26102;&#31454;&#20215;&#24191;&#21578;&#30340;&#25928;&#26524;&#65292;&#26368;&#23567;&#21270;&#23454;&#39564;&#25104;&#26412;&#65292;&#24182;&#33719;&#24471;&#20102;&#39034;&#24207;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/1908.08600</link><description>&lt;p&gt;
&#23454;&#26102;&#31454;&#20215;&#24191;&#21578;&#20013;&#30340;&#22312;&#32447;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Online Causal Inference for Advertising in Real-Time Bidding Auctions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1908.08600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#20215;&#21644;&#20108;&#20215;&#25293;&#21334;&#30340;&#32463;&#27982;&#32467;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#26469;&#26377;&#25928;&#35782;&#21035;&#23454;&#26102;&#31454;&#20215;&#24191;&#21578;&#30340;&#25928;&#26524;&#65292;&#26368;&#23567;&#21270;&#23454;&#39564;&#25104;&#26412;&#65292;&#24182;&#33719;&#24471;&#20102;&#39034;&#24207;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#31454;&#20215;&#65288;RTB&#65289;&#31995;&#32479;&#36890;&#36807;&#25293;&#21334;&#23558;&#29992;&#25143;&#26333;&#20809;&#20998;&#37197;&#32473;&#31454;&#20105;&#23545;&#25163;&#30340;&#24191;&#21578;&#21830;&#65292;&#32487;&#32493;&#22312;&#25968;&#23383;&#24191;&#21578;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#12290;&#35780;&#20272;&#36825;&#31181;&#24191;&#21578;&#30340;&#26377;&#25928;&#24615;&#22312;&#30740;&#31350;&#21644;&#23454;&#36341;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#36890;&#36807;&#36825;&#31181;&#26426;&#21046;&#36141;&#20080;&#30340;&#24191;&#21578;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#21033;&#29992;&#19968;&#20215;&#21644;&#20108;&#20215;&#25293;&#21334;&#30340;&#32463;&#27982;&#32467;&#26500;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#24191;&#21578;&#25928;&#26524;&#30001;&#26368;&#20339;&#20986;&#20215;&#30830;&#23450;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#36825;&#20123;&#26368;&#20339;&#20986;&#20215;&#26159;&#21807;&#19968;&#38656;&#35201;&#24674;&#22797;&#30340;&#23545;&#35937;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;TS&#65289;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#19968;&#20010;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#25104;&#21151;&#24674;&#22797;&#36825;&#20123;&#20986;&#20215;&#21644;&#22240;&#27492;&#24191;&#21578;&#25928;&#26524;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23454;&#39564;&#25104;&#26412;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#31639;&#27861;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#26159;&#39034;&#24207;&#26368;&#20248;&#30340;&#65292;&#24182;&#21033;&#29992;RTB&#25293;&#21334;&#25968;&#25454;&#23637;&#31034;&#20102;&#23427;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1908.08600v4 Announce Type: replace  Abstract: Real-time bidding (RTB) systems, which utilize auctions to allocate user impressions to competing advertisers, continue to enjoy success in digital advertising. Assessing the effectiveness of such advertising remains a challenge in research and practice. This paper proposes a new approach to perform causal inference on advertising bought through such mechanisms. Leveraging the economic structure of first- and second-price auctions, we first show that the effects of advertising are identified by the optimal bids. Hence, since these optimal bids are the only objects that need to be recovered, we introduce an adapted Thompson sampling (TS) algorithm to solve a multi-armed bandit problem that succeeds in recovering such bids and, consequently, the effects of advertising while minimizing the costs of experimentation. We derive a regret bound for our algorithm which is order optimal and use data from RTB auctions to show that it outperform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#38789;&#30456;&#20851;&#25110;&#21487;&#20132;&#25442;&#38543;&#26426;&#23545;&#31216;&#30697;&#38453;&#30340;&#26032;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#36825;&#20123;&#19981;&#31561;&#24335;&#22312;&#22810;&#31181;&#23614;&#26465;&#20214;&#19979;&#25104;&#31435;&#65292;&#22312;&#27931;&#20234;&#32435;&#39034;&#24207;&#34920;&#31034;&#65292;&#24182;&#19988;&#26377;&#26102;&#22312;&#20219;&#24847;&#25968;&#25454;&#30456;&#20851;&#20572;&#27490;&#26102;&#38388;&#37117;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15567</link><description>&lt;p&gt;
&#30697;&#38453;&#36229;&#38789;&#21644;&#38543;&#26426;&#30697;&#38453;&#38598;&#20013;&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
Matrix Supermartingales and Randomized Matrix Concentration Inequalities. (arXiv:2401.15567v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#38789;&#30456;&#20851;&#25110;&#21487;&#20132;&#25442;&#38543;&#26426;&#23545;&#31216;&#30697;&#38453;&#30340;&#26032;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#36825;&#20123;&#19981;&#31561;&#24335;&#22312;&#22810;&#31181;&#23614;&#26465;&#20214;&#19979;&#25104;&#31435;&#65292;&#22312;&#27931;&#20234;&#32435;&#39034;&#24207;&#34920;&#31034;&#65292;&#24182;&#19988;&#26377;&#26102;&#22312;&#20219;&#24847;&#25968;&#25454;&#30456;&#20851;&#20572;&#27490;&#26102;&#38388;&#37117;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#22810;&#31181;&#23614;&#26465;&#20214;&#19979;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#38789;&#30456;&#20851;&#25110;&#21487;&#20132;&#25442;&#38543;&#26426;&#23545;&#31216;&#30697;&#38453;&#30340;&#26032;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#21253;&#25324;&#26631;&#20934;&#30340;&#20999;&#23572;&#35834;&#22827;&#19978;&#30028;&#21644;&#33258;&#24402;&#19968;&#21270;&#37325;&#23614;&#35774;&#32622;&#12290;&#36825;&#20123;&#19981;&#31561;&#24335;&#36890;&#24120;&#20197;&#27931;&#20234;&#32435;&#39034;&#24207;&#34920;&#31034;&#65292;&#24182;&#19988;&#26377;&#26102;&#22312;&#20219;&#24847;&#25968;&#25454;&#30456;&#20851;&#20572;&#27490;&#26102;&#38388;&#37117;&#25104;&#31435;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#30697;&#38453;&#36229;&#38789;&#21644;&#26497;&#20540;&#19981;&#31561;&#24335;&#30340;&#29702;&#35770;&#65292;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present new concentration inequalities for either martingale dependent or exchangeable random symmetric matrices under a variety of tail conditions, encompassing standard Chernoff bounds to self-normalized heavy-tailed settings. These inequalities are often randomized in a way that renders them strictly tighter than existing deterministic results in the literature, are typically expressed in the Loewner order, and are sometimes valid at arbitrary data-dependent stopping times.  Along the way, we explore the theory of matrix supermartingales and maximal inequalities, potentially of independent interest.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#23398;&#20064;&#27969;&#24418;&#65292;&#24182;&#20351;&#29992;Ricci&#27969;&#20351;&#27969;&#24418;&#28508;&#31354;&#38388;&#36880;&#27493;&#36866;&#24212;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;PDE&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.14591</link><description>&lt;p&gt;
&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#26102;&#21464;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Ricci flow-guided autoencoders in learning time-dependent dynamics. (arXiv:2401.14591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14591
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#23398;&#20064;&#27969;&#24418;&#65292;&#24182;&#20351;&#29992;Ricci&#27969;&#20351;&#27969;&#24418;&#28508;&#31354;&#38388;&#36880;&#27493;&#36866;&#24212;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;PDE&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#20854;&#20013;&#27969;&#24418;&#28508;&#31354;&#38388;&#26681;&#25454;Ricci&#27969;&#21457;&#23637;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#22312;&#29289;&#29702;&#20449;&#24687;&#35774;&#32622;&#20013;&#27169;&#25311;Ricci&#27969;&#26469;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#21305;&#37197;&#27969;&#24418;&#37327;&#65292;&#20197;&#20415;&#23454;&#29616;Ricci&#27969;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#27969;&#24418;&#26159;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#23398;&#20064;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#35782;&#21035;&#20986;&#29702;&#24819;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#21516;&#26102;&#28436;&#21464;&#20063;&#33021;&#22312;&#38745;&#24577;&#26041;&#27861;&#19978;&#24341;&#36215;&#26356;&#23485;&#23481;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#31561;&#29702;&#24819;&#29305;&#24449;&#30340;PDE&#65292;&#24182;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#36827;&#34892;&#35823;&#24046;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a manifold-based autoencoder method for learning nonlinear dynamics in time, notably partial differential equations (PDEs), in which the manifold latent space evolves according to Ricci flow. This can be accomplished by simulating Ricci flow in a physics-informed setting, and manifold quantities can be matched so that Ricci flow is empirically achieved. With our methodology, the manifold is learned as part of the training procedure, so ideal geometries may be discerned, while the evolution simultaneously induces a more accommodating latent representation over static methods. We present our method on a range of numerical experiments consisting of PDEs that encompass desirable characteristics such as periodicity and randomness, remarking error on in-distribution and extrapolation scenarios.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.14142</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#32479;&#19968;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14142
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411; (CBM)&#65292;&#22312;&#20026;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#27010;&#24565;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26368;&#32456;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#32416;&#27491;&#19968;&#20010;&#39044;&#27979;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33016;&#37096;&#8221;&#65289;&#26080;&#27861;&#24110;&#21161;&#32416;&#27491;&#39640;&#24230;&#30456;&#20851;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33145;&#37096;&#8221;&#65289;&#65292;&#23548;&#33268;&#26368;&#32456;&#20934;&#30830;&#29575;&#19981;&#29702;&#24819;&#65307;&#23427;&#20204;&#26080;&#27861;&#33258;&#28982;&#22320;&#37327;&#21270;&#19981;&#21516;&#27010;&#24565;&#21644;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65288;&#20363;&#22914;&#23545;&#20110;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#26631;&#31614;&#8220;Kentucky Warbler&#8221;&#21644;&#27010;&#24565;&#8220;&#40657;&#33394;&#22068;&#24052;&#8221;&#30340;&#22270;&#20687;&#65292;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#21478;&#19968;&#20010;&#27010;&#24565;&#8220;&#40657;&#33394;&#20896;&#8221;&#30340;&#27010;&#29575;&#26159;&#22810;&#23569;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#40657;&#30418;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#26356;&#28145;&#23618;&#27425;&#30340;&#27934;&#23519;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;Energy-based Concept Bottleneck Models&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FlowDRO&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#27969;&#27169;&#22411;&#21644;Wasserstein&#36817;&#31471;&#26799;&#24230;&#27969;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#26356;&#22823;&#26679;&#26412;&#22823;&#23567;&#30340;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.19253</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Flow-based Distributionally Robust Optimization. (arXiv:2310.19253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19253
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FlowDRO&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#27969;&#27169;&#22411;&#21644;Wasserstein&#36817;&#31471;&#26799;&#24230;&#27969;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#26356;&#22823;&#26679;&#26412;&#22823;&#23567;&#30340;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FlowDRO&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#35201;&#27714;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#65288;&#20063;&#31216;&#20026;&#26368;&#19981;&#21033;&#20998;&#24067;&#65292;LFD&#65289;&#26159;&#36830;&#32493;&#30340;&#65292;&#20174;&#32780;&#20351;&#24471;&#31639;&#27861;&#33021;&#22815;&#21487;&#25193;&#23637;&#21040;&#20855;&#26377;&#26356;&#22823;&#26679;&#26412;&#22823;&#23567;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#23545;&#35825;&#23548;&#30340;&#40065;&#26834;&#31639;&#27861;&#30340;&#26356;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26080;&#38480;&#32500;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#65292;&#22312;&#25968;&#25454;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36830;&#32493;&#26102;&#38388;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;Wasserstein&#36817;&#31471;&#26799;&#24230;&#27969;&#31867;&#22411;&#30340;&#31639;&#27861;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36880;&#27493;&#35757;&#32451;&#22359;&#20869;&#30340;&#31070;&#32463;&#32593;&#32476;&#24207;&#21015;&#26469;&#21442;&#25968;&#21270;&#20256;&#36755;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#26694;&#26550;&#36890;&#29992;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#21644;&#22823;&#26679;&#26412;&#22823;&#23567;&#65292;&#24182;&#21487;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a computationally efficient framework, called \texttt{FlowDRO}, for solving flow-based distributionally robust optimization (DRO) problems with Wasserstein uncertainty sets, when requiring the worst-case distribution (also called the Least Favorable Distribution, LFD) to be continuous so that the algorithm can be scalable to problems with larger sample sizes and achieve better generalization capability for the induced robust algorithms. To tackle the computationally challenging infinitely dimensional optimization problem, we leverage flow-based models, continuous-time invertible transport maps between the data distribution and the target distribution, and develop a Wasserstein proximal gradient flow type of algorithm. In practice, we parameterize the transport maps by a sequence of neural networks progressively trained in blocks by gradient descent. Our computational framework is general, can handle high-dimensional data with large sample sizes, and can be useful for various
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#24378;&#30340;&#31616;&#21333;&#38750;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;GraphACL&#65292;&#36890;&#36807;&#32771;&#34385;&#37051;&#23621;&#33410;&#28857;&#30340;&#38750;&#23545;&#31216;&#35270;&#22270;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#21516;&#31867;&#21644;&#24322;&#31867;&#22270;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#23545;&#20110;&#24314;&#27169;&#24322;&#31867;&#22270;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.18884</link><description>&lt;p&gt;
&#26080;&#38656;&#22686;&#24378;&#30340;&#31616;&#21333;&#38750;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simple and Asymmetric Graph Contrastive Learning without Augmentations. (arXiv:2310.18884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#24378;&#30340;&#31616;&#21333;&#38750;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;GraphACL&#65292;&#36890;&#36807;&#32771;&#34385;&#37051;&#23621;&#33410;&#28857;&#30340;&#38750;&#23545;&#31216;&#35270;&#22270;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#21516;&#31867;&#21644;&#24322;&#31867;&#22270;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#23545;&#20110;&#24314;&#27169;&#24322;&#31867;&#22270;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#26174;&#31034;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GCL&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#21046;&#30340;&#22270;&#22686;&#24378;&#21644;&#21516;&#31867;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#36830;&#36890;&#33410;&#28857;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#31867;&#26631;&#31614;&#21644;&#19981;&#30456;&#20284;&#29305;&#24449;&#30340;&#24322;&#31867;&#22270;&#19978;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21516;&#31867;&#21644;&#24322;&#31867;&#22270;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#32771;&#34385;&#37051;&#23621;&#33410;&#28857;&#30340;&#38750;&#23545;&#31216;&#35270;&#22270;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#31216;&#20026;&#22270;&#30340;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;(GraphACL)&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19981;&#20381;&#36182;&#20110;&#22270;&#22686;&#24378;&#21644;&#21516;&#31867;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;GraphACL&#33021;&#22815;&#25429;&#25417;&#21333;&#36339;&#26412;&#22320;&#37051;&#22495;&#20449;&#24687;&#21644;&#21452;&#36339;&#21333;&#19968;&#30456;&#20284;&#24615;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#24314;&#27169;&#24322;&#31867;&#22270;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning (GCL) has shown superior performance in representation learning in graph-structured data. Despite their success, most existing GCL methods rely on prefabricated graph augmentation and homophily assumptions. Thus, they fail to generalize well to heterophilic graphs where connected nodes may have different class labels and dissimilar features. In this paper, we study the problem of conducting contrastive learning on homophilic and heterophilic graphs. We find that we can achieve promising performance simply by considering an asymmetric view of the neighboring nodes. The resulting simple algorithm, Asymmetric Contrastive Learning for Graphs (GraphACL), is easy to implement and does not rely on graph augmentations and homophily assumptions. We provide theoretical and empirical evidence that GraphACL can capture one-hop local neighborhood information and two-hop monophily similarity, which are both important for modeling heterophilic graphs. Experimental results s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#26469;&#25913;&#21892;&#20809;&#35889;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.18306</link><description>&lt;p&gt;
&#30417;&#30563;&#21644;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Supervised and Penalized Baseline Correction. (arXiv:2310.18306v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#26469;&#25913;&#21892;&#20809;&#35889;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#35889;&#27979;&#37327;&#21487;&#20197;&#26174;&#31034;&#30001;&#21560;&#25910;&#21644;&#25955;&#23556;&#25104;&#20998;&#28151;&#21512;&#24341;&#36215;&#30340;&#25197;&#26354;&#20809;&#35889;&#24418;&#29366;&#12290;&#36825;&#20123;&#25197;&#26354;&#65288;&#25110;&#22522;&#32447;&#65289;&#36890;&#24120;&#34920;&#29616;&#20026;&#38750;&#24658;&#23450;&#20559;&#31227;&#25110;&#20302;&#39057;&#25391;&#33633;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#22522;&#32447;&#21487;&#33021;&#23545;&#20998;&#26512;&#21644;&#23450;&#37327;&#32467;&#26524;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22522;&#32447;&#26657;&#27491;&#26159;&#19968;&#20010;&#28085;&#30422;&#20102;&#39044;&#22788;&#29702;&#26041;&#27861;&#30340;&#24635;&#31216;&#65292;&#36890;&#36807;&#33719;&#21462;&#22522;&#32447;&#20809;&#35889;&#65288;&#19981;&#38656;&#35201;&#30340;&#25197;&#26354;&#65289;&#24182;&#36890;&#36807;&#24046;&#24322;&#21270;&#21435;&#38500;&#25197;&#26354;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#21363;&#20351;&#21487;&#29992;&#20998;&#26512;&#29289;&#27987;&#24230;&#25110;&#32773;&#23427;&#20204;&#23545;&#35266;&#23519;&#21040;&#30340;&#20809;&#35889;&#21464;&#24322;&#26377;&#37325;&#35201;&#36129;&#29486;&#65292;&#20063;&#27809;&#26377;&#21033;&#29992;&#23427;&#20204;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#65289;&#24182;&#23545;&#20854;&#36827;&#34892;&#20462;&#25913;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#23558;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#24615;&#33021;&#65292;&#21253;&#25324;&#32463;&#20856;&#21463;&#32602;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectroscopic measurements can show distorted spectra shapes arising from a mixture of absorbing and scattering contributions. These distortions (or baselines) often manifest themselves as non-constant offsets or low-frequency oscillations. As a result, these baselines can adversely affect analytical and quantitative results. Baseline correction is an umbrella term where one applies pre-processing methods to obtain baseline spectra (the unwanted distortions) and then remove the distortions by differencing. However, current state-of-the art baseline correction methods do not utilize analyte concentrations even if they are available, or even if they contribute significantly to the observed spectral variability. We examine a class of state-of-the-art methods (penalized baseline correction) and modify them such that they can accommodate a priori analyte concentration such that prediction can be enhanced. Performance will be access on two near infra-red data sets across both classical penal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26641;&#24230;&#37327;&#26377;&#22122;&#22768;&#30340;&#20248;&#21270;&#20256;&#36755;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#26641;&#32467;&#26500;&#25200;&#21160;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13653</link><description>&lt;p&gt;
&#37319;&#29992;&#26377;&#22122;&#22768;&#26641;&#24230;&#37327;&#30340;&#20248;&#21270;&#20256;&#36755;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Measures with Noisy Tree Metric. (arXiv:2310.13653v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26641;&#24230;&#37327;&#26377;&#22122;&#22768;&#30340;&#20248;&#21270;&#20256;&#36755;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#26641;&#32467;&#26500;&#25200;&#21160;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26641;&#24230;&#37327;&#31354;&#38388;&#19978;&#25903;&#25345;&#30340;&#27010;&#29575;&#27979;&#24230;&#30340;&#20248;&#21270;&#20256;&#36755;&#65288;OT&#65289;&#38382;&#39064;&#12290;&#24050;&#30693;&#36825;&#31181;OT&#38382;&#39064;&#65288;&#21363;&#26641;-&#29926;&#29926;&#26031;&#22374;&#65288;TW&#65289;&#65289;&#20855;&#26377;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#65292;&#20294;&#22522;&#26412;&#19978;&#21462;&#20915;&#20110;&#36755;&#20837;&#27979;&#24230;&#25903;&#25345;&#19978;&#30340;&#24213;&#23618;&#26641;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#25805;&#20316;&#20013;&#65292;&#30001;&#20110;&#22122;&#22768;&#25110;&#23545;&#25239;&#24615;&#27979;&#37327;&#65292;&#32473;&#23450;&#30340;&#26641;&#32467;&#26500;&#21487;&#33021;&#20250;&#34987;&#25200;&#21160;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#26368;&#22823;-&#26368;&#23567;&#40065;&#26834;OT&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#22312;&#19968;&#20010;&#26641;&#24230;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#19978;&#20004;&#20010;&#36755;&#20837;&#27979;&#24230;&#20043;&#38388;&#30340;&#26368;&#22823;&#21487;&#33021;&#36317;&#31163;&#12290;&#24635;&#20307;&#19978;&#35828;&#65292;&#30001;&#20110;&#20854;&#38750;&#20984;&#24615;&#21644;&#38750;&#20809;&#28369;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#35745;&#31639;&#65292;&#21363;&#20415;&#26159;&#22312;&#25903;&#25345;&#20026;1&#32500;&#31354;&#38388;&#30340;&#27979;&#24230;&#24773;&#20917;&#19979;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#35268;&#27169;&#24773;&#26223;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#36793;&#32536;&#21024;&#38500;/&#28155;&#21152;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26641;&#24230;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#36825;&#20010;&#38598;&#21512;&#22312;&#19968;&#20010;&#20248;&#38597;&#30340;&#26694;&#26550;&#19979;&#28085;&#30422;&#20102;&#22810;&#26679;&#30340;&#26641;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study optimal transport (OT) problem for probability measures supported on a tree metric space. It is known that such OT problem (i.e., tree-Wasserstein (TW)) admits a closed-form expression, but depends fundamentally on the underlying tree structure over supports of input measures. In practice, the given tree structure may be, however, perturbed due to noisy or adversarial measurements. In order to mitigate this issue, we follow the max-min robust OT approach which considers the maximal possible distances between two input measures over an uncertainty set of tree metrics. In general, this approach is hard to compute, even for measures supported in $1$-dimensional space, due to its non-convexity and non-smoothness which hinders its practical applications, especially for large-scale settings. In this work, we propose \emph{novel uncertainty sets of tree metrics} from the lens of edge deletion/addition which covers a diversity of tree structures in an elegant framework. Consequently, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#23398;&#20064;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#20855;&#26377;&#29305;&#23450;&#22870;&#21169;&#21644;&#27491;&#21017;&#21270;&#22120;&#32467;&#26500;&#30340;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29983;&#25104;&#27969;&#32593;&#32476;&#35757;&#32451;&#20013;&#20855;&#26377;&#23454;&#38469;&#25928;&#29575;&#21644;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12934</link><description>&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#20316;&#20026;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks as Entropy-Regularized RL. (arXiv:2310.12934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#23398;&#20064;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#20855;&#26377;&#29305;&#23450;&#22870;&#21169;&#21644;&#27491;&#21017;&#21270;&#22120;&#32467;&#26500;&#30340;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29983;&#25104;&#27969;&#32593;&#32476;&#35757;&#32451;&#20013;&#20855;&#26377;&#23454;&#38469;&#25928;&#29575;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;(GFlowNets)&#26159;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;&#20197;&#20415;&#26679;&#26412;&#20855;&#26377;&#19982;&#32473;&#23450;&#22870;&#21169;&#25104;&#27604;&#20363;&#30340;&#32452;&#21512;&#31163;&#25955;&#23545;&#35937;&#30340;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#21160;&#20316;&#12290; GFlowNets&#21033;&#29992;&#38382;&#39064;&#30340;&#24207;&#21015;&#24615;&#36136;&#65292;&#19982;&#24378;&#21270;&#23398;&#20064;(RL)&#36827;&#34892;&#31867;&#27604;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;RL&#21644;GFlowNets&#20043;&#38388;&#30340;&#32852;&#31995;&#25193;&#23637;&#21040;&#20102;&#19968;&#33324;&#24773;&#20917;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#23398;&#20064;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#20219;&#21153;&#39640;&#25928;&#22320;&#37325;&#26032;&#23450;&#20041;&#20026;&#20855;&#26377;&#29305;&#23450;&#22870;&#21169;&#21644;&#27491;&#21017;&#21270;&#22120;&#32467;&#26500;&#30340;&#29109;&#27491;&#21017;&#21270;RL&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#26631;&#20934;&#30340;&#36719;RL&#31639;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#27010;&#29575;&#24314;&#27169;&#20219;&#21153;&#30340;GFlowNet&#35757;&#32451;&#65292;&#26469;&#35828;&#26126;&#36825;&#31181;&#37325;&#23450;&#20041;&#30340;&#23454;&#38469;&#25928;&#29575;&#12290;&#19982;&#20808;&#21069;&#25253;&#36947;&#30340;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#34920;&#26126;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#19982;&#24050;&#26377;&#30340;GFlowNet&#35757;&#32451;&#26041;&#27861;&#31454;&#20105;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20010;&#35266;&#28857;&#20026;&#23558;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#34701;&#20837;&#23454;&#38469;&#38382;&#39064;&#25552;&#20379;&#20102;&#30452;&#25509;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently proposed generative flow networks (GFlowNets) are a method of training a policy to sample compositional discrete objects with probabilities proportional to a given reward via a sequence of actions. GFlowNets exploit the sequential nature of the problem, drawing parallels with reinforcement learning (RL). Our work extends the connection between RL and GFlowNets to a general case. We demonstrate how the task of learning a generative flow network can be efficiently redefined as an entropy-regularized RL problem with a specific reward and regularizer structure. Furthermore, we illustrate the practical efficiency of this reformulation by applying standard soft RL algorithms to GFlowNet training across several probabilistic modeling tasks. Contrary to previously reported results, we show that entropic RL approaches can be competitive against established GFlowNet training methods. This perspective opens a direct path for integrating reinforcement learning principles into the real
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;&#65288;NDMs&#65289;&#65292;&#23427;&#26159;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#23450;&#20041;&#21644;&#23398;&#20064;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#26080;&#38656;&#27169;&#25311;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#21464;&#20998;&#30028;&#23545;NDMs&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21487;&#23398;&#20064;&#21464;&#25442;&#30340;NDMs&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08337</link><description>&lt;p&gt;
&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Diffusion Models. (arXiv:2310.08337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;&#65288;NDMs&#65289;&#65292;&#23427;&#26159;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#23450;&#20041;&#21644;&#23398;&#20064;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#26080;&#38656;&#27169;&#25311;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#21464;&#20998;&#30028;&#23545;NDMs&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21487;&#23398;&#20064;&#21464;&#25442;&#30340;NDMs&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35768;&#22810;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#21482;&#20801;&#35768;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#32447;&#24615;&#36716;&#25442;&#65292;&#21463;&#21040;&#20102;&#19968;&#23450;&#30340;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26356;&#24191;&#27867;&#30340;&#21464;&#25442;&#23478;&#26063;&#21487;&#33021;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;&#29983;&#25104;&#20998;&#24067;&#65292;&#31616;&#21270;&#36870;&#36807;&#31243;&#24182;&#32553;&#23567;&#30495;&#23454;&#36127;&#23545;&#25968;&#20284;&#28982;&#21644;&#21464;&#20998;&#36817;&#20284;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;&#65288;NDMs&#65289;&#65292;&#23427;&#26159;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#23450;&#20041;&#21644;&#23398;&#20064;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#26080;&#38656;&#27169;&#25311;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#21464;&#20998;&#30028;&#23545;NDMs&#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;NDMs&#30340;&#26102;&#38388;&#36830;&#32493;&#24418;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#29616;&#25104;&#30340;&#25968;&#20540;ODE&#21644;SDE&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#24555;&#36895;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#21487;&#23398;&#20064;&#21464;&#25442;&#30340;NDMs&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimise NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs with learnable transformations through experiments on standard image ge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Ground-A-Video &#30340;&#22522;&#20110;&#24341;&#23548;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#23646;&#24615;&#35270;&#39057;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36755;&#20837;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#30340;&#22810;&#23646;&#24615;&#32534;&#36753;&#65292;&#24182;&#19988;&#35299;&#20915;&#20102;&#20854;&#20182;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01107</link><description>&lt;p&gt;
Ground-A-Video: &#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models. (arXiv:2310.01107v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Ground-A-Video &#30340;&#22522;&#20110;&#24341;&#23548;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#23646;&#24615;&#35270;&#39057;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36755;&#20837;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#30340;&#22810;&#23646;&#24615;&#32534;&#36753;&#65292;&#24182;&#19988;&#35299;&#20915;&#20102;&#20854;&#20182;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#39057;&#32534;&#36753;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#26524;&#65292;&#23454;&#29616;&#20102;&#21333;&#23646;&#24615;&#32534;&#36753;&#25110;&#39118;&#26684;&#20256;&#36882;&#30340;&#20219;&#21153;&#65292;&#19981;&#35770;&#36890;&#36807;&#22312;&#25991;&#26412;-&#35270;&#39057;&#25968;&#25454;&#19978;&#35757;&#32451;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#27169;&#22411;&#36824;&#26159;&#37319;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#22810;&#23646;&#24615;&#32534;&#36753;&#24773;&#26223;&#30340;&#22797;&#26434;&#24615;&#26102;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#27604;&#22914;&#24573;&#30053;&#25110;&#24573;&#35270;&#25152;&#26399;&#26395;&#30340;&#23646;&#24615;&#21464;&#21270;&#65292;&#20462;&#25913;&#36755;&#20837;&#35270;&#39057;&#30340;&#38169;&#35823;&#20803;&#32032;&#65292;&#20197;&#21450;&#26080;&#27861;&#20445;&#30041;&#24212;&#35813;&#20445;&#25345;&#21407;&#26679;&#30340;&#36755;&#20837;&#35270;&#39057;&#21306;&#22495;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24341;&#23548;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26694;&#26550;&#65292;&#21517;&#20026; Ground-A-Video&#65292;&#29992;&#20110;&#22810;&#23646;&#24615;&#35270;&#39057;&#32534;&#36753;&#12290;Ground-A-Video&#20197;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#36755;&#20837;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#30340;&#22810;&#23646;&#24615;&#32534;&#36753;&#65292;&#24182;&#19988;&#27809;&#26377;&#19978;&#36848;&#32570;&#28857;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#24341;&#20837;&#20102;&#20132;&#21449;&#24103;&#38376;&#25511;&#27880;&#24847;&#21147;&#65292;&#20197;&#19968;&#31181;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#26041;&#24335;&#23558;&#23450;&#20301;&#20449;&#24687;&#34701;&#20837;&#21040;&#28508;&#22312;&#34920;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent endeavors in video editing have showcased promising results in single-attribute editing or style transfer tasks, either by training text-to-video (T2V) models on text-video data or adopting training-free methods. However, when confronted with the complexities of multi-attribute editing scenarios, they exhibit shortcomings such as omitting or overlooking intended attribute changes, modifying the wrong elements of the input video, and failing to preserve regions of the input video that should remain intact. To address this, here we present a novel grounding-guided video-to-video translation framework called Ground-A-Video for multi-attribute video editing. Ground-A-Video attains temporally consistent multi-attribute editing of input videos in a training-free manner without aforementioned shortcomings. Central to our method is the introduction of Cross-Frame Gated Attention which incorporates groundings information into the latent representations in a temporally consistent fashion,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20445;&#24207;GFlowNets&#65288;OP-GFNs&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#19982;&#20505;&#36873;&#32773;&#30340;&#25490;&#24207;&#30456;&#19968;&#33268;&#30340;&#27010;&#29575;&#36827;&#34892;&#37319;&#26679;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#39044;&#23450;&#20041;&#26631;&#37327;&#22870;&#21169;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#35777;&#26126;&#35757;&#32451;&#36807;&#31243;&#31232;&#30095;&#22870;&#21169;&#26223;&#35266;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.00386</link><description>&lt;p&gt;
&#20445;&#24207;GFlowNets
&lt;/p&gt;
&lt;p&gt;
Order-Preserving GFlowNets. (arXiv:2310.00386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20445;&#24207;GFlowNets&#65288;OP-GFNs&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#19982;&#20505;&#36873;&#32773;&#30340;&#25490;&#24207;&#30456;&#19968;&#33268;&#30340;&#27010;&#29575;&#36827;&#34892;&#37319;&#26679;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#39044;&#23450;&#20041;&#26631;&#37327;&#22870;&#21169;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#35777;&#26126;&#35757;&#32451;&#36807;&#31243;&#31232;&#30095;&#22870;&#21169;&#26223;&#35266;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#34987;&#24341;&#20837;&#20316;&#20026;&#19968;&#31181;&#26681;&#25454;&#32473;&#23450;&#22870;&#21169;&#27010;&#29575;&#37319;&#26679;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#38598;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;GFlowNets&#21482;&#33021;&#19982;&#39044;&#23450;&#20041;&#30340;&#26631;&#37327;&#22870;&#21169;&#19968;&#36215;&#20351;&#29992;&#65292;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#20219;&#21153;&#20013;&#65292;&#36825;&#21487;&#33021;&#26159;&#35745;&#31639;&#26114;&#36149;&#30340;&#25110;&#32773;&#30452;&#25509;&#19981;&#21487;&#35775;&#38382;&#30340;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20248;&#20808;&#35782;&#21035;&#39640;&#22870;&#21169;&#20505;&#36873;&#32773;&#65292;&#20256;&#32479;&#20570;&#27861;&#26159;&#23558;&#22870;&#21169;&#25552;&#39640;&#21040;&#26356;&#39640;&#30340;&#25351;&#25968;&#65292;&#32780;&#36825;&#20010;&#26368;&#20248;&#36873;&#25321;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#24207;GFlowNets&#65288;OP-GFNs&#65289;&#65292;&#23427;&#20204;&#20197;&#19982;&#25552;&#20379;&#30340;&#65288;&#37096;&#20998;&#65289;&#20505;&#36873;&#32773;&#25490;&#24207;&#19968;&#33268;&#30340;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#27010;&#29575;&#36827;&#34892;&#37319;&#26679;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#22870;&#21169;&#20989;&#25968;&#30340;&#26174;&#24335;&#34920;&#36798;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;OP-GFNs&#30340;&#35757;&#32451;&#36807;&#31243;&#36880;&#28176;&#31232;&#30095;&#20102;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#26223;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates with probabilities proportional to a given reward. However, GFlowNets can only be used with a predefined scalar reward, which can be either computationally expensive or not directly accessible, in the case of multi-objective optimization (MOO) tasks for example. Moreover, to prioritize identifying high-reward candidates, the conventional practice is to raise the reward to a higher exponent, the optimal choice of which may vary across different environments. To address these issues, we propose Order-Preserving GFlowNets (OP-GFNs), which sample with probabilities in proportion to a learned reward function that is consistent with a provided (partial) order on the candidates, thus eliminating the need for an explicit formulation of the reward function. We theoretically prove that the training process of OP-GFNs gradually sparsifies the learned reward landscape in single-objective max
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#21069;K&#31232;&#30095;softmax&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;&#22312;&#23494;&#24230;&#21644;&#21442;&#25968;&#20272;&#35745;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#23450;&#20041;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25506;&#35752;&#20102;&#36755;&#20837;&#21306;&#22495;&#30340;&#19981;&#21516;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#30495;&#23454;&#19987;&#23478;&#25968;&#37327;&#24050;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#23494;&#24230;&#21644;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#26679;&#26412;&#37327;&#25104;&#27491;&#27604;&#65292;&#20294;&#24403;&#30495;&#23454;&#27169;&#24335;&#26410;&#30693;&#26102;</title><link>http://arxiv.org/abs/2309.13850</link><description>&lt;p&gt;
&#32479;&#35745;&#35282;&#24230;&#19979;&#30340;&#21069;K&#31232;&#30095;Softmax&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts. (arXiv:2309.13850v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13850
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#21069;K&#31232;&#30095;softmax&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;&#22312;&#23494;&#24230;&#21644;&#21442;&#25968;&#20272;&#35745;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#23450;&#20041;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25506;&#35752;&#20102;&#36755;&#20837;&#21306;&#22495;&#30340;&#19981;&#21516;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#30495;&#23454;&#19987;&#23478;&#25968;&#37327;&#24050;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#23494;&#24230;&#21644;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#26679;&#26412;&#37327;&#25104;&#27491;&#27604;&#65292;&#20294;&#24403;&#30495;&#23454;&#27169;&#24335;&#26410;&#30693;&#26102;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;K&#31232;&#30095;softmax&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;&#34987;&#24191;&#27867;&#29992;&#20110;&#22312;&#19981;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#23613;&#31649;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#23545;&#35813;&#38376;&#25511;&#20989;&#25968;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20027;&#35201;&#25361;&#25112;&#26469;&#33258;&#20110;&#21069;K&#31232;&#30095;softmax&#38376;&#25511;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#23427;&#23558;&#36755;&#20837;&#31354;&#38388;&#21010;&#20998;&#20026;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#30340;&#22810;&#20010;&#21306;&#22495;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#65292;&#25105;&#20204;&#23545;&#21069;K&#31232;&#30095;softmax&#38376;&#25511;&#20989;&#25968;&#23545;&#23494;&#24230;&#21644;&#21442;&#25968;&#20272;&#35745;&#30340;&#24433;&#21709;&#24314;&#31435;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20381;&#36182;&#20110;&#23450;&#20041;&#21442;&#25968;&#20043;&#38388;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25429;&#25417;&#36755;&#20837;&#21306;&#22495;&#30340;&#19981;&#21516;&#34892;&#20026;&#12290;&#24403;&#30495;&#23454;&#19987;&#23478;&#25968;&#37327;$k_{\ast}$&#24050;&#30693;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23494;&#24230;&#21644;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#24230;&#37117;&#19982;&#26679;&#26412;&#37327;&#25104;&#27491;&#27604;&#12290;&#28982;&#32780;&#65292;&#24403;$k_{\ast}$&#21464;&#20026;&#26410;&#30693;&#19988;&#30495;&#23454;&#27169;&#24335;&#26102;
&lt;/p&gt;
&lt;p&gt;
Top-K sparse softmax gating mixture of experts has been widely used for scaling up massive deep-learning architectures without increasing the computational cost. Despite its popularity in real-world applications, the theoretical understanding of that gating function has remained an open problem. The main challenge comes from the structure of the top-K sparse softmax gating function, which partitions the input space into multiple regions with distinct behaviors. By focusing on a Gaussian mixture of experts, we establish theoretical results on the effects of the top-K sparse softmax gating function on both density and parameter estimations. Our results hinge upon defining novel loss functions among parameters to capture different behaviors of the input regions. When the true number of experts $k_{\ast}$ is known, we demonstrate that the convergence rates of density and parameter estimations are both parametric on the sample size. However, when $k_{\ast}$ becomes unknown and the true mode
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21367;&#31215;&#28145;&#24230;&#26680;&#26426;&#22120;&#30340;&#26032;&#22411;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32431;&#31929;&#20351;&#29992;&#26680;&#32780;&#19981;&#20351;&#29992;&#29305;&#24449;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#36328;&#22495;&#35825;&#23548;&#28857;&#36817;&#20284;&#26041;&#26696;&#21644;&#22810;&#31181;&#27169;&#22411;&#21464;&#20307;&#30340;&#35774;&#35745;&#65292;&#36798;&#21040;&#20102;&#22312;MNIST&#12289;CIFAR-10&#21644;CIFAR-100&#19978;&#25509;&#36817;&#29978;&#33267;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.09814</link><description>&lt;p&gt;
&#21367;&#31215;&#28145;&#24230;&#26680;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Convolutional Deep Kernel Machines. (arXiv:2309.09814v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09814
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21367;&#31215;&#28145;&#24230;&#26680;&#26426;&#22120;&#30340;&#26032;&#22411;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32431;&#31929;&#20351;&#29992;&#26680;&#32780;&#19981;&#20351;&#29992;&#29305;&#24449;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#36328;&#22495;&#35825;&#23548;&#28857;&#36817;&#20284;&#26041;&#26696;&#21644;&#22810;&#31181;&#27169;&#22411;&#21464;&#20307;&#30340;&#35774;&#35745;&#65292;&#36798;&#21040;&#20102;&#22312;MNIST&#12289;CIFAR-10&#21644;CIFAR-100&#19978;&#25509;&#36817;&#29978;&#33267;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26680;&#26426;&#22120;(DKMs)&#26159;&#19968;&#31181;&#26368;&#36817;&#24341;&#20837;&#30340;&#20855;&#26377;&#20854;&#20182;&#28145;&#24230;&#27169;&#22411;&#28789;&#27963;&#24615;&#30340;&#26680;&#26041;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#12290;DKMs&#32431;&#31929;&#20351;&#29992;&#26680;&#65292;&#32780;&#19981;&#20351;&#29992;&#29305;&#24449;&#65292;&#22240;&#27492;&#19982;&#20854;&#20182;&#26041;&#27861;&#65288;&#20174;&#31070;&#32463;&#32593;&#32476;&#21040;&#28145;&#24230;&#26680;&#23398;&#20064;&#29978;&#33267;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#65289;&#19981;&#21516;&#65292;&#21518;&#32773;&#37117;&#20351;&#29992;&#29305;&#24449;&#20316;&#20026;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21367;&#31215;DKMs&#65292;&#24182;&#37197;&#20197;&#19968;&#31181;&#39640;&#25928;&#30340;&#36328;&#22495;&#35825;&#23548;&#28857;&#36817;&#20284;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#24182;&#23454;&#39564;&#35780;&#20272;&#20102;&#35768;&#22810;&#27169;&#22411;&#21464;&#20307;&#65292;&#21253;&#25324;9&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20026;&#21367;&#31215;DKMs&#35774;&#35745;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#20004;&#31181;&#20284;&#28982;&#20989;&#25968;&#21644;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#39030;&#23618;&#12290;&#23613;&#31649;&#21482;&#22312;&#32422;28&#20010;GPU&#23567;&#26102;&#20869;&#35757;&#32451;&#65288;&#27604;&#23436;&#20840;&#30340;NNGP / NTK / Myrtle kernel&#24555;1-2&#20010;&#25968;&#37327;&#32423;&#65289;&#65292;&#20294;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;MNIST&#19978;&#23454;&#29616;&#20102;&#32422;99&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#65292;&#22312;CIFAR-10&#19978;&#20026;92&#65285;&#65292;&#22312;CIFAR-100&#19978;&#20026;71&#65285;&#65292;&#21516;&#26102;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep kernel machines (DKMs) are a recently introduced kernel method with the flexibility of other deep models including deep NNs and deep Gaussian processes. DKMs work purely with kernels, never with features, and are therefore different from other methods ranging from NNs to deep kernel learning and even deep Gaussian processes, which all use features as a fundamental component. Here, we introduce convolutional DKMs, along with an efficient inter-domain inducing point approximation scheme. Further, we develop and experimentally assess a number of model variants, including 9 different types of normalisation designed for the convolutional DKMs, two likelihoods, and two different types of top-layer. The resulting models achieve around 99% test accuracy on MNIST, 92% on CIFAR-10 and 71% on CIFAR-100, despite training in only around 28 GPU hours, 1-2 orders of magnitude faster than full NNGP / NTK / Myrtle kernels, whilst achieving comparable performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#28176;&#21464;&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#20174;&#27169;&#24335;&#35782;&#21035;&#21040;&#20915;&#31574;&#21644;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#28041;&#21450;&#22343;&#34913;&#21644;&#21338;&#24328;&#35770;&#30340;&#25968;&#23398;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#20110;&#25552;&#20379;&#21160;&#26426;&#21644;&#30452;&#35266;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.04877</link><description>&lt;p&gt;
&#28176;&#21464;&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28201;&#21644;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
A Gentle Introduction to Gradient-Based Optimization and Variational Inequalities for Machine Learning. (arXiv:2309.04877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04877
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#28176;&#21464;&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#20174;&#27169;&#24335;&#35782;&#21035;&#21040;&#20915;&#31574;&#21644;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#28041;&#21450;&#22343;&#34913;&#21644;&#21338;&#24328;&#35770;&#30340;&#25968;&#23398;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#20110;&#25552;&#20379;&#21160;&#26426;&#21644;&#30452;&#35266;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#22522;&#20110;&#19982;&#28176;&#21464;&#20248;&#21270;&#30340;&#32039;&#23494;&#32852;&#31995;&#12290;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#37096;&#20998;&#21462;&#20915;&#20110;&#20174;&#27169;&#24335;&#35782;&#21035;&#21040;&#20915;&#31574;&#21644;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#30340;&#36716;&#21464;&#12290;&#22312;&#36825;&#20123;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#65292;&#28041;&#21450;&#22343;&#34913;&#21644;&#21338;&#24328;&#35770;&#32780;&#19981;&#26159;&#26497;&#20540;&#30340;&#26032;&#30340;&#25968;&#23398;&#25361;&#25112;&#20986;&#29616;&#20102;&#12290;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;--&#32771;&#34385;&#21040;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#39640;&#32500;&#24230;&#21644;&#22823;&#35268;&#27169;--&#20294;&#31616;&#21333;&#30340;&#26799;&#24230;&#19979;&#38477;&#19981;&#20877;&#26159;&#31639;&#27861;&#35774;&#35745;&#30340;&#20986;&#21457;&#28857;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#30340;&#26356;&#24191;&#27867;&#26694;&#26550;&#30340;&#28201;&#21644;&#20171;&#32461;&#65292;&#20174;&#38797;&#28857;&#21644;&#21333;&#35843;&#21338;&#24328;&#24320;&#22987;&#65292;&#28982;&#21518;&#21040;&#19968;&#33324;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#12290;&#34429;&#28982;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#20960;&#20010;&#31639;&#27861;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#20294;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#25552;&#20379;&#21160;&#26426;&#21644;&#30452;&#35266;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress in machine learning in recent years has been based on a highly productive connection to gradient-based optimization. Further progress hinges in part on a shift in focus from pattern recognition to decision-making and multi-agent problems. In these broader settings, new mathematical challenges emerge that involve equilibria and game theory instead of optima. Gradient-based methods remain essential -- given the high dimensionality and large scale of machine-learning problems -- but simple gradient descent is no longer the point of departure for algorithm design. We provide a gentle introduction to a broader framework for gradient-based algorithms in machine learning, beginning with saddle points and monotone games, and proceeding to general variational inequalities. While we provide convergence proofs for several of the algorithms that we present, our main focus is that of providing motivation and intuition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#24046;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#31163;&#25955;&#20998;&#24067;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#31163;&#25955;&#19988;&#20445;&#25345;&#24230;&#37327;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#38656;&#35201;&#36830;&#32493;&#23884;&#20837;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#36830;&#32493;&#23884;&#20837;&#27969;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2308.15613</link><description>&lt;p&gt;
&#28151;&#21512;&#26041;&#24046;&#27969;&#29992;&#20110;&#31163;&#25955;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Mixed Variational Flows for Discrete Variables. (arXiv:2308.15613v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#24046;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#31163;&#25955;&#20998;&#24067;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#31163;&#25955;&#19988;&#20445;&#25345;&#24230;&#37327;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#38656;&#35201;&#36830;&#32493;&#23884;&#20837;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#36830;&#32493;&#23884;&#20837;&#27969;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#27969;&#20801;&#35768;&#20174;&#20107;&#32773;&#23398;&#20064;&#22797;&#26434;&#30340;&#36830;&#32493;&#20998;&#24067;&#65292;&#20294;&#26159;&#36817;&#20284;&#31163;&#25955;&#20998;&#24067;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#31163;&#25955;&#30446;&#26631;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#20013;-&#36890;&#24120;&#26159;&#36890;&#36807;&#36830;&#32493;&#26494;&#24347;&#25110;&#21435;&#37327;&#21270;-&#28982;&#21518;&#24212;&#29992;&#36830;&#32493;&#27969;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#28041;&#21450;&#19968;&#20010;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;&#21407;&#22987;&#31163;&#25955;&#30446;&#26631;&#30340;&#26367;&#20195;&#30446;&#26631;&#65292;&#21487;&#33021;&#20855;&#26377;&#20559;&#20506;&#25110;&#19981;&#31283;&#23450;&#30340;&#26799;&#24230;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#21019;&#24314;&#19968;&#20010;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;&#31163;&#25955;&#20998;&#24067;&#30340;&#21464;&#20998;&#27969;&#26063;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#36830;&#32493;&#23884;&#20837;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20445;&#25345;&#24230;&#37327;&#30340;&#31163;&#25955;&#21487;&#36870;&#26144;&#23556;&#65292;&#20351;&#31163;&#25955;&#30446;&#26631;&#20445;&#25345;&#19981;&#21464;&#65292;&#28982;&#21518;&#22522;&#20110;&#35813;&#26144;&#23556;&#21019;&#24314;&#20102;&#19968;&#20010;&#28151;&#21512;&#21464;&#20998;&#27969;(MAD Mix)&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#25193;&#23637;&#65292;&#29992;&#20110;&#22788;&#29702;&#32852;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MAD Mix&#20135;&#29983;&#20102;&#27604;&#36830;&#32493;&#23884;&#20837;&#27969;&#26356;&#21487;&#38752;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational flows allow practitioners to learn complex continuous distributions, but approximating discrete distributions remains a challenge. Current methodologies typically embed the discrete target in a continuous space - usually via continuous relaxation or dequantization - and then apply a continuous flow. These approaches involve a surrogate target that may not capture the original discrete target, might have biased or unstable gradients, and can create a difficult optimization problem. In this work, we develop a variational flow family for discrete distributions without any continuous embedding. First, we develop a measure-preserving and discrete (MAD) invertible map that leaves the discrete target invariant, and then create a mixed variational flow (MAD Mix) based on that map. We also develop an extension to MAD Mix that handles joint discrete and continuous models. Our experiments suggest that MAD Mix produces more reliable approximations than continuous-embedding flows while 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11838</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#26657;&#20934;&#38382;&#39064;&#65292;&#23613;&#31649;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#20351;&#29992;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#26694;&#26550;&#26469;&#25913;&#21892;&#26657;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#26657;&#20934;&#23646;&#24615;&#30340;&#30740;&#31350;&#26377;&#28857;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#20840;&#38754;&#25506;&#32034;&#26657;&#20934;&#23646;&#24615;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#23613;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#12290;&#25105;&#20204;&#29305;&#21035;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;NATS-Bench&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#20102;90&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#26657;&#20934;&#24230;&#37327;&#21644;12&#20010;&#20854;&#20182;&#26657;&#20934;&#24230;&#37327;&#65292;&#28085;&#30422;&#20102;117,702&#20010;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#22238;&#31572;&#35813;&#39046;&#22495;&#19968;&#20123;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27169;&#22411;&#26657;&#20934;&#33021;&#21542;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65311;&#65288;ii&#65289;&#33021;&#21542;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#20215;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#65292;&#36824;&#33021;&#22815;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#65292;&#21363;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;KGF&#21644;KRR&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;KRR&#27867;&#21270;&#65292;&#20351;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.16838</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Kernel Ridge Regression with Gradient-Based Optimization Methods. (arXiv:2306.16838v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#20215;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#65292;&#36824;&#33021;&#22815;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#65292;&#21363;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;KGF&#21644;KRR&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;KRR&#27867;&#21270;&#65292;&#20351;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#26159;&#32447;&#24615;&#23725;&#22238;&#24402;&#30340;&#38750;&#32447;&#24615;&#25512;&#24191;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KRR&#30446;&#26631;&#20989;&#25968;&#30340;&#31561;&#20215;&#24418;&#24335;&#65292;&#20026;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#21644;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#25171;&#24320;&#20102;&#21487;&#33021;&#12290;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#8212;&#8212;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#35753;&#25105;&#20204;&#33021;&#22815;&#22312;KGF&#21644;KRR&#20043;&#38388;&#29702;&#35770;&#19978;&#30028;&#23450;&#24046;&#24322;&#12290;&#25105;&#20204;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#23558;KRR&#27867;&#21270;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;KGF&#21644;KRR&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#29992;&#36825;&#20123;&#24809;&#32602;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#20351;&#29992;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#65288;&#20063;&#31216;&#20026;&#22352;&#26631;&#19979;&#38477;&#65289;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel ridge regression, KRR, is a non-linear generalization of linear ridge regression. Here, we introduce an equivalent formulation of the objective function of KRR, opening up both for using other penalties than the ridge penalty and for studying kernel ridge regression from the perspective of gradient descent. Using a continuous-time perspective, we derive a closed-form solution, kernel gradient flow, KGF, with regularization through early stopping, which allows us to theoretically bound the differences between KGF and KRR. We generalize KRR by replacing the ridge penalty with the $\ell_1$ and $\ell_\infty$ penalties and utilize the fact that analogously to the similarities between KGF and KRR, the solutions obtained when using these penalties are very similar to those obtained from forward stagewise regression (also known as coordinate descent) and sign gradient descent in combination with early stopping. Thus the need for computationally heavy proximal gradient descent algorithms
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22343;&#21248;&#30340;$k$-dag&#24191;&#25773;&#27169;&#22411;&#65292;&#30830;&#23450;&#20102;&#19982;$p$&#21644;$k$&#26377;&#20851;&#30340;&#38408;&#20540;&#65292;&#24182;&#35752;&#35770;&#20102;&#22823;&#22810;&#25968;&#35268;&#21017;&#30340;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01727</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#36882;&#24402;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#30340;&#24191;&#25773;
&lt;/p&gt;
&lt;p&gt;
Broadcasting in random recursive dags. (arXiv:2306.01727v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01727
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22343;&#21248;&#30340;$k$-dag&#24191;&#25773;&#27169;&#22411;&#65292;&#30830;&#23450;&#20102;&#19982;$p$&#21644;$k$&#26377;&#20851;&#30340;&#38408;&#20540;&#65292;&#24182;&#35752;&#35770;&#20102;&#22823;&#22810;&#25968;&#35268;&#21017;&#30340;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#22343;&#21248;&#30340;$k$-dag&#36890;&#36807;&#20174;&#29616;&#26377;&#33410;&#28857;&#20013;&#22343;&#21248;&#38543;&#26426;&#36873;&#25321;$k$&#20010;&#29238;&#33410;&#28857;&#26469;&#25512;&#24191;&#22343;&#21248;&#30340;&#38543;&#26426;&#36882;&#24402;&#26641;&#12290;&#23427;&#20197;$k$&#20010;&#8220;&#26681;&#8221;&#24320;&#22987;&#12290;&#27599;&#20010;$k$&#20010;&#26681;&#33410;&#28857;&#37117;&#34987;&#20998;&#37197;&#19968;&#20010;&#20301;&#12290;&#36825;&#20123;&#20301;&#36890;&#36807;&#19968;&#20010;&#22024;&#26434;&#30340;&#20449;&#36947;&#20256;&#25773;&#12290;&#27599;&#20010;&#29238;&#33410;&#28857;&#30340;&#20301;&#37117;&#20197;&#27010;&#29575;$p$&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#36827;&#34892;&#22823;&#22810;&#25968;&#34920;&#20915;&#12290;&#24403;&#25152;&#26377;&#33410;&#28857;&#37117;&#25509;&#25910;&#21040;&#23427;&#20204;&#30340;&#20301;&#21518;&#65292;$k$-dag&#34987;&#26174;&#31034;&#65292;&#19981;&#35782;&#21035;&#26681;&#33410;&#28857;&#12290;&#30446;&#26631;&#26159;&#20272;&#35745;&#25152;&#26377;&#26681;&#33410;&#28857;&#20013;&#30340;&#22823;&#22810;&#25968;&#20301;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;$p$&#30340;&#38408;&#20540;&#65292;&#20316;&#20026;&#19968;&#20010;&#20851;&#20110;$k$&#30340;&#20989;&#25968;&#65292;&#20351;&#24471;&#25152;&#26377;&#33410;&#28857;&#30340;&#22823;&#22810;&#25968;&#35268;&#21017;&#20135;&#29983;&#38169;&#35823;$c+o(1)$&#30340;&#27010;&#29575;&#23567;&#20110;$1/2$&#12290;&#22312;&#38408;&#20540;&#20197;&#19978;&#65292;&#22823;&#22810;&#25968;&#35268;&#21017;&#30340;&#38169;&#35823;&#27010;&#29575;&#20026;$1/2+o(1)$&#12290;
&lt;/p&gt;
&lt;p&gt;
A uniform $k$-{\sc dag} generalizes the uniform random recursive tree by picking $k$ parents uniformly at random from the existing nodes. It starts with $k$ ''roots''. Each of the $k$ roots is assigned a bit. These bits are propagated by a noisy channel. The parents' bits are flipped with probability $p$, and a majority vote is taken. When all nodes have received their bits, the $k$-{\sc dag} is shown without identifying the roots. The goal is to estimate the majority bit among the roots. We identify the threshold for $p$ as a function of $k$ below which the majority rule among all nodes yields an error $c+o(1)$ with $c&lt;1/2$. Above the threshold the majority rule errs with probability $1/2+o(1)$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#20248;&#21270;&#25439;&#22833;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00742</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Going Deeper with Spectral Embeddings. (arXiv:2306.00742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#20248;&#21270;&#25439;&#22833;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#22320;&#22788;&#29702;&#28023;&#37327;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#34920;&#24449;&#65292;&#31185;&#23398;&#23478;&#20204;&#37319;&#29992;&#34920;&#31034;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#19968;&#20123;&#24213;&#23618;&#36816;&#31639;&#30340;&#35889;&#20998;&#35299;&#20043;&#38388;&#23637;&#29616;&#20986;&#26126;&#26174;&#30340;&#32852;&#31995;&#12290;&#22312;&#21382;&#21490;&#19978;&#65292;&#26159;&#36890;&#36807;&#22312;&#25968;&#25454;&#30340;&#39030;&#37096;&#26500;&#24314;&#22270;&#24418;&#26469;&#24314;&#31435;&#26126;&#30830;&#30340;&#35889;&#23884;&#20837;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#26500;&#24314;&#30340;&#65292;&#36825;&#23558;&#23548;&#33268;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#20197;&#20248;&#21270;&#22522;&#26412;&#21464;&#20998;&#25439;&#22833;&#30340;&#31639;&#27861;&#65292;&#23427;&#20204;&#20135;&#29983;&#20102;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#26469;&#22312;&#19968;&#27493;&#20013;&#29983;&#25104;&#26032;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make sense of millions of raw data and represent them efficiently, practitioners rely on representation learning. Recently, deep connections have been shown between these approaches and the spectral decompositions of some underlying operators. Historically, explicit spectral embeddings were built from graphs constructed on top of the data. In contrast, we propose two new methods to build spectral embeddings: one based on functional analysis principles and kernel methods, which leads to algorithms with theoretical guarantees, and the other based on deep networks trained to optimize principled variational losses, which yield practically efficient algorithms. Furthermore, we provide a new sampling algorithm that leverages learned representations to generate new samples in a single step.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11854</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577; Web &#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027; Web &#23548;&#33322;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20381;&#36182;&#25968;&#21313;&#20159;&#27425;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;&#24615;&#20132;&#20114;&#21644;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#21033;&#29992;&#26469;&#33258;&#20016;&#23500;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33073;&#26426;&#35757;&#32451;&#65292;&#29992;&#20110;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340; Web &#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;&#65292; WebGUM&#65292;&#23427;&#35266;&#23519;&#20102;&#32593;&#39029;&#25130;&#22270;&#21644; HTML &#39029;&#38754;&#65292;&#24182;&#36755;&#20986; Web &#23548;&#33322;&#25805;&#20316;&#65292;&#22914;&#21333;&#20987;&#21644;&#36755;&#20837;&#12290;WebGUM &#26159;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#22312;&#22823;&#37327;&#30340;&#28436;&#31034;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312; MiniWoB &#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#36229;&#36807;&#20043;&#21069;&#26368;&#20339;&#33073;&#26426;&#26041;&#27861; 31.9% &#20197;&#19978;&#65292;&#25509;&#36817;&#23454;&#29616;&#22312;&#32447;&#20132;&#20114;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SVD-QCQP&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#30340;&#39640;&#25928;&#20869;&#26680;&#23398;&#20064;&#23454;&#29616;&#65292;&#29992;&#20110;&#23398;&#20064;&#21322;&#20998;&#31163;&#26680;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.07472</link><description>&lt;p&gt;
&#36890;&#29992;&#26680;&#23398;&#20064;&#30340;&#39640;&#25928;&#20984;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Convex Algorithms for Universal Kernel Learning. (arXiv:2304.07472v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SVD-QCQP&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#30340;&#39640;&#25928;&#20869;&#26680;&#23398;&#20064;&#23454;&#29616;&#65292;&#29992;&#20110;&#23398;&#20064;&#21322;&#20998;&#31163;&#26680;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26680;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#21462;&#20915;&#20110;&#23427;&#20204;&#33021;&#22815;&#20248;&#21270;&#30340;&#26680;&#38598;&#12290;&#29702;&#24819;&#30340;&#26680;&#38598;&#24212;&#35813;&#65306;&#20855;&#26377;&#32447;&#24615;&#21442;&#25968;&#21270;&#65288;&#20197;&#20415;&#20110;&#21487;&#22788;&#29702;&#24615;&#65289;&#65307;&#22312;&#25152;&#26377;&#26680;&#38598;&#20013;&#23494;&#38598;&#65288;&#20197;&#20415;&#20110;&#40065;&#26834;&#24615;&#65289;&#65307;&#26159;&#36890;&#29992;&#30340;&#65288;&#20197;&#20415;&#20110;&#20934;&#30830;&#24615;&#65289;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#20351;&#29992;&#27491;&#23450;&#30697;&#38453;&#26469;&#21442;&#25968;&#21270;&#19968;&#31867;&#27491;&#21322;&#20998;&#31163;&#26680;&#12290;&#23613;&#31649;&#27492;&#31867;&#26680;&#33021;&#22815;&#28385;&#36275;&#25152;&#26377;&#19977;&#20010;&#26631;&#20934;&#65292;&#20294;&#20043;&#21069;&#29992;&#20110;&#20248;&#21270;&#27492;&#31867;&#26680;&#30340;&#31639;&#27861;&#20165;&#38480;&#20110;&#20998;&#31867;&#65292;&#24182;&#19988;&#36824;&#20381;&#36182;&#20110;&#35745;&#31639;&#22797;&#26434;&#30340;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23398;&#20064;&#21322;&#20998;&#31163;&#26680;&#30340;&#38382;&#39064;&#20316;&#20026;&#26497;&#23567;&#21270;&#26497;&#22823;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;SVD-QCQP&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#65292;&#20854;&#19982;&#20043;&#21069;&#22522;&#20110;SDP&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20869;&#26680;&#23398;&#20064;&#23454;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy and complexity of machine learning algorithms based on kernel optimization are determined by the set of kernels over which they are able to optimize. An ideal set of kernels should: admit a linear parameterization (for tractability); be dense in the set of all kernels (for robustness); be universal (for accuracy). Recently, a framework was proposed for using positive matrices to parameterize a class of positive semi-separable kernels. Although this class can be shown to meet all three criteria, previous algorithms for optimization of such kernels were limited to classification and furthermore relied on computationally complex Semidefinite Programming (SDP) algorithms. In this paper, we pose the problem of learning semiseparable kernels as a minimax optimization problem and propose a SVD-QCQP primal-dual algorithm which dramatically reduces the computational complexity as compared with previous SDP-based approaches. Furthermore, we provide an efficient implementation of thi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65288;FwA&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#32593;&#32476;&#19978;&#30340;&#24322;&#26500;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#36951;&#25022;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21327;&#35758;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05445</link><description>&lt;p&gt;
&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65306;&#22797;&#26434;&#32593;&#32476;&#19978;&#24322;&#26500;&#36172;&#21338;&#26426;&#30340;&#39640;&#25928;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Flooding with Absorption: An Efficient Protocol for Heterogeneous Bandits over Complex Networks. (arXiv:2303.05445v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65288;FwA&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#32593;&#32476;&#19978;&#30340;&#24322;&#26500;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#36951;&#25022;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21327;&#35758;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#39034;&#24207;&#20915;&#31574;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#22914;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#21644;&#26080;&#32447;&#32593;&#32476;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#22810;&#20195;&#29702;&#30340;&#22330;&#26223;&#65292;&#27599;&#20010;&#20195;&#29702;&#35299;&#20915;&#33258;&#24049;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#36172;&#21338;&#26426;&#25317;&#26377;&#19981;&#21516;&#30340;&#33218;&#12290;&#20182;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#36890;&#36807;&#32473;&#23450;&#32593;&#32476;&#30340;&#36890;&#20449;&#21327;&#35758;&#21327;&#20316;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#20182;&#20204;&#30340;&#38598;&#20307;&#36951;&#25022;&#12290;&#20808;&#21069;&#20851;&#20110;&#27492;&#38382;&#39064;&#30340;&#25991;&#29486;&#21482;&#32771;&#34385;&#20102;&#33218;&#30340;&#24322;&#36136;&#24615;&#21644;&#32593;&#32476;&#21270;&#20195;&#29702;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21516;&#26102;&#21253;&#21547;&#36825;&#20004;&#20010;&#29305;&#24615;&#30340;&#35774;&#32622;&#12290;&#38024;&#23545;&#36825;&#19968;&#26032;&#39062;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#26631;&#20934;&#27867;&#27946;&#21327;&#35758;&#32467;&#21512;&#32463;&#20856;&#30340;&#19978;&#32622;&#20449;&#30028;&#31574;&#30053;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#36951;&#25022;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20943;&#36731;&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#27867;&#27946;&#36896;&#25104;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#35758;&#65292;&#31216;&#20026;&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65288;FwA&#65289;&#12290;&#25105;&#20204;&#23545;&#30001;&#27492;&#20135;&#29983;&#30340;&#36951;&#25022;&#19978;&#30028;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#21327;&#35758;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-armed bandits are extensively used to model sequential decision-making, making them ubiquitous in many real-life applications such as online recommender systems and wireless networking. We consider a multi-agent setting where each agent solves their own bandit instance endowed with a different set of arms. Their goal is to minimize their group regret while collaborating via some communication protocol over a given network. Previous literature on this problem only considered arm heterogeneity and networked agents separately. In this work, we introduce a setting that encompasses both features. For this novel setting, we first provide a rigorous regret analysis for a standard flooding protocol combined with the classic UCB policy. Then, to mitigate the issue of high communication costs incurred by flooding in complex networks, we propose a new protocol called Flooding with Absorption (FwA). We provide a theoretical analysis of the resulting regret bound and discuss the advantages of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#20869;&#29983;&#24615;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Two-Stage Least Squares&#26041;&#27861;&#30340;&#22312;&#32447;&#21464;&#20307;O2SLS&#26469;&#22788;&#29702;&#20869;&#29983;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#35782;&#21035;&#29575;&#21644;&#39044;&#27979;&#36951;&#25022;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.09357</link><description>&lt;p&gt;
&#22312;&#32447;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;: &#36951;&#25022;&#20998;&#26512;&#21644;Bandit&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Online Instrumental Variable Regression: Regret Analysis and Bandit Feedback. (arXiv:2302.09357v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09357
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#20869;&#29983;&#24615;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Two-Stage Least Squares&#26041;&#27861;&#30340;&#22312;&#32447;&#21464;&#20307;O2SLS&#26469;&#22788;&#29702;&#20869;&#29983;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#35782;&#21035;&#29575;&#21644;&#39044;&#27979;&#36951;&#25022;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#29983;&#24615;&#26159;&#23454;&#38469;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#29616;&#35937;&#65292;&#22240;&#20026;&#36951;&#28431;&#21464;&#37327;&#12289;&#25112;&#30053;&#34892;&#20026;&#12289;&#27979;&#37327;&#35823;&#24046;&#31561;&#21407;&#22240;&#23548;&#33268;&#22122;&#22768;&#21644;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#29616;&#26377;&#30340;&#26080;&#30028;&#22122;&#22768;&#21644;&#32447;&#24615;Bandit&#38543;&#26426;&#22312;&#32447;&#32447;&#24615;&#22238;&#24402;&#20998;&#26512;&#20005;&#37325;&#20381;&#36182;&#22806;&#29983;&#24615;&#65292;&#21363;&#22122;&#22768;&#21644;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#12290;&#37492;&#20110;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24037;&#20855;&#21464;&#37327;&#65288;IV&#65289;&#22238;&#24402;&#22312;&#38543;&#26426;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#36229;&#35782;&#21035;&#21644;&#24688;&#22909;&#35782;&#21035;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Two-Stage Least Squares&#26041;&#27861;&#30340;&#22312;&#32447;&#21464;&#20307;&#65288;&#21363;O2SLS&#65289;&#26469;&#22788;&#29702;&#20869;&#29983;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;O2SLS&#23454;&#29616;&#20102;$ \mathcal{O} \left(d_x d_z \log ^ 2 T \right)$&#30340;&#35782;&#21035;&#29575;&#21644;$ \tilde {\mathcal {O}} \left(\gamma \sqrt {d_x T} \right)$&#30340;&#39044;&#27979;&#36951;&#25022;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Endogeneity, i.e. the dependence between noise and covariates, is a common phenomenon in real data due to omitted variables, strategic behaviours, measurement errors etc. In contrast, the existing analyses of stochastic online linear regression with unbounded noise and linear bandits depend heavily on exogeneity, i.e. the independence between noise and covariates. Motivated by this gap, we study the over-and just-identified Instrumental Variable (IV) regression for stochastic online learning. IV regression and the Two-Stage Least Squares approach to it are widely deployed in economics and causal inference to identify the underlying model from an endogenous dataset. Thus, we propose to use an online variant of Two-Stage Least Squares approach, namely O2SLS, to tackle endogeneity in stochastic online learning. Our analysis shows that O2SLS achieves $\mathcal{O}\left(d_x d_z \log ^2 T\right)$ identification and $\tilde{\mathcal{O}}\left(\gamma \sqrt{d_x T}\right)$ oracle regret after $T$ 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#26377;&#30028;f-&#25955;&#24230;&#32422;&#26463;&#19979;&#65292;&#36817;&#20284;&#25298;&#32477;&#37319;&#26679;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#36890;&#36807;&#920;(~(D/f'(n)))&#20989;&#25968;&#26469;&#34920;&#31034;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#30456;&#20851;&#31639;&#27861;&#30340;&#24615;&#33021;&#20381;&#28982;&#25104;&#31435;&#12290;</title><link>http://arxiv.org/abs/2302.04658</link><description>&lt;p&gt;
&#36817;&#20284;&#25298;&#32477;&#37319;&#26679;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21450;&#20854;&#22312;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Sample Complexity of Approximate Rejection Sampling with Applications to Smoothed Online Learning. (arXiv:2302.04658v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#26377;&#30028;f-&#25955;&#24230;&#32422;&#26463;&#19979;&#65292;&#36817;&#20284;&#25298;&#32477;&#37319;&#26679;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#36890;&#36807;&#920;(~(D/f'(n)))&#20989;&#25968;&#26469;&#34920;&#31034;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#30456;&#20851;&#31639;&#27861;&#30340;&#24615;&#33021;&#20381;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#26469;&#33258;&#20998;&#24067;&#956;&#30340;n&#20010;&#29420;&#31435;&#26679;&#26412;&#65292;&#24182;&#19988;&#25105;&#20204;&#24076;&#26395;&#36755;&#20986;&#20854;&#20013;&#19968;&#20010;&#26679;&#26412;&#65292;&#20351;&#24471;&#36755;&#20986;&#30340;&#20998;&#24067;&#23613;&#21487;&#33021;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#957;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25152;&#26377;&#20855;&#26377;&#26377;&#30028;f-&#25955;&#24230;Df(&#957;|&#956;)&#8804;D&#30340;&#957;,&#956;&#23545;&#20013;&#65292;&#20851;&#20110;n&#30340;&#26368;&#20248;&#24635;&#21464;&#24046;&#36317;&#31163;&#30001;&#920;(~(D/f'(n)))&#32473;&#20986;&#12290;&#20043;&#21069;&#65292;&#36825;&#20010;&#38382;&#39064;&#21482;&#30740;&#31350;&#20102;&#957;&#30456;&#23545;&#20110;&#956;&#30340;Radon-Nikodym&#23548;&#25968;&#19968;&#33268;&#26377;&#30028;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#20284;&#20046;&#38750;&#24120;&#19981;&#21516;&#30340;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#23567;&#21270;&#36951;&#25022;&#21644;&#20855;&#26377;oracle&#25928;&#29575;&#30340;&#31639;&#27861;&#30340;&#36951;&#25022;&#21363;&#20351;&#22312;&#23545;&#25163;&#26377;&#36793;&#30028;f-&#25955;&#24230;&#65288;&#32780;&#19981;&#26159;&#26377;&#30028;Radon-Nikodym&#23548;&#25968;&#65289;&#30340;&#26494;&#24347;&#32422;&#26463;&#19979;&#65292;&#20173;&#28982;&#25104;&#31435;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#22343;&#21248;&#20272;&#35745;&#20013;&#29992;&#20110;&#24179;&#22343;&#20272;&#35745;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Suppose we are given access to $n$ independent samples from distribution $\mu$ and we wish to output one of them with the goal of making the output distributed as close as possible to a target distribution $\nu$. In this work we show that the optimal total variation distance as a function of $n$ is given by $\tilde\Theta(\frac{D}{f'(n)})$ over the class of all pairs $\nu,\mu$ with a bounded $f$-divergence $D_f(\nu\|\mu)\leq D$. Previously, this question was studied only for the case when the Radon-Nikodym derivative of $\nu$ with respect to $\mu$ is uniformly bounded. We then consider an application in the seemingly very different field of smoothed online learning, where we show that recent results on the minimax regret and the regret of oracle-efficient algorithms still hold even under relaxed constraints on the adversary (to have bounded $f$-divergence, as opposed to bounded Radon-Nikodym derivative). Finally, we also study efficacy of importance sampling for mean estimates uniform o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20960;&#20309;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#65292;&#24182;&#22312;&#39640;&#32500;&#24230;&#25968;&#25454;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.03660</link><description>&lt;p&gt;
&#19968;&#33324;&#20960;&#20309;&#19978;&#30340;&#40654;&#26364;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Riemannian Flow Matching on General Geometries. (arXiv:2302.03660v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20960;&#20309;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#65292;&#24182;&#22312;&#39640;&#32500;&#24230;&#25968;&#25454;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#65288;RFM&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#27969;&#24418;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#12290;&#29616;&#26377;&#30340;&#27969;&#24418;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#27169;&#25311;&#65292;&#35201;&#20040;&#26080;&#27861;&#26412;&#36136;&#19978;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#35201;&#20040;&#20351;&#29992;&#38480;&#21046;&#37327;&#30340;&#36817;&#20284;&#26469;&#20135;&#29983;&#26377;&#20559;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#40654;&#26364;&#27969;&#21305;&#37197;&#32469;&#36807;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#22810;&#30340;&#20248;&#21183;&#65306;&#23427;&#22312;&#31616;&#21333;&#20960;&#20309;&#19978;&#26080;&#38656;&#27169;&#25311;&#65292;&#19981;&#38656;&#35201;&#25955;&#24230;&#35745;&#31639;&#65292;&#24182;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;&#20854;&#30446;&#26631;&#21521;&#37327;&#22330;&#12290; RFM&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#26500;&#24314;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#21069;&#24230;&#37327;&#65292;&#20197;&#23450;&#20041;&#30446;&#26631;&#21521;&#37327;&#22330;&#65292;&#20854;&#20013;&#21253;&#25324;&#29616;&#26377;&#30340;&#27431;&#20960;&#37324;&#24471;&#24773;&#20917;&#12290;&#20026;&#20102;&#25193;&#23637;&#21040;&#19968;&#33324;&#20960;&#20309;&#65292;&#25105;&#20204;&#20381;&#38752;&#20351;&#29992;&#35889;&#20998;&#35299;&#26469;&#26377;&#25928;&#22320;&#21363;&#20852;&#35745;&#31639;&#21069;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;3D&#32593;&#26684;&#21644;&#21452;&#26354;&#31354;&#38388;&#19978;&#35757;&#32451;&#26631;&#20934;&#21270;&#27969;&#26469;&#35777;&#26126;&#20854;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on real-world non-Euclidean datasets, and we demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#31639;&#27861;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#21487;&#20197;&#26597;&#35810;&#22870;&#21169;&#30340;&#38543;&#26426;&#24615;&#20197;&#21450;&#33218;&#30340;&#21472;&#21152;&#24577;&#26102;&#21487;&#20197;&#23454;&#29616;&#20108;&#27425;&#21152;&#36895;&#65292;&#20294;&#22312;&#21482;&#33021;&#26377;&#38480;&#22320;&#35775;&#38382;&#22870;&#21169;&#30340;&#38543;&#26426;&#24615;&#26102;&#65292;&#26597;&#35810;&#22797;&#26434;&#24230;&#19982;&#32463;&#20856;&#31639;&#27861;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2301.08544</link><description>&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#37327;&#23376;&#36890;&#36947;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Armed Bandits and Quantum Channel Oracles. (arXiv:2301.08544v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#31639;&#27861;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#21487;&#20197;&#26597;&#35810;&#22870;&#21169;&#30340;&#38543;&#26426;&#24615;&#20197;&#21450;&#33218;&#30340;&#21472;&#21152;&#24577;&#26102;&#21487;&#20197;&#23454;&#29616;&#20108;&#27425;&#21152;&#36895;&#65292;&#20294;&#22312;&#21482;&#33021;&#26377;&#38480;&#22320;&#35775;&#38382;&#22870;&#21169;&#30340;&#38543;&#26426;&#24615;&#26102;&#65292;&#26597;&#35810;&#22797;&#26434;&#24230;&#19982;&#32463;&#20856;&#31639;&#27861;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#26159;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#30340;&#37325;&#35201;&#25903;&#26609;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#24320;&#22987;&#30740;&#31350;&#29992;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#24403;&#21487;&#20197;&#22312;&#21472;&#21152;&#24577;&#20013;&#26597;&#35810;&#33218;&#21644;&#22870;&#21169;&#38543;&#26426;&#24615;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#20108;&#27425;&#21152;&#36895;&#65288;&#22312;&#26597;&#35810;&#22797;&#26434;&#24230;&#19978;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36827;&#19968;&#27493;&#30340;&#36172;&#21338;&#26426;&#27169;&#22411;&#65292;&#20854;&#20013;&#25105;&#20204;&#21482;&#33021;&#26377;&#38480;&#22320;&#35775;&#38382;&#22870;&#21169;&#30340;&#38543;&#26426;&#24615;&#65292;&#20294;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#22312;&#21472;&#21152;&#24577;&#20013;&#26597;&#35810;&#33218;&#12290;&#25105;&#20204;&#35777;&#26126;&#24403;&#22914;&#27492;&#26102;&#26597;&#35810;&#22797;&#26434;&#24230;&#19982;&#32463;&#20856;&#31639;&#27861;&#30456;&#21516;&#12290;&#36825;&#25512;&#24191;&#20102;&#20808;&#21069;&#30340;&#32467;&#26524;&#65292;&#21363;&#24403;&#39044;&#27979;&#22120;&#20855;&#26377;&#27491;&#30340;&#22833;&#25928;&#27010;&#29575;&#26102;&#65292;&#23545;&#20110;&#26410;&#32467;&#26500;&#21270;&#25628;&#32034;&#26080;&#27861;&#23454;&#29616;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-armed bandits are one of the theoretical pillars of reinforcement learning. Recently, the investigation of quantum algorithms for multi-armed bandit problems was started, and it was found that a quadratic speed-up (in query complexity) is possible when the arms and the randomness of the rewards of the arms can be queried in superposition. Here we introduce further bandit models where we only have limited access to the randomness of the rewards, but we can still query the arms in superposition. We show that then the query complexity is the same as for classical algorithms. This generalizes the prior result that no speed-up is possible for unstructured search when the oracle has positive failure probability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#21363;&#32467;&#26500;&#31232;&#30095;&#24615;&#65292;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;ICA&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.07751</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#30340;&#21487;&#36776;&#35782;&#24615;&#65306;&#31232;&#30095;&#24615;&#21450;&#20854;&#23427;
&lt;/p&gt;
&lt;p&gt;
On the Identifiability of Nonlinear ICA: Sparsity and Beyond. (arXiv:2206.07751v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#21363;&#32467;&#26500;&#31232;&#30095;&#24615;&#65292;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;ICA&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#26088;&#22312;&#20174;&#20854;&#21487;&#35266;&#27979;&#30340;&#38750;&#32447;&#24615;&#28151;&#21512;&#20013;&#24674;&#22797;&#20986;&#28508;&#22312;&#29420;&#31435;&#20998;&#37327;&#12290;&#22914;&#20309;&#20351;&#38750;&#32447;&#24615;ICA&#27169;&#22411;&#21487;&#36776;&#35782;&#30452;&#21040;&#26576;&#20123;&#24179;&#20961;&#19981;&#30830;&#23450;&#24615;&#26159;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#31361;&#30772;&#26159;&#23558;&#28304;&#30340;&#26631;&#20934;&#29420;&#31435;&#24615;&#20551;&#35774;&#37325;&#26032;&#23450;&#20041;&#20026;&#22312;&#26576;&#20123;&#36741;&#21161;&#21464;&#37327;&#65288;&#20363;&#22914;&#31867;&#26631;&#31614;&#21644;/&#25110;&#22495;/&#26102;&#38388;&#32034;&#24341;&#65289;&#32473;&#23450;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#20316;&#20026;&#24369;&#30417;&#30563;&#25110;&#24402;&#32435;&#20559;&#32622;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26080;&#26465;&#20214;&#20808;&#39564;&#30340;&#38750;&#32447;&#24615;ICA&#26080;&#27861;&#20174;&#36825;&#20123;&#21457;&#23637;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#26465;&#26367;&#20195;&#36335;&#24452;&#65292;&#24182;&#20165;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#20363;&#22914;&#32467;&#26500;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#32422;&#26463;&#30340;&#20855;&#20307;&#23454;&#20363;&#19979;&#65292;&#29420;&#31435;&#30340;&#28508;&#22312;&#20998;&#37327;&#21487;&#20197;&#20174;&#20854;&#38750;&#32447;&#24615;&#28151;&#21512;&#20013;&#36776;&#35782;&#20986;&#26469;&#65292;&#36798;&#21040;&#38750;&#24179;&#20961;&#30340;&#38750;&#32447;&#24615;ICA&#21487;&#35782;&#21035;&#24615;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear independent component analysis (ICA) aims to recover the underlying independent latent sources from their observable nonlinear mixtures. How to make the nonlinear ICA model identifiable up to certain trivial indeterminacies is a long-standing problem in unsupervised learning. Recent breakthroughs reformulate the standard independence assumption of sources as conditional independence given some auxiliary variables (e.g., class labels and/or domain/time indexes) as weak supervision or inductive bias. However, nonlinear ICA with unconditional priors cannot benefit from such developments. We explore an alternative path and consider only assumptions on the mixing process, such as Structural Sparsity. We show that under specific instantiations of such constraints, the independent latent sources can be identified from their nonlinear mixtures up to a permutation and a component-wise transformation, thus achieving nontrivial identifiability of nonlinear ICA without auxiliary variable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#20851;&#30340;&#22522;&#20110;&#26680;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#32479;&#35745;&#37327;&#22312;&#27979;&#37327;&#27969;&#24418;&#25968;&#25454;&#26102;&#30340;&#24212;&#29992;&#12290;&#25991;&#31456;&#23637;&#31034;&#20102;&#26816;&#39564;&#27700;&#24179;&#21644;&#21151;&#29575;&#19982;&#26680;&#24102;&#23485;&#12289;&#26679;&#26412;&#25968;&#37327;&#21644;&#27969;&#24418;&#20869;&#22312;&#32500;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#24314;&#31435;&#20102;&#27979;&#35797;&#21151;&#29575;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2105.03425</link><description>&lt;p&gt;
&#27979;&#37327;&#27969;&#24418;&#25968;&#25454;&#30340;&#26680;&#21452;&#26679;&#26412;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Kernel Two-Sample Tests for Manifold Data. (arXiv:2105.03425v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.03425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#20851;&#30340;&#22522;&#20110;&#26680;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#32479;&#35745;&#37327;&#22312;&#27979;&#37327;&#27969;&#24418;&#25968;&#25454;&#26102;&#30340;&#24212;&#29992;&#12290;&#25991;&#31456;&#23637;&#31034;&#20102;&#26816;&#39564;&#27700;&#24179;&#21644;&#21151;&#29575;&#19982;&#26680;&#24102;&#23485;&#12289;&#26679;&#26412;&#25968;&#37327;&#21644;&#27969;&#24418;&#20869;&#22312;&#32500;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#24314;&#31435;&#20102;&#27979;&#35797;&#21151;&#29575;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#27969;&#24418;&#25968;&#25454;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#20851;&#30340;&#22522;&#20110;&#26680;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#32479;&#35745;&#37327;&#65292;&#20551;&#35774;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#25509;&#36817;&#20110;&#20302;&#32500;&#27969;&#24418;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#27979;&#35797;&#27700;&#24179;&#21644;&#21151;&#29575;&#19982;&#26680;&#24102;&#23485;&#12289;&#26679;&#26412;&#25968;&#37327;&#21644;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#25968;&#25454;&#23494;&#24230;&#25903;&#25345;&#22312;&#19968;&#20010;&#23884;&#20837;&#21040;$m$&#32500;&#31354;&#38388;&#20013;&#30340;$d$&#32500;&#23376;&#27969;&#24418;$\mathcal{M}$&#19978;&#26102;&#65292;&#20174;&#26381;&#20174;&#20110;&#19968;&#23545;&#20998;&#24067;$p$&#21644;$q$&#25277;&#21462;&#30340;&#25968;&#25454;&#36827;&#34892;&#26680;&#21452;&#26679;&#26412;&#26816;&#39564;&#65292;&#36825;&#23545;&#20998;&#24067;$ p $&#21644;$q$&#26159;&#20855;&#26377;H\"older&#38454;$\beta$&#65288;&#26368;&#39640;2&#65289;&#65292;&#26679;&#26412;&#25968;&#37327;$n$&#36275;&#22815;&#22823;&#65292;&#20351;&#24471;$\Delta_2\gtrsim n^{- {2\beta/(d+4\beta)}}$&#65292;&#20854;&#20013;$\Delta_2$&#26159;&#27969;&#24418;&#19978;$p$&#21644;$q$&#20043;&#38388;&#30340;&#24179;&#26041;$L^2$-&#24046;&#24322;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36275;&#22815;&#22823;&#19988;&#26377;&#38480;$n$&#30340;&#27979;&#35797;&#21151;&#29575;&#19979;&#30028;&#65292;&#20854;&#20013;&#26680;&#24102;&#23485;&#21442;&#25968;$\gamma$&#30340;&#27604;&#20363;&#23610;&#24230;&#20026;$n^ {-1/(d+4\beta)}$&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a study of a kernel-based two-sample test statistic related to the Maximum Mean Discrepancy (MMD) in the manifold data setting, assuming that high-dimensional observations are close to a low-dimensional manifold. We characterize the test level and power in relation to the kernel bandwidth, the number of samples, and the intrinsic dimensionality of the manifold. Specifically, we show that when data densities are supported on a $d$-dimensional sub-manifold $\mathcal{M}$ embedded in an $m$-dimensional space, the kernel two-sample test for data sampled from a pair of distributions $p$ and $q$ that are H\"older with order $\beta$ (up to 2) is powerful when the number of samples $n$ is large such that $\Delta_2 \gtrsim n^{- { 2 \beta/( d + 4 \beta ) }}$, where $\Delta_2$ is the squared $L^2$-divergence between $p$ and $q$ on manifold. We establish a lower bound on the test power for finite $n$ that is sufficiently large, where the kernel bandwidth parameter $\gamma$ scales as $n^{
&lt;/p&gt;</description></item></channel></rss>