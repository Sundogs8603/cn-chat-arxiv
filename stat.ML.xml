<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#26469;&#23454;&#29616;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#36127;&#37319;&#26679;&#26041;&#27861;&#24182;&#22312;&#22810;&#39033;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17475</link><description>&lt;p&gt;
&#36229;&#36234;&#36127;&#37319;&#26679;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient distributed representations beyond negative sampling. (arXiv:2303.17475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#26469;&#23454;&#29616;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#36127;&#37319;&#26679;&#26041;&#27861;&#24182;&#22312;&#22810;&#39033;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23398;&#20064;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#20063;&#31216;&#20026;&#23884;&#20837;&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#31867;&#20284;&#20110;Word2Vec&#31639;&#27861;&#20013;&#24341;&#20837;&#24182;&#22312;&#22810;&#20010;&#24037;&#20316;&#20013;&#37319;&#29992;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#20248;&#21270;&#35745;&#31639;&#30340;&#29942;&#39048;&#26159;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#35745;&#31639;&#65292;&#36825;&#38656;&#35201;&#19982;&#26679;&#26412;&#22823;&#23567;&#21576;&#20108;&#27425;&#27604;&#20363;&#30340;&#25805;&#20316;&#25968;&#12290;&#36825;&#31181;&#22797;&#26434;&#24230;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#25152;&#20197;&#36127;&#37319;&#26679;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19982;&#26679;&#26412;&#22823;&#23567;&#32447;&#24615;&#30456;&#20851;&#30340;&#26102;&#38388;&#20869;&#33719;&#24471;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36127;&#37319;&#26679;&#20250;&#25913;&#21464;&#25439;&#22833;&#20989;&#25968;&#65292;&#22240;&#27492;&#35299;&#20915;&#30340;&#26159;&#19982;&#26368;&#21021;&#25552;&#20986;&#30340;&#19981;&#21516;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#65292;&#20174;&#32780;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#23398;&#20064;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23884;&#20837;&#36136;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#36127;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article describes an efficient method to learn distributed representations, also known as embeddings. This is accomplished minimizing an objective function similar to the one introduced in the Word2Vec algorithm and later adopted in several works. The optimization computational bottleneck is the calculation of the softmax normalization constants for which a number of operations scaling quadratically with the sample size is required. This complexity is unsuited for large datasets and negative sampling is a popular workaround, allowing one to obtain distributed representations in linear time with respect to the sample size. Negative sampling consists, however, in a change of the loss function and hence solves a different optimization problem from the one originally proposed. Our contribution is to show that the sotfmax normalization constants can be estimated in linear time, allowing us to design an efficient optimization strategy to learn distributed representations. We test our ap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24040;&#22411;&#20851;&#31995;&#20107;&#20214;&#32593;&#32476;&#30340;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#25512;&#29702;&#20986;&#28508;&#22312;&#31354;&#38388;&#21160;&#24577;&#65292;&#24182;&#23454;&#29616;&#20998;&#23618;&#25512;&#26029;&#32593;&#32476;&#31038;&#21306;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.17460</link><description>&lt;p&gt;
&#24040;&#22411;&#20851;&#31995;&#20107;&#20214;&#32593;&#32476;&#20013;&#28508;&#22312;&#31354;&#38388;&#21160;&#24577;&#30340;&#24555;&#36895;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Fast inference of latent space dynamics in huge relational event networks. (arXiv:2303.17460v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24040;&#22411;&#20851;&#31995;&#20107;&#20214;&#32593;&#32476;&#30340;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#25512;&#29702;&#20986;&#28508;&#22312;&#31354;&#38388;&#21160;&#24577;&#65292;&#24182;&#23454;&#29616;&#20998;&#23618;&#25512;&#26029;&#32593;&#32476;&#31038;&#21306;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#20107;&#20214;&#26159;&#31038;&#20132;&#20114;&#21160;&#30340;&#19968;&#31181;&#31867;&#22411;&#65292;&#26377;&#26102;&#34987;&#31216;&#20026;&#21160;&#24577;&#32593;&#32476;&#12290;&#23427;&#30340;&#21160;&#24577;&#36890;&#24120;&#21462;&#20915;&#20110;&#26032;&#20852;&#30340;&#27169;&#24335;&#65292;&#21363;&#20869;&#29983;&#21464;&#37327;&#65292;&#25110;&#22806;&#37096;&#21147;&#37327;&#65292;&#21363;&#22806;&#29983;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#32773;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#22411;&#32593;&#32476;&#65292;&#20840;&#38754;&#30340;&#20449;&#24687;&#26159;&#32597;&#35265;&#30340;&#12290;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;&#26041;&#27861;&#26159;&#35299;&#37322;&#39537;&#21160;&#32593;&#32476;&#37197;&#32622;&#30340;&#26410;&#27979;&#37327;&#21327;&#21464;&#37327;&#30340;&#27969;&#34892;&#26041;&#24335;&#12290;&#36125;&#21494;&#26031;&#21644;EM&#31867;&#22411;&#30340;&#31639;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#25512;&#26029;&#28508;&#22312;&#31354;&#38388;&#65292;&#20294;&#26159;&#35768;&#22810;&#31038;&#20132;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#30340;&#35268;&#27169;&#20197;&#21450;&#36807;&#31243;&#65288;&#22240;&#27492;&#26159;&#28508;&#22312;&#31354;&#38388;&#65289;&#30340;&#21160;&#24577;&#29305;&#24615;&#20351;&#24471;&#35745;&#31639;&#21464;&#24471;&#26497;&#20854;&#26114;&#36149;&#12290;&#22312;&#26412;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#24040;&#22411;&#20851;&#31995;&#20107;&#20214;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#21040;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25512;&#26029;&#32593;&#32476;&#31038;&#21306;&#21160;&#24577;&#30340;&#20998;&#23618;&#31574;&#30053;&#12290;&#33410;&#28857;&#21160;&#24577;&#26159;&#36890;&#36807;&#21521;&#21518;&#27010;&#29575;&#25512;&#26029;&#30340;&#36125;&#21494;&#26031;&#32479;&#35745;&#27169;&#22411;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational events are a type of social interactions, that sometimes are referred to as dynamic networks. Its dynamics typically depends on emerging patterns, so-called endogenous variables, or external forces, referred to as exogenous variables. Comprehensive information on the actors in the network, especially for huge networks, is rare, however. A latent space approach in network analysis has been a popular way to account for unmeasured covariates that are driving network configurations. Bayesian and EM-type algorithms have been proposed for inferring the latent space, but both the sheer size many social network applications as well as the dynamic nature of the process, and therefore the latent space, make computations prohibitively expensive. In this work we propose a likelihood-based algorithm that can deal with huge relational event networks. We propose a hierarchical strategy for inferring network community dynamics embedded into an interpretable latent space. Node dynamics are d
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#28508;&#22312;&#20301;&#32622;&#27169;&#22411;&#19978;&#30340;&#22270;&#24418;Nadaraya-Watson&#20272;&#35745;&#22120;&#30340;&#24615;&#36136;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#26377;&#29702;&#35770;&#25351;&#23548;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.17229</link><description>&lt;p&gt;
&#28508;&#22312;&#20301;&#32622;&#27169;&#22411;&#19978;&#30340;&#22270;&#24418;Nadaraya-Watson&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
The Graphical Nadaraya-Watson Estimator on Latent Position Models. (arXiv:2303.17229v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17229
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#28508;&#22312;&#20301;&#32622;&#27169;&#22411;&#19978;&#30340;&#22270;&#24418;Nadaraya-Watson&#20272;&#35745;&#22120;&#30340;&#24615;&#36136;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#26377;&#29702;&#35770;&#25351;&#23548;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26377;&#26631;&#35760;&#33410;&#28857;&#30340;&#22270;&#24418;&#65292;&#25105;&#20204;&#23545;&#20272;&#35745;&#22120;&#30340;&#36136;&#37327;&#24863;&#20852;&#36259;&#65292;&#35813;&#20272;&#35745;&#22120;&#38024;&#23545;&#26410;&#26631;&#35760;&#33410;&#28857;&#39044;&#27979;&#20854;&#26631;&#35760;&#37051;&#23621;&#30340;&#35266;&#27979;&#20540;&#30340;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#20005;&#26684;&#30740;&#31350;&#20102;&#27987;&#24230;&#23646;&#24615;&#12289;&#26041;&#24046;&#30028;&#21644;&#39118;&#38505;&#30028;&#12290;&#34429;&#28982;&#20272;&#35745;&#22120;&#26412;&#36523;&#38750;&#24120;&#31616;&#21333;&#65292;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#36807;&#20110;&#29702;&#24819;&#65292;&#20294;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#23567;&#27493;&#39588;&#23558;&#26377;&#21161;&#20110;&#26356;&#22797;&#26434;&#26041;&#27861;&#65288;&#22914;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a graph with a subset of labeled nodes, we are interested in the quality of the averaging estimator which for an unlabeled node predicts the average of the observations of its labeled neighbours. We rigorously study concentration properties, variance bounds and risk bounds in this context. While the estimator itself is very simple and the data generating process is too idealistic for practical applications, we believe that our small steps will contribute towards the theoretical understanding of more sophisticated methods such as Graph Neural Networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22806;&#25512;&#26799;&#24230;&#26041;&#27861;&#21644;&#20854;&#21464;&#31181;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#27425;&#32447;&#24615;&#26368;&#20339;&#36845;&#20195;&#21644;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.17192</link><description>&lt;p&gt;
Extragradient&#31867;&#22411;&#26041;&#27861;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#65306;&#23545;&#32463;&#20856;&#21644;&#26368;&#26032;&#36827;&#23637;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Sublinear Convergence Rates of Extragradient-Type Methods: A Survey on Classical and Recent Developments. (arXiv:2303.17192v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22806;&#25512;&#26799;&#24230;&#26041;&#27861;&#21644;&#20854;&#21464;&#31181;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#27425;&#32447;&#24615;&#26368;&#20339;&#36845;&#20195;&#21644;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20854;&#20013;&#65292;&#22806;&#25512;&#26799;&#24230;&#65288;EG&#65289;&#26159;G. M. Korpelevich&#22312;1976&#24180;&#24341;&#20837;&#30340;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#36817;&#20284;&#35299;&#20915;&#26368;&#23567;&#26012;&#29575;&#38382;&#39064;&#21644;&#20854;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#22914;&#21464;&#20998;&#19981;&#31561;&#24335;&#21644;&#21333;&#35843;&#21253;&#21547;&#12290;&#22810;&#24180;&#26469;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;EG&#30340;&#21508;&#31181;&#21464;&#20307;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#40065;&#26834;&#20248;&#21270;&#20013;&#30340;&#26032;&#24212;&#29992;&#65292;&#36825;&#20123;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;EG&#26041;&#27861;&#21450;&#20854;&#21464;&#31181;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29992;&#20110;&#36817;&#20284;&#27714;&#35299;&#38750;&#32447;&#24615;&#26041;&#31243;&#21644;&#21253;&#21547;&#65292;&#37325;&#28857;&#20851;&#27880;&#21333;&#35843;&#24615;&#21644;&#21327;&#21516;&#21333;&#35843;&#24615;&#35774;&#32622;&#12290;&#25105;&#20204;&#20026;&#19981;&#21516;&#31867;&#21035;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#37325;&#28857;&#26159;&#27425;&#32447;&#24615;&#26368;&#20339;&#36845;&#20195;&#21644;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22522;&#20110;Halpern&#22266;&#23450;&#28857;&#36845;&#20195;&#21644;Nesterov&#21152;&#36895;&#25216;&#26415;&#30340;&#26368;&#36817;&#21152;&#36895;&#21464;&#20307;&#30340;EG&#12290;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#30340;&#35770;&#35777;&#21644;&#22522;&#26412;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#26131;&#20110;&#29702;&#35299;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extragradient (EG), introduced by G. M. Korpelevich in 1976, is a well-known method to approximate solutions of saddle-point problems and their extensions such as variational inequalities and monotone inclusions. Over the years, numerous variants of EG have been proposed and studied in the literature. Recently, these methods have gained popularity due to new applications in machine learning and robust optimization. In this work, we survey the latest developments in the EG method and its variants for approximating solutions of nonlinear equations and inclusions, with a focus on the monotonicity and co-hypomonotonicity settings. We provide a unified convergence analysis for different classes of algorithms, with an emphasis on sublinear best-iterate and last-iterate convergence rates. We also discuss recent accelerated variants of EG based on both Halpern fixed-point iteration and Nesterov's accelerated techniques. Our approach uses simple arguments and basic mathematical tools to mak
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26631;&#23450;&#26041;&#27861;&#65292;&#20351;&#29992;&#28909;&#24230;&#22270;&#22238;&#24402;&#26469;&#28040;&#38500;&#26364;&#21704;&#39039;&#19990;&#30028;&#20551;&#35774;&#19979;&#40060;&#30524;&#22270;&#29255;&#20013;&#27178;&#21521;&#35282;&#24230;&#27495;&#20041;&#65292;&#21516;&#26102;&#24674;&#22797;&#26059;&#36716;&#21644;&#28040;&#38500;&#40060;&#30524;&#22833;&#30495;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20248;&#21270;&#30340;&#23545;&#35282;&#32447;&#28857;&#32531;&#35299;&#22270;&#20687;&#20013;&#32570;&#20047;&#28040;&#22833;&#28857;&#30340;&#24773;&#20917;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.17166</link><description>&lt;p&gt;
&#20351;&#29992;&#28909;&#24230;&#22270;&#22238;&#24402;&#36827;&#34892;&#28145;&#24230;&#21333;&#24352;&#22270;&#29255;&#25668;&#20687;&#26426;&#26631;&#23450;&#65292;&#22312;&#26364;&#21704;&#39039;&#19990;&#30028;&#20551;&#35774;&#19979;&#22312;&#19981;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#36824;&#21407;&#40060;&#30524;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
Deep Single Image Camera Calibration by Heatmap Regression to Recover Fisheye Images Under ManhattanWorld AssumptionWithout Ambiguity. (arXiv:2303.17166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26631;&#23450;&#26041;&#27861;&#65292;&#20351;&#29992;&#28909;&#24230;&#22270;&#22238;&#24402;&#26469;&#28040;&#38500;&#26364;&#21704;&#39039;&#19990;&#30028;&#20551;&#35774;&#19979;&#40060;&#30524;&#22270;&#29255;&#20013;&#27178;&#21521;&#35282;&#24230;&#27495;&#20041;&#65292;&#21516;&#26102;&#24674;&#22797;&#26059;&#36716;&#21644;&#28040;&#38500;&#40060;&#30524;&#22833;&#30495;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20248;&#21270;&#30340;&#23545;&#35282;&#32447;&#28857;&#32531;&#35299;&#22270;&#20687;&#20013;&#32570;&#20047;&#28040;&#22833;&#28857;&#30340;&#24773;&#20917;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27491;&#20132;&#19990;&#30028;&#22352;&#26631;&#31995;&#20013;&#65292;&#26364;&#21704;&#39039;&#19990;&#30028;&#27839;&#30528;&#38271;&#26041;&#20307;&#24314;&#31569;&#29289;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26364;&#21704;&#39039;&#19990;&#30028;&#38656;&#35201;&#25913;&#36827;&#65292;&#22240;&#20026;&#22270;&#20687;&#20013;&#30340;&#27178;&#21521;&#35282;&#24230;&#30340;&#21407;&#28857;&#26159;&#20219;&#24847;&#30340;&#65292;&#21363;&#20855;&#26377;&#22235;&#20493;&#36718;&#25442;&#23545;&#31216;&#30340;&#27178;&#21521;&#35282;&#24230;&#30340;&#27495;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25668;&#20687;&#26426;&#21644;&#34892;&#39542;&#26041;&#21521;&#30340;&#36947;&#36335;&#26041;&#21521;&#30340;&#24179;&#35282;&#23450;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26631;&#23450;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#28909;&#24230;&#22270;&#22238;&#24402;&#26469;&#28040;&#38500;&#27495;&#20041;&#65292;&#31867;&#20284;&#20110;&#23039;&#24577;&#20272;&#35745;&#20851;&#38190;&#28857;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#20004;&#20010;&#20998;&#25903;&#32593;&#32476;&#24674;&#22797;&#26059;&#36716;&#24182;&#20174;&#19968;&#33324;&#22330;&#26223;&#22270;&#20687;&#20013;&#28040;&#38500;&#40060;&#30524;&#22833;&#30495;&#12290;&#20026;&#20102;&#32531;&#35299;&#22270;&#20687;&#20013;&#32570;&#20047;&#28040;&#22833;&#28857;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20855;&#26377;&#31354;&#38388;&#22343;&#21248;&#24615;&#26368;&#20339;&#30340;&#23545;&#35282;&#32447;&#28857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26364;&#21704;&#39039;&#19990;&#30028;&#20551;&#35774;&#19979;&#23545;&#40060;&#30524;&#22270;&#20687;&#30340;&#28145;&#24230;&#21333;&#24352;&#22270;&#29255;&#25668;&#20687;&#26426;&#26631;&#23450;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#27809;&#26377;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In orthogonal world coordinates, a Manhattan world lying along cuboid buildings is widely useful for various computer vision tasks. However, the Manhattan world has much room for improvement because the origin of pan angles from an image is arbitrary, that is, four-fold rotational symmetric ambiguity of pan angles. To address this problem, we propose a definition for the pan-angle origin based on the directions of the roads with respect to a camera and the direction of travel. We propose a learning-based calibration method that uses heatmap regression to remove the ambiguity by each direction of labeled image coordinates, similar to pose estimation keypoints. Simultaneously, our two-branched network recovers the rotation and removes fisheye distortion from a general scene image. To alleviate the lack of vanishing points in images, we introduce auxiliary diagonal points that have the optimal 3D arrangement of spatial uniformity. Extensive experiments demonstrated that our method outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#21644;VAC$^2$-UCB&#31639;&#27861;&#65292;&#24182;&#20998;&#21035;&#23548;&#20986;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#20026;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2303.17110</link><description>&lt;p&gt;
&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Contextual Combinatorial Bandits with Probabilistically Triggered Arms. (arXiv:2303.17110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#21644;VAC$^2$-UCB&#31639;&#27861;&#65292;&#24182;&#20998;&#21035;&#23548;&#20986;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#20026;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25429;&#25417;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#30340;&#19968;&#31995;&#21015;&#24179;&#28369;&#26465;&#20214;&#19979;&#30340;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;(C$^2$MAB-T)&#65292;&#20363;&#22914;&#24773;&#22659;&#32423;&#32852;&#36172;&#21338;&#26426;&#21644;&#24773;&#22659;&#26368;&#22823;&#21270;&#36172;&#21338;&#26426;&#12290;&#22312;&#27169;&#25311;&#35302;&#21457;&#27010;&#29575;(TPM)&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;$\tilde{O}(d\sqrt{KT})$&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#28040;&#38500;&#20102;&#19968;&#20010;&#21487;&#33021;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#22240;&#23376;$O(1/p_{\min})$&#65292;&#20854;&#20013;$d$&#26159;&#24773;&#22659;&#30340;&#32500;&#25968;&#65292;$p_{\min}$&#26159;&#33021;&#34987;&#35302;&#21457;&#30340;&#20219;&#20309;&#33218;&#30340;&#26368;&#23567;&#27491;&#27010;&#29575;&#65292;&#25209;&#22823;&#23567;$K$&#26159;&#27599;&#36718;&#33021;&#34987;&#35302;&#21457;&#30340;&#33218;&#30340;&#26368;&#22823;&#25968;&#37327;&#12290;&#22312;&#26041;&#24046;&#35843;&#21046;(VM)&#25110;&#35302;&#21457;&#27010;&#29575;&#21644;&#26041;&#24046;&#35843;&#21046;(TPVM)&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#31639;&#27861;VAC$^2$-UCB&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#20010;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#35813;&#19978;&#38480;&#19982;&#25209;&#22823;&#23567;$K$&#26080;&#20851;&#12290;&#20316;&#20026;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
We study contextual combinatorial bandits with probabilistically triggered arms (C$^2$MAB-T) under a variety of smoothness conditions that capture a wide range of applications, such as contextual cascading bandits and contextual influence maximization bandits. Under the triggering probability modulated (TPM) condition, we devise the C$^2$-UCB-T algorithm and propose a novel analysis that achieves an $\tilde{O}(d\sqrt{KT})$ regret bound, removing a potentially exponentially large factor $O(1/p_{\min})$, where $d$ is the dimension of contexts, $p_{\min}$ is the minimum positive probability that any arm can be triggered, and batch-size $K$ is the maximum number of arms that can be triggered per round. Under the variance modulated (VM) or triggering probability and variance modulated (TPVM) conditions, we propose a new variance-adaptive algorithm VAC$^2$-UCB and derive a regret bound $\tilde{O}(d\sqrt{T})$, which is independent of the batch-size $K$. As a valuable by-product, we find our a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#27491;&#21322;&#23450;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#39640;&#25928;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#27491;&#21322;&#23450;-PSD&#27169;&#22411;&#22312;&#31934;&#24230;$\varepsilon$&#19979;&#29983;&#25104;iid&#26679;&#26412;&#12290;&#31639;&#27861;&#22797;&#26434;&#24230;&#20026;$O(T d \log(1/\varepsilon) m^2 + d m^{\beta+1} \log(T)/\varepsilon^2)$&#65292;&#20854;&#20013;$T$&#26159;&#26102;&#38388;&#27493;&#25968;&#65292;$\beta$&#26159;Fokker-Planck&#35299;&#30340;&#27491;&#21017;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17109</link><description>&lt;p&gt;
&#27491;&#21322;&#23450;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#39640;&#25928;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Efficient Sampling of Stochastic Differential Equations with Positive Semi-Definite Models. (arXiv:2303.17109v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#27491;&#21322;&#23450;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#39640;&#25928;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#27491;&#21322;&#23450;-PSD&#27169;&#22411;&#22312;&#31934;&#24230;$\varepsilon$&#19979;&#29983;&#25104;iid&#26679;&#26412;&#12290;&#31639;&#27861;&#22797;&#26434;&#24230;&#20026;$O(T d \log(1/\varepsilon) m^2 + d m^{\beta+1} \log(T)/\varepsilon^2)$&#65292;&#20854;&#20013;$T$&#26159;&#26102;&#38388;&#27493;&#25968;&#65292;$\beta$&#26159;Fokker-Planck&#35299;&#30340;&#27491;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#24050;&#30693;&#28418;&#31227;&#20989;&#25968;&#21644;&#25193;&#25955;&#30697;&#38453;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#39640;&#25928;&#37319;&#26679;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#26368;&#36817;&#30340;&#27010;&#29575;&#27169;&#22411;&#65288;&#27491;&#21322;&#23450;-PSD&#27169;&#22411;&#65289;\citep{rudi2021psd}&#65292;&#20174;&#20013;&#21487;&#20197;&#33719;&#24471;&#31934;&#24230;&#20026;$\varepsilon$&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;iid&#65289;&#26679;&#26412;&#65292;&#20854;&#25104;&#26412;&#20026;$m^2 d \log(1/\varepsilon)$&#65292;&#20854;&#20013;$m$&#26159;&#27169;&#22411;&#30340;&#32500;&#24230;&#65292;$d$&#26159;&#31354;&#38388;&#30340;&#32500;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#65306;&#39318;&#20808;&#35745;&#31639;&#28385;&#36275;&#19982;SDE&#30456;&#20851;&#32852;&#30340;Fokker-Planck&#26041;&#31243;&#65288;&#25110;&#20854;&#20998;&#25968;&#21464;&#20307;&#65289;&#30340;PSD&#27169;&#22411;&#65292;&#35823;&#24046;&#20026;$\varepsilon$&#65292;&#28982;&#21518;&#20174;&#29983;&#25104;&#30340;PSD&#27169;&#22411;&#20013;&#37319;&#26679;&#12290;&#20551;&#35774;Fokker-Planck&#35299;&#20855;&#26377;&#19968;&#23450;&#30340;&#27491;&#21017;&#24615;&#65288;&#21363;$\beta$&#38454;&#21487;&#24494;&#24615;&#20197;&#21450;&#20854;&#38646;&#28857;&#30340;&#19968;&#20123;&#20960;&#20309;&#26465;&#20214;&#65289;&#65292;&#25105;&#20204;&#24471;&#21040;&#19968;&#20010;&#31639;&#27861;&#65306;&#65288;a&#65289;&#22312;&#20934;&#22791;&#38454;&#27573;&#65292;&#33719;&#24471;&#20855;&#26377;L2&#36317;&#31163;$\varepsilon$&#30340;PSD&#27169;&#22411;&#20316;&#20026;&#30495;&#23454;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#20272;&#35745;&#65307;&#65288;b&#65289;&#22312;&#37319;&#26679;&#38454;&#27573;&#65292;&#20197;&#31934;&#24230;$\varepsilon$&#29983;&#25104;SDE&#35299;&#30340;iid&#26679;&#26412;&#12290;&#25152;&#24471;&#21040;&#30340;&#22797;&#26434;&#24230;&#20026;$O(T d \log(1/\varepsilon) m^2 + d m^{\beta+1} \log(T)/\varepsilon^2)$&#65292;&#20854;&#20013;$T$&#26159;SDE&#30340;&#26102;&#38388;&#27493;&#25968;&#65292;$\beta$&#26159;Fokker-Planck&#35299;&#30340;&#27491;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper deals with the problem of efficient sampling from a stochastic differential equation, given the drift function and the diffusion matrix. The proposed approach leverages a recent model for probabilities \citep{rudi2021psd} (the positive semi-definite -- PSD model) from which it is possible to obtain independent and identically distributed (i.i.d.) samples at precision $\varepsilon$ with a cost that is $m^2 d \log(1/\varepsilon)$ where $m$ is the dimension of the model, $d$ the dimension of the space. The proposed approach consists in: first, computing the PSD model that satisfies the Fokker-Planck equation (or its fractional variant) associated with the SDE, up to error $\varepsilon$, and then sampling from the resulting PSD model. Assuming some regularity of the Fokker-Planck solution (i.e. $\beta$-times differentiability plus some geometric condition on its zeros) We obtain an algorithm that: (a) in the preparatory phase obtains a PSD model with L2 distance $\varepsilon$ fr
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#35757;&#32451;&#20855;&#26377;ReLU&#21644;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#32500;&#24230;&#19979;&#30340;NP&#38590;&#24230;&#12290; &#22238;&#31572;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#22312;&#20108;&#32500;&#24773;&#20917;&#19979;&#26159;NP&#38590;&#30340;&#65292;&#27492;&#22806;&#22312;ReLU&#26696;&#20363;&#20013;&#35777;&#26126;&#20102;&#22266;&#23450;&#21442;&#25968;&#38382;&#39064;&#30340;&#21442;&#25968;&#21270;&#22266;&#23450;&#22797;&#26434;&#24230;&#32500;&#25968;&#21644;ReLU&#25968;&#37327;&#30340;&#32452;&#21512;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.17045</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#22266;&#23450;&#32500;&#24230;&#19978;&#26159;NP&#38590;&#30340;
&lt;/p&gt;
&lt;p&gt;
Training Neural Networks is NP-Hard in Fixed Dimension. (arXiv:2303.17045v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17045
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35757;&#32451;&#20855;&#26377;ReLU&#21644;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#32500;&#24230;&#19979;&#30340;NP&#38590;&#24230;&#12290; &#22238;&#31572;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#22312;&#20108;&#32500;&#24773;&#20917;&#19979;&#26159;NP&#38590;&#30340;&#65292;&#27492;&#22806;&#22312;ReLU&#26696;&#20363;&#20013;&#35777;&#26126;&#20102;&#22266;&#23450;&#21442;&#25968;&#38382;&#39064;&#30340;&#21442;&#25968;&#21270;&#22266;&#23450;&#22797;&#26434;&#24230;&#32500;&#25968;&#21644;ReLU&#25968;&#37327;&#30340;&#32452;&#21512;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36755;&#20837;&#25968;&#25454;&#32500;&#24230;&#21644;&#38544;&#34255;&#31070;&#32463;&#20803;&#25968;&#37327;&#26041;&#38754;&#23545;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21270;&#22797;&#26434;&#24615;&#30340;&#30740;&#31350;&#65292;&#32771;&#34385;ReLU&#21644;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#12290;&#23613;&#31649;&#36825;&#20123;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#36817;&#24180;&#26469;&#24050;&#32463;&#34987;&#22810;&#27425;&#30740;&#31350;&#65292;&#20294;&#20173;&#26377;&#20960;&#20010;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;Arora et al. [ICLR '18]&#21644;Khalife&#21644;Basu [IPCO '22]&#30340;&#38382;&#39064;&#65292;&#26174;&#31034;&#20004;&#20010;&#38382;&#39064;&#22312;&#20108;&#32500;&#24773;&#20917;&#19979;&#37117;&#26159;NP&#38590;&#30340;&#65292;&#36825;&#25490;&#38500;&#20102;&#20219;&#20309;&#24120;&#25968;&#32500;&#24230;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#22238;&#31572;&#20102;Froese&#31561;&#20154;[JAIR '22]&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#38646;&#22521;&#35757;&#35823;&#24046;&#30340;&#22235;&#20010;ReLU(&#25110;&#20004;&#20010;&#32447;&#24615;&#38408;&#20540;&#31070;&#32463;&#20803;)&#30340;W [1]-hardness&#12290;&#26368;&#21518;&#65292;&#22312;ReLU&#26696;&#20363;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21442;&#25968;&#21270;&#22266;&#23450;&#22797;&#26434;&#24230;&#32500;&#25968;&#21644;ReLU&#25968;&#37327;&#30340;&#32452;&#21512;&#21442;&#25968;&#65292;&#22914;&#26524;&#32593;&#32476;&#34987;&#20551;&#23450;&#20026;&#35745;&#31639;&#20984;&#26144;&#23556;&#65292;&#21017;&#21487;&#29992;&#20110;&#22266;&#23450;&#21442;&#25968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20960;&#20046;&#23436;&#20840;&#35299;&#20915;&#20102;&#36825;&#20123;&#21442;&#25968;&#30340;&#22797;&#26434;&#24615;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the parameterized complexity of training two-layer neural networks with respect to the dimension of the input data and the number of hidden neurons, considering ReLU and linear threshold activation functions. Albeit the computational complexity of these problems has been studied numerous times in recent years, several questions are still open. We answer questions by Arora et al. [ICLR '18] and Khalife and Basu [IPCO '22] showing that both problems are NP-hard for two dimensions, which excludes any polynomial-time algorithm for constant dimension. We also answer a question by Froese et al. [JAIR '22] proving W[1]-hardness for four ReLUs (or two linear threshold neurons) with zero training error. Finally, in the ReLU case, we show fixed-parameter tractability for the combined parameter number of dimensions and number of ReLUs if the network is assumed to compute a convex map. Our results settle the complexity status regarding these parameters almost completely.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#38543;&#26426;&#22810;&#33218;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65292;&#38024;&#23545;&#26410;&#30693;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#36890;&#36807;&#25191;&#34892;&#29305;&#24449;&#21521;&#37327;&#36716;&#25442;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17043</link><description>&lt;p&gt;
&#26080;&#35266;&#27979;&#19978;&#19979;&#25991;&#30340;&#32852;&#37030;&#38543;&#26426;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Stochastic Bandit Learning with Unobserved Context. (arXiv:2303.17043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#38543;&#26426;&#22810;&#33218;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65292;&#38024;&#23545;&#26410;&#30693;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#36890;&#36807;&#25191;&#34892;&#29305;&#24449;&#21521;&#37327;&#36716;&#25442;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26410;&#30693;&#19978;&#19979;&#25991;&#30340;&#32852;&#37030;&#38543;&#26426;&#22810;&#33218;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;M&#20010;&#20195;&#29702;&#38754;&#20020;&#19981;&#21516;&#30340;&#36172;&#21338;&#26426;&#24182;&#21327;&#20316;&#23398;&#20064;&#12290;&#36890;&#20449;&#27169;&#22411;&#30001;&#20013;&#22830;&#26381;&#21153;&#22120;&#32452;&#25104;&#65292;&#24182;&#19988;&#20195;&#29702;&#20250;&#23450;&#26399;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20849;&#20139;&#20854;&#20272;&#35745;&#32467;&#26524;&#65292;&#20197;&#20415;&#36873;&#25321;&#26368;&#20248;&#21160;&#20316;&#20197;&#26368;&#23567;&#21270;&#24635;&#21518;&#24724;&#12290;&#25105;&#20204;&#20551;&#35774;&#31934;&#30830;&#30340;&#19978;&#19979;&#25991;&#19981;&#21487;&#35266;&#23519;&#65292;&#20195;&#29702;&#20165;&#35266;&#27979;&#19978;&#19979;&#25991;&#30340;&#20998;&#24067;&#12290;&#20363;&#22914;&#65292;&#24403;&#19978;&#19979;&#25991;&#26412;&#36523;&#26159;&#22122;&#22768;&#27979;&#37327;&#25110;&#22522;&#20110;&#39044;&#27979;&#26426;&#21046;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#31639;&#27861;&#65292;&#20419;&#36827;&#20195;&#29702;&#20043;&#38388;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#36873;&#25321;&#19968;&#31995;&#21015;&#26368;&#20248;&#21160;&#20316;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#12290;&#36890;&#36807;&#25191;&#34892;&#29305;&#24449;&#21521;&#37327;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#38500;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#32447;&#24615;&#21442;&#25968;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of federated stochastic multi-arm contextual bandits with unknown contexts, in which M agents are faced with different bandits and collaborate to learn. The communication model consists of a central server and the agents share their estimates with the central server periodically to learn to choose optimal actions in order to minimize the total regret. We assume that the exact contexts are not observable and the agents observe only a distribution of the contexts. Such a situation arises, for instance, when the context itself is a noisy measurement or based on a prediction mechanism. Our goal is to develop a distributed and federated algorithm that facilitates collaborative learning among the agents to select a sequence of optimal actions so as to maximize the cumulative reward. By performing a feature vector transformation, we propose an elimination-based algorithm and prove the regret bound for linearly parametrized reward functions. Finally, we validated the perfo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#25972;&#20307;&#25968;&#25454;&#38598;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20256;&#36882;SJS&#12289;&#20462;&#27491;&#31867;&#21518;&#39564;&#27010;&#29575;&#12289;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#12289;SJS&#19982;&#21327;&#21464;&#37327;&#36716;&#31227;&#20851;&#31995;&#31561;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16971</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#20998;&#31867;&#20013;&#30340;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Sparse joint shift in multinomial classification. (arXiv:2303.16971v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#25972;&#20307;&#25968;&#25454;&#38598;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20256;&#36882;SJS&#12289;&#20462;&#27491;&#31867;&#21518;&#39564;&#27010;&#29575;&#12289;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#12289;SJS&#19982;&#21327;&#21464;&#37327;&#36716;&#31227;&#20851;&#31995;&#31561;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#65288;SJS&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#25972;&#20307;&#20559;&#31227;&#30340;&#21487;&#22788;&#29702;&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#36793;&#38469;&#20998;&#24067;&#20197;&#21450;&#21518;&#39564;&#27010;&#29575;&#21644;&#31867;&#26465;&#20214;&#29305;&#24449;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#22312;&#27809;&#26377;&#26631;&#31614;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#30446;&#26631;&#25968;&#25454;&#38598;&#25311;&#21512;SJS&#21487;&#33021;&#20250;&#20135;&#29983;&#26631;&#31614;&#30340;&#26377;&#25928;&#39044;&#27979;&#21644;&#31867;&#20808;&#39564;&#27010;&#29575;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#29305;&#24449;&#38598;&#20043;&#38388;&#20256;&#36882;SJS&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30446;&#26631;&#20998;&#24067;&#30340;&#31867;&#21518;&#39564;&#27010;&#29575;&#30340;&#26465;&#20214;&#20462;&#27491;&#20844;&#24335;&#65292;&#30830;&#23450;&#24615;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#20197;&#21450;SJS&#21644;&#21327;&#21464;&#37327;&#36716;&#31227;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#29992;&#20110;&#20272;&#35745;SJS&#29305;&#24449;&#30340;&#31639;&#27861;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#20250;&#22952;&#30861;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse joint shift (SJS) was recently proposed as a tractable model for general dataset shift which may cause changes to the marginal distributions of features and labels as well as the posterior probabilities and the class-conditional feature distributions. Fitting SJS for a target dataset without label observations may produce valid predictions of labels and estimates of class prior probabilities. We present new results on the transmission of SJS from sets of features to larger sets of features, a conditional correction formula for the class posterior probabilities under the target distribution, identifiability of SJS, and the relationship between SJS and covariate shift. In addition, we point out inconsistencies in the algorithms which were proposed for estimating the characteristics of SJS, as they could hamper the search for optimal solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22810;&#20010;&#27979;&#37327;&#21521;&#37327;&#20013;&#25512;&#26029;&#32852;&#21512;&#31232;&#30095;&#30340;&#21442;&#25968;&#21521;&#37327;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20849;&#21516;&#30340;&#20285;&#39532;&#20998;&#24067;&#36229;&#21442;&#25968;&#26469;&#24378;&#21046;&#32852;&#21512;&#31232;&#30095;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.16954</link><description>&lt;p&gt;
&#21033;&#29992;&#32852;&#21512;&#31232;&#30095;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging joint sparsity in hierarchical Bayesian learning. (arXiv:2303.16954v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22810;&#20010;&#27979;&#37327;&#21521;&#37327;&#20013;&#25512;&#26029;&#32852;&#21512;&#31232;&#30095;&#30340;&#21442;&#25968;&#21521;&#37327;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20849;&#21516;&#30340;&#20285;&#39532;&#20998;&#24067;&#36229;&#21442;&#25968;&#26469;&#24378;&#21046;&#32852;&#21512;&#31232;&#30095;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#22810;&#20010;&#27979;&#37327;&#21521;&#37327;&#20013;&#25512;&#26029;&#32852;&#21512;&#31232;&#30095;&#30340;&#21442;&#25968;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20026;&#27599;&#20010;&#21442;&#25968;&#21521;&#37327;&#20351;&#29992;&#21333;&#29420;&#30340;&#26465;&#20214;&#39640;&#26031;&#20808;&#39564;&#65292;&#24182;&#20351;&#29992;&#20849;&#21516;&#30340;&#20285;&#39532;&#20998;&#24067;&#36229;&#21442;&#25968;&#26469;&#24378;&#21046;&#32852;&#21512;&#31232;&#30095;&#24615;&#12290;&#24471;&#21040;&#30340;&#32852;&#21512;&#31232;&#30095;&#24615;&#20808;&#39564;&#19982;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#20102;&#19968;&#31995;&#21015;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#21253;&#25324;&#22810;&#32447;&#22280;&#30913;&#20849;&#25391;&#25104;&#20687;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#24120;&#29992;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a hierarchical Bayesian learning approach to infer jointly sparse parameter vectors from multiple measurement vectors. Our model uses separate conditionally Gaussian priors for each parameter vector and common gamma-distributed hyper-parameters to enforce joint sparsity. The resulting joint-sparsity-promoting priors are combined with existing Bayesian inference methods to generate a new family of algorithms. Our numerical experiments, which include a multi-coil magnetic resonance imaging application, demonstrate that our new approach consistently outperforms commonly used hierarchical Bayesian methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;NAS&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#30340;&#25805;&#20316;&#21363;&#21487;&#29983;&#25104;&#25509;&#36817;&#26368;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#32570;&#28857;&#21487;&#33021;&#24433;&#21709;&#20844;&#24179;&#27604;&#36739;&#24182;&#25552;&#20379;&#19981;&#21487;&#38752;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16938</link><description>&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#22522;&#20934;&#27979;&#35797;&#26159;&#21542;&#35774;&#35745;&#33391;&#22909;&#65311;&#23545;&#25805;&#20316;&#37325;&#35201;&#24615;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Are Neural Architecture Search Benchmarks Well Designed? A Deeper Look Into Operation Importance. (arXiv:2303.16938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;NAS&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#30340;&#25805;&#20316;&#21363;&#21487;&#29983;&#25104;&#25509;&#36817;&#26368;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#32570;&#28857;&#21487;&#33021;&#24433;&#21709;&#20844;&#24179;&#27604;&#36739;&#24182;&#25552;&#20379;&#19981;&#21487;&#38752;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#22522;&#20934;&#27979;&#35797;&#26174;&#33879;&#25552;&#39640;&#20102;&#24320;&#21457;&#21644;&#27604;&#36739;NAS&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#25968;&#21315;&#20010;&#35757;&#32451;&#36807;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#20449;&#24687;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#34920;&#26684;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#20960;&#20010;&#32570;&#28857;&#65292;&#21487;&#33021;&#20250;&#38459;&#30861;&#20844;&#24179;&#27604;&#36739;&#24182;&#25552;&#20379;&#19981;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;NAS-Bench-101&#12289;NAS-Bench-201&#21644;TransNAS-Bench-101&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#30340;&#36890;&#29992;&#24615;&#20197;&#21450;&#19981;&#21516;&#25805;&#20316;&#22914;&#20309;&#24433;&#21709;&#25152;&#29983;&#25104;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#38656;&#35201;&#25805;&#20316;&#27744;&#30340;&#19968;&#37096;&#20998;&#21363;&#21487;&#29983;&#25104;&#25509;&#36817;&#26368;&#39640;&#24615;&#33021;&#33539;&#22260;&#30340;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#24615;&#33021;&#20998;&#24067;&#20855;&#26377;&#36127;&#20559;&#26012;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) benchmarks significantly improved the capability of developing and comparing NAS methods while at the same time drastically reduced the computational overhead by providing meta-information about thousands of trained neural networks. However, tabular benchmarks have several drawbacks that can hinder fair comparisons and provide unreliable results. These usually focus on providing a small pool of operations in heavily constrained search spaces -- usually cell-based neural networks with pre-defined outer-skeletons. In this work, we conducted an empirical analysis of the widely used NAS-Bench-101, NAS-Bench-201 and TransNAS-Bench-101 benchmarks in terms of their generability and how different operations influence the performance of the generated architectures. We found that only a subset of the operation pool is required to generate architectures close to the upper-bound of the performance range. Also, the performance distribution is negatively skewed, havi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;</title><link>http://arxiv.org/abs/2303.16372</link><description>&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#30340;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#26102;&#31169;&#26377;&#23398;&#20064;&#31639;&#27861;&#30340;&#35821;&#20041;&#20445;&#35777;&#24378;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23548;&#20986;&#38750;&#28176;&#36827;&#37327;&#32423;&#19979;&#30028;&#26469;&#30740;&#31350;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21644;&#24230;&#37327;&#38544;&#31169;&#65288;mDP&#65289;&#30340;&#23398;&#20064;&#22120;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#23545;mDP&#30340;&#20998;&#26512;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#23545;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;DP-SGD&#21644;Projected Noisy SGD&#36827;&#34892;&#20102;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#25193;&#23637;&#38544;&#31169;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#23637;&#20102;PCA-Net&#30340;&#36817;&#20284;&#29702;&#35770;&#65292;&#24471;&#20986;&#20102;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#24182;&#35782;&#21035;&#20986;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#28508;&#22312;&#38556;&#30861;&#65306;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#21644;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16317</link><description>&lt;p&gt;
PCA-Net&#65306;&#25805;&#20316;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#19978;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Operator learning with PCA-Net: upper and lower complexity bounds. (arXiv:2303.16317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#23637;&#20102;PCA-Net&#30340;&#36817;&#20284;&#29702;&#35770;&#65292;&#24471;&#20986;&#20102;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#24182;&#35782;&#21035;&#20986;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#28508;&#22312;&#38556;&#30861;&#65306;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#21644;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;PCA-Net&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#23427;&#23558;&#20027;&#25104;&#20998;&#20998;&#26512;(PCA)&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#36924;&#36817;&#28508;&#22312;&#30340;&#31639;&#23376;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#36817;&#20284;&#29702;&#35770;&#30340;&#21457;&#23637;&#65292;&#25913;&#36827;&#24182;&#26174;&#30528;&#25193;&#23637;&#20102;&#27492;&#26041;&#21521;&#30340;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312;&#23450;&#24615;&#30028;&#38480;&#26041;&#38754;&#65292;&#26412;&#25991;&#24471;&#20986;&#20102;&#26032;&#39062;&#30340;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#22312;&#23545;&#28508;&#22312;&#31639;&#23376;&#21644;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#26368;&#23567;&#20551;&#35774;&#30340;&#21069;&#25552;&#19979;&#12290;&#22312;&#23450;&#37327;&#38480;&#21046;&#26041;&#38754;&#65292;&#26412;&#25991;&#35782;&#21035;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#20004;&#20010;&#28508;&#22312;&#38556;&#30861;&#65292;&#36890;&#36807;&#23548;&#20986;&#19979;&#30028;&#36827;&#34892;&#20102;&#20005;&#26684;&#35777;&#26126;&#65292;&#31532;&#19968;&#20010;&#38556;&#30861;&#19982;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#26377;&#20851;&#65292;&#30001;PCA&#29305;&#24449;&#20540;&#30340;&#32531;&#24930;&#34928;&#20943;&#26469;&#34913;&#37327;&#65307;&#21478;&#19968;&#20010;&#38556;&#30861;&#28041;&#21450;&#26080;&#38480;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#30340;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators are gaining attention in computational science and engineering. PCA-Net is a recently proposed neural operator architecture which combines principal component analysis (PCA) with neural networks to approximate an underlying operator. The present work develops approximation theory for this approach, improving and significantly extending previous work in this direction. In terms of qualitative bounds, this paper derives a novel universal approximation result, under minimal assumptions on the underlying operator and the data-generating distribution. In terms of quantitative bounds, two potential obstacles to efficient operator learning with PCA-Net are identified, and made rigorous through the derivation of lower complexity bounds; the first relates to the complexity of the output distribution, measured by a slow decay of the PCA eigenvalues. The other obstacle relates the inherent complexity of the space of operators between infinite-dimensional input and output spaces, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#20219;&#20309;&#22312;&#22343;&#21248;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#36716;&#25442;&#25104;&#19968;&#20010;&#22312;&#20219;&#24847;&#26410;&#30693;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#32780;&#19988;&#23545;&#20110;&#21333;&#35843;&#20998;&#24067;&#65292;&#21482;&#38656;&#35201;&#29992;$\mathcal{D}$&#20013;&#30340;&#26679;&#26412;&#12290;&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#19968;&#20010;&#31639;&#27861;&#23558;$\mathcal{D}$&#36924;&#36817;&#25104;&#30001;&#23376;&#31435;&#26041;&#20307;&#28151;&#21512;&#32780;&#25104;&#30340;&#28151;&#21512;&#22343;&#21248;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.16208</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#24067;&#20998;&#35299;&#25552;&#39640;&#22343;&#21248;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Lifting uniform learners via distributional decomposition. (arXiv:2303.16208v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#20219;&#20309;&#22312;&#22343;&#21248;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#36716;&#25442;&#25104;&#19968;&#20010;&#22312;&#20219;&#24847;&#26410;&#30693;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#32780;&#19988;&#23545;&#20110;&#21333;&#35843;&#20998;&#24067;&#65292;&#21482;&#38656;&#35201;&#29992;$\mathcal{D}$&#20013;&#30340;&#26679;&#26412;&#12290;&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#19968;&#20010;&#31639;&#27861;&#23558;$\mathcal{D}$&#36924;&#36817;&#25104;&#30001;&#23376;&#31435;&#26041;&#20307;&#28151;&#21512;&#32780;&#25104;&#30340;&#28151;&#21512;&#22343;&#21248;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20219;&#20309;&#22312;&#22343;&#21248;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#36716;&#25442;&#25104;&#19968;&#20010;&#22312;&#20219;&#24847;&#26410;&#30693;&#20998;&#24067;$\mathcal{D}$&#19979;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#36716;&#25442;&#25928;&#29575;&#38543;$\mathcal{D}$&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#32780;&#21464;&#21270;&#65292;&#23545;&#20110;&#22312;$\{\pm 1\}^n$&#19978;&#30340;&#20998;&#24067;&#65292;&#20854;pmf&#30001;&#28145;&#24230;&#20026;$d$&#30340;&#20915;&#31574;&#26641;&#35745;&#31639;&#65292;&#21017;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$\mathrm{poly}(n, (md)^d)$&#65292;&#20854;&#20013;$m$&#26159;&#21407;&#22987;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23545;&#20110;&#21333;&#35843;&#20998;&#24067;&#65292;&#25105;&#20204;&#30340;&#36716;&#25442;&#20165;&#20351;&#29992;$\mathcal{D}$&#20013;&#30340;&#26679;&#26412;&#65292;&#32780;&#23545;&#20110;&#19968;&#33324;&#20998;&#24067;&#65292;&#25105;&#20204;&#20351;&#29992;&#23376;&#31435;&#26041;&#20307;&#26465;&#20214;&#26679;&#26412;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#19968;&#20010;&#31639;&#27861;&#65292;&#23427;&#22312;&#32473;&#20986;$\mathcal{D}$&#30340;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26368;&#20248;&#20915;&#31574;&#26641;&#20998;&#35299;$\mathcal{D}$&#65306;&#19968;&#20010;&#36924;&#36817;&#20102;$\mathcal{D}$&#30340;&#28151;&#21512;&#22343;&#21248;&#20998;&#24067;&#30340;&#20998;&#31163;&#23376;&#31435;&#26041;&#20307;&#12290;&#36890;&#36807;&#36825;&#20010;&#20998;&#35299;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#23376;&#31435;&#26041;&#20307;&#19978;&#36816;&#34892;&#22343;&#21248;&#20998;&#24067;&#23398;&#20064;&#22120;&#65292;&#24182;&#23558;&#32467;&#26524;&#21512;&#24182;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how any PAC learning algorithm that works under the uniform distribution can be transformed, in a blackbox fashion, into one that works under an arbitrary and unknown distribution $\mathcal{D}$. The efficiency of our transformation scales with the inherent complexity of $\mathcal{D}$, running in $\mathrm{poly}(n, (md)^d)$ time for distributions over $\{\pm 1\}^n$ whose pmfs are computed by depth-$d$ decision trees, where $m$ is the sample complexity of the original algorithm. For monotone distributions our transformation uses only samples from $\mathcal{D}$, and for general ones it uses subcube conditioning samples.  A key technical ingredient is an algorithm which, given the aforementioned access to $\mathcal{D}$, produces an optimal decision tree decomposition of $\mathcal{D}$: an approximation of $\mathcal{D}$ as a mixture of uniform distributions over disjoint subcubes. With this decomposition in hand, we run the uniform-distribution learner on each subcube and combine the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#27010;&#24565;&#30340;UQ&#24230;&#37327;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#24050;&#26377;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;UQ&#24230;&#37327;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.07170</link><description>&lt;p&gt;
&#22522;&#20110;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#27010;&#24565;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24230;&#37327;&#30340;&#39564;&#35777;&#65306;&#19968;&#20010;&#20837;&#38376;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Validation of uncertainty quantification metrics: a primer based on the consistency and adaptivity concepts. (arXiv:2303.07170v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#27010;&#24565;&#30340;UQ&#24230;&#37327;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#24050;&#26377;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;UQ&#24230;&#37327;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#39564;&#35777;&#30340;&#23454;&#36341;&#65292;&#23588;&#20854;&#26159;&#22312;&#29289;&#29702;&#21270;&#23398;&#31185;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20381;&#36182;&#20110;&#20960;&#31181;&#22270;&#24418;&#26041;&#27861;&#65288;&#25955;&#28857;&#22270;&#12289;&#26657;&#20934;&#26354;&#32447;&#12289;&#21487;&#38752;&#24615;&#22270;&#21644;&#32622;&#20449;&#26354;&#32447;&#65289;&#65292;&#36825;&#20123;&#22270;&#24418;&#26041;&#27861;&#25506;&#32034;&#26657;&#20934;&#30340;&#20114;&#34917;&#26041;&#38754;&#65292;&#20294;&#26377;&#20123;&#26041;&#38754;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#20363;&#22914;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#27809;&#26377;&#19968;&#31181;&#28041;&#21450;&#21040;UQ&#24230;&#37327;&#22312;&#36755;&#20837;&#29305;&#24449;&#33539;&#22260;&#20869;&#30340;&#21487;&#38752;&#24615;&#65288;&#36866;&#24212;&#24615;&#65289;&#12290;&#22522;&#20110;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#20114;&#34917;&#27010;&#24565;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#26041;&#24046;&#21644;&#38388;&#38548;&#30340;UQ&#24230;&#37327;&#30340;&#24120;&#35265;&#39564;&#35777;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#20379;&#26356;&#22909;&#30340;&#29702;&#35299;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20171;&#32461;UQ&#39564;&#35777;&#65292;&#24182;&#20174;&#20960;&#20010;&#22522;&#26412;&#35268;&#21017;&#20013;&#23548;&#20986;&#25152;&#26377;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20174;&#26368;&#36817;&#30340;&#29289;&#29702;&#21270;&#23398;&#26426;&#22120;&#23398;&#20064;UQ&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#20195;&#34920;&#24615;&#31034;&#20363;&#19978;&#36827;&#34892;&#20102;&#35828;&#26126;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The practice of uncertainty quantification (UQ) validation, notably in machine learning for the physico-chemical sciences, rests on several graphical methods (scattering plots, calibration curves, reliability diagrams and confidence curves) which explore complementary aspects of calibration, without covering all the desirable ones. For instance, none of these methods deals with the reliability of UQ metrics across the range of input features (adaptivity). Based on the complementary concepts of consistency and adaptivity, the toolbox of common validation methods for variance- and intervals- based UQ metrics is revisited with the aim to provide a better grasp on their capabilities. This study is conceived as an introduction to UQ validation, and all methods are derived from a few basic rules. The methods are illustrated and tested on synthetic datasets and representative examples extracted from the recent physico-chemical machine learning UQ literature.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#20449;&#24687;&#22330;&#29702;&#35770;(IFT)&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;(PIFT)&#65292;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2301.07609</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#30693;&#35782;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#30340;&#20449;&#24687;&#22330;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification. (arXiv:2301.07609v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#20449;&#24687;&#22330;&#29702;&#35770;(IFT)&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;(PIFT)&#65292;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#32467;&#21512;&#29289;&#29702;&#23398;&#30693;&#35782;&#26159;&#24314;&#27169;&#31995;&#32479;&#30340;&#24378;&#26377;&#21147;&#25216;&#26415;&#12290;&#27492;&#31867;&#27169;&#22411;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#27979;&#37327;&#32467;&#26524;&#19982;&#24050;&#30693;&#29289;&#29702;&#23450;&#24459;&#30456;&#32467;&#21512;&#65292;&#39640;&#25928;&#22320;&#27714;&#35299;&#22522;&#26412;&#22330;&#12290;&#30001;&#20110;&#35768;&#22810;&#31995;&#32479;&#21253;&#21547;&#26410;&#30693;&#20803;&#32032;&#65292;&#22914;&#32570;&#22833;&#21442;&#25968;&#12289;&#22024;&#26434;&#25968;&#25454;&#25110;&#19981;&#23436;&#25972;&#30340;&#29289;&#29702;&#23450;&#24459;&#65292;&#22240;&#27492;&#36825;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#22788;&#29702;&#25152;&#26377;&#21464;&#37327;&#30340;&#24120;&#35265;&#25216;&#26415;&#36890;&#24120;&#21462;&#20915;&#20110;&#29992;&#20110;&#36817;&#20284;&#21518;&#39564;&#30340;&#25968;&#20540;&#26041;&#26696;&#65292;&#24182;&#19988;&#24076;&#26395;&#26377;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#12290;&#20449;&#24687;&#22330;&#29702;&#35770;&#65288;IFT&#65289;&#25552;&#20379;&#20102;&#23545;&#19981;&#19968;&#23450;&#26159;&#39640;&#26031;&#22330;&#30340;&#22330;&#36827;&#34892;&#32479;&#35745;&#23398;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#26469;&#25193;&#23637;IFT&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;&#65288;PIFT&#65289;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven approaches coupled with physical knowledge are powerful techniques to model systems. The goal of such models is to efficiently solve for the underlying field by combining measurements with known physical laws. As many systems contain unknown elements, such as missing parameters, noisy data, or incomplete physical laws, this is widely approached as an uncertainty quantification problem. The common techniques to handle all the variables typically depend on the numerical scheme used to approximate the posterior, and it is desirable to have a method which is independent of any such discretization. Information field theory (IFT) provides the tools necessary to perform statistics over fields that are not necessarily Gaussian. We extend IFT to physics-informed IFT (PIFT) by encoding the functional priors with information about the physical laws which describe the field. The posteriors derived from this PIFT remain independent of any numerical scheme and can capture multiple modes,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20999;&#29255;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2212.08049</link><description>&lt;p&gt;
&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;
&lt;/p&gt;
&lt;p&gt;
Sliced Optimal Partial Transport. (arXiv:2212.08049v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20999;&#29255;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#24050;&#32463;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#31185;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21464;&#24471;&#26497;&#20854;&#27969;&#34892;&#12290;OT&#38382;&#39064;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#28304;&#21644;&#30446;&#26631;&#27979;&#24230;&#30340;&#24635;&#36136;&#37327;&#30456;&#31561;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#30340;&#24212;&#29992;&#12290;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#65288;OPT&#65289;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#30340;&#26041;&#27861;&#12290;&#19982;OT&#38382;&#39064;&#31867;&#20284;&#65292;OPT&#30340;&#35745;&#31639;&#20381;&#36182;&#20110;&#35299;&#20915;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65288;&#36890;&#24120;&#22312;&#39640;&#32500;&#24230;&#20013;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#21464;&#24471;&#35745;&#31639;&#19978;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;OPT&#38382;&#39064;&#30340;&#26377;&#25928;&#31639;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#36981;&#24490;&#20999;&#29255;OT&#36317;&#31163;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#21033;&#29992;&#20999;&#29255;&#23450;&#20041;&#20102;&#20999;&#29255;OPT&#36317;&#31163;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20999;&#29255;OPT-based&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#20540;&#23454;&#39564;&#20013;&#30340;&#35745;&#31639;&#21644;&#31934;&#24230;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;Sliced-OPT&#22312;&#22122;&#22768;&#28857;&#20113;&#37197;&#20934;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) has become exceedingly popular in machine learning, data science, and computer vision. The core assumption in the OT problem is the equal total amount of mass in source and target measures, which limits its application. Optimal Partial Transport (OPT) is a recently proposed solution to this limitation. Similar to the OT problem, the computation of OPT relies on solving a linear programming problem (often in high dimensions), which can become computationally prohibitive. In this paper, we propose an efficient algorithm for calculating the OPT problem between two non-negative measures in one dimension. Next, following the idea of sliced OT distances, we utilize slicing to define the sliced OPT distance. Finally, we demonstrate the computational and accuracy benefits of the sliced OPT-based method in various numerical experiments. In particular, we show an application of our proposed Sliced-OPT in noisy point cloud registration.
&lt;/p&gt;</description></item><item><title>Packed-Ensembles&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#20869;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#32467;&#26500;&#21270;&#38598;&#21512;&#65292;&#23427;&#36890;&#36807;&#31934;&#24515;&#35843;&#33410;&#32534;&#30721;&#31354;&#38388;&#30340;&#32500;&#24230;&#26469;&#35774;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#25439;&#22833;&#25928;&#26524;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.09184</link><description>&lt;p&gt;
&#32039;&#20945;&#38598;&#25104;&#29992;&#20110;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Packed-Ensembles for Efficient Uncertainty Estimation. (arXiv:2210.09184v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09184
&lt;/p&gt;
&lt;p&gt;
Packed-Ensembles&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#20869;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#32467;&#26500;&#21270;&#38598;&#21512;&#65292;&#23427;&#36890;&#36807;&#31934;&#24515;&#35843;&#33410;&#32534;&#30721;&#31354;&#38388;&#30340;&#32500;&#24230;&#26469;&#35774;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#25439;&#22833;&#25928;&#26524;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#26159;&#23454;&#29616;&#20851;&#38190;&#25351;&#26631;&#65288;&#22914;&#20934;&#30830;&#24615;&#12289;&#26657;&#20934;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#65289;&#21331;&#36234;&#24615;&#33021;&#30340;&#31361;&#20986;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#29616;&#23454;&#31995;&#32479;&#30340;&#30828;&#20214;&#38480;&#21046;&#38480;&#21046;&#20102;&#26356;&#23567;&#30340;&#38598;&#21512;&#21644;&#36739;&#20302;&#23481;&#37327;&#30340;&#32593;&#32476;&#65292;&#20005;&#37325;&#25439;&#23475;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Packed-Ensembles&#65288;PE&#65289;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31934;&#24515;&#35843;&#33410;&#20854;&#32534;&#30721;&#31354;&#38388;&#30340;&#32500;&#24230;&#26469;&#35774;&#35745;&#21644;&#35757;&#32451;&#36731;&#37327;&#32423;&#32467;&#26500;&#21270;&#38598;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;&#32452;&#21367;&#31215;&#23558;&#38598;&#21512;&#24182;&#34892;&#21270;&#20026;&#21333;&#20010;&#20849;&#20139;&#39592;&#24178;&#65292;&#24182;&#36827;&#34892;&#21069;&#21521;&#20256;&#36882;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;PE&#26088;&#22312;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#23384;&#38480;&#21046;&#20869;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Ensembles (DE) are a prominent approach for achieving excellent performance on key metrics such as accuracy, calibration, uncertainty estimation, and out-of-distribution detection. However, hardware limitations of real-world systems constrain to smaller ensembles and lower-capacity networks, significantly deteriorating their performance and properties. We introduce Packed-Ensembles (PE), a strategy to design and train lightweight structured ensembles by carefully modulating the dimension of their encoding space. We leverage grouped convolutions to parallelize the ensemble into a single shared backbone and forward pass to improve training and inference speeds. PE is designed to operate within the memory limits of a standard neural network. Our extensive research indicates that PE accurately preserves the properties of DE, such as diversity, and performs equally well in terms of accuracy, calibration, out-of-distribution detection, and robustness to distribution shift. We make our c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39030;&#28857;&#23545;&#40784;&#30340;&#24179;&#22343;&#22270;&#65292;&#32858;&#31867;&#24179;&#22343;&#22270;&#21644;&#28151;&#28102;&#32593;&#32476;&#21305;&#37197;&#30340;&#31574;&#30053;&#65292;&#27604;&#36215;&#20256;&#32479;&#30340;&#20840;&#23616;&#24179;&#22343;&#22270;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25552;&#39640;&#21305;&#37197;&#24615;&#33021;&#21644;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2205.03486</link><description>&lt;p&gt;
&#32858;&#31867;&#22270;&#21305;&#37197;&#29992;&#20110;&#26631;&#31614;&#24674;&#22797;&#21644;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Clustered Graph Matching for Label Recovery and Graph Classification. (arXiv:2205.03486v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39030;&#28857;&#23545;&#40784;&#30340;&#24179;&#22343;&#22270;&#65292;&#32858;&#31867;&#24179;&#22343;&#22270;&#21644;&#28151;&#28102;&#32593;&#32476;&#21305;&#37197;&#30340;&#31574;&#30053;&#65292;&#27604;&#36215;&#20256;&#32479;&#30340;&#20840;&#23616;&#24179;&#22343;&#22270;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25552;&#39640;&#21305;&#37197;&#24615;&#33021;&#21644;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#19968;&#32452;&#39030;&#28857;&#23545;&#40784;&#32593;&#32476;&#21644;&#19968;&#20010;&#39069;&#22806;&#30340;&#26631;&#31614;&#28151;&#28102;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39030;&#28857;&#23545;&#40784;&#38598;&#21512;&#20013;&#30340;&#20449;&#21495;&#26469;&#24674;&#22797;&#28151;&#28102;&#32593;&#32476;&#26631;&#31614;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#23558;&#28151;&#28102;&#32593;&#32476;&#19982;&#19981;&#21516;&#31890;&#24230;&#19979;&#30340;&#39030;&#28857;&#23545;&#40784;&#38598;&#21512;&#20013;&#30340;&#24179;&#22343;&#32593;&#32476;&#36827;&#34892;&#21305;&#37197;&#12290;&#25105;&#20204;&#35777;&#26126;&#24182;&#35777;&#23454;&#65292;&#22312;&#32593;&#32476;&#26469;&#33258;&#19981;&#21516;&#32593;&#32476;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#32593;&#32476;&#32858;&#31867;&#21040;&#31867;&#20013;&#65292;&#28982;&#21518;&#23558;&#26032;&#32593;&#32476;&#21305;&#37197;&#21040;&#32858;&#31867;&#24179;&#22343;&#20540;&#65292;&#21487;&#20197;&#27604;&#23558;&#20854;&#21305;&#37197;&#21040;&#20840;&#23616;&#24179;&#22343;&#22270;&#20135;&#29983;&#26356;&#39640;&#30340;&#21305;&#37197;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#30456;&#23545;&#20110;&#27599;&#20010;&#32858;&#31867;&#24179;&#22343;&#20540;&#30340;&#22270;&#21305;&#37197;&#30446;&#26631;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21516;&#26102;&#23545;&#28151;&#28102;&#22270;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#39030;&#28857;&#26631;&#31614;&#24674;&#22797;&#12290;&#36825;&#20123;&#29702;&#35770;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26377;&#21551;&#31034;&#24847;&#20041;&#30340;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#21305;&#37197;&#20154;&#31867;&#36830;&#25509;&#20307;&#26469;&#24471;&#21040;&#26356;&#22810;&#24041;&#22266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a collection of vertex-aligned networks and an additional label-shuffled network, we propose procedures for leveraging the signal in the vertex-aligned collection to recover the labels of the shuffled network. We consider matching the shuffled network to averages of the networks in the vertex-aligned collection at different levels of granularity. We demonstrate both in theory and practice that if the graphs come from different network classes, then clustering the networks into classes followed by matching the new graph to cluster-averages can yield higher fidelity matching performance than matching to the global average graph. Moreover, by minimizing the graph matching objective function with respect to each cluster average, this approach simultaneously classifies and recovers the vertex labels for the shuffled graph. These theoretical developments are further reinforced via an illuminating real data experiment matching human connectomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340; $\ell_{2,1}$ &#21644; $\ell_{F}$ &#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#33719;&#24471;&#26368;&#30456;&#20851;&#30340;&#20960;&#20010;&#29305;&#24449;&#65292;&#24182;&#22312;&#27969;&#24418;&#27491;&#21017;&#21270;&#20013;&#23454;&#29616;&#20102;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#30340;&#39640;&#24230;&#31283;&#20581;&#30340;&#37051;&#22495;&#22270;&#12290;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2204.06445</link><description>&lt;p&gt;
&#38543;&#26426;&#27969;&#24418;&#37319;&#26679;&#21644;&#32852;&#21512;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Random Manifold Sampling and Joint Sparse Regularization for Multi-label Feature Selection. (arXiv:2204.06445v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340; $\ell_{2,1}$ &#21644; $\ell_{F}$ &#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#33719;&#24471;&#26368;&#30456;&#20851;&#30340;&#20960;&#20010;&#29305;&#24449;&#65292;&#24182;&#22312;&#27969;&#24418;&#27491;&#21017;&#21270;&#20013;&#23454;&#29616;&#20102;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#30340;&#39640;&#24230;&#31283;&#20581;&#30340;&#37051;&#22495;&#22270;&#12290;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#23398;&#20064;&#36890;&#24120;&#29992;&#20110;&#25366;&#25496;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#29305;&#24449;&#36873;&#25321;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#29305;&#24449;&#20445;&#30041;&#23613;&#21487;&#33021;&#22810;&#30340;&#20449;&#24687;&#12290; $\ell_{2,1}$ &#27491;&#21017;&#21270;&#21487;&#20197;&#33719;&#24471;&#31232;&#30095;&#31995;&#25968;&#30697;&#38453;&#65292;&#20294;&#19981;&#33021;&#26377;&#25928;&#22320;&#35299;&#20915;&#22810;&#37325;&#20849;&#32447;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#35299;&#20915; $\ell_{2,1}$ &#21644; $\ell_{F}$ &#27491;&#21017;&#21270;&#30340;&#32852;&#21512;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26469;&#33719;&#21462;&#26368;&#30456;&#20851;&#30340;&#20960;&#20010;&#29305;&#24449;&#12290;&#22312;&#27969;&#24418;&#27491;&#21017;&#21270;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#32852;&#21512;&#20449;&#24687;&#30697;&#38453;&#23454;&#29616;&#20102;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#30340;&#39640;&#24230;&#31283;&#20581;&#30340;&#37051;&#22495;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#35299;&#20915;&#35813;&#27169;&#22411;&#30340;&#31639;&#27861;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label learning is usually used to mine the correlation between features and labels, and feature selection can retain as much information as possible through a small number of features. $\ell_{2,1}$ regularization method can get sparse coefficient matrix, but it can not solve multicollinearity problem effectively. The model proposed in this paper can obtain the most relevant few features by solving the joint constrained optimization problems of $\ell_{2,1}$ and $\ell_{F}$ regularization.In manifold regularization, we implement random walk strategy based on joint information matrix, and get a highly robust neighborhood graph.In addition, we given the algorithm for solving the model and proved its convergence.Comparative experiments on real-world data sets show that the proposed method outperforms other methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#33539;&#25968;&#32422;&#26463;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#28369;&#20989;&#25968;&#31867;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#36924;&#36817;&#35823;&#24046;&#26377;&#19978;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#24212;&#29992;&#32467;&#26524;&#20998;&#26512;&#20102;&#22238;&#24402;&#21644;GAN&#20998;&#24067;&#20272;&#35745;&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#65292;&#26368;&#32456;&#35777;&#26126;&#20102;&#24403;GAN&#30340;&#21028;&#21035;&#22120;&#36873;&#25321;&#21512;&#36866;&#30340;&#20855;&#33539;&#25968;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;&#27010;&#29575;&#20998;&#24067;&#30340;&#26368;&#20248;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2201.09418</link><description>&lt;p&gt;
&#20855;&#33539;&#25968;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#35823;&#24046;&#30028;&#19982;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Approximation bounds for norm constrained neural networks with applications to regression and GANs. (arXiv:2201.09418v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#33539;&#25968;&#32422;&#26463;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#28369;&#20989;&#25968;&#31867;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#36924;&#36817;&#35823;&#24046;&#26377;&#19978;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#24212;&#29992;&#32467;&#26524;&#20998;&#26512;&#20102;&#22238;&#24402;&#21644;GAN&#20998;&#24067;&#20272;&#35745;&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#65292;&#26368;&#32456;&#35777;&#26126;&#20102;&#24403;GAN&#30340;&#21028;&#21035;&#22120;&#36873;&#25321;&#21512;&#36866;&#30340;&#20855;&#33539;&#25968;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;&#27010;&#29575;&#20998;&#24067;&#30340;&#26368;&#20248;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24102;&#26435;&#37325;&#33539;&#25968;&#32422;&#26463;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#23545;&#20110;&#24179;&#28369;&#30340;&#20989;&#25968;&#31867;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#36924;&#36817;&#35823;&#24046;&#19978;&#19979;&#30028;&#12290;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;Rademacher&#22797;&#26434;&#24230;&#23548;&#20986;&#19979;&#30028;&#35777;&#26126;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#30740;&#31350;&#20215;&#20540;&#12290;&#25105;&#20204;&#24212;&#29992;&#36825;&#20123;&#36924;&#36817;&#35823;&#24046;&#30028;&#38480;&#26469;&#20998;&#26512;&#20351;&#29992;&#20855;&#33539;&#25968;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22238;&#24402;&#21644;GAN&#20998;&#24067;&#20272;&#35745;&#30340;&#25910;&#25947;&#24615;&#12290;&#29305;&#21035;&#30340;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36807;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#24403;&#21028;&#21035;&#22120;&#36873;&#25321;&#21512;&#36866;&#30340;&#20855;&#33539;&#25968;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;GAN&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;&#27010;&#29575;&#20998;&#24067;&#30340;&#26368;&#20248;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the approximation capacity of ReLU neural networks with norm constraint on the weights. We prove upper and lower bounds on the approximation error of these networks for smooth function classes. The lower bound is derived through the Rademacher complexity of neural networks, which may be of independent interest. We apply these approximation bounds to analyze the convergences of regression using norm constrained neural networks and distribution estimation by GANs. In particular, we obtain convergence rates for over-parameterized neural networks. It is also shown that GANs can achieve optimal rate of learning probability distributions, when the discriminator is a properly chosen norm constrained neural network.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#35745;&#19978;&#24847;&#20041;&#30340;&#36817;&#20284;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#25442;&#22120;&#30340;SM&#36817;&#20284;&#22312;&#24067;&#23572;&#30005;&#36335;&#21644;&#22270;&#28789;&#26426;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#22312;&#20110;&#25506;&#32034;&#36817;&#20284;&#32593;&#32476;&#24212;&#35813;&#20855;&#26377;&#33391;&#22909;&#30340;&#32479;&#35745;&#21487;&#23398;&#24615;&#30340;&#27010;&#24565;&#65292;&#36798;&#21040;&#26356;&#26377;&#24847;&#20041;&#30340;&#36817;&#20284;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2107.13163</link><description>&lt;p&gt;
&#32479;&#35745;&#19978;&#24847;&#20041;&#30340;&#36817;&#20284;&#65306;&#19968;&#31181;&#22312;&#21464;&#25442;&#22120;&#20013;&#36817;&#20284;&#22270;&#28789;&#26426;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers. (arXiv:2107.13163v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.13163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#35745;&#19978;&#24847;&#20041;&#30340;&#36817;&#20284;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#25442;&#22120;&#30340;SM&#36817;&#20284;&#22312;&#24067;&#23572;&#30005;&#36335;&#21644;&#22270;&#28789;&#26426;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#22312;&#20110;&#25506;&#32034;&#36817;&#20284;&#32593;&#32476;&#24212;&#35813;&#20855;&#26377;&#33391;&#22909;&#30340;&#32479;&#35745;&#21487;&#23398;&#24615;&#30340;&#27010;&#24565;&#65292;&#36798;&#21040;&#26356;&#26377;&#24847;&#20041;&#30340;&#36817;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35770;&#19978;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#20998;&#26512;&#23427;&#20204;&#21487;&#20197;&#36817;&#20284;&#30340;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#29702;&#35770;&#20013;&#30340;&#26500;&#36896;&#21487;&#33021;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#22240;&#27492;&#24847;&#20041;&#19981;&#22826;&#26126;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#35745;&#19978;&#24847;&#20041;&#30340;&#65288;SM&#65289;&#36817;&#20284;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#35201;&#27714;&#36817;&#20284;&#32593;&#32476;&#20855;&#26377;&#33391;&#22909;&#30340;&#32479;&#35745;&#21487;&#23398;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#20989;&#25968;&#31867;&#21035;&#30340;SM&#36817;&#20284;&#65306;&#24067;&#23572;&#30005;&#36335;&#21644;&#22270;&#28789;&#26426;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;SM&#36817;&#20284;&#24067;&#23572;&#30005;&#36335;&#65292;&#37319;&#26679;&#22797;&#26434;&#24230;&#20165;&#21462;&#20915;&#20110;&#30005;&#36335;&#22823;&#23567;&#65292;&#32780;&#19981;&#26159;&#32593;&#32476;&#22823;&#23567;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21464;&#25442;&#22120;&#21487;&#20197;SM&#36817;&#20284;&#35745;&#31639;&#26102;&#38388;&#21463;$T$&#38480;&#21046;&#30340;&#22270;&#28789;&#26426;&#65292;&#37319;&#26679;&#22797;&#26434;&#24230;&#22810;&#39033;&#24335;&#22320;&#21462;&#20915;&#20110;&#23383;&#27597;&#22823;&#23567;&#12289;&#29366;&#24577;&#31354;&#38388;&#22823;&#23567;&#21644;$\log (T)$&#12290;&#25105;&#20204;&#36824;&#22312;...
&lt;/p&gt;
&lt;p&gt;
A common lens to theoretically study neural net architectures is to analyze the functions they can approximate. However, constructions from approximation theory may be unrealistic and therefore less meaningful. For example, a common unrealistic trick is to encode target function values using infinite precision. To address these issues, this work proposes a formal definition of statistically meaningful (SM) approximation which requires the approximating network to exhibit good statistical learnability. We study SM approximation for two function classes: boolean circuits and Turing machines. We show that overparameterized feedforward neural nets can SM approximate boolean circuits with sample complexity depending only polynomially on the circuit size, not the size of the network. In addition, we show that transformers can SM approximate Turing machines with computation time bounded by $T$ with sample complexity polynomial in the alphabet size, state space size, and $\log (T)$. We also in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26679;&#22806;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#27491;&#21017;&#21270;&#20855;&#26377;&#20984;&#24809;&#32602;&#30340;&#40065;&#26834;$M$-&#20272;&#35745;&#65292;&#35813;&#26041;&#27861;&#20165;&#36890;&#36807;&#22266;&#23450;&#30340;&#35266;&#27979;&#25968;&#25454;&#20381;&#36182;&#20110;&#29305;&#23450;&#37327;&#65292;&#20854;&#20013;&#22312;&#39640;&#32500;&#28176;&#36817;&#21306;&#22495;&#20013;&#65292;&#35813;&#20272;&#35745;&#20855;&#26377;&#30456;&#23545;&#35823;&#24046;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2008.11840</link><description>&lt;p&gt;
&#38024;&#23545;&#20855;&#26377;&#20984;&#24809;&#32602;&#30340;&#40065;&#26834;M-&#20272;&#35745;&#30340;&#26679;&#22806;&#35823;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Out-of-sample error estimate for robust M-estimators with convex penalty. (arXiv:2008.11840v5 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.11840
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26679;&#22806;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#27491;&#21017;&#21270;&#20855;&#26377;&#20984;&#24809;&#32602;&#30340;&#40065;&#26834;$M$-&#20272;&#35745;&#65292;&#35813;&#26041;&#27861;&#20165;&#36890;&#36807;&#22266;&#23450;&#30340;&#35266;&#27979;&#25968;&#25454;&#20381;&#36182;&#20110;&#29305;&#23450;&#37327;&#65292;&#20854;&#20013;&#22312;&#39640;&#32500;&#28176;&#36817;&#21306;&#22495;&#20013;&#65292;&#35813;&#20272;&#35745;&#20855;&#26377;&#30456;&#23545;&#35823;&#24046;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35266;&#27979;&#21040;$(X,y)$&#19988;$p,n$&#21516;&#38454;&#30340;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27491;&#21017;&#21270;&#20855;&#26377;&#20984;&#24809;&#32602;&#30340;&#40065;&#26834;$M$-&#20272;&#35745;&#30340;&#36890;&#29992;&#26679;&#22806;&#35823;&#24046;&#20272;&#35745;&#12290;&#22914;&#26524;$\psi$&#26159;&#40065;&#26834;&#25968;&#25454;&#25311;&#21512;&#25439;&#22833;&#20989;&#25968;$\rho$&#30340;&#23548;&#25968;&#65292;&#21017;&#35813;&#20272;&#35745;&#20165;&#36890;&#36807;$\hat\psi = \psi(y-X\hat\beta)$&#12289;$X^\top \hat\psi$&#20197;&#21450;$X$&#22266;&#23450;&#26102;&#30340;&#23548;&#25968;$(\partial/\partial y)\hat\psi$&#21644;$(\partial/\partial y)X\hat\beta$&#20381;&#36182;&#20110;&#35266;&#27979;&#25968;&#25454;&#12290;&#22312;&#20855;&#26377;&#39640;&#26031;&#21327;&#21464;&#37327;&#21644;&#29420;&#31435;&#22122;&#22768;&#30340;&#32447;&#24615;&#27169;&#22411;&#20013;&#65292;&#36825;&#31181;&#26679;&#22806;&#35823;&#24046;&#20272;&#35745;&#22312;$n^{-1/2}$&#38454;&#20855;&#26377;&#30456;&#23545;&#35823;&#24046;&#65292;&#26080;&#35770;&#26159;&#22312;$p/n\le \gamma$&#30340;&#38750;&#28176;&#36817;&#24773;&#20917;&#19979;&#36824;&#26159;&#22312;&#39640;&#32500;&#28176;&#36817;&#21306;&#22495;$p/n\to\gamma'\in(0,\infty)$&#20013;&#22343;&#25104;&#31435;&#12290;&#21482;&#35201;$\psi=\rho'$&#26159;1-Lipschitz&#30340;&#65292;&#21363;&#20351;&#36890;&#29992;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;$\rho$&#20063;&#26159;&#34987;&#20801;&#35768;&#30340;&#12290;&#24403;&#24809;&#32602;&#21442;&#25968;&#36827;&#34892;&#36866;&#24403;&#32553;&#25918;&#26102;&#65292;&#26679;&#22806;&#35823;&#24046;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#22312;&#28385;&#36275;&#24378;&#20984;&#24615;&#20551;&#35774;&#19979;&#25110;$\ell_1$&#24809;&#32602;&#30340;Huber&#25439;&#22833;&#20013;&#22343;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
A generic out-of-sample error estimate is proposed for robust $M$-estimators regularized with a convex penalty in high-dimensional linear regression where $(X,y)$ is observed and $p,n$ are of the same order. If $\psi$ is the derivative of the robust data-fitting loss $\rho$, the estimate depends on the observed data only through the quantities $\hat\psi = \psi(y-X\hat\beta)$, $X^\top \hat\psi$ and the derivatives $(\partial/\partial y) \hat\psi$ and $(\partial/\partial y) X\hat\beta$ for fixed $X$.  The out-of-sample error estimate enjoys a relative error of order $n^{-1/2}$ in a linear model with Gaussian covariates and independent noise, either non-asymptotically when $p/n\le \gamma$ or asymptotically in the high-dimensional asymptotic regime $p/n\to\gamma'\in(0,\infty)$. General differentiable loss functions $\rho$ are allowed provided that $\psi=\rho'$ is 1-Lipschitz. The validity of the out-of-sample error estimate holds either under a strong convexity assumption, or for the $\ell
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#21464;&#20998;Wasserstein&#36136;&#24515;&#35299;&#20915;&#20960;&#20309;&#32858;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;Monge WBs&#19982;K-means&#32858;&#31867;&#21644;&#20849;&#21516;&#32858;&#31867;&#30456;&#20851;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#38382;&#39064;&#8212;&#8212;&#27491;&#21017;&#21270;K-means&#21644;Wasserstein&#36136;&#24515;&#21387;&#32553;&#65292;&#24182;&#28436;&#31034;&#20102;VWBs&#22312;&#35299;&#20915;&#36825;&#20123;&#32858;&#31867;&#30456;&#20851;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2002.10543</link><description>&lt;p&gt;
&#21464;&#20998;Wasserstein&#36136;&#24515;&#29992;&#20110;&#20960;&#20309;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Wasserstein Barycenters for Geometric Clustering. (arXiv:2002.10543v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.10543
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#21464;&#20998;Wasserstein&#36136;&#24515;&#35299;&#20915;&#20960;&#20309;&#32858;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;Monge WBs&#19982;K-means&#32858;&#31867;&#21644;&#20849;&#21516;&#32858;&#31867;&#30456;&#20851;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#38382;&#39064;&#8212;&#8212;&#27491;&#21017;&#21270;K-means&#21644;Wasserstein&#36136;&#24515;&#21387;&#32553;&#65292;&#24182;&#28436;&#31034;&#20102;VWBs&#22312;&#35299;&#20915;&#36825;&#20123;&#32858;&#31867;&#30456;&#20851;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#35299;&#20915;&#20855;&#26377;&#21464;&#20998;&#21407;&#29702;&#30340;Monge&#26144;&#23556;&#26469;&#35745;&#31639;Wasserstein&#36136;&#24515;(WBs)&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;WBs&#30340;&#24230;&#37327;&#29305;&#24615;&#65292;&#24182;&#25506;&#32034;&#23427;&#20204;&#30340;&#32852;&#31995;&#65292;&#29305;&#21035;&#26159;Monge WBs&#19982;K-means&#32858;&#31867;&#21644;&#20849;&#21516;&#32858;&#31867;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;Monge WBs&#22312;&#38750;&#24179;&#34913;&#24230;&#37327;&#21644;&#29699;&#24418;&#22495;&#19978;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#38382;&#39064;&#8212;&#8212;&#27491;&#21017;&#21270;K-means&#21644;Wasserstein&#36136;&#24515;&#21387;&#32553;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;VWBs&#35299;&#20915;&#36825;&#20123;&#32858;&#31867;&#30456;&#20851;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to compute Wasserstein barycenters (WBs) by solving for Monge maps with variational principle. We discuss the metric properties of WBs and explore their connections, especially the connections of Monge WBs, to K-means clustering and co-clustering. We also discuss the feasibility of Monge WBs on unbalanced measures and spherical domains. We propose two new problems -regularized K-means and Wasserstein barycenter compression. We demonstrate the use of VWBs in solving these clustering-related problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38548;&#24320;&#24335;&#35797;&#39564;&#30340;&#26368;&#20248;&#35774;&#35745;&#38382;&#39064;&#12290;&#23545;&#20110;&#38750;&#33258;&#36866;&#24212;&#23454;&#39564;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#65307;&#23545;&#20110;&#33258;&#36866;&#24212;&#23454;&#39564;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212;&#31934;&#24230;&#23548;&#21521;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#65288;PGAE&#65289;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#26469;&#26368;&#22823;&#21270;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#30340;&#39044;&#26399;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/1911.03764</link><description>&lt;p&gt;
&#38548;&#24320;&#24335;&#35797;&#39564;&#30340;&#26368;&#20248;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal Experimental Design for Staggered Rollouts. (arXiv:1911.03764v5 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.03764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38548;&#24320;&#24335;&#35797;&#39564;&#30340;&#26368;&#20248;&#35774;&#35745;&#38382;&#39064;&#12290;&#23545;&#20110;&#38750;&#33258;&#36866;&#24212;&#23454;&#39564;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#65307;&#23545;&#20110;&#33258;&#36866;&#24212;&#23454;&#39564;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212;&#31934;&#24230;&#23548;&#21521;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#65288;PGAE&#65289;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#26469;&#26368;&#22823;&#21270;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#30340;&#39044;&#26399;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#26102;&#26399;&#20869;&#26576;&#32452;&#25968;&#25454;&#21333;&#20803;&#30340;&#27835;&#30103;&#24320;&#22987;&#26102;&#38388;&#23384;&#22312;&#24046;&#24322;&#26102;&#65292;&#23545;&#23454;&#39564;&#36827;&#34892;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#38382;&#39064;&#12290;&#35774;&#35745;&#38382;&#39064;&#28041;&#21450;&#36873;&#25321;&#27599;&#20010;&#25968;&#25454;&#21333;&#20803;&#30340;&#21021;&#22987;&#27835;&#30103;&#26102;&#38388;&#20197;&#20415;&#26368;&#31934;&#30830;&#22320;&#20272;&#35745;&#27835;&#30103;&#30340;&#30636;&#26102;&#25928;&#24212;&#21644;&#32047;&#31215;&#25928;&#24212;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#38750;&#33258;&#36866;&#24212;&#23454;&#39564;&#65292;&#20854;&#20013;&#25152;&#26377;&#30340;&#27835;&#30103;&#20998;&#37197;&#20915;&#31574;&#37117;&#22312;&#23454;&#39564;&#24320;&#22987;&#20043;&#21069;&#20570;&#20986;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#26159;NP&#38590;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;&#22312;&#35813;&#35299;&#20915;&#26041;&#26696;&#19979;&#65292;&#27599;&#20010;&#26102;&#26399;&#36827;&#20837;&#27835;&#30103;&#30340;&#20998;&#25968;&#26368;&#21021;&#36739;&#20302;&#65292;&#28982;&#21518;&#21464;&#39640;&#65292;&#26368;&#21518;&#20877;&#27425;&#38477;&#20302;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#22312;&#25910;&#38598;&#27599;&#20010;&#26102;&#26399;&#30340;&#25968;&#25454;&#21518;&#26356;&#26032;&#32487;&#32493;&#23454;&#39564;&#21644;&#27835;&#30103;&#20998;&#37197;&#20915;&#31574;&#12290;&#23545;&#20110;&#33258;&#36866;&#24212;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212;&#31934;&#24230;&#23548;&#21521;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#65288;PGAE&#65289;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#26469;&#26368;&#22823;&#21270;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#30340;&#39044;&#26399;&#31934;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;PGAE&#31639;&#27861;&#36798;&#21040;&#20102;&#24724;&#24680;&#30340;&#19979;&#38480;&#65292;&#24724;&#24680;&#23450;&#20041;&#20026;&#26399;&#26395;&#32047;&#35745;&#24179;&#26041;&#26631;&#20934;&#35823;&#24046;&#21644;&#20219;&#24847;&#27835;&#30103;&#20998;&#37197;&#31574;&#30053;&#25152;&#33021;&#23454;&#29616;&#30340;&#26368;&#20339;&#35823;&#24046;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the design and analysis of experiments conducted on a set of units over multiple time periods where the starting time of the treatment may vary by unit. The design problem involves selecting an initial treatment time for each unit in order to most precisely estimate both the instantaneous and cumulative effects of the treatment. We first consider non-adaptive experiments, where all treatment assignment decisions are made prior to the start of the experiment. For this case, we show that the optimization problem is generally NP-hard, and we propose a near-optimal solution. Under this solution, the fraction entering treatment each period is initially low, then high, and finally low again. Next, we study an adaptive experimental design problem, where both the decision to continue the experiment and treatment assignment decisions are updated after each period's data is collected. For the adaptive case, we propose a new algorithm, the Precision-Guided Adaptive Experim
&lt;/p&gt;</description></item></channel></rss>