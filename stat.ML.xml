<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18571</link><description>&lt;p&gt;
&#29992;&#20110;&#28385;&#36275;&#22810;&#26679;&#29992;&#25143;&#20559;&#22909;&#30340;&#31639;&#26415;&#25511;&#21046;LLMs&#65306;&#20855;&#26377;&#22810;&#30446;&#26631;&#22870;&#21169;&#30340;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31934;&#32454;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#36866;&#24212;&#21508;&#31181;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#24314;&#27169;&#26469;&#34920;&#31034;&#22810;&#26679;&#21270;&#30340;&#20559;&#22909;&#37197;&#32622;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20026;&#22870;&#21169;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65288;&#21363;&#21333;&#20301;&#21521;&#37327;&#65289;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18571v1 Announce Type: cross  Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#35757;&#32451;&#19979;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#20102;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#20854;&#19979;&#30028;&#65307;&#21516;&#26102;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#28982;&#25104;&#31435;&#12290;</title><link>https://arxiv.org/abs/2402.18551</link><description>&lt;p&gt;
&#38544;&#24615;&#20559;&#35265;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias of Next-Token Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18551
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#35757;&#32451;&#19979;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#20102;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#20854;&#19979;&#30028;&#65307;&#21516;&#26102;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65288;NTP&#65289;&#26159;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39318;&#36873;&#33539;&#24335;&#65292;&#23427;&#28041;&#21450;&#39044;&#27979;&#24207;&#21015;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#19982;&#20256;&#32479;&#30340;&#29420;&#28909;&#20998;&#31867;&#19981;&#21516;&#65292;&#22312;NTP&#20013;&#65292;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#39057;&#29575;&#30340;&#26631;&#35760;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#21518;&#32487;&#12290;&#26412;&#25991;&#23558;NTP&#35757;&#32451;&#26694;&#26550;&#21270;&#20026;&#36328;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#29109;&#26368;&#23567;&#21270;&#65292;&#27599;&#20010;&#19978;&#19979;&#25991;&#37117;&#19982;&#26377;&#38480;&#35789;&#27719;&#34920;&#20013;&#30340;&#31232;&#30095;&#32463;&#39564;&#27010;&#29575;&#21521;&#37327;&#30456;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#23427;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#24403;NTP&#35757;&#32451;&#25439;&#22833;&#36798;&#21040;&#20854;&#19979;&#30028;&#65288;&#29109;&#65289;&#26102;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#26159;&#21542;&#20250;&#23545;&#20855;&#26377;&#29305;&#23450;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#20559;&#35265;&#65311;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#35757;&#32451;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#65292;&#25105;&#20204;&#20570;&#20986;&#20197;&#19979;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25968;&#25454;&#19978;&#30340;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;GD&#33021;&#22815;&#36798;&#21040;&#20854;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18551v1 Announce Type: cross  Abstract: Next-token prediction (NTP), the go-to training paradigm in training large language models, involves predicting the next token in a sequence. Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context. This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary. It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)? Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound. We also demonstrate that these conditions hold under overparameterization. Secondly, we establish that th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.18510</link><description>&lt;p&gt;
RNNs&#36824;&#19981;&#26159;Transformer&#65306;&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#20013;&#30340;&#20851;&#38190;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#21644;Transformer&#22312;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#31034;&#33021;&#21147;&#24046;&#36317;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;RNNs&#26159;&#21542;&#33021;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#65292;&#36890;&#36807;Chain-of-Thought (CoT)&#25552;&#31034;&#65292;&#19982;Transformer&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#26174;&#31034;CoT&#21487;&#20197;&#25913;&#36827;RNNs&#65292;&#20294;&#26080;&#27861;&#24357;&#34917;&#19982;Transformer&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20851;&#38190;&#29942;&#39048;&#22312;&#20110;RNNs&#26080;&#27861;&#23436;&#20840;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#21363;&#20351;&#32463;&#36807;CoT&#30340;&#22686;&#24378;&#65306;&#23545;&#20110;&#20960;&#20010;&#26126;&#30830;&#25110;&#38544;&#24335;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#65292;&#22914;&#32852;&#24819;&#21484;&#22238;&#21644;&#30830;&#23450;&#22270;&#26159;&#21542;&#20026;&#26641;&#65292;&#25105;&#20204;&#35777;&#26126;RNNs&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#20197;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#65292;&#32780;Transformer&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#37319;&#29992;&#22686;&#24378;RNNs&#19978;&#19979;&#25991;&#26816;&#32034;&#33021;&#21147;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18510v1 Announce Type: cross  Abstract: This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, inclu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#26680;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#20197;&#21450;&#24320;&#21457;&#20102;&#32422;&#26463;&#26465;&#20214;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.18477</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#31614;&#21517;&#26680;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#29992;&#20110;&#38543;&#26426;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#26680;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#20197;&#21450;&#24320;&#21457;&#20102;&#32422;&#26463;&#26465;&#20214;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#22312;&#31185;&#23398;&#12289;&#20581;&#24247;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#31614;&#21517;&#26680;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#30340;&#8220;&#36335;&#24452;&#31354;&#38388;&#8221;&#19978;&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#27979;&#35797;&#65292;&#29992;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#36335;&#24452;&#31354;&#38388;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;CI&#27979;&#35797;&#34920;&#29616;&#20986;&#20005;&#26684;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#38750;&#24490;&#29615;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#24320;&#21457;&#20102;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#26469;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;&#22312;&#20551;&#35774;&#24544;&#23454;&#24615;&#21644;CI&#39044;&#35328;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#23436;&#22791;&#19988;&#27491;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18477v1 Announce Type: cross  Abstract: Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via "which variables enter the differential of which other variables". In this paper, we develop a kernel-based test of conditional independence (CI) on "path-space" -- solutions to SDEs -- by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph. Assuming faithfulness and a CI oracle, our algorithm is sound and complete. We empirical
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#20581;&#22766;&#24230;&#37327;&#65288;DRM&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#29702;&#24819;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#20013;&#20581;&#22766;&#20272;&#35745;&#22120;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.18392</link><description>&lt;p&gt;
&#25581;&#31034;&#20581;&#22766;&#24615;&#22312;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Potential of Robustness in Evaluating Causal Inference Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18392
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#20581;&#22766;&#24230;&#37327;&#65288;DRM&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#29702;&#24819;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#20013;&#20581;&#22766;&#20272;&#35745;&#22120;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#23545;&#20010;&#24615;&#21270;&#20915;&#31574;&#21046;&#23450;&#30340;&#38656;&#27714;&#23548;&#33268;&#20154;&#20204;&#23545;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#20132;&#21449;&#39046;&#22495;&#24050;&#32463;&#20135;&#29983;&#20102;&#21508;&#31181;&#26377;&#25928;&#30340;CATE&#20272;&#35745;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#36825;&#20123;&#20272;&#35745;&#22120;&#36890;&#24120;&#21463;&#21046;&#20110;&#32570;&#20047;&#21453;&#20107;&#23454;&#26631;&#31614;&#65292;&#22240;&#27492;&#20351;&#29992;&#20256;&#32479;&#30340;&#20132;&#21449;&#39564;&#35777;&#31561;&#27169;&#22411;&#36873;&#25321;&#31243;&#24207;&#26469;&#36873;&#25321;&#29702;&#24819;&#30340;CATE&#20272;&#35745;&#22120;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;CATE&#20272;&#35745;&#22120;&#36873;&#25321;&#26041;&#27861;&#65292;&#22914;&#25554;&#20540;&#21644;&#20266;&#32467;&#26524;&#24230;&#37327;&#65292;&#38754;&#20020;&#30528;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#38656;&#35201;&#30830;&#23450;&#24230;&#37327;&#24418;&#24335;&#21644;&#25311;&#21512;&#24178;&#25200;&#21442;&#25968;&#25110;&#25554;&#20214;&#23398;&#20064;&#32773;&#30340;&#22522;&#30784;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#32570;&#20047;&#38024;&#23545;&#36873;&#25321;&#20581;&#22766;&#20272;&#35745;&#22120;&#30340;&#29305;&#23450;&#37325;&#28857;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20998;&#24067;&#24335;&#20581;&#22766;&#24230;&#37327;&#65288;DRM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18392v1 Announce Type: cross  Abstract: The growing demand for personalized decision-making has led to a surge of interest in estimating the Conditional Average Treatment Effect (CATE). The intersection of machine learning and causal inference has yielded various effective CATE estimators. However, deploying these estimators in practice is often hindered by the absence of counterfactual labels, making it challenging to select the desirable CATE estimator using conventional model selection procedures like cross-validation. Existing approaches for CATE estimator selection, such as plug-in and pseudo-outcome metrics, face two inherent challenges. Firstly, they are required to determine the metric form and the underlying machine learning models for fitting nuisance parameters or plug-in learners. Secondly, they lack a specific focus on selecting a robust estimator. To address these challenges, this paper introduces a novel approach, the Distributionally Robust Metric (DRM), for 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#32422;&#26463;&#30340;Weibull AFT&#27169;&#22411;&#65292;&#22312;&#21464;&#37327;&#36873;&#25321;&#21644;&#20272;&#35745;&#26041;&#38754;&#37319;&#29992;&#24809;&#32602;&#20284;&#28982;&#26041;&#27861;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#31232;&#30095;&#24615;&#21644;&#20998;&#32452;&#25928;&#24212;&#65292;&#20855;&#26377;&#29702;&#35770;&#19968;&#33268;&#24615;&#21644;&#39640;&#25928;&#30340;&#35745;&#31639;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18242</link><description>&lt;p&gt;
&#22522;&#20110;&#32593;&#32476;&#32422;&#26463;&#30340;Weibull AFT&#27169;&#22411;&#29992;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
A network-constrain Weibull AFT model for biomarkers discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18242
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#32422;&#26463;&#30340;Weibull AFT&#27169;&#22411;&#65292;&#22312;&#21464;&#37327;&#36873;&#25321;&#21644;&#20272;&#35745;&#26041;&#38754;&#37319;&#29992;&#24809;&#32602;&#20284;&#28982;&#26041;&#27861;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#31232;&#30095;&#24615;&#21644;&#20998;&#32452;&#25928;&#24212;&#65292;&#20855;&#26377;&#29702;&#35770;&#19968;&#33268;&#24615;&#21644;&#39640;&#25928;&#30340;&#35745;&#31639;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;AFTNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Weibull&#21152;&#36895;&#22833;&#25928;&#26102;&#38388;&#65288;AFT&#65289;&#27169;&#22411;&#30340;&#32593;&#32476;&#32422;&#26463;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24809;&#32602;&#20284;&#28982;&#26041;&#27861;&#36827;&#34892;&#21464;&#37327;&#36873;&#25321;&#21644;&#20272;&#35745;&#12290;&#24403;&#20351;&#29992;&#23545;&#25968;&#32447;&#24615;&#34920;&#31034;&#26102;&#65292;&#25512;&#26029;&#38382;&#39064;&#21464;&#25104;&#20102;&#19968;&#20010;&#32467;&#26500;&#31232;&#30095;&#22238;&#24402;&#38382;&#39064;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#32467;&#21512;&#20102;&#39044;&#27979;&#22240;&#23376;&#20043;&#38388;&#30340;&#30456;&#20851;&#27169;&#24335;&#65292;&#20351;&#29992;&#21452;&#37325;&#24809;&#32602;&#20419;&#36827;&#31232;&#30095;&#24615;&#21644;&#20998;&#32452;&#25928;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;AFTNet&#20272;&#35745;&#22120;&#24314;&#31435;&#20102;&#29702;&#35770;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#39640;&#25928;&#36845;&#20195;&#35745;&#31639;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#19978;&#35780;&#20272;&#20102;AFTNet&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18242v1 Announce Type: new  Abstract: We propose AFTNet, a novel network-constraint survival analysis method based on the Weibull accelerated failure time (AFT) model solved by a penalized likelihood approach for variable selection and estimation. When using the log-linear representation, the inference problem becomes a structured sparse regression problem for which we explicitly incorporate the correlation patterns among predictors using a double penalty that promotes both sparsity and grouping effect. Moreover, we establish the theoretical consistency for the AFTNet estimator and present an efficient iterative computational algorithm based on the proximal gradient descent method. Finally, we evaluate AFTNet performance both on synthetic and real data examples.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.18213</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#21487;&#24494;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Differentiable Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18213
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#20013;&#30340;Pareto&#21069;&#27839;&#36718;&#24275;&#21078;&#26512;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#20687;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36825;&#26679;&#30340;&#26114;&#36149;&#30446;&#26631;&#20013;&#12290; &#30456;&#23545;&#20110;&#20256;&#32479;&#30340;NAS&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#24182;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36229;&#32593;&#32476;&#21442;&#25968;&#21270;&#36328;&#22810;&#20010;&#35774;&#22791;&#21644;&#22810;&#20010;&#30446;&#26631;&#30340;&#32852;&#21512;&#26550;&#26500;&#20998;&#24067;&#65292;&#36229;&#32593;&#32476;&#21487;&#20197;&#26681;&#25454;&#30828;&#20214;&#29305;&#24449;&#21644;&#20559;&#22909;&#21521;&#37327;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#23454;&#29616;&#38646;&#27425;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18213v1 Announce Type: new  Abstract: Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#24102;&#26377;&#20107;&#21518;&#35266;&#23519;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#29305;&#23450;&#31639;&#27861;&#24182;&#35777;&#26126;&#20854;&#22312;&#27492;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#39640;&#25928;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18149</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#20107;&#21518;&#35266;&#23519;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#35777;&#30340;&#39640;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Partially Observable Risk-Sensitive Reinforcement Learning with Hindsight Observation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#24102;&#26377;&#20107;&#21518;&#35266;&#23519;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#29305;&#23450;&#31639;&#27861;&#24182;&#35777;&#26126;&#20854;&#22312;&#27492;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#39640;&#25928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#22312;&#24102;&#26377;&#20107;&#21518;&#35266;&#23519;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#25506;&#32034;&#20102;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#24724;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#29702;&#35770;&#25506;&#32034;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#26696;&#65292;&#23558;&#20107;&#21518;&#35266;&#23519;&#25972;&#21512;&#21040;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#26694;&#26550;&#20013;&#65292;&#30446;&#26631;&#26159;&#22312;&#29109;&#39118;&#38505;&#24230;&#37327;&#19979;&#20248;&#21270;&#32047;&#31215;&#22870;&#21169;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#30340;&#31532;&#19968;&#20010;&#21487;&#35777;&#39640;&#25928;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20005;&#26684;&#20998;&#26512;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#21518;&#24724;$\tilde{O}\left(\frac{e^{|{\gamma}|H}-1}{|{\gamma}|H}H^2\sqrt{KHS^2OA}\right)$&#65292;&#24403;&#27169;&#22411;&#36864;&#21270;&#20026;&#39118;&#38505;&#20013;&#24615;&#25110;&#23436;&#20840;&#21487;&#35266;&#27979;&#35774;&#32622;&#26102;&#65292;&#23427;&#36229;&#36234;&#25110;&#21305;&#37197;&#29616;&#26377;&#30340;&#19978;&#38480;&#12290;&#25105;&#20204;&#37319;&#29992;&#21464;&#25442;&#27979;&#24230;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;beta&#21521;&#37327;&#20998;&#26512;&#24037;&#20855;&#26469;&#31616;&#21270;&#25968;&#23398;&#25512;&#23548;&#12290;&#36825;&#20123;&#25216;&#26415;&#29305;&#21035;&#24341;&#20154;&#27880;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18149v1 Announce Type: new  Abstract: This work pioneers regret analysis of risk-sensitive reinforcement learning in partially observable environments with hindsight observation, addressing a gap in theoretical exploration. We introduce a novel formulation that integrates hindsight observations into a Partially Observable Markov Decision Process (POMDP) framework, where the goal is to optimize accumulated reward under the entropic risk measure. We develop the first provably efficient RL algorithm tailored for this setting. We also prove by rigorous analysis that our algorithm achieves polynomial regret $\tilde{O}\left(\frac{e^{|{\gamma}|H}-1}{|{\gamma}|H}H^2\sqrt{KHS^2OA}\right)$, which outperforms or matches existing upper bounds when the model degenerates to risk-neutral or fully observable settings. We adopt the method of change-of-measure and develop a novel analytical tool of beta vectors to streamline mathematical derivations. These techniques are of particular interes
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#36125;&#21494;&#26031;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#26469;&#26377;&#25928;&#22320;&#27719;&#24635;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#65292;&#20197;&#25913;&#36827;&#26080;&#20154;&#26426;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17987</link><description>&lt;p&gt;
&#22810;&#24577;&#38647;&#36798;&#23545;&#31354;&#20013;&#39134;&#34892;&#22120;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A Bayesian Fusion Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17987
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#36125;&#21494;&#26031;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#26469;&#26377;&#25928;&#22320;&#27719;&#24635;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#65292;&#20197;&#25913;&#36827;&#26080;&#20154;&#26426;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17987v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#26080;&#20154;&#26426;&#30340;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#65288;RATR&#65289;&#28041;&#21450;&#21457;&#23556;&#30005;&#30913;&#27874;&#24182;&#23545;&#25509;&#25910;&#21040;&#30340;&#38647;&#36798;&#22238;&#27874;&#25191;&#34892;&#30446;&#26631;&#31867;&#22411;&#35782;&#21035;&#65292;&#23545;&#22269;&#38450;&#21644;&#33322;&#31354;&#33322;&#22825;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#22810;&#24577;&#38647;&#36798;&#37197;&#32622;&#22312;RATR&#20013;&#20248;&#20110;&#21333;&#24577;&#38647;&#36798;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22810;&#24577;&#38647;&#36798;&#37197;&#32622;&#20013;&#30340;&#34701;&#21512;&#26041;&#27861;&#36890;&#24120;&#20197;&#27010;&#29575;&#26041;&#24335;&#27425;&#20248;&#22320;&#32452;&#21512;&#26469;&#33258;&#21508;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#21521;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#36125;&#21494;&#26031;RATR&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#65288;OBF&#65289;&#26469;&#32858;&#21512;&#26469;&#33258;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#12290;OBF&#22522;&#20110;&#26399;&#26395;0-1&#25439;&#22833;&#65292;&#26681;&#25454;&#22810;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#21382;&#21490;&#35266;&#27979;&#26356;&#26032;&#30446;&#26631;&#26080;&#20154;&#26426;&#31867;&#22411;&#30340;&#36882;&#24402;&#36125;&#21494;&#26031;&#20998;&#31867;&#65288;RBC&#65289;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#30340;&#38543;&#26426;&#34892;&#36208;&#36712;&#36857;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#20849;&#28041;&#21450;&#19971;&#31181;&#26426;&#21160;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17987v1 Announce Type: cross  Abstract: Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs) involves transmitting Electromagnetic Waves (EMWs) and performing target type recognition on the received radar echo, crucial for defense and aerospace applications. Previous studies highlighted the advantages of multistatic radar configurations over monostatic ones in RATR. However, fusion methods in multistatic radar configurations often suboptimally combine classification vectors from individual radars probabilistically. To address this, we propose a fully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to aggregate classification probability vectors from multiple radars. OBF, based on expected 0-1 loss, updates a Recursive Bayesian Classification (RBC) posterior distribution for target UAV type, conditioned on historical observations across multiple time steps. We evaluate the approach using simulated random walk trajectories for seven dro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SoS&#23494;&#24230;&#21644;&#945;-&#31163;&#25955;&#24230;&#26469;&#36817;&#20284;&#20013;&#38388;&#23494;&#24230;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20004;&#32773;&#32467;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36827;&#32780;&#23454;&#29616;&#20174;&#26410;&#26631;&#20934;&#21270;&#30340;&#23494;&#24230;&#29983;&#25104;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.17943</link><description>&lt;p&gt;
&#20351;&#29992;SoS&#23494;&#24230;&#20272;&#35745;&#21644;&#945;-&#31163;&#25955;&#24230;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Sequential transport maps using SoS density estimation and $\alpha$-divergences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SoS&#23494;&#24230;&#21644;&#945;-&#31163;&#25955;&#24230;&#26469;&#36817;&#20284;&#20013;&#38388;&#23494;&#24230;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20004;&#32773;&#32467;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36827;&#32780;&#23454;&#29616;&#20174;&#26410;&#26631;&#20934;&#21270;&#30340;&#23494;&#24230;&#29983;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20256;&#36755;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#22240;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#36817;&#20284;&#23494;&#24230;&#29983;&#25104;&#26679;&#26412;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35843;&#26597;&#20102;&#25552;&#20986;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#31995;&#21015;&#32452;&#25104;&#30340;Knothe-Rosenblatt&#65288;KR&#65289;&#26144;&#23556;&#20043;&#19978;&#12290;&#20854;&#20013;&#27599;&#20010;&#26144;&#23556;&#37117;&#26159;&#36890;&#36807;&#39318;&#20808;&#20272;&#35745;&#20013;&#31561;&#22797;&#26434;&#24230;&#30340;&#20013;&#38388;&#23494;&#24230;&#65292;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#20174;&#21442;&#32771;&#23494;&#24230;&#21040;&#39044;&#35745;&#31639;&#36817;&#20284;&#23494;&#24230;&#30340;&#31934;&#30830;KR&#26144;&#23556;&#32780;&#26500;&#24314;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;SoS&#23494;&#24230;&#21644;&#945;-&#31163;&#25955;&#24230;&#26469;&#36817;&#20284;&#20013;&#38388;&#23494;&#24230;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#23558;SoS&#23494;&#24230;&#19982;&#945;-&#31163;&#25955;&#24230;&#30456;&#32467;&#21512;&#20135;&#29983;&#20102;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#21322;&#23450;&#32534;&#31243;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#945;-&#31163;&#25955;&#24230;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#20351;&#24471;&#33021;&#22815;&#22788;&#29702;&#26410;&#26631;&#20934;&#21270;&#30340;&#23494;&#24230;&#65292;&#20174;&#32780;&#25552;&#20379;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17943v1 Announce Type: cross  Abstract: Transport-based density estimation methods are receiving growing interest because of their ability to efficiently generate samples from the approximated density. We further invertigate the sequential transport maps framework proposed from arXiv:2106.04170 arXiv:2303.02554, which builds on a sequence of composed Knothe-Rosenblatt (KR) maps. Each of those maps are built by first estimating an intermediate density of moderate complexity, and then by computing the exact KR map from a reference density to the precomputed approximate density. In our work, we explore the use of Sum-of-Squares (SoS) densities and $\alpha$-divergences for approximating the intermediate densities. Combining SoS densities with $\alpha$-divergence interestingly yields convex optimization problems which can be efficiently solved using semidefinite programming. The main advantage of $\alpha$-divergences is to enable working with unnormalized densities, which provide
&lt;/p&gt;</description></item><item><title>&#21487;&#20197;&#30452;&#25509;&#20174;&#24102;&#26377;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20934;&#30830;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#26816;&#26597;&#25968;&#25454;&#22635;&#20805;&#24517;&#35201;&#24615;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#22635;&#20805;&#30340;&#24773;&#20917;&#19979;&#36820;&#22238;&#20934;&#30830;&#27169;&#22411;&#65292;&#26174;&#33879;&#20943;&#23569;&#25968;&#25454;&#22635;&#20805;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;</title><link>https://arxiv.org/abs/2402.17926</link><description>&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#30340;&#30830;&#23450;&#24615;&#21644;&#36817;&#20284;&#30830;&#23450;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Certain and Approximately Certain Models for Statistical Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17926
&lt;/p&gt;
&lt;p&gt;
&#21487;&#20197;&#30452;&#25509;&#20174;&#24102;&#26377;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20934;&#30830;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#26816;&#26597;&#25968;&#25454;&#22635;&#20805;&#24517;&#35201;&#24615;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#22635;&#20805;&#30340;&#24773;&#20917;&#19979;&#36820;&#22238;&#20934;&#30830;&#27169;&#22411;&#65292;&#26174;&#33879;&#20943;&#23569;&#25968;&#25454;&#22635;&#20805;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#36890;&#24120;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#24182;&#19988;&#21253;&#21547;&#32570;&#22833;&#20540;&#12290;&#20026;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#29992;&#25143;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#36164;&#28304;&#22635;&#20805;&#21644;&#25214;&#21040;&#32570;&#22833;&#25968;&#25454;&#39033;&#30340;&#27491;&#30830;&#20540;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#35757;&#32451;&#25968;&#25454;&#21644;&#30446;&#26631;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#26597;&#25968;&#25454;&#22635;&#20805;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#20415;&#22312;&#21508;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#20013;&#23398;&#20064;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#26469;&#26816;&#26597;&#27492;&#24517;&#35201;&#24615;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#22635;&#20805;&#30340;&#24773;&#20917;&#19979;&#36820;&#22238;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#25968;&#25454;&#22635;&#20805;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#32780;&#27809;&#26377;&#24102;&#26469;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17926v1 Announce Type: cross  Abstract: Real-world data is often incomplete and contains missing values. To train accurate models over real-world datasets, users need to spend a substantial amount of time and resources imputing and finding proper values for missing data items. In this paper, we demonstrate that it is possible to learn accurate models directly from data with missing values for certain training data and target models. We propose a unified approach for checking the necessity of data imputation to learn accurate models across various widely-used machine learning paradigms. We build efficient algorithms with theoretical guarantees to check this necessity and return accurate models in cases where imputation is unnecessary. Our extensive experiments indicate that our proposed algorithms significantly reduce the amount of time and effort needed for data imputation without imposing considerable computational overhead.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#37319;&#26679;&#20013;&#30340;&#20122;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2402.17886</link><description>&lt;p&gt;
&#29992;&#20110;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#30340;&#38646;&#38454;&#37319;&#26679;&#26041;&#27861;&#65306;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#32531;&#35299;&#20122;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#37319;&#26679;&#20013;&#30340;&#20122;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32771;&#34385;&#20102;&#22522;&#20110;&#20854;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#26410;&#24402;&#19968;&#21270;&#23494;&#24230;&#26597;&#35810;&#30340;&#37319;&#26679;&#38382;&#39064;&#12290;&#39318;&#20808;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#25311;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#21363;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#65288;DMC&#65289;&#65292;&#20854;&#24471;&#20998;&#20989;&#25968;&#36890;&#36807;&#36890;&#29992;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#36924;&#36817;&#12290;DMC&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#35861;&#30340;&#20803;&#31639;&#27861;&#65292;&#20854;&#20013;&#31070;&#35861;&#26159;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#29983;&#25104;&#33945;&#29305;&#21345;&#27931;&#20998;&#25968;&#20272;&#35745;&#22120;&#30340;&#26679;&#26412;&#30340;&#35775;&#38382;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#25298;&#32477;&#37319;&#26679;&#30340;&#36825;&#20010;&#31070;&#35861;&#30340;&#23454;&#29616;&#65292;&#36825;&#23558;DMC&#36716;&#21270;&#20026;&#19968;&#20010;&#30495;&#27491;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#65288;ZOD-MC&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21363;DMC&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#32780;&#19981;&#20551;&#35774;&#30446;&#26631;&#20998;&#24067;&#20026;&#23545;&#25968;&#20985;&#25110;&#28385;&#36275;&#20219;&#20309;&#31561;&#21608;&#19981;&#31561;&#24335;&#65292;&#25552;&#20379;&#20102;&#25910;&#25947;&#20998;&#26512;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;ZOD-MC&#23545;&#25152;&#38656;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#65292;&#23613;&#31649;&#20173;&#28982;&#21463;&#21040;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17886v1 Announce Type: cross  Abstract: This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Diffusion Monte Carlo (DMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZOD-MC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit sti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#20559;&#32622;MCMC&#27493;&#39588;&#30340;SAEM&#30340;&#28176;&#36817;&#24615;&#21644;&#38750;&#28176;&#36817;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#20559;&#32622;&#30340;&#24433;&#21709;&#65292;&#24182;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#29702;&#35770;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.17870</link><description>&lt;p&gt;
&#20351;&#29992;&#20559;&#32622;MCMC&#36827;&#34892;&#38543;&#26426;&#36924;&#36817;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Approximation with Biased MCMC for Expectation Maximization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#20559;&#32622;MCMC&#27493;&#39588;&#30340;SAEM&#30340;&#28176;&#36817;&#24615;&#21644;&#38750;&#28176;&#36817;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#20559;&#32622;&#30340;&#24433;&#21709;&#65292;&#24182;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#29702;&#35770;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17870v1 &#36890;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#32463;&#39564;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20294;&#20854;&#26399;&#26395;&#27493;&#39588;&#65288;E&#27493;&#39588;&#65289;&#32463;&#24120;&#38590;&#20197;&#22788;&#29702;&#12290;&#37319;&#29992;&#24102;&#26377;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#21487;&#20197;&#36991;&#24320;&#36825;&#20010;&#38382;&#39064;&#65292;&#24471;&#21040;&#19968;&#31181;&#31216;&#20026;MCMC-SAEM&#30340;&#31639;&#27861;&#12290;&#34429;&#28982;&#20808;&#21069;&#24050;&#32463;&#30830;&#31435;&#20102;MCMC-SAEM&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#20165;&#36866;&#29992;&#20110;&#20351;&#29992;&#28176;&#36817;&#26080;&#20559;MCMC&#31639;&#27861;&#30340;&#24773;&#20917;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;MCMC-SAEM&#32463;&#24120;&#20351;&#29992;&#28176;&#36817;&#26377;&#20559;&#30340;MCMC&#36816;&#34892;&#65292;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#20854;&#29702;&#35770;&#21518;&#26524;&#23578;&#19981;&#20026;&#20154;&#20102;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#24102;&#26377;&#20559;&#32622;MCMC&#27493;&#39588;&#30340;SAEM&#30340;&#28176;&#36817;&#24615;&#21644;&#38750;&#28176;&#36817;&#24615;&#65292;&#29305;&#21035;&#26159;&#20559;&#32622;&#30340;&#24433;&#21709;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#27604;&#36739;Metropolis&#35843;&#25972;&#30340;&#26391;&#32500;&#26032;&#31639;&#27861;&#65288;MALA&#65289;&#21644;&#26410;&#32463;&#35843;&#25972;&#30340;&#26391;&#32500;&#26032;&#31639;&#27861;&#65288;ULA&#65289;&#30340;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17870v1 Announce Type: cross  Abstract: The expectation maximization (EM) algorithm is a widespread method for empirical Bayesian inference, but its expectation step (E-step) is often intractable. Employing a stochastic approximation scheme with Markov chain Monte Carlo (MCMC) can circumvent this issue, resulting in an algorithm known as MCMC-SAEM. While theoretical guarantees for MCMC-SAEM have previously been established, these results are restricted to the case where asymptotically unbiased MCMC algorithms are used. In practice, MCMC-SAEM is often run with asymptotically biased MCMC, for which the consequences are theoretically less understood. In this work, we fill this gap by analyzing the asymptotics and non-asymptotics of SAEM with biased MCMC steps, particularly the effect of bias. We also provide numerical experiments comparing the Metropolis-adjusted Langevin algorithm (MALA), which is asymptotically unbiased, and the unadjusted Langevin algorithm (ULA), which is a
&lt;/p&gt;</description></item><item><title>&#31283;&#23450;LM 2 1.6B&#26159;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#20013;&#30340;&#26032;&#19968;&#20195;&#20135;&#21697;&#65292;&#22312;&#35813;&#25253;&#21578;&#20013;&#35814;&#32454;&#20171;&#32461;&#20102;&#20854;&#25968;&#25454;&#12289;&#35757;&#32451;&#36807;&#31243;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#35813;&#27169;&#22411;&#22312;&#21457;&#24067;&#26102;&#26159;2B&#21442;&#25968;&#33539;&#22260;&#20869;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#20043;&#19968;&#65292;&#24182;&#25552;&#20379;&#20102;&#19979;&#36733;&#38142;&#25509;&#21644;&#24615;&#33021;&#23545;&#27604;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.17834</link><description>&lt;p&gt;
&#31283;&#23450;LM 2 1.6B&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Stable LM 2 1.6B Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17834
&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;LM 2 1.6B&#26159;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#20013;&#30340;&#26032;&#19968;&#20195;&#20135;&#21697;&#65292;&#22312;&#35813;&#25253;&#21578;&#20013;&#35814;&#32454;&#20171;&#32461;&#20102;&#20854;&#25968;&#25454;&#12289;&#35757;&#32451;&#36807;&#31243;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#35813;&#27169;&#22411;&#22312;&#21457;&#24067;&#26102;&#26159;2B&#21442;&#25968;&#33539;&#22260;&#20869;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#20043;&#19968;&#65292;&#24182;&#25552;&#20379;&#20102;&#19979;&#36733;&#38142;&#25509;&#21644;&#24615;&#33021;&#23545;&#27604;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31283;&#23450;LM 2 1.6B&#65292;&#36825;&#26159;&#25105;&#20204;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#30340;&#26032;&#19968;&#20195;&#20135;&#21697;&#12290;&#22312;&#36825;&#20221;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#23548;&#33268;StableLM 2 1.6B&#22522;&#30784;&#29256;&#26412;&#21644;&#25351;&#23548;&#35843;&#20248;&#29256;&#26412;&#30340;&#25968;&#25454;&#21644;&#35757;&#32451;&#36807;&#31243;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#30340;&#26435;&#37325;&#22343;&#21487;&#36890;&#36807;Hugging Face&#19979;&#36733;&#21644;&#20351;&#29992;&#12290;&#35813;&#25253;&#21578;&#21253;&#21547;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24443;&#24213;&#35780;&#20272;&#65292;&#21253;&#25324;&#38646; shot &#21644;&#23569; shot &#22522;&#20934;&#27979;&#35797;&#65292;&#22810;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21450;&#37325;&#28857;&#25918;&#22312;&#22810;&#36718;&#23545;&#35805;&#30340; MT &#22522;&#20934;&#27979;&#35797;&#19978;&#12290;&#22312;&#21457;&#24067;&#26412;&#25253;&#21578;&#26102;&#65292;StableLM 2 1.6B&#26159;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#30340; 2B &#21442;&#25968;&#33539;&#22260;&#20869;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#12290;&#37492;&#20110;&#20854;&#21560;&#24341;&#20154;&#30340;&#23567;&#23610;&#23544;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22312;&#22810;&#31181;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#21534;&#21520;&#37327;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#20960;&#20010;&#37327;&#21270;&#26816;&#26597;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#23427;&#20204;&#19982;&#21407;&#22987;&#27169;&#22411;&#30340;&#24615;&#33021;&#25351;&#26631;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17834v1 Announce Type: new  Abstract: We introduce StableLM 2 1.6B, the first in a new generation of our language model series. In this technical report, we present in detail the data and training procedure leading to the base and instruction-tuned versions of StableLM 2 1.6B. The weights for both models are available via Hugging Face for anyone to download and use. The report contains thorough evaluations of these models, including zero- and few-shot benchmarks, multilingual benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of publishing this report, StableLM 2 1.6B was the state-of-the-art open model under 2B parameters by a significant margin. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.17826</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Prediction-Powered Ranking of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#26681;&#25454;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#36827;&#34892;&#25490;&#21517;--&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#21463;&#20154;&#31867;&#20559;&#22909;&#65292;&#37027;&#20040;&#23427;&#23601;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#26469;&#24357;&#21512;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#21487;&#33021;&#24341;&#20837;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17826v1 Announce Type: cross  Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provid
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;VAE-&#22238;&#24402;&#19982;&#22810;&#27169;&#24577;&#20808;&#39564;&#35774;&#35745;&#26448;&#26009;&#24494;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;VAE&#19982;&#22238;&#24402;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#21452;&#23618;&#20808;&#39564;&#26469;&#38142;&#25509;&#36825;&#20004;&#20010;&#27169;&#22411;&#65292;&#23398;&#20064;&#20102;&#24494;&#32467;&#26500;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#27491;&#21521;&#21644;&#36870;&#21521;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.17806</link><description>&lt;p&gt;
&#20351;&#29992;VAE-&#22238;&#24402;&#19982;&#22810;&#27169;&#24577;&#20808;&#39564;&#35774;&#35745;&#26448;&#26009;&#24494;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Material Microstructure Design Using VAE-Regression with Multimodal Prior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17806
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;VAE-&#22238;&#24402;&#19982;&#22810;&#27169;&#24577;&#20808;&#39564;&#35774;&#35745;&#26448;&#26009;&#24494;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;VAE&#19982;&#22238;&#24402;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#21452;&#23618;&#20808;&#39564;&#26469;&#38142;&#25509;&#36825;&#20004;&#20010;&#27169;&#22411;&#65292;&#23398;&#20064;&#20102;&#24494;&#32467;&#26500;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#27491;&#21521;&#21644;&#36870;&#21521;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#27491;&#21521;&#21644;&#36870;&#21521;&#32467;&#26500;-&#24615;&#33021;&#38142;&#25509;&#65292;&#22312;&#35745;&#31639;&#26448;&#26009;&#31185;&#23398;&#20013;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#31995;&#32479;&#22320;&#23558;VAE&#19982;&#22238;&#24402;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#19968;&#20010;&#21452;&#23618;&#20808;&#39564;&#26469;&#38142;&#25509;&#36825;&#20004;&#20010;&#27169;&#22411;&#65292;&#35813;&#20808;&#39564;&#21463;&#21040;&#22238;&#24402;&#21464;&#37327;&#30340;&#26465;&#20214;&#32422;&#26463;&#12290;&#22238;&#24402;&#25439;&#22833;&#19982;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#25439;&#22833;&#19968;&#36215;&#20248;&#21270;&#65292;&#23398;&#20064;&#19982;&#24615;&#33021;&#39044;&#27979;&#21644;&#37325;&#26500;&#30456;&#20851;&#30340;&#24494;&#32467;&#26500;&#29305;&#24449;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#21487;&#29992;&#20110;&#27491;&#21521;&#21644;&#36870;&#21521;&#39044;&#27979;&#65292;&#21363;&#29992;&#20110;&#39044;&#27979;&#32473;&#23450;&#24494;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#29992;&#20110;&#39044;&#27979;&#33719;&#21462;&#32473;&#23450;&#24615;&#33021;&#25152;&#38656;&#30340;&#24494;&#32467;&#26500;&#12290;&#30001;&#20110;&#36870;&#38382;&#39064;&#26159;&#19981;&#36866;&#23450;&#30340;&#65288;&#19968;&#23545;&#22810;&#65289;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#27169;&#24577;&#39640;&#26031;&#28151;&#21512;&#20808;&#39564;&#25512;&#23548;&#20986;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20986;&#30446;&#26631;&#24615;&#33021;&#38598;&#21512;&#30340;&#22810;&#20010;&#24494;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17806v1 Announce Type: new  Abstract: We propose a variational autoencoder (VAE)-based model for building forward and inverse structure-property linkages, a problem of paramount importance in computational materials science. Our model systematically combines VAE with regression, linking the two models through a two-level prior conditioned on the regression variables. The regression loss is optimized jointly with the reconstruction loss of the variational autoencoder, learning microstructure features relevant for property prediction and reconstruction. The resultant model can be used for both forward and inverse prediction i.e., for predicting the properties of a given microstructure as well as for predicting the microstructure required to obtain given properties. Since the inverse problem is ill-posed (one-to-many), we derive the objective function using a multi-modal Gaussian mixture prior enabling the model to infer multiple microstructures for a target set of properties. 
&lt;/p&gt;</description></item><item><title>DAGnosis&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#26469;&#35299;&#20915;&#25968;&#25454;&#19968;&#33268;&#24615;&#26816;&#27979;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#23450;&#20301;&#20026;&#20309;&#26679;&#26412;&#20250;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2402.17599</link><description>&lt;p&gt;
DAGnosis&#65306;&#20351;&#29992;&#32467;&#26500;&#36827;&#34892;&#25968;&#25454;&#19981;&#19968;&#33268;&#24615;&#30340;&#23616;&#37096;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DAGnosis: Localized Identification of Data Inconsistencies using Structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17599
&lt;/p&gt;
&lt;p&gt;
DAGnosis&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#26469;&#35299;&#20915;&#25968;&#25454;&#19968;&#33268;&#24615;&#26816;&#27979;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#23450;&#20301;&#20026;&#20309;&#26679;&#26412;&#20250;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#26102;&#35782;&#21035;&#21644;&#36866;&#24403;&#22788;&#29702;&#25968;&#25454;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#23545;&#21487;&#38752;&#22320;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#26399;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#33021;&#22815;&#35782;&#21035;&#19982;&#35757;&#32451;&#38598;&#30456;&#20851;&#30340;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#65288;1&#65289;&#22312;&#29305;&#24449;&#23637;&#29616;&#32479;&#35745;&#29420;&#31435;&#24615;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#21387;&#32553;&#34920;&#31034;&#65307;&#65288;2&#65289;&#32570;&#20047;&#23616;&#37096;&#21270;&#65292;&#26080;&#27861;&#20934;&#30830;&#23450;&#20301;&#26679;&#26412;&#20026;&#20309;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#65292;&#36825;&#23545;&#25351;&#23548;&#26410;&#26469;&#25968;&#25454;&#25910;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#26469;&#32534;&#30721;&#35757;&#32451;&#38598;&#30340;&#29305;&#24449;&#27010;&#29575;&#20998;&#24067;&#21644;&#29420;&#31435;&#24615;&#20316;&#20026;&#32467;&#26500;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#22522;&#26412;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;DAGnosis&#65292;&#21033;&#29992;&#36825;&#20123;&#32467;&#26500;&#20132;&#20114;&#24102;&#26469;&#26377;&#20215;&#20540;&#30340;&#12289;&#28145;&#21051;&#30340;&#25968;&#25454;&#20013;&#24515;&#32467;&#35770;&#12290;DAGnosis&#35299;&#38145;&#20102;&#22312;DAG&#19978;&#23450;&#20301;&#19981;&#19968;&#33268;&#24615;&#21407;&#22240;&#30340;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17599v1 Announce Type: cross  Abstract: Identification and appropriate handling of inconsistencies in data at deployment time is crucial to reliably use machine learning models. While recent data-centric methods are able to identify such inconsistencies with respect to the training set, they suffer from two key limitations: (1) suboptimality in settings where features exhibit statistical independencies, due to their usage of compressive representations and (2) lack of localization to pin-point why a sample might be flagged as inconsistent, which is important to guide future data collection. We solve these two fundamental limitations using directed acyclic graphs (DAGs) to encode the training set's features probability distribution and independencies as a structure. Our method, called DAGnosis, leverages these structural interactions to bring valuable and insightful data-centric conclusions. DAGnosis unlocks the localization of the causes of inconsistencies on a DAG, an aspec
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#26469;&#20197;&#30446;&#26631;&#23548;&#21521;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#21487;&#33021;&#20250;&#38754;&#20020;&#22870;&#21169;&#23849;&#28291;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15194</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#20316;&#20026;&#29109;&#27491;&#21017;&#21270;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15194
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#26469;&#20197;&#30446;&#26631;&#23548;&#21521;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#21487;&#33021;&#20250;&#38754;&#20020;&#22870;&#21169;&#23849;&#28291;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;&#33258;&#28982;&#22270;&#20687;&#21644;&#34507;&#30333;&#36136;&#30340;&#20998;&#24067;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21487;&#20195;&#34920;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#65292;&#20294;&#25105;&#20204;&#36890;&#24120;&#26356;&#20851;&#27880;&#20854;&#20182;&#23646;&#24615;&#65292;&#20363;&#22914;&#29983;&#25104;&#22270;&#20687;&#30340;&#32654;&#23398;&#36136;&#37327;&#25110;&#29983;&#25104;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#23646;&#24615;&#12290;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#26576;&#20123;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#65288;&#20363;&#22914;&#22270;&#20687;&#30340;&#32654;&#23398;&#36136;&#37327;&#65289;&#20197;&#30446;&#26631;&#23548;&#21521;&#30340;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#26679;&#26412;&#22810;&#26679;&#24615;&#20943;&#23569;&#65292;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#20986;&#29616;&#26174;&#33879;&#20559;&#24046;&#65292;&#29978;&#33267;&#30001;&#20110;&#21033;&#29992;&#19981;&#23436;&#32654;&#30340;&#22870;&#21169;&#20989;&#25968;&#32780;&#23548;&#33268;&#26679;&#26412;&#36136;&#37327;&#36739;&#24046;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#22870;&#21169;&#20989;&#25968;&#26159;&#29992;&#20110;&#36817;&#20284;&#30495;&#23454;&#8220;&#30495;&#23454;&#8221;&#22870;&#21169;&#30340;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#26368;&#21518;&#19968;&#20010;&#38382;&#39064;&#32463;&#24120;&#20250;&#20135;&#29983;&#12290;&#36825;&#20123;&#25361;&#25112;&#24635;&#31216;&#20026;&#8220;&#22870;&#21169;&#23849;&#28291;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15194v1 Announce Type: cross  Abstract: Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth "genuine" reward, as is the case in many practical applications. These challenges, collectively termed "reward collapse," pose
&lt;/p&gt;</description></item><item><title>HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10228</link><description>&lt;p&gt;
HyperAgent&#65306;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#29992;&#20110;&#22797;&#26434;&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10228
&lt;/p&gt;
&lt;p&gt;
HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#19981;&#26029;&#31215;&#32047;&#30340;&#20132;&#20114;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyperAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36229;&#27169;&#22411;&#12289;&#32034;&#24341;&#25277;&#26679;&#26041;&#26696;&#21644;&#22686;&#37327;&#26356;&#26032;&#26426;&#21046;&#30340;RL&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#20013;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#39034;&#24207;&#21518;&#39564;&#36924;&#36817;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#36229;&#36234;&#20102;&#20849;&#36717;&#24615;&#12290;HyperAgent&#30340;&#23454;&#29616;&#31616;&#21333;&#65292;&#21482;&#38656;&#35201;&#22312;DDQN&#20013;&#28155;&#21152;&#19968;&#20010;&#27169;&#22359;&#21644;&#19968;&#34892;&#39069;&#22806;&#20195;&#30721;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;HyperAgent&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#25968;&#25454;&#36824;&#26159;&#35745;&#31639;&#26041;&#38754;&#37117;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#22312;&#23454;&#38469;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#20013;&#65292;HyperAgent&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
&lt;/p&gt;</description></item><item><title>Stein Boltzmann&#25277;&#26679;&#65288;SBS&#65289;&#26159;&#19968;&#31181;&#20840;&#23616;&#20248;&#21270;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;Boltzmann&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#30001;Stein Variational Gradient Descent&#31639;&#27861;&#23454;&#29616;&#65292;&#20855;&#26377;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#20989;&#25968;&#19978;&#27604;&#36739;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#65292;&#23588;&#20854;&#36866;&#21512;&#20316;&#20026;&#39640;&#25928;&#20840;&#23616;&#20248;&#21270;&#26041;&#27861;&#30340;&#24310;&#32493;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#26377;&#25928;&#21033;&#29992;&#39044;&#31639;&#12290;</title><link>https://arxiv.org/abs/2402.04689</link><description>&lt;p&gt;
Stein Boltzmann&#25277;&#26679;&#65306;&#19968;&#31181;&#20840;&#23616;&#20248;&#21270;&#30340;&#21464;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stein Boltzmann Sampling: A Variational Approach for Global Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04689
&lt;/p&gt;
&lt;p&gt;
Stein Boltzmann&#25277;&#26679;&#65288;SBS&#65289;&#26159;&#19968;&#31181;&#20840;&#23616;&#20248;&#21270;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;Boltzmann&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#30001;Stein Variational Gradient Descent&#31639;&#27861;&#23454;&#29616;&#65292;&#20855;&#26377;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#20989;&#25968;&#19978;&#27604;&#36739;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#65292;&#23588;&#20854;&#36866;&#21512;&#20316;&#20026;&#39640;&#25928;&#20840;&#23616;&#20248;&#21270;&#26041;&#27861;&#30340;&#24310;&#32493;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#26377;&#25928;&#21033;&#29992;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27969;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;Lipschitz&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#65292;&#31216;&#20026;Stein Boltzmann&#25277;&#26679;&#65288;SBS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;Boltzmann&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#35813;&#20998;&#24067;&#22312;&#20248;&#21270;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#38598;&#21512;&#19978;&#28176;&#36817;&#22343;&#21248;&#12290;&#20505;&#36873;&#35299;&#36890;&#36807;Stein Variational Gradient Descent&#31639;&#27861;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;SBS&#21464;&#20307;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#20989;&#25968;&#19978;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#35774;&#35745;&#12289;&#29702;&#35770;&#32467;&#26524;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;SBS&#29305;&#21035;&#36866;&#21512;&#20316;&#20026;&#39640;&#25928;&#20840;&#23616;&#20248;&#21270;&#26041;&#27861;&#30340;&#24310;&#32493;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#24456;&#22909;&#22320;&#21033;&#29992;&#39044;&#31639;&#30340;&#21516;&#26102;&#20135;&#29983;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new flow-based method for global optimization of Lipschitz functions, called Stein Boltzmann Sampling (SBS). Our method samples from the Boltzmann distribution that becomes asymptotically uniform over the set of the minimizers of the function to be optimized. Candidate solutions are sampled via the \emph{Stein Variational Gradient Descent} algorithm. We prove the asymptotic convergence of our method, introduce two SBS variants, and provide a detailed comparison with several state-of-the-art global optimization algorithms on various benchmark functions. The design of our method, the theoretical results, and our experiments, suggest that SBS is particularly well-suited to be used as a continuation of efficient global optimization methods as it can produce better solutions while making a good use of the budget.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#65292;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.05440</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#21644;&#24555;&#36895;&#27169;&#25311;&#25512;&#26029;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Consistency Models for Scalable and Fast Simulation-Based Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05440
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#65292;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#25512;&#26029;&#65288;SBI&#65289;&#19981;&#26029;&#23547;&#25214;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#31639;&#27861;&#65292;&#20197;&#20934;&#30830;&#25512;&#26029;&#22797;&#26434;&#27169;&#22411;&#30340;&#21442;&#25968;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CMPE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#30340;&#26032;&#33258;&#30001;&#24418;&#24335;&#26465;&#20214;&#37319;&#26679;&#22120;&#65292;&#21033;&#29992;&#29983;&#25104;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;CMPE&#23558;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#21040;&#21333;&#20010;&#29983;&#25104;&#26550;&#26500;&#20013;&#65306;&#23427;&#26412;&#36136;&#19978;&#25552;&#28860;&#20102;&#36830;&#32493;&#27010;&#29575;&#27969;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#26080;&#32422;&#26463;&#30340;&#32467;&#26500;&#24555;&#36895;&#36827;&#34892;&#23569;&#23556;&#25512;&#26029;&#65292;&#35813;&#32467;&#26500;&#21487;&#20197;&#23450;&#21046;&#21040;&#20272;&#35745;&#38382;&#39064;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;CMPE&#19981;&#20165;&#22312;&#19977;&#20010;&#22256;&#38590;&#30340;&#20302;&#32500;&#38382;&#39064;&#19978;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#65292;&#32780;&#19988;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#21435;&#22122;&#23454;&#39564;&#21644;&#20272;&#35745;&#35745;&#31639;&#23494;&#38598;&#22411;&#22810;&#23610;&#24230;&#20013;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05440v2 Announce Type: replace-cross  Abstract: Simulation-based inference (SBI) is constantly in search of more expressive algorithms for accurately inferring the parameters of complex models from noisy data. We present consistency models for neural posterior estimation (CMPE), a new free-form conditional sampler for scalable, fast, and amortized SBI with generative neural networks. CMPE combines the advantages of normalizing flows and flow matching methods into a single generative architecture: It essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be tailored to the structure of the estimation problem. Our empirical evaluation demonstrates that CMPE not only outperforms current state-of-the-art algorithms on three hard low-dimensional problems but also achieves competitive performance in a high-dimensional Bayesian denoising experiment and in estimating a computationally demanding multi-scale 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21307;&#30103;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#31639;&#27861;&#20559;&#20506;&#65292;&#36890;&#36807;&#37319;&#29992;CART&#31639;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#20559;&#20506;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#21644;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.02959</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#26816;&#27979;&#31639;&#27861;&#20559;&#20506;
&lt;/p&gt;
&lt;p&gt;
Detecting algorithmic bias in medical AI-models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21307;&#30103;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#31639;&#27861;&#20559;&#20506;&#65292;&#36890;&#36807;&#37319;&#29992;CART&#31639;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#20559;&#20506;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#21644;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26085;&#30410;&#26222;&#21450;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#20197;&#20844;&#24179;&#12289;&#20844;&#27491;&#30340;&#26041;&#24335;&#25552;&#20379;&#24739;&#32773;&#32467;&#26524;&#21464;&#24471;&#21516;&#26679;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21307;&#30103;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#31639;&#27861;&#20559;&#20506;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20998;&#31867;&#19982;&#22238;&#24402;&#26641;&#65288;CART&#65289;&#31639;&#27861;&#65292;&#22312;&#33043;&#27602;&#30151;&#39044;&#27979;&#32972;&#26223;&#19979;&#26377;&#25928;&#22320;&#35782;&#21035;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#20559;&#20506;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#19968;&#31995;&#21015;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#20934;&#30830;&#20272;&#35745;&#20559;&#20506;&#21306;&#22495;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#27010;&#24565;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#20351;&#29992;&#20122;&#29305;&#20848;&#22823;&#20052;&#27835;&#20122;&#24030;&#26684;&#38647;&#36842;&#32426;&#24565;&#21307;&#38498;&#30340;&#30005;&#23376;&#30149;&#21382;&#36827;&#34892;&#23454;&#39564;&#36827;&#19968;&#27493;&#24471;&#21040;&#39564;&#35777;&#12290;&#36825;&#20123;&#27979;&#35797;&#23637;&#31034;&#20102;&#25105;&#20204;&#31574;&#30053;&#22312;&#20020;&#24202;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02959v3 Announce Type: replace-cross  Abstract: With the growing prevalence of machine learning and artificial intelligence-based medical decision support systems, it is equally important to ensure that these systems provide patient outcomes in a fair and equitable fashion. This paper presents an innovative framework for detecting areas of algorithmic bias in medical-AI decision support systems. Our approach efficiently identifies potential biases in medical-AI models, specifically in the context of sepsis prediction, by employing the Classification and Regression Trees (CART) algorithm. We verify our methodology by conducting a series of synthetic data experiments, showcasing its ability to estimate areas of bias in controlled settings precisely. The effectiveness of the concept is further validated by experiments using electronic medical records from Grady Memorial Hospital in Atlanta, Georgia. These tests demonstrate the practical implementation of our strategy in a clini
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#26031;&#22240;&#23376;&#22270;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32622;&#20449;&#20256;&#25773;&#35299;&#20915;&#35757;&#32451;&#21644;&#39044;&#27979;&#38382;&#39064;&#65292;&#25903;&#25345;&#20998;&#24067;&#24335;&#21644;&#24322;&#27493;&#35757;&#32451;&#65292;&#21487;&#25193;&#23637;&#33267;&#28145;&#24230;&#32593;&#32476;&#65292;&#25552;&#20379;&#25345;&#32493;&#23398;&#20064;&#30340;&#33258;&#28982;&#26041;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35270;&#39057;&#21435;&#22122;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2311.14649</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#30340;&#28145;&#24230;&#22240;&#23376;&#22270;&#20013;&#36827;&#34892;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning in Deep Factor Graphs with Gaussian Belief Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14649
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#26031;&#22240;&#23376;&#22270;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32622;&#20449;&#20256;&#25773;&#35299;&#20915;&#35757;&#32451;&#21644;&#39044;&#27979;&#38382;&#39064;&#65292;&#25903;&#25345;&#20998;&#24067;&#24335;&#21644;&#24322;&#27493;&#35757;&#32451;&#65292;&#21487;&#25193;&#23637;&#33267;&#28145;&#24230;&#32593;&#32476;&#65292;&#25552;&#20379;&#25345;&#32493;&#23398;&#20064;&#30340;&#33258;&#28982;&#26041;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35270;&#39057;&#21435;&#22122;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#26031;&#22240;&#23376;&#22270;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#30456;&#20851;&#25968;&#37327;&#65288;&#36755;&#20837;&#12289;&#36755;&#20986;&#12289;&#21442;&#25968;&#12289;&#28508;&#21464;&#37327;&#65289;&#35270;&#20026;&#22270;&#27169;&#22411;&#20013;&#30340;&#38543;&#26426;&#21464;&#37327;&#65292;&#24182;&#23558;&#35757;&#32451;&#21644;&#39044;&#27979;&#37117;&#35270;&#20026;&#20855;&#26377;&#19981;&#21516;&#35266;&#23519;&#33410;&#28857;&#30340;&#25512;&#29702;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#32622;&#20449;&#20256;&#25773;&#65288;BP&#65289;&#26377;&#25928;&#22320;&#35299;&#20915;&#65292;&#20854;&#26356;&#26032;&#26412;&#36136;&#19978;&#26159;&#26412;&#22320;&#30340;&#65292;&#20026;&#20998;&#24067;&#24335;&#21644;&#24322;&#27493;&#35757;&#32451;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#28145;&#23618;&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#24335;&#65306;&#20351;&#29992;&#24403;&#21069;&#20219;&#21153;&#30340;BP&#20272;&#35745;&#21442;&#25968;&#36793;&#38469;&#20316;&#20026;&#19979;&#19968;&#20010;&#20219;&#21153;&#30340;&#21442;&#25968;&#20808;&#39564;&#12290;&#22312;&#35270;&#39057;&#21435;&#22122;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#23398;&#20064;&#21442;&#25968;&#30456;&#23545;&#20110;&#20256;&#32479;&#22240;&#23376;&#22270;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#28145;&#24230;&#22240;&#23376;&#22270;&#22312;&#25345;&#32493;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#30340;&#40723;&#33310;&#20154;&#24515;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14649v2 Announce Type: replace  Abstract: We propose an approach to do learning in Gaussian factor graphs. We treat all relevant quantities (inputs, outputs, parameters, latents) as random variables in a graphical model, and view both training and prediction as inference problems with different observed nodes. Our experiments show that these problems can be efficiently solved with belief propagation (BP), whose updates are inherently local, presenting exciting opportunities for distributed and asynchronous training. Our approach can be scaled to deep networks and provides a natural means to do continual learning: use the BP-estimated parameter marginals of the current task as parameter priors for the next. On a video denoising task we demonstrate the benefit of learnable parameters over a classical factor graph approach and we show encouraging performance of deep factor graphs for continual image classification.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36125;&#21494;&#26031;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#26041;&#27861;&#65292;&#22312;&#39640;&#38454;&#12289;&#19981;&#23436;&#25972;&#21644;&#22024;&#26434;&#30340;&#27969;&#25968;&#25454;&#20013;&#65292;&#23454;&#29616;&#20102;&#27969;&#25968;&#25454;&#30340;&#24674;&#22797;&#21644;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2302.12148</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#36827;&#34892;&#27969;&#25968;&#25454;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Streaming data recovery via Bayesian tensor train decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.12148
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#26041;&#27861;&#65292;&#22312;&#39640;&#38454;&#12289;&#19981;&#23436;&#25972;&#21644;&#22024;&#26434;&#30340;&#27969;&#25968;&#25454;&#20013;&#65292;&#23454;&#29616;&#20102;&#27969;&#25968;&#25454;&#30340;&#24674;&#22797;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#24352;&#37327;&#21015;&#36710;(TT)&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#39640;&#38454;&#27969;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#32467;&#26500;&#26469;&#24674;&#22797;&#27969;&#25968;&#25454;&#12290;&#20511;&#37492;&#27969;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;TT&#26684;&#24335;&#24341;&#20837;&#36125;&#21494;&#26031;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#20197;&#29992;&#20110;&#27969;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;TT&#26680;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#30001;&#20110;TT&#26684;&#24335;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;(SPTT)&#22312;&#24674;&#22797;&#20855;&#26377;&#39640;&#38454;&#12289;&#19981;&#23436;&#25972;&#21644;&#22024;&#26434;&#29305;&#24615;&#30340;&#27969;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#27969;&#25968;&#25454;&#30340;&#26368;&#26032;&#36125;&#21494;&#26031;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.12148v2 Announce Type: replace  Abstract: In this paper, we study a Bayesian tensor train (TT) decomposition method to recover streaming data by approximating the latent structure in high-order streaming data. Drawing on the streaming variational Bayes method, we introduce the TT format into Bayesian tensor decomposition methods for streaming data, and formulate posteriors of TT cores. Thanks to the Bayesian framework of the TT format, the proposed algorithm (SPTT) excels in recovering streaming data with high-order, incomplete, and noisy properties. The experiments in synthetic and real-world datasets show the accuracy of our method compared to state-of-the-art Bayesian tensor decomposition methods for streaming data.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#22686;&#24378;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#32780;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#22686;&#24378;&#36890;&#36807;&#38544;&#24335;&#35889;&#27491;&#21017;&#21270;&#23545;&#32447;&#24615;&#27169;&#22411;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2210.05021</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22351;&#19982;&#19985;&#38475;&#38754;&#65306;&#38544;&#24335;&#35889;&#27491;&#21017;&#21270;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
The good, the bad and the ugly sides of data augmentation: An implicit spectral regularization perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.05021
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#32780;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#22686;&#24378;&#36890;&#36807;&#38544;&#24335;&#35889;&#27491;&#21017;&#21270;&#23545;&#32447;&#24615;&#27169;&#22411;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#22686;&#24378;&#24615;&#33021;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20256;&#32479;&#19978;&#35748;&#20026;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#29305;&#23450;&#22686;&#24378;&#65292;&#27604;&#22914;&#24179;&#31227;&#21644;&#32553;&#25918;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#30456;&#21516;&#20998;&#24067;&#29983;&#25104;&#26032;&#30340;&#65288;&#20154;&#24037;&#65289;&#25968;&#25454;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20256;&#32479;&#35266;&#28857;&#19981;&#33021;&#35299;&#37322;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#27969;&#34892;&#22686;&#24378;&#30340;&#25104;&#21151;&#65288;&#20363;&#22914;&#38543;&#26426;&#36974;&#25377;&#12289;cutout&#12289;mixup&#65289;&#65292;&#36825;&#20123;&#22686;&#24378;&#22823;&#22823;&#25913;&#21464;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#34920;&#24449;&#19968;&#33324;&#31867;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#23545;&#27424;&#21442;&#25968;&#21270;&#21644;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25581;&#31034;&#20102;&#25968;&#25454;&#22686;&#24378;&#36890;&#36807;&#20004;&#31181;&#19981;&#21516;&#25928;&#24212;&#30340;&#32452;&#21512;&#65306;a)&#20197;&#35757;&#32451;&#25968;&#25454;&#20026;&#22522;&#30784;&#25805;&#32437;&#25968;&#25454;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#30340;&#30456;&#23545;&#27604;&#20363;&#65292;b)&#32479;&#19968;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.05021v3 Announce Type: replace  Abstract: Data augmentation (DA) is a powerful workhorse for bolstering performance in modern machine learning. Specific augmentations like translations and scaling in computer vision are traditionally believed to improve generalization by generating new (artificial) data from the same distribution. However, this traditional viewpoint does not explain the success of prevalent augmentations in modern machine learning (e.g. randomized masking, cutout, mixup), that greatly alter the training data distribution. In this work, we develop a new theoretical framework to characterize the impact of a general class of DA on underparameterized and overparameterized linear model generalization. Our framework reveals that DA induces implicit spectral regularization through a combination of two distinct effects: a) manipulating the relative proportion of eigenvalues of the data covariance matrix in a training-data-dependent manner, and b) uniformly boosting 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#24230;&#19979;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#32622;&#20449;&#21306;&#38388;&#26500;&#24314;&#26041;&#27861;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;&#32570;&#22833;&#25968;&#25454;&#21644;&#24322;&#26041;&#24046;&#22122;&#22768;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2107.12365</link><description>&lt;p&gt;
&#20855;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#24322;&#26041;&#24046;PCA&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference for Heteroskedastic PCA with Missing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2107.12365
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#24230;&#19979;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#32622;&#20449;&#21306;&#38388;&#26500;&#24314;&#26041;&#27861;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;&#32570;&#22833;&#25968;&#25454;&#21644;&#24322;&#26041;&#24046;&#22122;&#22768;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#39640;&#32500;&#24230;&#20013;&#26500;&#24314;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#36825;&#26159;&#19968;&#20010;&#24191;&#27867;&#26410;&#34987;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312;&#39640;&#32500;&#24230;&#20013;&#35745;&#31639;&#38750;&#32447;&#24615;/&#38750;&#20984;&#20272;&#35745;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#36890;&#24120;&#26159;&#22256;&#38590;&#30340;&#65292;&#32780;&#36825;&#19968;&#25361;&#25112;&#26356;&#21152;&#22797;&#26434;&#65292;&#22240;&#20026;&#32570;&#22833;&#25968;&#25454;&#21644;&#24322;&#26041;&#24046;&#22122;&#22768;&#26222;&#36941;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24102;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#23574;&#23792;&#21327;&#26041;&#24046;&#27169;&#22411;&#19979;&#23545;&#20027;&#23376;&#31354;&#38388;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#65292;&#22522;&#20110;&#19968;&#31181;&#21517;&#20026;HeteroPCA&#30340;&#20272;&#35745;&#37327;&#65288;Zhang&#31561;&#20154;&#65292;2022&#24180;&#65289;&#12290;&#25105;&#20204;&#20026;HeteroPCA&#24320;&#21457;&#20102;&#38750;&#28176;&#36817;&#20998;&#24067;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#20445;&#35777;&#26469;&#35745;&#31639;&#20027;&#23376;&#31354;&#38388;&#30340;&#32622;&#20449;&#21306;&#38388;&#20197;&#21450;&#23574;&#23792;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#26465;&#30446;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#25512;&#26029;&#31243;&#24207;&#23436;&#20840;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#24322;&#26041;&#24046;&#38543;&#26426;&#22122;&#22768;&#65292;&#32780;&#26080;&#38656;&#20808;&#39564;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2107.12365v2 Announce Type: replace-cross  Abstract: This paper studies how to construct confidence regions for principal component analysis (PCA) in high dimension, a problem that has been vastly under-explored. While computing measures of uncertainty for nonlinear/nonconvex estimators is in general difficult in high dimension, the challenge is further compounded by the prevalent presence of missing data and heteroskedastic noise. We propose a novel approach to performing valid inference on the principal subspace under a spiked covariance model with missing data, on the basis of an estimator called HeteroPCA (Zhang et al., 2022). We develop non-asymptotic distributional guarantees for HeteroPCA, and demonstrate how these can be invoked to compute both confidence regions for the principal subspace and entrywise confidence intervals for the spiked covariance matrix. Our inference procedures are fully data-driven and adaptive to heteroskedastic random noise, without requiring prior
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ELLE&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#20943;&#36731;&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;&#23427;&#33021;&#22815;&#20445;&#25345;&#25439;&#22833;&#20989;&#25968;&#22312;&#36755;&#20837;&#19978;&#30340;&#23616;&#37096;&#32447;&#24615;&#24615;&#65292;&#19982;&#20256;&#32479;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;ELLE&#26356;&#21152;&#39640;&#25928;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22823;&#23545;&#25239;&#24615;&#25200;&#21160;&#21644;&#38271;&#35757;&#32451;&#35745;&#21010;&#31561;&#22256;&#38590;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.11618</link><description>&lt;p&gt;
&#29992;&#20110;&#20811;&#26381;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#30340;&#39640;&#25928;&#26412;&#22320;&#32447;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient local linearity regularization to overcome catastrophic overfitting. (arXiv:2401.11618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ELLE&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#20943;&#36731;&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;&#23427;&#33021;&#22815;&#20445;&#25345;&#25439;&#22833;&#20989;&#25968;&#22312;&#36755;&#20837;&#19978;&#30340;&#23616;&#37096;&#32447;&#24615;&#24615;&#65292;&#19982;&#20256;&#32479;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;ELLE&#26356;&#21152;&#39640;&#25928;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22823;&#23545;&#25239;&#24615;&#25200;&#21160;&#21644;&#38271;&#35757;&#32451;&#35745;&#21010;&#31561;&#22256;&#38590;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512; (CO) &#23548;&#33268;&#23545;&#25239;&#24615;&#27979;&#35797;&#20934;&#30830;&#29575;&#31361;&#28982;&#19979;&#38477;&#65288;&#29978;&#33267;&#38477;&#33267;0%&#65289;&#12290;&#23545;&#20110;&#20351;&#29992;&#22810;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24050;&#35266;&#23519;&#21040;&#25439;&#22833;&#20989;&#25968;&#22312;&#36755;&#20837;&#19978;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#24615;&#65292;&#20294;&#36825;&#31181;&#29305;&#24615;&#22312;&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;CO&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#24378;&#21046;&#25439;&#22833;&#20989;&#25968;&#23616;&#37096;&#32447;&#24615;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21452;&#37325;&#21453;&#21521;&#20256;&#25773;&#65292;&#36825;&#20123;&#27491;&#21017;&#21270;&#39033;&#20250;&#26174;&#33879;&#20943;&#24930;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;ELLE&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#20197;&#22312;&#32463;&#20856;&#23545;&#25239;&#24615;&#35757;&#32451;&#35780;&#20272;&#20013;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#20943;&#36731;CO&#38382;&#39064;&#65292;&#22312;&#19968;&#20123;&#26356;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#36215;&#20316;&#29992;&#65292;&#20363;&#22914;&#22823;&#23545;&#25239;&#24615;&#25200;&#21160;&#21644;&#38271;&#35757;&#32451;&#35745;&#21010;&#12290;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#39033;&#22312;&#29702;&#35770;&#19978;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#26354;&#29575;&#26377;&#32852;&#31995;&#65292;&#24182;&#19988;&#36890;&#36807;&#36991;&#20813;&#21452;&#37325;&#21453;&#21521;&#20256;&#25773;&#32780;&#20855;&#26377;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#24443;&#24213;&#30340;&#23454;&#39564;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Catastrophic overfitting (CO) in single-step adversarial training (AT) results in abrupt drops in the adversarial test accuracy (even down to 0%). For models trained with multi-step AT, it has been observed that the loss function behaves locally linearly with respect to the input, this is however lost in single-step AT. To address CO in single-step AT, several methods have been proposed to enforce local linearity of the loss via regularization. However, these regularization terms considerably slow down training due to Double Backpropagation. Instead, in this work, we introduce a regularization term, called ELLE, to mitigate CO effectively and efficiently in classical AT evaluations, as well as some more difficult regimes, e.g., large adversarial perturbations and long training schedules. Our regularization term can be theoretically linked to curvature of the loss function and is computationally cheaper than previous methods by avoiding Double Backpropagation. Our thorough experimental 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#39044;&#21518;&#21327;&#21464;&#37327;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21327;&#21464;&#37327;&#35843;&#25972;&#21644;&#21382;&#21490;&#23545;&#29031;&#20449;&#24687;&#21033;&#29992;&#30340;&#31574;&#30053;&#65292;&#22312;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#29992;&#20110;&#26377;&#25928;&#21644;&#24555;&#36895;&#20915;&#31574;&#12290;&#36890;&#36807;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26500;&#24314;&#25968;&#23383;&#23402;&#29983;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#21382;&#21490;&#23545;&#29031;&#25968;&#25454;&#20135;&#29983;&#25968;&#23383;&#23402;&#29983;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#32780;&#36827;&#34892;&#21333;&#19968;&#21327;&#21464;&#37327;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2310.18027</link><description>&lt;p&gt;
&#20855;&#26377;&#21472;&#21152;&#28151;&#21512;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#39044;&#21518;&#21327;&#21464;&#37327;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayesian Prognostic Covariate Adjustment With Additive Mixture Priors. (arXiv:2310.18027v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18027
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#39044;&#21518;&#21327;&#21464;&#37327;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21327;&#21464;&#37327;&#35843;&#25972;&#21644;&#21382;&#21490;&#23545;&#29031;&#20449;&#24687;&#21033;&#29992;&#30340;&#31574;&#30053;&#65292;&#22312;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#29992;&#20110;&#26377;&#25928;&#21644;&#24555;&#36895;&#20915;&#31574;&#12290;&#36890;&#36807;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26500;&#24314;&#25968;&#23383;&#23402;&#29983;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#21382;&#21490;&#23545;&#29031;&#25968;&#25454;&#20135;&#29983;&#25968;&#23383;&#23402;&#29983;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#32780;&#36827;&#34892;&#21333;&#19968;&#21327;&#21464;&#37327;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCTs&#65289;&#20013;&#36827;&#34892;&#26377;&#25928;&#21644;&#24555;&#36895;&#30340;&#20915;&#31574;&#38656;&#35201;&#26080;&#20559;&#21644;&#20934;&#30830;&#30340;&#27835;&#30103;&#25928;&#26524;&#25512;&#26029;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#35201;&#27714;&#65292;&#26377;&#20004;&#31181;&#31574;&#30053;&#65306;&#35843;&#25972;&#19982;&#32467;&#26524;&#39640;&#24230;&#30456;&#20851;&#30340;&#21327;&#21464;&#37327;&#65292;&#20197;&#21450;&#36890;&#36807;&#36125;&#21494;&#26031;&#23450;&#29702;&#21033;&#29992;&#21382;&#21490;&#23545;&#29031;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#39044;&#21518;&#21327;&#21464;&#37327;&#35843;&#25972;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;PROCOVA&#65292;&#23558;&#36825;&#20004;&#31181;&#31574;&#30053;&#32467;&#21512;&#36215;&#26469;&#12290;&#21327;&#21464;&#37327;&#35843;&#25972;&#22522;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#65292;&#26500;&#24314;&#20102;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#21442;&#19982;&#32773;&#30340;&#25968;&#23383;&#23402;&#29983;&#29983;&#25104;&#22120;&#65288;DTG&#65289;&#12290;DTG&#36890;&#36807;&#21382;&#21490;&#23545;&#29031;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#27599;&#20010;&#21442;&#19982;&#32773;&#30340;&#23545;&#29031;&#32467;&#26524;&#20135;&#29983;&#20102;&#19968;&#20010;&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#27010;&#29575;&#20998;&#24067;&#12290;DT&#20998;&#24067;&#30340;&#26399;&#26395;&#23450;&#20041;&#20102;&#29992;&#20110;&#35843;&#25972;&#30340;&#21333;&#19968;&#21327;&#21464;&#37327;&#12290;&#21382;&#21490;&#23545;&#29031;&#20449;&#24687;&#36890;&#36807;&#20855;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#21472;&#21152;&#28151;&#21512;&#20808;&#39564;&#36827;&#34892;&#21033;&#29992;&#65306;&#22522;&#20110;&#20808;&#39564;&#20449;&#24687;&#30830;&#23450;&#30340;&#19968;&#20010;&#20449;&#24687;&#20808;&#39564;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective and rapid decision-making from randomized controlled trials (RCTs) requires unbiased and precise treatment effect inferences. Two strategies to address this requirement are to adjust for covariates that are highly correlated with the outcome, and to leverage historical control information via Bayes' theorem. We propose a new Bayesian prognostic covariate adjustment methodology, referred to as Bayesian PROCOVA, that combines these two strategies. Covariate adjustment is based on generative artificial intelligence (AI) algorithms that construct a digital twin generator (DTG) for RCT participants. The DTG is trained on historical control data and yields a digital twin (DT) probability distribution for each participant's control outcome. The expectation of the DT distribution defines the single covariate for adjustment. Historical control information are leveraged via an additive mixture prior with two components: an informative prior probability distribution specified based on h
&lt;/p&gt;</description></item><item><title>&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;(CoExBO)&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#24490;&#29615;&#65292;&#24179;&#34913;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#23558;&#29992;&#25143;&#35265;&#35299;&#34701;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#35299;&#37322;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#23545;&#20248;&#21270;&#36807;&#31243;&#30340;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17273</link><description>&lt;p&gt;
&#23558;&#24490;&#29615;&#24341;&#20837;&#20154;&#31867;&#65306;&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Looping in the Human: Collaborative and Explainable Bayesian Optimization. (arXiv:2310.17273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17273
&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;(CoExBO)&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#24490;&#29615;&#65292;&#24179;&#34913;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#23558;&#29992;&#25143;&#35265;&#35299;&#34701;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#35299;&#37322;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#23545;&#20248;&#21270;&#36807;&#31243;&#30340;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;&#35768;&#22810;&#20248;&#21270;&#22120;&#19968;&#26679;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#33719;&#24471;&#29992;&#25143;&#20449;&#20219;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;&#20854;&#19981;&#36879;&#26126;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#23581;&#35797;&#24320;&#21457;&#38754;&#21521;&#20154;&#31867;&#30340;&#20248;&#21270;&#22120;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20551;&#35774;&#29992;&#25143;&#30693;&#35782;&#26159;&#26126;&#30830;&#19988;&#26080;&#35823;&#30340;&#65292;&#24182;&#20027;&#35201;&#23558;&#29992;&#25143;&#20316;&#20026;&#20248;&#21270;&#36807;&#31243;&#30340;&#30417;&#30563;&#32773;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24179;&#34913;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#21512;&#20316;&#20249;&#20276;&#20851;&#31995;&#65292;&#21363;&#25105;&#20204;&#30340;&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CoExBO&#65289;&#26694;&#26550;&#12290;CoExBO&#20351;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#26080;&#32541;&#22320;&#23558;&#20154;&#31867;&#35265;&#35299;&#25972;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#19982;&#29992;&#25143;&#20351;&#29992;&#20559;&#22909;&#19968;&#33268;&#30340;&#31639;&#27861;&#24314;&#35758;&#12290;CoExBO&#35299;&#37322;&#20854;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20197;&#22521;&#20859;&#20449;&#20219;&#65292;&#20351;&#29992;&#25143;&#26356;&#28165;&#26970;&#22320;&#25484;&#25569;&#20248;&#21270;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;CoExBO&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#65292;&#20801;&#35768;&#29992;&#25143;&#29359;&#38169;&#35823;&#65307;&#21363;&#20351;&#22312;&#26497;&#31471;&#23545;&#25239;&#24615;&#24178;&#25200;&#19979;&#65292;&#31639;&#27861;&#20063;&#20250;&#28176;&#36827;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like many optimizers, Bayesian optimization often falls short of gaining user trust due to opacity. While attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. We relax these assumptions and propose a more balanced human-AI partnership with our Collaborative and Explainable Bayesian Optimization (CoExBO) framework. Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. Furthermore, CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#19981;&#21516;&#27169;&#24335;&#30340;&#36861;&#36394;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#26469;&#21306;&#20998;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#21644;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#12290;</title><link>http://arxiv.org/abs/2309.10688</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#19981;&#21516;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
On the different regimes of Stochastic Gradient Descent. (arXiv:2309.10688v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10688
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#19981;&#21516;&#27169;&#24335;&#30340;&#36861;&#36394;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#26469;&#21306;&#20998;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#21644;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#32593;&#32476;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20851;&#38190;&#21442;&#25968;&#26159;&#27599;&#20010;&#27493;&#39588;&#32771;&#34385;&#30340;&#25968;&#25454;&#37327;&#25110;&#25209;&#37327;&#22823;&#23567;B&#20197;&#21450;&#27493;&#38271;&#25110;&#23398;&#20064;&#29575;&#951;&#12290;&#23545;&#20110;&#23567;&#30340;B&#21644;&#22823;&#30340;&#951;&#65292;SGD&#23545;&#24212;&#20110;&#21442;&#25968;&#30340;&#38543;&#26426;&#28436;&#21270;&#65292;&#20854;&#22122;&#22768;&#24133;&#24230;&#30001;&#8220;&#28201;&#24230;&#8221;T=&#951;/B&#25511;&#21046;&#12290;&#28982;&#32780;&#24403;&#25209;&#37327;&#22823;&#23567;B&#8805;B^*&#36275;&#22815;&#22823;&#26102;&#65292;&#36825;&#31181;&#25551;&#36848;&#34987;&#35266;&#23519;&#21040;&#22833;&#25928;&#65292;&#25110;&#32773;&#22312;&#28201;&#24230;&#36275;&#22815;&#23567;&#26102;&#31616;&#21270;&#20026;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#12290;&#29702;&#35299;&#36825;&#20123;&#20132;&#21449;&#21457;&#29983;&#30340;&#20301;&#32622;&#20173;&#28982;&#26159;&#19968;&#20010;&#20013;&#24515;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#19968;&#20010;&#25945;&#24072;-&#23398;&#29983;&#24863;&#30693;&#22120;&#20998;&#31867;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20851;&#38190;&#39044;&#27979;&#20173;&#36866;&#29992;&#20110;&#28145;&#24230;&#32593;&#32476;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;B-&#951;&#24179;&#38754;&#19978;&#33719;&#24471;&#20102;&#19968;&#20010;&#30456;&#20301;&#22270;&#65292;&#23558;&#19977;&#20010;&#21160;&#24577;&#38454;&#27573;&#20998;&#24320;&#65306;&#65288;i&#65289;&#21463;&#28201;&#24230;&#25511;&#21046;&#30340;&#22122;&#22768;&#20027;&#23548;&#30340;SGD&#65292;&#65288;ii&#65289;&#22823;&#27493;&#39588;&#20027;&#23548;&#30340;SGD&#21644;
&lt;/p&gt;
&lt;p&gt;
Modern deep networks are trained with stochastic gradient descent (SGD) whose key parameters are the number of data considered at each step or batch size $B$, and the step size or learning rate $\eta$. For small $B$ and large $\eta$, SGD corresponds to a stochastic evolution of the parameters, whose noise amplitude is governed by the `temperature' $T\equiv \eta/B$. Yet this description is observed to break down for sufficiently large batches $B\geq B^*$, or simplifies to gradient descent (GD) when the temperature is sufficiently small. Understanding where these cross-overs take place remains a central challenge. Here we resolve these questions for a teacher-student perceptron classification model, and show empirically that our key predictions still apply to deep networks. Specifically, we obtain a phase diagram in the $B$-$\eta$ plane that separates three dynamical phases: $\textit{(i)}$ a noise-dominated SGD governed by temperature, $\textit{(ii)}$ a large-first-step-dominated SGD and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#24182;&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#20855;&#26377;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#21644;&#26816;&#27979;&#24322;&#24120;&#20540;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#25193;&#23637;&#26041;&#27861;&#20197;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.09055</link><description>&lt;p&gt;
&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#29992;&#20110;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Outlier-Robust Tensor Low-Rank Representation for Data Clustering. (arXiv:2307.09055v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#24182;&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#20855;&#26377;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#21644;&#26816;&#27979;&#24322;&#24120;&#20540;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#25193;&#23637;&#26041;&#27861;&#20197;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#24352;&#37327;&#20998;&#26512;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24352;&#37327;&#25968;&#25454;&#32463;&#24120;&#21463;&#21040;&#24322;&#24120;&#20540;&#25110;&#26679;&#26412;&#29305;&#23450;&#30340;&#27745;&#26579;&#12290;&#22914;&#20309;&#24674;&#22797;&#34987;&#24322;&#24120;&#20540;&#25439;&#22351;&#30340;&#24352;&#37327;&#25968;&#25454;&#24182;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#24352;&#37327;&#25968;&#25454;&#32858;&#31867;&#30340;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#65288;OR-TLRR&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21463;&#21040;&#26368;&#36817;&#25552;&#20986;&#30340;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#21487;&#36870;&#32447;&#24615;&#21464;&#25442;&#24341;&#36215;&#30340;&#24352;&#37327;&#24352;&#37327;&#31215;&#30340;&#21551;&#21457;&#12290;&#23545;&#20110;&#24102;&#26377;&#20219;&#24847;&#24322;&#24120;&#20540;&#27745;&#26579;&#30340;&#24352;&#37327;&#35266;&#27979;&#65292;OR-TLRR&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#33021;&#22815;&#30830;&#20999;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#24182;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;OR-TLRR&#30340;&#25193;&#23637;&#26469;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank tensor analysis has received widespread attention with many practical applications. However, the tensor data are often contaminated by outliers or sample-specific corruptions. How to recover the tensor data that are corrupted by outliers and perform data clustering remains a challenging problem. This paper develops an outlier-robust tensor low-rank representation (OR-TLRR) method for simultaneous outlier detection and tensor data clustering based on the tensor singular value decomposition (t-SVD) algebraic framework. It is motivated by the recently proposed tensor-tensor product induced by invertible linear transforms that satisfy certain conditions. For tensor observations with arbitrary outlier corruptions, OR-TLRR has provable performance guarantee for exactly recovering the row space of clean data and detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is also proposed to handle the case when parts of the data are missing. Finally, extensive experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#35777;&#26497;&#23567;&#26497;&#20540;&#20248;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17224</link><description>&lt;p&gt;
&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#27861;&#24555;&#36895;&#26497;&#23567;&#21270;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fast and Minimax Optimal Estimation of Low-Rank Matrices via Non-Convex Gradient Descent. (arXiv:2305.17224v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#35777;&#26497;&#23567;&#26497;&#20540;&#20248;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22122;&#22768;&#27979;&#37327;&#20013;&#20272;&#35745;&#20302;&#31209;&#30697;&#38453;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#26088;&#22312;&#23454;&#29616;&#26497;&#23567;&#26497;&#20540;&#35823;&#24046;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#30340;&#33021;&#21147;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#20351;&#29992;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#26469;&#35299;&#20915;&#12290;&#29702;&#35770;&#19978;&#65292;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#26497;&#23567;&#26497;&#20540;&#35823;&#24046;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#32463;&#24120;&#25910;&#25947;&#24471;&#38750;&#24120;&#32531;&#24930;&#65292;&#20197;&#33267;&#20110;&#29978;&#33267;&#26080;&#27861;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#25552;&#20379;&#36866;&#24230;&#20934;&#30830;&#30340;&#20272;&#35745;&#20540;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#37325;&#26032;&#32553;&#25918;&#25110;&#39044;&#22788;&#29702;&#25913;&#36827;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#26041;&#27861;&#20063;&#20250;&#22823;&#22823;&#25918;&#22823;&#27979;&#37327;&#35823;&#24046;&#65292;&#23548;&#33268;&#24471;&#21040;&#30340;&#20272;&#35745;&#27604;&#29702;&#35770;&#19978;&#21487;&#23454;&#29616;&#30340;&#26497;&#23567;&#26497;&#20540;&#35823;&#24046;&#23569;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36890;&#24120;&#30340;&#38750;&#20984;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21487;&#35777;&#26126;&#20445;&#30041;&#20854;&#26497;&#23567;&#26497;&#20540;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of estimating a low-rank matrix from noisy measurements, with the specific goal of achieving minimax optimal error. In practice, the problem is commonly solved using non-convex gradient descent, due to its ability to scale to large-scale real-world datasets. In theory, non-convex gradient descent is capable of achieving minimax error. But in practice, it often converges extremely slowly, such that it cannot even deliver estimations of modest accuracy within reasonable time. On the other hand, methods that improve the convergence of non-convex gradient descent, through rescaling or preconditioning, also greatly amplify the measurement noise, resulting in estimations that are orders of magnitude less accurate than what is theoretically achievable with minimax optimal error. In this paper, we propose a slight modification to the usual non-convex gradient descent method that remedies the issue of slow convergence, while provably preserving its minimax optimality. Our p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#23558;&#26631;&#31614;&#20449;&#24687;&#19982;&#31232;&#30095;&#22270;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#32858;&#31867;&#22312;&#31232;&#30095;&#22270;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.11677</link><description>&lt;p&gt;
&#31232;&#30095;&#22270;&#30340;&#21322;&#30417;&#30563;&#32858;&#31867;&#65306;&#36328;&#36234;&#20102;&#20449;&#24687;&#29702;&#35770;&#38376;&#27099;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Clustering of Sparse Graphs: Crossing the Information-Theoretic Threshold. (arXiv:2205.11677v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#23558;&#26631;&#31614;&#20449;&#24687;&#19982;&#31232;&#30095;&#22270;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#32858;&#31867;&#22312;&#31232;&#30095;&#22270;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22359;&#27169;&#22411;&#26159;&#19968;&#31181;&#29992;&#20110;&#32593;&#32476;&#32467;&#26500;&#25968;&#25454;&#32858;&#31867;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#22522;&#26412;&#38543;&#26426;&#22270;&#27169;&#22411;&#12290;&#25968;&#21313;&#24180;&#26469;&#23545;&#35813;&#38382;&#39064;&#30340;&#24191;&#27867;&#30740;&#31350;&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;Kesten-Stigum&#38376;&#27099;&#22788;&#30340;&#30456;&#21464;&#29616;&#35937;&#29305;&#21035;&#26377;&#36259;&#65292;&#20174;&#25968;&#23398;&#21644;&#24212;&#29992;&#35282;&#24230;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23427;&#34920;&#26126;&#65292;&#22914;&#26524;&#27169;&#22411;&#21442;&#25968;&#22312;&#26576;&#20010;&#38376;&#27099;&#20197;&#19979;&#65292;&#22522;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#20219;&#20309;&#20272;&#35745;&#22120;&#22312;&#31232;&#30095;&#22270;&#19978;&#37117;&#19981;&#33021;&#27604;&#38543;&#26426;&#29468;&#27979;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#31245;&#24494;&#25193;&#23637;&#35270;&#37326;&#21040;&#26222;&#36941;&#23384;&#22312;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#65292;&#36825;&#26679;&#30340;&#22522;&#26412;&#38480;&#21046;&#23558;&#23436;&#20840;&#28040;&#22833;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#25581;&#31034;&#20986;&#20219;&#24847;&#19968;&#37096;&#20998;&#26631;&#35760;&#65292;&#21487;&#20197;&#22312;&#25972;&#20010;&#21442;&#25968;&#22495;&#20869;&#23545;&#26816;&#27979;&#38382;&#39064;&#36827;&#34892;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#32452;&#21512;&#30340;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#20248;&#21270;&#30340;&#65292;&#29992;&#20110;&#23558;&#26631;&#31614;&#20449;&#24687;&#19982;&#22270;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#38543;&#26426;&#22359;&#27169;&#22411;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#24102;&#26469;&#20102;&#20840;&#26032;&#30340;&#35270;&#35282;&#65292;&#26631;&#24535;&#30528;&#31232;&#30095;&#22270;&#32858;&#31867;&#39046;&#22495;&#30340;&#37325;&#22823;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic block model is a canonical random graph model for clustering and community detection on network-structured data. Decades of extensive study on the problem have established many profound results, among which the phase transition at the Kesten-Stigum threshold is particularly interesting both from a mathematical and an applied standpoint. It states that no estimator based on the network topology can perform substantially better than chance on sparse graphs if the model parameter is below certain threshold. Nevertheless, if we slightly extend the horizon to the ubiquitous semi-supervised setting, such a fundamental limitation will disappear completely. We prove that with arbitrary fraction of the labels revealed, the detection problem is feasible throughout the parameter domain. Moreover, we introduce two efficient algorithms, one combinatorial and one based on optimization, to integrate label information with graph structures. Our work brings a new perspective to stochasti
&lt;/p&gt;</description></item></channel></rss>