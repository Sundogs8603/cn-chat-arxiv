<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#20957;&#32858;&#27010;&#24565;&#65292;&#26500;&#24314;&#22312;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#20102;&#26089;&#26399;&#32467;&#26524;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#30340;&#31038;&#21306;&#21457;&#29616;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.10167</link><description>&lt;p&gt;
&#24191;&#20041;&#21010;&#20998;&#23616;&#37096;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
Generalized partitioned local depth. (arXiv:2303.10167v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#20957;&#32858;&#27010;&#24565;&#65292;&#26500;&#24314;&#22312;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#20102;&#26089;&#26399;&#32467;&#26524;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#30340;&#31038;&#21306;&#21457;&#29616;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#36817;&#30001;Berenhaut&#12289;Moore&#21644;Melvin [Proccedings of the National Academy of Sciences, 119 (4) (2022)]&#25552;&#20986;&#30340;&#20957;&#32858;&#27010;&#24565;&#30340;&#27010;&#25324;&#12290;&#25152;&#25552;&#20986;&#30340;&#34920;&#36848;&#22522;&#20110;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#24182;&#25552;&#28860;&#20102;&#20004;&#20010;&#20851;&#38190;&#27010;&#29575;&#27010;&#24565;&#65306;&#23616;&#37096;&#30456;&#20851;&#24615;&#21644;&#25903;&#25345;&#20998;&#21106;&#12290;&#26089;&#26399;&#32467;&#26524;&#22312;&#26032;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#25193;&#23637;&#65292;&#24182;&#21253;&#25324;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#20013;&#25581;&#31034;&#31038;&#21306;&#30340;&#24212;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide a generalization of the concept of cohesion as introduced recently by Berenhaut, Moore and Melvin [Proceedings of the National Academy of Sciences, 119 (4) (2022)]. The formulation presented builds on the technique of partitioned local depth by distilling two key probabilistic concepts: local relevance and support division. Earlier results are extended within the new context, and examples of applications to revealing communities in data with uncertainty are included.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#25972;&#26356;&#26032;&#21040;&#25968;&#25454;&#27604;&#29575;&#65288;UTD&#65289;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#23567;&#35268;&#27169;&#30340;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#36830;&#32493;&#25910;&#38598;&#30340;&#32463;&#39564;&#19978;&#26816;&#27979;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;DreamerV2&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#19982;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.10144</link><description>&lt;p&gt;
&#21160;&#24577;&#26356;&#26032;&#25968;&#25454;&#27604;&#29575;&#65306;&#20943;&#23569;&#19990;&#30028;&#27169;&#22411;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Dynamic Update-to-Data Ratio: Minimizing World Model Overfitting. (arXiv:2303.10144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10144
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#25972;&#26356;&#26032;&#21040;&#25968;&#25454;&#27604;&#29575;&#65288;UTD&#65289;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#23567;&#35268;&#27169;&#30340;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#36830;&#32493;&#25910;&#38598;&#30340;&#32463;&#39564;&#19978;&#26816;&#27979;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;DreamerV2&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#19982;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#30340;&#24773;&#22659;&#19979;&#65292;&#22522;&#20110;&#39564;&#35777;&#38598;&#34920;&#29616;&#30340;&#26089;&#26399;&#20572;&#27490;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#21363;&#20351;&#22312;&#35832;&#22914;&#19990;&#30028;&#27169;&#22411;&#23398;&#20064;&#20043;&#31867;&#30340;&#30417;&#30563;&#23376;&#38382;&#39064;&#20013;&#65292;&#20063;&#19981;&#33021;&#24212;&#29992;&#26089;&#26399;&#20572;&#27490;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#22312;&#19981;&#26029;&#28436;&#21464;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#26681;&#25454;&#22312;&#26410;&#21442;&#19982;&#35757;&#32451;&#30340;&#19968;&#23567;&#37096;&#20998;&#36830;&#32493;&#25910;&#38598;&#30340;&#32463;&#39564;&#19978;&#26816;&#27979;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#26469;&#21160;&#24577;&#35843;&#25972;&#35757;&#32451;&#20013;&#30340;&#26356;&#26032;&#21040;&#25968;&#25454;&#27604;&#29575;&#65288;UTD&#65289;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;DreamerV2&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;DeepMind Control Suite&#21644;Atari $100$k&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;DreamerV2&#20013;&#30340;&#40664;&#35748;&#35774;&#32622;&#30456;&#27604;&#65292;&#36890;&#36807;&#35843;&#25972;UTD&#27604;&#29575;&#26469;&#24179;&#34913;&#27424;&#25311;&#21512;&#19982;&#36807;&#25311;&#21512;&#30340;&#25928;&#26524;&#26356;&#22909;&#65292;&#32780;&#19988;&#20855;&#26377;&#19982;&#24191;&#27867;&#36229;&#21442;&#25968;&#25628;&#32034;&#31454;&#20105;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early stopping based on the validation set performance is a popular approach to find the right balance between under- and overfitting in the context of supervised learning. However, in reinforcement learning, even for supervised sub-problems such as world model learning, early stopping is not applicable as the dataset is continually evolving. As a solution, we propose a new general method that dynamically adjusts the update to data (UTD) ratio during training based on under- and overfitting detection on a small subset of the continuously collected experience not used for training. We apply our method to DreamerV2, a state-of-the-art model-based reinforcement learning algorithm, and evaluate it on the DeepMind Control Suite and the Atari $100$k benchmark. The results demonstrate that one can better balance under- and overestimation by adjusting the UTD ratio with our approach compared to the default setting in DreamerV2 and that it is competitive with an extensive hyperparameter search 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#26694;&#26550;&#29992;&#20197;&#37327;&#21270;&#30450;&#28304;&#20998;&#31163;&#22312;&#32467;&#26500;&#20559;&#24046;&#19979;&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#26126;&#30830;&#30340;&#36830;&#32493;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.10104</link><description>&lt;p&gt;
&#30450;&#28304;&#20998;&#31163;&#30340;&#40065;&#26834;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Robustness Analysis of Blind Source Separation. (arXiv:2303.10104v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#26694;&#26550;&#29992;&#20197;&#37327;&#21270;&#30450;&#28304;&#20998;&#31163;&#22312;&#32467;&#26500;&#20559;&#24046;&#19979;&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#26126;&#30830;&#30340;&#36830;&#32493;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30450;&#28304;&#20998;&#31163;&#65288;BSS&#65289;&#26088;&#22312;&#20174;&#20854;&#28151;&#21512;&#29289;$ X = f&#65288;S&#65289;$&#20013;&#24674;&#22797;&#26410;&#35266;&#23519;&#21040;&#30340;&#20449;&#21495;$ S $&#65292;&#20854;&#26465;&#20214;&#26159;&#20316;&#29992;&#20110;&#21464;&#25442;$ f $&#26159;&#21487;&#36870;&#30340;&#20294;&#26410;&#30693;&#30340;&#12290;&#30001;&#20110;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#20851;&#38190;&#38382;&#39064;&#26159;&#29702;&#35299;&#24403;&#25903;&#25345;&#30340;&#32479;&#35745;&#20808;&#39564;&#20551;&#35774;&#34987;&#36829;&#21453;&#26102;&#65292;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#34892;&#20026;&#22914;&#20309;&#12290;&#22312;&#32447;&#24615;&#28151;&#21512;&#30340;&#32463;&#20856;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#27492;&#31867;&#36829;&#35268;&#34892;&#20026;&#65292;&#24182;&#37327;&#21270;&#20854;&#23545;&#20174;$ X $&#20013;&#30450;&#30446;&#24674;&#22797;$ S $&#30340;&#24433;&#21709;&#12290;&#23558;$ S $&#24314;&#27169;&#20026;&#22810;&#32500;&#38543;&#26426;&#36807;&#31243;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28085;&#30422;&#21487;&#33021;&#22312;&#28151;&#21512;&#29289;$ X $&#20013;&#28508;&#22312;&#21407;&#22240;&#30340;&#31354;&#38388;&#30340;&#20449;&#24687;&#25299;&#25169;&#65292;&#24182;&#34920;&#26126;&#36890;&#29992;BSS&#35299;&#20915;&#26041;&#26696;&#23545;&#20854;&#23450;&#20041;&#32467;&#26500;&#20551;&#35774;&#30340;&#19968;&#33324;&#20559;&#24046;&#30340;&#21709;&#24212;&#34892;&#20026;&#65292;&#21487;&#20197;&#33719;&#24471;&#26126;&#30830;&#30340;&#36830;&#32493;&#24615;&#20445;&#35777;&#65292;&#36825;&#26679;&#21487;&#20197;&#28789;&#27963;&#26041;&#20415;&#22320;&#37327;&#21270;BSS&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#23427;&#22914;&#20309;&#28085;&#30422;&#24050;&#32463;&#21457;&#34920;&#30340;&#20998;&#26512;&#24182;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25200;&#21160;&#23548;&#20986;&#26032;&#30340;&#36830;&#32493;&#24615;&#20445;&#35777;&#65292;&#20174;&#32780;&#35828;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blind source separation (BSS) aims to recover an unobserved signal $S$ from its mixture $X=f(S)$ under the condition that the effecting transformation $f$ is invertible but unknown. As this is a basic problem with many practical applications, a fundamental issue is to understand how the solutions to this problem behave when their supporting statistical prior assumptions are violated. In the classical context of linear mixtures, we present a general framework for analysing such violations and quantifying their impact on the blind recovery of $S$ from $X$. Modelling $S$ as a multidimensional stochastic process, we introduce an informative topology on the space of possible causes underlying a mixture $X$, and show that the behaviour of a generic BSS-solution in response to general deviations from its defining structural assumptions can be profitably analysed in the form of explicit continuity guarantees with respect to this topology. This allows for a flexible and convenient quantificatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#27010;&#29575;&#25512;&#26029;&#26041;&#27861;&#65292;&#22522;&#20110;&#32422;&#26463;&#20256;&#36755;&#24230;&#37327;&#65292;&#21033;&#29992;&#32463;&#39564;&#20284;&#28982;&#32467;&#21512;&#20808;&#39564;&#20998;&#24067;&#65292;&#29992;&#20110;&#40065;&#26834;&#25512;&#26029;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#20013;&#24515;&#20998;&#24067;&#21442;&#25968;&#30340;&#25512;&#26029;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.10085</link><description>&lt;p&gt;
&#36890;&#36807;&#32422;&#26463;&#20256;&#36755;&#24230;&#37327;&#23454;&#29616;&#40065;&#26834;&#27010;&#29575;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Robust probabilistic inference via a constrained transport metric. (arXiv:2303.10085v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#27010;&#29575;&#25512;&#26029;&#26041;&#27861;&#65292;&#22522;&#20110;&#32422;&#26463;&#20256;&#36755;&#24230;&#37327;&#65292;&#21033;&#29992;&#32463;&#39564;&#20284;&#28982;&#32467;&#21512;&#20808;&#39564;&#20998;&#24067;&#65292;&#29992;&#20110;&#40065;&#26834;&#25512;&#26029;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#20013;&#24515;&#20998;&#24067;&#21442;&#25968;&#30340;&#25512;&#26029;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#36125;&#21494;&#26031;&#27169;&#22411;&#36890;&#24120;&#26159;&#20351;&#29992;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#19988;&#24120;&#24120;&#19981;&#21487;&#35299;&#37322;&#30340;&#21442;&#25968;&#27169;&#22411;&#30340;&#26497;&#38480;&#26500;&#24314;&#32780;&#25104;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20542;&#26012;&#30340;&#32463;&#39564;&#20284;&#28982;&#30340;&#26500;&#36896;&#65292;&#32467;&#21512;&#19968;&#31181;&#26032;&#22411;&#30340;Wasserstein&#24230;&#37327;&#65292;&#38598;&#20013;&#22312;&#29305;&#23450;&#21442;&#25968;&#26063;&#38468;&#36817;&#65292;&#28982;&#21518;&#32467;&#21512;&#27169;&#22411;&#21442;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#40065;&#26834;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#40065;&#26834;&#25512;&#26029;&#38382;&#39064;&#20013;&#25214;&#21040;&#24212;&#29992;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#23545;&#19982;&#20013;&#24515;&#20998;&#24067;&#30456;&#20851;&#30340;&#21442;&#25968;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20256;&#36755;&#24230;&#37327;&#20855;&#26377;&#24456;&#39640;&#30340;&#35745;&#31639;&#31616;&#20415;&#24615;&#65292;&#21033;&#29992;&#20102;&#31163;&#25955;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;Sinkhorn&#27491;&#21017;&#21270;&#65292;&#24182;&#26412;&#36136;&#19978;&#21487;&#20197;&#24182;&#34892;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible Bayesian models are typically constructed using limits of large parametric models with a multitude of parameters that are often uninterpretable. In this article, we offer a novel alternative by constructing an exponentially tilted empirical likelihood carefully designed to concentrate near a parametric family of distributions of choice with respect to a novel variant of the Wasserstein metric, which is then combined with a prior distribution on model parameters to obtain a robustified posterior. The proposed approach finds applications in a wide variety of robust inference problems, where we intend to perform inference on the parameters associated with the centering distribution in presence of outliers. Our proposed transport metric enjoys great computational simplicity, exploiting the Sinkhorn regularization for discrete optimal transport problems, and being inherently parallelizable. We demonstrate superior performance of our methodology when compared against state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#20803;&#27010;&#29575;CRPS&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26085;&#21069;&#30005;&#20215;&#39044;&#27979;&#20013;&#65292;&#30456;&#27604;&#20110;&#32479;&#19968;&#32452;&#21512;&#22312;CRPS&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.10019</link><description>&lt;p&gt;
&#22810;&#20803;&#27010;&#29575;CRPS&#23398;&#20064;&#21450;&#20854;&#22312;&#26085;&#21069;&#30005;&#20215;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices. (arXiv:2303.10019v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#20803;&#27010;&#29575;CRPS&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26085;&#21069;&#30005;&#20215;&#39044;&#27979;&#20013;&#65292;&#30456;&#27604;&#20110;&#32479;&#19968;&#32452;&#21512;&#22312;CRPS&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20998;&#20301;&#25968;&#21644;&#21327;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#20803;&#27010;&#29575;&#39044;&#27979;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24179;&#28369;&#36807;&#31243;&#20801;&#35768;&#22312;&#32447;&#23398;&#20064;&#12290;&#36890;&#36807;&#32500;&#25968;&#38477;&#20302;&#21644;&#32602;&#20989;&#25968;&#24179;&#28369;&#31561;&#20004;&#31181;&#24179;&#28369;&#26041;&#27861;&#26469;&#23558;&#26631;&#20934;CRPS&#23398;&#20064;&#26694;&#26550;&#25512;&#24191;&#21040;&#22810;&#20803;&#32500;&#24230;&#20013;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#39044;&#27979;&#26085;&#21069;&#30005;&#20215;&#65292;&#30456;&#27604;&#20110;&#32479;&#19968;&#32452;&#21512;&#65292;&#22312;CRPS&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new method for combining (or aggregating or ensembling) multivariate probabilistic forecasts, taking into account dependencies between quantiles and covariates through a smoothing procedure that allows for online learning. Two smoothing methods are discussed: dimensionality reduction using Basis matrices and penalized smoothing. The new online learning algorithm generalizes the standard CRPS learning framework into multivariate dimensions. It is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic learning properties. We provide an in-depth discussion on possible extensions of the algorithm and several nested cases related to the existing literature on online forecast combination. The methodology is applied to forecasting day-ahead electricity prices, which are 24-dimensional distributional forecasts. The proposed method yields significant improvements over uniform combination in terms of continuous ranked probability score (CRPS). We discuss 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#20808;&#39564;&#38543;&#26426;&#22359;&#27169;&#22411;&#65292;&#23558;&#31038;&#21306;&#24314;&#27169;&#20026;&#30001;&#33410;&#28857;&#23646;&#24615;&#20915;&#23450;&#12290;&#22522;&#20110;&#32622;&#20449;&#20256;&#36882;&#21644;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#30340;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#31038;&#20132;&#32593;&#32476;&#12289;&#22270;&#20687;&#20998;&#21106;&#65292;&#29983;&#29289;&#29289;&#31181;&#31561;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.09995</link><description>&lt;p&gt;
&#31070;&#32463;&#20808;&#39564;&#38543;&#26426;&#22359;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural-prior stochastic block model. (arXiv:2303.09995v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#20808;&#39564;&#38543;&#26426;&#22359;&#27169;&#22411;&#65292;&#23558;&#31038;&#21306;&#24314;&#27169;&#20026;&#30001;&#33410;&#28857;&#23646;&#24615;&#20915;&#23450;&#12290;&#22522;&#20110;&#32622;&#20449;&#20256;&#36882;&#21644;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#30340;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#31038;&#20132;&#32593;&#32476;&#12289;&#22270;&#20687;&#20998;&#21106;&#65292;&#29983;&#29289;&#29289;&#31181;&#31561;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#34987;&#24191;&#27867;&#30740;&#31350;&#20316;&#20026;&#22270;&#32858;&#31867;&#25110;&#31038;&#21306;&#26816;&#27979;&#30340;&#22522;&#20934;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22270;&#25968;&#25454;&#36890;&#24120;&#24102;&#26377;&#33410;&#28857;&#23646;&#24615;&#65292;&#36825;&#20123;&#23646;&#24615;&#25215;&#36733;&#26377;&#20851;&#31038;&#21306;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#33410;&#28857;&#23646;&#24615;&#26159;&#30001;&#33410;&#28857;&#31038;&#21306;&#25104;&#21592;&#36523;&#20221;&#29983;&#25104;&#30340;&#26469;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20808;&#39564;&#30340;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#20013;&#19968;&#31995;&#21015;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#31038;&#21306;&#24314;&#27169;&#20026;&#30001;&#33410;&#28857;&#23646;&#24615;&#20915;&#23450;&#32780;&#19981;&#26159;&#30456;&#21453;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#30456;&#24212;&#30340;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#31216;&#20026;&#31070;&#32463;&#20808;&#39564;SBM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#26469;&#33258;&#20110;&#32479;&#35745;&#29289;&#29702;&#23398;&#65292;&#22522;&#20110;&#32622;&#20449;&#20256;&#36882;&#21644;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#20197;&#21450;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#33021;&#12290;&#25105;&#20204;&#35782;&#21035;&#20102;&#21487;&#26816;&#27979;&#21644;&#31934;&#30830;&#24674;&#22797;&#30456;&#21464;&#65292;&#20197;&#21450;&#19968;&#31181;&#31639;&#27861;&#38590;&#21306;&#22495;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21644;&#31639;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#35832;&#22914;&#31038;&#20132;&#32593;&#32476;&#12289;&#22270;&#20687;&#20998;&#21106;&#65292;&#29983;&#29289;&#29289;&#31181;&#31561;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic block model (SBM) is widely studied as a benchmark for graph clustering aka community detection. In practice, graph data often come with node attributes that bear additional information about the communities. Previous works modeled such data by considering that the node attributes are generated from the node community memberships. In this work, motivated by a recent surge of works in signal processing using deep neural networks as priors, we propose to model the communities as being determined by the node attributes rather than the opposite. We define the corresponding model; we call it the neural-prior SBM. We propose an algorithm, stemming from statistical physics, based on a combination of belief propagation and approximate message passing. We analyze the performance of the algorithm as well as the Bayes-optimal performance. We identify detectability and exact recovery phase transitions, as well as an algorithmically hard region. The proposed model and algorithm can b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#23398;&#20064;&#25298;&#32477;&#8221;&#26694;&#26550;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#40664;&#40664;&#22833;&#36133;&#38382;&#39064;&#12290;&#36890;&#36807;&#39044;&#27979;&#21487;&#20449;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#25509;&#21463;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#33021;&#21147;&#21306;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#23398;&#20064;&#34920;&#31034;&#34913;&#37327;&#26080;&#33021;&#65292;&#22686;&#21152;&#26080;&#33021;&#24471;&#20998;&#20250;&#39044;&#31034;&#30528;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09989</link><description>&lt;p&gt;
&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#25214;&#21040;&#33021;&#21147;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Finding Competence Regions in Domain Generalization. (arXiv:2303.09989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09989
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#23398;&#20064;&#25298;&#32477;&#8221;&#26694;&#26550;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#40664;&#40664;&#22833;&#36133;&#38382;&#39064;&#12290;&#36890;&#36807;&#39044;&#27979;&#21487;&#20449;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#25509;&#21463;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#33021;&#21147;&#21306;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#23398;&#20064;&#34920;&#31034;&#34913;&#37327;&#26080;&#33021;&#65292;&#22686;&#21152;&#26080;&#33021;&#24471;&#20998;&#20250;&#39044;&#31034;&#30528;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#23398;&#20064;&#25298;&#32477;&#8221;&#26694;&#26550;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#20013;&#40664;&#40664;&#22833;&#36133;&#30340;&#38382;&#39064;&#65292;&#21363;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#20551;&#35774;&#26377;&#19968;&#20010;&#28201;&#21644;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27169;&#22411;&#20272;&#35745;&#30340;&#33021;&#21147;&#39044;&#31034;&#30528;&#21487;&#20449;&#21709;&#24212;&#26102;&#25509;&#21463;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#25298;&#32477;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#21487;&#20449;&#24230;&#36890;&#36807;&#19982;&#20998;&#31867;&#22120;&#24615;&#33021;&#23494;&#20999;&#30456;&#20851;&#30340;&#20195;&#29702;&#26080;&#33021;&#20998;&#25968;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23545;&#20998;&#31867;&#30340;&#26080;&#33021;&#24471;&#20998;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#24378;&#35843;&#20102;&#25298;&#32477;&#29575;&#19982;&#20934;&#30830;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;&#26631;&#20934;&#39046;&#22495;&#27867;&#21270;&#22522;&#20934;&#65292;&#24182;&#32771;&#34385;&#22312;&#38381;&#21512;&#21644;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#19979;&#36890;&#36807;&#19981;&#21516;&#30340;&#23398;&#20064;&#34920;&#31034;&#26469;&#34913;&#37327;&#26080;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#26080;&#33021;&#20998;&#25968;&#30830;&#23454;&#39044;&#31034;&#30528;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#26174;&#30528;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We propose a "learning to reject" framework to address the problem of silent failures in Domain Generalization (DG), where the test distribution differs from the training distribution. Assuming a mild distribution shift, we wish to accept out-of-distribution (OOD) data whenever a model's estimated competence foresees trustworthy responses, instead of rejecting OOD data outright. Trustworthiness is then predicted via a proxy incompetence score that is tightly linked to the performance of a classifier. We present a comprehensive experimental evaluation of incompetence scores for classification and highlight the resulting trade-offs between rejection rate and accuracy gain. For comparability with prior work, we focus on standard DG benchmarks and consider the effect of measuring incompetence via different learned representations in a closed versus an open world setting. Our results suggest that increasing incompetence scores are indeed predictive of reduced accuracy, leading to significan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;DeepMVC&#65292;&#29992;&#20110;&#28145;&#24230;&#22810;&#35270;&#35282;&#32858;&#31867;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20851;&#38190;&#35266;&#23519;&#65292;&#21457;&#29616;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#34920;&#31034;&#20250;&#23545;&#32858;&#31867;&#21487;&#20998;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35282;&#25968;&#37327;&#36739;&#22810;&#26102;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#32570;&#38519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20123;&#26032;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;DeepMVC&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2303.09877</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#22810;&#35270;&#35282;&#32858;&#31867;&#20013;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#23545;&#27604;&#23545;&#40784;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Effects of Self-supervision and Contrastive Alignment in Deep Multi-view Clustering. (arXiv:2303.09877v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;DeepMVC&#65292;&#29992;&#20110;&#28145;&#24230;&#22810;&#35270;&#35282;&#32858;&#31867;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20851;&#38190;&#35266;&#23519;&#65292;&#21457;&#29616;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#34920;&#31034;&#20250;&#23545;&#32858;&#31867;&#21487;&#20998;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35282;&#25968;&#37327;&#36739;&#22810;&#26102;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#32570;&#38519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20123;&#26032;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;DeepMVC&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#36817;&#24180;&#26469;&#28145;&#24230;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#20013;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28145;&#24230;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#22312;&#21457;&#23637;&#26041;&#38754;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#65292;&#36825;&#21487;&#33021;&#20250;&#25302;&#24930;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;DeepMVC&#65292;&#29992;&#20110;&#28145;&#24230;&#22810;&#35270;&#35282;&#32858;&#31867;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#26368;&#36817;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24433;&#21709;&#36827;&#34892;&#20851;&#38190;&#35266;&#23519;&#65292;&#29305;&#21035;&#26159;&#23545;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#34920;&#31034;&#30340;&#32570;&#28857;&#30340;&#35266;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#27604;&#23545;&#40784;&#21487;&#33021;&#20250;&#23545;&#32858;&#31867;&#21487;&#20998;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;&#24403;&#35270;&#35282;&#30340;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#24433;&#21709;&#20250;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#21457;&#29616;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#26032;&#30340;DeepMVC&#23454;&#20363;&#65292;&#20855;&#26377;&#26032;&#24418;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#65288;i&#65289;&#19982;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#19968;&#33268;&#65292;&#23545;&#27604;&#23545;&#40784;&#20250;&#38477;&#20302;&#20855;&#26377;&#35768;&#22810;&#35270;&#35282;&#30340;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65307;&#65288;ii&#65289;&#25152;&#26377;&#26041;&#27861;&#37117;&#21463;&#30410;&#20110;&#19968;&#23450;&#31243;&#24230;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning is a central component in recent approaches to deep multi-view clustering (MVC). However, we find large variations in the development of self-supervision-based methods for deep MVC, potentially slowing the progress of the field. To address this, we present DeepMVC, a unified framework for deep MVC that includes many recent methods as instances. We leverage our framework to make key observations about the effect of self-supervision, and in particular, drawbacks of aligning representations with contrastive learning. Further, we prove that contrastive alignment can negatively influence cluster separability, and that this effect becomes worse when the number of views increases. Motivated by our findings, we develop several new DeepMVC instances with new forms of self-supervision. We conduct extensive experiments and find that (i) in line with our theoretical findings, contrastive alignments decreases performance on datasets with many views; (ii) all methods benefit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#28145;&#24230;&#38750;&#21442;&#25968;&#20272;&#35745;&#20869;&#37096;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24191;&#20041;&#35823;&#24046;&#20445;&#35777;&#21644;&#21435;&#22122;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.09863</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20869;&#37096;&#25968;&#25454;&#32467;&#26500;&#30340;&#28145;&#24230;&#38750;&#21442;&#25968;&#20272;&#35745;&#65306;&#24191;&#20041;&#35823;&#24046;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Nonparametric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness. (arXiv:2303.09863v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#28145;&#24230;&#38750;&#21442;&#25968;&#20272;&#35745;&#20869;&#37096;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24191;&#20041;&#35823;&#24046;&#20445;&#35777;&#21644;&#21435;&#22122;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#22312;&#23398;&#20064;&#39640;&#32500;&#25968;&#25454;&#30340;&#20302;&#32500;&#28508;&#22312;&#29305;&#24449;&#26041;&#38754;&#24050;&#32463;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#20551;&#35774;&#25968;&#25454;&#22312;&#20302;&#32500;&#27969;&#24418;&#38468;&#36817;&#37319;&#26679;&#65292;&#25105;&#20204;&#37319;&#29992;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#25968;&#25454;&#32534;&#30721;&#20026;&#19968;&#32452;&#22270;&#34920;&#19978;&#30340;&#20302;&#32500;&#28508;&#22312;&#29305;&#24449;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20026;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#30340;&#24191;&#20041;&#35823;&#24046;&#24314;&#31435;&#20102;&#32479;&#35745;&#20445;&#35777;&#65292;&#24182;&#19988;&#36890;&#36807;&#32771;&#34385;$d$&#32500;&#27969;&#24418;&#19978;$n$&#20010;&#24102;&#22122;&#22768;&#35757;&#32451;&#26679;&#26412;&#21450;&#20854;&#26080;&#22122;&#22768;&#23545;&#24212;&#29289;&#26469;&#23637;&#31034;&#23427;&#20204;&#30340;&#21435;&#22122;&#33021;&#21147;&#12290;&#36890;&#36807;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#26377;&#25928;&#22320;&#21435;&#22122;&#36755;&#20837;&#25968;&#25454;&#21644;&#27491;&#24577;&#20998;&#24067;&#22122;&#22768;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#32593;&#32476;&#26550;&#26500;&#19979;&#65292;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#19968;&#20010;&#22823;&#33268;&#20026;$\displaystyle n^{-\frac{2}{d+2}}\log^4 n$&#38454;&#30340;&#24179;&#26041;&#24191;&#20041;&#35823;&#24046;&#65292;&#35813;&#35823;&#24046;&#21462;&#20915;&#20110;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#24182;&#19988;&#20165;&#24369;&#20381;&#36182;&#20110;&#26679;&#26412;&#25968;&#37327;$n$&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoencoders have demonstrated remarkable success in learning low-dimensional latent features of high-dimensional data across various applications. Assuming that data are sampled near a low-dimensional manifold, we employ chart autoencoders, which encode data into low-dimensional latent features on a collection of charts, preserving the topology and geometry of the data manifold. Our paper establishes statistical guarantees on the generalization error of chart autoencoders, and we demonstrate their denoising capabilities by considering $n$ noisy training samples, along with their noise-free counterparts, on a $d$-dimensional manifold. By training autoencoders, we show that chart autoencoders can effectively denoise the input data with normal noise. We prove that, under proper network architectures, chart autoencoders achieve a squared generalization error in the order of $\displaystyle n^{-\frac{2}{d+2}}\log^4 n$, which depends on the intrinsic dimension of the manifold and only weakly
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36229;&#21442;&#25968;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#21487;&#38752;&#22320;&#37327;&#21270;&#20272;&#35745;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30495;&#23454;&#36229;&#21442;&#25968;&#30340;&#39640;&#27010;&#29575;&#38598;&#21512;&#26469;&#33719;&#24471;&#36793;&#30028;&#65292;&#24182;&#22312;&#20854;&#20013;&#25214;&#21040;&#26368;&#22351;&#30340;&#21518;&#39564;&#21327;&#26041;&#24046;&#12290;&#35813;&#26041;&#27861;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.09842</link><description>&lt;p&gt;
&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#22522;&#20110;&#26680;&#30340;&#32447;&#24615;&#31995;&#32479;&#36776;&#35782;&#30340;&#35823;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Error Bounds for Kernel-Based Linear System Identification with Unknown Hyperparameters. (arXiv:2303.09842v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36229;&#21442;&#25968;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#21487;&#38752;&#22320;&#37327;&#21270;&#20272;&#35745;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30495;&#23454;&#36229;&#21442;&#25968;&#30340;&#39640;&#27010;&#29575;&#38598;&#21512;&#26469;&#33719;&#24471;&#36793;&#30028;&#65292;&#24182;&#22312;&#20854;&#20013;&#25214;&#21040;&#26368;&#22351;&#30340;&#21518;&#39564;&#21327;&#26041;&#24046;&#12290;&#35813;&#26041;&#27861;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20351;&#29992;&#31283;&#23450;&#26680;&#35774;&#35745;&#30340;&#32447;&#24615;&#31995;&#32479;&#36776;&#35782;&#20013;&#12290;&#20174;&#39640;&#26031;&#36807;&#31243;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21518;&#39564;&#21327;&#26041;&#24046;&#20026;&#35782;&#21035;&#30340;&#27169;&#22411;&#33258;&#21160;&#25552;&#20379;&#27010;&#29575;&#35823;&#24046;&#30028;&#38480;&#65292;&#36825;&#22312;&#40065;&#26834;&#21644;&#38543;&#26426;&#25511;&#21046;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35823;&#24046;&#30028;&#38480;&#38656;&#35201;&#20869;&#26680;&#35774;&#35745;&#20013;&#30495;&#23454;&#36229;&#21442;&#25968;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#30340;&#36229;&#21442;&#25968;&#19981;&#20934;&#30830;&#25110;&#22312;&#23384;&#22312;&#39640;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#34987;&#35777;&#26126;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#36229;&#21442;&#25968;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#21487;&#38752;&#22320;&#37327;&#21270;&#20272;&#35745;&#35823;&#24046;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#36793;&#30028;&#26159;&#36890;&#36807;&#39318;&#20808;&#20174;&#36793;&#38469;&#20284;&#28982;&#20989;&#25968;&#26500;&#24314;&#30495;&#23454;&#36229;&#21442;&#25968;&#30340;&#39640;&#27010;&#29575;&#38598;&#21512;&#65292;&#28982;&#21518;&#22312;&#35813;&#38598;&#21512;&#20869;&#25214;&#21040;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21518;&#39564;&#21327;&#26041;&#24046;&#32780;&#33719;&#24471;&#30340;&#12290;&#25152;&#25552;&#20986;&#30340;&#36793;&#30028;&#34987;&#35777;&#26126;&#20855;&#26377;&#39640;&#27010;&#29575;&#21253;&#21547;&#30495;&#23454;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The kernel-based method has been successfully applied in linear system identification using stable kernel designs. From a Gaussian process perspective, it automatically provides probabilistic error bounds for the identified models from the posterior covariance, which are useful in robust and stochastic control. However, the error bounds require knowledge of the true hyperparameters in the kernel design and are demonstrated to be inaccurate with estimated hyperparameters for lightly damped systems or in the presence of high noise. In this work, we provide reliable quantification of the estimation error when the hyperparameters are unknown. The bounds are obtained by first constructing a high-probability set for the true hyperparameters from the marginal likelihood function and then finding the worst-case posterior covariance within the set. The proposed bound is proven to contain the true model with a high probability and its validity is verified in numerical simulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#39640;&#25928;&#30340;&#25209;&#37327;&#26356;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20803;&#26641;&#19978;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.09705</link><description>&lt;p&gt;
&#22312;&#20803;&#26641;&#19978;&#25209;&#37327;&#26356;&#26032;&#21518;&#39564;&#26641;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Updating of a Posterior Tree Distribution over a Meta-Tree. (arXiv:2303.09705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#39640;&#25928;&#30340;&#25209;&#37327;&#26356;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20803;&#26641;&#19978;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#21069;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#19981;&#21487;&#35266;&#23519;&#30340;&#26641;&#21644;&#19968;&#20010;&#24207;&#21015;&#26356;&#26032;&#26041;&#27861;&#34920;&#31034;&#30340;&#27010;&#29575;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#19968;&#32452;&#26641;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#35813;&#38598;&#21512;&#31216;&#20026;&#20803;&#26641;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#25209;&#37327;&#26356;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previously, we proposed a probabilistic data generation model represented by an unobservable tree and a sequential updating method to calculate a posterior distribution over a set of trees. The set is called a meta-tree. In this paper, we propose a more efficient batch updating method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65288;DALUPI&#65289;&#31639;&#27861;&#65292;&#20197;&#22312;&#23398;&#20064;&#20013;&#25918;&#23485;&#20551;&#35774;&#26465;&#20214;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#20943;&#23569;&#38169;&#35823;&#26469;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.09350</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation by learning using privileged information. (arXiv:2303.09350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65288;DALUPI&#65289;&#31639;&#27861;&#65292;&#20197;&#22312;&#23398;&#20064;&#20013;&#25918;&#23485;&#20551;&#35774;&#26465;&#20214;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#20943;&#23569;&#38169;&#35823;&#26469;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#21482;&#22312;&#24378;&#20551;&#35774;&#26465;&#20214;&#19979;&#24471;&#20197;&#23454;&#29616;&#65292;&#22914;&#21327;&#21464;&#37327;&#31227;&#20301;&#21644;&#36755;&#20837;&#39046;&#22495;&#20043;&#38388;&#30340;&#37325;&#21472;&#12290;&#21518;&#32773;&#22312;&#39640;&#32500;&#24212;&#29992;&#20013;&#32463;&#24120;&#34987;&#36829;&#21453;&#65292;&#27604;&#22914;&#22270;&#20687;&#20998;&#31867;&#65292;&#22312;&#38754;&#23545;&#36825;&#31181;&#25361;&#25112;&#26102;&#65292;&#22270;&#20687;&#20998;&#31867;&#20173;&#28982;&#26159;&#31639;&#27861;&#24320;&#21457;&#30340;&#28789;&#24863;&#21644;&#22522;&#20934;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#33719;&#21462;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#26679;&#26412;&#30340;&#26377;&#20851;&#20449;&#24687;&#33021;&#22815;&#24110;&#21161;&#25918;&#23485;&#36825;&#20123;&#20551;&#35774;&#65292;&#24182;&#22312;&#23398;&#20064;&#20013;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#20195;&#20215;&#26159;&#25910;&#38598;&#26356;&#20016;&#23500;&#30340;&#21464;&#37327;&#38598;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65288;DALUPI&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#30340;&#23454;&#29992;&#31471;&#21040;&#31471;&#31639;&#27861;&#65292;&#21463;&#21040;&#25105;&#20204;&#20998;&#26512;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#29305;&#26435;&#20449;&#24687;&#21487;&#20197;&#20943;&#23569;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful unsupervised domain adaptation (UDA) is guaranteed only under strong assumptions such as covariate shift and overlap between input domains. The latter is often violated in high-dimensional applications such as image classification which, despite this challenge, continues to serve as inspiration and benchmark for algorithm development. In this work, we show that access to side information about examples from the source and target domains can help relax these assumptions and increase sample efficiency in learning, at the cost of collecting a richer variable set. We call this domain adaptation by learning using privileged information (DALUPI). Tailored for this task, we propose a simple two-stage learning algorithm inspired by our analysis and a practical end-to-end algorithm for multi-label image classification. In a suite of experiments, including an application to medical image analysis, we demonstrate that incorporating privileged information in learning can reduce errors i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#31215;&#20998;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26550;&#26500;&#20284;&#28982;&#34920;&#38754;&#26377;&#20998;&#25955;&#12289;&#29421;&#31364;&#23792;&#26102;&#26500;&#24314;&#21152;&#26435;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#27604;&#24403;&#21069;&#21516;&#31867;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#20284;&#28982;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#26356;&#20026;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2303.08874</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#31215;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Bayesian Quadrature for Neural Ensemble Search. (arXiv:2303.08874v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#31215;&#20998;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26550;&#26500;&#20284;&#28982;&#34920;&#38754;&#26377;&#20998;&#25955;&#12289;&#29421;&#31364;&#23792;&#26102;&#26500;&#24314;&#21152;&#26435;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#27604;&#24403;&#21069;&#21516;&#31867;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#20284;&#28982;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#26356;&#20026;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#26550;&#26500;&#20284;&#28982;&#34920;&#38754;&#26377;&#20998;&#25955;&#12289;&#29421;&#31364;&#23792;&#26102;&#25928;&#26524;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#26500;&#24314;&#22343;&#31561;&#21152;&#26435;&#30340;&#38598;&#25104;&#65292;&#36825;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#36739;&#24369;&#26550;&#26500;&#30340;&#22833;&#25928;&#27169;&#24335;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#38598;&#25104;&#35270;&#20026;&#36817;&#20284;&#36793;&#32536;&#21270;&#26550;&#26500;&#65292;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#31215;&#20998;&#30340;&#24037;&#20855;&#26500;&#24314;&#38598;&#25104;&#26041;&#27861;&#8212;&#8212;&#36825;&#20123;&#24037;&#20855;&#38750;&#24120;&#36866;&#21512;&#25506;&#32034;&#26550;&#26500;&#20284;&#28982;&#34920;&#38754;&#26377;&#20998;&#25955;&#12289;&#29421;&#31364;&#23792;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#38598;&#25104;&#30001;&#20307;&#29616;&#20854;&#24615;&#33021;&#30340;&#26550;&#26500;&#21152;&#26435;&#26435;&#37325;&#32452;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#8212;&#8212;&#22312;&#27979;&#35797;&#20284;&#28982;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#8212;&#8212;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#65292;&#24182;&#36890;&#36807;&#21066;&#20943;&#30740;&#31350;&#39564;&#35777;&#20854;&#21508;&#25104;&#20998;&#30340;&#29420;&#31435;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensembling can improve the performance of Neural Networks, but existing approaches struggle when the architecture likelihood surface has dispersed, narrow peaks. Furthermore, existing methods construct equally weighted ensembles, and this is likely to be vulnerable to the failure modes of the weaker architectures. By viewing ensembling as approximately marginalising over architectures we construct ensembles using the tools of Bayesian Quadrature -tools which are well suited to the exploration of likelihood surfaces with dispersed, narrow peaks. Additionally, the resulting ensembles consist of architectures weighted commensurate with their performance. We show empirically -- in terms of test likelihood, accuracy, and expected calibration error -that our method outperforms state-of-the-art baselines, and verify via ablation studies that its components do so independently.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20114;&#20449;&#24687;&#32422;&#26463;&#19979;&#30340;&#23545;&#27604;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30001;&#19968;&#20010;&#25552;&#21462;&#39118;&#26684;&#26080;&#20851;&#29305;&#24449;&#30340;&#23545;&#27604;&#23398;&#20064;&#37096;&#20998;&#21644;&#19968;&#20010;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;CVAE&#37096;&#20998;&#32452;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.08068</link><description>&lt;p&gt;
&#20351;&#29992;&#20114;&#20449;&#24687;&#32422;&#26463;&#19979;&#30340;&#23545;&#27604;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#39118;&#26684;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Style Feature Extraction Using Contrastive Conditioned Variational Autoencoders with Mutual Information Constraints. (arXiv:2303.08068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20114;&#20449;&#24687;&#32422;&#26463;&#19979;&#30340;&#23545;&#27604;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30001;&#19968;&#20010;&#25552;&#21462;&#39118;&#26684;&#26080;&#20851;&#29305;&#24449;&#30340;&#23545;&#27604;&#23398;&#20064;&#37096;&#20998;&#21644;&#19968;&#20010;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;CVAE&#37096;&#20998;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#29305;&#24449;&#65288;&#22914;&#39118;&#26684;&#65289;&#38750;&#24120;&#37325;&#35201;&#12290;&#26080;&#30417;&#30563;&#26041;&#27861;&#65288;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65289;&#21487;&#20197;&#25552;&#21462;&#39118;&#26684;&#65292;&#20294;&#25552;&#21462;&#30340;&#39118;&#26684;&#36890;&#24120;&#19982;&#20854;&#20182;&#29305;&#24449;&#28151;&#21512;&#12290;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20998;&#31867;&#26631;&#31614;&#26469;&#25351;&#23548;VAEs&#25552;&#21462;&#39118;&#26684;&#65292;&#21363;&#26465;&#20214;VAEs&#65288;CVAEs&#65289;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20165;&#25552;&#21462;&#39118;&#26684;&#30340;&#26041;&#27861;&#23578;&#26410;&#24314;&#31435;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;CVAE&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20165;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#26469;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22823;&#33268;&#30001;&#20004;&#20010;&#24182;&#34892;&#37096;&#20998;&#32452;&#25104;; &#25552;&#21462;&#39118;&#26684;&#26080;&#20851;&#29305;&#24449;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#37096;&#20998;&#65292;&#20197;&#21450;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;CVAE&#37096;&#20998;&#12290;CL&#27169;&#22411;&#36890;&#24120;&#20197;&#26080;&#38656;&#25968;&#25454;&#25193;&#20805;&#30340;&#33258;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#19982;&#26679;&#24335;&#26080;&#20851;&#30340;&#34920;&#31034;&#65292;&#21487;&#20197;&#35270;&#20026;&#26679;&#24335;&#20013;&#30340;&#25200;&#21160;&#12290;&#20197;&#25552;&#21462;&#30340;&#39118;&#26684;&#26080;&#20851;&#29305;&#24449;&#20026;&#26465;&#20214;&#65292;CVAE&#23398;&#20064;&#20165;&#25552;&#21462;&#39118;&#26684;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20808;&#35757;&#32451;CL&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#35757;&#32451;&#36807;&#30340;CL&#27169;&#22411;&#25351;&#23548;CVAE&#30340;&#35757;&#32451;&#12290;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is crucial to extract fine-grained features such as styles from unlabeled data in data analysis. Unsupervised methods, such as variational autoencoders (VAEs), can extract styles, but the extracted styles are usually mixed with other features. We can isolate the styles using VAEs conditioned by class labels, known as conditional VAEs (CVAEs). However, methods to extract only styles using unlabeled data are not established. In this paper, we construct a CVAE-based method that extracts style features using only unlabeled data. The proposed model roughly consists of two parallel parts; a contrastive learning (CL) part that extracts style-independent features and a CVAE part that extracts style features. CL models generally learn representations independent of data augmentation, which can be seen as a perturbation in styles, in a self-supervised way. Taking the style-independent features as a condition, the CVAE learns to extract only styles. In the training procedure, a CL model is tra
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#31227;&#21160;&#20581;&#24247;&#24212;&#29992;&#21644;&#26426;&#22120;&#23398;&#20064;&#33258;&#36866;&#24212;&#24178;&#39044;&#26469;&#21152;&#24378;&#30111;&#30142;&#30417;&#27979;&#21644;&#27835;&#30103;&#20381;&#20174;&#24615;&#65292;&#25913;&#21892;&#21307;&#25252;&#36136;&#37327;&#65292;&#25552;&#39640;&#26816;&#27979;&#29575;&#21644;&#20844;&#20849;&#21355;&#29983;&#65292;&#20943;&#23569;&#33647;&#21697;&#30701;&#32570;&#21644;&#20026;&#25919;&#31574;&#24178;&#39044;&#25552;&#20379;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.02075</link><description>&lt;p&gt;
&#20840;&#29699;&#20581;&#24247;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#65306;&#20197;&#30111;&#30142;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adaptive Interventions for Global Health: A Case Study of Malaria. (arXiv:2303.02075v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02075
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#31227;&#21160;&#20581;&#24247;&#24212;&#29992;&#21644;&#26426;&#22120;&#23398;&#20064;&#33258;&#36866;&#24212;&#24178;&#39044;&#26469;&#21152;&#24378;&#30111;&#30142;&#30417;&#27979;&#21644;&#27835;&#30103;&#20381;&#20174;&#24615;&#65292;&#25913;&#21892;&#21307;&#25252;&#36136;&#37327;&#65292;&#25552;&#39640;&#26816;&#27979;&#29575;&#21644;&#20844;&#20849;&#21355;&#29983;&#65292;&#20943;&#23569;&#33647;&#21697;&#30701;&#32570;&#21644;&#20026;&#25919;&#31574;&#24178;&#39044;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30111;&#30142;&#26159;&#21487;&#20197;&#39044;&#38450;&#12289;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#30142;&#30149;&#65307;&#20294;&#27599;&#24180;&#20173;&#26377;&#36229;&#36807;&#20004;&#20159;&#20010;&#30149;&#20363;&#21644;&#20004;&#19975;&#20010;&#21487;&#39044;&#38450;&#27515;&#20129;&#12290;&#23588;&#20854;&#22312;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#30340;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#65292;&#30111;&#30142;&#20173;&#28982;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#31227;&#21160;&#20581;&#24247;&#24212;&#29992;&#12289;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#65292;&#21487;&#20197;&#21152;&#24378;&#30111;&#30142;&#30417;&#27979;&#21644;&#27835;&#30103;&#30340;&#20381;&#20174;&#24615;&#65292;&#22686;&#21152;&#26816;&#27979;&#65292;&#34913;&#37327;&#25552;&#20379;&#32773;&#30340;&#25216;&#33021;&#21644;&#25252;&#29702;&#36136;&#37327;&#65292;&#36890;&#36807;&#25903;&#25345;&#19968;&#32447;&#24037;&#20316;&#20154;&#21592;&#21644;&#24739;&#32773;&#65288;&#22914;&#23481;&#37327;&#24314;&#35774;&#21644;&#40723;&#21169;&#34892;&#20026;&#21464;&#21270;&#65292;&#22914;&#20351;&#29992;&#34442;&#24080;&#65289;&#25913;&#21892;&#20844;&#20849;&#21355;&#29983;&#65292;&#20943;&#23569;&#33647;&#24215;&#21644;&#35786;&#25152;&#30340;&#27979;&#35797;&#24211;&#23384;&#30701;&#32570;&#24182;&#20026;&#25919;&#31574;&#24178;&#39044;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malaria can be prevented, diagnosed, and treated; however, every year, there are more than 200 million cases and 200.000 preventable deaths. Malaria remains a pressing public health concern in low- and middle-income countries, especially in sub-Saharan Africa. We describe how by means of mobile health applications, machine-learning-based adaptive interventions can strengthen malaria surveillance and treatment adherence, increase testing, measure provider skills and quality of care, improve public health by supporting front-line workers and patients (e.g., by capacity building and encouraging behavioral changes, like using bed nets), reduce test stockouts in pharmacies and clinics and informing public health for policy intervention.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#32463;&#39564;&#39118;&#38505;&#30340;&#19979;&#38477;&#36895;&#29575;&#26159;&#38750;&#21333;&#35843;&#30340;&#12290;&#22312;&#20998;&#24067;&#31526;&#21512;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#39640;&#32500;&#23485;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#29575;&#21442;&#25968;&#21270;&#28165;&#26224;&#30340;&#38454;&#27573;&#36716;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#20026;&#26089;&#26399;&#23398;&#20064;&#26102;&#25152;&#23398;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.00055</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#26102;&#38388;&#23610;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning time-scales in two-layers neural networks. (arXiv:2303.00055v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#32463;&#39564;&#39118;&#38505;&#30340;&#19979;&#38477;&#36895;&#29575;&#26159;&#38750;&#21333;&#35843;&#30340;&#12290;&#22312;&#20998;&#24067;&#31526;&#21512;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#39640;&#32500;&#23485;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#29575;&#21442;&#25968;&#21270;&#28165;&#26224;&#30340;&#38454;&#27573;&#36716;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#20026;&#26089;&#26399;&#23398;&#20064;&#26102;&#25152;&#23398;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20855;&#26377;&#22810;&#20010;&#24341;&#20154;&#27880;&#24847;&#30340;&#29305;&#28857;&#12290;&#23588;&#20854;&#26159;&#65292;&#22312;&#22823;&#25209;&#37327;&#25968;&#25454;&#24179;&#22343;&#21518;&#65292;&#32463;&#39564;&#39118;&#38505;&#30340;&#19979;&#38477;&#36895;&#29575;&#26159;&#38750;&#21333;&#35843;&#30340;&#12290;&#20960;&#20046;&#27809;&#26377;&#36827;&#23637;&#30340;&#38271;&#21608;&#26399;&#21644;&#24555;&#36895;&#19979;&#38477;&#30340;&#38388;&#38548;&#20132;&#26367;&#20986;&#29616;&#12290;&#36825;&#20123;&#36830;&#32493;&#30340;&#23398;&#20064;&#38454;&#27573;&#24448;&#24448;&#22312;&#38750;&#24120;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#12290;&#26368;&#21518;&#65292;&#22312;&#26089;&#26399;&#38454;&#27573;&#23398;&#20064;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#8220;&#31616;&#21333;&#30340;&#8221;&#25110;&#8220;&#26131;&#20110;&#23398;&#20064;&#30340;&#8221;&#65292;&#23613;&#31649;&#20197;&#38590;&#20197;&#24418;&#24335;&#21270;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#31526;&#21512;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#39640;&#32500;&#23485;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#65292;&#22312;&#19968;&#31995;&#21015;&#26032;&#30340;&#20005;&#23494;&#32467;&#26524;&#12289;&#38750;&#20005;&#23494;&#25968;&#23398;&#25512;&#23548;&#21644;&#25968;&#20540;&#23454;&#39564;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#29305;&#21035;&#25351;&#20986;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#29575;&#21442;&#25968;&#21270;&#28165;&#26224;&#30340;&#38454;&#27573;&#36716;&#25442;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#38271;&#21608;&#26399;&#30340;&#20986;&#29616;&#21644;&#28040;&#22833;&#26377;&#20851;&#12290;&#25105;&#20204;&#36824;&#20026;&#26089;&#26399;&#23398;&#20064;&#26102;&#25152;&#23398;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#35268;&#33539;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based learning in multi-layer neural networks displays a number of striking features. In particular, the decrease rate of empirical risk is non-monotone even after averaging over large batches. Long plateaus in which one observes barely any progress alternate with intervals of rapid decrease. These successive phases of learning often take place on very different time scales. Finally, models learnt in an early phase are typically `simpler' or `easier to learn' although in a way that is difficult to formalize.  Although theoretical explanations of these phenomena have been put forward, each of them captures at best certain specific regimes. In this paper, we study the gradient flow dynamics of a wide two-layer neural network in high-dimension, when data are distributed according to a single-index model (i.e., the target function depends on a one-dimensional projection of the covariates). Based on a mixture of new rigorous results, non-rigorous mathematical derivations, and numer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20316;&#29992;&#20110;&#20855;&#26377;&#35768;&#22810;&#27979;&#37327;&#21464;&#37327;&#30340;&#25968;&#25454;&#30340;&#20869;&#26680;&#26041;&#27861;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#32473;&#20986;&#20102;&#26497;&#38480;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2302.14446</link><description>&lt;p&gt;
&#22343;&#22330;&#26497;&#38480;&#20013;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Reproducing kernel Hilbert spaces in the mean field limit. (arXiv:2302.14446v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20316;&#29992;&#20110;&#20855;&#26377;&#35768;&#22810;&#27979;&#37327;&#21464;&#37327;&#30340;&#25968;&#25454;&#30340;&#20869;&#26680;&#26041;&#27861;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#32473;&#20986;&#20102;&#26497;&#38480;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#26680;&#26041;&#27861;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#21463;&#27426;&#36814;&#21644;&#25104;&#21151;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#23427;&#20204;&#26377;&#19968;&#20010;&#25104;&#29087;&#30340;&#29702;&#35770;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#25903;&#25345;&#12290;&#20174;&#25968;&#23398;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#20869;&#26680;&#30340;&#27010;&#24565;&#21644;&#20869;&#26680;&#29983;&#25104;&#30340;&#20989;&#25968;&#31354;&#38388;&#65292;&#21363;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#12290;&#21463;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#23398;&#20064;&#26041;&#27861;&#26368;&#36817;&#30340;&#21457;&#23637;&#30340;&#25512;&#21160;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20316;&#29992;&#20110;&#20855;&#26377;&#35768;&#22810;&#27979;&#37327;&#21464;&#37327;&#30340;&#25968;&#25454;&#30340;&#20869;&#26680;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20869;&#26680;&#30340;&#20005;&#26684;&#22343;&#22330;&#26497;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#26497;&#38480;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#20869;&#26680;&#30340;&#31034;&#20363;&#65292;&#36825;&#20123;&#20869;&#26680;&#20801;&#35768;&#20005;&#26684;&#30340;&#22343;&#22330;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel methods, being supported by a well-developed theory and coming with efficient algorithms, are among the most popular and successful machine learning techniques. From a mathematical point of view, these methods rest on the concept of kernels and function spaces generated by kernels, so called reproducing kernel Hilbert spaces. Motivated by recent developments of learning approaches in the context of interacting particle systems, we investigate kernel methods acting on data with many measurement variables. We show the rigorous mean field limit of kernels and provide a detailed analysis of the limiting reproducing kernel Hilbert space. Furthermore, several examples of kernels, that allow a rigorous mean field limit, are presented.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rashomon&#30340;&#22235;&#37325;&#22863;&#65292;&#36825;&#26159;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#22235;&#20010;&#27169;&#22411;&#20855;&#26377;&#20960;&#20046;&#30456;&#21516;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20854;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#26497;&#20854;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.13356</link><description>&lt;p&gt;
&#34920;&#29616;&#19981;&#36275;&#20197;&#20026;&#30408;&#65292;&#28145;&#31350;Rashomon&#30340;&#22235;&#37325;&#22863;
&lt;/p&gt;
&lt;p&gt;
Performance is not enough: a story of the Rashomon's quartet. (arXiv:2302.13356v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rashomon&#30340;&#22235;&#37325;&#22863;&#65292;&#36825;&#26159;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#22235;&#20010;&#27169;&#22411;&#20855;&#26377;&#20960;&#20046;&#30456;&#21516;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20854;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#26497;&#20854;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24314;&#27169;&#36890;&#24120;&#34987;&#31616;&#21270;&#20026;&#23547;&#25214;&#26368;&#20248;&#27169;&#22411;&#26469;&#20248;&#21270;&#36873;&#23450;&#30340;&#24615;&#33021;&#24230;&#37327;&#12290;&#20294;&#22914;&#26524;&#31532;&#20108;&#20248;&#27169;&#22411;&#33021;&#22815;&#20197;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#21516;&#26679;&#25551;&#36848;&#25968;&#25454;&#21602;&#65311;&#31532;&#19977;&#20010;&#27169;&#22411;&#21602;&#65311;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#20250;&#23398;&#21040;&#23436;&#20840;&#19981;&#21516;&#30340;&#25968;&#25454;&#20851;&#31995;&#21527;&#65311;&#21463;&#21040;Anscombe&#22235;&#37325;&#22863;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Rashomon&#30340;&#22235;&#37325;&#22863;&#65292;&#36825;&#26159;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#22235;&#20010;&#27169;&#22411;&#20855;&#26377;&#20960;&#20046;&#30456;&#21516;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#26497;&#20854;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#12290;&#24341;&#20837;&#30340;&#31616;&#21333;&#31034;&#20363;&#26088;&#22312;&#36827;&#19968;&#27493;&#20419;&#36827;&#21487;&#35270;&#21270;&#20316;&#20026;&#27604;&#36739;&#39044;&#27979;&#27169;&#22411;&#36229;&#36234;&#24615;&#33021;&#30340;&#24517;&#35201;&#24037;&#20855;&#12290;&#25105;&#20204;&#38656;&#35201;&#24320;&#21457;&#23500;&#26377;&#27934;&#23519;&#21147;&#30340;&#25216;&#26415;&#26469;&#35299;&#37322;&#27169;&#22411;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive modelling is often reduced to finding the best model that optimizes a selected performance measure. But what if the second-best model describes the data equally well but in a completely different way? What about the third? Is it possible that the most effective models learn completely different relationships in the data? Inspired by Anscombe's quartet, this paper introduces Rashomon's quartet, a synthetic dataset for which four models from different classes have practically identical predictive performance. However, their visualization reveals drastically distinct ways of understanding the correlation structure in data. The introduced simple illustrative example aims to further facilitate visualization as a mandatory tool to compare predictive models beyond their performance. We need to develop insightful techniques for the explanatory analysis of model sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#21442;&#25968;&#24341;&#20837;&#30340;&#27491;&#20132;&#24352;&#37327;&#32553;&#20943;&#26426;&#21046;&#65292;&#20197;&#39640;&#25928;&#22320;&#20174;&#24102;&#22122;&#22768;&#30340;&#24352;&#37327;&#20013;&#24674;&#22797;&#30456;&#20851;&#20302;&#31209;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2302.05798</link><description>&lt;p&gt;
&#38543;&#26426;&#24352;&#37327;&#29702;&#35770;&#20248;&#21270;&#27491;&#20132;&#24352;&#37327;&#32553;&#20943;
&lt;/p&gt;
&lt;p&gt;
Optimizing Orthogonalized Tensor Deflation via Random Tensor Theory. (arXiv:2302.05798v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#21442;&#25968;&#24341;&#20837;&#30340;&#27491;&#20132;&#24352;&#37327;&#32553;&#20943;&#26426;&#21046;&#65292;&#20197;&#39640;&#25928;&#22320;&#20174;&#24102;&#22122;&#22768;&#30340;&#24352;&#37327;&#20013;&#24674;&#22797;&#30456;&#20851;&#20302;&#31209;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#38543;&#26426;&#22122;&#22768;&#24352;&#37327;&#20013;&#24674;&#22797;&#21487;&#33021;&#23384;&#22312;&#30456;&#20851;&#37096;&#20998;&#30340;&#20302;&#31209;&#20449;&#21495;&#24352;&#37327;&#38382;&#39064;&#12290;&#24403;&#24213;&#23618;&#37096;&#20998;&#26159;&#27491;&#20132;&#30340;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#24352;&#37327;&#32553;&#20943;&#65288;&#30001;&#19968;&#31995;&#21015;&#31209;&#20026;&#19968;&#30340;&#36924;&#36817;&#32452;&#25104;&#65289;&#39640;&#25928;&#22320;&#36827;&#34892;&#24674;&#22797;&#65292;&#32780;&#38750;&#27491;&#20132;&#30340;&#37096;&#20998;&#21487;&#33021;&#20250;&#24433;&#21709;&#24352;&#37327;&#32553;&#20943;&#26426;&#21046;&#65292;&#20174;&#32780;&#23548;&#33268;&#24674;&#22797;&#25928;&#29575;&#20302;&#19979;&#12290;&#22522;&#20110;&#26368;&#36817;&#21457;&#23637;&#30340;&#38543;&#26426;&#24352;&#37327;&#24037;&#20855;&#65292;&#26412;&#25991;&#36890;&#36807;&#23545;&#19968;&#20010;&#19977;&#38454;&#12289;&#31209;&#20026;&#20108;&#30340;&#23574;&#23792;&#24352;&#37327;&#30340;&#21442;&#25968;&#21270;&#32553;&#20943;&#36807;&#31243;&#36827;&#34892;&#28176;&#36817;&#20998;&#26512;&#65292;&#31934;&#30830;&#22320;&#22788;&#29702;&#20102;&#38750;&#27491;&#20132;&#24773;&#20917;&#12290;&#22522;&#20110;&#35813;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#32553;&#20943;&#26426;&#21046;&#20013;&#24341;&#20837;&#30340;&#21442;&#25968;&#26469;&#36827;&#34892;&#39640;&#25928;&#24352;&#37327;&#32553;&#20943;&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#26681;&#25454;&#26500;&#36896;&#35777;&#26126;&#22312;&#25152;&#30740;&#31350;&#30340;&#24352;&#37327;&#27169;&#22411;&#19979;&#26368;&#20248;&#12290;&#30456;&#21516;&#30340;&#24605;&#24819;&#20063;&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#20302;&#31209;&#24352;&#37327;&#27169;&#22411;&#65292;&#20363;&#22914;&#26356;&#39640;&#38454;&#21644;&#20854;&#20182;&#31867;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles the problem of recovering a low-rank signal tensor with possibly correlated components from a random noisy tensor, or so-called spiked tensor model. When the underlying components are orthogonal, they can be recovered efficiently using tensor deflation which consists of successive rank-one approximations, while non-orthogonal components may alter the tensor deflation mechanism, thereby preventing efficient recovery. Relying on recently developed random tensor tools, this paper deals precisely with the non-orthogonal case by deriving an asymptotic analysis of a parameterized deflation procedure performed on an order-three and rank-two spiked tensor. Based on this analysis, an efficient tensor deflation algorithm is proposed by optimizing the parameter introduced in the deflation mechanism, which in turn is proven to be optimal by construction for the studied tensor model. The same ideas could be extended to more general low-rank tensor models, e.g., higher ranks and o
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;CausalEGM&#26694;&#26550;&#65292;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#22240;&#26524;&#25928;&#24212;&#30340;&#35299;&#32806;&#20197;&#21450;&#23558;&#28151;&#28102;&#21464;&#37327;&#26144;&#23556;&#21040;&#20302;&#32500;&#28508;&#21464;&#37327;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2212.05925</link><description>&lt;p&gt;
CausalEGM: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#33324;&#24615;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CausalEGM: a general causal inference framework by encoding generative modeling. (arXiv:2212.05925v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05925
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;CausalEGM&#26694;&#26550;&#65292;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#22240;&#26524;&#25928;&#24212;&#30340;&#35299;&#32806;&#20197;&#21450;&#23558;&#28151;&#28102;&#21464;&#37327;&#26144;&#23556;&#21040;&#20302;&#32500;&#28508;&#21464;&#37327;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29702;&#35299;&#21644;&#34920;&#24449;&#22240;&#26524;&#25928;&#24212;&#24050;&#32463;&#25104;&#20026;&#35266;&#23519;&#30740;&#31350;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#20294;&#24403;&#28151;&#28102;&#21464;&#37327;&#20855;&#26377;&#39640;&#32500;&#24615;&#26102;&#65292;&#36825;&#31181;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#32534;&#30721;&#29983;&#25104;&#24314;&#27169;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#36890;&#29992;&#26694;&#26550;"CausalEGM"&#65292;&#21487;&#24212;&#29992;&#20110;&#20108;&#20803;&#21644;&#36830;&#32493;&#30340;&#27835;&#30103;&#35774;&#32622;&#12290;&#22312;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#39640;&#32500;&#28151;&#28102;&#21464;&#37327;&#31354;&#38388;&#21644;&#24050;&#30693;&#23494;&#24230;&#65288;&#20363;&#22914;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#65289;&#30340;&#20302;&#32500;&#28508;&#21464;&#37327;&#31354;&#38388;&#20043;&#38388;&#30340;&#21452;&#21521;&#36716;&#25442;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CausalEGM&#21516;&#26102;&#23558;&#28151;&#28102;&#21464;&#37327;&#23545;&#27835;&#30103;&#21644;&#32467;&#26524;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#35299;&#32806;&#65292;&#24182;&#26144;&#23556;&#28151;&#28102;&#21464;&#37327;&#21040;&#20302;&#32500;&#28508;&#21464;&#37327;&#31354;&#38388;&#12290;&#36890;&#36807;&#23545;&#20302;&#32500;&#28508;&#29305;&#24449;&#30340;&#35843;&#33410;&#65292;CausalEGM&#21487;&#20197;&#20272;&#35745;&#27599;&#20010;&#20010;&#20307;&#30340;&#22240;&#26524;&#25928;&#24212;&#25110;&#20154;&#32676;&#20013;&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Although understanding and characterizing causal effects have become essential in observational studies, it is challenging when the confounders are high-dimensional. In this article, we develop a general framework $\textit{CausalEGM}$ for estimating causal effects by encoding generative modeling, which can be applied in both binary and continuous treatment settings. Under the potential outcome framework with unconfoundedness, we establish a bidirectional transformation between the high-dimensional confounders space and a low-dimensional latent space where the density is known (e.g., multivariate normal distribution). Through this, CausalEGM simultaneously decouples the dependencies of confounders on both treatment and outcome and maps the confounders to the low-dimensional latent space. By conditioning on the low-dimensional latent features, CausalEGM can estimate the causal effect for each individual or the average causal effect within a population. Our theoretical analysis shows that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#32416;&#27491;&#21518;&#30340;&#36196;&#27744;&#20449;&#24687;&#20934;&#21017;&#34987;&#35777;&#26126;&#20316;&#20026;&#24211;&#23572;&#24052;&#20811;-&#33713;&#24067;&#21202;&#24046;&#24322;&#26412;&#36523;&#30340;&#20272;&#35745;&#37327;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#25552;&#20379;&#20102;&#25913;&#36827;&#20272;&#35745;&#37327;&#65292;&#24182;&#22312;&#38477;&#20302;&#31209;&#30340;&#24773;&#20917;&#19979;&#33391;&#22909;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2211.09326</link><description>&lt;p&gt;
&#32416;&#27491;&#21518;&#30340;&#36196;&#27744;&#20449;&#24687;&#20934;&#21017;&#30340;&#19981;&#21487;&#20801;&#35768;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Inadmissibility of the corrected Akaike information criterion. (arXiv:2211.09326v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#32416;&#27491;&#21518;&#30340;&#36196;&#27744;&#20449;&#24687;&#20934;&#21017;&#34987;&#35777;&#26126;&#20316;&#20026;&#24211;&#23572;&#24052;&#20811;-&#33713;&#24067;&#21202;&#24046;&#24322;&#26412;&#36523;&#30340;&#20272;&#35745;&#37327;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#25552;&#20379;&#20102;&#25913;&#36827;&#20272;&#35745;&#37327;&#65292;&#24182;&#22312;&#38477;&#20302;&#31209;&#30340;&#24773;&#20917;&#19979;&#33391;&#22909;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#32416;&#27491;&#21518;&#30340;&#36196;&#27744;&#20449;&#24687;&#20934;&#21017;&#26159;&#26399;&#26395;&#24211;&#23572;&#24052;&#20811;-&#33713;&#24067;&#21202;&#24046;&#24322;&#26368;&#23567;&#26041;&#24046;&#26080;&#20559;&#20272;&#35745;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#25439;&#22833;&#20272;&#35745;&#26694;&#26550;&#65292;&#25105;&#20204;&#34920;&#26126;&#23427;&#20316;&#20026;&#24211;&#23572;&#24052;&#20811;-&#33713;&#24067;&#21202;&#24046;&#24322;&#26412;&#36523;&#30340;&#20272;&#35745;&#37327;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#65292;&#32780;&#19981;&#26159;&#26399;&#26395;&#30340;&#24211;&#23572;&#24052;&#20811;-&#33713;&#24067;&#21202;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#38477;&#20302;&#31209;&#30340;&#24773;&#20917;&#19979;&#33391;&#22909;&#24037;&#20316;&#30340;&#24211;&#23572;&#24052;&#20811;-&#33713;&#24067;&#21202;&#24046;&#24322;&#30340;&#25913;&#36827;&#20272;&#35745;&#37327;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#26816;&#39564;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the multivariate linear regression model with unknown covariance, the corrected Akaike information criterion is the minimum variance unbiased estimator of the expected Kullback--Leibler discrepancy. In this study, based on the loss estimation framework, we show its inadmissibility as an estimator of the Kullback--Leibler discrepancy itself, instead of the expected Kullback--Leibler discrepancy. We provide improved estimators of the Kullback--Leibler discrepancy that work well in reduced-rank situations and examine their performance numerically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#28176;&#36827;&#30340;AMP&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#39640;&#32500;&#32479;&#35745;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;AMP&#29702;&#35770;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;AMP&#22312;&#29420;&#31435;&#21021;&#22987;&#21270;&#21644;&#35889;&#21021;&#22987;&#21270;&#24773;&#20917;&#19979;&#30340;&#26377;&#38480;&#26679;&#26412;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2208.03313</link><description>&lt;p&gt;
&#19981;&#23545;&#31216;&#27169;&#22411;&#20013;&#36817;&#20284;&#20449;&#24687;&#20256;&#36882;&#30340;&#38750;&#28176;&#36817;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Non-Asymptotic Framework for Approximate Message Passing in Spiked Models. (arXiv:2208.03313v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#28176;&#36827;&#30340;AMP&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#39640;&#32500;&#32479;&#35745;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;AMP&#29702;&#35770;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;AMP&#22312;&#29420;&#31435;&#21021;&#22987;&#21270;&#21644;&#35889;&#21021;&#22987;&#21270;&#24773;&#20917;&#19979;&#30340;&#26377;&#38480;&#26679;&#26412;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#20449;&#24687;&#20256;&#36882;&#65288;AMP&#65289;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#36845;&#20195;&#33539;&#24335;&#65292;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#39640;&#32500;&#32479;&#35745;&#38382;&#39064;&#30340;&#27714;&#35299;&#20013;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;AMP&#29702;&#35770;&#20027;&#35201;&#38598;&#20013;&#22312;&#39640;&#32500;&#28176;&#36817;&#24615;&#26041;&#38754;&#65292;&#26410;&#33021;&#39044;&#27979;&#24403;&#36845;&#20195;&#27425;&#25968;&#36229;&#36807;$o\big(\frac{\log n}{\log\log n}\big)$&#65288;&#20854;&#20013;$n$&#26159;&#38382;&#39064;&#30340;&#32500;&#24230;&#65289;&#26102;AMP&#30340;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#38750;&#28176;&#36817;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#38181;&#24418;&#30697;&#38453;&#20272;&#35745;&#20013;&#30340;AMP&#12290;&#25105;&#20204;&#22522;&#20110;&#26032;&#30340;AMP&#26356;&#26032;&#20998;&#35299;&#21644;&#21487;&#25511;&#27531;&#24046;&#39033;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#34920;&#24449;&#29420;&#31435;&#21021;&#22987;&#21270;&#24773;&#20917;&#19979;AMP&#30340;&#26377;&#38480;&#26679;&#26412;&#34892;&#20026;&#65292;&#24182;&#36827;&#19968;&#27493;&#25512;&#24191;&#21040;&#21253;&#25324;&#35889;&#21021;&#22987;&#21270;&#30340;&#24773;&#20917;&#12290;&#20316;&#20026;&#36825;&#31181;&#20998;&#26512;&#26041;&#27861;&#30340;&#20004;&#20010;&#20855;&#20307;&#24212;&#29992;&#65292;&#24403;&#35299;&#20915;$\mathbb{Z}_2$&#21516;&#27493;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#39044;&#27979;&#20102;&#35889;&#21021;&#22987;&#21270;AMP&#30340;&#34892;&#20026;&#65292;&#26368;&#22810;&#21487;&#20197;&#36827;&#34892;$O\big(\frac{n}{\mathrm{poly}\log n}\big)$&#20010;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate message passing (AMP) emerges as an effective iterative paradigm for solving high-dimensional statistical problems. However, prior AMP theory -which focused mostly on high-dimensional asymptotics -- fell short of predicting the AMP dynamics when the number of iterations surpasses $o\big(\frac{\log n}{\log\log n}\big)$ (with $n$ the problem dimension). To address this inadequacy, this paper develops a non-asymptotic framework for understanding AMP in spiked matrix estimation. Built upon new decomposition of AMP updates and controllable residual terms, we lay out an analysis recipe to characterize the finite-sample behavior of AMP in the presence of an independent initialization, which is further generalized to allow for spectral initialization. As two concrete consequences of the proposed analysis recipe: (i) when solving $\mathbb{Z}_2$ synchronization, we predict the behavior of spectrally initialized AMP for up to $O\big(\frac{n}{\mathrm{poly}\log n}\big)$ iterations, sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#31639;&#27861;&#8212;&#8212;&#25200;&#21160;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#21644;&#20445;&#23432;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#22312;&#36739;&#23567;&#30340;&#26679;&#26412;&#22823;&#23567;&#19979;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26497;&#23567;&#21270;&#26368;&#22823;&#31639;&#27861;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2005.12900</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#27169;&#22411;&#31361;&#30772;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#22823;&#23567;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model. (arXiv:2005.12900v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.12900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#31639;&#27861;&#8212;&#8212;&#25200;&#21160;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#21644;&#20445;&#23432;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#22312;&#36739;&#23567;&#30340;&#26679;&#26412;&#22823;&#23567;&#19979;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26497;&#23567;&#21270;&#26368;&#22823;&#31639;&#27861;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30528;&#30524;&#20110;&#22312;&#26377;&#29983;&#25104;&#27169;&#22411;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#22686;&#24378;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#39318;&#20808;&#65292;&#32771;&#34385;&#24102;&#26377;&#25240;&#25187;&#30340;&#26080;&#38480;&#26102;&#38388;&#27493;&#38271;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#20854;&#29366;&#24577;&#31354;&#38388;&#20026;$\mathcal{S}$&#65292;&#21160;&#20316;&#31354;&#38388;&#20026;$\mathcal{A}$&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#32479;&#35745;&#31934;&#24230;&#20043;&#38388;&#26435;&#34913;&#30340;&#23436;&#25972;&#22270;&#26223;&#23578;&#26410;&#30830;&#23450;&#12290;&#29305;&#21035;&#26159;&#65292;&#25152;&#26377;&#30340;&#20808;&#21069;&#32467;&#26524;&#37117;&#21463;&#21040;&#20005;&#37325;&#30340;&#26679;&#26412;&#22823;&#23567;&#38556;&#30861;&#65292;&#22240;&#20026;&#23427;&#20204;&#22768;&#31216;&#30340;&#32479;&#35745;&#20445;&#35777;&#20165;&#22312;&#26679;&#26412;&#22823;&#23567;&#36229;&#36807;&#33267;&#23569;$\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^2}$&#26102;&#25165;&#25104;&#31435;&#12290;&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#20004;&#20010;&#31639;&#27861;&#8212;&#8212;&#25200;&#21160;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#21644;&#20445;&#23432;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#8212;&#8212;&#22312;&#26679;&#26412;&#22823;&#23567;&#36229;&#36807;$\frac{|\mathcal{S}||\mathcal{A}|}{1-\gamma}$&#30340;&#24773;&#20917;&#19979;&#23601;&#33021;&#35777;&#26126;&#23427;&#20204;&#30340;&#26497;&#23567;&#21270;&#26368;&#22823;&#31639;&#27861;&#20248;&#21270;&#24615;&#33021;&#65288;&#20960;&#20046;&#31526;&#21512;&#19968;&#20123;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#38500;&#20102;&#26080;&#38480;&#26102;&#38388;&#27493;&#38271;M&#35299;&#20915;&#26041;&#26696;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#26377;&#38480;&#26679;&#26412;&#21644;&#36817;&#20284;&#20215;&#20540;&#36845;&#20195;&#38382;&#39064;&#65292;&#20197;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#31639;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with the sample efficiency of reinforcement learning, assuming access to a generative model (or simulator). We first consider $\gamma$-discounted infinite-horizon Markov decision processes (MDPs) with state space $\mathcal{S}$ and action space $\mathcal{A}$. Despite a number of prior works tackling this problem, a complete picture of the trade-offs between sample complexity and statistical accuracy is yet to be determined. In particular, all prior results suffer from a severe sample size barrier, in the sense that their claimed statistical guarantees hold only when the sample size exceeds at least $\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^2}$. The current paper overcomes this barrier by certifying the minimax optimality of two algorithms -- a perturbed model-based algorithm and a conservative model-based algorithm -- as soon as the sample size exceeds the order of $\frac{|\mathcal{S}||\mathcal{A}|}{1-\gamma}$ (modulo some log factor). Moving beyond infinite-
&lt;/p&gt;</description></item></channel></rss>