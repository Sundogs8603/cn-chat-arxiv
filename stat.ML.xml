<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#25200;&#21160;&#21644;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.11122</link><description>&lt;p&gt;
&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Amortized Bayesian Inference. (arXiv:2310.11122v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#25200;&#21160;&#21644;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#26159;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#21644;&#20915;&#31574;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#29616;&#20195;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#22522;&#26412;&#36873;&#25321;&#28041;&#21450;&#20284;&#28982;&#20989;&#25968;&#21644;&#20808;&#39564;&#20998;&#24067;&#30340;&#35268;&#33539;&#12289;&#21518;&#39564;&#36924;&#36817;&#22120;&#21644;&#25968;&#25454;&#12290;&#27599;&#20010;&#36873;&#25321;&#37117;&#21487;&#20197;&#26174;&#30528;&#24433;&#21709;&#22522;&#20110;&#27169;&#22411;&#30340;&#25512;&#26029;&#21644;&#21518;&#32493;&#20915;&#31574;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#23558;&#25935;&#24863;&#24615;&#20998;&#26512;&#25972;&#21512;&#21040;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#65288;ABI&#65292;&#21363;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#25311;&#25512;&#26029;&#65289;&#20013;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#26435;&#37325;&#20849;&#20139;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32534;&#30721;&#26367;&#20195;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#25512;&#26029;&#26469;&#35780;&#20272;&#23545;&#21508;&#31181;&#25968;&#25454;&#25200;&#21160;&#25110;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#36125;&#21494;&#26031;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20004;&#20010;&#27493;&#39588;&#37117;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference is a powerful framework for making probabilistic inferences and decisions under uncertainty. Fundamental choices in modern Bayesian workflows concern the specification of the likelihood function and prior distributions, the posterior approximator, and the data. Each choice can significantly influence model-based inference and subsequent decisions, thereby necessitating sensitivity analysis. In this work, we propose a multifaceted approach to integrate sensitivity analyses into amortized Bayesian inference (ABI, i.e., simulation-based inference with neural networks). First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to various data perturbations or pre-processing procedures. In contrast to most other Bayesian approaches, both steps circumvent the costly
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.07918</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#20223;&#23398;&#20064;&#23545;&#21307;&#30103;&#20915;&#31574;&#36827;&#34892;&#24314;&#27169;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#20013;&#20272;&#35745;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#38480;&#21046;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#37322;&#65292;&#20363;&#22914;&#65292;&#23457;&#35745;&#21307;&#30103;&#20915;&#31574;&#30340;&#20559;&#35265;&#21644;&#27425;&#20248;&#23454;&#36341;&#65292;&#25105;&#20204;&#38656;&#35201;&#20915;&#31574;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;&#29616;&#26377;&#26041;&#27861;&#22522;&#26412;&#19978;&#30001;&#20110;&#23558;&#28508;&#22312;&#20915;&#31574;&#36807;&#31243;&#34920;&#31034;&#20026;&#36890;&#29992;&#31574;&#30053;&#32780;&#36127;&#25285;&#20102;&#36825;&#31181;&#26435;&#34913;&#65292;&#32780;&#23454;&#38469;&#19978;&#20154;&#31867;&#20915;&#31574;&#26159;&#21160;&#24577;&#30340;&#65292;&#21487;&#20197;&#38543;&#19978;&#19979;&#25991;&#20449;&#24687;&#32780;&#22823;&#24133;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65288;CPR&#65289;&#65292;&#23558;&#24314;&#27169;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#30340;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22797;&#26434;&#20915;&#31574;&#31574;&#30053;&#30001;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#32452;&#25104;&#12290;CPR&#23558;&#27599;&#20010;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#24314;&#27169;&#20026;&#32447;&#24615;&#30340;&#35266;&#23519;-&#21160;&#20316;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#32039;&#31995;&#25968;&#26679;&#26465;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#27169;&#24335;&#30340;&#25968;&#37327;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26680;&#20272;&#35745;&#22120;&#21644;&#32452;&#21512;&#26679;&#26465;&#65292;&#23454;&#29616;&#20102;&#29305;&#24449;&#25506;&#32034;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#24335;&#26816;&#39564;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#19987;&#23478;&#21028;&#26029;&#12290;&#36890;&#36807;&#22312;&#20307;&#32946;&#20998;&#26512;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05825</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#32039;&#31995;&#25968;&#26679;&#26465;&#20272;&#35745;&#27169;&#24335;&#30340;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
Bayesian taut splines for estimating the number of modes. (arXiv:2307.05825v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#32039;&#31995;&#25968;&#26679;&#26465;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#27169;&#24335;&#30340;&#25968;&#37327;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26680;&#20272;&#35745;&#22120;&#21644;&#32452;&#21512;&#26679;&#26465;&#65292;&#23454;&#29616;&#20102;&#29305;&#24449;&#25506;&#32034;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#24335;&#26816;&#39564;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#19987;&#23478;&#21028;&#26029;&#12290;&#36890;&#36807;&#22312;&#20307;&#32946;&#20998;&#26512;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#27169;&#24335;&#30340;&#25968;&#37327;&#20195;&#34920;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#20063;&#21487;&#20197;&#30475;&#20316;&#29616;&#26377;&#20122;&#32676;&#20307;&#30340;&#25968;&#37327;&#12290;&#23613;&#31649;&#20854;&#30456;&#20851;&#24615;&#65292;&#23545;&#20854;&#20272;&#35745;&#30340;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#38024;&#23545;&#21333;&#21464;&#37327;&#24773;&#20917;&#25552;&#20986;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#33268;&#21147;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21463;&#21040;&#20102;&#38382;&#39064;&#30340;&#19968;&#20123;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#35748;&#20026;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#32467;&#26500;&#65292;&#27169;&#24335;&#30340;&#20027;&#35266;&#19988;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#23494;&#24230;&#29305;&#24615;&#30340;&#25972;&#20307;&#35270;&#22270;&#30340;&#20415;&#21033;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28789;&#27963;&#30340;&#26680;&#20272;&#35745;&#22120;&#21644;&#31616;&#27905;&#30340;&#32452;&#21512;&#26679;&#26465;&#12290;&#29305;&#24449;&#25506;&#32034;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#24335;&#26816;&#39564;&#37117;&#22312;&#36125;&#21494;&#26031;&#25512;&#29702;&#33539;&#24335;&#20013;&#23454;&#29616;&#65292;&#20026;&#36719;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#24182;&#20801;&#35768;&#22312;&#36807;&#31243;&#20013;&#24341;&#20837;&#19987;&#23478;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#30340;&#23454;&#29992;&#24615;&#36890;&#36807;&#22312;&#20307;&#32946;&#20998;&#26512;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#38506;&#20276;&#30340;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The number of modes in a probability density function is representative of the model's complexity and can also be viewed as the number of existing subpopulations. Despite its relevance, little research has been devoted to its estimation. Focusing on the univariate setting, we propose a novel approach targeting prediction accuracy inspired by some overlooked aspects of the problem. We argue for the need for structure in the solutions, the subjective and uncertain nature of modes, and the convenience of a holistic view blending global and local density properties. Our method builds upon a combination of flexible kernel estimators and parsimonious compositional splines. Feature exploration, model selection and mode testing are implemented in the Bayesian inference paradigm, providing soft solutions and allowing to incorporate expert judgement in the process. The usefulness of our proposal is illustrated through a case study in sports analytics, showcasing multiple companion visualisation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#32447;&#24615;&#32422;&#26463;&#21327;&#26041;&#24046;&#30697;&#38453;&#21464;&#25442;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#20801;&#35768;&#30456;&#23545;&#31616;&#21333;&#30340;&#28176;&#36817;&#24615;&#21644;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#12290;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#20851;&#20110;&#24314;&#27169;&#30456;&#20851;&#30697;&#38453;&#21644;&#31232;&#30095;&#24615;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2306.03590</link><description>&lt;p&gt;
&#29109;&#21327;&#26041;&#24046;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Entropic covariance models. (arXiv:2306.03590v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#32447;&#24615;&#32422;&#26463;&#21327;&#26041;&#24046;&#30697;&#38453;&#21464;&#25442;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#20801;&#35768;&#30456;&#23545;&#31616;&#21333;&#30340;&#28176;&#36817;&#24615;&#21644;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#12290;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#20851;&#20110;&#24314;&#27169;&#30456;&#20851;&#30697;&#38453;&#21644;&#31232;&#30095;&#24615;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#20013;&#65292;&#25214;&#21040;&#21512;&#36866;&#30340;&#27169;&#22411;&#21644;&#26377;&#25928;&#30340;&#20272;&#35745;&#26041;&#27861;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#25991;&#29486;&#20013;&#36890;&#24120;&#37319;&#29992;&#20004;&#31181;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#23545;&#21327;&#26041;&#24046;&#30697;&#38453;&#25110;&#20854;&#36870;&#26045;&#21152;&#32447;&#24615;&#32422;&#26463;&#65292;&#21478;&#19968;&#31181;&#26159;&#32771;&#34385;&#26045;&#21152;&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#30697;&#38453;&#23545;&#25968;&#19978;&#30340;&#32447;&#24615;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#32447;&#24615;&#32422;&#26463;&#21327;&#26041;&#24046;&#30697;&#38453;&#21464;&#25442;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19978;&#36848;&#20363;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20272;&#35745;&#26041;&#27861;&#35299;&#20915;&#20102;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#24182;&#20135;&#29983;&#20102;&#19968;&#20010;M&#20272;&#35745;&#37327;&#65292;&#20801;&#35768;&#30456;&#23545;&#31616;&#21333;&#30340;&#28176;&#36817;&#24615;&#21644;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#12290;&#22312;&#24320;&#21457;&#20102;&#19968;&#33324;&#29702;&#35770;&#20043;&#21518;&#65292;&#25105;&#20204;&#38598;&#20013;&#22312;&#24314;&#27169;&#30456;&#20851;&#30697;&#38453;&#21644;&#31232;&#30095;&#24615;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#20960;&#20309;&#27934;&#23519;&#21147;&#20801;&#35768;&#25105;&#20204;&#25193;&#23637;&#21327;&#26041;&#24046;&#30697;&#38453;&#24314;&#27169;&#20013;&#30340;&#19968;&#20123;&#26368;&#26032;&#32467;&#26524;&#12290;&#36825;&#21253;&#25324;&#25552;&#20379;&#30456;&#20851;&#30697;&#38453;&#31354;&#38388;&#30340;&#26080;&#38480;&#21046;&#21442;&#25968;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#26367;&#20195;&#21033;&#29992;&#21464;&#25442;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23545;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;Cholesky&#22240;&#23376;&#26045;&#21152;&#31232;&#30095;&#24615;&#38480;&#21046;&#65292;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In covariance matrix estimation, one of the challenges lies in finding a suitable model and an efficient estimation method. Two commonly used approaches in the literature involve imposing linear restrictions on the covariance matrix or its inverse. Another approach considers linear restrictions on the matrix logarithm of the covariance matrix. In this paper, we present a general framework for linear restrictions on different transformations of the covariance matrix, including the mentioned examples. Our proposed estimation method solves a convex problem and yields an M-estimator, allowing for relatively straightforward asymptotic and finite sample analysis. After developing the general theory, we focus on modelling correlation matrices and on sparsity. Our geometric insights allow to extend various recent results in covariance matrix modelling. This includes providing unrestricted parametrizations of the space of correlation matrices, which is alternative to a recent result utilizing t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20316;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#26631;&#20934;GP&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.20028</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Study of Bayesian Neural Network Surrogates for Bayesian Optimization. (arXiv:2305.20028v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20316;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#26631;&#20934;GP&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#38590;&#20197;&#26597;&#35810;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#36825;&#20123;&#30446;&#26631;&#20989;&#25968;&#36890;&#24120;&#30001;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20195;&#29702;&#27169;&#22411;&#34920;&#31034;&#65292;&#20854;&#26131;&#20110;&#20248;&#21270;&#24182;&#25903;&#25345;&#31934;&#30830;&#25512;&#29702;&#12290;&#34429;&#28982;&#26631;&#20934;&#30340;GP&#20195;&#29702;&#24050;&#32463;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#26368;&#36817;&#25104;&#20026;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#65292;&#19982;&#26631;&#20934;&#30340;GP&#30456;&#27604;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20363;&#22914;&#22825;&#28982;&#22788;&#29702;&#38750;&#24179;&#31283;&#24615;&#20197;&#21450;&#23398;&#20064;&#39640;&#32500;&#25968;&#25454;&#30340;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;BNN&#20316;&#20026;&#26631;&#20934;GP&#20195;&#29702;&#30340;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21508;&#31181;&#26377;&#38480;&#23485;&#24230;BNN&#30340;&#36817;&#20284;&#25512;&#29702;&#36807;&#31243;&#65292;&#21253;&#25324;&#39640;&#36136;&#37327;Hamiltonian Monte Carlo&#65292;&#20302;&#25104;&#26412;&#30340;&#38543;&#26426;MCMC&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;&#22914;&#28145;&#24230;&#38598;&#25104;&#65289;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#26080;&#38480;&#23485;&#24230;BNN&#21644;&#37096;&#20998;&#38543;&#26426;&#27169;&#22411;&#65292;&#20363;&#22914;&#28145;&#24230;&#26680;&#23398;&#20064;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#20195;&#29702;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#20248;&#20110;&#26631;&#20934;GP&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BNN&#26159;&#20256;&#32479;&#20195;&#29702;&#27169;&#22411;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#24456;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is a highly efficient approach to optimizing objective functions which are expensive to query. These objectives are typically represented by Gaussian process (GP) surrogate models which are easy to optimize and support exact inference. While standard GP surrogates have been well-established in Bayesian optimization, Bayesian neural networks (BNNs) have recently become practical function approximators, with many benefits over standard GPs such as the ability to naturally handle non-stationarity and learn representations for high-dimensional data. In this paper, we study BNNs as alternatives to standard GP surrogates for optimization. We consider a variety of approximate inference procedures for finite-width BNNs, including high-quality Hamiltonian Monte Carlo, low-cost stochastic MCMC, and heuristics such as deep ensembles. We also consider infinite-width BNNs and partially stochastic models such as deep kernel learning. We evaluate this collection of surrogate mod
&lt;/p&gt;</description></item><item><title>StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#21333;&#27493;&#21644;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.03853</link><description>&lt;p&gt;
StepMix: &#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables. (arXiv:2304.03853v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03853
&lt;/p&gt;
&lt;p&gt;
StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#21333;&#27493;&#21644;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#24191;&#20041;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;(&#28508;&#22312;&#21078;&#38754;&#21644;&#28508;&#22312;&#31867;&#20998;&#26512;)&#19982;&#22806;&#37096;&#21464;&#37327;(&#21327;&#21464;&#37327;&#21644;&#36828;&#31243;&#32467;&#26524;)&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;(&#21333;&#27493;&#12289;&#20004;&#27493;&#21644;&#19977;&#27493;&#26041;&#27861;)&#30340;&#24320;&#28304;&#36719;&#20214;&#21253;&#12290;&#22312;&#35768;&#22810;&#31038;&#20250;&#31185;&#23398;&#30340;&#24212;&#29992;&#20013;&#65292;&#20027;&#35201;&#30446;&#26631;&#19981;&#20165;&#26159;&#23558;&#20010;&#20307;&#32858;&#31867;&#25104;&#28508;&#22312;&#31867;&#21035;&#65292;&#36824;&#21253;&#25324;&#20351;&#29992;&#36825;&#20123;&#31867;&#21035;&#26469;&#24320;&#21457;&#26356;&#22797;&#26434;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20998;&#20026;&#19968;&#20010;&#23558;&#28508;&#22312;&#31867;&#21035;&#19982;&#35266;&#23519;&#25351;&#26631;&#30456;&#20851;&#32852;&#30340;&#27979;&#37327;&#27169;&#22411;&#21644;&#19968;&#20010;&#23558;&#21327;&#21464;&#37327;&#21644;&#32467;&#26524;&#21464;&#37327;&#19982;&#28508;&#22312;&#31867;&#21035;&#30456;&#20851;&#32852;&#30340;&#32467;&#26500;&#27169;&#22411;&#12290;&#27979;&#37327;&#21644;&#32467;&#26500;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#25152;&#35859;&#30340;&#19968;&#27493;&#27861;&#20849;&#21516;&#20272;&#35745;&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#36880;&#27493;&#26041;&#27861;&#36880;&#27493;&#20272;&#35745;&#65292;&#23545;&#20110;&#20174;&#19994;&#20154;&#21592;&#26469;&#35828;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20272;&#35745;&#28508;&#22312;&#31867;&#21035;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#38500;&#20102;&#19968;&#27493;&#27861;&#65292;StepMix&#36824;&#23454;&#29616;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#26368;&#37325;&#35201;&#30340;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#26041;&#20415;&#27169;&#22411;&#30340;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
StepMix is an open-source software package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;&#21452;&#19992;&#38477;&#26354;&#32447;&#20986;&#29616;&#8220;&#26368;&#32456;&#19978;&#21319;&#8221;&#65292;&#21363;&#22312;&#36275;&#22815;&#22823;&#30340;&#22122;&#22768;&#26679;&#26412;&#27604;&#29575;&#19979;&#65292;&#20013;&#31561;&#23485;&#24230;&#19979;&#23454;&#29616;&#26368;&#20339;&#27867;&#21270;&#24615;&#33021;&#12290;&#38543;&#26426;&#20002;&#24323;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#20943;&#23569;&#23494;&#24230;&#21487;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.08003</link><description>&lt;p&gt;
&#30740;&#31350;&#27169;&#22411;&#23485;&#24230;&#21644;&#23494;&#24230;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise. (arXiv:2208.08003v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;&#21452;&#19992;&#38477;&#26354;&#32447;&#20986;&#29616;&#8220;&#26368;&#32456;&#19978;&#21319;&#8221;&#65292;&#21363;&#22312;&#36275;&#22815;&#22823;&#30340;&#22122;&#22768;&#26679;&#26412;&#27604;&#29575;&#19979;&#65292;&#20013;&#31561;&#23485;&#24230;&#19979;&#23454;&#29616;&#26368;&#20339;&#27867;&#21270;&#24615;&#33021;&#12290;&#38543;&#26426;&#20002;&#24323;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#20943;&#23569;&#23494;&#24230;&#21487;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#22823;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#26159;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#36825;&#26159;&#36890;&#36807;&#21452;&#19992;&#38477;&#29616;&#35937;&#25429;&#25417;&#30340;&#65292;&#20854;&#20013;&#27979;&#35797;&#25439;&#22833;&#38543;&#30528;&#27169;&#22411;&#23485;&#24230;&#30340;&#22686;&#21152;&#21576;&#29616;&#20986;&#38477;&#20302;-&#22686;&#21152;-&#38477;&#20302;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#26631;&#31614;&#22122;&#22768;&#23545;&#27979;&#35797;&#25439;&#22833;&#26354;&#32447;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#21363;&#26631;&#31614;&#22122;&#22768;&#23548;&#33268;&#21407;&#26412;&#35266;&#23519;&#21040;&#30340;&#21452;&#19992;&#38477;&#26354;&#32447;&#20986;&#29616;&#20102;&#8220;&#26368;&#32456;&#19978;&#21319;&#8221;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36275;&#22815;&#22823;&#30340;&#22122;&#22768;&#26679;&#26412;&#27604;&#29575;&#19979;&#65292;&#20013;&#31561;&#23485;&#24230;&#19979;&#23454;&#29616;&#26368;&#20339;&#27867;&#21270;&#24615;&#33021;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#29616;&#35937;&#24402;&#22240;&#20110;&#26631;&#31614;&#22122;&#22768;&#24341;&#36215;&#30340;&#27979;&#35797;&#25439;&#22833;&#26041;&#24046;&#24418;&#29366;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#26368;&#32456;&#19978;&#21319;&#29616;&#35937;&#25193;&#23637;&#21040;&#27169;&#22411;&#23494;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#34920;&#24449;&#65292;&#34920;&#26126;&#38543;&#26426;&#20002;&#24323;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#20943;&#23569;&#23494;&#24230;&#21487;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increasing the size of overparameterized neural networks has been a key in achieving state-of-the-art performance. This is captured by the double descent phenomenon, where the test loss follows a decreasing-increasing-decreasing pattern as model width increases. However, the effect of label noise on the test loss curve has not been fully explored. In this work, we uncover an intriguing phenomenon where label noise leads to a \textit{final ascent} in the originally observed double descent curve. Specifically, under a sufficiently large noise-to-sample-size ratio, optimal generalization is achieved at intermediate widths. Through theoretical analysis, we attribute this phenomenon to the shape transition of test loss variance induced by label noise. Furthermore, we extend the final ascent phenomenon to model density and provide the first theoretical characterization showing that reducing density by randomly dropping trainable parameters improves generalization under label noise. We also t
&lt;/p&gt;</description></item></channel></rss>