<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#26071;&#22411;&#27969;&#24418;&#19978;&#35745;&#31639;&#19968;&#32452;&#28857;&#30340;&#26071;&#24418;&#22343;&#20540;&#21644;&#26071;&#24418;&#20013;&#20540;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13501</link><description>&lt;p&gt;
&#26071;&#22411;&#27969;&#24418;&#19978;&#30340;&#24358;&#22343;&#20540;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Chordal Averaging on Flag Manifolds and Its Applications. (arXiv:2303.13501v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#26071;&#22411;&#27969;&#24418;&#19978;&#35745;&#31639;&#19968;&#32452;&#28857;&#30340;&#26071;&#24418;&#22343;&#20540;&#21644;&#26071;&#24418;&#20013;&#20540;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#24358;&#24230;&#37327;&#19979;&#35745;&#31639;&#26071;&#22411;&#27969;&#24418;&#19978;&#19968;&#32452;&#28857;&#30340;&#26071;&#24418;&#22343;&#20540;&#21644;&#26071;&#24418;&#20013;&#20540;&#12290;&#26071;&#22411;&#27969;&#24418;&#26159;&#19968;&#31181;&#25968;&#23398;&#31354;&#38388;&#65292;&#30001;&#23884;&#22871;&#30340;&#21521;&#37327;&#31354;&#38388;&#23376;&#31354;&#38388;&#24207;&#21015;&#32452;&#25104;&#65292;&#24182;&#19988;&#22312;&#32500;&#24230;&#19978;&#36880;&#28176;&#22686;&#21152;&#12290;&#26071;&#22411;&#27969;&#24418;&#26159;&#24050;&#30693;&#30340;&#35768;&#22810;&#30697;&#38453;&#32676;&#30340;&#36229;&#38598;&#65292;&#21253;&#25324;Stiefel&#21644;Grassmanians&#65292;&#20351;&#20854;&#25104;&#20026;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#20013;&#38750;&#24120;&#26377;&#29992;&#30340;&#36890;&#29992;&#23545;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#31639;&#19968;&#38454;&#26071;&#24092;&#32479;&#35745;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#28041;&#21450;&#36741;&#21161;&#21464;&#37327;&#21463;Stiefel&#27969;&#24418;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;Stiefel&#27969;&#24418;&#26159;&#19968;&#32452;&#27491;&#20132;&#26694;&#26550;&#30340;&#31354;&#38388;&#65292;&#21033;&#29992;Stiefel&#27969;&#24418;&#20248;&#21270;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#26071;&#24418;&#22343;&#20540;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Grassmann&#21644;&#26059;&#36716;&#22343;&#20540;&#20197;&#21450;&#20027;&#25104;&#20998;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new, provably-convergent algorithm for computing the flag-mean and flag-median of a set of points on a flag manifold under the chordal metric. The flag manifold is a mathematical space consisting of flags, which are sequences of nested subspaces of a vector space that increase in dimension. The flag manifold is a superset of a wide range of known matrix groups, including Stiefel and Grassmanians, making it a general object that is useful in a wide variety computer vision problems.  To tackle the challenge of computing first order flag statistics, we first transform the problem into one that involves auxiliary variables constrained to the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and leveraging the numerical stability and efficiency of Stiefel-manifold optimization enables us to compute the flag-mean effectively. Through a series of experiments, we show the competence of our method in Grassmann and rotation averaging, as well as princi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#29992;&#25968;&#25454;&#30340;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#26469;&#35780;&#20272;&#25104;&#21151;&#35757;&#32451;&#21644;&#27867;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#21435;&#38500;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#21457;&#29616;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#27604;&#20351;&#29992;&#30456;&#21516;&#20998;&#24067;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2303.13462</link><description>&lt;p&gt;
&#21033;&#29992;&#37327;&#23376;&#20960;&#20309;&#36827;&#34892;&#23398;&#20064;&#24186;&#27491;&#21464;&#25442;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization with quantum geometry for learning unitaries. (arXiv:2303.13462v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#29992;&#25968;&#25454;&#30340;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#26469;&#35780;&#20272;&#25104;&#21151;&#35757;&#32451;&#21644;&#27867;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#21435;&#38500;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#21457;&#29616;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#27604;&#20351;&#29992;&#30456;&#21516;&#20998;&#24067;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#20934;&#30830;&#39044;&#27979;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#25968;&#25454;&#30340;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;(DQFIM)&#26469;&#30830;&#23450;&#27169;&#22411;&#20309;&#26102;&#33021;&#22815;&#27867;&#21270;&#12290;&#23545;&#20110;&#24186;&#27491;&#21464;&#25442;&#30340;&#21487;&#21464;&#23398;&#20064;&#65292;DQFIM&#37327;&#21270;&#20102;&#25104;&#21151;&#35757;&#32451;&#21644;&#27867;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#24212;&#29992;DQFIM&#26469;&#35299;&#37322;&#20309;&#26102;&#24658;&#23450;&#25968;&#37327;&#30340;&#35757;&#32451;&#29366;&#24577;&#21644;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#21442;&#25968;&#36275;&#20197;&#23454;&#29616;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#21024;&#38500;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26174;&#31034;&#65292;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#27604;&#20351;&#29992;&#30456;&#21516;&#20998;&#24067;&#30340;&#33021;&#21147;&#26356;&#20248;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25552;&#39640;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#24320;&#36767;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization is the ability of quantum machine learning models to make accurate predictions on new data by learning from training data. Here, we introduce the data quantum Fisher information metric (DQFIM) to determine when a model can generalize. For variational learning of unitaries, the DQFIM quantifies the amount of circuit parameters and training data needed to successfully train and generalize. We apply the DQFIM to explain when a constant number of training states and polynomial number of parameters are sufficient for generalization. Further, we can improve generalization by removing symmetries from training data. Finally, we show that out-of-distribution generalization, where training and testing data are drawn from different data distributions, can be better than using the same distribution. Our work opens up new approaches to improve generalization in quantum machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376; Langevin &#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#12290;&#20351;&#29992;&#27492;&#31639;&#27861;&#65292;&#20272;&#35745;&#22120;&#30340;&#20248;&#21270;&#35823;&#24046;&#20855;&#26377;&#38750;&#28176;&#36817;&#27987;&#24230;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2303.13429</link><description>&lt;p&gt;
&#26368;&#22823;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376; Langevin &#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interacting Particle Langevin Algorithm for Maximum Marginal Likelihood Estimation. (arXiv:2303.13429v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376; Langevin &#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#12290;&#20351;&#29992;&#27492;&#31639;&#27861;&#65292;&#20272;&#35745;&#22120;&#30340;&#20248;&#21270;&#35823;&#24046;&#20855;&#26377;&#38750;&#28176;&#36817;&#27987;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#65292;&#29992;&#20110;&#23454;&#29616;&#28508;&#21464;&#37327;&#27169;&#22411;&#21442;&#25968;&#30340;&#26368;&#22823;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#36807;&#31243;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#22312;&#25193;&#23637;&#30340;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340; Langevin&#28418;&#31227;&#65292;&#20854;&#20013;&#22312;&#32463;&#20856;&#30340;&#20248;&#21270;&#20013;&#65292;&#31890;&#23376;&#25968;&#37327;&#20316;&#20026;&#30456;&#21453;&#28201;&#24230;&#21442;&#25968;&#12290;&#20351;&#29992;Langevin&#28418;&#31227;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#22823;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#22120;&#30340;&#20248;&#21270;&#35823;&#24046;&#30340;&#38750;&#28176;&#36817;&#27987;&#24230;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#19982;&#31890;&#23376;&#31995;&#32479;&#20013;&#30340;&#31890;&#23376;&#25968;&#37327;&#65292;&#31639;&#27861;&#30340;&#36845;&#20195;&#27425;&#25968;&#20197;&#21450;&#26102;&#38388;&#31163;&#25955;&#21270;&#20998;&#26512;&#30340;&#27493;&#38271;&#21442;&#25968;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a class of interacting particle systems for implementing a marginal maximum likelihood estimation (MLE) procedure to optimize over the parameters of a latent variable model. To do so, we propose a continuous-time interacting particle system which can be seen as a Langevin diffusion over an extended state space, where the number of particles acts as the inverse temperature parameter in classical settings for optimisation. Using Langevin diffusions, we prove nonasymptotic concentration bounds for the optimisation error of the maximum marginal likelihood estimator in terms of the number of particles in the particle system, the number of iterations of the algorithm, and the step-size parameter for the time discretisation analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;&#19981;&#21516;&#20122;&#32676;&#20013;&#25311;&#21512;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36890;&#36807;&#31561;&#20215;&#24615;&#27979;&#35797;&#26469;&#28608;&#21169;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13330</link><description>&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#31561;&#20215;&#24615;:&#19968;&#31181;&#27604;&#36739;&#19981;&#21516;&#26063;&#32676;&#19979;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Logistic Regression Equivalence: A Framework for Comparing Logistic Regression Models Across Populations. (arXiv:2303.13330v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;&#19981;&#21516;&#20122;&#32676;&#20013;&#25311;&#21512;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36890;&#36807;&#31561;&#20215;&#24615;&#27979;&#35797;&#26469;&#28608;&#21169;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#22914;&#20309;&#35780;&#20272;&#19981;&#21516;&#20122;&#32676;&#20013;&#25311;&#21512;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#20197;&#30740;&#31350;&#23398;&#20064;&#38556;&#30861;&#30340;&#35745;&#31639;&#26426;&#35786;&#26029;&#20026;&#20363;&#65292;&#20854;&#20013;&#20197;&#24615;&#21035;&#20026;&#22522;&#30784;&#30340;&#20122;&#32676;&#21487;&#33021;&#38656;&#35201;&#20998;&#21035;&#24314;&#31435;&#27169;&#22411;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#38646;&#24046;&#24322;&#20551;&#35774;&#30340;&#26174;&#33879;&#24615;&#26816;&#39564;&#21487;&#33021;&#20250;&#20135;&#29983;&#36870;&#21521;&#28608;&#21169;&#65292;&#22240;&#20026;&#36739;&#22823;&#30340;&#26041;&#24046;&#21644;&#36739;&#23567;&#30340;&#26679;&#26412;&#20250;&#22686;&#21152;&#19981;&#25298;&#32477;&#38646;&#20551;&#35774;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#39044;&#20808;&#35774;&#23450;&#30340;&#23481;&#24046;&#27700;&#24179;&#19978;&#36827;&#34892;&#31561;&#20215;&#24615;&#27979;&#35797;&#21487;&#28608;&#21169;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#32452;&#32423;&#32852;&#30340;&#31561;&#20215;&#24615;&#27979;&#35797;&#65292;&#27599;&#20010;&#27979;&#35797;&#37117;&#28041;&#21450;&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#38754;&#65306;&#29616;&#35937;&#22312;&#22238;&#24402;&#31995;&#25968;&#20013;&#30340;&#32534;&#30721;&#26041;&#24335;&#12289;&#27599;&#20010;&#26679;&#26412;&#20013;&#30340;&#21333;&#29420;&#39044;&#27979;&#21644;&#24179;&#22343;&#24179;&#26041;&#39044;&#27979;&#35823;&#24046;&#20013;&#30340;&#24635;&#20307;&#20934;&#30830;&#24615;&#12290;&#38024;&#23545;&#27599;&#20010;&#31561;&#20215;&#24615;&#27979;&#35797;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#32622;&#26368;&#23567;&#25928;&#24212;&#37327;&#38408;&#20540;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we discuss how to evaluate the differences between fitted logistic regression models across sub-populations. Our motivating example is in studying computerized diagnosis for learning disabilities, where sub-populations based on gender may or may not require separate models. In this context, significance tests for hypotheses of no difference between populations may provide perverse incentives, as larger variances and smaller samples increase the probability of not-rejecting the null. We argue that equivalence testing for a prespecified tolerance level on population differences incentivizes accuracy in the inference. We develop a cascading set of equivalence tests, in which each test addresses a different aspect of the model: the way the phenomenon is coded in the regression coefficients, the individual predictions in the per example log odds ratio and the overall accuracy in the mean square prediction error. For each equivalence test, we propose a strategy for setting the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340; AI &#26234;&#33021;&#20307;&#20316;&#20026;&#35745;&#31639;&#22522;&#20934;&#65292;&#20197;&#35780;&#20272;&#20154;&#31867;&#22312;&#22256;&#38590;&#30340;&#28041;&#21450;&#22810;&#20010;&#20154;&#31867;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#21450;&#21457;&#29616;&#26377;&#24453;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;&#35813;&#35770;&#25991;&#20197;&#36275;&#29699;&#34920;&#29616;&#20998;&#26512;&#20026;&#20363;&#65292;&#20351;&#29992;&#22823;&#22411;&#29699;&#21592;&#21644;&#29699;&#20301;&#32622;&#36319;&#36394;&#25968;&#25454;&#38598;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#36827;&#34892;&#36275;&#29699;&#27604;&#36187;&#20013;&#30340;&#20132;&#20114;&#27169;&#20223;&#12290;</title><link>http://arxiv.org/abs/2303.13323</link><description>&lt;p&gt;
&#22797;&#26434;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#20154;&#31867;&#34920;&#29616;&#35780;&#20272;&#65306;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#22810;&#26234;&#33021;&#20307;&#27169;&#22411;&#30340;&#35745;&#31639;&#22522;&#20934;&#26041;&#27861;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#36275;&#29699;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Multi-Agent Imitation Model as a Computational Benchmark for Evaluating Human Performance in Complex Interactive Tasks: A Case Study in Football. (arXiv:2303.13323v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340; AI &#26234;&#33021;&#20307;&#20316;&#20026;&#35745;&#31639;&#22522;&#20934;&#65292;&#20197;&#35780;&#20272;&#20154;&#31867;&#22312;&#22256;&#38590;&#30340;&#28041;&#21450;&#22810;&#20010;&#20154;&#31867;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#21450;&#21457;&#29616;&#26377;&#24453;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;&#35813;&#35770;&#25991;&#20197;&#36275;&#29699;&#34920;&#29616;&#20998;&#26512;&#20026;&#20363;&#65292;&#20351;&#29992;&#22823;&#22411;&#29699;&#21592;&#21644;&#29699;&#20301;&#32622;&#36319;&#36394;&#25968;&#25454;&#38598;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#36827;&#34892;&#36275;&#29699;&#27604;&#36187;&#20013;&#30340;&#20132;&#20114;&#27169;&#20223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#22914;&#24037;&#31243;&#21644;&#20307;&#32946;&#20013;&#65292;&#35780;&#20272;&#20154;&#31867;&#30340;&#34920;&#29616;&#26159;&#24120;&#35265;&#30340;&#38656;&#27714;&#12290;&#35780;&#20272;&#20154;&#31867;&#22312;&#23436;&#25104;&#22797;&#26434;&#30340;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#24050;&#34987;&#35777;&#26126;&#22312;&#35813;&#24773;&#22659;&#19979;&#26377;&#25928;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25110;&#20351;&#29992;&#20027;&#35266;&#27979;&#37327;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#26159;&#19968;&#20010;&#23481;&#26131;&#20986;&#38169;&#21644;&#19981;&#21487;&#38752;&#30340;&#36807;&#31243;&#65292;&#22240;&#20026;&#38745;&#24577;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#25429;&#25417;&#21040;&#19982;&#36825;&#20123;&#20219;&#21153;&#30456;&#20851;&#30340;&#25152;&#26377;&#22797;&#26434;&#24773;&#22659;&#65292;&#24182;&#19988;&#20027;&#35266;&#27979;&#37327;&#23384;&#22312;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340; AI &#26234;&#33021;&#20307;&#65292;&#20316;&#20026;&#35745;&#31639;&#22522;&#20934;&#26469;&#35780;&#20272;&#20154;&#31867;&#22312;&#35299;&#20915;&#28041;&#21450;&#22810;&#20010;&#20154;&#31867;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#22256;&#38590;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#22312;&#36275;&#29699;&#34920;&#29616;&#20998;&#26512;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#25105;&#20204;&#22312;&#22823;&#22411;&#29699;&#21592;&#21644;&#29699;&#20301;&#32622;&#36319;&#36394;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#22522;&#20110; Conditional Variational Recurrent Neural Network&#65288;VRNN&#65289;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#35757;&#32451;&#21518;&#30340;&#27169;&#22411;&#29992;&#20110;&#27169;&#20223;&#20004;&#20010;&#22242;&#38431;&#22312;&#36275;&#29699;&#27604;&#36187;&#20013;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#27604;&#36187;&#20013;&#36924;&#30495;&#30340;&#29699;&#21592;&#31227;&#21160;&#21644;&#22242;&#38431;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#29992;&#20316;&#35780;&#20272;&#20154;&#31867;&#36275;&#29699;&#34920;&#29616;&#30340;&#22522;&#20934;&#65292;&#24182;&#30830;&#23450;&#38656;&#35201;&#25552;&#39640;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the performance of human is a common need across many applications, such as in engineering and sports. When evaluating human performance in completing complex and interactive tasks, the most common way is to use a metric having been proved efficient for that context, or to use subjective measurement techniques. However, this can be an error prone and unreliable process since static metrics cannot capture all the complex contexts associated with such tasks and biases exist in subjective measurement. The objective of our research is to create data-driven AI agents as computational benchmarks to evaluate human performance in solving difficult tasks involving multiple humans and contextual factors. We demonstrate this within the context of football performance analysis. We train a generative model based on Conditional Variational Recurrent Neural Network (VRNN) Model on a large player and ball tracking dataset. The trained model is used to imitate the interactions between two te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#20016;&#23500;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20943;&#23569;&#26368;&#22351;&#24773;&#20917;&#30340;&#36829;&#35268;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.13228</link><description>&lt;p&gt;
&#20016;&#23500;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#25552;&#39640;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Enriching Neural Network Training Dataset to Improve Worst-Case Performance Guarantees. (arXiv:2303.13228v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#20016;&#23500;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20943;&#23569;&#26368;&#22351;&#24773;&#20917;&#30340;&#36829;&#35268;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#26159;&#29992;&#20110;&#36817;&#20284;&#38750;&#32447;&#24615;&#20851;&#31995;&#65288;&#20363;&#22914;AC-OPF&#65289;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#24182;&#22312;&#37096;&#32626;&#26102;&#23454;&#29616;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#12290;&#36890;&#24120;&#22312;&#30005;&#21147;&#31995;&#32479;&#25991;&#29486;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#26159;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20043;&#21069;&#29983;&#25104;&#30340;&#22266;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#20854;&#26368;&#22351;&#24773;&#20917;&#36829;&#35268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#20016;&#23500;&#20851;&#38190;&#25968;&#25454;&#28857;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20197;&#20943;&#23569;&#26368;&#22351;&#24773;&#20917;&#36829;&#35268;&#65292;&#25552;&#20379;&#20855;&#26377;&#25913;&#36827;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27979;&#35797;&#30005;&#21147;&#31995;&#32479;&#20013;&#28436;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#33539;&#22260;&#20174;39&#20010;&#24635;&#32447;&#21040;162&#20010;&#24635;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms, especially Neural Networks (NNs), are a valuable tool used to approximate non-linear relationships, like the AC-Optimal Power Flow (AC-OPF), with considerable accuracy -- and achieving a speedup of several orders of magnitude when deployed for use. Often in power systems literature, the NNs are trained with a fixed dataset generated prior to the training process. In this paper, we show that adapting the NN training dataset during training can improve the NN performance and substantially reduce its worst-case violations. This paper proposes an algorithm that identifies and enriches the training dataset with critical datapoints that reduce the worst-case violations and deliver a neural network with improved worst-case performance guarantees. We demonstrate the performance of our algorithm in four test power systems, ranging from 39-buses to 162-buses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30456;&#21464;&#29616;&#35937;&#30340;&#31616;&#21333;&#35299;&#37322;&#65292;&#21033;&#29992;&#21015;&#34920;&#35793;&#30721;&#22120;&#24314;&#27169;&#65292;&#23427;&#33021;&#22815;&#20445;&#35777;&#22312;LLM&#20302;&#20110;&#20020;&#30028;&#38408;&#20540;&#26102;&#38169;&#35823;&#20505;&#36873;&#24207;&#21015;&#25968;&#30340;&#26399;&#26395;&#20445;&#25345;&#26377;&#30028;&#65292;&#32780;&#22312;&#39640;&#20110;&#35813;&#38408;&#20540;&#26102;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2303.13112</link><description>&lt;p&gt;
&#21033;&#29992;&#21015;&#34920;&#35793;&#30721;&#35299;&#37322;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30456;&#21464;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
A Simple Explanation for the Phase Transition in Large Language Models with List Decoding. (arXiv:2303.13112v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30456;&#21464;&#29616;&#35937;&#30340;&#31616;&#21333;&#35299;&#37322;&#65292;&#21033;&#29992;&#21015;&#34920;&#35793;&#30721;&#22120;&#24314;&#27169;&#65292;&#23427;&#33021;&#22815;&#20445;&#35777;&#22312;LLM&#20302;&#20110;&#20020;&#30028;&#38408;&#20540;&#26102;&#38169;&#35823;&#20505;&#36873;&#24207;&#21015;&#25968;&#30340;&#26399;&#26395;&#20445;&#25345;&#26377;&#30028;&#65292;&#32780;&#22312;&#39640;&#20110;&#35813;&#38408;&#20540;&#26102;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21576;&#29616;&#20986;&#23567;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#31361;&#20986;&#33021;&#21147;&#12290;&#24403;&#27169;&#22411;&#36798;&#21040;&#19968;&#23450;&#30340;&#35268;&#27169;&#20851;&#38190;&#28857;&#26102;&#65292;&#31995;&#32479;&#24615;&#33021;&#24471;&#21040;&#20102;&#26497;&#22823;&#22320;&#25552;&#39640;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#37322;&#65292;&#24182;&#23558;LLM&#24314;&#27169;&#20026;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38543;&#26426;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#21015;&#34920;&#35793;&#30721;&#22120;&#20195;&#26367;&#27599;&#20010;&#27493;&#39588;&#30340;&#21363;&#26102;&#29983;&#25104;&#65292;&#35813;&#35793;&#30721;&#22120;&#22312;&#27599;&#20010;&#27493;&#39588;&#20445;&#30041;&#19968;&#20010;&#20505;&#36873;&#24207;&#21015;&#21015;&#34920;&#65292;&#24182;&#22312;&#32467;&#26463;&#26102;&#25512;&#36831;&#36755;&#20986;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23384;&#22312;&#19968;&#20010;&#20020;&#30028;&#38408;&#20540;&#65292;&#24403;LLM&#20302;&#20110;&#27492;&#38408;&#20540;&#26102;&#65292;&#26399;&#26395;&#30340;&#38169;&#35823;&#20505;&#36873;&#24207;&#21015;&#25968;&#20445;&#25345;&#26377;&#30028;&#65292;&#24403;LLM&#39640;&#20110;&#27492;&#38408;&#20540;&#26102;&#65292;&#26399;&#26395;&#38169;&#35823;&#24207;&#21015;&#25968;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#36825;&#26679;&#30340;&#38408;&#20540;&#19982;&#20256;&#26579;&#30149;&#30340;&#22522;&#26412;&#32321;&#27542;&#25968;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various recent experimental results show that large language models (LLM) exhibit emergent abilities that are not present in small models. System performance is greatly improved after passing a certain critical threshold of scale. In this letter, we provide a simple explanation for such a phase transition phenomenon. For this, we model an LLM as a sequence-to-sequence random function. Instead of using instant generation at each step, we use a list decoder that keeps a list of candidate sequences at each step and defers the generation of the output sequence at the end. We show that there is a critical threshold such that the expected number of erroneous candidate sequences remains bounded when an LLM is below the threshold, and it grows exponentially when an LLM is above the threshold. Such a threshold is related to the basic reproduction number in a contagious disease.
&lt;/p&gt;</description></item><item><title>PAC-MOO&#26159;&#19968;&#20010;&#20559;&#22909;&#24863;&#30693;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22312;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#21644;&#20174;&#19994;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#20559;&#22909;&#19979;&#65292;&#22823;&#37096;&#20998;&#36755;&#20837;&#31354;&#38388;&#26159;&#19981;&#21487;&#34892;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13034</link><description>&lt;p&gt;
&#20559;&#22909;&#24863;&#30693;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference-Aware Constrained Multi-Objective Bayesian Optimization. (arXiv:2303.13034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13034
&lt;/p&gt;
&lt;p&gt;
PAC-MOO&#26159;&#19968;&#20010;&#20559;&#22909;&#24863;&#30693;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22312;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#21644;&#20174;&#19994;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#20559;&#22909;&#19979;&#65292;&#22823;&#37096;&#20998;&#36755;&#20837;&#31354;&#38388;&#26159;&#19981;&#21487;&#34892;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#22823;&#37096;&#20998;&#36755;&#20837;&#31354;&#38388;&#26159;&#19981;&#21487;&#34892;&#65288;&#21363;&#36829;&#21453;&#32422;&#26463;&#26465;&#20214;&#65289;&#26102;&#65292;&#22522;&#20110;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#21644;&#20174;&#19994;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#20559;&#22909;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#35768;&#22810;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#20013;&#37117;&#23384;&#22312;&#65292;&#21253;&#25324;&#27169;&#25311;&#30005;&#36335;&#21644;&#30005;&#21147;&#31995;&#32479;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#22312;&#21487;&#34892;&#30340;&#36755;&#20837;&#35774;&#35745;&#30340;&#23567;&#37096;&#20998;&#19978;&#36817;&#20284;&#26368;&#20248;Pareto&#38598;&#21512;&#12290;&#20027;&#35201;&#25361;&#25112;&#21253;&#25324;&#35774;&#35745;&#31354;&#38388;&#30340;&#24040;&#22823;&#22823;&#23567;&#12289;&#22810;&#20010;&#30446;&#26631;&#21644;&#22823;&#37327;&#30340;&#32422;&#26463;&#26465;&#20214;&#20197;&#21450;&#21482;&#33021;&#22312;&#36827;&#34892;&#26114;&#36149;&#30340;&#20223;&#30495;&#21518;&#25165;&#33021;&#30830;&#35748;&#30340;&#21487;&#34892;&#30340;&#36755;&#20837;&#35774;&#35745;&#30340;&#23567;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#20559;&#22909;&#24863;&#30693;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65288;PAC-MOO&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23398;&#20064;&#36755;&#20986;&#30446;&#26631;&#21644;&#32422;&#26463;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20174;&#19994;&#32773;&#39044;&#27979;&#30340;&#20559;&#22909;&#36873;&#25321;&#35780;&#20272;&#30446;&#26631;&#30340;&#20505;&#36873;&#36755;&#20837;&#12290;PAC-MOO&#26681;&#25454;&#39044;&#27979;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#30340;&#32852;&#21512;&#20559;&#22909;&#36845;&#20195;&#22320;&#36873;&#25321;&#19979;&#19968;&#20010;&#35201;&#27169;&#25311;&#30340;&#36755;&#20837;&#35774;&#35745;&#65292;&#24182;&#20351;&#29992;&#26032;&#33719;&#24471;&#30340;&#25968;&#25454;&#26356;&#26032;&#20195;&#29702;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of constrained multi-objective optimization over black-box objective functions with practitioner-specified preferences over the objectives when a large fraction of the input space is infeasible (i.e., violates constraints). This problem arises in many engineering design problems including analog circuits and electric power system design. Our overall goal is to approximate the optimal Pareto set over the small fraction of feasible input designs. The key challenges include the huge size of the design space, multiple objectives and large number of constraints, and the small fraction of feasible input designs which can be identified only after performing expensive simulations. We propose a novel and efficient preference-aware constrained multi-objective Bayesian optimization approach referred to as PAC-MOO to address these challenges. The key idea is to learn surrogate models for both output objectives and constraints, and select the candidate input for eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CIPNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#23548;&#20986;&#36830;&#32493;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#35299;&#26512;&#35299;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;CIPAE&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#26041;&#27861;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12964</link><description>&lt;p&gt;
&#36830;&#32493;&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous Indeterminate Probability Neural Network. (arXiv:2303.12964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CIPNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#23548;&#20986;&#36830;&#32493;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#35299;&#26512;&#35299;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;CIPAE&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#26041;&#27861;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;CIPNN&#65288;Continuous Indeterminate Probability Neural Network&#65289;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;IPNN&#65292;&#29992;&#20110;&#31163;&#25955;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#12290;&#30446;&#21069;&#65292;&#36830;&#32493;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#65292;&#20294;&#26159;IPNN&#25552;&#20986;&#20102;&#26032;&#30340;&#29702;&#35770;&#65292;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#22235;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#36830;&#32493;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#21518;&#39564;&#35745;&#31639;&#30340;&#35299;&#26512;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27169;&#22411;&#65288;CIPNN&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#33258;&#32534;&#30721;&#22120;&#8212;&#8212;CIPAE&#65288;Continuous Indeterminate Probability Auto-Encoder&#65289;&#65292;&#20854;&#20013;&#35299;&#30721;&#22120;&#37096;&#20998;&#19981;&#26159;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#26159;&#31532;&#19968;&#27425;&#20351;&#29992;&#20840;&#27010;&#29575;&#25512;&#29702;&#27169;&#22411;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;N&#32500;&#28508;&#22312;&#21464;&#37327;&#20043;&#19968;&#20316;&#20026;&#35299;&#30721;&#22120;&#26469;&#37325;&#24314;&#36755;&#20837;&#22270;&#20687;&#65292;&#21363;&#20351;&#26159;&#20998;&#31867;&#20219;&#21153;&#20063;&#33021;&#36798;&#21040;&#25928;&#26524;&#65292;&#36825;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#30475;&#21040;&#27599;&#20010;&#28508;&#22312;&#21464;&#37327;&#20195;&#34920;&#20160;&#20040;&#12290;&#31532;&#22235;&#65292;&#25105;&#20204;&#36890;&#36807;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a general model called CIPNN - Continuous Indeterminate Probability Neural Network, and this model is based on IPNN, which is used for discrete latent random variables. Currently, posterior of continuous latent variables is regarded as intractable, with the new theory proposed by IPNN this problem can be solved. Our contributions are Four-fold. First, we derive the analytical solution of the posterior calculation of continuous latent random variables and propose a general classification model (CIPNN). Second, we propose a general auto-encoder called CIPAE - Continuous Indeterminate Probability Auto-Encoder, the decoder part is not a neural network and uses a fully probabilistic inference model for the first time. Third, we propose a new method to visualize the latent random variables, we use one of N dimensional latent variables as a decoder to reconstruct the input image, which can work even for classification tasks, in this way, we can see what each latent varia
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;MDP&#20998;&#35299;&#20026;&#22806;&#29983;&#21644;&#20869;&#29983;&#20004;&#20010;&#37096;&#20998;&#65292;&#20248;&#21270;&#20869;&#29983;&#22870;&#21169;&#65292;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#20869;&#29983;&#21644;&#22806;&#29983;&#29366;&#24577;&#31354;&#38388;&#27809;&#26377;&#20107;&#20808;&#32473;&#20986;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#27491;&#30830;&#30340;&#31639;&#27861;&#36827;&#34892;&#33258;&#21160;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.12957</link><description>&lt;p&gt;
&#20855;&#26377;&#22806;&#37096;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Exogenous States and Rewards. (arXiv:2303.12957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12957
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;MDP&#20998;&#35299;&#20026;&#22806;&#29983;&#21644;&#20869;&#29983;&#20004;&#20010;&#37096;&#20998;&#65292;&#20248;&#21270;&#20869;&#29983;&#22870;&#21169;&#65292;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#20869;&#29983;&#21644;&#22806;&#29983;&#29366;&#24577;&#31354;&#38388;&#27809;&#26377;&#20107;&#20808;&#32473;&#20986;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#27491;&#30830;&#30340;&#31639;&#27861;&#36827;&#34892;&#33258;&#21160;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#29366;&#24577;&#21464;&#37327;&#21644;&#22870;&#21169;&#20250;&#36890;&#36807;&#21521;&#22870;&#21169;&#20449;&#21495;&#27880;&#20837;&#19981;&#21487;&#25511;&#30340;&#21464;&#21270;&#32780;&#20943;&#24930;&#24378;&#21270;&#23398;&#20064;&#30340;&#36895;&#24230;&#12290;&#26412;&#25991;&#23545;&#22806;&#37096;&#29366;&#24577;&#21464;&#37327;&#21644;&#22870;&#21169;&#36827;&#34892;&#20102;&#27491;&#24335;&#21270;&#65292;&#24182;&#34920;&#26126;&#22914;&#26524;&#22870;&#21169;&#20989;&#25968;&#21152;&#27861;&#20998;&#35299;&#25104;&#20869;&#29983;&#21644;&#22806;&#29983;&#20004;&#20010;&#37096;&#20998;&#65292;MDP&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#20010;&#22806;&#29983;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#36807;&#31243;&#65288;&#22522;&#20110;&#22806;&#37096;&#22870;&#21169;&#65289;&#21644;&#19968;&#20010;&#20869;&#29983;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;&#20248;&#21270;&#20869;&#29983;&#22870;&#21169;&#65289;&#12290;&#20869;&#29983;MDP&#30340;&#20219;&#20309;&#26368;&#20248;&#31574;&#30053;&#20063;&#26159;&#21407;&#22987;MDP&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#20294;&#30001;&#20110;&#20869;&#29983;&#22870;&#21169;&#36890;&#24120;&#20855;&#26377;&#38477;&#20302;&#30340;&#26041;&#24046;&#65292;&#22240;&#27492;&#20869;&#29983;MDP&#26356;&#23481;&#26131;&#27714;&#35299;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29366;&#24577;&#31354;&#38388;&#20998;&#35299;&#20026;&#20869;&#22806;&#29983;&#29366;&#24577;&#31354;&#38388;&#30340;&#24773;&#20917;&#65292;&#32780;&#36825;&#31181;&#29366;&#24577;&#31354;&#38388;&#20998;&#35299;&#24182;&#27809;&#26377;&#32473;&#20986;&#65292;&#32780;&#26159;&#24517;&#39035;&#21457;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#24182;&#35777;&#26126;&#20102;&#22312;&#32447;&#24615;&#32452;&#21512;&#19979;&#21457;&#29616;&#20869;&#29983;&#21644;&#22806;&#29983;&#29366;&#24577;&#31354;&#38388;&#30340;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exogenous state variables and rewards can slow reinforcement learning by injecting uncontrolled variation into the reward signal. This paper formalizes exogenous state variables and rewards and shows that if the reward function decomposes additively into endogenous and exogenous components, the MDP can be decomposed into an exogenous Markov Reward Process (based on the exogenous reward) and an endogenous Markov Decision Process (optimizing the endogenous reward). Any optimal policy for the endogenous MDP is also an optimal policy for the original MDP, but because the endogenous reward typically has reduced variance, the endogenous MDP is easier to solve. We study settings where the decomposition of the state space into exogenous and endogenous state spaces is not given but must be discovered. The paper introduces and proves correctness of algorithms for discovering the exogenous and endogenous subspaces of the state space when they are mixed through linear combination. These algorithms
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#22522;&#20110;&#20805;&#20998;&#32479;&#35745;&#37327;&#30340;&#36890;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#26494;&#24347;&#27714;&#21644;&#35201;&#27714;&#24182;&#20165;&#35201;&#27714;&#20989;&#25968;&#37325;&#26500;&#38543;&#26426;&#21464;&#37327;X&#65292;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#25968;&#25454;&#31232;&#21270;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#21487;&#36827;&#34892;&#31232;&#21270;&#30340;&#20998;&#24067;&#26063;&#65292;&#24182;&#32479;&#19968;&#20102;&#26679;&#26412;&#20998;&#35010;&#21644;&#25968;&#25454;&#31232;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.12931</link><description>&lt;p&gt;
&#22522;&#20110;&#20805;&#20998;&#32479;&#35745;&#37327;&#30340;&#24191;&#20041;&#25968;&#25454;&#31232;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalized Data Thinning Using Sufficient Statistics. (arXiv:2303.12931v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#22522;&#20110;&#20805;&#20998;&#32479;&#35745;&#37327;&#30340;&#36890;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#26494;&#24347;&#27714;&#21644;&#35201;&#27714;&#24182;&#20165;&#35201;&#27714;&#20989;&#25968;&#37325;&#26500;&#38543;&#26426;&#21464;&#37327;X&#65292;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#25968;&#25454;&#31232;&#21270;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#21487;&#36827;&#34892;&#31232;&#21270;&#30340;&#20998;&#24067;&#26063;&#65292;&#24182;&#32479;&#19968;&#20102;&#26679;&#26412;&#20998;&#35010;&#21644;&#25968;&#25454;&#31232;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#23558;&#38543;&#26426;&#21464;&#37327;X&#20998;&#35299;&#20026;&#22810;&#20010;&#29420;&#31435;&#38543;&#26426;&#21464;&#37327;&#30340;&#36890;&#29992;&#31574;&#30053;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#20219;&#20309;&#26377;&#20851;&#26410;&#30693;&#21442;&#25968;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#26494;&#24347;&#27714;&#21644;&#35201;&#27714;&#24182;&#20165;&#35201;&#27714;&#19968;&#20123;&#24050;&#30693;&#30340;&#29420;&#31435;&#38543;&#26426;&#21464;&#37327;&#30340;&#20989;&#25968;&#23436;&#20840;&#37325;&#26500;X&#26469;&#25512;&#24191;&#20102;&#26368;&#36817;&#19968;&#31687;&#35770;&#25991;&#30340;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#30340;&#25512;&#24191;&#26377;&#20004;&#20010;&#30446;&#30340;&#12290;&#39318;&#20808;&#65292;&#23427;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#21487;&#36827;&#34892;&#31232;&#21270;&#30340;&#20998;&#24067;&#26063;&#12290;&#20854;&#27425;&#65292;&#23427;&#32479;&#19968;&#20102;&#26679;&#26412;&#20998;&#35010;&#21644;&#25968;&#25454;&#31232;&#21270;&#65292;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20284;&#20046;&#38750;&#24120;&#19981;&#21516;&#65292;&#20294;&#24212;&#29992;&#20102;&#21516;&#26679;&#30340;&#21407;&#29702;&#12290;&#36825;&#20010;&#20849;&#21516;&#30340;&#21407;&#29702;&#26159;&#20805;&#20998;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35748;&#35782;&#23545;&#21508;&#31181;&#19981;&#21516;&#30340;&#23478;&#26063;&#36827;&#34892;&#24191;&#20041;&#31232;&#30095;&#21270;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our goal is to develop a general strategy to decompose a random variable $X$ into multiple independent random variables, without sacrificing any information about unknown parameters. A recent paper showed that for some well-known natural exponential families, $X$ can be "thinned" into independent random variables $X^{(1)}, \ldots, X^{(K)}$, such that $X = \sum_{k=1}^K X^{(k)}$. In this paper, we generalize their procedure by relaxing this summation requirement and simply asking that some known function of the independent random variables exactly reconstruct $X$. This generalization of the procedure serves two purposes. First, it greatly expands the families of distributions for which thinning can be performed. Second, it unifies sample splitting and data thinning, which on the surface seem to be very different, as applications of the same principle. This shared principle is sufficiency. We use this insight to perform generalized thinning operations for a diverse set of families.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#22312;&#38750;&#20984;&#26465;&#20214;&#19979;&#20351;&#29992;&#28145;&#23618;&#27169;&#22411;&#21644;&#26356;&#22797;&#26434;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12922</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Fragility of Influence Functions. (arXiv:2303.12922v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#22312;&#38750;&#20984;&#26465;&#20214;&#19979;&#20351;&#29992;&#28145;&#23618;&#27169;&#22411;&#21644;&#26356;&#22797;&#26434;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26377;&#24456;&#22810;&#35770;&#25991;&#33268;&#21147;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#39564;&#35777;&#36825;&#20123;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#25110;&#21487;&#20449;&#24230;&#12290;&#26368;&#36817;&#65292;&#24433;&#21709;&#20989;&#25968;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21333;&#20010;&#26679;&#26412;&#19978;&#30340;&#28789;&#25935;&#24230;&#30340;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#24433;&#21709;&#20989;&#25968;&#26131;&#21463;&#22122;&#22768;&#21644;&#25968;&#25454;&#20998;&#24067;&#19981;&#23545;&#31216;&#24615;&#24433;&#21709;&#65292;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25506;&#31350;&#24433;&#21709;&#20989;&#25968;&#32972;&#21518;&#30340;&#26426;&#29702;&#65292;&#20174;&#32780;&#20026;&#22686;&#24378;&#24433;&#21709;&#20989;&#25968;&#30340;&#40065;&#26834;&#24615;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, many works have tried to explain the predictions of deep learning models. Few methods, however, have been proposed to verify the accuracy or faithfulness of these explanations. Recently, influence functions, which is a method that approximates the effect that leave-one-out training has on the loss function, has been shown to be fragile. The proposed reason for their fragility remains unclear. Although previous work suggests the use of regularization to increase robustness, this does not hold in all cases. In this work, we seek to investigate the experiments performed in the prior work in an effort to understand the underlying mechanisms of influence function fragility. First, we verify influence functions using procedures from the literature under conditions where the convexity assumptions of influence functions are met. Then, we relax these assumptions and study the effects of non-convexity by using deeper models and more complex datasets. Here, we analyze the k
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#25490;&#24207;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#40065;&#26834;&#24615;&#20849;&#35782;&#38382;&#39064;&#20197;&#21450;&#35813;&#38382;&#39064;&#30456;&#20851;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#20854;&#20013;Consensus Ranking&#38382;&#39064;&#26159;&#37325;&#28857;&#65292;&#26088;&#22312;&#36890;&#36807;&#20013;&#20301;&#25968;&#25490;&#21517;&#26469;&#24635;&#32467;&#25490;&#21015;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.12878</link><description>&lt;p&gt;
&#25490;&#24207;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#40065;&#26834;&#24615;&#20849;&#35782;&#65306;&#23450;&#20041;&#12289;&#23646;&#24615;&#21644;&#35745;&#31639;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Robust Consensus in Ranking Data Analysis: Definitions, Properties and Computational Issues. (arXiv:2303.12878v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#25490;&#24207;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#40065;&#26834;&#24615;&#20849;&#35782;&#38382;&#39064;&#20197;&#21450;&#35813;&#38382;&#39064;&#30456;&#20851;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#20854;&#20013;Consensus Ranking&#38382;&#39064;&#26159;&#37325;&#28857;&#65292;&#26088;&#22312;&#36890;&#36807;&#20013;&#20301;&#25968;&#25490;&#21517;&#26469;&#24635;&#32467;&#25490;&#21015;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#40065;&#26834;&#24615;&#38382;&#39064;&#26085;&#30410;&#31361;&#20986;&#65292;&#38656;&#35201;&#24320;&#21457;&#21487;&#38752;&#30340;&#32479;&#35745;&#23398;&#20064;&#25216;&#26415;&#65292;&#21363;&#20351;&#22312;&#37096;&#20998;&#21463;&#25439;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#30830;&#20445;&#21487;&#38752;&#24615;&#12290;&#20559;&#22909;&#25968;&#25454;&#20197; (&#23436;&#25972;) &#25490;&#24207;&#30340;&#24418;&#24335;&#20986;&#29616;&#26102;&#20063;&#19981;&#20363;&#22806;&#65292;&#23588;&#20854;&#26159;&#39281;&#21463;&#27492;&#31867;&#25968;&#25454;&#25903;&#25345;&#25110;&#20135;&#29983;&#30340;&#25216;&#26415;(&#20363;&#22914;&#65292;&#25628;&#32034;&#24341;&#25806;&#65292;&#25512;&#33616;&#31995;&#32479;)&#22823;&#35268;&#27169;&#37096;&#32626;&#20043;&#26102;&#65292;&#38656;&#35201;&#30456;&#24212;&#30340;&#27010;&#24565;&#21644;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25490;&#21015;&#30340;&#38598;&#21512; (&#21363;&#23545;&#31216;&#32676; $\mathfrak{S}_n$) &#27809;&#26377;&#21521;&#37327;&#31354;&#38388;&#32467;&#26500;&#65292;&#19988;&#25490;&#24207;&#25968;&#25454;&#20998;&#26512;&#20013;&#32771;&#34385;&#30340;&#32479;&#35745;&#37327;&#30340;&#22797;&#26434;&#24615;&#65292;&#20351;&#24471;&#22312;&#35813;&#39046;&#22495;&#20013;&#21046;&#23450;&#40065;&#26834;&#24615;&#30446;&#26631;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#40065;&#26834;&#24615;&#27010;&#24565;&#20197;&#21450;&#19987;&#29992;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#38024;&#23545;&#25490;&#21517;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#26071;&#33328;&#38382;&#39064;&#65306;Consensus Ranking&#65292;&#26088;&#22312;&#36890;&#36807;&#20013;&#20301;&#25968;&#25490;&#21517;&#26469;&#24635;&#32467;&#25490;&#21015;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the issue of robustness in AI systems becomes vital, statistical learning techniques that are reliable even in presence of partly contaminated data have to be developed. Preference data, in the form of (complete) rankings in the simplest situations, are no exception and the demand for appropriate concepts and tools is all the more pressing given that technologies fed by or producing this type of data (e.g. search engines, recommending systems) are now massively deployed. However, the lack of vector space structure for the set of rankings (i.e. the symmetric group $\mathfrak{S}_n$) and the complex nature of statistics considered in ranking data analysis make the formulation of robustness objectives in this domain challenging. In this paper, we introduce notions of robustness, together with dedicated statistical methods, for Consensus Ranking the flagship problem in ranking data analysis, aiming at summarizing a probability distribution on $\mathfrak{S}_n$ by a median ranking. Precise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#27973;&#23618;&#27979;&#37327;&#21482;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32416;&#32544;&#38376;&#12290;&#34429;&#28982;&#35813;&#31867;&#26694;&#26550;&#20026;&#25105;&#20204;&#22312;&#19981;&#21516;&#29289;&#29702;&#24179;&#21488;&#20043;&#38388;&#36716;&#31227;&#37327;&#23376;&#36807;&#31243;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12834</link><description>&lt;p&gt;
&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#23398;&#20064;&#30340;&#33021;&#21147;&#19982;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
The power and limitations of learning quantum dynamics incoherently. (arXiv:2303.12834v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#27973;&#23618;&#27979;&#37327;&#21482;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32416;&#32544;&#38376;&#12290;&#34429;&#28982;&#35813;&#31867;&#26694;&#26550;&#20026;&#25105;&#20204;&#22312;&#19981;&#21516;&#29289;&#29702;&#24179;&#21488;&#20043;&#38388;&#36716;&#31227;&#37327;&#23376;&#36807;&#31243;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#36807;&#31243;&#23398;&#20064;&#26159;&#30740;&#31350;&#37327;&#23376;&#31995;&#32479;&#30340;&#37325;&#35201;&#24037;&#20855;&#20043;&#19968;&#12290;&#28982;&#32780;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#25918;&#22312;&#20102;&#33258;&#26059;&#30456;&#24178;&#21644;&#22120;&#20214;&#33258;&#32806;&#21512;&#30340;&#27874;&#21160;&#20989;&#25968;&#19978;&#65292;&#30740;&#31350;&#37327;&#23376;&#21160;&#21147;&#23398;&#22312;&#31995;&#32479;&#21644;&#30446;&#26631;&#19981;&#30452;&#25509;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#21487;&#20197;&#34987;&#23398;&#20064;&#24182;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#36825;&#31867;&#19981;&#30456;&#24178;&#30340;&#26694;&#26550;&#23454;&#38469;&#19978;&#38750;&#24120;&#21560;&#24341;&#20154;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#25361;&#25112;&#24615;&#30340;&#28151;&#21512;&#32416;&#32544;&#26041;&#26696;&#20013;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#22312;&#19981;&#21516;&#29289;&#29702;&#24179;&#21488;&#20043;&#38388;&#36716;&#31227;&#37327;&#23376;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#38656;&#35201;&#20223;&#30495;&#30340;&#26126;&#30830;&#30340;&#30456;&#24178;&#23398;&#20064;&#31574;&#30053;&#30340;&#27979;&#37327;&#27425;&#25968;&#65292;&#25552;&#20379;&#20102;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#19979;&#23398;&#20064;&#24186;&#27491;&#36807;&#31243;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#20801;&#35768;&#20219;&#24847;&#27979;&#37327;&#65292;&#21017;&#20219;&#20309;&#26377;&#25928;&#34920;&#31034;&#30340;&#24186;&#27491;&#30697;&#38453;&#37117;&#21487;&#20197;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#20869;&#34987;&#26377;&#25928;&#22320;&#23398;&#20064;&#65307;&#28982;&#32780;&#65292;&#22914;&#26524;&#20165;&#38480;&#20110;&#27973;&#23618;&#27979;&#37327;&#65292;&#21017;&#21482;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32416;&#32544;&#38376;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#20013;&#30340;&#33021;&#21147;&#19982;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum process learning is emerging as an important tool to study quantum systems. While studied extensively in coherent frameworks, where the target and model system can share quantum information, less attention has been paid to whether the dynamics of quantum systems can be learned without the system and target directly interacting. Such incoherent frameworks are practically appealing since they open up methods of transpiling quantum processes between the different physical platforms without the need for technically challenging hybrid entanglement schemes. Here we provide bounds on the sample complexity of learning unitary processes incoherently by analyzing the number of measurements that are required to emulate well-established coherent learning strategies. We prove that if arbitrary measurements are allowed, then any efficiently representable unitary can be efficiently learned within the incoherent framework; however, when restricted to shallow-depth measurements only low-entangl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;&#23545;&#25968;S&#22411;&#28608;&#27963;&#20989;&#25968;&#30340;&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#26368;&#22810;&#21482;&#26377;&#19977;&#20010;&#19981;&#21160;&#28857;&#65292;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#21644;&#29702;&#35770;&#20043;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#24517;&#35201;&#30340;&#26725;&#26753;&#12290;</title><link>http://arxiv.org/abs/2303.12814</link><description>&lt;p&gt;
&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21160;&#28857;
&lt;/p&gt;
&lt;p&gt;
Fixed points of arbitrarily deep 1-dimensional neural networks. (arXiv:2303.12814v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;&#23545;&#25968;S&#22411;&#28608;&#27963;&#20989;&#25968;&#30340;&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#26368;&#22810;&#21482;&#26377;&#19977;&#20010;&#19981;&#21160;&#28857;&#65292;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#21644;&#29702;&#35770;&#20043;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#24517;&#35201;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;$\mathbb{R}$&#19978;&#20855;&#26377;&#21512;&#25104;&#24615;&#19988;&#21253;&#21547;&#23545;&#25968;S&#22411;&#20989;&#25968;&#30340;&#26032;&#20989;&#25968;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#31867;&#26469;&#35777;&#26126;&#20855;&#26377;&#23545;&#25968;S&#22411;&#28608;&#27963;&#20989;&#25968;&#30340;&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#26368;&#22810;&#21482;&#26377;&#19977;&#20010;&#19981;&#21160;&#28857;&#12290;&#34429;&#28982;&#36825;&#26679;&#30340;&#31070;&#32463;&#32593;&#32476;&#36828;&#31163;&#23454;&#38469;&#24212;&#29992;&#65292;&#20294;&#25105;&#20204;&#33021;&#22815;&#23436;&#20840;&#29702;&#35299;&#23427;&#20204;&#30340;&#19981;&#21160;&#28857;&#65292;&#24182;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#21644;&#29702;&#35770;&#20043;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#24517;&#35201;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new class of functions on $\mathbb{R}$ that is closed under composition, and contains the logistic sigmoid function. We use this class to show that any 1-dimensional neural network of arbitrary depth with logistic sigmoid activation functions has at most three fixed points. While such neural networks are far from real world applications, we are able to completely understand their fixed points, providing a foundation to the much needed connection between application and theory of deep neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Langevin&#22411;&#31639;&#27861;&#24182;&#24212;&#29992;&#20110;&#21513;&#24067;&#26031;&#20998;&#24067;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;2-Wasserstein&#36317;&#31163;&#19978;&#38480;&#65292;&#25105;&#20204;&#21457;&#29616;&#21183;&#20989;&#25968;&#30340;&#32791;&#25955;&#24615;&#20197;&#21450;&#26799;&#24230; $\alpha&gt;1/3$ &#19979;&#30340; $\alpha$-H\"{o}lder&#36830;&#32493;&#24615;&#21487;&#20197;&#20445;&#35777;&#31639;&#27861;&#20855;&#26377;&#25509;&#36817;&#38646;&#30340;&#35823;&#24046;&#12290;&#26032;&#30340;Langevin&#22411;&#31639;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#26080;&#20984;&#24615;&#25110;&#36830;&#32493;&#21487;&#24494;&#24615;&#30340;&#21183;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.12407</link><description>&lt;p&gt;
Langevin&#22411;Monte Carlo&#31639;&#27861;&#30340;&#38750;&#28176;&#36827;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Non-asymptotic analysis of Langevin-type Monte Carlo algorithms. (arXiv:2303.12407v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Langevin&#22411;&#31639;&#27861;&#24182;&#24212;&#29992;&#20110;&#21513;&#24067;&#26031;&#20998;&#24067;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;2-Wasserstein&#36317;&#31163;&#19978;&#38480;&#65292;&#25105;&#20204;&#21457;&#29616;&#21183;&#20989;&#25968;&#30340;&#32791;&#25955;&#24615;&#20197;&#21450;&#26799;&#24230; $\alpha&gt;1/3$ &#19979;&#30340; $\alpha$-H\"{o}lder&#36830;&#32493;&#24615;&#21487;&#20197;&#20445;&#35777;&#31639;&#27861;&#20855;&#26377;&#25509;&#36817;&#38646;&#30340;&#35823;&#24046;&#12290;&#26032;&#30340;Langevin&#22411;&#31639;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#26080;&#20984;&#24615;&#25110;&#36830;&#32493;&#21487;&#24494;&#24615;&#30340;&#21183;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Langevin&#22411;&#31639;&#27861;&#24212;&#29992;&#20110;&#21513;&#24067;&#26031;&#20998;&#24067;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#21183;&#20989;&#25968;&#26159;&#32791;&#25955;&#30340;&#65292;&#19988;&#20854;&#24369;&#26799;&#24230;&#20855;&#26377;&#26377;&#38480;&#30340;&#36830;&#32493;&#24615;&#27169;&#37327;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;2-Wasserstein&#36317;&#31163;&#19978;&#38480;&#30340;&#38750;&#28176;&#36827;&#24615;&#65292;&#23427;&#34913;&#37327;&#20102;&#21513;&#24067;&#26031;&#20998;&#24067;&#19982;&#22522;&#20110;Liptser-Shiryaev&#29702;&#35770;&#21644;&#20989;&#25968;&#19981;&#31561;&#24335;&#30340;Langevin&#22411;&#31639;&#27861;&#30340;&#19968;&#33324;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#24212;&#29992;&#36825;&#20010;&#19978;&#38480;&#26469;&#23637;&#31034;&#21183;&#20989;&#25968;&#30340;&#32791;&#25955;&#24615;&#20197;&#21450;&#26799;&#24230; $\alpha&gt;1/3$ &#19979;&#30340; $\alpha$-H\"{o}lder&#36830;&#32493;&#24615;&#26159;&#20805;&#20998;&#30340;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#25511;&#21046;&#21442;&#25968;&#26469;&#33719;&#24471;Langevin Monte Carlo&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#38024;&#23545;&#26080;&#20984;&#24615;&#25110;&#36830;&#32493;&#21487;&#24494;&#24615;&#30340;&#21183;&#20989;&#25968;&#25552;&#20986;&#20102;&#29699;&#24418;&#24179;&#28369;&#25216;&#26415;&#30340;Langevin&#22411;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the Langevin-type algorithms for Gibbs distributions such that the potentials are dissipative and their weak gradients have the finite moduli of continuity. Our main result is a non-asymptotic upper bound of the 2-Wasserstein distance between the Gibbs distribution and the law of general Langevin-type algorithms based on the Liptser--Shiryaev theory and functional inequalities. We apply this bound to show that the dissipativity of the potential and the $\alpha$-H\"{o}lder continuity of the gradient with $\alpha&gt;1/3$ are sufficient for the convergence of the Langevin Monte Carlo algorithm with appropriate control of the parameters. We also propose Langevin-type algorithms with spherical smoothing for potentials without convexity or continuous differentiability.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.08431</link><description>&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#25910;&#25947;&#20110;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#30340;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators. (arXiv:2303.08431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#32773;&#21482;&#33719;&#24471;&#20102;&#38750;&#23436;&#25972;&#20449;&#24687;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#31995;&#32479;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21160;&#24577;&#31995;&#32479;&#65292;&#32467;&#21512;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#30001;&#30456;&#21516;&#32467;&#26500;&#30340;&#31574;&#30053;&#36827;&#34892;&#31649;&#29702;&#12290;&#22312;&#20551;&#35774;&#38750;&#32447;&#24615;&#32452;&#25104;&#37096;&#20998;&#21253;&#21547;&#20855;&#26377;&#23567;&#22411;Lipschitz&#31995;&#25968;&#30340;&#20869;&#26680;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#25104;&#26412;&#20989;&#25968;&#30340;&#20248;&#21270;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#34429;&#28982;&#25104;&#26412;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#20984;&#30340;&#65292;&#20294;&#25105;&#20204;&#30830;&#31435;&#20102;&#20840;&#23616;&#26368;&#20248;&#35299;&#38468;&#36817;&#23616;&#37096;&#30340;&#24378;&#20984;&#24615;&#21644;&#20809;&#28369;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21021;&#22987;&#21270;&#26426;&#21046;&#65292;&#20197;&#21033;&#29992;&#36825;&#20123;&#23646;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear control systems with partial information to the decision maker are prevalent in a variety of applications. As a step toward studying such nonlinear systems, this work explores reinforcement learning methods for finding the optimal policy in the nearly linear-quadratic regulator systems. In particular, we consider a dynamic system that combines linear and nonlinear components, and is governed by a policy with the same structure. Assuming that the nonlinear component comprises kernels with small Lipschitz coefficients, we characterize the optimization landscape of the cost function. Although the cost function is nonconvex in general, we establish the local strong convexity and smoothness in the vicinity of the global optimizer. Additionally, we propose an initialization mechanism to leverage these properties. Building on the developments, we design a policy gradient algorithm that is guaranteed to converge to the globally optimal policy with a linear rate.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20998;&#26512;&#22240;&#23376;&#21270;&#39640;&#26031;&#36924;&#36817;&#22312;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#20302;&#20272;&#25152;&#36924;&#36817;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#24403;&#29992;&#23545;&#35282;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#36924;&#36817;&#20855;&#26377;&#23494;&#38598;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#26102;&#65292;&#25152;&#25512;&#26029;&#30340;&#39640;&#26031;&#24635;&#26159;&#20302;&#20272;&#20102;&#21407;&#22987;&#39640;&#26031;&#30340;&#20998;&#37327;&#26041;&#24046;&#21644;&#29109;&#12290;</title><link>http://arxiv.org/abs/2302.09163</link><description>&lt;p&gt;
&#25910;&#32553;-&#35299;&#32806;&#24179;&#34913;&#65306;&#20998;&#26512;&#22240;&#23376;&#21270;&#39640;&#26031;&#36924;&#36817;&#22312;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Shrinkage-Delinkage Trade-off: An Analysis of Factorized Gaussian Approximations for Variational Inference. (arXiv:2302.09163v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09163
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20998;&#26512;&#22240;&#23376;&#21270;&#39640;&#26031;&#36924;&#36817;&#22312;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#20302;&#20272;&#25152;&#36924;&#36817;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#24403;&#29992;&#23545;&#35282;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#36924;&#36817;&#20855;&#26377;&#23494;&#38598;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#26102;&#65292;&#25152;&#25512;&#26029;&#30340;&#39640;&#26031;&#24635;&#26159;&#20302;&#20272;&#20102;&#21407;&#22987;&#39640;&#26031;&#30340;&#20998;&#37327;&#26041;&#24046;&#21644;&#29109;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#20351;&#29992;&#22240;&#23376;&#21270;&#36924;&#36817;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#20302;&#20272;&#23427;&#20204;&#29992;&#26469;&#36924;&#36817;&#30340;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22914;&#20197;&#21508;&#31181;&#26041;&#24335;&#27979;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20004;&#31181;&#34913;&#37327;VI&#19981;&#30830;&#23450;&#24615;&#20111;&#25439;&#30340;&#27969;&#34892;&#26041;&#27861;&#65306;&#65288;i&#65289;&#23427;&#20302;&#20272;&#20998;&#37327;&#26041;&#24046;&#30340;&#31243;&#24230;&#65292;&#65288;ii&#65289;&#23427;&#20302;&#20272;&#29109;&#30340;&#31243;&#24230;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#24433;&#21709;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;&#35774;&#32622;&#65292;&#21487;&#20197;&#22312;&#20854;&#20013;&#26126;&#30830;&#65288;&#21644;&#20248;&#38597;&#22320;&#65289;&#20998;&#26512;&#36825;&#20123;&#24433;&#21709;&#65306;&#20351;&#29992;&#23545;&#35282;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#65288;$q$&#65289;&#36924;&#36817;&#20855;&#26377;&#23494;&#38598;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#65288;$p$&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;$q$&#24635;&#26159;&#20302;&#20272;&#20102;$p$&#30340;&#20998;&#37327;&#26041;&#24046;&#21644;&#29109;&#65292;&#23613;&#31649;&#19981;&#19968;&#23450;&#20302;&#20272;&#30340;&#31243;&#24230;&#30456;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;$q$&#30340;&#29109;&#30001;&#20004;&#20010;&#30456;&#20114;&#31454;&#20105;&#30340;&#22240;&#32032;&#30340;&#24179;&#34913;&#20915;&#23450;&#65306;&#23427;&#30340;&#20998;&#37327;&#26041;&#24046;&#25910;&#32553;&#20250;&#38477;&#20302;&#23427;&#30340;&#29109;&#12290;
&lt;/p&gt;
&lt;p&gt;
When factorized approximations are used for variational inference (VI), they tend to underestimate the uncertainty -- as measured in various ways -- of the distributions they are meant to approximate. We consider two popular ways to measure the uncertainty deficit of VI: (i) the degree to which it underestimates the componentwise variance, and (ii) the degree to which it underestimates the entropy. To better understand these effects, and the relationship between them, we examine an informative setting where they can be explicitly (and elegantly) analyzed: the approximation of a Gaussian,~$p$, with a dense covariance matrix, by a Gaussian,~$q$, with a diagonal covariance matrix. We prove that $q$ always underestimates both the componentwise variance and the entropy of $p$, \textit{though not necessarily to the same degree}. Moreover we demonstrate that the entropy of $q$ is determined by the trade-off of two competing forces: it is decreased by the shrinkage of its componentwise varianc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20999;&#29255;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2212.08049</link><description>&lt;p&gt;
&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;
&lt;/p&gt;
&lt;p&gt;
Sliced Optimal Partial Transport. (arXiv:2212.08049v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20999;&#29255;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#24050;&#32463;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#31185;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21464;&#24471;&#26497;&#20854;&#27969;&#34892;&#12290;OT&#38382;&#39064;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#28304;&#21644;&#30446;&#26631;&#27979;&#24230;&#30340;&#24635;&#36136;&#37327;&#30456;&#31561;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#30340;&#24212;&#29992;&#12290;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#65288;OPT&#65289;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#30340;&#26041;&#27861;&#12290;&#19982;OT&#38382;&#39064;&#31867;&#20284;&#65292;OPT&#30340;&#35745;&#31639;&#20381;&#36182;&#20110;&#35299;&#20915;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65288;&#36890;&#24120;&#22312;&#39640;&#32500;&#24230;&#20013;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#21464;&#24471;&#35745;&#31639;&#19978;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;OPT&#38382;&#39064;&#30340;&#26377;&#25928;&#31639;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#36981;&#24490;&#20999;&#29255;OT&#36317;&#31163;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#21033;&#29992;&#20999;&#29255;&#23450;&#20041;&#20102;&#20999;&#29255;OPT&#36317;&#31163;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20999;&#29255;OPT-based&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#20540;&#23454;&#39564;&#20013;&#30340;&#35745;&#31639;&#21644;&#31934;&#24230;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;Sliced-OPT&#22312;&#22122;&#22768;&#28857;&#20113;&#37197;&#20934;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) has become exceedingly popular in machine learning, data science, and computer vision. The core assumption in the OT problem is the equal total amount of mass in source and target measures, which limits its application. Optimal Partial Transport (OPT) is a recently proposed solution to this limitation. Similar to the OT problem, the computation of OPT relies on solving a linear programming problem (often in high dimensions), which can become computationally prohibitive. In this paper, we propose an efficient algorithm for calculating the OPT problem between two non-negative measures in one dimension. Next, following the idea of sliced OT distances, we utilize slicing to define the sliced OPT distance. Finally, we demonstrate the computational and accuracy benefits of the sliced OPT-based method in various numerical experiments. In particular, we show an application of our proposed Sliced-OPT in noisy point cloud registration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#21457;&#29616;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#26159;grokking&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#8220;LU&#26426;&#21046;&#8221;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#31639;&#27861;&#25968;&#25454;&#38598;&#30340;grokking&#21644;&#28040;&#38500;&#20102;&#20854;grokking&#29616;&#35937;&#12290;&#23427;&#20204;&#30340;dramatic grokking&#20381;&#36182;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.01117</link><description>&lt;p&gt;
Omnigrok&#65306;&#29702;&#35299;&#36229;&#36234;&#31639;&#27861;&#25968;&#25454;&#30340;&#8220;Grokking&#8221;
&lt;/p&gt;
&lt;p&gt;
Omnigrok: Grokking Beyond Algorithmic Data. (arXiv:2210.01117v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#21457;&#29616;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#26159;grokking&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#8220;LU&#26426;&#21046;&#8221;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#31639;&#27861;&#25968;&#25454;&#38598;&#30340;grokking&#21644;&#28040;&#38500;&#20102;&#20854;grokking&#29616;&#35937;&#12290;&#23427;&#20204;&#30340;dramatic grokking&#20381;&#36182;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grokking&#26159;&#19968;&#31181;&#19981;&#23547;&#24120;&#30340;&#29616;&#35937;&#65292;&#25351;&#31639;&#27861;&#25968;&#25454;&#38598;&#22312;&#36807;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#21518;&#38271;&#26102;&#38388;&#20173;&#28982;&#33021;&#36827;&#34892;&#27867;&#21270;&#65292;&#19968;&#30452;&#20197;&#26469;&#19968;&#30452;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#26469;&#29702;&#35299;grokking&#65292;&#24182;&#30830;&#23450;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#26159;grokking&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;LU&#26426;&#21046;&#8221;&#65292;&#22240;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#65288;&#23545;&#27169;&#22411;&#26435;&#37325;&#35268;&#33539;&#65289;&#36890;&#24120;&#20998;&#21035;&#31867;&#20284;&#20110;&#8220;L&#8221;&#21644;&#8220;U&#8221;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#26426;&#21046;&#21487;&#20197;&#24456;&#22909;&#22320;&#35299;&#37322;grokking&#30340;&#35768;&#22810;&#26041;&#38754;&#65306;&#25968;&#25454;&#22823;&#23567;&#20381;&#36182;&#24615;&#12289;&#26435;&#37325;&#34928;&#20943;&#20381;&#36182;&#24615;&#12289;&#34920;&#31034;&#30340;&#20986;&#29616;&#31561;&#12290;&#22312;&#30452;&#35273;&#19978;&#32473;&#23450;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#28041;&#21450;&#22270;&#20687;&#12289;&#35821;&#35328;&#21644;&#20998;&#23376;&#30340;&#20219;&#21153;&#20013;&#35825;&#23548;grokking&#12290;&#21453;&#21521;&#26469;&#30475;&#65292;&#25105;&#20204;&#33021;&#22815;&#28040;&#38500;&#31639;&#27861;&#25968;&#25454;&#38598;&#30340;grokking&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#25968;&#25454;&#38598;&#30340;dramatic grokking&#24402;&#22240;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test losses as the cause for grokking. We refer to this as the "LU mechanism" because training and test losses (against model weight norm) typically resemble "L" and "U", respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules. In the reverse direction, we are able to eliminate grokking for algorithmic datasets. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24212;&#29992;&#20110;&#20174;&#21307;&#30103;&#20445;&#20581;&#25110;&#25945;&#32946;&#39046;&#22495;&#25910;&#38598;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#37096;&#20998;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#20013;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#35266;&#23519;&#21040;&#30340;&#34892;&#21160;&#21487;&#33021;&#21463;&#21040;&#26410;&#35266;&#23519;&#21040;&#30340;&#22240;&#32032;&#24433;&#21709;&#65292;&#23548;&#33268;&#20272;&#35745;&#20540;&#20986;&#29616;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2110.15332</link><description>&lt;p&gt;
&#36817;&#31471;&#24378;&#21270;&#23398;&#20064;&#65306;&#37096;&#20998;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#39640;&#25928;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in Partially Observed Markov Decision Processes. (arXiv:2110.15332v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24212;&#29992;&#20110;&#20174;&#21307;&#30103;&#20445;&#20581;&#25110;&#25945;&#32946;&#39046;&#22495;&#25910;&#38598;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#37096;&#20998;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#20013;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#35266;&#23519;&#21040;&#30340;&#34892;&#21160;&#21487;&#33021;&#21463;&#21040;&#26410;&#35266;&#23519;&#21040;&#30340;&#22240;&#32032;&#24433;&#21709;&#65292;&#23548;&#33268;&#20272;&#35745;&#20540;&#20986;&#29616;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20174;&#21307;&#30103;&#20445;&#20581;&#25110;&#25945;&#32946;&#39046;&#22495;&#25910;&#38598;&#30340;&#35266;&#23519;&#25968;&#25454;&#24212;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26102;&#65292;&#19968;&#20010;&#26222;&#36941;&#30340;&#20851;&#27880;&#28857;&#26159;&#65292;&#35266;&#23519;&#21040;&#30340;&#34892;&#21160;&#21487;&#33021;&#21463;&#21040;&#26410;&#35266;&#23519;&#21040;&#30340;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#24341;&#36215;&#28151;&#28102;&#24182;&#23548;&#33268;&#22312;&#20551;&#35774;&#23436;&#32654;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#30340;&#20272;&#35745;&#20540;&#20986;&#29616;&#20559;&#24046;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#37096;&#20998;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#20013;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#32473;&#23450;&#21482;&#30001;&#19981;&#21516;&#19988;&#26410;&#30693;&#30340;&#31574;&#30053;&#29983;&#25104;&#30340;&#20855;&#26377;&#37096;&#20998;&#29366;&#24577;&#35266;&#27979;&#30340;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;POMDP&#20013;&#32473;&#23450;&#30446;&#26631;&#31574;&#30053;&#30340;&#20540;&#65292;&#35813;&#31574;&#30053;&#21487;&#33021;&#20381;&#36182;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#20160;&#20040;&#26465;&#20214;&#20801;&#35768;&#25105;&#20204;&#20174;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#20013;&#35782;&#21035;&#30446;&#26631;&#31574;&#30053;&#20540;&#65292;&#24182;&#19988;&#22312;&#35782;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#20272;&#35745;&#23427;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#25193;&#23637;&#21040;&#25105;&#20204;&#30340;POMDP&#35774;&#32622;&#20013;&#65292;&#25552;&#20379;&#20102;&#35768;&#22810;&#22330;&#26223;&#65292;&#20854;&#20013;&#36890;&#36807;&#25152;&#35859;&#30340;&#26725;&#25509;&#20989;&#25968;&#30340;&#23384;&#22312;&#23454;&#29616;&#20102;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
In applications of offline reinforcement learning to observational data, such as in healthcare or education, a general concern is that observed actions might be affected by unobserved factors, inducing confounding and biasing estimates derived under the assumption of a perfect Markov decision process (MDP) model. Here we tackle this by considering off-policy evaluation in a partially observed MDP (POMDP). Specifically, we consider estimating the value of a given target policy in a POMDP given trajectories with only partial state observations generated by a different and unknown policy that may depend on the unobserved state. We tackle two questions: what conditions allow us to identify the target policy value from the observed data and, given identification, how to best estimate it. To answer these, we extend the framework of proximal causal inference to our POMDP setting, providing a variety of settings where identification is made possible by the existence of so-called bridge functio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20013;&#24515;&#32479;&#35745;&#37327;&#8212;&#8212;&#26368;&#36817;&#37051;&#27979;&#24230;&#65292;&#24182;&#36890;&#36807;&#22343;&#21248;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#21644;&#19968;&#31181;&#22343;&#21248;&#30340;&#38750;&#28176;&#36817;&#30028;&#38480;&#30740;&#31350;&#20102;&#23427;&#12290;&#35813;&#27979;&#24230;&#21487;&#33021;&#20026;&#25512;&#26029;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.15083</link><description>&lt;p&gt;
&#26368;&#36817;&#37051;&#36807;&#31243;&#65306;&#24369;&#25910;&#25947;&#21644;&#38750;&#28176;&#36817;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Nearest neighbor process: weak convergence and non-asymptotic bound. (arXiv:2110.15083v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20013;&#24515;&#32479;&#35745;&#37327;&#8212;&#8212;&#26368;&#36817;&#37051;&#27979;&#24230;&#65292;&#24182;&#36890;&#36807;&#22343;&#21248;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#21644;&#19968;&#31181;&#22343;&#21248;&#30340;&#38750;&#28176;&#36817;&#30028;&#38480;&#30740;&#31350;&#20102;&#23427;&#12290;&#35813;&#27979;&#24230;&#21487;&#33021;&#20026;&#25512;&#26029;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#30001;&#32473;&#23450;&#28857;&#30340;&#26368;&#36817;&#37051;&#25152;&#24471;&#21040;&#30340;&#32463;&#39564;&#27979;&#24230;&#8212;&#8212;&#26368;&#36817;&#37051;&#27979;&#24230;&#20316;&#20026;&#19968;&#31181;&#20013;&#24515;&#32479;&#35745;&#37327;&#12290;&#39318;&#20808;&#65292;&#22312;&#24213;&#23618;&#20989;&#25968;&#31867;&#19978;&#28385;&#36275;&#65288;&#21453;&#26144;&#26368;&#36817;&#37051;&#31639;&#27861;&#30340;&#26412;&#22320;&#21270;&#29305;&#24615;&#30340;&#65289;&#65288;&#26412;&#22320;&#65289;&#25903;&#25745;&#29109;&#26465;&#20214;&#19979;&#65292;&#23558;&#30456;&#20851;&#32463;&#39564;&#36807;&#31243;&#35777;&#26126;&#20026;&#28385;&#36275;&#22343;&#21248;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;&#20854;&#27425;&#65292;&#22312;&#32479;&#19968;&#29109;&#25968;&#30340;&#33879;&#21517;&#26465;&#20214;&#65288;&#36890;&#24120;&#31216;&#20026;Vapnik-Chervonenkis&#65289;&#19979;&#24314;&#31435;&#20102;&#19968;&#31181;&#22343;&#21248;&#30340;&#38750;&#28176;&#36817;&#30028;&#38480;&#12290;&#22312;&#22343;&#21248;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#20013;&#25152;&#33719;&#24471;&#30340;&#39640;&#26031;&#26497;&#38480;&#30340;&#21327;&#26041;&#24046;&#31561;&#20110;&#26465;&#20214;&#21327;&#26041;&#24046;&#31639;&#23376;&#65288;&#32473;&#20986;&#20852;&#36259;&#28857;&#65289;&#12290;&#36825;&#25552;&#31034;&#20102;&#19968;&#31181;&#21487;&#33021;&#24615;&#65292;&#21363;&#22312;&#20351;&#29992;&#30456;&#21516;&#30340;&#25512;&#29702;&#26041;&#24335;&#20294;&#20165;&#20351;&#29992;&#26368;&#36817;&#37051;&#32780;&#19981;&#26159;&#20840;&#37096;&#26367;&#25442;&#26631;&#20934;&#32463;&#39564;&#27979;&#24230;&#30340;&#26631;&#20934;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#25193;&#23637;&#26631;&#20934;&#26041;&#27861; - &#38750;&#23616;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
The empirical measure resulting from the nearest neighbors to a given point \textit{the nearest neighbor measure} - is introduced and studied as a central statistical quantity. First, the associated empirical process is shown to satisfy a uniform central limit theorem under a (local) bracketing entropy condition on the underlying class of functions (reflecting the localizing nature of the nearest neighbor algorithm). Second a uniform non-asymptotic bound is established under a well-known condition, often referred to as Vapnik-Chervonenkis, on the uniform entropy numbers. The covariance of the Gaussian limit obtained in the uniform central limit theorem is equal to the conditional covariance operator (given the point of interest). This suggests the possibility of extending standard approaches - non local - replacing simply the standard empirical measure by the nearest neighbor measure while using the same way of making inference but with the nearest neighbors only instead of the full 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36127;&#23545;&#29031;&#12289;&#20195;&#29702;&#21464;&#37327;&#21644;&#24037;&#20855;&#21464;&#37327;&#30340;&#26680;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#27835;&#30103;&#25928;&#26524;&#24182;&#23398;&#20064;&#38750;&#21442;&#25968;&#27835;&#30103;&#25928;&#26524;&#12290; &#20316;&#32773;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#19968;&#33268;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20272;&#35745;&#20102;&#39321;&#28895;&#21560;&#28895;&#30340;&#21058;&#37327;&#21453;&#24212;&#26354;&#32447;&#12290;</title><link>http://arxiv.org/abs/2012.10315</link><description>&lt;p&gt;
&#26080;&#27861;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#26680;&#26041;&#27861;&#65306;&#36127;&#23545;&#29031;&#12289;&#20195;&#29702;&#21464;&#37327;&#21644;&#24037;&#20855;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Kernel Methods for Unobserved Confounding: Negative Controls, Proxies, and Instruments. (arXiv:2012.10315v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.10315
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36127;&#23545;&#29031;&#12289;&#20195;&#29702;&#21464;&#37327;&#21644;&#24037;&#20855;&#21464;&#37327;&#30340;&#26680;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#27835;&#30103;&#25928;&#26524;&#24182;&#23398;&#20064;&#38750;&#21442;&#25968;&#27835;&#30103;&#25928;&#26524;&#12290; &#20316;&#32773;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#19968;&#33268;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20272;&#35745;&#20102;&#39321;&#28895;&#21560;&#28895;&#30340;&#21058;&#37327;&#21453;&#24212;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#23545;&#29031;&#26159;&#19968;&#31181;&#22312;&#23384;&#22312;&#26410;&#27979;&#37327;&#28151;&#28102;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27835;&#30103;&#19982;&#32467;&#26524;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#31574;&#30053;&#12290;&#22914;&#26524;&#26377;&#20004;&#20010;&#36741;&#21161;&#21464;&#37327;&#21487;&#29992;&#65306;&#19968;&#20010;&#36127;&#23545;&#29031;&#27835;&#30103;&#65288;&#23545;&#23454;&#38469;&#32467;&#26524;&#27809;&#26377;&#24433;&#21709;&#65289;&#21644;&#19968;&#20010;&#36127;&#23545;&#29031;&#32467;&#26524;&#65288;&#19981;&#21463;&#23454;&#38469;&#27835;&#30103;&#24433;&#21709;&#65289;&#65292;&#21017;&#20173;&#28982;&#21487;&#20197;&#35782;&#21035;&#27835;&#30103;&#25928;&#26524;&#12290; &#36825;&#20123;&#36741;&#21161;&#21464;&#37327;&#20063;&#21487;&#20197;&#35270;&#20026;&#20256;&#32479;&#25511;&#21046;&#21464;&#37327;&#38598;&#30340;&#20195;&#29702;&#21464;&#37327;&#65292;&#24182;&#19988;&#23427;&#20204;&#31867;&#20284;&#20110;&#24037;&#20855;&#21464;&#37327;&#12290;&#25105;&#25552;&#20986;&#20102;&#19968;&#26063;&#22522;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#31639;&#27861;&#65292;&#22312;&#36127;&#23545;&#29031;&#19979;&#23398;&#20064;&#38750;&#21442;&#25968;&#27835;&#30103;&#25928;&#26524;&#12290;&#31034;&#20363;&#21253;&#25324;&#21058;&#37327;&#21453;&#24212;&#26354;&#32447;&#12289;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#21058;&#37327;&#21453;&#24212;&#26354;&#32447;&#21644;&#24322;&#36136;&#24615;&#27835;&#30103;&#25928;&#26524;&#12290; &#25968;&#25454;&#21487;&#20197;&#26159;&#31163;&#25955;&#12289;&#36830;&#32493;&#21644;&#20302;&#32500;&#12289;&#39640;&#32500;&#25110;&#26080;&#38480;&#32500;&#12290;&#25105;&#35777;&#26126;&#20102;&#22343;&#21248;&#19968;&#33268;&#24615;&#24182;&#25552;&#20379;&#20102;&#26377;&#38480;&#26679;&#26412;&#25910;&#25947;&#29575;&#12290; &#25105;&#20272;&#35745;&#20102;&#39321;&#28895;&#21560;&#28895;&#21058;&#37327;&#21453;&#24212;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Negative control is a strategy for learning the causal relationship between treatment and outcome in the presence of unmeasured confounding. The treatment effect can nonetheless be identified if two auxiliary variables are available: a negative control treatment (which has no effect on the actual outcome), and a negative control outcome (which is not affected by the actual treatment). These auxiliary variables can also be viewed as proxies for a traditional set of control variables, and they bear resemblance to instrumental variables. I propose a family of algorithms based on kernel ridge regression for learning nonparametric treatment effects with negative controls. Examples include dose response curves, dose response curves with distribution shift, and heterogeneous treatment effects. Data may be discrete or continuous, and low, high, or infinite dimensional. I prove uniform consistency and provide finite sample rates of convergence. I estimate the dose response curve of cigarette sm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#36890;&#29992;&#30340;&#26465;&#20214;&#30697;&#38382;&#39064;&#20272;&#35745;&#22120;&#31867; - &#21464;&#20998;&#30697;&#26041;&#27861;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;&#26080;&#38480;&#25968;&#37327;&#30340;&#30697;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#26680;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20010;VMM&#20272;&#35745;&#22120;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2012.09422</link><description>&lt;p&gt;
&#26465;&#20214;&#30697;&#38382;&#39064;&#30340;&#21464;&#20998;&#30697;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Variational Method of Moments. (arXiv:2012.09422v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.09422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#36890;&#29992;&#30340;&#26465;&#20214;&#30697;&#38382;&#39064;&#20272;&#35745;&#22120;&#31867; - &#21464;&#20998;&#30697;&#26041;&#27861;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;&#26080;&#38480;&#25968;&#37327;&#30340;&#30697;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#26680;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20010;VMM&#20272;&#35745;&#22120;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#30697;&#38382;&#39064;&#26159;&#25551;&#36848;&#32467;&#26500;&#24615;&#22240;&#26524;&#21442;&#25968;&#30340;&#26377;&#21147;&#24418;&#24335;&#21270;&#24037;&#20855;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#32452;&#30340;&#36793;&#38469;&#30697;&#26465;&#20214;&#65292;&#24182;&#24212;&#29992;&#26368;&#20248;&#21152;&#26435;&#24191;&#20041;&#30697;&#27861;&#65288;OWGMM&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20043;&#20026;&#21464;&#20998;&#30697;&#26041;&#27861;&#65288;VMM&#65289;&#30340;&#38750;&#24120;&#36890;&#29992;&#30340;&#26465;&#20214;&#30697;&#38382;&#39064;&#20272;&#35745;&#22120;&#31867;&#65292;&#24182;&#33258;&#28982;&#22320;&#20351;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;&#26080;&#38480;&#25968;&#37327;&#30340;&#30697;&#12290;&#20316;&#32773;&#23545;&#22810;&#20010;VMM&#20272;&#35745;&#22120;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21253;&#25324;&#22522;&#20110;&#26680;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#19968;&#33268;&#20272;&#35745;&#30495;&#23454;&#22240;&#26524;&#21442;&#25968;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conditional moment problem is a powerful formulation for describing structural causal parameters in terms of observables, a prominent example being instrumental variable regression. A standard approach reduces the problem to a finite set of marginal moment conditions and applies the optimally weighted generalized method of moments (OWGMM), but this requires we know a finite set of identifying moments, can still be inefficient even if identifying, or can be theoretically efficient but practically unwieldy if we use a growing sieve of moment conditions. Motivated by a variational minimax reformulation of OWGMM, we define a very general class of estimators for the conditional moment problem, which we term the variational method of moments (VMM) and which naturally enables controlling infinitely-many moments. We provide a detailed theoretical analysis of multiple VMM estimators, including ones based on kernel methods and neural nets, and provide conditions under which these are consist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RELL&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21327;&#21516;&#38598;&#25104;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;RELL&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26174;&#30528;&#25913;&#21892;&#21453;&#21521;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2004.12908</link><description>&lt;p&gt;
&#20195;&#34920;&#24615;&#38598;&#25104;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#21327;&#21516;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity. (arXiv:2004.12908v16 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.12908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RELL&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21327;&#21516;&#38598;&#25104;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;RELL&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26174;&#30528;&#25913;&#21892;&#21453;&#21521;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#24403;&#21069;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#20043;&#21069;&#21644;&#23578;&#26410;&#36935;&#21040;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21017;&#20174;&#31354;&#30333;&#29366;&#24577;&#24320;&#22987;&#65292;&#20165;&#38024;&#23545;&#21333;&#20010;&#20219;&#21153;&#20351;&#29992;&#25968;&#25454;&#12290;&#34429;&#28982;&#20256;&#32479;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#26410;&#26469;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#21518;&#23545;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65288;&#31216;&#20026;&#36951;&#24536;&#65289;&#12290;&#36817;&#26399;&#38024;&#23545;&#36830;&#32493;&#25110;&#32456;&#36523;&#23398;&#20064;&#30340;&#35768;&#22810;&#26041;&#27861;&#37117;&#35797;&#22270;&#22312;&#32473;&#23450;&#26032;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23545;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#20165;&#21162;&#21147;&#36991;&#20813;&#24536;&#35760;&#23558;&#30446;&#26631;&#23450;&#24471;&#36807;&#20302;&#12290;&#32456;&#36523;&#23398;&#20064;&#30340;&#30446;&#26631;&#19981;&#20165;&#24212;&#35813;&#26159;&#25552;&#39640;&#26410;&#26469;&#20219;&#21153;&#65288;&#21069;&#21521;&#20256;&#36882;&#65289;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#24212;&#35813;&#26159;&#29992;&#20219;&#20309;&#26032;&#25968;&#25454;&#25552;&#39640;&#36807;&#21435;&#20219;&#21153;&#65288;&#21453;&#21521;&#20256;&#36882;&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#21327;&#21516;&#38598;&#25104;&#20998;&#21035;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#32456;&#36523;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#38598;&#25104;&#65288;RELL&#65289;&#8221;&#65292;&#23427;&#38598;&#25104;&#20102;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#19981;&#21516;&#34920;&#31034;&#20013;&#21253;&#21547;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RELL&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26174;&#30528;&#26356;&#22909;&#30340;&#21453;&#21521;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
In lifelong learning, data are used to improve performance not only on the current task, but also on previously encountered, and as yet unencountered tasks. In contrast, classical machine learning, which we define as, starts from a blank slate, or tabula rasa and uses data only for the single task at hand. While typical transfer learning algorithms can improve performance on future tasks, their performance on prior tasks degrades upon learning new tasks (called forgetting). Many recent approaches for continual or lifelong learning have attempted to maintain performance on old tasks given new tasks. But striving to avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning should be not only to improve performance on future tasks (forward transfer) but also on past tasks (backward transfer) with any new data. Our key insight is that we can synergistically ensemble representations -- that were learned independently on disparate tasks -- to enable both forward and bac
&lt;/p&gt;</description></item></channel></rss>