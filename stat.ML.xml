<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35821;&#35328;&#26657;&#27491;&#27969;&#26159;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;&#27010;&#29575;&#27969;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#20256;&#36755;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#21644;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#39046;&#22495;&#36716;&#31227;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.16995</link><description>&lt;p&gt;
&#35821;&#35328;&#26657;&#27491;&#27969;&#65306;&#36890;&#36807;&#27010;&#29575;&#27969;&#25512;&#21160;&#25193;&#25955;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16995
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26657;&#27491;&#27969;&#26159;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;&#27010;&#29575;&#27969;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#20256;&#36755;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#21644;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#39046;&#22495;&#36716;&#31227;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#19978;&#25511;&#21046;&#21477;&#23376;&#23646;&#24615;&#65288;&#20363;&#22914;&#24773;&#24863;&#65289;&#21644;&#32467;&#26500;&#65288;&#20363;&#22914;&#21477;&#27861;&#32467;&#26500;&#65289;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#19968;&#20010;&#25512;&#21160;&#39640;&#36136;&#37327;&#26679;&#26412;&#29983;&#25104;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#36845;&#20195;&#21435;&#22122;&#25968;&#21315;&#27493;&#12290;&#23613;&#31649;&#26377;&#30410;&#65292;&#20294;&#20174;&#22122;&#22768;&#24320;&#22987;&#30340;&#22797;&#26434;&#24615;&#21644;&#23398;&#20064;&#27493;&#39588;&#38480;&#21046;&#20102;&#20854;&#22312;&#35768;&#22810;NLP&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Language Rectified Flow&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26631;&#20934;&#27010;&#29575;&#27969;&#27169;&#22411;&#30340;&#37325;&#26500;&#12290;&#35821;&#35328;&#26657;&#27491;&#27969;&#23398;&#20064;&#65288;&#31070;&#32463;&#65289;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#20256;&#36755;&#65292;&#20026;&#29983;&#25104;&#24314;&#27169;&#21644;&#22495;&#36716;&#31227;&#25552;&#20379;&#20102;&#32479;&#19968;&#21644;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20174;&#28304;&#20998;&#24067;&#24320;&#22987;&#65292;&#25105;&#20204;&#30340;&#35821;&#35328;&#26657;&#27491;&#27969;&#20135;&#29983;&#24555;&#36895;&#20223;&#30495;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16995v1 Announce Type: cross  Abstract: Recent works have demonstrated success in controlling sentence attributes ($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the diffusion language model. A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps. While beneficial, the complexity of starting from the noise and the learning steps has limited its implementation to many NLP real-world applications. This paper proposes Language Rectified Flow ({\ours}). Our method is based on the reformulation of the standard probabilistic flow models. Language rectified flow learns (neural) ordinary differential equation models to transport between the source distribution and the target distribution, hence providing a unified and effective solution to generative modeling and domain transfer. From the source distribution, our language rectified flow yields fast simulation and effe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23548;&#20986;&#20102;&#19968;&#20010;&#20844;&#24335;&#65292;&#29992;&#20110;&#21051;&#30011;&#31616;&#21333;&#20108;&#20803;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65288;&#20056;&#27861;&#24120;&#25968;&#29420;&#31435;&#20110;$p$&#12289;$q$&#21644;&#25152;&#26377;&#38169;&#35823;&#21442;&#25968;&#65289;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35774;&#32622;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.16981</link><description>&lt;p&gt;
&#31616;&#21333;&#20108;&#20803;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Sample Complexity of Simple Binary Hypothesis Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16981
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23548;&#20986;&#20102;&#19968;&#20010;&#20844;&#24335;&#65292;&#29992;&#20110;&#21051;&#30011;&#31616;&#21333;&#20108;&#20803;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65288;&#20056;&#27861;&#24120;&#25968;&#29420;&#31435;&#20110;$p$&#12289;$q$&#21644;&#25152;&#26377;&#38169;&#35823;&#21442;&#25968;&#65289;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35774;&#32622;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#30340;&#20108;&#20803;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26159;&#21306;&#20998;&#20004;&#20010;&#20998;&#24067;$p$&#21644;$q$&#25152;&#38656;&#30340;&#26368;&#23567;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#25968;&#37327;&#65292;&#21487;&#20197;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#20043;&#19968;&#36827;&#34892;&#65306;(i) &#26080;&#20808;&#39564;&#35774;&#32622;&#65292;&#31867;&#22411;-I&#38169;&#35823;&#26368;&#22823;&#20026;$\alpha$&#65292;&#31867;&#22411;-II&#38169;&#35823;&#26368;&#22823;&#20026;$\beta$; &#25110;&#32773; (ii) &#36125;&#21494;&#26031;&#35774;&#32622;&#65292;&#36125;&#21494;&#26031;&#38169;&#35823;&#26368;&#22823;&#20026;$\delta$&#65292;&#20808;&#39564;&#20998;&#24067;&#20026;$(\alpha, 1-\alpha)$&#12290; &#36804;&#20170;&#20026;&#27490;&#65292;&#21482;&#22312;$\alpha = \beta$&#65288;&#26080;&#20808;&#39564;&#65289;&#25110;$\alpha = 1/2$&#65288;&#36125;&#21494;&#26031;&#65289;&#26102;&#30740;&#31350;&#20102;&#27492;&#38382;&#39064;&#65292;&#24182;&#19988;&#24050;&#30693;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#29992;$p$&#21644;$q$&#20043;&#38388;&#30340;Hellinger&#25955;&#24230;&#26469;&#21051;&#30011;&#65292;&#30452;&#21040;&#20056;&#27861;&#24120;&#25968;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#20010;&#20844;&#24335;&#65292;&#29992;&#26469;&#21051;&#30011;&#26679;&#26412;&#22797;&#26434;&#24230;&#65288;&#20056;&#27861;&#24120;&#25968;&#29420;&#31435;&#20110;$p$&#12289;$q$&#21644;&#25152;&#26377;&#38169;&#35823;&#21442;&#25968;&#65289;&#65292;&#36866;&#29992;&#20110;&#65306;(i) &#20808;&#39564;&#35774;&#32622;&#20013;&#25152;&#26377;$0 \le \alpha, \beta \le 1/8$&#65307;&#20197;&#21450; (ii) &#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#25152;&#26377;$\delta \le \alpha/4$&#12290; &#29305;&#21035;&#22320;&#65292;&#35813;&#20844;&#24335;&#36866;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16981v1 Announce Type: cross  Abstract: The sample complexity of simple binary hypothesis testing is the smallest number of i.i.d. samples required to distinguish between two distributions $p$ and $q$ in either: (i) the prior-free setting, with type-I error at most $\alpha$ and type-II error at most $\beta$; or (ii) the Bayesian setting, with Bayes error at most $\delta$ and prior distribution $(\alpha, 1-\alpha)$. This problem has only been studied when $\alpha = \beta$ (prior-free) or $\alpha = 1/2$ (Bayesian), and the sample complexity is known to be characterized by the Hellinger divergence between $p$ and $q$, up to multiplicative constants. In this paper, we derive a formula that characterizes the sample complexity (up to multiplicative constants that are independent of $p$, $q$, and all error parameters) for: (i) all $0 \le \alpha, \beta \le 1/8$ in the prior-free setting; and (ii) all $\delta \le \alpha/4$ in the Bayesian setting. In particular, the formula admits eq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;SCOD&#38382;&#39064;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#21253;&#25324;&#22522;&#20110;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#21644;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#22120;&#65292;&#20197;&#35299;&#20915;&#19981;&#30830;&#23450;&#25110;&#31163;&#32676;&#26679;&#26412;&#39044;&#27979;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16916</link><description>&lt;p&gt;
&#20174;&#21551;&#21457;&#24335;&#21040;&#29702;&#35770;&#65306;SCOD
&lt;/p&gt;
&lt;p&gt;
SCOD: From Heuristics to Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;SCOD&#38382;&#39064;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#21253;&#25324;&#22522;&#20110;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#21644;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#22120;&#65292;&#20197;&#35299;&#20915;&#19981;&#30830;&#23450;&#25110;&#31163;&#32676;&#26679;&#26412;&#39044;&#27979;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#35774;&#35745;&#21487;&#38752;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24403;&#38754;&#23545;&#19981;&#30830;&#23450;&#25110;&#31163;&#32676;&#26679;&#26412;&#26102;&#36991;&#20813;&#39044;&#27979;&#30340;&#38382;&#39064; - &#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#34987;&#31216;&#20026;Selective Classification in the presence of Out-of-Distribution data (SCOD)&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;SCOD&#20570;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20248;&#30340;SCOD&#31574;&#30053;&#28041;&#21450;&#22522;&#20110;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#36827;&#34892;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#21644;&#22312;2D&#31354;&#38388;&#20013;&#34920;&#31034;&#20026;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#22120;&#65292;&#20351;&#29992;ID&#20998;&#31867;&#22120;&#30340;&#26465;&#20214;&#39118;&#38505;&#21644;ID&#19982;&#31163;&#32676;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#30340;&#20284;&#28982;&#27604;&#20316;&#20026;&#36755;&#20837;&#12290;&#36825;&#19982;&#24403;&#21069;OOD&#26816;&#27979;&#26041;&#27861;&#21644;&#19987;&#20026;SCOD&#24320;&#21457;&#30340;Softmax Information Retaining Combination (SIRC)&#30340;&#27425;&#20248;&#31574;&#30053;&#24418;&#25104;&#23545;&#27604;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22312;&#19968;&#20010;&#26080;&#20998;&#24067;&#35774;&#32622;&#20013;&#65292;&#24403;&#20165;&#20381;&#36182;&#20110;&#26465;&#20214;&#20998;&#24067;&#21644;IID&#26679;&#26412;&#30340;&#36924;&#36817;&#21487;&#33021;&#24615;&#26102;&#65292;SCOD&#38382;&#39064;&#19981;&#21487;&#33021;&#34987;&#27491;&#30830;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16916v1 Announce Type: new  Abstract: This paper addresses the problem of designing reliable prediction models that abstain from predictions when faced with uncertain or out-of-distribution samples - a recently proposed problem known as Selective Classification in the presence of Out-of-Distribution data (SCOD). We make three key contributions to SCOD. Firstly, we demonstrate that the optimal SCOD strategy involves a Bayes classifier for in-distribution (ID) data and a selector represented as a stochastic linear classifier in a 2D space, using i) the conditional risk of the ID classifier, and ii) the likelihood ratio of ID and out-of-distribution (OOD) data as input. This contrasts with suboptimal strategies from current OOD detection methods and the Softmax Information Retaining Combination (SIRC), specifically developed for SCOD. Secondly, we establish that in a distribution-free setting, the SCOD problem is not Probably Approximately Correct learnable when relying solely 
&lt;/p&gt;</description></item><item><title>GLAD&#26159;&#19968;&#20010;&#22312;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#25193;&#25955;&#26725;&#32467;&#26500;&#23398;&#20064;&#20854;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#20110;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#30340;&#20998;&#35299;&#65292;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16883</link><description>&lt;p&gt;
&#24102;&#25193;&#25955;&#26725;&#30340;&#31163;&#25955;&#28508;&#22312;&#22270;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Discrete Latent Graph Generative Modeling with Diffusion Bridges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16883
&lt;/p&gt;
&lt;p&gt;
GLAD&#26159;&#19968;&#20010;&#22312;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#25193;&#25955;&#26725;&#32467;&#26500;&#23398;&#20064;&#20854;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#20110;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#30340;&#20998;&#35299;&#65292;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#20110;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#27169;&#22411;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#65292;&#36804;&#20170;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#20047;&#21892;&#21487;&#38472;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GLAD&#65292;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#22270;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#28508;&#22312;&#31354;&#38388;&#22270;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;GLAD&#22312;&#20445;&#30041;&#22270;&#32467;&#26500;&#30340;&#31163;&#25955;&#24615;&#36136;&#26041;&#38754;&#36816;&#34892;&#65292;&#26080;&#38656;&#36827;&#34892;&#35832;&#22914;&#28508;&#22312;&#31354;&#38388;&#36830;&#32493;&#24615;&#31561;&#19981;&#33258;&#28982;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25193;&#25955;&#26725;&#35843;&#25972;&#21040;&#20854;&#32467;&#26500;&#65292;&#26469;&#23398;&#20064;&#25105;&#20204;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#12290;&#36890;&#36807;&#22312;&#36866;&#24403;&#26500;&#24314;&#30340;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#25105;&#20204;&#36991;&#20813;&#20381;&#36182;&#20110;&#24120;&#29992;&#20110;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#25805;&#20316;&#30340;&#27169;&#22411;&#20013;&#30340;&#20998;&#35299;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#22270;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#26126;&#26174;&#23637;&#31034;&#20102;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#29983;&#25104;&#24615;&#33021;&#65292;&#20351;GLA
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16883v1 Announce Type: new  Abstract: Learning graph generative models over latent spaces has received less attention compared to models that operate on the original data space and has so far demonstrated lacklustre performance. We present GLAD a latent space graph generative model. Unlike most previous latent space graph generative models, GLAD operates on a discrete latent space that preserves to a significant extent the discrete nature of the graph structures making no unnatural assumptions such as latent space continuity. We learn the prior of our discrete latent space by adapting diffusion bridges to its structure. By operating over an appropriately constructed latent space we avoid relying on decompositions that are often used in models that operate in the original data space. We present experiments on a series of graph benchmark datasets which clearly show the superiority of the discrete latent space and obtain state of the art graph generative performance, making GLA
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MA-COPP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#28041;&#21450;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#31163;&#31574;&#30053;&#39044;&#27979;&#38382;&#39064;&#30340;&#19968;&#33268;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16871</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#19968;&#33268;&#31163;&#31574;&#30053;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Off-Policy Prediction for Multi-Agent Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16871
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MA-COPP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#28041;&#21450;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#31163;&#31574;&#30053;&#39044;&#27979;&#38382;&#39064;&#30340;&#19968;&#33268;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#31574;&#30053;&#39044;&#27979;&#65288;OPP&#65289;&#65292;&#21363;&#20165;&#20351;&#29992;&#22312;&#19968;&#20010;&#27491;&#24120;&#65288;&#34892;&#20026;&#65289;&#31574;&#30053;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#39044;&#27979;&#30446;&#26631;&#31574;&#30053;&#30340;&#32467;&#26524;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20998;&#26512;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;&#37096;&#32626;&#26032;&#31574;&#30053;&#21487;&#33021;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#20449;&#30340;&#31163;&#31574;&#30053;&#39044;&#27979;&#65292;&#26368;&#36817;&#20851;&#20110;&#19968;&#33268;&#31163;&#31574;&#30053;&#39044;&#27979;&#65288;COPP&#65289;&#30340;&#24037;&#20316;&#21033;&#29992;&#19968;&#33268;&#39044;&#27979;&#26694;&#26550;&#26469;&#22312;&#30446;&#26631;&#36807;&#31243;&#19979;&#25512;&#23548;&#24102;&#26377;&#27010;&#29575;&#20445;&#35777;&#30340;&#39044;&#27979;&#21306;&#22495;&#12290;&#29616;&#26377;&#30340;COPP&#26041;&#27861;&#21487;&#20197;&#32771;&#34385;&#30001;&#31574;&#30053;&#20999;&#25442;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#20294;&#20165;&#38480;&#20110;&#21333;&#26234;&#33021;&#20307;&#31995;&#32479;&#21644;&#26631;&#37327;&#32467;&#26524;&#65288;&#20363;&#22914;&#65292;&#22870;&#21169;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MA-COPP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#28041;&#21450;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;OPP&#38382;&#39064;&#30340;&#19968;&#33268;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#8220;&#33258;&#25105;&#8221;&#26234;&#33021;&#20307;&#25913;&#21464;&#31574;&#30053;&#26102;&#20026;&#25152;&#26377;&#26234;&#33021;&#20307;&#36712;&#36857;&#25512;&#23548;&#32852;&#21512;&#39044;&#27979;&#21306;&#22495;&#12290;&#19982;&#21333;&#26234;&#33021;&#20307;&#22330;&#26223;&#19981;&#21516;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16871v1 Announce Type: cross  Abstract: Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy using only data collected under a nominal (behavioural) policy, is a paramount problem in data-driven analysis of safety-critical systems where the deployment of a new policy may be unsafe. To achieve dependable off-policy predictions, recent work on Conformal Off-Policy Prediction (COPP) leverage the conformal prediction framework to derive prediction regions with probabilistic guarantees under the target process. Existing COPP methods can account for the distribution shifts induced by policy switching, but are limited to single-agent systems and scalar outcomes (e.g., rewards). In this work, we introduce MA-COPP, the first conformal prediction method to solve OPP problems involving multi-agent systems, deriving joint prediction regions for all agents' trajectories when one or more "ego" agents change their policies. Unlike the single-agent scenario, this se
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#31070;&#32463;&#28436;&#21592;-&#35780;&#35770;&#31639;&#27861;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#38544;&#34255;&#21333;&#20803;&#21644;&#35757;&#32451;&#27493;&#25968;&#30340;&#25968;&#37327;$\rightarrow \infty$&#26102;&#65292;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#23558;&#25910;&#25947;&#20110;&#38543;&#26426;ODE&#65292;&#36890;&#36807;&#24314;&#31435;&#25968;&#25454;&#26679;&#26412;&#30340;&#20960;&#20309;&#36941;&#21382;&#24615;&#21644;&#20351;&#29992;&#27850;&#26494;&#26041;&#31243;&#35777;&#26126;&#27169;&#22411;&#26356;&#26032;&#27874;&#21160;&#28040;&#22833;&#65292;&#28436;&#21592;&#31070;&#32463;&#32593;&#32476;&#21644;&#35780;&#35770;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#21040;&#20855;&#26377;&#38543;&#26426;&#21021;&#22987;&#26465;&#20214;&#30340;ODE&#31995;&#32479;&#30340;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16825</link><description>&lt;p&gt;
&#22312;&#32447;&#31070;&#32463;&#28436;&#21592;-&#35780;&#35770;&#31639;&#27861;&#30340;&#24369;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Weak Convergence Analysis of Online Neural Actor-Critic Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16825
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31070;&#32463;&#28436;&#21592;-&#35780;&#35770;&#31639;&#27861;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#38544;&#34255;&#21333;&#20803;&#21644;&#35757;&#32451;&#27493;&#25968;&#30340;&#25968;&#37327;$\rightarrow \infty$&#26102;&#65292;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#23558;&#25910;&#25947;&#20110;&#38543;&#26426;ODE&#65292;&#36890;&#36807;&#24314;&#31435;&#25968;&#25454;&#26679;&#26412;&#30340;&#20960;&#20309;&#36941;&#21382;&#24615;&#21644;&#20351;&#29992;&#27850;&#26494;&#26041;&#31243;&#35777;&#26126;&#27169;&#22411;&#26356;&#26032;&#27874;&#21160;&#28040;&#22833;&#65292;&#28436;&#21592;&#31070;&#32463;&#32593;&#32476;&#21644;&#35780;&#35770;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#21040;&#20855;&#26377;&#38543;&#26426;&#21021;&#22987;&#26465;&#20214;&#30340;ODE&#31995;&#32479;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#22312;&#32447;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#35757;&#32451;&#30340;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#38544;&#34255;&#21333;&#20803;&#21644;&#35757;&#32451;&#27493;&#25968;&#30340;&#25968;&#37327;$\rightarrow \infty$&#26102;&#65292;&#25910;&#25947;&#20110;&#19968;&#20010;&#38543;&#26426;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#22312;&#32447;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#20013;&#65292;&#38543;&#30528;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#25968;&#25454;&#26679;&#26412;&#30340;&#20998;&#24067;&#20250;&#21160;&#24577;&#21464;&#21270;&#65292;&#36825;&#23545;&#20110;&#20219;&#20309;&#25910;&#25947;&#20998;&#26512;&#26469;&#35828;&#37117;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#22266;&#23450;&#28436;&#21592;&#31574;&#30053;&#19979;&#24314;&#31435;&#20102;&#25968;&#25454;&#26679;&#26412;&#30340;&#20960;&#20309;&#36941;&#21382;&#24615;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#27850;&#26494;&#26041;&#31243;&#65292;&#25105;&#20204;&#35777;&#26126;&#30001;&#20110;&#38543;&#26426;&#21040;&#36798;&#30340;&#25968;&#25454;&#26679;&#26412;&#24102;&#26469;&#30340;&#27169;&#22411;&#26356;&#26032;&#27874;&#21160;&#20250;&#38543;&#30528;&#21442;&#25968;&#26356;&#26032;&#27425;&#25968;&#30340;&#22686;&#21152;$\rightarrow \infty$&#32780;&#28040;&#22833;&#12290;&#21033;&#29992;&#27850;&#26494;&#26041;&#31243;&#21644;&#24369;&#25910;&#25947;&#25216;&#26415;&#65292;&#25105;&#20204;&#35777;&#26126;&#28436;&#21592;&#31070;&#32463;&#32593;&#32476;&#21644;&#35780;&#35770;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#21040;&#20855;&#26377;&#38543;&#26426;&#21021;&#22987;&#26465;&#20214;&#30340;ODE&#31995;&#32479;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16825v1 Announce Type: new  Abstract: We prove that a single-layer neural network trained with the online actor critic algorithm converges in distribution to a random ordinary differential equation (ODE) as the number of hidden units and the number of training steps $\rightarrow \infty$. In the online actor-critic algorithm, the distribution of the data samples dynamically changes as the model is updated, which is a key challenge for any convergence analysis. We establish the geometric ergodicity of the data samples under a fixed actor policy. Then, using a Poisson equation, we prove that the fluctuations of the model updates around the limit distribution due to the randomly-arriving data samples vanish as the number of parameter updates $\rightarrow \infty$. Using the Poisson equation and weak convergence techniques, we prove that the actor neural network and critic neural network converge to the solutions of a system of ODEs with random initial conditions. Analysis of the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24471;&#20998;&#21305;&#37197;&#23454;&#29616;&#26368;&#20339;&#20984;$M$-&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#33021;&#22815;&#36798;&#21040;&#26368;&#20339;&#30340;&#28176;&#36817;&#26041;&#24046;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#39640;&#25928;&#65292;&#35777;&#26126;&#20855;&#26377;&#25152;&#26377;&#20984;$M$-&#20272;&#35745;&#20013;&#26368;&#23567;&#30340;&#28176;&#36817;&#21327;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.16688</link><description>&lt;p&gt;
&#36890;&#36807;&#24471;&#20998;&#21305;&#37197;&#23454;&#29616;&#26368;&#20339;&#20984;$M$-&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal convex $M$-estimation via score matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16688
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24471;&#20998;&#21305;&#37197;&#23454;&#29616;&#26368;&#20339;&#20984;$M$-&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#33021;&#22815;&#36798;&#21040;&#26368;&#20339;&#30340;&#28176;&#36817;&#26041;&#24046;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#39640;&#25928;&#65292;&#35777;&#26126;&#20855;&#26377;&#25152;&#26377;&#20984;$M$-&#20272;&#35745;&#20013;&#26368;&#23567;&#30340;&#28176;&#36817;&#21327;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24615;&#22238;&#24402;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#35813;&#20989;&#25968;&#36827;&#34892;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21487;&#20197;&#22312;&#22238;&#24402;&#31995;&#25968;&#30340;&#19979;&#28216;&#20272;&#35745;&#20013;&#23454;&#29616;&#26368;&#20339;&#30340;&#28176;&#36817;&#26041;&#24046;&#12290;&#25105;&#20204;&#30340;&#21322;&#21442;&#25968;&#26041;&#27861;&#26088;&#22312;&#26368;&#20339;&#36924;&#36817;&#22122;&#22768;&#20998;&#24067;&#23545;&#25968;&#23494;&#24230;&#30340;&#23548;&#25968;&#12290;&#22312;&#24635;&#20307;&#23618;&#38754;&#19978;&#65292;&#36825;&#20010;&#25311;&#21512;&#36807;&#31243;&#26159;&#23545;&#24471;&#20998;&#21305;&#37197;&#30340;&#38750;&#21442;&#25968;&#25299;&#23637;&#65292;&#23545;&#24212;&#20110;&#26681;&#25454;Fisher&#25955;&#24230;&#36827;&#34892;&#22122;&#22768;&#20998;&#24067;&#30340;&#23545;&#25968;&#20985;&#26144;&#23556;&#12290;&#35813;&#36807;&#31243;&#22312;&#35745;&#31639;&#19978;&#26159;&#39640;&#25928;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#31243;&#24207;&#36798;&#21040;&#20102;&#25152;&#26377;&#20984;$M$-&#20272;&#35745;&#20013;&#26368;&#23567;&#30340;&#28176;&#36817;&#21327;&#26041;&#24046;&#12290;&#20316;&#20026;&#38750;&#23545;&#25968;&#20985;&#35774;&#32622;&#30340;&#19968;&#20010;&#20363;&#23376;&#65292;&#23545;&#20110;&#26607;&#35199;&#35823;&#24046;&#65292;&#26368;&#20339;&#20984;&#25439;&#22833;&#20989;&#25968;&#31867;&#20284;&#20110;Huber&#20989;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#36807;&#31243;&#30456;&#23545;&#20110;oracle&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#23454;&#29616;&#20102;&#22823;&#20110;0.87&#30340;&#28176;&#36817;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16688v1 Announce Type: cross  Abstract: In the context of linear regression, we construct a data-driven convex loss function with respect to which empirical risk minimisation yields optimal asymptotic variance in the downstream estimation of the regression coefficients. Our semiparametric approach targets the best decreasing approximation of the derivative of the log-density of the noise distribution. At the population level, this fitting process is a nonparametric extension of score matching, corresponding to a log-concave projection of the noise distribution with respect to the Fisher divergence. The procedure is computationally efficient, and we prove that our procedure attains the minimal asymptotic covariance among all convex $M$-estimators. As an example of a non-log-concave setting, for Cauchy errors, the optimal convex loss function is Huber-like, and our procedure yields an asymptotic efficiency greater than 0.87 relative to the oracle maximum likelihood estimator o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#26377;&#38480;&#30697;&#30340;&#27867;&#21270;&#30028;&#65292;&#24182;&#25512;&#23548;&#20102;&#39640;&#27010;&#29575;&#30340;PAC-Bayes&#30028;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#23545;&#25439;&#22833;&#20989;&#25968;&#26377;&#30028;&#26041;&#24046;&#30340;&#24773;&#20917;&#19979;&#30028;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#26399;&#26395;&#21644;&#21333;&#27425;&#25277;&#26679;PAC-Bayes&#20013;&#65292;&#24182;&#33719;&#24471;&#20102;&#38024;&#23545;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;&#24555;&#36895;&#36895;&#29575;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.16681</link><description>&lt;p&gt;
&#19968;&#31687;&#20851;&#20110;&#20855;&#26377;&#26377;&#38480;&#30697;&#30340;&#25439;&#22833;&#20989;&#25968;&#27867;&#21270;&#30028;&#30340;&#27880;&#35760;
&lt;/p&gt;
&lt;p&gt;
A note on generalization bounds for losses with finite moments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#26377;&#38480;&#30697;&#30340;&#27867;&#21270;&#30028;&#65292;&#24182;&#25512;&#23548;&#20102;&#39640;&#27010;&#29575;&#30340;PAC-Bayes&#30028;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#23545;&#25439;&#22833;&#20989;&#25968;&#26377;&#30028;&#26041;&#24046;&#30340;&#24773;&#20917;&#19979;&#30028;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#26399;&#26395;&#21644;&#21333;&#27425;&#25277;&#26679;PAC-Bayes&#20013;&#65292;&#24182;&#33719;&#24471;&#20102;&#38024;&#23545;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;&#24555;&#36895;&#36895;&#29575;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Alquier [1]&#25552;&#20986;&#30340;&#25130;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#23548;&#20855;&#26377;&#37325;&#23614;&#29305;&#24615;&#30340;&#26080;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#12290;&#20551;&#35774;$p$-&#38454;&#30697;&#26377;&#30028;&#65292;&#24471;&#21040;&#30340;&#30028;&#22312;$p=2$&#26102;&#25554;&#20540;&#20026;&#32531;&#24930;&#30340;&#36895;&#29575;$1 / \sqrt{n}$&#65292;&#22312;$p \to \infty$&#19988;&#25439;&#22833;&#20989;&#25968;&#22522;&#26412;&#26377;&#30028;&#26102;&#25554;&#20540;&#20026;&#24555;&#36895;&#30340;&#36895;&#29575;$1 / n$&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23548;&#20986;&#20102;&#20855;&#26377;&#26377;&#30028;&#26041;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#30028;&#12290;&#35813;&#30028;&#23545;&#32622;&#20449;&#21442;&#25968;&#21644;&#20381;&#36182;&#24230;&#37327;&#30340;&#20381;&#36182;&#20851;&#31995;&#30456;&#27604;&#25991;&#29486;&#20013;&#20808;&#21069;&#30340;&#30028;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#23558;&#25152;&#26377;&#32467;&#26524;&#25512;&#24191;&#21040;&#26399;&#26395;&#20445;&#35777;&#21644;&#21333;&#27425;&#25277;&#26679;PAC-Bayes&#20013;&#12290;&#20026;&#27492;&#65292;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#23427;&#33719;&#24471;&#20102;[2]&#20013;&#38024;&#23545;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;PAC-Bayes&#24555;&#36895;&#36895;&#29575;&#30028;&#30340;&#31867;&#20284;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16681v1 Announce Type: cross  Abstract: This paper studies the truncation method from Alquier [1] to derive high-probability PAC-Bayes bounds for unbounded losses with heavy tails. Assuming that the $p$-th moment is bounded, the resulting bounds interpolate between a slow rate $1 / \sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p \to \infty$ and the loss is essentially bounded. Moreover, the paper derives a high-probability PAC-Bayes bound for losses with a bounded variance. This bound has an exponentially better dependence on the confidence parameter and the dependency measure than previous bounds in the literature. Finally, the paper extends all results to guarantees in expectation and single-draw PAC-Bayes. In order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded losses from [2] in these settings.
&lt;/p&gt;</description></item><item><title>&#20174;&#35745;&#25968;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#38750;&#21487;&#36776;&#35782;&#24615;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#27850;&#26494;&#20998;&#25903;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#22914;&#26524;&#26681;&#39030;&#28857;$X$&#26159;&#24050;&#30693;&#30340;&#65292;&#21017;&#21487;&#20197;&#30830;&#23450;&#20174;$X$&#21040;&#20854;&#23376;&#33410;&#28857;$Y$&#30340;&#22240;&#26524;&#39034;&#24207;&#12290;</title><link>https://arxiv.org/abs/2403.16523</link><description>&lt;p&gt;
&#21033;&#29992;&#39640;&#38454;&#32047;&#31215;&#37327;&#21644;&#36335;&#24452;&#20998;&#26512;&#20174;&#27850;&#26494;&#20998;&#25903;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16523
&lt;/p&gt;
&lt;p&gt;
&#20174;&#35745;&#25968;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#38750;&#21487;&#36776;&#35782;&#24615;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#27850;&#26494;&#20998;&#25903;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#22914;&#26524;&#26681;&#39030;&#28857;$X$&#26159;&#24050;&#30693;&#30340;&#65292;&#21017;&#21487;&#20197;&#30830;&#23450;&#20174;$X$&#21040;&#20854;&#23376;&#33410;&#28857;$Y$&#30340;&#22240;&#26524;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#25968;&#25968;&#25454;&#22312;&#37329;&#34701;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#27969;&#34892;&#30149;&#23398;&#31561;&#39046;&#22495;&#20013;&#33258;&#28982;&#20135;&#29983;&#65292;&#22312;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#19994;&#22330;&#26223;&#20013;&#21457;&#29616;&#35745;&#25968;&#25968;&#25454;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#35745;&#25968;&#25968;&#25454;&#30340;&#19968;&#20010;&#26368;&#24120;&#35265;&#29305;&#24449;&#26159;&#30001;&#20108;&#39033;&#24335;&#31232;&#30095;&#36816;&#31639;&#31526;&#21644;&#29420;&#31435;&#30340;&#27850;&#26494;&#20998;&#24067;&#25551;&#36848;&#30340;&#22266;&#26377;&#20998;&#25903;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#25429;&#25417;&#20102;&#20998;&#25903;&#21644;&#22122;&#22768;&#12290;&#20363;&#22914;&#65292;&#22312;&#20154;&#21475;&#35745;&#25968;&#24773;&#26223;&#20013;&#65292;&#27515;&#20129;&#21644;&#31227;&#27665;&#23545;&#35745;&#25968;&#26377;&#36129;&#29486;&#65292;&#20854;&#20013;&#29983;&#23384;&#36981;&#24490;&#20271;&#21162;&#21033;&#20998;&#24067;&#65292;&#31227;&#27665;&#36981;&#24490;&#27850;&#26494;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21487;&#36776;&#35782;&#24615;&#38382;&#39064;&#65292;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#21333;&#19968;&#22240;&#26524;&#23545;&#26159;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#65292;&#21363;$X\rightarrow Y$&#21644;$Y\rightarrow X$&#22312;&#20998;&#24067;&#19978;&#26159;&#31561;&#20215;&#30340;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22914;&#26524;$X$&#26159;&#19968;&#20010;&#26681;&#39030;&#28857;&#65292;&#37027;&#20040;&#20174;$X$&#21040;&#20854;&#23376;&#33410;&#28857;$Y$&#30340;&#22240;&#26524;&#39034;&#24207;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16523v1 Announce Type: cross  Abstract: Count data naturally arise in many fields, such as finance, neuroscience, and epidemiology, and discovering causal structure among count data is a crucial task in various scientific and industrial scenarios. One of the most common characteristics of count data is the inherent branching structure described by a binomial thinning operator and an independent Poisson distribution that captures both branching and noise. For instance, in a population count scenario, mortality and immigration contribute to the count, where survival follows a Bernoulli distribution, and immigration follows a Poisson distribution. However, causal discovery from such data is challenging due to the non-identifiability issue: a single causal pair is Markov equivalent, i.e., $X\rightarrow Y$ and $Y\rightarrow X$ are distributed equivalent. Fortunately, in this work, we found that the causal order from $X$ to its child $Y$ is identifiable if $X$ is a root vertex and
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#19968;&#23450;&#26435;&#37325;&#32422;&#26463;&#30340;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#65292;&#20197;&#21450;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#20570;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#20026;&#22522;&#20110;CNNs&#30340;&#23398;&#20064;&#38382;&#39064;&#25512;&#23548;&#20102;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#21644;&#20108;&#20803;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16459</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the rates of convergence for learning with convolutional neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16459
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#19968;&#23450;&#26435;&#37325;&#32422;&#26463;&#30340;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#65292;&#20197;&#21450;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#20570;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#20026;&#22522;&#20110;CNNs&#30340;&#23398;&#20064;&#38382;&#39064;&#25512;&#23548;&#20102;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#21644;&#20108;&#20803;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#36924;&#36817;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;&#31532;&#19968;&#20010;&#32467;&#26524;&#35777;&#26126;&#20102;&#22312;&#26435;&#37325;&#19978;&#26377;&#19968;&#23450;&#32422;&#26463;&#26465;&#20214;&#19979;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#12290;&#31532;&#20108;&#20010;&#32467;&#26524;&#32473;&#20986;&#20102;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#30340;&#26032;&#20998;&#26512;&#65292;&#20854;&#20013;CNNs&#26159;&#20854;&#29305;&#20363;&#12290;&#35813;&#20998;&#26512;&#35814;&#32454;&#32771;&#34385;&#20102;&#26435;&#37325;&#30340;&#22823;&#23567;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#27604;&#29616;&#26377;&#25991;&#29486;&#26356;&#22909;&#30340;&#19978;&#30028;&#12290;&#21033;&#29992;&#36825;&#20004;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#22522;&#20110;CNNs&#30340;&#20272;&#35745;&#22120;&#22312;&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#35774;&#32622;&#20013;&#20026;&#22522;&#20110;CNNs&#30340;&#26368;&#23567;&#20108;&#20056;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#24314;&#31435;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#23545;&#20110;&#20108;&#20803;&#20998;&#31867;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20855;&#26377;&#38128;&#38142;&#25439;&#22833;&#21644;&#36923;&#36753;&#25439;&#22833;&#30340;CNN&#20998;&#31867;&#22120;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#21516;&#26102;&#36824;&#34920;&#26126;&#25152;&#24471;&#21040;&#30340;&#36895;&#29575;&#22312;&#20960;&#31181;&#24773;&#20917;&#19979;&#26159;&#26497;&#23567;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16459v1 Announce Type: new  Abstract: We study the approximation and learning capacities of convolutional neural networks (CNNs). Our first result proves a new approximation bound for CNNs with certain constraint on the weights. Our second result gives a new analysis on the covering number of feed-forward neural networks, which include CNNs as special cases. The analysis carefully takes into account the size of the weights and hence gives better bounds than existing literature in some situations. Using these two results, we are able to derive rates of convergence for estimators based on CNNs in many learning problems. In particular, we establish minimax optimal convergence rates of the least squares based on CNNs for learning smooth functions in the nonparametric regression setting. For binary classification, we derive convergence rates for CNN classifiers with hinge loss and logistic loss. It is also shown that the obtained rates are minimax optimal in several settings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#36807;&#31243;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#26465;&#20214;&#30417;&#27979;&#20449;&#21495;&#39044;&#27979;&#20013;&#23454;&#29616;&#34920;&#31034;&#33021;&#21147;&#21644;&#25935;&#25463;&#24615;&#30340;&#26435;&#34913;</title><link>https://arxiv.org/abs/2403.16377</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#26631;&#31614;&#24863;&#30693;&#30340;&#31070;&#32463;&#36807;&#31243;&#36827;&#34892;&#23454;&#26102;&#36866;&#24212;&#26465;&#20214;&#30417;&#27979;&#20449;&#21495;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Real-time Adaptation for Condition Monitoring Signal Prediction using Label-aware Neural Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16377
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#36807;&#31243;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#26465;&#20214;&#30417;&#27979;&#20449;&#21495;&#39044;&#27979;&#20013;&#23454;&#29616;&#34920;&#31034;&#33021;&#21147;&#21644;&#25935;&#25463;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#19968;&#20010;&#24555;&#36895;&#36866;&#24212;&#23454;&#26102;&#26465;&#20214;&#30417;&#27979;&#65288;CM&#65289;&#20449;&#21495;&#30340;&#39044;&#27979;&#27169;&#22411;&#23545;&#20110;&#24037;&#31243;&#31995;&#32479;/&#21333;&#20803;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24403;&#21069;&#26041;&#27861;&#22312;&#34920;&#31034;&#33021;&#21147;&#21644;&#22312;&#32447;&#29615;&#22659;&#20013;&#30340;&#25935;&#25463;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#26435;&#34913;&#38382;&#39064;&#12290;&#23427;&#23558;CM&#20449;&#21495;&#20013;&#30340;&#21487;&#29992;&#35266;&#27979;&#32534;&#30721;&#21040;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#37325;&#24314;&#20449;&#21495;&#30340;&#21382;&#21490;&#21644;&#28436;&#21464;&#20197;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16377v1 Announce Type: new  Abstract: Building a predictive model that rapidly adapts to real-time condition monitoring (CM) signals is critical for engineering systems/units. Unfortunately, many current methods suffer from a trade-off between representation power and agility in online settings. For instance, parametric methods that assume an underlying functional form for CM signals facilitate efficient online prediction updates. However, this simplification leads to vulnerability to model specifications and an inability to capture complex signals. On the other hand, approaches based on over-parameterized or non-parametric models can excel at explaining complex nonlinear signals, but real-time updates for such models pose a challenging task. In this paper, we propose a neural process-based approach that addresses this trade-off. It encodes available observations within a CM signal into a representation space and then reconstructs the signal's history and evolution for predi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;</title><link>https://arxiv.org/abs/2403.16369</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21464;&#24615;&#23398;&#20064;&#22522;&#20110;&#21160;&#20316;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Action-based Representations Using Invariance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20351;&#29992;&#39640;&#32500;&#24230;&#35266;&#27979;&#24517;&#39035;&#33021;&#22815;&#22312;&#35768;&#22810;&#22806;&#28304;&#24615;&#24178;&#25200;&#20013;&#35782;&#21035;&#30456;&#20851;&#29366;&#24577;&#29305;&#24449;&#12290;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21487;&#25511;&#24615;&#30340;&#34920;&#31034;&#36890;&#36807;&#30830;&#23450;&#24433;&#21709;&#20195;&#29702;&#25511;&#21046;&#30340;&#22240;&#32032;&#26469;&#35782;&#21035;&#36825;&#20123;&#29366;&#24577;&#20803;&#32032;&#12290;&#34429;&#28982;&#35832;&#22914;&#36870;&#21160;&#21147;&#23398;&#21644;&#20114;&#20449;&#24687;&#31561;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#26377;&#38480;&#25968;&#37327;&#30340;&#26102;&#38388;&#27493;&#30340;&#21487;&#25511;&#24615;&#65292;&#20294;&#25429;&#33719;&#38271;&#26102;&#38388;&#20803;&#32032;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30701;&#35270;&#30340;&#21487;&#25511;&#24615;&#21487;&#20197;&#25429;&#25417;&#20195;&#29702;&#21363;&#23558;&#25758;&#21521;&#22681;&#22721;&#30340;&#30636;&#38388;&#65292;&#20294;&#19981;&#33021;&#22312;&#20195;&#29702;&#36824;&#26377;&#19968;&#23450;&#36317;&#31163;&#20043;&#26102;&#25429;&#25417;&#22681;&#22721;&#30340;&#25511;&#21046;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#27169;&#25311;&#19981;&#21464;&#37327;&#20551;&#24230;&#37327;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#23398;&#20064;&#20102;&#19968;&#20010;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16369v1 Announce Type: cross  Abstract: Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors. A representation that captures controllability identifies these state elements by determining what affects agent control. While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem. Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away. To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint. By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts dist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#22810;&#29615;&#22659;&#39044;&#27979;&#38382;&#39064;&#20013;&#26500;&#24314;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#21644;&#32622;&#20449;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#26041;&#27861;&#20197;&#36866;&#24212;&#38382;&#39064;&#38590;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#39044;&#27979;&#38598;&#22823;&#23567;&#65292;&#36825;&#22312;&#31070;&#32463;&#24863;&#24212;&#21644;&#29289;&#31181;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#38469;&#34920;&#29616;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.16336</link><description>&lt;p&gt;
&#22810;&#29615;&#22659;&#22330;&#26223;&#20013;&#30340;&#39044;&#27979;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Predictive Inference in Multi-environment Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#22810;&#29615;&#22659;&#39044;&#27979;&#38382;&#39064;&#20013;&#26500;&#24314;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#21644;&#32622;&#20449;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#26041;&#27861;&#20197;&#36866;&#24212;&#38382;&#39064;&#38590;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#39044;&#27979;&#38598;&#22823;&#23567;&#65292;&#36825;&#22312;&#31070;&#32463;&#24863;&#24212;&#21644;&#29289;&#31181;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#38469;&#34920;&#29616;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#36328;&#22810;&#20010;&#29615;&#22659;&#30340;&#39044;&#27979;&#38382;&#39064;&#20013;&#26500;&#24314;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#21644;&#32622;&#20449;&#38598;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;&#20004;&#31181;&#35206;&#30422;&#31867;&#22411;&#65292;&#25193;&#23637;&#20102;Jackknife&#21644;&#20998;&#35010;&#19968;&#33268;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#36825;&#31181;&#38750;&#20256;&#32479;&#30340;&#23618;&#27425;&#25968;&#25454;&#29983;&#25104;&#22330;&#26223;&#20013;&#33719;&#24471;&#26080;&#20998;&#24067;&#35206;&#30422;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#36824;&#21253;&#25324;&#23545;&#38750;&#23454;&#20540;&#21709;&#24212;&#35774;&#32622;&#30340;&#25193;&#23637;&#65292;&#20197;&#21450;&#36825;&#20123;&#19968;&#33324;&#38382;&#39064;&#20013;&#39044;&#27979;&#25512;&#26029;&#30340;&#19968;&#33268;&#24615;&#29702;&#35770;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#38382;&#39064;&#38590;&#24230;&#65292;&#36825;&#36866;&#29992;&#20110;&#20855;&#26377;&#23618;&#27425;&#25968;&#25454;&#30340;&#39044;&#27979;&#25512;&#26029;&#30340;&#29616;&#26377;&#26041;&#27861;&#20197;&#21450;&#25105;&#20204;&#24320;&#21457;&#30340;&#26041;&#27861;&#65307;&#36825;&#36890;&#36807;&#31070;&#32463;&#21270;&#23398;&#24863;&#24212;&#21644;&#29289;&#31181;&#20998;&#31867;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16336v1 Announce Type: cross  Abstract: We address the challenge of constructing valid confidence intervals and sets in problems of prediction across multiple environments. We investigate two types of coverage suitable for these problems, extending the jackknife and split-conformal methods to show how to obtain distribution-free coverage in such non-traditional, hierarchical data-generating scenarios. Our contributions also include extensions for settings with non-real-valued responses and a theory of consistency for predictive inference in these general problems. We demonstrate a novel resizing method to adapt to problem difficulty, which applies both to existing approaches for predictive inference with hierarchical data and the methods we develop; this reduces prediction set sizes using limited information from the test environment, a key to the methods' practical performance, which we evaluate through neurochemical sensing and species classification datasets.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#20316;&#32773;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#20256;&#32479;&#27169;&#22411;&#38598;&#25104;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#22810;&#26679;&#24615;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16260</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#22810;&#29702;&#35299;&#38598;&#25104;&#23454;&#29616;&#36234;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16260
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#20316;&#32773;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#20256;&#32479;&#27169;&#22411;&#38598;&#25104;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#22810;&#26679;&#24615;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#36234;&#30028;&#65288;OOD&#65289;&#29305;&#24449;&#34920;&#31034;&#39046;&#22495;&#35268;&#27169;&#23545;&#27169;&#22411;&#22312;OOD&#26816;&#27979;&#20013;&#25928;&#26524;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#27169;&#22411;&#38598;&#25104;&#20316;&#20026;&#22686;&#24378;&#36825;&#19968;&#29305;&#24449;&#34920;&#31034;&#39046;&#22495;&#30340;&#31361;&#20986;&#31574;&#30053;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#26399;&#30340;&#27169;&#22411;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#25439;&#22833;&#30406;/&#38556;&#30861;&#21487;&#35270;&#21270;&#21644;&#33258;&#32806;&#21512;&#25351;&#25968;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#21253;&#21547;&#21487;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#30340;&#26435;&#37325;&#65292;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#21487;&#21464;&#24615;&#65292;&#20174;&#32780;&#26410;&#33021;&#23454;&#29616;&#29305;&#24449;&#34920;&#31034;&#20013;&#25152;&#38656;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16260v1 Announce Type: cross  Abstract: Recent research underscores the pivotal role of the Out-of-Distribution (OOD) feature representation field scale in determining the efficacy of models in OOD detection. Consequently, the adoption of model ensembles has emerged as a prominent strategy to augment this feature representation field, capitalizing on anticipated model diversity.   However, our introduction of novel qualitative and quantitative model ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and the Self-Coupling Index, reveals a critical drawback in existing ensemble methods. We find that these methods incorporate weights that are affine-transformable, exhibiting limited variability and thus failing to achieve the desired diversity in feature representation.   To address this limitation, we elevate the dimensions of traditional model ensembles, incorporating various factors such as different weight initializations, data holdout, etc., into di
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26679;&#26412;&#30340;&#30697;&#20256;&#25773;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#34920;&#24449;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#20854;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#25552;&#20379;&#20102;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#20256;&#36882;&#30340;&#38543;&#26426;&#21464;&#37327;&#21327;&#26041;&#24046;&#30340;&#35299;&#26512;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16163</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#21327;&#26041;&#24046;&#20256;&#25773;&#30340;&#35299;&#26512;&#35299;
&lt;/p&gt;
&lt;p&gt;
An Analytic Solution to Covariance Propagation in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26679;&#26412;&#30340;&#30697;&#20256;&#25773;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#34920;&#24449;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#20854;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#25552;&#20379;&#20102;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#20256;&#36882;&#30340;&#38543;&#26426;&#21464;&#37327;&#21327;&#26041;&#24046;&#30340;&#35299;&#26512;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#34913;&#37327;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#28041;&#21450;&#26114;&#36149;&#25110;&#19981;&#20934;&#30830;&#30340;&#37319;&#26679;&#26041;&#27861;&#21644;&#36817;&#20284;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26679;&#26412;&#30340;&#30697;&#20256;&#25773;&#25216;&#26415;&#65292;&#36890;&#36807;&#32593;&#32476;&#20256;&#25773;&#22343;&#20540;&#21521;&#37327;&#21644;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#20934;&#30830;&#34920;&#24449;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#20026;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;Heaviside&#12289;ReLU&#21644;GELU&#65289;&#20256;&#36882;&#30340;&#38543;&#26426;&#21464;&#37327;&#30340;&#21327;&#26041;&#24046;&#25552;&#20379;&#20102;&#35299;&#26512;&#35299;&#12290;&#36890;&#36807;&#20998;&#26512;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#20197;&#21450;&#35757;&#32451;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#25216;&#26415;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16163v1 Announce Type: cross  Abstract: Uncertainty quantification of neural networks is critical to measuring the reliability and robustness of deep learning systems. However, this often involves costly or inaccurate sampling methods and approximations. This paper presents a sample-free moment propagation technique that propagates mean vectors and covariance matrices across a network to accurately characterize the input-output distributions of neural networks. A key enabler of our technique is an analytic solution for the covariance of random variables passed through nonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide applicability and merits of the proposed technique are shown in experiments analyzing the input-output distributions of trained neural networks and training Bayesian neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25913;&#36827;&#25193;&#25955;&#26144;&#23556;&#30340;&#27969;&#24418;&#27491;&#21017;&#21270;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#26631;&#31614;&#20256;&#25773;&#27169;&#22411;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#21407;&#22987;&#27969;&#24418;&#27491;&#21017;&#21270;&#27169;&#22411;&#22312;&#23616;&#37096;&#21306;&#22495;&#24615;&#33021;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16059</link><description>&lt;p&gt;
&#22522;&#20110;&#25913;&#36827;&#30340;&#25193;&#25955;&#26144;&#23556;&#30340;&#27969;&#24418;&#27491;&#21017;&#21270;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Manifold Regularization Classification Model Based On Improved Diffusion Map
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25913;&#36827;&#25193;&#25955;&#26144;&#23556;&#30340;&#27969;&#24418;&#27491;&#21017;&#21270;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#26631;&#31614;&#20256;&#25773;&#27169;&#22411;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#21407;&#22987;&#27969;&#24418;&#27491;&#21017;&#21270;&#27169;&#22411;&#22312;&#23616;&#37096;&#21306;&#22495;&#24615;&#33021;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#27491;&#21017;&#21270;&#27169;&#22411;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#21253;&#25324;&#23569;&#37327;&#26377;&#26631;&#31614;&#26679;&#26412;&#21644;&#22823;&#37327;&#26080;&#26631;&#31614;&#26679;&#26412;&#65292;&#29983;&#25104;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;&#27969;&#24418;&#33539;&#25968;&#38480;&#21046;&#20102;&#27169;&#22411;&#24615;&#33021;&#21482;&#23616;&#38480;&#20110;&#23616;&#37096;&#21306;&#22495;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#27969;&#24418;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#26631;&#31614;&#20256;&#25773;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#22686;&#24378;&#25193;&#25955;&#26144;&#23556;&#31639;&#27861;&#30340;&#27010;&#29575;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;Neumann&#28909;&#26680;&#65292;&#20351;&#20854;&#33021;&#22815;&#20934;&#30830;&#25551;&#36848;&#27969;&#24418;&#19978;&#30340;&#26631;&#31614;&#20256;&#25773;&#36807;&#31243;&#12290;&#21033;&#29992;&#35813;&#30697;&#38453;&#65292;&#22312;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#19968;&#20010;&#25551;&#36848;&#19981;&#21516;&#26102;&#38388;&#27493;&#39588;&#19979;&#26631;&#31614;&#20998;&#24067;&#30340;&#26631;&#31614;&#20256;&#25773;&#20989;&#25968;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#26631;&#31614;&#20256;&#25773;&#20989;&#25968;&#25193;&#23637;&#21040;&#25972;&#20010;&#25968;&#25454;&#27969;&#24418;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25193;&#23637;&#30340;&#26631;&#31614;&#20256;&#25773;&#20989;&#25968;c
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16059v1 Announce Type: cross  Abstract: Manifold regularization model is a semi-supervised learning model that leverages the geometric structure of a dataset, comprising a small number of labeled samples and a large number of unlabeled samples, to generate classifiers. However, the original manifold norm limits the performance of models to local regions. To address this limitation, this paper proposes an approach to improve manifold regularization based on a label propagation model. We initially enhance the probability transition matrix of the diffusion map algorithm, which can be used to estimate the Neumann heat kernel, enabling it to accurately depict the label propagation process on the manifold. Using this matrix, we establish a label propagation function on the dataset to describe the distribution of labels at different time steps. Subsequently, we extend the label propagation function to the entire data manifold. We prove that the extended label propagation function c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#24403;&#21487;&#29992;&#37096;&#20998;&#22240;&#26524;&#39034;&#24207;&#21464;&#37327;&#26102;&#23398;&#20064;DAGs&#30340;&#20013;&#38388;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20272;&#35745;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#30340;&#20272;&#35745;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16031</link><description>&lt;p&gt;
&#20174;&#20559;&#24207;&#20851;&#31995;&#20013;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning Directed Acyclic Graphs from Partial Orderings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24403;&#21487;&#29992;&#37096;&#20998;&#22240;&#26524;&#39034;&#24207;&#21464;&#37327;&#26102;&#23398;&#20064;DAGs&#30340;&#20013;&#38388;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20272;&#35745;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#30340;&#20272;&#35745;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#36890;&#24120;&#29992;&#20110;&#27169;&#25311;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#24120;&#26469;&#35828;&#65292;&#23398;&#20064;DAG&#32467;&#26500;&#22312;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#27809;&#26377;&#39069;&#22806;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#36793;&#30340;&#26041;&#21521;&#21487;&#33021;&#26080;&#27861;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#20986;&#26469;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#21487;&#29992;&#37096;&#20998;&#22240;&#26524;&#39034;&#24207;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;DAGs&#30340;&#20013;&#38388;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#37096;&#20998;&#39034;&#24207;&#30340;&#36890;&#29992;&#20272;&#35745;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#20302;&#32500;&#21644;&#39640;&#32500;&#38382;&#39064;&#30340;&#26377;&#25928;&#20272;&#35745;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#20248;&#21183;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16031v1 Announce Type: cross  Abstract: Directed acyclic graphs (DAGs) are commonly used to model causal relationships among random variables. In general, learning the DAG structure is both computationally and statistically challenging. Moreover, without additional information, the direction of edges may not be estimable from observational data. In contrast, given a complete causal ordering of the variables, the problem can be solved efficiently, even in high dimensions. In this paper, we consider the intermediate problem of learning DAGs when a partial causal ordering of variables is available. We propose a general estimation framework for leveraging the partial ordering and present efficient estimation algorithms for low- and high-dimensional problems. The advantages of the proposed framework are illustrated via numerical studies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#30165;&#36857;&#22238;&#24402;&#27169;&#22411;&#19979;&#20197;&#39640;&#26031;&#27979;&#37327;&#30697;&#38453;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#20445;&#35777;&#31169;&#26377;&#21021;&#22987;&#21270;&#30340;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;DP&#21021;&#22987;&#21270;&#31639;&#27861;&#21644;&#22522;&#20110;Riemannian&#20248;&#21270;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#20272;&#35745;&#32467;&#26524;&#30340;&#38750;&#24179;&#20961;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.15999</link><description>&lt;p&gt;
&#20855;&#26377;&#20445;&#35777;&#31169;&#26377;&#21021;&#22987;&#21270;&#30340;&#36817;&#26368;&#20248;&#24046;&#20998;&#38544;&#31169;&#20302;&#31209;&#30165;&#36857;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal differentially private low-rank trace regression with guaranteed private initialization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15999
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#30165;&#36857;&#22238;&#24402;&#27169;&#22411;&#19979;&#20197;&#39640;&#26031;&#27979;&#37327;&#30697;&#38453;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#20445;&#35777;&#31169;&#26377;&#21021;&#22987;&#21270;&#30340;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;DP&#21021;&#22987;&#21270;&#31639;&#27861;&#21644;&#22522;&#20110;Riemannian&#20248;&#21270;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#20272;&#35745;&#32467;&#26524;&#30340;&#38750;&#24179;&#20961;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#39640;&#26031;&#27979;&#37327;&#30697;&#38453;&#30340;&#30165;&#36857;&#22238;&#24402;&#27169;&#22411;&#19979;&#65292;&#23545;&#31209;&#20026;$r$&#30340;&#30697;&#38453;$M \in \RR^{d_1\times d_2}$&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20272;&#35745;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#31934;&#30830;&#34920;&#24449;&#20102;&#38750;&#31169;&#26377;&#35889;&#21021;&#22987;&#21270;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#22312;Schatten-$q$&#33539;&#25968;&#19979;&#20272;&#35745;$M$&#30340;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#26497;&#23567;&#27010;&#29575;&#19979;&#30028;&#12290;&#22312;&#26041;&#27861;&#35770;&#19978;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;DP&#21021;&#22987;&#21270;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#22823;&#23567;&#20026;$n \geq \wt O (r^2 (d_1\vee d_2))$&#12290;&#22312;&#19968;&#23450;&#30340;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;DP&#21021;&#22987;&#21270;&#33853;&#20837;&#22260;&#32469;$M$&#30340;&#23616;&#37096;&#29699;&#20869;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40654;&#26364;&#20248;&#21270;&#30340;&#29992;&#20110;&#20272;&#35745;$M$&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65288;DP-RGrad&#65289;&#65292;&#36890;&#36807;DP&#21021;&#22987;&#21270;&#21644;&#26679;&#26412;&#22823;&#23567;$n \geq \wt O(r (d_1 + d_2))$&#23454;&#29616;&#20102;&#36817;&#26368;&#20248;&#25910;&#25947;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#21021;&#22987;&#21270;&#21644;&#26679;&#26412;&#22823;&#23567;&#26465;&#20214;&#19979;&#30340;&#25152;&#20272;&#35745;&#30340;$M$&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15999v1 Announce Type: cross  Abstract: We study differentially private (DP) estimation of a rank-$r$ matrix $M \in \RR^{d_1\times d_2}$ under the trace regression model with Gaussian measurement matrices. Theoretically, the sensitivity of non-private spectral initialization is precisely characterized, and the differential-privacy-constrained minimax lower bound for estimating $M$ under the Schatten-$q$ norm is established. Methodologically, the paper introduces a computationally efficient algorithm for DP-initialization with a sample size of $n \geq \wt O (r^2 (d_1\vee d_2))$. Under certain regularity conditions, the DP-initialization falls within a local ball surrounding $M$. We also propose a differentially private algorithm for estimating $M$ based on Riemannian optimization (DP-RGrad), which achieves a near-optimal convergence rate with the DP-initialization and sample size of $n \geq \wt O(r (d_1 + d_2))$. Finally, the paper discusses the non-trivial gap between the mi
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#36712;&#36857;&#25277;&#26679;&#21644;&#28145;&#24230;&#39640;&#26031;&#21327;&#26041;&#24046;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#25968;&#25454;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15908</link><description>&lt;p&gt;
&#20351;&#29992;&#36712;&#36857;&#25277;&#26679;&#30340;&#28145;&#24230;&#39640;&#26031;&#21327;&#26041;&#24046;&#32593;&#32476;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#31574;&#30053;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15908
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#36712;&#36857;&#25277;&#26679;&#21644;&#28145;&#24230;&#39640;&#26031;&#21327;&#26041;&#24046;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#25968;&#25454;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#19990;&#30028;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#20854;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#25351;&#23548;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#25913;&#21892;&#20102;&#25506;&#32034;&#24615;&#33021;&#24182;&#33719;&#24471;&#20102;&#26032;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#27010;&#29575;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#23398;&#20064;&#27969;&#31243;&#23548;&#33268;&#30340;&#31283;&#20581;&#31574;&#30053;&#27604;&#19981;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#23545;&#22122;&#22768;&#35266;&#27979;&#26356;&#19981;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#36712;&#36857;&#25277;&#26679;&#21644;&#28145;&#24230;&#39640;&#26031;&#21327;&#26041;&#24046;&#32593;&#32476;&#65288;DGCN&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#26368;&#20248;&#25511;&#21046;&#29615;&#22659;&#20013;&#23454;&#29616;MBRL&#38382;&#39064;&#30340;&#25968;&#25454;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#12289;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;DGCN&#19977;&#31181;&#19981;&#21516;&#30340;&#27010;&#29575;&#19990;&#30028;&#27169;&#22411;&#65292;&#27604;&#36739;&#20102;&#36712;&#36857;&#25277;&#26679;&#21644;&#22522;&#20110;&#23494;&#24230;&#30340;&#36817;&#20284;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#19981;&#21516;&#30340;&#30693;&#21517;&#27979;&#35797;&#29615;&#22659;&#25552;&#20379;&#20102;&#32463;&#39564;&#35777;&#25454;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#20854;&#20182;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#26041;&#27861;&#21644;&#27010;&#29575;&#19990;&#30028;&#27169;&#22411;&#32452;&#21512;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15908v1 Announce Type: new  Abstract: Probabilistic world models increase data efficiency of model-based reinforcement learning (MBRL) by guiding the policy with their epistemic uncertainty to improve exploration and acquire new samples. Moreover, the uncertainty-aware learning procedures in probabilistic approaches lead to robust policies that are less sensitive to noisy observations compared to uncertainty unaware solutions. We propose to combine trajectory sampling and deep Gaussian covariance network (DGCN) for a data-efficient solution to MBRL problems in an optimal control setting. We compare trajectory sampling with density-based approximation for uncertainty propagation using three different probabilistic world models; Gaussian processes, Bayesian neural networks, and DGCNs. We provide empirical evidence using four different well-known test environments, that our method improves the sample-efficiency over other combinations of uncertainty propagation methods and prob
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#36866;&#29992;&#20110;&#25152;&#26377;&#23454;&#29992;&#30340;&#24402;&#19968;&#21270;&#27969;&#26550;&#26500;&#65292;&#20855;&#26377;&#27491;&#21017;&#21270;&#25928;&#26524;&#24182;&#20943;&#23567;&#20102;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.15881</link><description>&lt;p&gt;
&#24555;&#36895;&#32479;&#19968;&#30340;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#22120;&#29992;&#20110;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Fast and Unified Path Gradient Estimators for Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15881
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#36866;&#29992;&#20110;&#25152;&#26377;&#23454;&#29992;&#30340;&#24402;&#19968;&#21270;&#27969;&#26550;&#26500;&#65292;&#20855;&#26377;&#27491;&#21017;&#21270;&#25928;&#26524;&#24182;&#20943;&#23567;&#20102;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24402;&#19968;&#21270;&#27969;&#30340;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#22120;&#19982;&#21464;&#20998;&#25512;&#26029;&#30340;&#26631;&#20934;&#20272;&#35745;&#22120;&#30456;&#27604;&#20855;&#26377;&#26356;&#20302;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20174;&#35745;&#31639;&#35282;&#24230;&#30475;&#65292;&#23427;&#20204;&#24448;&#24448;&#26114;&#36149;&#19988;&#26080;&#27861;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#65292;&#20005;&#37325;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20123;&#20851;&#38190;&#38480;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#36866;&#29992;&#20110;&#25152;&#26377;&#23454;&#29992;&#30340;&#24402;&#19968;&#21270;&#27969;&#26550;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#20272;&#35745;&#22120;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#65292;&#23545;&#27492;&#20855;&#26377;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#32771;&#34385;&#32473;&#23450;&#30446;&#26631;&#33021;&#37327;&#20989;&#25968;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#20854;&#22312;&#22810;&#20010;&#33258;&#28982;&#31185;&#23398;&#24212;&#29992;&#20013;&#34920;&#29616;&#21331;&#36234;&#65292;&#24182;&#20943;&#23567;&#20102;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15881v1 Announce Type: new  Abstract: Recent work shows that path gradient estimators for normalizing flows have lower variance compared to standard estimators for variational inference, resulting in improved training. However, they are often prohibitively more expensive from a computational point of view and cannot be applied to maximum likelihood training in a scalable manner, which severely hinders their widespread adoption. In this work, we overcome these crucial limitations. Specifically, we propose a fast path gradient estimator which improves computational efficiency significantly and works for all normalizing flow architectures of practical relevance. We then show that this estimator can also be applied to maximum likelihood training for which it has a regularizing effect as it can take the form of a given target energy function into account. We empirically establish its superior performance and reduced variance for several natural sciences applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#31283;&#23450;&#36335;&#24452;&#30340;&#26032;&#31283;&#23450;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#25552;&#39640;&#29305;&#24449;&#36873;&#25321;&#30340;&#28789;&#25935;&#24230;&#24182;&#26356;&#22909;&#22320;&#26657;&#20934;&#30446;&#26631;&#20551;&#38451;&#24615;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.15877</link><description>&lt;p&gt;
&#38598;&#25104;&#36335;&#24452;&#31283;&#23450;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Integrated path stability selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#31283;&#23450;&#36335;&#24452;&#30340;&#26032;&#31283;&#23450;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#25552;&#39640;&#29305;&#24449;&#36873;&#25321;&#30340;&#28789;&#25935;&#24230;&#24182;&#26356;&#22909;&#22320;&#26657;&#20934;&#30446;&#26631;&#20551;&#38451;&#24615;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#36873;&#25321;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#25913;&#21892;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24050;&#21457;&#29616;&#31283;&#23450;&#36873;&#25321;&#36807;&#20110;&#20445;&#23432;&#65292;&#23548;&#33268;&#28789;&#25935;&#24230;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#23545;&#26399;&#26395;&#30340;&#20551;&#38451;&#24615;&#25968;&#37327;&#30340;&#29702;&#35770;&#30028;&#38480;E(FP)&#30456;&#23545;&#36739;&#26494;&#65292;&#38590;&#20197;&#30693;&#36947;&#23454;&#36341;&#20013;&#20250;&#26377;&#22810;&#23569;&#20551;&#38451;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#31283;&#23450;&#36335;&#24452;&#32780;&#38750;&#26368;&#22823;&#21270;&#31283;&#23450;&#36335;&#24452;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20135;&#29983;&#20102;&#23545;E(FP)&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#65292;&#23548;&#33268;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#28789;&#25935;&#24230;&#30340;&#29305;&#24449;&#36873;&#25321;&#26631;&#20934;&#65292;&#24182;&#19988;&#22312;&#19982;&#30446;&#26631;E(FP)&#21305;&#37197;&#26041;&#38754;&#26356;&#22909;&#22320;&#26657;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#21407;&#22987;&#31283;&#23450;&#36873;&#25321;&#31639;&#27861;&#38656;&#35201;&#30456;&#21516;&#25968;&#37327;&#30340;&#35745;&#31639;&#65292;&#19988;&#20165;&#38656;&#35201;&#29992;&#25143;&#25351;&#23450;&#19968;&#20010;&#36755;&#20837;&#21442;&#25968;&#65292;&#21363;E(FP)&#30340;&#30446;&#26631;&#20540;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24615;&#33021;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15877v1 Announce Type: cross  Abstract: Stability selection is a widely used method for improving the performance of feature selection algorithms. However, stability selection has been found to be highly conservative, resulting in low sensitivity. Further, the theoretical bound on the expected number of false positives, E(FP), is relatively loose, making it difficult to know how many false positives to expect in practice. In this paper, we introduce a novel method for stability selection based on integrating the stability paths rather than maximizing over them. This yields a tighter bound on E(FP), resulting in a feature selection criterion that has higher sensitivity in practice and is better calibrated in terms of matching the target E(FP). Our proposed method requires the same amount of computation as the original stability selection algorithm, and only requires the user to specify one input parameter, a target value for E(FP). We provide theoretical bounds on performance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#24230;&#37327;&#33021;&#22815;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#38405;&#35835;&#36895;&#24230;&#65292;&#20026;&#26410;&#26469;&#25972;&#21512;LLMs&#21644;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.15822</link><description>&lt;p&gt;
&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Computational Sentence-level Metrics Predicting Human Sentence Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#24230;&#37327;&#33021;&#22815;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#38405;&#35835;&#36895;&#24230;&#65292;&#20026;&#26410;&#26469;&#25972;&#21512;LLMs&#21644;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#24515;&#29702;&#35821;&#35328;&#23398;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#21333;&#35789;&#22788;&#29702;&#19978;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#21477;&#23376;&#32423;&#24230;&#37327;&#12290;&#24320;&#21457;&#30340;&#24230;&#37327;&#21253;&#25324;&#21477;&#23376;&#24847;&#22806;&#24615;&#21644;&#21477;&#23376;&#30456;&#20851;&#24615;&#65292;&#28982;&#21518;&#32463;&#36807;&#27979;&#35797;&#21644;&#27604;&#36739;&#20197;&#39564;&#35777;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;&#20154;&#31867;&#22914;&#20309;&#36328;&#35821;&#35328;&#25972;&#20307;&#29702;&#35299;&#21477;&#23376;&#12290;&#36825;&#20123;&#24230;&#37327;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#39044;&#27979;&#20154;&#31867;&#21477;&#23376;&#38405;&#35835;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#35745;&#31639;&#30340;&#21477;&#23376;&#32423;&#24230;&#37327;&#22312;&#39044;&#27979;&#21644;&#38416;&#26126;&#35835;&#32773;&#22312;&#29702;&#35299;&#25972;&#20307;&#21477;&#23376;&#26102;&#36935;&#21040;&#30340;&#22788;&#29702;&#22256;&#38590;&#26041;&#38754;&#24322;&#24120;&#26377;&#25928;&#65292;&#21487;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#12290;&#23427;&#20204;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#20026;&#26410;&#26469;&#22312;&#25972;&#21512;LLMs&#21644;&#35748;&#30693;&#31185;&#23398;&#26041;&#38754;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15822v1 Announce Type: new  Abstract: The majority of research in computational psycholinguistics has concentrated on the processing of words. This study introduces innovative methods for computing sentence-level metrics using multilingual large language models. The metrics developed sentence surprisal and sentence relevance and then are tested and compared to validate whether they can predict how humans comprehend sentences as a whole across languages. These metrics offer significant interpretability and achieve high accuracy in predicting human sentence reading speeds. Our results indicate that these computational sentence-level metrics are exceptionally effective at predicting and elucidating the processing difficulties encountered by readers in comprehending sentences as a whole across a variety of languages. Their impressive performance and generalization capabilities provide a promising avenue for future research in integrating LLMs and cognitive science.
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#26412;&#25991;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#20013;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28151;&#21512;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#32553;&#25918;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#22635;&#34917;&#20102;&#30740;&#31350;&#20013;&#30340;&#32570;&#21475;&#12290;</title><link>https://arxiv.org/abs/2403.15790</link><description>&lt;p&gt;
ISS&#30331;&#26426;&#65306;&#19981;&#24179;&#34913;&#30340;&#33258;&#30417;&#30563;&#65306;&#21457;&#29616;&#29992;&#20110;&#28151;&#21512;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#32553;&#25918;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled Autoencoder for Mixed Tabular Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15790
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#26412;&#25991;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#20013;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28151;&#21512;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#32553;&#25918;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#22635;&#34917;&#20102;&#30740;&#31350;&#20013;&#30340;&#32570;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15790v1&#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#65292;&#19981;&#22826;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#25506;&#35752;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#24179;&#34913;&#22312;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#20013;&#25152;&#24102;&#26469;&#30340;&#20855;&#20307;&#25361;&#25112;&#65292;&#37325;&#28857;&#25918;&#22312;&#33258;&#21160;&#32534;&#30721;&#22120;&#19978;&#12290;&#33258;&#21160;&#32534;&#30721;&#22120;&#24191;&#27867;&#29992;&#20110;&#23398;&#20064;&#21644;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#26032;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#38477;&#32500;&#12290;&#23427;&#20204;&#20063;&#32463;&#24120;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#65292;&#22914;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#25152;&#35265;&#12290;&#22312;&#22788;&#29702;&#28151;&#21512;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#23450;&#24615;&#21464;&#37327;&#36890;&#24120;&#20351;&#29992;&#29420;&#28909;&#32534;&#30721;&#22120;&#19982;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#65288;&#22343;&#26041;&#35823;&#24046;&#25110;&#20132;&#21449;&#29109;&#65289;&#36827;&#34892;&#32534;&#30721;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#29305;&#21035;&#26159;&#24403;&#20998;&#31867;&#21464;&#37327;&#19981;&#24179;&#34913;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#20197;&#24179;&#34913;&#23398;&#20064;&#65306;&#19968;&#20010;&#22810;&#30417;&#30563;&#30340; Ba
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15790v1 Announce Type: new  Abstract: The field of imbalanced self-supervised learning, especially in the context of tabular data, has not been extensively studied. Existing research has predominantly focused on image datasets. This paper aims to fill this gap by examining the specific challenges posed by data imbalance in self-supervised learning in the domain of tabular data, with a primary focus on autoencoders. Autoencoders are widely employed for learning and constructing a new representation of a dataset, particularly for dimensionality reduction. They are also often used for generative model learning, as seen in variational autoencoders. When dealing with mixed tabular data, qualitative variables are often encoded using a one-hot encoder with a standard loss function (MSE or Cross Entropy). In this paper, we analyze the drawbacks of this approach, especially when categorical variables are imbalanced. We propose a novel metric to balance learning: a Multi-Supervised Ba
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35770;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;&#21151;&#33021;&#25968;&#25454;&#30340;&#38598;&#25104;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#30340;&#21151;&#33021;&#25968;&#25454;&#34920;&#31034;&#35757;&#32451;&#38598;&#25104;&#25104;&#21592;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#32452;&#21512;&#22522;&#27169;&#22411;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.15778</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#21270;&#21151;&#33021;&#34920;&#31034;&#30340;&#38598;&#25104;&#23398;&#20064;&#65306;&#21151;&#33021;&#25237;&#31080;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Supervised Learning via Ensembles of Diverse Functional Representations: the Functional Voting Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15778
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35770;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;&#21151;&#33021;&#25968;&#25454;&#30340;&#38598;&#25104;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#30340;&#21151;&#33021;&#25968;&#25454;&#34920;&#31034;&#35757;&#32451;&#38598;&#25104;&#25104;&#21592;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#32452;&#21512;&#22522;&#27169;&#22411;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20256;&#32479;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#30452;&#25509;&#24212;&#29992;&#20110;&#39640;&#32500;&#26102;&#24207;&#35266;&#27979;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#26368;&#36817;&#20960;&#21313;&#24180;&#20013;&#65292;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;(FDA)&#20316;&#20026;&#19968;&#31181;&#27169;&#25311;&#21644;&#20998;&#26512;&#22825;&#28982;&#20026;&#26102;&#38388;&#22495;&#20869;&#30340;&#20989;&#25968;&#30340;&#25968;&#25454;&#30340;&#26694;&#26550;&#24050;&#32463;&#24191;&#27867;&#27969;&#34892;&#36215;&#26469;&#12290;&#34429;&#28982;&#22312;FDA&#25991;&#29486;&#20013;&#23545;&#30417;&#30563;&#20998;&#31867;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#35752;&#65292;&#20294;&#21151;&#33021;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#23398;&#20064;&#21364;&#26368;&#36817;&#25165;&#25104;&#20026;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#35805;&#39064;&#12290;&#22240;&#27492;&#65292;&#21518;&#32773;&#20174;&#21508;&#31181;&#32479;&#35745;&#35282;&#24230;&#21576;&#29616;&#20986;&#26410;&#32463;&#25506;&#32034;&#30340;&#26041;&#38754;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#30340;&#28966;&#28857;&#22312;&#20110;&#21151;&#33021;&#25968;&#25454;&#30340;&#38598;&#25104;&#23398;&#20064;&#65292;&#24182;&#26088;&#22312;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#30340;&#21151;&#33021;&#25968;&#25454;&#34920;&#31034;&#26469;&#35757;&#32451;&#38598;&#25104;&#25104;&#21592;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#26469;&#32452;&#21512;&#22522;&#27169;&#22411;&#39044;&#27979;&#12290;&#25152;&#35859;&#30340;&#21151;&#33021;&#25237;&#31080;Cla
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15778v1 Announce Type: cross  Abstract: Many conventional statistical and machine learning methods face challenges when applied directly to high dimensional temporal observations. In recent decades, Functional Data Analysis (FDA) has gained widespread popularity as a framework for modeling and analyzing data that are, by their nature, functions in the domain of time. Although supervised classification has been extensively explored in recent decades within the FDA literature, ensemble learning of functional classifiers has only recently emerged as a topic of significant interest. Thus, the latter subject presents unexplored facets and challenges from various statistical perspectives. The focal point of this paper lies in the realm of ensemble learning for functional data and aims to show how different functional data representations can be used to train ensemble members and how base model predictions can be combined through majority voting. The so-called Functional Voting Cla
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#30830;&#23450;&#20102;&#22312;&#28508;&#22312;&#38468;&#21152;&#22122;&#22768;&#27169;&#22411;&#32972;&#26223;&#19979;&#23548;&#33268;&#21487;&#35782;&#21035;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#31867;&#22411;&#30340;&#20805;&#20998;&#19988;&#24517;&#35201;&#26465;&#20214;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#24403;&#21482;&#26377;&#37096;&#20998;&#20998;&#24067;&#21464;&#21270;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15711</link><description>&lt;p&gt;
&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Identifiable Latent Neural Causal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15711
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30830;&#23450;&#20102;&#22312;&#28508;&#22312;&#38468;&#21152;&#22122;&#22768;&#27169;&#22411;&#32972;&#26223;&#19979;&#23548;&#33268;&#21487;&#35782;&#21035;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#31867;&#22411;&#30340;&#20805;&#20998;&#19988;&#24517;&#35201;&#26465;&#20214;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#24403;&#21482;&#26377;&#37096;&#20998;&#20998;&#24067;&#21464;&#21270;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#26088;&#22312;&#20174;&#20302;&#32423;&#35266;&#27979;&#25968;&#25454;&#20013;&#25581;&#31034;&#28508;&#22312;&#30340;&#39640;&#32423;&#22240;&#26524;&#34920;&#24449;&#12290;&#23427;&#29305;&#21035;&#25797;&#38271;&#39044;&#27979;&#22312;&#26410;&#35265;&#20998;&#24067;&#21464;&#21270;&#19979;&#65292;&#22240;&#20026;&#36825;&#20123;&#21464;&#21270;&#36890;&#24120;&#21487;&#20197;&#35299;&#37322;&#20026;&#24178;&#39044;&#30340;&#21518;&#26524;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;{&#24050;&#35265;}&#20998;&#24067;&#21464;&#21270;&#25104;&#20026;&#24110;&#21161;&#35782;&#21035;&#22240;&#26524;&#34920;&#24449;&#30340;&#33258;&#28982;&#31574;&#30053;&#65292;&#36827;&#32780;&#26377;&#21161;&#20110;&#39044;&#27979;&#20197;&#21069;{&#26410;&#35265;}&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#30830;&#23450;&#36825;&#20123;&#20998;&#24067;&#21464;&#21270;&#30340;&#31867;&#22411;&#65288;&#25110;&#26465;&#20214;&#65289;&#23545;&#20110;&#22240;&#26524;&#34920;&#24449;&#30340;&#21487;&#35782;&#21035;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#35813;&#24037;&#20316;&#24314;&#31435;&#20102;&#22312;&#28508;&#22312;&#38468;&#21152;&#22122;&#22768;&#27169;&#22411;&#32972;&#26223;&#19979;&#65292;&#34920;&#24449;&#23548;&#33268;&#21487;&#35782;&#21035;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#31867;&#22411;&#30340;&#20805;&#20998;&#19988;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24403;&#21482;&#26377;&#37096;&#20998;&#20998;&#24067;&#21464;&#21270;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15711v1 Announce Type: new  Abstract: Causal representation learning seeks to uncover latent, high-level causal representations from low-level observed data. It is particularly good at predictions under unseen distribution shifts, because these shifts can generally be interpreted as consequences of interventions. Hence leveraging {seen} distribution shifts becomes a natural strategy to help identifying causal representations, which in turn benefits predictions where distributions are previously {unseen}. Determining the types (or conditions) of such distribution shifts that do contribute to the identifiability of causal representations is critical. This work establishes a {sufficient} and {necessary} condition characterizing the types of distribution shifts for identifiability in the context of latent additive noise models. Furthermore, we present partial identifiability results when only a portion of distribution shifts meets the condition. In addition, we extend our findin
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#26032;&#30340;Dynamic Signal Distribution (DSD)&#20998;&#31867;&#20219;&#21153;&#65292;&#27169;&#25311;&#22270;&#20687;&#30001;$k$&#20010;&#32500;&#24230;&#20026;$d$&#30340;&#34917;&#19969;&#32452;&#25104;&#65292;&#20197;&#35299;&#20915;CNNs&#30456;&#23545;&#20110;LCNs&#21644;FCNs&#30340;&#32479;&#35745;&#20248;&#21183;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.15707</link><description>&lt;p&gt;
&#22320;&#22495;&#24615;&#21644;&#26435;&#37325;&#20849;&#20139;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65306;CNN&#12289;LCN&#21644;FCN&#20043;&#38388;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15707
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#26032;&#30340;Dynamic Signal Distribution (DSD)&#20998;&#31867;&#20219;&#21153;&#65292;&#27169;&#25311;&#22270;&#20687;&#30001;$k$&#20010;&#32500;&#24230;&#20026;$d$&#30340;&#34917;&#19969;&#32452;&#25104;&#65292;&#20197;&#35299;&#20915;CNNs&#30456;&#23545;&#20110;LCNs&#21644;FCNs&#30340;&#32479;&#35745;&#20248;&#21183;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20219;&#21153;&#30340;&#29305;&#28857;&#26159;&#22320;&#22495;&#24615;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#20854;&#26550;&#26500;&#20013;&#22266;&#26377;&#30340;&#22320;&#22495;&#24615;&#21644;&#26435;&#37325;&#20849;&#20139;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#29616;&#26377;&#30340;&#35797;&#22270;&#37327;&#21270;&#36825;&#20123;&#20559;&#24046;&#22312;CNNs&#19978;&#30456;&#23545;&#20110;&#23616;&#37096;&#36830;&#25509;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;LCNs&#65289;&#21644;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FCNs&#65289;&#30340;&#32479;&#35745;&#20248;&#21183;&#30340;&#23581;&#35797;&#21487;&#20197;&#24402;&#20026;&#20197;&#19979;&#20960;&#31867;&#65306;&#35201;&#20040;&#23427;&#20204;&#24573;&#35270;&#20248;&#21270;&#22120;&#65292;&#20165;&#25552;&#20379;&#20855;&#26377;&#32479;&#19968;&#25910;&#25947;&#19978;&#30028;&#20294;&#27809;&#26377;&#20998;&#38548;&#19979;&#30028;&#30340;&#32479;&#35745;&#25910;&#25947;&#24615;&#65292;&#35201;&#20040;&#32771;&#34385;&#21040;&#19981;&#30495;&#23454;&#22320;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#22320;&#22495;&#24615;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#30340;&#31616;&#21333;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#19981;&#36275;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21160;&#24577;&#20449;&#21495;&#20998;&#24067;&#65288;DSD&#65289;&#20998;&#31867;&#20219;&#21153;&#65292;&#23427;&#23558;&#22270;&#20687;&#24314;&#27169;&#20026;&#21253;&#21547;$k$&#20010;&#23610;&#23544;&#20026;$d$&#30340;&#34917;&#19969;&#65292;&#26631;&#31614;&#26159;de
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15707v1 Announce Type: cross  Abstract: Vision tasks are characterized by the properties of locality and translation invariance. The superior performance of convolutional neural networks (CNNs) on these tasks is widely attributed to the inductive bias of locality and weight sharing baked into their architecture. Existing attempts to quantify the statistical benefits of these biases in CNNs over locally connected convolutional neural networks (LCNs) and fully connected neural networks (FCNs) fall into one of the following categories: either they disregard the optimizer and only provide uniform convergence upper bounds with no separating lower bounds, or they consider simplistic tasks that do not truly mirror the locality and translation invariance as found in real-world vision tasks. To address these deficiencies, we introduce the Dynamic Signal Distribution (DSD) classification task that models an image as consisting of $k$ patches, each of dimension $d$, and the label is de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22312;&#32447;&#20381;&#20174;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#36807;&#21435;&#34920;&#29616;&#35843;&#25972;&#27169;&#22411;&#26435;&#37325;&#12290;</title><link>https://arxiv.org/abs/2403.15527</link><description>&lt;p&gt;
&#20381;&#20174;&#22312;&#32447;&#27169;&#22411;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Conformal online model aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15527
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22312;&#32447;&#20381;&#20174;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#36807;&#21435;&#34920;&#29616;&#35843;&#25972;&#27169;&#22411;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#20174;&#39044;&#27979;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27010;&#24565;&#65292;&#32780;&#19981;&#38656;&#35201;&#20570;&#20986;&#24378;&#28872;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;&#23427;&#36866;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#23558;&#28857;&#39044;&#27979;&#36716;&#25442;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#36793;&#38469;&#35206;&#30422;&#20445;&#35777;&#30340;&#38598;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20381;&#20174;&#39044;&#27979;&#21482;&#22312;&#20107;&#20808;&#30830;&#23450;&#24213;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36215;&#20316;&#29992;&#12290;&#20381;&#20174;&#39044;&#27979;&#20013;&#30456;&#23545;&#36739;&#23569;&#28041;&#21450;&#30340;&#38382;&#39064;&#26159;&#27169;&#22411;&#36873;&#25321;&#21644;/&#25110;&#32858;&#21512;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;&#24212;&#35813;&#22914;&#20309;&#20381;&#20174;&#21270;&#20247;&#22810;&#39044;&#27979;&#26041;&#27861;&#65288;&#38543;&#26426;&#26862;&#26519;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#27491;&#21017;&#21270;&#32447;&#24615;&#27169;&#22411;&#31561;&#65289;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20381;&#20174;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#35774;&#32622;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#26469;&#33258;&#22810;&#20010;&#31639;&#27861;&#30340;&#39044;&#27979;&#38598;&#36827;&#34892;&#25237;&#31080;&#65292;&#20854;&#20013;&#26681;&#25454;&#36807;&#21435;&#34920;&#29616;&#35843;&#25972;&#27169;&#22411;&#19978;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15527v1 Announce Type: cross  Abstract: Conformal prediction equips machine learning models with a reasonable notion of uncertainty quantification without making strong distributional assumptions. It wraps around any black-box prediction model and converts point predictions into set predictions that have a predefined marginal coverage guarantee. However, conformal prediction only works if we fix the underlying machine learning model in advance. A relatively unaddressed issue in conformal prediction is that of model selection and/or aggregation: for a given problem, which of the plethora of prediction methods (random forests, neural nets, regularized linear models, etc.) should we conformalize? This paper proposes a new approach towards conformal model aggregation in online settings that is based on combining the prediction sets from several algorithms by voting, where weights on the models are adapted over time based on past performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26680;&#26041;&#27861;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#24179;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#27604;&#20219;&#20309;&#26680;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#20010;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#24182;&#38598;&#65292;&#24182;&#19988;&#31070;&#32463;&#32593;&#32476;&#20250;&#33719;&#24471;&#19982;&#30446;&#26631;&#20989;&#25968;&#23545;&#40784;&#30340;&#25968;&#25454;&#30456;&#20851;&#26680;&#12290;</title><link>https://arxiv.org/abs/2403.14917</link><description>&lt;p&gt;
&#20174;&#26680;&#26041;&#27861;&#30340;&#35282;&#24230;&#23545;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24179;&#22343;&#22330;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Mean-field Analysis on Two-layer Neural Networks from a Kernel Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26680;&#26041;&#27861;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#24179;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#27604;&#20219;&#20309;&#26680;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#20010;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#24182;&#38598;&#65292;&#24182;&#19988;&#31070;&#32463;&#32593;&#32476;&#20250;&#33719;&#24471;&#19982;&#30446;&#26631;&#20989;&#25968;&#23545;&#40784;&#30340;&#25968;&#25454;&#30456;&#20851;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26680;&#26041;&#27861;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#24179;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#20102;&#32858;&#28966;&#20110;&#31532;&#19968;&#23618;&#35825;&#23548;&#30340;&#26680;&#30340;&#21160;&#24577;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#26497;&#38480;&#65292;&#20854;&#20013;&#31532;&#20108;&#23618;&#27604;&#31532;&#19968;&#23618;&#31227;&#21160;&#24471;&#24555;&#24471;&#22810;&#12290;&#22312;&#36825;&#20010;&#26497;&#38480;&#19979;&#65292;&#23398;&#20064;&#38382;&#39064;&#34987;&#31616;&#21270;&#20026;&#22312;&#20869;&#22312;&#26680;&#19978;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#22343;&#22330; Langevin &#21160;&#21147;&#23398;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#25512;&#23548;&#20102;&#26102;&#38388;&#21644;&#31890;&#23376;&#31163;&#25955;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#27604;&#20219;&#20309;&#26680;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#20010;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#24182;&#38598;&#65292;&#24182;&#19988;&#31070;&#32463;&#32593;&#32476;&#20250;&#33719;&#24471;&#19982;&#30446;&#26631;&#20989;&#25968;&#23545;&#40784;&#30340;&#25968;&#25454;&#30456;&#20851;&#26680;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#30340;&#26631;&#31614;&#22122;&#22768;&#36807;&#31243;&#65292;&#24182;&#23637;&#31034;&#33258;&#30001;&#24230;&#20986;&#29616;&#20316;&#20026;&#19968;&#31181;&#38544;&#24335;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14917v1 Announce Type: new  Abstract: In this paper, we study the feature learning ability of two-layer neural networks in the mean-field regime through the lens of kernel methods. To focus on the dynamics of the kernel induced by the first layer, we utilize a two-timescale limit, where the second layer moves much faster than the first layer. In this limit, the learning problem is reduced to the minimization problem over the intrinsic kernel. Then, we show the global convergence of the mean-field Langevin dynamics and derive time and particle discretization error. We also demonstrate that two-layer neural networks can learn a union of multiple reproducing kernel Hilbert spaces more efficiently than any kernel methods, and neural networks acquire data-dependent kernel which aligns with the target function. In addition, we develop a label noise procedure, which converges to the global optimum and show that the degrees of freedom appears as an implicit regularization.
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#29702;&#24615;&#21407;&#21017;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11782</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#20174;&#20559;&#22909;&#21644;&#36873;&#25321;&#20013;&#23398;&#20064;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
A tutorial on learning from preferences and choices with Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11782
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#29702;&#24615;&#21407;&#21017;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24314;&#27169;&#20301;&#20110;&#32463;&#27982;&#23398;&#12289;&#20915;&#31574;&#29702;&#35770;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#30340;&#20132;&#21449;&#28857;&#12290;&#36890;&#36807;&#29702;&#35299;&#20010;&#20307;&#30340;&#20559;&#22909;&#21450;&#20854;&#36873;&#25321;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#26356;&#25509;&#36817;&#20182;&#20204;&#26399;&#26395;&#30340;&#20135;&#21697;&#65292;&#20026;&#36328;&#39046;&#22495;&#30340;&#26356;&#39640;&#25928;&#12289;&#20010;&#24615;&#21270;&#24212;&#29992;&#38138;&#24179;&#36947;&#36335;&#12290;&#27492;&#25945;&#31243;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#36830;&#36143;&#12289;&#20840;&#38754;&#30340;&#20559;&#22909;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#28436;&#31034;&#22914;&#20309;&#23558;&#29702;&#24615;&#21407;&#21017;&#65288;&#26469;&#33258;&#32463;&#27982;&#23398;&#21644;&#20915;&#31574;&#29702;&#35770;&#65289;&#26080;&#32541;&#22320;&#32435;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#21512;&#36866;&#22320;&#23450;&#21046;&#20284;&#28982;&#20989;&#25968;&#65292;&#36825;&#19968;&#26694;&#26550;&#20351;&#24471;&#33021;&#22815;&#26500;&#24314;&#28085;&#30422;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#12289;&#36776;&#35782;&#38480;&#21046;&#21644;&#23545;&#35937;&#21644;&#26631;&#31614;&#20559;&#22909;&#30340;&#22810;&#37325;&#20914;&#31361;&#25928;&#29992;&#24773;&#26223;&#30340;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11782v1 Announce Type: new  Abstract: Preference modelling lies at the intersection of economics, decision theory, machine learning and statistics. By understanding individuals' preferences and how they make choices, we can build products that closely match their expectations, paving the way for more efficient and personalised applications across a wide range of domains. The objective of this tutorial is to present a cohesive and comprehensive framework for preference learning with Gaussian Processes (GPs), demonstrating how to seamlessly incorporate rationality principles (from economics and decision theory) into the learning process. By suitably tailoring the likelihood function, this framework enables the construction of preference learning models that encompass random utility models, limits of discernment, and scenarios with multiple conflicting utilities for both object- and label-preference. This tutorial builds upon established research while simultaneously introducin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22522;&#20110;&#21306;&#22495;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25512;&#23548;&#20986;&#20102;&#38543;&#26426;&#26862;&#26519;&#39044;&#27979;&#30340;&#39640;&#26031;&#36924;&#36817;&#30028;&#38480;&#65292;&#24182;&#24314;&#31435;&#20102;&#36866;&#29992;&#20110;&#21508;&#31181;&#30456;&#20851;&#32479;&#35745;&#38382;&#39064;&#30340;&#27010;&#29575;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.09960</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#21306;&#22495;&#31283;&#23450;&#24615;&#30340;&#22810;&#20803;&#39640;&#26031;&#36924;&#36817;&#25913;&#36827;&#38543;&#26426;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Multivariate Gaussian Approximation for Random Forest via Region-based Stabilization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09960
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22522;&#20110;&#21306;&#22495;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25512;&#23548;&#20986;&#20102;&#38543;&#26426;&#26862;&#26519;&#39044;&#27979;&#30340;&#39640;&#26031;&#36924;&#36817;&#30028;&#38480;&#65292;&#24182;&#24314;&#31435;&#20102;&#36866;&#29992;&#20110;&#21508;&#31181;&#30456;&#20851;&#32479;&#35745;&#38382;&#39064;&#30340;&#27010;&#29575;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#32473;&#23450;&#30001;&#27850;&#26494;&#36807;&#31243;&#20135;&#29983;&#30340;&#19968;&#32452;&#35757;&#32451;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#25512;&#23548;&#20102;&#38543;&#26426;&#26862;&#26519;&#39044;&#27979;&#30340;&#39640;&#26031;&#36924;&#36817;&#30028;&#38480;&#65292;&#20551;&#35774;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#23384;&#22312;&#30456;&#24403;&#28201;&#21644;&#30340;&#27491;&#21017;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#28385;&#36275;&#19968;&#23450;&#30340;&#31216;&#20026;&#22522;&#20110;&#21306;&#22495;&#31283;&#23450;&#24615;&#30340;&#20960;&#20309;&#23646;&#24615;&#12290;&#22312;&#20026;&#38543;&#26426;&#26862;&#26519;&#24320;&#21457;&#32467;&#26524;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#20026;&#22522;&#20110;&#21306;&#22495;&#31283;&#23450;&#30340;&#27850;&#26494;&#36807;&#31243;&#30340;&#19968;&#33324;&#27867;&#20989;&#24314;&#31435;&#20102;&#19968;&#20010;&#27010;&#29575;&#32467;&#26524;&#65292;&#36825;&#21487;&#33021;&#26159;&#29420;&#31435;&#24863;&#20852;&#36259;&#30340;&#12290;&#36825;&#19968;&#26222;&#36941;&#32467;&#26524;&#21033;&#29992;&#20102;Malliavin-Stein&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#33021;&#36866;&#29992;&#20110;&#21508;&#31181;&#30456;&#20851;&#30340;&#32479;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09960v1 Announce Type: cross  Abstract: We derive Gaussian approximation bounds for random forest predictions based on a set of training points given by a Poisson process, under fairly mild regularity assumptions on the data generating process. Our approach is based on the key observation that the random forest predictions satisfy a certain geometric property called region-based stabilization. In the process of developing our results for the random forest, we also establish a probabilistic result, which might be of independent interest, on multivariate Gaussian approximation bounds for general functionals of Poisson process that are region-based stabilizing. This general result makes use of the Malliavin-Stein method, and is potentially applicable to various related statistical problems.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29615;&#22659;-&#22266;&#26377;&#32500;&#24230;&#24046;&#24322;&#30340;&#27010;&#24565;&#65292;&#35770;&#25991;&#35777;&#26126;&#20102;&#32500;&#24230;&#24046;&#24322;&#20351;&#24178;&#20928;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#31354;&#38388;&#33073;&#31163;&#27969;&#24418;&#26041;&#21521;&#30340;&#23545;&#25239;&#25200;&#21160;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.03967</link><description>&lt;p&gt;
&#29615;&#22659;-&#22266;&#26377;&#32500;&#24230;&#24046;&#24322;&#23545;&#23545;&#25239;&#33030;&#24369;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03967
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29615;&#22659;-&#22266;&#26377;&#32500;&#24230;&#24046;&#24322;&#30340;&#27010;&#24565;&#65292;&#35770;&#25991;&#35777;&#26126;&#20102;&#32500;&#24230;&#24046;&#24322;&#20351;&#24178;&#20928;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#31354;&#38388;&#33073;&#31163;&#27969;&#24418;&#26041;&#21521;&#30340;&#23545;&#25239;&#25200;&#21160;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#23384;&#22312;&#19988;&#23545;&#20154;&#31867;&#26469;&#35828;&#20960;&#20046;&#26080;&#27861;&#23519;&#35273;&#36825;&#19968;&#20107;&#23454;&#65292;&#22312;&#29702;&#35770;&#19978;&#20173;&#28982;&#30456;&#24403;&#31070;&#31192;&#12290;&#25991;&#31456;&#24341;&#20837;&#20102;&#20004;&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#27010;&#24565;&#65306;&#33258;&#28982;&#25110;&#22312;&#27969;&#24418;&#19978;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#26159;&#21487;&#20197;&#34987;&#20154;&#31867;/&#31070;&#35861;&#24863;&#30693;&#21040;&#30340;&#65307;&#38750;&#33258;&#28982;&#25110;&#33073;&#31163;&#27969;&#24418;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21017;&#26080;&#27861;&#34987;&#24863;&#30693;&#21040;&#12290;&#25991;&#31456;&#35748;&#20026;&#33073;&#31163;&#27969;&#24418;&#30340;&#25915;&#20987;&#23384;&#22312;&#26159;&#25968;&#25454;&#22266;&#26377;&#32500;&#24230;&#19982;&#29615;&#22659;&#32500;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#24517;&#28982;&#32467;&#26524;&#12290;&#23545;&#20110;2&#23618;ReLU&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#32500;&#24230;&#24046;&#24322;&#19981;&#24433;&#21709;&#20174;&#35266;&#27979;&#25968;&#25454;&#31354;&#38388;&#20013;&#25277;&#21462;&#26679;&#26412;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#23427;&#20173;&#20250;&#20351;&#24178;&#20928;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#31354;&#38388;&#33073;&#31163;&#27969;&#24418;&#26041;&#21521;&#30340;&#23545;&#25239;&#25200;&#21160;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#25552;&#20379;&#20102;&#22312;/&#33073;&#31163;&#27969;&#24418;&#25915;&#20987;&#30340;$\ell_2,\ell_{\infty}$&#25915;&#20987;&#24378;&#24230;&#19982;&#32500;&#24230;&#24046;&#24322;&#20043;&#38388;&#26126;&#30830;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03967v1 Announce Type: new  Abstract: The existence of adversarial attacks on machine learning models imperceptible to a human is still quite a mystery from a theoretical perspective. In this work, we introduce two notions of adversarial attacks: natural or on-manifold attacks, which are perceptible by a human/oracle, and unnatural or off-manifold attacks, which are not. We argue that the existence of the off-manifold attacks is a natural consequence of the dimension gap between the intrinsic and ambient dimensions of the data. For 2-layer ReLU networks, we prove that even though the dimension gap does not affect generalization performance on samples drawn from the observed data space, it makes the clean-trained model more vulnerable to adversarial perturbations in the off-manifold direction of the data space. Our main results provide an explicit relationship between the $\ell_2,\ell_{\infty}$ attack strength of the on/off-manifold attack and the dimension gap.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#31934;&#30830;&#35206;&#30422;&#30340;&#39044;&#27979;&#38598;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#22312;&#25968;&#25454;&#39537;&#21160;&#24773;&#22659;&#20013;&#30001;&#20110;&#36873;&#25321;&#20559;&#24046;&#23548;&#33268;&#30340;&#36793;&#32536;&#26377;&#25928;&#39044;&#27979;&#21306;&#38388;&#35823;&#23548;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03868</link><description>&lt;p&gt;
&#28966;&#28857;&#32622;&#20449;: &#24102;&#26377;&#36873;&#25321;&#26465;&#20214;&#35206;&#30422;&#30340;&#25972;&#20307;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Confidence on the Focal: Conformal Prediction with Selection-Conditional Coverage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03868
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#31934;&#30830;&#35206;&#30422;&#30340;&#39044;&#27979;&#38598;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#22312;&#25968;&#25454;&#39537;&#21160;&#24773;&#22659;&#20013;&#30001;&#20110;&#36873;&#25321;&#20559;&#24046;&#23548;&#33268;&#30340;&#36793;&#32536;&#26377;&#25928;&#39044;&#27979;&#21306;&#38388;&#35823;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#20307;&#39044;&#27979;&#24314;&#31435;&#22312;&#36793;&#32536;&#26377;&#25928;&#30340;&#39044;&#27979;&#21306;&#38388;&#19978;&#65292;&#35813;&#21306;&#38388;&#20197;&#26576;&#31181;&#35268;&#23450;&#30340;&#27010;&#29575;&#35206;&#30422;&#20102;&#38543;&#26426;&#25277;&#21462;&#30340;&#26032;&#27979;&#35797;&#28857;&#30340;&#26410;&#30693;&#32467;&#26524;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24120;&#35265;&#24773;&#20917;&#26159;&#65292;&#22312;&#30475;&#21040;&#27979;&#35797;&#21333;&#20803;&#21518;&#65292;&#20174;&#19994;&#32773;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#20915;&#23450;&#20851;&#27880;&#21738;&#20123;&#27979;&#35797;&#21333;&#20803;&#65292;&#24182;&#24076;&#26395;&#37327;&#21270;&#28966;&#28857;&#21333;&#20803;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#36825;&#20123;&#28966;&#28857;&#21333;&#20803;&#30340;&#36793;&#32536;&#26377;&#25928;&#39044;&#27979;&#21306;&#38388;&#21487;&#33021;&#20250;&#22240;&#36873;&#25321;&#20559;&#24046;&#32780;&#20855;&#26377;&#35823;&#23548;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26500;&#24314;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#31934;&#30830;&#35206;&#30422;&#30340;&#39044;&#27979;&#38598;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#35206;&#30422;&#26159;&#26377;&#26465;&#20214;&#20110;&#25152;&#36873;&#21333;&#20803;&#30340;&#12290;&#20854;&#19968;&#33324;&#24418;&#24335;&#36866;&#29992;&#20110;&#20219;&#24847;&#36873;&#25321;&#35268;&#21017;&#65292;&#24182;&#23558;Mondrian&#25972;&#20307;&#39044;&#27979;&#25512;&#24191;&#21040;&#22810;&#20010;&#27979;&#35797;&#21333;&#20803;&#21644;&#38750;&#31561;&#21464;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20026;&#22810;&#20010;&#29616;&#23454;&#30340;&#36873;&#25321;&#35268;&#21017;&#35745;&#31639;&#20102;&#36866;&#29992;&#20110;&#25105;&#20204;&#26694;&#26550;&#30340;&#35745;&#31639;&#25928;&#29575;&#23454;&#29616;&#65292;&#21253;&#25324;top-K&#36873;&#25321;&#12289;&#20248;&#21270;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03868v1 Announce Type: cross  Abstract: Conformal prediction builds marginally valid prediction intervals which cover the unknown outcome of a randomly drawn new test point with a prescribed probability. In practice, a common scenario is that, after seeing the test unit(s), practitioners decide which test unit(s) to focus on in a data-driven manner, and wish to quantify the uncertainty for the focal unit(s). In such cases, marginally valid prediction intervals for these focal units can be misleading due to selection bias. This paper presents a general framework for constructing a prediction set with finite-sample exact coverage conditional on the unit being selected. Its general form works for arbitrary selection rules, and generalizes Mondrian Conformal Prediction to multiple test units and non-equivariant classifiers. We then work out computationally efficient implementation of our framework for a number of realistic selection rules, including top-K selection, optimization
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#19978;&#28216;&#21644;&#19979;&#28216;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#30340;&#19968;&#27493;&#31574;&#30053;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20559;&#35823;&#65292;&#22312;CEO&#26102;&#38388;&#21033;&#29992;&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#20135;&#29983;&#20102;&#37325;&#35201;&#25928;&#26524;&#65292;&#36866;&#21512;&#24212;&#29992;&#30740;&#31350;&#20154;&#21592;&#12290;</title><link>https://arxiv.org/abs/2402.15585</link><description>&lt;p&gt;
&#20351;&#29992;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#30340;&#21464;&#37327;&#36827;&#34892;&#22238;&#24402;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference for Regression with Variables Generated from Unstructured Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15585
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#19978;&#28216;&#21644;&#19979;&#28216;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#30340;&#19968;&#27493;&#31574;&#30053;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20559;&#35823;&#65292;&#22312;CEO&#26102;&#38388;&#21033;&#29992;&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#20135;&#29983;&#20102;&#37325;&#35201;&#25928;&#26524;&#65292;&#36866;&#21512;&#24212;&#29992;&#30740;&#31350;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20027;&#35201;&#31574;&#30053;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#19978;&#28216;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#20272;&#35745;&#24863;&#20852;&#36259;&#30340;&#28508;&#22312;&#32463;&#27982;&#21464;&#37327;&#12290;&#20854;&#27425;&#65292;&#23558;&#20272;&#35745;&#20540;&#35270;&#20026;&#19979;&#28216;&#35745;&#37327;&#32463;&#27982;&#27169;&#22411;&#20013;&#30340;&#8220;&#25968;&#25454;&#8221;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29702;&#35770;&#35770;&#28857;&#65292;&#35299;&#37322;&#20026;&#20160;&#20040;&#22312;&#23454;&#35777;&#21512;&#29702;&#30340;&#35774;&#32622;&#20013;&#65292;&#36825;&#31181;&#20004;&#27493;&#31574;&#30053;&#20250;&#23548;&#33268;&#20559;&#35823;&#30340;&#25512;&#26029;&#12290;&#26356;&#20855;&#24314;&#35774;&#24615;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#25512;&#26029;&#30340;&#19968;&#27493;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21516;&#26102;&#20351;&#29992;&#19978;&#28216;&#21644;&#19979;&#28216;&#27169;&#22411;&#12290;&#22312;&#27169;&#25311;&#20013;&#65292;&#36825;&#19968;&#27493;&#31574;&#30053;(i) &#26174;&#33879;&#20943;&#23569;&#20102;&#20559;&#35823;&#65307;(ii) &#22312;&#20351;&#29992;CEO&#26102;&#38388;&#21033;&#29992;&#25968;&#25454;&#30340;&#20027;&#35201;&#24212;&#29992;&#20013;&#20135;&#29983;&#20102;&#23450;&#37327;&#37325;&#35201;&#30340;&#25928;&#26524;&#65307;(iii) &#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#34987;&#24212;&#29992;&#30740;&#31350;&#20154;&#21592;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15585v1 Announce Type: new  Abstract: The leading strategy for analyzing unstructured data uses two steps. First, latent variables of economic interest are estimated with an upstream information retrieval model. Second, the estimates are treated as "data" in a downstream econometric model. We establish theoretical arguments for why this two-step strategy leads to biased inference in empirically plausible settings. More constructively, we propose a one-step strategy for valid inference that uses the upstream and downstream models jointly. The one-step strategy (i) substantially reduces bias in simulations; (ii) has quantitatively important effects in a leading application using CEO time-use data; and (iii) can be readily adapted by applied researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#26159;&#24378;&#20984;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.11858</link><description>&lt;p&gt;
&#22312;&#26446;&#32676;&#19978;&#30340;&#38543;&#26426;Hessian&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Stochastic Hessian Fitting on Lie Group
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#26159;&#24378;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#12290;&#20351;&#29992;&#20102;&#19968;&#20010;Hessian&#25311;&#21512;&#20934;&#21017;&#65292;&#21487;&#29992;&#20110;&#25512;&#23548;&#22823;&#37096;&#20998;&#24120;&#29992;&#26041;&#27861;&#65292;&#22914;BFGS&#12289;&#39640;&#26031;&#29275;&#39039;&#12289;AdaGrad&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#19981;&#21516;&#25910;&#25947;&#36895;&#29575;&#65292;&#20363;&#22914;&#65292;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#27425;&#32447;&#24615;&#36895;&#29575;&#21644;&#23545;&#31216;&#27491;&#23450;&#65288;SPL&#65289;&#30697;&#38453;&#21644;&#26576;&#20123;&#26446;&#32676;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#32447;&#24615;&#36895;&#29575;&#12290;&#22312;&#29305;&#23450;&#19988;&#36275;&#22815;&#19968;&#33324;&#30340;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#34987;&#35777;&#26126;&#26159;&#24378;&#20984;&#30340;&#12290;&#20026;&#20102;&#30830;&#35748;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22914;&#26377;&#22122;&#22768;&#30340;Hessian-&#21521;&#37327;&#20056;&#31215;&#12289;&#26102;&#21464;&#30340;Hessians&#21644;&#20302;&#31934;&#24230;&#31639;&#26415;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20381;&#36182;&#20110;&#38543;&#26426;&#20108;&#38454;&#20248;&#21270;&#30340;&#26041;&#27861;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11858v1 Announce Type: cross  Abstract: This paper studies the fitting of Hessian or its inverse with stochastic Hessian-vector products. A Hessian fitting criterion, which can be used to derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad, etc., is used for the analysis. Our studies reveal different convergence rates for different Hessian fitting methods, e.g., sublinear rates for gradient descent in the Euclidean space and a commonly used closed-form solution, linear rates for gradient descent on the manifold of symmetric positive definite (SPL) matrices and certain Lie groups. The Hessian fitting problem is further shown to be strongly convex under mild conditions on a specific yet general enough Lie group. To confirm our analysis, these methods are tested under different settings like noisy Hessian-vector products, time varying Hessians, and low precision arithmetic. These findings are useful for stochastic second order optimizations that rely 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21629;&#21517;&#20026;FKMD&#30340;&#20808;&#36827;KMD&#25216;&#26415;&#65292;&#36890;&#36807;&#26102;&#38388;&#23884;&#20837;&#21644;&#39532;&#27663;&#36317;&#31163;&#32553;&#25918;&#65292;&#21487;&#20197;&#22686;&#24378;&#23545;&#39640;&#32500;&#21160;&#21147;&#31995;&#32479;&#30340;&#20998;&#26512;&#21644;&#39044;&#27979;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#29305;&#24449;&#26410;&#30693;&#30340;&#24773;&#20917;&#65292;&#24182;&#22312;&#19993;&#27688;&#37240;&#20108;&#32957;&#25968;&#25454;&#38477;&#32500;&#21644;&#20998;&#26512;Lorenz&#21560;&#24341;&#23376;&#21644;&#30284;&#30151;&#30740;&#31350;&#20013;&#32454;&#32990;&#20449;&#21495;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2312.09146</link><description>&lt;p&gt;
&#23545;Koopman&#27169;&#24577;&#20998;&#35299;&#36827;&#34892;&#29305;&#24449;&#21270;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Featurizing Koopman Mode Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21629;&#21517;&#20026;FKMD&#30340;&#20808;&#36827;KMD&#25216;&#26415;&#65292;&#36890;&#36807;&#26102;&#38388;&#23884;&#20837;&#21644;&#39532;&#27663;&#36317;&#31163;&#32553;&#25918;&#65292;&#21487;&#20197;&#22686;&#24378;&#23545;&#39640;&#32500;&#21160;&#21147;&#31995;&#32479;&#30340;&#20998;&#26512;&#21644;&#39044;&#27979;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#29305;&#24449;&#26410;&#30693;&#30340;&#24773;&#20917;&#65292;&#24182;&#22312;&#19993;&#27688;&#37240;&#20108;&#32957;&#25968;&#25454;&#38477;&#32500;&#21644;&#20998;&#26512;Lorenz&#21560;&#24341;&#23376;&#21644;&#30284;&#30151;&#30740;&#31350;&#20013;&#32454;&#32990;&#20449;&#21495;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;Koopman&#27169;&#24577;&#20998;&#35299;&#65288;KMD&#65289;&#25216;&#26415;&#65306;&#21629;&#21517;&#20026;&#29305;&#24449;&#21270;Koopman&#27169;&#24577;&#20998;&#35299;&#65288;FKMD&#65289;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#26102;&#38388;&#23884;&#20837;&#21644;&#39532;&#27663;&#36317;&#31163;&#32553;&#25918;&#26469;&#22686;&#24378;&#23545;&#39640;&#32500;&#21160;&#21147;&#31995;&#32479;&#30340;&#20998;&#26512;&#21644;&#39044;&#27979;&#12290;&#26102;&#38388;&#23884;&#20837;&#25193;&#23637;&#20102;&#35266;&#27979;&#31354;&#38388;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#22522;&#30784;&#27969;&#24418;&#32467;&#26500;&#65292;&#32780;&#24212;&#29992;&#20110;&#26680;&#20989;&#25968;&#25110;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#30340;&#39532;&#27663;&#36317;&#31163;&#32553;&#25918;&#65292;&#21017;&#26681;&#25454;&#31995;&#32479;&#30340;&#21160;&#24577;&#35843;&#25972;&#35266;&#27979;&#20540;&#12290;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#20107;&#20808;&#30693;&#36947;&#33391;&#22909;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#23545;KMD&#36827;&#34892;&#29305;&#24449;&#21270;&#22788;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;FKMD&#20013;&#30340;&#39532;&#27663;&#36317;&#31163;&#32553;&#25918;&#21487;&#29992;&#20110;&#23545;&#19993;&#27688;&#37240;&#20108;&#32957;&#25968;&#25454;&#36827;&#34892;&#26377;&#25928;&#30340;&#38477;&#32500;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;FKMD&#22914;&#20309;&#25913;&#21892;&#23545;&#39640;&#32500;Lorenz&#21560;&#24341;&#23376;&#21644;&#30284;&#30151;&#30740;&#31350;&#20013;&#30340;&#32454;&#32990;&#20449;&#21495;&#38382;&#39064;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09146v3 Announce Type: replace-cross  Abstract: This article introduces an advanced Koopman mode decomposition (KMD) technique -- coined Featurized Koopman Mode Decomposition (FKMD) -- that uses time embedding and Mahalanobis scaling to enhance analysis and prediction of high dimensional dynamical systems. The time embedding expands the observation space to better capture underlying manifold structure, while the Mahalanobis scaling, applied to kernel or random Fourier features, adjusts observations based on the system's dynamics. This aids in featurizing KMD in cases where good features are not a priori known. We find that the Mahalanobis scaling from FKMD can be used for effective dimensionality reduction of alanine dipeptide data. We also show that FKMD improves predictions for a high-dimensional Lorenz attractor and a cell signaling problem from cancer research.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#20195;&#26367;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#20687;&#24674;&#22797;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.17744</link><description>&lt;p&gt;
&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Variational Bayes image restoration with compressive autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17744
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#20195;&#26367;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#20687;&#24674;&#22797;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#30340;&#27491;&#21017;&#21270;&#22312;&#35745;&#31639;&#25104;&#20687;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26377;&#25928;&#22270;&#20687;&#34920;&#31034;&#30340;&#33021;&#21147;&#24050;&#34987;&#21033;&#29992;&#26469;&#35774;&#35745;&#24378;&#22823;&#30340;&#25968;&#25454;&#39537;&#21160;&#27491;&#21017;&#21270;&#22120;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#34987;&#30475;&#20316;&#20855;&#26377;&#28789;&#27963;&#28508;&#22312;&#20808;&#39564;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#27604;&#36215;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#26356;&#23567;&#26356;&#23481;&#26131;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17744v2 Announce Type: replace-cross  Abstract: Regularization of inverse problems is of paramount importance in computational imaging. The ability of neural networks to learn efficient image representations has been recently exploited to design powerful data-driven regularizers. While state-of-the-art plug-and-play methods rely on an implicit regularization provided by neural denoisers, alternative Bayesian approaches consider Maximum A Posteriori (MAP) estimation in the latent space of a generative model, thus with an explicit regularization. However, state-of-the-art deep generative models require a huge amount of training data compared to denoisers. Besides, their complexity hampers the optimization involved in latent MAP derivation. In this work, we first propose to use compressive autoencoders instead. These networks, which can be seen as variational autoencoders with a flexible latent prior, are smaller and easier to train than state-of-the-art generative models. As a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#26497;&#38480;&#20013;&#22270;&#19978;&#20449;&#21495;&#30340;&#20449;&#21495;&#37319;&#26679;&#29702;&#35770;&#65292;&#35777;&#26126;&#20102;Poincar\'e&#19981;&#31561;&#24335;&#24182;&#23637;&#31034;&#20102;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.10610</link><description>&lt;p&gt;
&#20449;&#21495;&#22312;&#22823;&#22270;&#19978;&#30340;&#37319;&#26679;&#30340;Poincar\'e&#19981;&#31561;&#24335;&#21644;&#19968;&#33268;&#24615;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
A Poincar\'e Inequality and Consistency Results for Signal Sampling on Large Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10610
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#26497;&#38480;&#20013;&#22270;&#19978;&#20449;&#21495;&#30340;&#20449;&#21495;&#37319;&#26679;&#29702;&#35770;&#65292;&#35777;&#26126;&#20102;Poincar\'e&#19981;&#31561;&#24335;&#24182;&#23637;&#31034;&#20102;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#38543;&#30528;&#22270;&#30340;&#22823;&#23567;&#32780;&#22686;&#21152;&#12290;&#23545;&#22270;&#36827;&#34892;&#23376;&#37319;&#26679;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#22312;&#22270;&#19978;&#36827;&#34892;&#37319;&#26679;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#22240;&#20026;&#22270;&#26159;&#38750;&#27431;&#20960;&#37324;&#24471;&#30340;&#12290;&#29616;&#26377;&#30340;&#22270;&#37319;&#26679;&#25216;&#26415;&#19981;&#20165;&#38656;&#35201;&#35745;&#31639;&#22823;&#30697;&#38453;&#30340;&#35889;&#65292;&#32780;&#19988;&#22312;&#22270;&#21457;&#29983;&#21464;&#21270;&#65288;&#20363;&#22914;&#22686;&#38271;&#65289;&#26102;&#38656;&#35201;&#37325;&#22797;&#36825;&#20123;&#35745;&#31639;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#19968;&#31181;&#22270;&#26497;&#38480;--&#22270;&#19978;&#30340;&#20449;&#21495;&#37319;&#26679;&#29702;&#35770;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#19978;&#20449;&#21495;&#30340;Poincar\'e&#19981;&#31561;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#28385;&#36275;&#36825;&#19968;&#19981;&#31561;&#24335;&#30340;&#33410;&#28857;&#23376;&#38598;&#30340;&#34917;&#38598;&#26159;&#22270;&#19978;&#20449;&#21495;Paley-Wiener&#31354;&#38388;&#30340;&#21807;&#19968;&#37319;&#26679;&#38598;&#12290;&#36890;&#36807;&#19982;&#35889;&#32858;&#31867;&#21644;&#39640;&#26031;&#28040;&#20803;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#37319;&#26679;&#38598;&#26159;&#19968;&#33268;&#30340;&#65292;&#21363;&#25910;&#25947;&#30340;&#22270;&#24207;&#21015;&#19978;&#30340;&#21807;&#19968;&#37319;&#26679;&#38598;&#25910;&#25947;&#21040;&#22270;&#26497;&#38480;&#19978;&#30340;&#21807;&#19968;&#37319;&#26679;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10610v2 Announce Type: replace  Abstract: Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the spectra of large matrices but also repeating these computations when the graph changes, e.g., grows. In this paper, we introduce a signal sampling theory for a type of graph limit -- the graphon. We prove a Poincar\'e inequality for graphon signals and show that complements of node subsets satisfying this inequality are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting connections with spectral clustering and Gaussian elimination, we prove that such sampling sets are consistent in the sense that unique sampling sets on a convergent graph sequence converge to unique sampling sets on the graphon. We then propose a related
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#29615;&#22659;&#20013;&#20801;&#35768;&#23450;&#20301;&#22810;&#20010;&#21464;&#37327;&#30340;&#24178;&#39044;&#65292;&#24182;&#22312;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#20013;&#39318;&#27425;&#24471;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.02695</link><description>&lt;p&gt;
&#20174;&#22810;&#33410;&#28857;&#24178;&#39044;&#20013;&#35782;&#21035;&#32447;&#24615;&#28151;&#21512;&#22240;&#26524;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#29615;&#22659;&#20013;&#20801;&#35768;&#23450;&#20301;&#22810;&#20010;&#21464;&#37327;&#30340;&#24178;&#39044;&#65292;&#24182;&#22312;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#20013;&#39318;&#27425;&#24471;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#26029;&#20174;&#20302;&#32423;&#35266;&#23519;&#20013;&#24471;&#20986;&#39640;&#32423;&#22240;&#26524;&#21464;&#37327;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#31216;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#26412;&#36136;&#19978;&#26159;&#27424;&#32422;&#26463;&#30340;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#36817;&#24037;&#20316;&#38598;&#20013;&#22312;&#23548;&#33268;&#28508;&#22312;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#21487;&#35782;&#21035;&#24615;&#30340;&#21508;&#31181;&#20551;&#35774;&#19978;&#12290;&#22823;&#37327;&#20043;&#21069;&#30340;&#26041;&#27861;&#32771;&#34385;&#22312;&#22240;&#26524;&#27169;&#22411;&#19978;&#19981;&#21516;&#24178;&#39044;&#19979;&#25910;&#38598;&#30340;&#22810;&#29615;&#22659;&#25968;&#25454;&#12290;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#24037;&#20316;&#20849;&#21516;&#28857;&#26159;&#23545;&#27599;&#20010;&#29615;&#22659;&#20013;&#21482;&#24178;&#39044;&#19968;&#20010;&#21464;&#37327;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#24182;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20801;&#35768;&#22312;&#19968;&#20010;&#29615;&#22659;&#20013;&#36890;&#36807;&#24178;&#39044;&#23450;&#20301;&#22810;&#20010;&#21464;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#20915;&#20110;&#20851;&#20110;&#21508;&#20010;&#29615;&#22659;&#20013;&#24178;&#39044;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#22810;&#26679;&#24615;&#30340;&#19968;&#33324;&#20551;&#35774;&#65292;&#20854;&#20013;&#20063;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02695v2 Announce Type: replace-cross  Abstract: The task of inferring high-level causal variables from low-level observations, commonly referred to as causal representation learning, is fundamentally underconstrained. As such, recent works to address this problem focus on various assumptions that lead to identifiability of the underlying latent causal variables. A large corpus of these preceding approaches consider multi-environment data collected under different interventions on the causal model. What is common to virtually all of these works is the restrictive assumption that in each environment, only a single variable is intervened on. In this work, we relax this assumption and provide the first identifiability result for causal representation learning that allows for multiple variables to be targeted by an intervention within one environment. Our approach hinges on a general assumption on the coverage and diversity of interventions across environments, which also include
&lt;/p&gt;</description></item><item><title>Transformer&#32593;&#32476;&#20316;&#20026;&#20855;&#26377;&#26080;&#38480;&#32500;&#36755;&#20837;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20989;&#25968;&#65292;&#36890;&#36807;&#20854;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;&#21442;&#25968;&#20849;&#20139;&#23646;&#24615;&#65292;&#33021;&#22815;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#65292;&#23454;&#29616;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#36924;&#36817;&#21644;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2305.18699</link><description>&lt;p&gt;
Transformer&#23545;&#20855;&#26377;&#26080;&#38480;&#32500;&#36755;&#20837;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20989;&#25968;&#30340;&#36924;&#36817;&#21644;&#20272;&#35745;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Approximation and Estimation Ability of Transformers for Sequence-to-Sequence Functions with Infinite Dimensional Input
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.18699
&lt;/p&gt;
&lt;p&gt;
Transformer&#32593;&#32476;&#20316;&#20026;&#20855;&#26377;&#26080;&#38480;&#32500;&#36755;&#20837;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20989;&#25968;&#65292;&#36890;&#36807;&#20854;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;&#21442;&#25968;&#20849;&#20139;&#23646;&#24615;&#65292;&#33021;&#22815;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#65292;&#23454;&#29616;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#36924;&#36817;&#21644;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#30340;&#29702;&#35770;&#26041;&#38754;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#29702;&#35299;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#20316;&#20026;&#20855;&#26377;&#26080;&#38480;&#32500;&#36755;&#20837;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20989;&#25968;&#30340;&#36924;&#36817;&#21644;&#20272;&#35745;&#33021;&#21147;&#12290;&#23613;&#31649;&#36755;&#20837;&#21644;&#36755;&#20986;&#37117;&#26159;&#26080;&#38480;&#32500;&#30340;&#65292;&#25105;&#20204;&#34920;&#26126;&#24403;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#21508;&#21521;&#24322;&#24615;&#24179;&#28369;&#24615;&#26102;&#65292;Transformer&#21487;&#20197;&#36890;&#36807;&#20854;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;&#21442;&#25968;&#20849;&#20139;&#23646;&#24615;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#24179;&#28369;&#24615;&#22240;&#36755;&#20837;&#32780;&#24322;&#65292;Transformer&#20173;&#28982;&#21487;&#20197;&#20272;&#35745;&#27599;&#20010;&#36755;&#20837;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#24182;&#21160;&#24577;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Transformer&#23454;&#29616;&#20102;&#19982;&#22266;&#23450;&#24179;&#28369;&#24615;&#24773;&#20917;&#19979;&#30456;&#20284;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#20102;Transformer&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.18699v1 Announce Type: cross  Abstract: Despite the great success of Transformer networks in various applications such as natural language processing and computer vision, their theoretical aspects are not well understood. In this paper, we study the approximation and estimation ability of Transformers as sequence-to-sequence functions with infinite dimensional inputs. Although inputs and outputs are both infinite dimensional, we show that when the target function has anisotropic smoothness, Transformers can avoid the curse of dimensionality due to their feature extraction ability and parameter sharing property. In addition, we show that even if the smoothness changes depending on each input, Transformers can estimate the importance of features for each input and extract important features dynamically. Then, we proved that Transformers achieve similar convergence rate as in the case of the fixed smoothness. Our theoretical results support the practical success of Transformers
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#20108;&#36827;&#21046;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#31995;&#32479;&#30340;&#20271;&#23572;&#26364;&#20998;&#24067;&#31934;&#30830;&#26144;&#23556;&#20026;&#33258;&#22238;&#24402;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#30340;ARNN&#26550;&#26500;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#29289;&#29702;&#21547;&#20041;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#32479;&#35745;&#29289;&#29702;&#25216;&#26415;&#25512;&#23548;&#20986;&#29305;&#23450;&#31995;&#32479;&#30340;&#26032;ARNN&#12290;</title><link>https://arxiv.org/abs/2302.08347</link><description>&lt;p&gt;
&#33258;&#20271;&#23572;&#26364;&#20998;&#24067;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#33258;&#26059;&#31995;&#32479;&#30340;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
The autoregressive neural network architecture of the Boltzmann distribution of pairwise interacting spins systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.08347
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#20108;&#36827;&#21046;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#31995;&#32479;&#30340;&#20271;&#23572;&#26364;&#20998;&#24067;&#31934;&#30830;&#26144;&#23556;&#20026;&#33258;&#22238;&#24402;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#30340;ARNN&#26550;&#26500;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#29289;&#29702;&#21547;&#20041;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#32479;&#35745;&#29289;&#29702;&#25216;&#26415;&#25512;&#23548;&#20986;&#29305;&#23450;&#31995;&#32479;&#30340;&#26032;ARNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2302.08347v3 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#36328; Abstract: &#29983;&#25104;&#24335;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;ARNNs&#65289;&#26368;&#36817;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26480;&#20986;&#30340;&#32467;&#26524;&#65292;&#20419;&#25104;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#31185;&#23398;&#21644;&#21830;&#19994;&#24212;&#29992;&#20013;&#26085;&#30410;&#27969;&#34892;&#12290;&#26412;&#24037;&#20316;&#23558;&#20108;&#36827;&#21046;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#31995;&#32479;&#30340;&#20271;&#23572;&#26364;&#20998;&#24067;&#31934;&#30830;&#26144;&#23556;&#20026;&#33258;&#22238;&#24402;&#24418;&#24335;&#12290;&#32467;&#26524;&#30340;ARNN&#26550;&#26500;&#20855;&#26377;&#19982;&#21704;&#23494;&#39039;&#32806;&#21512;&#21644;&#22806;&#22330;&#23545;&#24212;&#30340;&#31532;&#19968;&#23618;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#65292;&#20855;&#26377;&#35832;&#22914;&#27531;&#24046;&#36830;&#25509;&#21644;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#29289;&#29702;&#21547;&#20041;&#30340;&#36882;&#24402;&#26550;&#26500;&#31561;&#24191;&#27867;&#20351;&#29992;&#30340;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#20854;&#26550;&#26500;&#30340;&#26126;&#30830;&#34920;&#36848;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#32479;&#35745;&#29289;&#29702;&#25216;&#26415;&#25512;&#23548;&#29305;&#23450;&#31995;&#32479;&#30340;&#26032;ARNN&#12290;&#20316;&#20026;&#31034;&#20363;&#65292;&#20174;&#20004;&#20010;&#30693;&#21517;&#30340;&#24179;&#22343;&#22330;&#31995;&#32479;&#65292;&#23621;&#37324;-&#39759;&#26031;&#21644;Sherrington-Kirkpatrick&#27169;&#22411;&#65292;&#23548;&#20986;&#20102;&#26032;&#30340;&#26377;&#25928;ARNN&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.08347v3 Announce Type: replace-cross  Abstract: Generative Autoregressive Neural Networks (ARNNs) have recently demonstrated exceptional results in image and language generation tasks, contributing to the growing popularity of generative models in both scientific and commercial applications. This work presents an exact mapping of the Boltzmann distribution of binary pairwise interacting systems into autoregressive form. The resulting ARNN architecture has weights and biases of its first layer corresponding to the Hamiltonian's couplings and external fields, featuring widely used structures such as the residual connections and a recurrent architecture with clear physical meanings. Moreover, its architecture's explicit formulation enables the use of statistical physics techniques to derive new ARNNs for specific systems. As examples, new effective ARNN architectures are derived from two well-known mean-field systems, the Curie-Weiss and Sherrington-Kirkpatrick models, showing 
&lt;/p&gt;</description></item><item><title>&#20998;&#24067;&#40065;&#26834;&#24615;&#30028;&#23450;&#20102;&#27867;&#21270;&#38169;&#35823;&#65292;Bayesian&#26041;&#27861;&#22312;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#24847;&#20041;&#19978;&#26159;&#20998;&#24067;&#40065;&#26834;&#30340;&#65292;&#21516;&#26102;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#20063;&#34987;&#35777;&#26126;&#26159;&#31561;&#20215;&#20110;Bayesian&#26041;&#27861;&#30340;&#12290;</title><link>https://arxiv.org/abs/2212.09962</link><description>&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#24615;&#30028;&#23450;&#20102;&#27867;&#21270;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Distributional Robustness Bounds Generalization Errors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.09962
&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#24615;&#30028;&#23450;&#20102;&#27867;&#21270;&#38169;&#35823;&#65292;Bayesian&#26041;&#27861;&#22312;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#24847;&#20041;&#19978;&#26159;&#20998;&#24067;&#40065;&#26834;&#30340;&#65292;&#21516;&#26102;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#20063;&#34987;&#35777;&#26126;&#26159;&#31561;&#20215;&#20110;Bayesian&#26041;&#27861;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bayesian methods, distributionally robust optimization methods, and regularization methods&#26159;&#20540;&#24471;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30707;&#65292;&#29992;&#20110;&#25269;&#25239;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;&#65292;&#27604;&#22914;&#32463;&#39564;&#20998;&#24067;&#19982;&#30495;&#23454;&#22522;&#30784;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#19977;&#31181;&#26694;&#26550;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#29305;&#21035;&#22320;&#25506;&#35752;&#20102;&#20026;&#20309;&#36825;&#20123;&#26694;&#26550;&#20542;&#21521;&#20110;&#20855;&#26377;&#26356;&#23567;&#30340;&#27867;&#21270;&#38169;&#35823;&#12290;&#20855;&#20307;&#22320;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20998;&#24067;&#40065;&#26834;&#24615;&#8221;&#30340;&#23450;&#37327;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#8220;&#40065;&#26834;&#24615;&#24230;&#37327;&#8221;&#30340;&#27010;&#24565;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#20998;&#24067;&#40065;&#26834;&#24615;&#20248;&#21270;&#20013;&#30340;&#20960;&#20010;&#21746;&#23398;&#27010;&#24565;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#34920;&#26126;Bayesian&#26041;&#27861;&#22312;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#24847;&#20041;&#19978;&#26159;&#20998;&#24067;&#40065;&#26834;&#30340;&#65307;&#27492;&#22806;&#65292;&#36890;&#36807;&#26500;&#36896;&#31867;&#20284;Dirichlet&#36807;&#31243;&#30340;&#20808;&#39564;&#20110;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#35777;&#26126;&#20219;&#20309;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#31561;&#20215;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.09962v3 Announce Type: replace  Abstract: Bayesian methods, distributionally robust optimization methods, and regularization methods are three pillars of trustworthy machine learning combating distributional uncertainty, e.g., the uncertainty of an empirical distribution compared to the true underlying distribution. This paper investigates the connections among the three frameworks and, in particular, explores why these frameworks tend to have smaller generalization errors. Specifically, first, we suggest a quantitative definition for "distributional robustness", propose the concept of "robustness measure", and formalize several philosophical concepts in distributionally robust optimization. Second, we show that Bayesian methods are distributionally robust in the probably approximately correct (PAC) sense; in addition, by constructing a Dirichlet-process-like prior in Bayesian nonparametrics, it can be proven that any regularized empirical risk minimization method is equival
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; Covariate-Assisted Ranking Estimation (CARE) &#27169;&#22411;&#65292;&#25193;&#23637;&#20102; Bradley-Terry-Luce (BTL) &#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21327;&#21464;&#37327;&#20449;&#24687;&#32467;&#21512;&#36827;&#25490;&#21517;&#20272;&#35745;&#20013;&#65292;&#35299;&#20915;&#20102;&#23454;&#20307;&#25490;&#21517;&#38382;&#39064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2212.09961</link><description>&lt;p&gt;
&#23454;&#20307;&#25490;&#21517;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#19982;&#22806;&#29983;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification of MLE for Entity Ranking with Covariates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.09961
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; Covariate-Assisted Ranking Estimation (CARE) &#27169;&#22411;&#65292;&#25193;&#23637;&#20102; Bradley-Terry-Luce (BTL) &#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21327;&#21464;&#37327;&#20449;&#24687;&#32467;&#21512;&#36827;&#25490;&#21517;&#20272;&#35745;&#20013;&#65292;&#35299;&#20915;&#20102;&#23454;&#20307;&#25490;&#21517;&#38382;&#39064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#22522;&#20110;&#25104;&#23545;&#27604;&#36739;&#21644;&#39069;&#22806;&#21327;&#21464;&#37327;&#20449;&#24687;&#65288;&#22914;&#25152;&#27604;&#36739;&#39033;&#30446;&#30340;&#23646;&#24615;&#65289;&#30340;&#25490;&#21517;&#38382;&#39064;&#30340;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#12290;&#23613;&#31649;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#20808;&#21069;&#30340;&#25991;&#29486;&#20013;&#24456;&#23569;&#26377;&#20154;&#22312;&#21327;&#21464;&#37327;&#20449;&#24687;&#23384;&#22312;&#30340;&#26356;&#29616;&#23454;&#24773;&#22659;&#19979;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21363; Covariate-Assisted Ranking Estimation (CARE) &#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#21327;&#21464;&#37327;&#20449;&#24687;&#25193;&#23637;&#20102;&#33879;&#21517;&#30340; Bradley-Terry-Luce (BTL) &#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#20010;&#27604;&#36739;&#39033;&#30446;&#30340;&#28508;&#22312;&#20998;&#25968;&#19981;&#26159;&#22266;&#23450;&#30340; $\{\theta_i^*\}_{i=1}^n$&#65292;&#32780;&#26159;&#30001; $\{\alpha_i^*+{x}_i^\top\beta^*\}_{i=1}^n$ &#32473;&#20986;&#65292;&#20854;&#20013; $\alpha_i^*$ &#21644; ${x}_i^\top\beta^*$ &#20998;&#21035;&#20195;&#34920;&#31532; $i$ &#20010;&#39033;&#30446;&#30340;&#28508;&#22312;&#22522;&#20934;&#20998;&#25968;&#21644;&#21327;&#21464;&#37327;&#20998;&#25968;&#12290;&#25105;&#20204;&#21152;&#20837;&#20102;&#33258;&#28982;&#30340;&#21487;&#35782;&#21035;&#24615;&#26465;&#20214;&#65292;&#24182;&#25512;&#23548;&#20102; $\ell_{\infty}$-
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.09961v2 Announce Type: replace-cross  Abstract: This paper concerns with statistical estimation and inference for the ranking problems based on pairwise comparisons with additional covariate information such as the attributes of the compared items. Despite extensive studies, few prior literatures investigate this problem under the more realistic setting where covariate information exists. To tackle this issue, we propose a novel model, Covariate-Assisted Ranking Estimation (CARE) model, that extends the well-known Bradley-Terry-Luce (BTL) model, by incorporating the covariate information. Specifically, instead of assuming every compared item has a fixed latent score $\{\theta_i^*\}_{i=1}^n$, we assume the underlying scores are given by $\{\alpha_i^*+{x}_i^\top\beta^*\}_{i=1}^n$, where $\alpha_i^*$ and ${x}_i^\top\beta^*$ represent latent baseline and covariate score of the $i$-th item, respectively. We impose natural identifiability conditions and derive the $\ell_{\infty}$-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;$\epsilon_t$-&#36138;&#24515;&#21551;&#21457;&#24335;&#26041;&#27861;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#24773;&#22659;&#20013;&#65292;&#37319;&#29992;&#20445;&#23432;&#23548;&#21521;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#23454;&#29992;&#24212;&#29992;&#20013;&#23545;&#26032;&#22855;&#24615;&#30340;&#37325;&#35270;&#65292;&#21516;&#26102;&#38480;&#21046;&#20102;&#37319;&#32435;&#19981;&#23547;&#24120;&#21160;&#20316;&#65292;&#26377;&#25928;&#25511;&#21046;&#20102;&#32047;&#31215;&#36951;&#25022;&#12290;</title><link>https://arxiv.org/abs/2009.13961</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#30340;&#22312;&#32447;&#21160;&#20316;&#23398;&#20064;&#65306;&#19968;&#20010;&#20445;&#23432;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Online Action Learning in High Dimensions: A Conservative Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2009.13961
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;$\epsilon_t$-&#36138;&#24515;&#21551;&#21457;&#24335;&#26041;&#27861;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#24773;&#22659;&#20013;&#65292;&#37319;&#29992;&#20445;&#23432;&#23548;&#21521;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#23454;&#29992;&#24212;&#29992;&#20013;&#23545;&#26032;&#22855;&#24615;&#30340;&#37325;&#35270;&#65292;&#21516;&#26102;&#38480;&#21046;&#20102;&#37319;&#32435;&#19981;&#23547;&#24120;&#21160;&#20316;&#65292;&#26377;&#25928;&#25511;&#21046;&#20102;&#32047;&#31215;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#23398;&#20064;&#38382;&#39064;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#26368;&#27969;&#34892;&#30340;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#65292;$\epsilon_t$-&#36138;&#24515;&#21551;&#21457;&#24335;&#65292;&#25193;&#23637;&#21040;&#32771;&#34385;&#20445;&#23432;&#23548;&#21521;&#30340;&#39640;&#32500;&#24773;&#22659;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#21407;&#22987;&#35268;&#21017;&#29992;&#20110;&#37319;&#32435;&#20840;&#26032;&#21160;&#20316;&#30340;&#26102;&#38388;&#30340;&#19968;&#37096;&#20998;&#65292;&#20998;&#37197;&#32473;&#22312;&#19968;&#32452;&#26377;&#21069;&#36884;&#30340;&#21160;&#20316;&#20013;&#36827;&#34892;&#26356;&#21152;&#19987;&#27880;&#30340;&#25628;&#32034;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25152;&#24471;&#35268;&#21017;&#21487;&#33021;&#23545;&#20173;&#28982;&#37325;&#35270;&#24778;&#21916;&#20294;&#38480;&#21046;&#37319;&#32435;&#19981;&#23547;&#24120;&#21160;&#20316;&#30340;&#23454;&#38469;&#24212;&#29992;&#26377;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#23545;&#20110;&#20445;&#23432;&#39640;&#32500;&#24230;&#34928;&#20943;$\epsilon_t$-&#36138;&#24515;&#35268;&#21017;&#30340;&#32047;&#31215;&#36951;&#25022;&#25552;&#20379;&#20102;&#21512;&#29702;&#36793;&#30028;&#30340;&#27010;&#29575;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2009.13961v4 Announce Type: replace-cross  Abstract: Sequential learning problems are common in several fields of research and practical applications. Examples include dynamic pricing and assortment, design of auctions and incentives and permeate a large number of sequential treatment experiments. In this paper, we extend one of the most popular learning solutions, the $\epsilon_t$-greedy heuristics, to high-dimensional contexts considering a conservative directive. We do this by allocating part of the time the original rule uses to adopt completely new actions to a more focused search in a restrictive set of promising actions. The resulting rule might be useful for practical applications that still values surprises, although at a decreasing rate, while also has restrictions on the adoption of unusual actions. With high probability, we find reasonable bounds for the cumulative regret of a conservative high-dimensional decaying $\epsilon_t$-greedy rule. Also, we provide a lower bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#38899;&#19978;&#19979;&#25991;&#30340;&#38543;&#26426;&#36172;&#33218;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;Thompson&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26694;&#26550;&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#36125;&#21494;&#26031;&#21518;&#24724;&#65292;&#24182;&#25193;&#23637;&#20102;&#38382;&#39064;&#21040;&#24310;&#36831;&#35266;&#23519;&#30495;&#23454;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#65292;&#24182;&#23454;&#35777;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11565</link><description>&lt;p&gt;
Thompson&#37319;&#26679;&#29992;&#20110;&#20855;&#26377;&#22122;&#38899;&#19978;&#19979;&#25991;&#30340;&#38543;&#26426;&#36172;&#33218;&#38382;&#39064;&#30340;&#20449;&#24687;&#35770;&#24615;&#21518;&#24724;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Thompson Sampling for Stochastic Bandits with Noisy Contexts: An Information-Theoretic Regret Analysis. (arXiv:2401.11565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#38899;&#19978;&#19979;&#25991;&#30340;&#38543;&#26426;&#36172;&#33218;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;Thompson&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26694;&#26550;&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#36125;&#21494;&#26031;&#21518;&#24724;&#65292;&#24182;&#25193;&#23637;&#20102;&#38382;&#39064;&#21040;&#24310;&#36831;&#35266;&#23519;&#30495;&#23454;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#65292;&#24182;&#23454;&#35777;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#38543;&#26426;&#19978;&#19979;&#25991;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#19968;&#20010;&#26410;&#30693;&#22122;&#22768;&#21442;&#25968;&#30340;&#22122;&#22768;&#20449;&#36947;&#35266;&#23519;&#21040;&#30495;&#23454;&#19978;&#19979;&#25991;&#30340;&#22122;&#22768;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#21160;&#20316;&#31574;&#30053;&#65292;&#21487;&#20197;&#36817;&#20284;&#20110;&#20855;&#26377;&#22870;&#21169;&#27169;&#22411;&#12289;&#22122;&#22768;&#21442;&#25968;&#21644;&#20174;&#35266;&#23519;&#21040;&#30340;&#22122;&#22768;&#19978;&#19979;&#25991;&#20013;&#30495;&#23454;&#19978;&#19979;&#25991;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;oracle&#30340;&#21160;&#20316;&#31574;&#30053;&#12290;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#39640;&#26031;&#19978;&#19979;&#25991;&#22122;&#22768;&#30340;&#39640;&#26031;&#36172;&#33218;&#30340;Thompson&#37319;&#26679;&#31639;&#27861;&#12290;&#37319;&#29992;&#20449;&#24687;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30456;&#23545;&#20110;oracle&#30340;&#21160;&#20316;&#31574;&#30053;&#30340;&#36125;&#21494;&#26031;&#21518;&#24724;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#20010;&#38382;&#39064;&#25193;&#23637;&#21040;&#20102;&#20195;&#29702;&#22312;&#25509;&#25910;&#21040;&#22870;&#21169;&#21518;&#24310;&#36831;&#35266;&#23519;&#21040;&#30495;&#23454;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#24310;&#36831;&#30495;&#23454;&#19978;&#19979;&#25991;&#23548;&#33268;&#26356;&#20302;&#30340;&#36125;&#21494;&#26031;&#21518;&#24724;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#22522;&#32447;&#31639;&#27861;&#30340;&#27604;&#36739;&#23454;&#35777;&#22320;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore a stochastic contextual linear bandit problem where the agent observes a noisy, corrupted version of the true context through a noise channel with an unknown noise parameter. Our objective is to design an action policy that can approximate" that of an oracle, which has access to the reward model, the channel parameter, and the predictive distribution of the true context from the observed noisy context. In a Bayesian framework, we introduce a Thompson sampling algorithm for Gaussian bandits with Gaussian context noise. Adopting an information-theoretic analysis, we demonstrate the Bayesian regret of our algorithm concerning the oracle's action policy. We also extend this problem to a scenario where the agent observes the true context with some delay after receiving the reward and show that delayed true contexts lead to lower Bayesian regret. Finally, we empirically demonstrate the performance of the proposed algorithms against baselines.
&lt;/p&gt;</description></item><item><title>PhyloGFN&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#22797;&#26434;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#33021;&#22815;&#20135;&#29983;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#36827;&#21270;&#20551;&#35774;&#65292;&#24182;&#22312;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08774</link><description>&lt;p&gt;
PhyloGFN: &#22522;&#20110;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
PhyloGFN: Phylogenetic inference with generative flow networks. (arXiv:2310.08774v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08774
&lt;/p&gt;
&lt;p&gt;
PhyloGFN&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#22797;&#26434;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#33021;&#22815;&#20135;&#29983;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#36827;&#21270;&#20551;&#35774;&#65292;&#24182;&#22312;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#21457;&#32946;&#23398;&#26159;&#35745;&#31639;&#29983;&#29289;&#23398;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#30740;&#31350;&#29983;&#29289;&#23454;&#20307;&#20043;&#38388;&#30340;&#36827;&#21270;&#20851;&#31995;&#12290;&#23613;&#31649;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#21644;&#20247;&#22810;&#24212;&#29992;&#65292;&#20294;&#20174;&#24207;&#21015;&#25968;&#25454;&#25512;&#26029;&#31995;&#32479;&#21457;&#32946;&#26641;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#26641;&#31354;&#38388;&#30340;&#39640;&#22797;&#26434;&#24615;&#23545;&#24403;&#21069;&#30340;&#32452;&#21512;&#21644;&#27010;&#29575;&#25216;&#26415;&#26500;&#25104;&#20102;&#37325;&#35201;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#31995;&#32479;&#21457;&#32946;&#23398;&#20013;&#30340;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#22522;&#20110;&#26368;&#31616;&#21407;&#21017;&#30340;&#21644;&#36125;&#21494;&#26031;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#12290;&#30001;&#20110;GFlowNets&#36866;&#29992;&#20110;&#37319;&#26679;&#22797;&#26434;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#23427;&#20204;&#26159;&#25506;&#32034;&#21644;&#37319;&#26679;&#26641;&#25299;&#25169;&#21644;&#36827;&#21270;&#36317;&#31163;&#30340;&#22810;&#27169;&#24577;&#21518;&#39564;&#20998;&#24067;&#30340;&#33258;&#28982;&#36873;&#25321;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25674;&#36824;&#21518;&#39564;&#37319;&#26679;&#22120;PhyloGFN&#22312;&#30495;&#23454;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#36827;&#21270;&#20551;&#35774;&#12290;PhyloGFN&#22312;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#26041;&#38754;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phylogenetics is a branch of computational biology that studies the evolutionary relationships among biological entities. Its long history and numerous applications notwithstanding, inference of phylogenetic trees from sequence data remains challenging: the high complexity of tree space poses a significant obstacle for the current combinatorial and probabilistic techniques. In this paper, we adopt the framework of generative flow networks (GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and Bayesian phylogenetic inference. Because GFlowNets are well-suited for sampling complex combinatorial structures, they are a natural choice for exploring and sampling from the multimodal posterior distribution over tree topologies and evolutionary distances. We demonstrate that our amortized posterior sampler, PhyloGFN, produces diverse and high-quality evolutionary hypotheses on real benchmark datasets. PhyloGFN is competitive with prior works in marginal likelihood estimat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#25628;&#32034;&#35757;&#32451;GFlowNets&#65292;&#36890;&#36807;&#30772;&#22351;&#21644;&#37325;&#26500;&#30340;&#26041;&#24335;&#25506;&#32034;&#23616;&#37096;&#37051;&#22495;&#65292;&#20998;&#21035;&#30001;&#21453;&#21521;&#21644;&#27491;&#21521;&#31574;&#30053;&#24341;&#23548;&#65292;&#20351;&#24471;&#26679;&#26412;&#20559;&#21521;&#39640;&#22870;&#21169;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.02710</link><description>&lt;p&gt;
&#26412;&#22320;&#25628;&#32034;GFlowNets
&lt;/p&gt;
&lt;p&gt;
Local Search GFlowNets. (arXiv:2310.02710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#25628;&#32034;&#35757;&#32451;GFlowNets&#65292;&#36890;&#36807;&#30772;&#22351;&#21644;&#37325;&#26500;&#30340;&#26041;&#24335;&#25506;&#32034;&#23616;&#37096;&#37051;&#22495;&#65292;&#20998;&#21035;&#30001;&#21453;&#21521;&#21644;&#27491;&#21521;&#31574;&#30053;&#24341;&#23548;&#65292;&#20351;&#24471;&#26679;&#26412;&#20559;&#21521;&#39640;&#22870;&#21169;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;(GFlowNets)&#26159;&#19968;&#31181;&#23398;&#20064;&#19982;&#22870;&#21169;&#25104;&#27604;&#20363;&#30340;&#31163;&#25955;&#23545;&#35937;&#20998;&#24067;&#30340;&#25674;&#36824;&#37319;&#26679;&#26041;&#27861;&#12290;GFlowNets&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#26679;&#26412;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#24191;&#27867;&#26679;&#26412;&#31354;&#38388;&#19978;&#30340;&#36807;&#24230;&#25506;&#32034;&#65292;&#26377;&#26102;&#38590;&#20197;&#19968;&#33268;&#22320;&#29983;&#25104;&#39640;&#22870;&#21169;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#25628;&#32034;&#35757;&#32451;GFlowNets&#65292;&#36890;&#36807;&#30772;&#22351;&#21644;&#37325;&#26500;&#30340;&#26041;&#24335;&#25506;&#32034;&#23616;&#37096;&#37051;&#22495;&#65292;&#20998;&#21035;&#30001;&#21453;&#21521;&#21644;&#27491;&#21521;&#31574;&#30053;&#24341;&#23548;&#12290;&#36825;&#20351;&#24471;&#26679;&#26412;&#20559;&#21521;&#39640;&#22870;&#21169;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#20256;&#32479;&#30340;GFlowNet&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#26041;&#26696;&#21017;&#20351;&#29992;&#27491;&#21521;&#31574;&#30053;&#20174;&#22836;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#22312;&#20960;&#20010;&#29983;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) are amortized sampling methods that learn a distribution over discrete objects proportional to their rewards. GFlowNets exhibit a remarkable ability to generate diverse samples, yet occasionally struggle to consistently produce samples with high rewards due to over-exploration on wide sample space. This paper proposes to train GFlowNets with local search which focuses on exploiting high rewarded sample space to resolve this issue. Our main idea is to explore the local neighborhood via destruction and reconstruction guided by backward and forward policies, respectively. This allows biasing the samples toward high-reward solutions, which is not possible for a typical GFlowNet solution generation scheme which uses the forward policy to generate the solution from scratch. Extensive experiments demonstrate a remarkable performance improvement in several biochemical tasks. Source code is available: \url{https://github.com/dbsxodud-11/ls_gfn}.
&lt;/p&gt;</description></item><item><title>TACTiS-2&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20851;&#27880;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#31616;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#32447;&#24615;&#21442;&#25968;&#25968;&#37327;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01327</link><description>&lt;p&gt;
TACTiS-2&#65306;&#26356;&#22909;&#12289;&#26356;&#24555;&#12289;&#26356;&#31616;&#21333;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20851;&#27880;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series. (arXiv:2310.01327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01327
&lt;/p&gt;
&lt;p&gt;
TACTiS-2&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20851;&#27880;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#31616;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#32447;&#24615;&#21442;&#25968;&#25968;&#37327;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#29992;&#20110;&#22810;&#21464;&#37327;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#26088;&#22312;&#28789;&#27963;&#22320;&#22788;&#29702;&#21253;&#25324;&#39044;&#27979;&#12289;&#25554;&#20540;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#31561;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#22522;&#20110;&#32852;&#21512;&#20998;&#24067;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#26368;&#36817;&#24341;&#20837;&#30340;&#22522;&#20110;Transformer&#30340;&#20851;&#27880;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;&#65288;TACTiS&#65289;&#12290;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#20998;&#24067;&#21442;&#25968;&#25968;&#37327;&#19982;&#21464;&#37327;&#25968;&#37327;&#21576;&#32447;&#24615;&#32780;&#38750;&#38454;&#20056;&#20851;&#31995;&#12290;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#38656;&#35201;&#24341;&#20837;&#19968;&#31181;&#35757;&#32451;&#35838;&#31243;&#65292;&#24182;&#19988;&#38656;&#35201;&#23545;&#21407;&#22987;&#26550;&#26500;&#36827;&#34892;&#24517;&#35201;&#30340;&#25913;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24471;&#21040;&#30340;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#25913;&#21892;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#24182;&#22312;&#22810;&#26679;&#30340;&#30495;&#23454;&#19990;&#30028;&#39044;&#27979;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20808;&#21069;&#24037;&#20316;&#30340;&#28789;&#27963;&#24615;&#65292;&#22914;&#26080;&#32541;&#22788;&#29702;&#19981;&#23545;&#40784;&#21644;&#37319;&#26679;&#19981;&#22343;&#21248;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new model for multivariate probabilistic time series prediction, designed to flexibly address a range of tasks including forecasting, interpolation, and their combinations. Building on copula theory, we propose a simplified objective for the recently-introduced transformer-based attentional copulas (TACTiS), wherein the number of distributional parameters now scales linearly with the number of variables instead of factorially. The new objective requires the introduction of a training curriculum, which goes hand-in-hand with necessary changes to the original architecture. We show that the resulting model has significantly better training dynamics and achieves state-of-the-art performance across diverse real-world forecasting tasks, while maintaining the flexibility of prior work, such as seamless handling of unaligned and unevenly-sampled time series.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#19968;&#31995;&#21015;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#65292;&#36890;&#36807;&#22312;&#19981;&#26029;&#22024;&#26434;&#21270;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#23450;&#20041;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;EBMs&#65292;&#24182;&#19982;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#21327;&#21516;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20851;&#38381;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#20043;&#38388;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.05153</link><description>&lt;p&gt;
&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#23398;&#20064;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood. (arXiv:2309.05153v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#19968;&#31995;&#21015;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#65292;&#36890;&#36807;&#22312;&#19981;&#26029;&#22024;&#26434;&#21270;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#23450;&#20041;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;EBMs&#65292;&#24182;&#19982;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#21327;&#21516;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20851;&#38381;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#20043;&#38388;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#25968;&#25454;&#19978;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#35757;&#32451;&#33021;&#37327;&#22522;&#20934;&#27169;&#22411;&#65288;EBMs&#65289;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#36739;&#38271;&#12290;&#22240;&#27492;&#65292;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#65288;&#22914;GANs&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#21463;&#26368;&#36817;&#36890;&#36807;&#26368;&#22823;&#21270;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;DRL&#65289;&#26469;&#23398;&#20064;EBMs&#30340;&#21162;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#21487;&#34892;&#22320;&#23398;&#20064;&#21644;&#20174;&#19968;&#31995;&#21015;EBMs&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#20123;EBMs&#23450;&#20041;&#22312;&#36234;&#26469;&#36234;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#65292;&#24182;&#19982;&#27599;&#20010;EBM&#30340;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#12290;&#22312;&#27599;&#20010;&#22122;&#22768;&#27700;&#24179;&#19978;&#65292;&#21021;&#22987;&#21270;&#27169;&#22411;&#23398;&#20064;&#22312;EBM&#30340;&#37319;&#26679;&#36807;&#31243;&#20013;&#20998;&#25674;&#65292;&#32780;&#20004;&#20010;&#27169;&#22411;&#22312;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#20869;&#20849;&#21516;&#20272;&#35745;&#12290;&#21021;&#22987;&#21270;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#20316;&#20026;&#36215;&#22987;&#28857;&#65292;&#32463;&#36807;EBM&#30340;&#20960;&#20010;&#37319;&#26679;&#27493;&#39588;&#36827;&#34892;&#25913;&#36827;&#12290;&#36890;&#36807;&#25913;&#36827;&#21518;&#30340;&#26679;&#26412;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24674;&#22797;&#20284;&#28982;&#26469;&#20248;&#21270;EBM&#12290;
&lt;/p&gt;
&lt;p&gt;
Training energy-based models (EBMs) with maximum likelihood estimation on high-dimensional data can be both challenging and time-consuming. As a result, there a noticeable gap in sample quality between EBMs and other generative frameworks like GANs and diffusion models. To close this gap, inspired by the recent efforts of learning EBMs by maximimizing diffusion recovery likelihood (DRL), we propose cooperative diffusion recovery likelihood (CDRL), an effective approach to tractably learn and sample from a series of EBMs defined on increasingly noisy versons of a dataset, paired with an initializer model for each EBM. At each noise level, the initializer model learns to amortize the sampling process of the EBM, and the two models are jointly estimated within a cooperative training framework. Samples from the initializer serve as starting points that are refined by a few sampling steps from the EBM. With the refined samples, the EBM is optimized by maximizing recovery likelihood, while t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#25512;&#26029;&#24605;&#24819;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#22312;&#25511;&#21046;&#34394;&#20551;&#21457;&#29616;&#29575;&#30340;&#21069;&#25552;&#19979;&#65292;&#35782;&#21035;&#19968;&#32452;&#30495;&#23454;&#30340;&#36793;&#12290;</title><link>http://arxiv.org/abs/2306.14693</link><description>&lt;p&gt;
&#25511;&#21046;&#35823;&#24046;&#29575;&#30340;&#19968;&#33268;&#24615;&#38142;&#36335;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformal link prediction to control the error rate. (arXiv:2306.14693v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#25512;&#26029;&#24605;&#24819;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#22312;&#25511;&#21046;&#34394;&#20551;&#21457;&#29616;&#29575;&#30340;&#21069;&#25552;&#19979;&#65292;&#35782;&#21035;&#19968;&#32452;&#30495;&#23454;&#30340;&#36793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#38142;&#36335;&#39044;&#27979;&#26041;&#27861;&#36820;&#22238;&#22270;&#20013;&#32570;&#22833;&#36793;&#30340;&#36830;&#25509;&#27010;&#29575;&#30340;&#20272;&#35745;&#20540;&#12290;&#36825;&#31181;&#36755;&#20986;&#21487;&#29992;&#20110;&#25353;&#21487;&#33021;&#24615;&#22823;&#23567;&#23545;&#32570;&#22833;&#36793;&#36827;&#34892;&#25490;&#24207;&#65292;&#20294;&#24182;&#26410;&#30452;&#25509;&#25552;&#20379;&#30495;&#23454;&#21644;&#19981;&#23384;&#22312;&#30340;&#20998;&#31867;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#22312;&#25511;&#21046;&#34394;&#20551;&#21457;&#29616;&#29575;&#30340;&#21069;&#25552;&#19979;&#65292;&#35782;&#21035;&#19968;&#32452;&#30495;&#23454;&#30340;&#36793;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#25512;&#26029;&#25991;&#29486;&#20013;&#39640;&#32423;&#24605;&#24819;&#30340;&#26032;&#26041;&#27861;&#12290;&#22270;&#24418;&#32467;&#26500;&#24341;&#20837;&#20102;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#20180;&#32454;&#32771;&#34385;&#20102;&#36825;&#19968;&#28857;&#65292;&#22240;&#20026;&#36825;&#20351;&#24471;&#35774;&#32622;&#19981;&#21516;&#20110;&#19968;&#33324;&#30340;&#19968;&#33268;&#24615;&#25512;&#26029;&#35774;&#32622;&#65292;&#37027;&#37324;&#20551;&#23450;&#20102;&#20132;&#25442;&#24615;&#12290;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FDR&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most link prediction methods return estimates of the connection probability of missing edges in a graph. Such output can be used to rank the missing edges, from most to least likely to be a true edge, but it does not directly provide a classification into true and non-existent. In this work, we consider the problem of identifying a set of true edges with a control of the false discovery rate (FDR). We propose a novel method based on high-level ideas from the literature on conformal inference. The graph structure induces intricate dependence in the data, which we carefully take into account, as this makes the setup different from the usual setup in conformal inference, where exchangeability is assumed. The FDR control is empirically demonstrated for both simulated and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Samplet&#22352;&#26631;&#19979;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24341;&#20837;l1&#27491;&#21017;&#21270;&#39033;&#21487;&#20197;&#22686;&#21152;&#31995;&#25968;&#30340;&#31232;&#30095;&#24615;&#12290;&#30456;&#27604;&#20110;&#21333;&#23610;&#24230;&#22522;&#65292;Samplet&#22522;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#26356;&#22810;&#31867;&#22411;&#30340;&#20449;&#21495;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;&#38408;&#20540;&#21644;&#21322;&#20809;&#28369;&#29275;&#39039;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10180</link><description>&lt;p&gt;
&#22522;&#20110;Samplet&#22522; Pursuit &#30340;&#26680;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Samplet basis pursuit. (arXiv:2306.10180v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Samplet&#22352;&#26631;&#19979;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24341;&#20837;l1&#27491;&#21017;&#21270;&#39033;&#21487;&#20197;&#22686;&#21152;&#31995;&#25968;&#30340;&#31232;&#30095;&#24615;&#12290;&#30456;&#27604;&#20110;&#21333;&#23610;&#24230;&#22522;&#65292;Samplet&#22522;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#26356;&#22810;&#31867;&#22411;&#30340;&#20449;&#21495;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;&#38408;&#20540;&#21644;&#21322;&#20809;&#28369;&#29275;&#39039;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22522;&#20110;l1&#27491;&#21017;&#21270;&#30340;Samplet&#22352;&#26631;&#19979;&#30340;&#26680;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;Samplet&#22522;&#30340;&#31995;&#25968;&#19978;&#65292;&#24212;&#29992;l1&#27491;&#21017;&#21270;&#39033;&#21487;&#20197;&#24378;&#21046;&#22686;&#21152;&#31232;&#30095;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;Samplet&#22522; Pursuit&#12290;Samplet&#22522;&#26159;&#27874;&#24418;&#31867;&#22411;&#30340;&#26377;&#31526;&#21495;&#27979;&#24230;&#65292;&#19987;&#38376;&#29992;&#20110;&#25955;&#20081;&#25968;&#25454;&#12290;&#23427;&#20204;&#20855;&#26377;&#19982;&#23567;&#27874;&#30456;&#20284;&#30340;&#26412;&#22320;&#21270;&#12289;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#21644;&#25968;&#25454;&#21387;&#32553;&#24615;&#36136;&#12290;&#21487;&#20197;&#22312;Samplet&#22522;&#19978;&#31232;&#30095;&#22320;&#34920;&#31034;&#30340;&#20449;&#21495;&#31867;&#27604;&#21333;&#23610;&#24230;&#22522;&#19978;&#33021;&#22815;&#34920;&#31034;&#31232;&#30095;&#30340;&#20449;&#21495;&#31867;&#21035;&#35201;&#22823;&#24471;&#22810;&#12290;&#29305;&#21035;&#22320;&#65292;&#20165;&#29992;&#22522;&#20989;&#25968;&#26144;&#23556;&#30340;&#20960;&#20010;&#29305;&#24449;&#21472;&#21152;&#21363;&#21487;&#34920;&#31034;&#30340;&#25152;&#26377;&#20449;&#21495;&#20063;&#21487;&#20197;&#22312;Samplet&#22352;&#26631;&#19979;&#23454;&#29616;&#31232;&#30095;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23558;&#36719;&#38408;&#20540;&#21644;&#21322;&#20809;&#28369;&#29275;&#39039;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#19982;&#24555;&#36895;&#36845;&#20195;&#25910;&#32553;&#38408;&#20540;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#31232;&#30095;&#24615;&#21644;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider kernel-based learning in samplet coordinates with l1-regularization. The application of an l1-regularization term enforces sparsity of the coefficients with respect to the samplet basis. Therefore, we call this approach samplet basis pursuit. Samplets are wavelet-type signed measures, which are tailored to scattered data. They provide similar properties as wavelets in terms of localization, multiresolution analysis, and data compression. The class of signals that can sparsely be represented in a samplet basis is considerably larger than the class of signals which exhibit a sparse representation in the single-scale basis. In particular, every signal that can be represented by the superposition of only a few features of the canonical feature map is also sparse in samplet coordinates. We propose the efficient solution of the problem under consideration by combining soft-shrinkage with the semi-smooth Newton method and compare the approach to the fast iterative shrinkage thresh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;Z&#20026;&#36830;&#32493;&#20540;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.06721</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Conditional Independence Testing. (arXiv:2306.06721v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;Z&#20026;&#36830;&#32493;&#20540;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#26816;&#39564;&#22312;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20363;&#22914;&#65292;&#23427;&#20204;&#26159;&#35768;&#22810;&#22240;&#26524;&#22270;&#21457;&#29616;&#31639;&#27861;&#30340;&#26500;&#24314;&#22359;&#12290;CI&#27979;&#35797;&#26088;&#22312;&#25509;&#21463;&#25110;&#25298;&#32477;$X \perp \!\!\! \perp Y \mid Z$&#30340;&#38646;&#20551;&#35774;&#65292;&#20854;&#20013;$X \in \mathbb{R}&#65292;Y \in \mathbb{R}&#65292;Z \in \mathbb{R}^d$&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;Shah&#21644;Peters&#65288;2020&#65289;&#30340;&#19968;&#33324;&#21270;&#21327;&#26041;&#24046;&#27979;&#37327;&#21644;&#22522;&#20110;Cand\`es&#31561;&#20154;&#30340;&#26465;&#20214;&#38543;&#26426;&#21270;&#26816;&#39564;&#30340;&#20004;&#31181;&#31169;&#20154;CI&#27979;&#35797;&#36807;&#31243;&#65288;&#22312;&#27169;&#22411;-X&#20551;&#35774;&#19979;&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25105;&#20204;&#27979;&#35797;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#23427;&#20204;&#12290;&#36825;&#20123;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;Z&#20026;&#36830;&#32493;&#30340;&#19968;&#33324;&#24773;&#20917;&#30340;&#31169;&#20154;CI&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional independence (CI) tests are widely used in statistical data analysis, e.g., they are the building block of many algorithms for causal graph discovery. The goal of a CI test is to accept or reject the null hypothesis that $X \perp \!\!\! \perp Y \mid Z$, where $X \in \mathbb{R}, Y \in \mathbb{R}, Z \in \mathbb{R}^d$. In this work, we investigate conditional independence testing under the constraint of differential privacy. We design two private CI testing procedures: one based on the generalized covariance measure of Shah and Peters (2020) and another based on the conditional randomization test of Cand\`es et al. (2016) (under the model-X assumption). We provide theoretical guarantees on the performance of our tests and validate them empirically. These are the first private CI tests that work for the general case when $Z$ is continuous.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;BootGen&#31639;&#27861;&#65292;&#20351;&#29992;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#22686;&#24378;&#29983;&#29289;&#24207;&#21015;&#29983;&#25104;&#22120;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#20248;&#21270;&#29983;&#29289;&#24207;&#21015;&#65292;&#21462;&#24471;&#20102;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03111</link><description>&lt;p&gt;
&#38024;&#23545;&#31163;&#32447;&#35774;&#35745;&#29983;&#29289;&#24207;&#21015;&#30340;&#24471;&#20998;&#26465;&#20214;&#29983;&#25104;&#22120;&#30340;&#33258;&#21161;&#22686;&#24378;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Bootstrapped Training of Score-Conditioned Generator for Offline Design of Biological Sequences. (arXiv:2306.03111v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;BootGen&#31639;&#27861;&#65292;&#20351;&#29992;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#22686;&#24378;&#29983;&#29289;&#24207;&#21015;&#29983;&#25104;&#22120;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#20248;&#21270;&#29983;&#29289;&#24207;&#21015;&#65292;&#21462;&#24471;&#20102;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20248;&#21270;&#29983;&#29289;&#24207;&#21015;&#65288;&#22914;&#34507;&#30333;&#36136;&#12289;DNA&#21644;RNA&#65289;&#20197;&#26368;&#22823;&#21270;&#20165;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35780;&#20272;&#30340;&#40657;&#21283;&#23376;&#24471;&#20998;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#24471;&#20998;&#26465;&#20214;&#29983;&#25104;&#22120;&#30340;&#33258;&#21161;&#22686;&#24378;&#35757;&#32451;&#65288;BootGen&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#37325;&#22797;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#36807;&#31243;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#25490;&#21517;&#21152;&#26435;&#27861;&#35757;&#32451;&#29983;&#29289;&#24207;&#21015;&#29983;&#25104;&#22120;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#39640;&#20998;&#25968;&#30340;&#24207;&#21015;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#12290;&#25509;&#19979;&#26469;&#30340;&#38454;&#27573;&#28041;&#21450;&#21040;&#33258;&#21161;&#22686;&#24378;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#24182;&#26631;&#35760;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#65292;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#19982;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#23545;&#40784;&#65292;&#23558;&#20195;&#29702;&#24471;&#20998;&#20989;&#25968;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#29983;&#25104;&#22120;&#12290;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#32858;&#21512;&#26469;&#33258;&#22810;&#20010;&#33258;&#21161;&#22686;&#24378;&#29983;&#25104;&#22120;&#21644;&#20195;&#29702;&#30340;&#26679;&#26412;&#65292;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#35774;&#35745;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#29289;&#24207;&#21015;&#20248;&#21270;&#26041;&#38754;&#32988;&#36807;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of optimizing biological sequences, e.g., proteins, DNA, and RNA, to maximize a black-box score function that is only evaluated in an offline dataset. We propose a novel solution, bootstrapped training of score-conditioned generator (BootGen) algorithm. Our algorithm repeats a two-stage process. In the first stage, our algorithm trains the biological sequence generator with rank-based weights to enhance the accuracy of sequence generation based on high scores. The subsequent stage involves bootstrapping, which augments the training dataset with self-generated data labeled by a proxy score function. Our key idea is to align the score-based generation with a proxy score function, which distills the knowledge of the proxy score function to the generator. After training, we aggregate samples from multiple bootstrapped generators and proxies to produce a diverse design. Extensive experiments show that our method outperforms competitive baselines on biological sequential
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20174;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#22359;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#22270;&#32858;&#31867;&#21644;&#23884;&#20837;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00979</link><description>&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#22359;&#27169;&#22411;&#20013;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Spectral clustering in the Gaussian mixture block model. (arXiv:2305.00979v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20174;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#22359;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#22270;&#32858;&#31867;&#21644;&#23884;&#20837;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#22359;&#27169;&#22411;&#26159;&#29992;&#20110;&#27169;&#25311;&#29616;&#20195;&#32593;&#32476;&#30340;&#22270;&#20998;&#24067;&#65306;&#23545;&#20110;&#36825;&#26679;&#30340;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#22270;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#39030;&#28857; $i$ &#19982;&#19968;&#20010;&#20174;&#39640;&#26031;&#28151;&#21512;&#20013;&#25277;&#26679;&#21040;&#30340;&#28508;&#22312;&#29305;&#24449;&#21521;&#37327; $u_i \in \mathbb{R}^d$ &#30456;&#20851;&#32852;&#65292;&#24403;&#19988;&#20165;&#24403;&#29305;&#24449;&#21521;&#37327;&#36275;&#22815;&#30456;&#20284;&#65292;&#21363; $\langle u_i,u_j \rangle \ge \tau$ &#26102;&#65292;&#25105;&#20204;&#25165;&#20250;&#28155;&#21152;&#36793; $(i,j)$&#12290;&#39640;&#26031;&#28151;&#21512;&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#34920;&#31034;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#65292;&#20363;&#22914;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#37117;&#34920;&#31034;&#29420;&#29305;&#31038;&#21306;&#30340;&#19981;&#21516;&#23646;&#24615;&#12290;&#36825;&#20123;&#32593;&#32476;&#28041;&#21450;&#21040;&#30340;&#33258;&#28982;&#31639;&#27861;&#20219;&#21153;&#26377;&#23884;&#20837;&#65288;&#24674;&#22797;&#28508;&#22312;&#30340;&#29305;&#24449;&#21521;&#37327;&#65289;&#21644;&#32858;&#31867;&#65288;&#36890;&#36807;&#20854;&#28151;&#21512;&#32452;&#20998;&#23558;&#33410;&#28857;&#20998;&#32452;&#65289;&#12290;&#26412;&#25991;&#24320;&#21551;&#20102;&#23545;&#20174;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#22359;&#27169;&#22411;&#25277;&#26679;&#30340;&#22270;&#36827;&#34892;&#32858;&#31867;&#21644;&#23884;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian mixture block models are distributions over graphs that strive to model modern networks: to generate a graph from such a model, we associate each vertex $i$ with a latent feature vector $u_i \in \mathbb{R}^d$ sampled from a mixture of Gaussians, and we add edge $(i,j)$ if and only if the feature vectors are sufficiently similar, in that $\langle u_i,u_j \rangle \ge \tau$ for a pre-specified threshold $\tau$. The different components of the Gaussian mixture represent the fact that there may be different types of nodes with different distributions over features -- for example, in a social network each component represents the different attributes of a distinct community. Natural algorithmic tasks associated with these networks are embedding (recovering the latent feature vectors) and clustering (grouping nodes by their mixture component).  In this paper we initiate the study of clustering and embedding graphs sampled from high-dimensional Gaussian mixture block models, where the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#25972;&#20307;&#25968;&#25454;&#38598;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20256;&#36882;SJS&#12289;&#20462;&#27491;&#31867;&#21518;&#39564;&#27010;&#29575;&#12289;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#12289;SJS&#19982;&#21327;&#21464;&#37327;&#36716;&#31227;&#20851;&#31995;&#31561;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16971</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#20998;&#31867;&#20013;&#30340;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Sparse joint shift in multinomial classification. (arXiv:2303.16971v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#25972;&#20307;&#25968;&#25454;&#38598;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20256;&#36882;SJS&#12289;&#20462;&#27491;&#31867;&#21518;&#39564;&#27010;&#29575;&#12289;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#12289;SJS&#19982;&#21327;&#21464;&#37327;&#36716;&#31227;&#20851;&#31995;&#31561;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#65288;SJS&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#25972;&#20307;&#20559;&#31227;&#30340;&#21487;&#22788;&#29702;&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#36793;&#38469;&#20998;&#24067;&#20197;&#21450;&#21518;&#39564;&#27010;&#29575;&#21644;&#31867;&#26465;&#20214;&#29305;&#24449;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#22312;&#27809;&#26377;&#26631;&#31614;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#30446;&#26631;&#25968;&#25454;&#38598;&#25311;&#21512;SJS&#21487;&#33021;&#20250;&#20135;&#29983;&#26631;&#31614;&#30340;&#26377;&#25928;&#39044;&#27979;&#21644;&#31867;&#20808;&#39564;&#27010;&#29575;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#29305;&#24449;&#38598;&#20043;&#38388;&#20256;&#36882;SJS&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30446;&#26631;&#20998;&#24067;&#30340;&#31867;&#21518;&#39564;&#27010;&#29575;&#30340;&#26465;&#20214;&#20462;&#27491;&#20844;&#24335;&#65292;&#30830;&#23450;&#24615;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#20197;&#21450;SJS&#21644;&#21327;&#21464;&#37327;&#36716;&#31227;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#29992;&#20110;&#20272;&#35745;SJS&#29305;&#24449;&#30340;&#31639;&#27861;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#20250;&#22952;&#30861;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse joint shift (SJS) was recently proposed as a tractable model for general dataset shift which may cause changes to the marginal distributions of features and labels as well as the posterior probabilities and the class-conditional feature distributions. Fitting SJS for a target dataset without label observations may produce valid predictions of labels and estimates of class prior probabilities. We present new results on the transmission of SJS from sets of features to larger sets of features, a conditional correction formula for the class posterior probabilities under the target distribution, identifiability of SJS, and the relationship between SJS and covariate shift. In addition, we point out inconsistencies in the algorithms which were proposed for estimating the characteristics of SJS, as they could hamper the search for optimal solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#65288;VMC&#65289;&#26041;&#27861;&#25910;&#25947;&#24615;&#30340;&#21487;&#39564;&#35777;&#26041;&#27861;&#65292;&#22312;&#20551;&#35774;&#23616;&#37096;&#33021;&#37327;&#26159;&#27425;&#25351;&#25968;&#30340;&#26465;&#20214;&#19979;&#65292;&#20351;&#29992;&#38750;&#24179;&#31283;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;Bernstein&#19981;&#31561;&#24335;&#25512;&#23548;&#20986;&#20102;MCMC&#20272;&#35745;&#37327;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;VMC&#20855;&#26377;&#19968;&#38454;&#25910;&#25947;&#36895;&#29575;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25910;&#25947;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.10599</link><description>&lt;p&gt;
&#21487;&#39564;&#35777;&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provable Convergence of Variational Monte Carlo Methods. (arXiv:2303.10599v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#65288;VMC&#65289;&#26041;&#27861;&#25910;&#25947;&#24615;&#30340;&#21487;&#39564;&#35777;&#26041;&#27861;&#65292;&#22312;&#20551;&#35774;&#23616;&#37096;&#33021;&#37327;&#26159;&#27425;&#25351;&#25968;&#30340;&#26465;&#20214;&#19979;&#65292;&#20351;&#29992;&#38750;&#24179;&#31283;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;Bernstein&#19981;&#31561;&#24335;&#25512;&#23548;&#20986;&#20102;MCMC&#20272;&#35745;&#37327;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;VMC&#20855;&#26377;&#19968;&#38454;&#25910;&#25947;&#36895;&#29575;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25910;&#25947;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#65288;VMC&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#37327;&#23376;&#22810;&#20307;&#38382;&#39064;&#22522;&#24577;&#33021;&#37327;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#24182;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;VMC&#26041;&#27861;&#20197;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#35797;&#25506;&#27874;&#20989;&#25968;&#65292;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#37319;&#26679;&#37327;&#23376;&#24577;&#65292;&#24182;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#24403;SGD&#19982;MCMC&#37319;&#26679;&#19982;&#35774;&#35745;&#33391;&#22909;&#30340;&#35797;&#25506;&#27874;&#20989;&#25968;&#20132;&#20114;&#20316;&#29992;&#26102;&#65292;VMC&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#20173;&#28982;&#26410;&#30693;&#12290;&#30001;&#20110;MCMC&#38477;&#20302;&#20102;&#26799;&#24230;&#20272;&#35745;&#30340;&#38590;&#24230;&#65292;&#23454;&#38469;&#19978;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#23616;&#37096;&#33021;&#37327;&#21487;&#33021;&#26159;&#26080;&#30028;&#30340;&#65292;&#36825;&#20351;&#24471;&#20998;&#26512;MCMC&#37319;&#26679;&#30340;&#35823;&#24046;&#26356;&#21152;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#23616;&#37096;&#33021;&#37327;&#26159;&#27425;&#25351;&#25968;&#30340;&#65292;&#24182;&#20351;&#29992;&#38750;&#24179;&#31283;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;Bernstein&#19981;&#31561;&#24335;&#25512;&#23548;&#20986;MCMC&#20272;&#35745;&#37327;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;VMC&#34987;&#35777;&#26126;&#20855;&#26377;&#19968;&#38454;&#25910;&#25947;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25910;&#25947;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Variational Monte Carlo (VMC) is a promising approach for computing the ground state energy of many-body quantum problems and attracts more and more interests due to the development of machine learning. The recent paradigms in VMC construct neural networks as trial wave functions, sample quantum configurations using Markov chain Monte Carlo (MCMC) and train neural networks with stochastic gradient descent (SGD) method. However, the theoretical convergence of VMC is still unknown when SGD interacts with MCMC sampling given a well-designed trial wave function. Since MCMC reduces the difficulty of estimating gradients, it has inevitable bias in practice. Moreover, the local energy may be unbounded, which makes it harder to analyze the error of MCMC sampling. Therefore, we assume that the local energy is sub-exponential and use the Bernstein inequality for non-stationary Markov chains to derive error bounds of the MCMC estimator. Consequently, VMC is proven to have a first order conver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31934;&#24230;&#36136;&#37327;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#33258;&#21160;&#22320;&#23398;&#20064;&#22522;&#20110;&#22238;&#24402;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#32780;&#38750;&#21482;&#25552;&#20379;&#20256;&#32479;&#30340;&#30446;&#26631;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#24179;&#22343;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#20197;&#21450;&#25552;&#39640;&#35206;&#30422;&#27010;&#29575;&#26469;&#25552;&#39640;PI&#30340;&#36136;&#37327;&#21644;&#31934;&#24230;&#65292;&#19988;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.06370</link><description>&lt;p&gt;
&#21452;&#37325;&#31934;&#24230;&#36136;&#37327;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#29983;&#25104;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation. (arXiv:2212.06370v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31934;&#24230;&#36136;&#37327;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#33258;&#21160;&#22320;&#23398;&#20064;&#22522;&#20110;&#22238;&#24402;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#32780;&#38750;&#21482;&#25552;&#20379;&#20256;&#32479;&#30340;&#30446;&#26631;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#24179;&#22343;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#20197;&#21450;&#25552;&#39640;&#35206;&#30422;&#27010;&#29575;&#26469;&#25552;&#39640;PI&#30340;&#36136;&#37327;&#21644;&#31934;&#24230;&#65292;&#19988;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#65292;&#24212;&#35813;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30830;&#23450;&#24615;&#39044;&#27979;&#20043;&#22806;&#25552;&#20379;&#39044;&#27979;&#21306;&#38388;(PIs)&#12290;&#21482;&#35201;Pis&#36275;&#22815;&#31364;&#32780;&#19988;&#25429;&#33719;&#20102;&#22823;&#37096;&#20998;&#30340;&#27010;&#29575;&#23494;&#24230;&#65292;&#36825;&#20123;Pis&#23601;&#26159;&#26377;&#29992;&#30340;&#25110;"&#39640;&#36136;&#37327;"&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#22320;&#20026;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#39044;&#27979;&#21306;&#38388;&#65292;&#38500;&#20102;&#20256;&#32479;&#30340;&#30446;&#26631;&#39044;&#27979;&#20043;&#22806;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#20004;&#20010;&#20276;&#20387;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#20010;&#20351;&#29992;&#19968;&#20010;&#36755;&#20986;&#65292;&#30446;&#26631;&#20272;&#35745;&#65292;&#21478;&#19968;&#20010;&#20351;&#29992;&#20004;&#20010;&#36755;&#20986;&#65292;&#30456;&#24212;PI&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#30340;&#20540;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#29983;&#25104;PI&#30340;&#32593;&#32476;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#32771;&#34385;&#20102;&#30446;&#26631;&#20272;&#35745;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#20855;&#26377;&#20004;&#20010;&#20248;&#21270;&#30446;&#26631;&#65306;&#20943;&#23567;&#24179;&#22343;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#21644;&#25552;&#39640;Pis&#30340;&#36136;&#37327;(&#36890;&#36807;&#20854;&#35206;&#30422;&#27010;&#29575;&#36827;&#34892;&#27979;&#37327;)&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#20934;&#30830;&#19988;&#36136;&#37327;&#26356;&#39640;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#21516;&#26102;&#21448;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty quantification is necessary to enhance the reliability of deep learning models in real-world applications. In the case of regression tasks, prediction intervals (PIs) should be provided along with the deterministic predictions of deep learning models. Such PIs are useful or "high-quality" as long as they are sufficiently narrow and capture most of the probability density. In this paper, we present a method to learn prediction intervals for regression-based neural networks automatically in addition to the conventional target predictions. In particular, we train two companion neural networks: one that uses one output, the target estimate, and another that uses two outputs, the upper and lower bounds of the corresponding PI. Our main contribution is the design of a novel loss function for the PI-generation network that takes into account the output of the target-estimation network and has two optimization objectives: minimizing the mean prediction interval width and e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#38750;&#38646;&#26799;&#24230;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;&#38598;&#20013;&#24615;&#30028;&#38480;&#65292;&#36825;&#23545;&#20110;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2208.07243</link><description>&lt;p&gt;
&#38750;&#38646;&#26799;&#24230;&#30340;&#38543;&#26426;&#36924;&#36817;&#30340;&#25351;&#25968;&#38598;&#20013;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exponential Concentration of Stochastic Approximation with Non-vanishing Gradient. (arXiv:2208.07243v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#38750;&#38646;&#26799;&#24230;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;&#38598;&#20013;&#24615;&#30028;&#38480;&#65292;&#36825;&#23545;&#20110;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#34892;&#20026;&#65292;&#20854;&#20013;&#27599;&#19968;&#27493;&#36845;&#20195;&#65292;&#26399;&#26395;&#20013;&#21521;&#30446;&#26631;&#21462;&#24471;&#36827;&#23637;&#12290;&#24403;&#36827;&#23637;&#19982;&#31639;&#27861;&#30340;&#27493;&#38271;&#25104;&#27604;&#20363;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;&#38598;&#20013;&#24615;&#30028;&#38480;&#12290;&#36825;&#20123;&#23614;&#37096;&#30028;&#38480;&#19982;&#26356;&#24120;&#35265;&#30340;&#38543;&#26426;&#36924;&#36817;&#30340;&#28176;&#36817;&#27491;&#24577;&#32467;&#26524;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20960;&#20309;&#38459;&#23612;&#24615;&#35777;&#26126;&#12290;&#36825;&#25193;&#23637;&#20102;Hajek&#65288;1982&#65289;&#23545;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#32467;&#26524;&#21040;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#39046;&#22495;&#12290;&#23545;&#20110;&#20855;&#26377;&#38750;&#38646;&#26799;&#24230;&#30340;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#29992;&#26469;&#35777;&#26126;$O(1/t)$&#21644;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the behavior of stochastic approximation algorithms where iterates, in expectation, make progress towards an objective at each step. When progress is proportional to the step size of the algorithm, we prove exponential concentration bounds. These tail-bounds contrast asymptotic normality results which are more frequently associated with stochastic approximation. The methods that we develop rely on a geometric ergodicity proof. This extends a result on Markov chains due to Hajek (1982) to the area of stochastic approximation algorithms. For Projected Stochastic Gradient Descent with a non-vanishing gradient, our results can be used to prove $O(1/t)$ and linear convergence rates.
&lt;/p&gt;</description></item><item><title>CrossQ&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#36816;&#29992;&#25209;&#24402;&#19968;&#21270;&#21644;&#21024;&#38500;&#30446;&#26631;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#23454;&#26045;&#31616;&#21333;&#12290;</title><link>http://arxiv.org/abs/1902.05605</link><description>&lt;p&gt;
CrossQ: &#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;&#21644;&#31616;&#27905;&#24615;&#30340;&#25209;&#24402;&#19968;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity. (arXiv:1902.05605v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.05605
&lt;/p&gt;
&lt;p&gt;
CrossQ&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#36816;&#29992;&#25209;&#24402;&#19968;&#21270;&#21644;&#21024;&#38500;&#30446;&#26631;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#23454;&#26045;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26679;&#26412;&#25928;&#29575;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#31639;&#27861;&#65292;&#22914;REDQ&#21644;DroQ&#65292;&#36890;&#36807;&#23558;&#25209;&#27425;&#26631;&#20934;&#21270;&#30340;&#26356;&#26032;&#25968;&#25454;&#65288;UTD&#65289;&#27604;&#29575;&#22686;&#21152;&#21040;&#27599;&#20010;&#29615;&#22659;&#26679;&#26412;&#19978;&#30340;20&#20010;&#26799;&#24230;&#26356;&#26032;&#27493;&#39588;&#65292;&#25913;&#21892;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#20250;&#24102;&#26469;&#22823;&#24133;&#22686;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#35745;&#31639;&#36127;&#25285;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CrossQ&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#23427;&#24039;&#22937;&#22320;&#36816;&#29992;&#25209;&#24402;&#19968;&#21270;&#65292;&#24182;&#21435;&#38500;&#20102;&#30446;&#26631;&#32593;&#32476;&#65292;&#20197;&#22312;&#20445;&#25345;&#20302;UTD&#27604;&#29575;&#20026;1&#30340;&#21516;&#26102;&#36229;&#36234;&#30446;&#21069;&#30340;&#26368;&#26032;&#26679;&#26412;&#25928;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CrossQ&#19981;&#20381;&#36182;&#20110;&#24403;&#21069;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#39640;&#32423;&#20559;&#24046;&#32553;&#20943;&#26041;&#26696;&#12290;CrossQ&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#26368;&#20808;&#36827;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#65288;2&#65289;&#19982;REDQ&#21644;DroQ&#30456;&#27604;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#65288;3&#65289;&#23454;&#26045;&#31616;&#21333;&#65292;&#20165;&#38656;&#35201;&#22312;SAC&#20043;&#19978;&#28155;&#21152;&#20960;&#34892;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce Cross$Q$: a lightweight algorithm that makes careful use of Batch Normalization and removes target networks to surpass the state-of-the-art in sample efficiency while maintaining a low UTD ratio of $1$. Notably, Cross$Q$ does not rely on advanced bias-reduction schemes used in current methods. Cross$Q$'s contributions are thus threefold: (1) state-of-the-art sample efficiency, (2) substantial reduction in computational cost compared to REDQ and DroQ, and (3) ease of implementation, requiring just a few lines of code on top of SAC.
&lt;/p&gt;</description></item></channel></rss>