<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24314;&#31435;&#28176;&#36817;&#25512;&#26029;&#38750;&#23545;&#31216;&#30697;&#38453;&#29305;&#24449;&#21521;&#37327;&#31243;&#24207;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#38024;&#23545;&#23436;&#20840;&#21521;&#37327;&#21644;&#27599;&#20010;&#31995;&#25968;&#20551;&#35774;&#20998;&#21035;&#24314;&#31435;&#20102; Wald &#21644; t &#26816;&#39564;&#30340;&#20998;&#24067;&#29702;&#35770;&#65292;&#26159;&#22810;&#20803;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#31181;&#26377;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2303.18233</link><description>&lt;p&gt;
&#38750;&#23545;&#31216;&#30697;&#38453;&#29305;&#24449;&#21521;&#37327;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference on eigenvectors of non-symmetric matrices. (arXiv:2303.18233v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24314;&#31435;&#28176;&#36817;&#25512;&#26029;&#38750;&#23545;&#31216;&#30697;&#38453;&#29305;&#24449;&#21521;&#37327;&#31243;&#24207;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#38024;&#23545;&#23436;&#20840;&#21521;&#37327;&#21644;&#27599;&#20010;&#31995;&#25968;&#20551;&#35774;&#20998;&#21035;&#24314;&#31435;&#20102; Wald &#21644; t &#26816;&#39564;&#30340;&#20998;&#24067;&#29702;&#35770;&#65292;&#26159;&#22810;&#20803;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#31181;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;&#65292;Tyler&#65288;1981&#65289;&#30340;&#21487;&#23545;&#31216;&#21270;&#26465;&#20214;&#24182;&#38750;&#24314;&#31435;&#28176;&#36817;&#25512;&#26029;&#29305;&#24449;&#21521;&#37327;&#31243;&#24207;&#25152;&#24517;&#38656;&#30340;&#12290; &#25105;&#20204;&#20026;&#23436;&#20840;&#21521;&#37327;&#21644;&#27599;&#20010;&#31995;&#25968;&#20551;&#35774;&#20998;&#21035;&#24314;&#31435;&#20102; Wald &#21644; t &#26816;&#39564;&#30340;&#20998;&#24067;&#29702;&#35770;&#12290; &#25105;&#20204;&#30340;&#26816;&#39564;&#32479;&#35745;&#37327;&#26469;&#28304;&#20110;&#38750;&#23545;&#31216;&#30697;&#38453;&#30340;&#29305;&#24449;&#25237;&#24433;&#12290; &#36890;&#36807;&#23558;&#25237;&#24433;&#34920;&#31034;&#20026;&#20174;&#22522;&#30784;&#30697;&#38453;&#21040;&#20854;&#35889;&#25968;&#25454;&#30340;&#26144;&#23556;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#26512;&#25668;&#21160;&#29702;&#35770;&#25214;&#21040;&#20102;&#23548;&#25968;&#12290; &#36825;&#20123;&#32467;&#26524;&#28436;&#31034;&#20102; Sun&#65288;1991&#65289;&#30340;&#35299;&#26512;&#25668;&#21160;&#29702;&#35770;&#26159;&#22810;&#20803;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#31181;&#26377;&#29992;&#24037;&#20855;&#65292;&#24182;&#19988;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;&#20316;&#20026;&#19968;&#31181;&#24212;&#29992;&#65292;&#25105;&#20204;&#20026;&#30001;&#26377;&#21521;&#22270;&#24341;&#21457;&#30340;&#37051;&#25509;&#30697;&#38453;&#20272;&#35745;&#30340; Bonacich &#20013;&#24515;&#24615;&#23450;&#20041;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper argues that the symmetrisability condition in Tyler(1981) is not necessary to establish asymptotic inference procedures for eigenvectors. We establish distribution theory for a Wald and t-test for full-vector and individual coefficient hypotheses, respectively. Our test statistics originate from eigenprojections of non-symmetric matrices. Representing projections as a mapping from the underlying matrix to its spectral data, we find derivatives through analytic perturbation theory. These results demonstrate how the analytic perturbation theory of Sun(1991) is a useful tool in multivariate statistics and are of independent interest. As an application, we define confidence sets for Bonacich centralities estimated from adjacency matrices induced by directed graphs.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#30340;&#24494;&#27491;&#21017; Langevin Monte Carlo &#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#37319;&#26679; $\exp[-S(\x)]$ &#20998;&#24067;&#65292;&#21516;&#26102;&#20855;&#26377;&#26080;&#20559;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.18221</link><description>&lt;p&gt;
&#24494;&#27491;&#21017; Langevin Monte Carlo
&lt;/p&gt;
&lt;p&gt;
Microcanonical Langevin Monte Carlo. (arXiv:2303.18221v1 [hep-lat])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18221
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#30340;&#24494;&#27491;&#21017; Langevin Monte Carlo &#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#37319;&#26679; $\exp[-S(\x)]$ &#20998;&#24067;&#65292;&#21516;&#26102;&#20855;&#26377;&#26080;&#20559;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#21487;&#29992;&#28176;&#21464; $ \nabla S(\x)$ &#30340;&#24418;&#24335;&#37319;&#26679;&#33258;&#19968;&#20219;&#24847;&#20998;&#24067; $ \exp[-S(\x)]$&#65292;&#35813;&#26041;&#27861;&#34987;&#21046;&#23450;&#20026;&#20445;&#25345;&#33021;&#37327;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986; Fokker-Planck &#26041;&#31243;&#65292;&#24182;&#35777;&#26126;&#30830;&#23450;&#24615;&#28418;&#31227;&#21644;&#38543;&#26426;&#25193;&#25955;&#20998;&#21035;&#20445;&#25345;&#24179;&#31283;&#20998;&#24067;&#12290;&#36825;&#24847;&#21619;&#30528;&#28418;&#31227;&#25193;&#25955;&#31163;&#25955;&#21270;&#26041;&#26696;&#26080;&#20559;&#65292;&#32780;&#26631;&#20934; Langevin &#21160;&#21147;&#23398;&#21017;&#19981;&#26159;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110; $\phi^4$ &#26230;&#26684;&#22330;&#35770;&#65292;&#23637;&#31034;&#20102;&#32467;&#26524;&#19982;&#26631;&#20934;&#37319;&#26679;&#26041;&#27861;&#19968;&#33268;&#65292;&#20294;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#22120;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method for sampling from an arbitrary distribution $\exp[-S(\x)]$ with an available gradient $\nabla S(\x)$, formulated as an energy-preserving stochastic differential equation (SDE). We derive the Fokker-Planck equation and show that both the deterministic drift and the stochastic diffusion separately preserve the stationary distribution. This implies that the drift-diffusion discretization schemes are bias-free, in contrast to the standard Langevin dynamics. We apply the method to the $\phi^4$ lattice field theory, showing the results agree with the standard sampling methods but with significantly higher efficiency compared to the current state-of-the-art samplers.
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25506;&#35752;&#20102;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#25214;&#21040;&#22240;&#26524;&#39034;&#24207;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#21457;&#29616;&#38500;&#20102;&#26041;&#24046;&#25490;&#24207;&#22806;&#65292;&#21464;&#37327;&#30340;&#20915;&#23450;&#31995;&#25968;$R^2$&#25490;&#24207;&#20063;&#21487;&#29992;&#20110;&#21305;&#37197;&#24050;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#19988;&#19981;&#21463;&#25968;&#25454;&#32553;&#25918;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.18211</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#25490;&#24207;&#26631;&#20934;&#26377;&#21161;&#20110;&#22312;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#25214;&#21040;&#22240;&#26524;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simple Sorting Criteria Help Find the Causal Order in Additive Noise Models. (arXiv:2303.18211v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18211
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25506;&#35752;&#20102;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#25214;&#21040;&#22240;&#26524;&#39034;&#24207;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#21457;&#29616;&#38500;&#20102;&#26041;&#24046;&#25490;&#24207;&#22806;&#65292;&#21464;&#37327;&#30340;&#20915;&#23450;&#31995;&#25968;$R^2$&#25490;&#24207;&#20063;&#21487;&#29992;&#20110;&#21305;&#37197;&#24050;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#19988;&#19981;&#21463;&#25968;&#25454;&#32553;&#25918;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65288;ANM&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21151;&#33021;&#20551;&#35774;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#12290;&#30001;&#20110;&#32570;&#20047;&#31526;&#21512;&#20551;&#35774;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#21512;&#25104;ANM&#25968;&#25454;&#32463;&#24120;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#12290;Reisach&#31561;&#20154;&#65288;2021&#65289;&#34920;&#26126;&#65292;&#23545;&#20110;&#24120;&#35265;&#30340;&#27169;&#25311;&#21442;&#25968;&#65292;&#25353;&#22686;&#22823;&#26041;&#24046;&#30340;&#39034;&#24207;&#21464;&#37327;&#25490;&#21015;&#19982;&#22240;&#26524;&#39034;&#24207;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#24341;&#20837;&#21464;&#24322;&#24615;&#21487;&#25490;&#24207;&#24615;&#26469;&#37327;&#21270;&#36825;&#31181;&#23545;&#40784;&#31243;&#24230;&#12290;&#26412;&#25991;&#36824;&#34920;&#26126;&#65292;&#38500;&#20102;&#26041;&#24046;&#65292;&#36824;&#26377;&#21464;&#37327;&#30340;&#26041;&#24046;&#34987;&#25152;&#26377;&#20854;&#20182;&#21464;&#37327;&#35299;&#37322;&#30340;&#27604;&#20363;&#65288;&#30001;&#20915;&#23450;&#31995;&#25968;$R^2$&#25429;&#33719;&#65289;&#20542;&#21521;&#20110;&#27839;&#30528;&#22240;&#26524;&#39034;&#24207;&#22686;&#21152;&#12290;&#31616;&#21333;&#30340;&#22522;&#20934;&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;$R^2$-sortability&#26469;&#21305;&#37197;&#24050;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;$R^2$&#21487;&#25490;&#24207;&#24615;&#19981;&#21463;&#25968;&#25454;&#32553;&#25918;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#26631;&#20934;&#21270;&#25110;&#37325;&#26032;&#32553;&#25918;&#30340;&#25968;&#25454;&#19978;&#34920;&#29616;&#21516;&#26679;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;&#21033;&#29992;&#21464;&#24322;&#24615;&#21487;&#25490;&#24207;&#24615;&#30340;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Additive Noise Models (ANM) encode a popular functional assumption that enables learning causal structure from observational data. Due to a lack of real-world data meeting the assumptions, synthetic ANM data are often used to evaluate causal discovery algorithms. Reisach et al. (2021) show that, for common simulation parameters, a variable ordering by increasing variance is closely aligned with a causal order and introduce var-sortability to quantify the alignment. Here, we show that not only variance, but also the fraction of a variable's variance explained by all others, as captured by the coefficient of determination $R^2$, tends to increase along the causal order. Simple baseline algorithms can use $R^2$-sortability to match the performance of established methods. Since $R^2$-sortability is invariant under data rescaling, these algorithms perform equally well on standardized or rescaled data, addressing a key limitation of algorithms exploiting var-sortability. We characterize and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#28041;&#21450;&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.18158</link><description>&lt;p&gt;
&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#31209;&#19968;&#20989;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Constrained Optimization of Rank-One Functions with Indicator Variables. (arXiv:2303.18158v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#28041;&#21450;&#25351;&#26631;&#21464;&#37327;&#38480;&#21046;&#19979;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#28041;&#21450;&#21040;&#36890;&#36807;&#32422;&#26463;&#26469;&#24314;&#27169;&#20915;&#31574;&#21464;&#37327;&#25903;&#25345;&#38598;&#21512;&#30340;&#31209;&#19968;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#37319;&#29992;&#25351;&#26631;&#21464;&#37327;&#26469;&#35782;&#21035;&#36830;&#32493;&#21464;&#37327;&#30340;&#25903;&#25345;&#12290;&#26412;&#25991;&#36890;&#36807;&#36879;&#35270;&#37325;&#26500;&#25216;&#26415;&#30740;&#31350;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#32039;&#20945;&#25193;&#23637;&#20844;&#24335;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#25903;&#25345;&#20989;&#25968;&#21442;&#25968;&#21644;&#31163;&#25955;&#35268;&#21010;&#25216;&#26415;&#20197;&#25552;&#20379;&#20984;&#21253;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#36896;&#26041;&#27861;&#65292;&#21033;&#29992;&#36879;&#35270;&#20989;&#25968;&#24341;&#36215;&#30340;&#38544;&#34255;&#22278;&#38181;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#27599;&#20010;&#22278;&#38181;&#32422;&#26463;&#28041;&#21450;&#29420;&#31435;&#36830;&#32493;&#21464;&#37327;&#30340;&#32447;&#24615;&#20989;&#25968;&#21644;&#19968;&#32452;&#20108;&#20803;&#21464;&#37327;&#30340;&#19968;&#33324;&#22278;&#38181;&#28151;&#21512;&#20108;&#36827;&#21046;&#38598;&#21512;&#24314;&#31435;&#20102;&#19968;&#20010;&#20984;&#21253;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#24212;&#23545;epi&#30456;&#20851;&#30340;&#38598;&#21512;&#30340;&#25193;&#23637;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization problems involving minimization of a rank-one convex function over constraints modeling restrictions on the support of the decision variables emerge in various machine learning applications. These problems are often modeled with indicator variables for identifying the support of the continuous variables. In this paper we investigate compact extended formulations for such problems through perspective reformulation techniques. In contrast to the majority of previous work that relies on support function arguments and disjunctive programming techniques to provide convex hull results, we propose a constructive approach that exploits a hidden conic structure induced by perspective functions. To this end, we first establish a convex hull result for a general conic mixed-binary set in which each conic constraint involves a linear function of independent continuous variables and a set of binary variables. We then demonstrate that extended representations of sets associated with epi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#21644;&#35745;&#31639;&#21487;&#34892;&#24615;&#65292;&#21457;&#29616;&#32500;&#24230;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#25104;&#32447;&#24615;&#20851;&#31995;&#65292;&#20294;&#20351;&#29992;&#26679;&#26412;&#23789;&#24230;&#26041;&#27861;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#38480;&#21046;&#20351;&#29992;&#20302;&#27425;&#22810;&#39033;&#24335;&#31639;&#27861;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#23558;&#25104;&#20026;&#32500;&#24230;&#30340;&#24179;&#26041;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#26377;&#35745;&#31639;&#21487;&#34892;&#24615;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26497;&#23567;&#21270;&#26368;&#22823;&#39118;&#38505;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.18156</link><description>&lt;p&gt;
&#22823;&#32500;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;: &#32479;&#35745;&#26368;&#20248;&#24615;&#19982;&#35745;&#31639;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large Dimensional Independent Component Analysis: Statistical Optimality and Computational Tractability. (arXiv:2303.18156v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#21644;&#35745;&#31639;&#21487;&#34892;&#24615;&#65292;&#21457;&#29616;&#32500;&#24230;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#25104;&#32447;&#24615;&#20851;&#31995;&#65292;&#20294;&#20351;&#29992;&#26679;&#26412;&#23789;&#24230;&#26041;&#27861;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#38480;&#21046;&#20351;&#29992;&#20302;&#27425;&#22810;&#39033;&#24335;&#31639;&#27861;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#23558;&#25104;&#20026;&#32500;&#24230;&#30340;&#24179;&#26041;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#26377;&#35745;&#31639;&#21487;&#34892;&#24615;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26497;&#23567;&#21270;&#26368;&#22823;&#39118;&#38505;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#30340;&#26368;&#20248;&#32479;&#35745;&#24615;&#33021;&#21644;&#35745;&#31639;&#38480;&#21046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20004;&#26041;&#38754;&#30340;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#32479;&#35745;&#20934;&#30830;&#24230;&#19982;&#32500;&#24230;&#31934;&#30830;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#35745;&#31639;&#32771;&#34385;&#22914;&#20309;&#24433;&#21709;&#23427;&#20204;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#32500;&#24230;&#25104;&#32447;&#24615;&#20851;&#31995;&#65292;&#21516;&#26102;&#65292;&#19968;&#20123;&#24120;&#29992;&#30340;&#22522;&#20110;&#26679;&#26412;&#23789;&#24230;&#30340;&#26041;&#27861;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#27425;&#20248;&#30340;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#38480;&#21046;&#21482;&#20351;&#29992;&#33021;&#22815;&#29992;&#20302;&#27425;&#22810;&#39033;&#24335;&#31639;&#27861;&#35745;&#31639;&#30340;&#20272;&#35745;&#65292;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#23558;&#21464;&#20026;&#32500;&#24230;&#30340;&#24179;&#26041;&#65292;&#31216;&#20026;&#23545;&#25968;&#22240;&#23376;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20855;&#26377;&#35745;&#31639;&#21487;&#34892;&#24615;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26497;&#23567;&#21270;&#26368;&#22823;&#39118;&#38505;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#30340;&#28176;&#36817;&#24615;&#36136;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the optimal statistical performance and the impact of computational constraints for independent component analysis (ICA). Our goal is twofold. On the one hand, we characterize the precise role of dimensionality on sample complexity and statistical accuracy, and how computational consideration may affect them. In particular, we show that the optimal sample complexity is linear in dimensionality, and interestingly, the commonly used sample kurtosis-based approaches are necessarily suboptimal. However, the optimal sample complexity becomes quadratic, up to a logarithmic factor, in the dimension if we restrict ourselves to estimates that can be computed with low-degree polynomial algorithms. On the other hand, we develop computationally tractable estimates that attain both the optimal sample complexity and minimax optimal rates of convergence. We study the asymptotic properties of the proposed estimates and establish their asymptotic normality that can be read
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BERTino&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;DistilBERT&#27169;&#22411;&#65292;&#26159;&#29992;&#20110;&#24847;&#22823;&#21033;&#35821;&#30340;&#31532;&#19968;&#20010;&#26367;&#20195;BERT&#20307;&#31995;&#32467;&#26500;&#30340;&#36873;&#25321;&#65292;&#20854;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;F1&#20998;&#25968;&#19982;BERTBASE&#30456;&#24403;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.18121</link><description>&lt;p&gt;
BERTino&#65306;&#19968;&#31181;&#24847;&#22823;&#21033;DistilBERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BERTino: an Italian DistilBERT model. (arXiv:2303.18121v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BERTino&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;DistilBERT&#27169;&#22411;&#65292;&#26159;&#29992;&#20110;&#24847;&#22823;&#21033;&#35821;&#30340;&#31532;&#19968;&#20010;&#26367;&#20195;BERT&#20307;&#31995;&#32467;&#26500;&#30340;&#36873;&#25321;&#65292;&#20854;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;F1&#20998;&#25968;&#19982;BERTBASE&#30456;&#24403;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#30340;Transformer&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#34429;&#28982;&#24778;&#20154;&#65292;&#20294;&#30001;&#20110;&#26500;&#25104;&#20854;&#32593;&#32476;&#30340;&#21442;&#25968;&#36807;&#22810;&#65292;&#23548;&#33268;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#39640;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BERTino&#65292;&#19968;&#31181;DistilBERT&#27169;&#22411;&#65292;&#23427;&#26159;&#29992;&#20110;&#24847;&#22823;&#21033;&#35821;&#30340;&#31532;&#19968;&#20010;&#36731;&#37327;&#32423;&#26367;&#20195;BERT&#20307;&#31995;&#32467;&#26500;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#23545;BERTino&#22312;&#24847;&#22823;&#21033;ISDT&#12289;&#24847;&#22823;&#21033;ParTUT&#12289;&#24847;&#22823;&#21033;WikiNER&#21644;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#33719;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#24182;&#33719;&#24471;&#20102;&#19982;BERTBASE&#30456;&#24403;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent introduction of Transformers language representation models allowed great improvements in many natural language processing (NLP) tasks. However, if on one hand the performances achieved by this kind of architectures are surprising, on the other their usability is limited by the high number of parameters which constitute their network, resulting in high computational and memory demands. In this work we present BERTino, a DistilBERT model which proposes to be the first lightweight alternative to the BERT architecture specific for the Italian language. We evaluated BERTino on the Italian ISDT, Italian ParTUT, Italian WikiNER and multiclass classification tasks, obtaining F1 scores comparable to those obtained by a BERTBASE with a remarkable improvement in training and inference speed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#36848;&#20102;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#22320;&#29702;&#31354;&#38388;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#30340;&#20855;&#20307;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.18087</link><description>&lt;p&gt;
&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#30340;&#35780;&#20272;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Evaluation Challenges for Geospatial ML. (arXiv:2303.18087v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#22320;&#29702;&#31354;&#38388;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#30340;&#20855;&#20307;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#30001;&#20854;&#39044;&#27979;&#20135;&#29983;&#30340;&#22320;&#22270;&#22312;&#31185;&#23398;&#21644;&#25919;&#31574;&#30340;&#19979;&#28216;&#20998;&#26512;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#20351;&#29992;&#65292;&#35780;&#20272;&#20854;&#20934;&#30830;&#24615;&#21644;&#36866;&#29992;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#19982;&#20854;&#20182;&#23398;&#20064;&#33539;&#20363;&#26377;&#30528;&#20851;&#38190;&#24046;&#24322;&#65292;&#22240;&#27492;&#65292;&#34913;&#37327;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#36755;&#20986;&#24615;&#33021;&#30340;&#27491;&#30830;&#26041;&#27861;&#19968;&#30452;&#26159;&#20105;&#35770;&#30340;&#35805;&#39064;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#20840;&#29699;&#25110;&#36965;&#24863;&#25968;&#25454;&#19979;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;&#25913;&#36827;&#22320;&#29702;&#31354;&#38388;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#30340;&#20855;&#20307;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As geospatial machine learning models and maps derived from their predictions are increasingly used for downstream analyses in science and policy, it is imperative to evaluate their accuracy and applicability. Geospatial machine learning has key distinctions from other learning paradigms, and as such, the correct way to measure performance of spatial machine learning outputs has been a topic of debate. In this paper, I delineate unique challenges of model evaluation for geospatial machine learning with global or remotely sensed datasets, culminating in concrete takeaways to improve evaluations of geospatial model performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#22270;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22788;&#29702;&#20855;&#26377;&#20849;&#21516;&#39030;&#28857;&#38598;&#30340;&#22810;&#20010;&#22270;&#65292;&#26377;&#30528;&#38750;&#24120;&#29702;&#24819;&#30340;&#8220;&#21327;&#21516;&#25928;&#24212;&#8221;&#65292;&#21363;&#39030;&#28857;&#20998;&#31867;&#20934;&#30830;&#24230;&#24635;&#26159;&#21463;&#30410;&#20110;&#39069;&#22806;&#30340;&#22270;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#20854;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18051</link><description>&lt;p&gt;
&#22522;&#20110;&#32534;&#30721;&#22120;&#23884;&#20837;&#30340;&#21327;&#21516;&#22270;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Synergistic Graph Fusion via Encoder Embedding. (arXiv:2303.18051v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#22270;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22788;&#29702;&#20855;&#26377;&#20849;&#21516;&#39030;&#28857;&#38598;&#30340;&#22810;&#20010;&#22270;&#65292;&#26377;&#30528;&#38750;&#24120;&#29702;&#24819;&#30340;&#8220;&#21327;&#21516;&#25928;&#24212;&#8221;&#65292;&#21363;&#39030;&#28857;&#20998;&#31867;&#20934;&#30830;&#24230;&#24635;&#26159;&#21463;&#30410;&#20110;&#39069;&#22806;&#30340;&#22270;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#20854;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#34701;&#21512;&#32534;&#30721;&#22120;&#23884;&#20837;&#30340;&#22810;&#22270;&#23884;&#20837;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#22788;&#29702;&#20855;&#26377;&#20849;&#21516;&#39030;&#28857;&#38598;&#30340;&#22810;&#20010;&#22270;&#12290;&#22312;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#21497;&#20294;&#38750;&#24120;&#29702;&#24819;&#30340;&#8220;&#21327;&#21516;&#25928;&#24212;&#8221;&#65306;&#23545;&#20110;&#36275;&#22815;&#22823;&#30340;&#39030;&#28857;&#25968;&#65292;&#20998;&#31867;&#20934;&#30830;&#24230;&#24635;&#26159;&#21463;&#30410;&#20110;&#39069;&#22806;&#30340;&#22270;&#12290;&#25105;&#20204;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#19979;&#25552;&#20379;&#20102;&#36825;&#31181;&#25928;&#24212;&#30340;&#25968;&#23398;&#35777;&#26126;&#65292;&#24182;&#30830;&#23450;&#20102;&#28176;&#36817;&#23436;&#32654;&#20998;&#31867;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#20013;&#22987;&#32456;&#20248;&#20110;&#26368;&#36817;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel approach to multi-graph embedding called graph fusion encoder embedding. The method is designed to work with multiple graphs that share a common vertex set. Under the supervised learning setting, we show that the resulting embedding exhibits a surprising yet highly desirable "synergistic effect": for sufficiently large vertex size, the vertex classification accuracy always benefits from additional graphs. We provide a mathematical proof of this effect under the stochastic block model, and identify the necessary and sufficient condition for asymptotically perfect classification. The simulations and real data experiments confirm the superiority of the proposed method, which consistently outperforms recent benchmark methods in classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#21644;&#19968;&#33324;&#30340;$\ell_p^d$&#31354;&#38388;&#20013;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;DP-SCO&#65289;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26041;&#27861;&#65292;&#20854;&#36755;&#20986;&#33021;&#22815;&#23454;&#29616;(&#39044;&#26399;)&#36807;&#37327;&#31181;&#32676;&#39118;&#38505;&#65292;&#36825;&#21482;&#21462;&#20915;&#20110;&#32422;&#26463;&#38598;&#21512;&#30340;&#39640;&#26031;&#23485;&#24230;&#65292;&#23545;&#24378;&#20984;&#20989;&#25968;&#65292;&#30028;&#38480;&#26159;&#26368;&#20248;&#30340;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#37325;&#23614;&#25968;&#25454;&#36827;&#34892;DP-SCO&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;$1&lt;p&lt;2$&#21644;$2&#8804;p&#8804;&#8734;$&#30340;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.18047</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#20984;&#20248;&#21270;&#22312;&#65288;&#38750;&#65289;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20877;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Stochastic Convex Optimization in (Non)-Euclidean Space Revisited. (arXiv:2303.18047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#21644;&#19968;&#33324;&#30340;$\ell_p^d$&#31354;&#38388;&#20013;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;DP-SCO&#65289;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26041;&#27861;&#65292;&#20854;&#36755;&#20986;&#33021;&#22815;&#23454;&#29616;(&#39044;&#26399;)&#36807;&#37327;&#31181;&#32676;&#39118;&#38505;&#65292;&#36825;&#21482;&#21462;&#20915;&#20110;&#32422;&#26463;&#38598;&#21512;&#30340;&#39640;&#26031;&#23485;&#24230;&#65292;&#23545;&#24378;&#20984;&#20989;&#25968;&#65292;&#30028;&#38480;&#26159;&#26368;&#20248;&#30340;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#37325;&#23614;&#25968;&#25454;&#36827;&#34892;DP-SCO&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;$1&lt;p&lt;2$&#21644;$2&#8804;p&#8804;&#8734;$&#30340;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#21644;&#19968;&#33324;&#30340;$\ell_p^d$&#31354;&#38388;&#20013;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;DP-SCO&#65289;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#19977;&#20010;&#20173;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#30340;&#35774;&#32622;: (1) &#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#38480;&#21046;&#19988;&#26377;&#30028;&#30340;&#65288;&#20984;&#65289;&#38598;&#21512;&#19978;&#36827;&#34892;DP-SCO; (2) &#22312;$\ell_p^d$&#31354;&#38388;&#19978;&#26080;&#38480;&#21046;&#30340;DP-SCO; (3) &#22312;&#38480;&#21046;&#19988;&#26377;&#30028;&#30340;$\ell_p^d$&#31354;&#38388;&#20013;&#65292;&#20351;&#29992;&#37325;&#23614;&#25968;&#25454;&#36827;&#34892;DP-SCO&#12290;&#23545;&#20110;&#38382;&#39064;&#65288;1&#65289;&#65292;&#38024;&#23545;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26041;&#27861;&#65292;&#20854;&#36755;&#20986;&#33021;&#22815;&#23454;&#29616;(&#39044;&#26399;)&#36807;&#37327;&#31181;&#32676;&#39118;&#38505;&#65292;&#36825;&#21482;&#21462;&#20915;&#20110;&#32422;&#26463;&#38598;&#21512;&#30340;&#39640;&#26031;&#23485;&#24230;&#65292;&#32780;&#19981;&#26159;&#31354;&#38388;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23545;&#20110;&#24378;&#20984;&#20989;&#25968;&#65292;&#30028;&#38480;&#26159;&#26368;&#20248;&#30340;&#65292;&#26368;&#22810;&#30456;&#24046;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#12290;&#23545;&#20110;&#38382;&#39064;&#65288;2&#65289;&#21644;&#65288;3&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#31639;&#27861;&#65292;&#24182;&#38024;&#23545;$1&lt;p&lt;2$&#21644;$2&#8804;p&#8804;&#8734;$&#30340;&#20004;&#31181;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we revisit the problem of Differentially Private Stochastic Convex Optimization (DP-SCO) in Euclidean and general $\ell_p^d$ spaces. Specifically, we focus on three settings that are still far from well understood: (1) DP-SCO over a constrained and bounded (convex) set in Euclidean space; (2) unconstrained DP-SCO in $\ell_p^d$ space; (3) DP-SCO with heavy-tailed data over a constrained and bounded set in $\ell_p^d$ space. For problem (1), for both convex and strongly convex loss functions, we propose methods whose outputs could achieve (expected) excess population risks that are only dependent on the Gaussian width of the constraint set rather than the dimension of the space. Moreover, we also show the bound for strongly convex functions is optimal up to a logarithmic factor. For problems (2) and (3), we propose several novel algorithms and provide the first theoretical results for both cases when $1&lt;p&lt;2$ and $2\leq p\leq \infty$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#32593;&#33180;&#21160;&#38745;&#33033;&#20998;&#21106;&#20013;&#30340;&#25299;&#25169;-&#37325;&#21472;&#25240;&#34935;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#25299;&#25169;&#20445;&#25345;&#39033;&#21644;&#26041;&#21521;&#35780;&#20998;&#24341;&#23548;&#30340;&#21367;&#31215;&#27169;&#22359;&#26469;&#25552;&#39640;&#20998;&#21106;&#30340;&#36830;&#32493;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.18022</link><description>&lt;p&gt;
&#35270;&#32593;&#33180;&#21160;&#38745;&#33033;&#20998;&#21106;&#20013;&#30340;&#25299;&#25169; - &#37325;&#21472;&#25240;&#34935;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Topology-Overlap Trade-Off in Retinal Arteriole-Venule Segmentation. (arXiv:2303.18022v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#32593;&#33180;&#21160;&#38745;&#33033;&#20998;&#21106;&#20013;&#30340;&#25299;&#25169;-&#37325;&#21472;&#25240;&#34935;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#25299;&#25169;&#20445;&#25345;&#39033;&#21644;&#26041;&#21521;&#35780;&#20998;&#24341;&#23548;&#30340;&#21367;&#31215;&#27169;&#22359;&#26469;&#25552;&#39640;&#20998;&#21106;&#30340;&#36830;&#32493;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#21487;&#20197;&#25104;&#20026;&#31579;&#26597;&#39640;&#34880;&#21387;&#25110;&#31958;&#23615;&#30149;&#31561;&#27969;&#34892;&#30149;&#30340;&#23453;&#36149;&#35786;&#26029;&#24037;&#20855;&#12290;&#36890;&#36807;&#33258;&#21160;&#20998;&#21106;&#20998;&#21449;&#22788;&#21644;&#20132;&#21449;&#22788;&#65292;&#21487;&#20197;&#22312;&#25552;&#39640;&#20998;&#21106;&#37325;&#21472;&#24230;&#30340;&#21516;&#26102;&#25552;&#39640;&#31649;&#29366;&#32467;&#26500;&#30340;&#25299;&#25169;&#27491;&#30830;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#32771;&#34385;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#25299;&#25169;&#20445;&#25345;&#39033;&#65292;&#20197;&#25552;&#39640;&#20998;&#21106;&#30340;&#36830;&#32493;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#20250;&#23548;&#33268;&#21160;&#33033;&#21644;&#38745;&#33033;&#20043;&#38388;&#30340;&#35823;&#20998;&#31867;&#21644;&#24635;&#20307;&#37325;&#21472;&#24230;&#38477;&#20302;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#21508;&#21521;&#24322;&#24615;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#26041;&#21521;&#35780;&#20998;&#24341;&#23548;&#30340;&#21367;&#31215;&#27169;&#22359;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21160;&#38745;&#33033;&#32454;&#20998;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retinal fundus images can be an invaluable diagnosis tool for screening epidemic diseases like hypertension or diabetes. And they become especially useful when the arterioles and venules they depict are clearly identified and annotated. However, manual annotation of these vessels is extremely time demanding and taxing, which calls for automatic segmentation. Although convolutional neural networks can achieve high overlap between predictions and expert annotations, they often fail to produce topologically correct predictions of tubular structures. This situation is exacerbated by the bifurcation versus crossing ambiguity which causes classification mistakes. This paper shows that including a topology preserving term in the loss function improves the continuity of the segmented vessels, although at the expense of artery-vein misclassification and overall lower overlap metrics. However, we show that by including an orientation score guided convolutional module, based on the anisotropic si
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24555;&#36895;&#39044;&#27979;&#23454;&#39564;&#23460;&#22521;&#20859;&#30340;&#32454;&#32990;/hydrogels&#30340;&#26426;&#26800;&#29305;&#24615;&#30340;&#29702;&#35770;&#35777;&#26126;&#65292;&#20943;&#23569;&#20102;&#29289;&#29702;&#23454;&#39564;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.18017</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24555;&#36895;&#39044;&#27979;&#23454;&#39564;&#23460;&#22521;&#20859;&#30340;&#32452;&#32455;&#26426;&#26800;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rapid prediction of lab-grown tissue properties using deep learning. (arXiv:2303.18017v1 [q-bio.TO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18017
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24555;&#36895;&#39044;&#27979;&#23454;&#39564;&#23460;&#22521;&#20859;&#30340;&#32454;&#32990;/hydrogels&#30340;&#26426;&#26800;&#29305;&#24615;&#30340;&#29702;&#35770;&#35777;&#26126;&#65292;&#20943;&#23569;&#20102;&#29289;&#29702;&#23454;&#39564;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#21644;&#32454;&#32990;&#22806;&#22522;&#36136;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26159;&#32452;&#32455;&#33258;&#32452;&#32455;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#39044;&#27979;&#22312;&#22266;&#23450;&#27169;&#20855;&#20013;&#22521;&#20859;&#30340;&#32454;&#32990;/hydrogels&#33258;&#32452;&#32455;&#20013;&#26426;&#26800;&#29983;&#29289;&#23398;&#20316;&#29992;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#25105;&#20204;&#36890;&#36807;CONTRACT network dipole orientation(CONDOR)&#27169;&#22411;&#22312;&#27169;&#20855;&#20013;&#27169;&#25311;&#20102;6500&#20010;&#32454;&#32990;/&#22522;&#36136;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#20351;&#29992;\texttt{pix2pix}&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#23454;&#29616;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#23558;&#20445;&#30041;&#30340;740&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#26696;&#20363;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#39044;&#27979;&#32467;&#26524;&#19982;&#29983;&#29289;&#29289;&#29702;&#31639;&#27861;&#30340;&#20445;&#30041;&#39044;&#27979;&#20043;&#38388;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#24555;&#36895;&#39044;&#27979;&#32454;&#32990;/hydrogels&#30340;&#26426;&#26800;&#29305;&#24615;&#65292;&#20943;&#23569;&#20102;&#32791;&#26102;&#21644;&#26114;&#36149;&#30340;&#29289;&#29702;&#23454;&#39564;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interactions between cells and the extracellular matrix are vital for the self-organisation of tissues. In this paper we present proof-of-concept to use machine learning tools to predict the role of this mechanobiology in the self-organisation of cell-laden hydrogels grown in tethered moulds. We develop a process for the automated generation of mould designs with and without key symmetries. We create a large training set with $N=6500$ cases by running detailed biophysical simulations of cell-matrix interactions using the contractile network dipole orientation (CONDOR) model for the self-organisation of cellular hydrogels within these moulds. These are used to train an implementation of the \texttt{pix2pix} deep learning model, reserving $740$ cases that were unseen in the training of the neural network for training and validation. Comparison between the predictions of the machine learning technique and the reserved predictions from the biophysical algorithm show that the machine le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17963</link><description>&lt;p&gt;
&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States. (arXiv:2303.17963v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25511;&#21046;&#24037;&#31243;&#26041;&#27861;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#36776;&#35782;&#26041;&#27861;&#25104;&#20026;&#29289;&#29702;&#24314;&#27169;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#29366;&#24577;&#27979;&#37327;&#30340;&#21487;&#29992;&#24615;&#65292;&#32780;&#22797;&#26434;&#31995;&#32479;&#30340;&#29366;&#24577;&#36890;&#24120;&#19981;&#26159;&#30452;&#25509;&#21487;&#27979;&#37327;&#30340;&#12290;&#22240;&#27492;&#65292;&#21487;&#33021;&#38656;&#35201;&#21516;&#26102;&#20272;&#35745;&#21160;&#21147;&#23398;&#21644;&#28508;&#22312;&#29366;&#24577;&#65292;&#20174;&#32780;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#22320;&#35774;&#35745;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#25511;&#21046;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#30340;&#26410;&#30693;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#36755;&#20837;&#36712;&#36857;&#12290;&#23545;&#32467;&#26524;&#36755;&#20837;&#36712;&#36857;&#36827;&#34892;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As control engineering methods are applied to increasingly complex systems, data-driven approaches for system identification appear as a promising alternative to physics-based modeling. While many of these approaches rely on the availability of state measurements, the states of a complex system are often not directly measurable. It may then be necessary to jointly estimate the dynamics and a latent state, making it considerably more challenging to design controllers with performance guarantees. This paper proposes a novel method for the computation of an optimal input trajectory for unknown nonlinear systems with latent states. Probabilistic performance guarantees are derived for the resulting input trajectory, and an approach to validate the performance of arbitrary control laws is presented. The effectiveness of the proposed method is demonstrated in a numerical simulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27599;&#20010;&#31034;&#20363;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270; (PEGR) &#25216;&#26415;&#65292;&#35777;&#26126;&#20854;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20449;&#21495;&#24182;&#25233;&#21046;&#22122;&#38899;&#65292;&#20174;&#32780;&#25552;&#39640;&#27979;&#35797;&#35823;&#24046;&#21644;&#25239;&#22122;&#22768;&#25200;&#21160;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.17940</link><description>&lt;p&gt;
&#27599;&#20010;&#31034;&#20363;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270;&#25913;&#36827;&#20102;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Per-Example Gradient Regularization Improves Learning Signals from Noisy Data. (arXiv:2303.17940v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27599;&#20010;&#31034;&#20363;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270; (PEGR) &#25216;&#26415;&#65292;&#35777;&#26126;&#20854;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20449;&#21495;&#24182;&#25233;&#21046;&#22122;&#38899;&#65292;&#20174;&#32780;&#25552;&#39640;&#27979;&#35797;&#35823;&#24046;&#21644;&#25239;&#22122;&#22768;&#25200;&#21160;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27599;&#20010;&#31034;&#20363;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270; (PEGR) &#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#39640;&#27979;&#35797;&#35823;&#24046;&#21644;&#25239;&#22122;&#22768;&#25200;&#21160;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102; \citet {cao2022benign} &#30340;&#20449;&#21495;&#22122;&#38899;&#25968;&#25454;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102; PEGR &#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20449;&#21495;&#24182;&#25233;&#21046;&#22122;&#38899;&#12290;&#19982;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#19981;&#21516;&#65292;PEGR &#21487;&#20197;&#21306;&#20998;&#20449;&#21495;&#21644;&#22122;&#38899;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102; PEGR &#24809;&#32602;&#27169;&#24335;&#23398;&#20064;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25233;&#21046;&#20102;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#35760;&#24518;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient regularization, as described in \citet{barrett2021implicit}, is a highly effective technique for promoting flat minima during gradient descent. Empirical evidence suggests that this regularization technique can significantly enhance the robustness of deep learning models against noisy perturbations, while also reducing test error. In this paper, we explore the per-example gradient regularization (PEGR) and present a theoretical analysis that demonstrates its effectiveness in improving both test error and robustness against noise perturbations. Specifically, we adopt a signal-noise data model from \citet{cao2022benign} and show that PEGR can learn signals effectively while suppressing noise. In contrast, standard gradient descent struggles to distinguish the signal from the noise, leading to suboptimal generalization performance. Our analysis reveals that PEGR penalizes the variance of pattern learning, thus effectively suppressing the memorization of noises from the training d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411; (N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#21487;&#20197;&#23545;&#36830;&#32493;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17823</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#22238;&#24212;&#26377;&#24207;&#22238;&#24402;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response. (arXiv:2303.17823v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411; (N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#21487;&#20197;&#23545;&#36830;&#32493;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411;&#65288;N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#20854;&#20013;&#21453;&#24212;&#21464;&#37327;&#19981;&#20165;&#21487;&#20197;&#21462;&#31163;&#25955;&#20540;&#65292;&#20063;&#21487;&#20197;&#21462;&#36830;&#32493;&#20540;&#65292;&#32780;&#22238;&#24402;&#31995;&#25968;&#26681;&#25454;&#39044;&#27979;&#39034;&#24207;&#21453;&#24212;&#20063;&#19981;&#21516;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30452;&#25509;&#20174;&#31163;&#25955;&#21453;&#24212;&#20272;&#35745;&#32447;&#24615;&#31995;&#25968;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20197;&#21453;&#24212;&#20026;&#36755;&#20837;&#20135;&#29983;&#32447;&#24615;&#31995;&#25968;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;N$^3$POM&#21487;&#20197;&#22312;&#20445;&#30041;&#20256;&#32479;&#26377;&#24207;&#22238;&#24402;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20805;&#20998;&#30340;&#26465;&#20214;&#65292;&#20351;&#24471;&#22312;&#25351;&#23450;&#30340;&#29992;&#25143;&#21306;&#22495;&#20869;&#65292;&#39044;&#27979;&#30340;&#26465;&#20214;&#32047;&#31215;&#27010;&#29575;&#65288;CCP&#65289;&#28385;&#36275;&#23616;&#37096;&#21333;&#35843;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20445;&#25345;&#21333;&#35843;&#24615;&#30340;&#38543;&#26426;&#65288;MPS&#65289;&#31639;&#27861;&#26469;&#20805;&#20998;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an interpretable neural network-based non-proportional odds model (N$^3$POM) for ordinal regression, where the response variable can take not only discrete but also continuous values, and the regression coefficients vary depending on the predicting ordinal response. In contrast to conventional approaches estimating the linear coefficients of regression directly from the discrete response, we train a non-linear neural network that outputs the linear coefficients by taking the response as its input. By virtue of the neural network, N$^3$POM may have flexibility while preserving the interpretability of the conventional ordinal regression. We show a sufficient condition so that the predicted conditional cumulative probability~(CCP) satisfies the monotonicity constraint locally over a user-specified region in the covariate space; we also provide a monotonicity-preserving stochastic (MPS) algorithm for training the neural network adequately.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2303.17765</link><description>&lt;p&gt;
&#23398;&#20064;&#30456;&#20284;&#30340;&#32447;&#24615;&#34920;&#31034;&#65306;&#36866;&#24212;&#24615;&#12289;&#26497;&#23567;&#21270;&#12289;&#20197;&#21450;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning from Similar Linear Representations: Adaptivity, Minimaxity, and Robustness. (arXiv:2303.17765v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#28982;&#32780;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#27424;&#32570;&#12290;&#26412;&#25991;&#26088;&#22312;&#29702;&#35299;&#20174;&#20855;&#26377;&#30456;&#20284;&#20294;&#24182;&#38750;&#23436;&#20840;&#30456;&#21516;&#30340;&#32447;&#24615;&#34920;&#31034;&#30340;&#20219;&#21153;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#22788;&#29702;&#24322;&#24120;&#20540;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21333;&#20219;&#21153;&#25110;&#20165;&#30446;&#26631;&#23398;&#20064;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation multi-task learning (MTL) and transfer learning (TL) have achieved tremendous success in practice. However, the theoretical understanding of these methods is still lacking. Most existing theoretical works focus on cases where all tasks share the same representation, and claim that MTL and TL almost always improve performance. However, as the number of tasks grow, assuming all tasks share the same representation is unrealistic. Also, this does not always match empirical findings, which suggest that a shared representation may not necessarily improve single-task or target-only learning performance. In this paper, we aim to understand how to learn from tasks with \textit{similar but not exactly the same} linear representations, while dealing with outlier tasks. We propose two algorithms that are \textit{adaptive} to the similarity structure and \textit{robust} to outlier tasks under both MTL and TL settings. Our algorithms outperform single-task or target-only learning when
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20204;&#21457;&#29616;&#23545;&#20110;&#20256;&#32479;&#30340;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#23384;&#22312;&#19968;&#20010;&#21487;&#20197;&#20351;&#29992;&#20984;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#35299;&#20915;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#35774;&#35745;&#26356;&#22797;&#26434;&#30340;&#31995;&#32479;&#21644;&#26356;&#26131;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17745</link><description>&lt;p&gt;
L2&#33539;&#25968;&#19979;&#30340;&#38750;&#32447;&#24615;&#22238;&#24402;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
A Note On Nonlinear Regression Under L2 Loss. (arXiv:2303.17745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17745
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20204;&#21457;&#29616;&#23545;&#20110;&#20256;&#32479;&#30340;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#23384;&#22312;&#19968;&#20010;&#21487;&#20197;&#20351;&#29992;&#20984;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#35299;&#20915;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#35774;&#35745;&#26356;&#22797;&#26434;&#30340;&#31995;&#32479;&#21644;&#26356;&#26131;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;L2&#33539;&#25968;&#65288;&#24179;&#26041;&#25439;&#22833;&#65289;&#20989;&#25968;&#19979;&#30340;&#38750;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#36890;&#24120;&#23548;&#33268;&#21442;&#25968;&#38598;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#38024;&#23545;&#20256;&#32479;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#30340;&#20984;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#35774;&#35745;&#26356;&#22797;&#26434;&#30340;&#31995;&#32479;&#21644;&#26356;&#26131;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the nonlinear regression problem under L2 loss (square loss) functions. Traditional nonlinear regression models often result in non-convex optimization problems with respect to the parameter set. We show that a convex nonlinear regression model exists for the traditional least squares problem, which can be a promising towards designing more complex systems with easier to train models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21517;&#31216;&#20026;&#946;4-IRT&#30340;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#26469;&#22686;&#24378;&#21028;&#21035;&#20272;&#35745;&#65292;&#35299;&#20915;&#20102;&#946;3-IRT&#30340;&#23545;&#31216;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17731</link><description>&lt;p&gt;
&#946;4-IRT&#65306;&#19968;&#31181;&#20855;&#26377;&#22686;&#24378;&#21028;&#21035;&#21147;&#20272;&#35745;&#30340;&#26032;&#946;3-IRT&#12290;
&lt;/p&gt;
&lt;p&gt;
$\beta^{4}$-IRT: A New $\beta^{3}$-IRT with Enhanced Discrimination Estimation. (arXiv:2303.17731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21517;&#31216;&#20026;&#946;4-IRT&#30340;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#26469;&#22686;&#24378;&#21028;&#21035;&#20272;&#35745;&#65292;&#35299;&#20915;&#20102;&#946;3-IRT&#30340;&#23545;&#31216;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#26088;&#22312;&#20174;&#19981;&#21516;&#38590;&#24230;&#31561;&#32423;&#30340;&#39033;&#30446;&#20013;&#25512;&#26029;&#20986;&#21463;&#35797;&#32773;&#26410;&#35266;&#23519;&#21040;&#30340;&#33021;&#21147;&#21644;&#29305;&#24449;&#12290;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#22914;&#20108;&#36827;&#21046;&#25110;&#27010;&#29575;&#21709;&#24212;&#12289;&#21453;&#24212;&#26102;&#38388;&#12289;&#22810;&#37325;&#21709;&#24212;&#31561;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#29256;&#26412;&#30340;&#946;3-IRT&#65292;&#31216;&#20026;&#946;4-IRT&#65292;&#35813;&#29256;&#26412;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#12290;&#22312;&#946;3-IRT&#20013;&#65292;&#33021;&#21147;&#21644;&#38590;&#24230;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#27492;&#25105;&#20204;&#37319;&#29992;&#38142;&#25509;&#20989;&#25968;&#23558;&#946;4-IRT&#36716;&#21464;&#20026;&#26080;&#32422;&#26463;&#30340;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#12290;&#21407;&#22987;&#30340;&#946;3-IRT&#23384;&#22312;&#23545;&#31216;&#38382;&#39064;&#65292;&#21363;&#22914;&#26524;&#26576;&#20010;&#39033;&#30446;&#30340;&#21028;&#21035;&#21147;&#20540;&#31526;&#21495;&#38169;&#35823;&#65288;&#20363;&#22914;&#36127;&#20540;&#65292;&#32780;&#23454;&#38469;&#19978;&#26159;&#27491;&#30340;&#65289;&#65292;&#25311;&#21512;&#36807;&#31243;&#21487;&#33021;&#26080;&#27861;&#24674;&#22797;&#35813;&#39033;&#30446;&#30340;&#27491;&#30830;&#21028;&#21035;&#21147;&#21644;&#38590;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Item response theory aims to estimate respondent's latent skills from their responses in tests composed of items with different levels of difficulty. Several models of item response theory have been proposed for different types of tasks, such as binary or probabilistic responses, response time, multiple responses, among others. In this paper, we propose a new version of $\beta^3$-IRT, called $\beta^{4}$-IRT, which uses the gradient descent method to estimate the model parameters. In $\beta^3$-IRT, abilities and difficulties are bounded, thus we employ link functions in order to turn $\beta^{4}$-IRT into an unconstrained gradient descent process. The original $\beta^3$-IRT had a symmetry problem, meaning that, if an item was initialised with a discrimination value with the wrong sign, e.g. negative when the actual discrimination should be positive, the fitting process could be unable to recover the correct discrimination and difficulty values for the item. In order to tackle this limita
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;Multiclass Littlestone&#32500;&#24230;&#21487;&#20197;&#21051;&#30011;&#26631;&#31614;&#25968;&#30446;&#20026;&#26080;&#30028;&#24773;&#20917;&#19979;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17716</link><description>&lt;p&gt;
&#22312;&#32447;&#22810;&#31867;&#21487;&#23398;&#20064;&#24615;&#30340;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Characterization of Online Multiclass Learnability. (arXiv:2303.17716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17716
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;Multiclass Littlestone&#32500;&#24230;&#21487;&#20197;&#21051;&#30011;&#26631;&#31614;&#25968;&#30446;&#20026;&#26080;&#30028;&#24773;&#20917;&#19979;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#24403;&#26631;&#31614;&#25968;&#30446;&#26159;&#26080;&#30028;&#30340;&#26102;&#20505;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Multiclass Littlestone&#32500;&#24230;&#65292;&#36825;&#20010;&#27010;&#24565;&#39318;&#27425;&#20986;&#29616;&#22312;\cite{DanielyERMprinciple}&#20013;&#65292;&#32487;&#32493;&#21051;&#30011;&#20102;&#35813;&#22330;&#26223;&#19979;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34917;&#20805;&#20102;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;\cite{Brukhimetal2022}&#32473;&#20986;&#20102;&#24403;&#26631;&#31614;&#31354;&#38388;&#26159;&#26080;&#30028;&#30340;&#24773;&#20917;&#19979;&#25209;&#22788;&#29702;&#22810;&#31867;&#21487;&#23398;&#20064;&#24615;&#30340;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of online multiclass learning when the number of labels is unbounded. We show that the Multiclass Littlestone dimension, first introduced in \cite{DanielyERMprinciple}, continues to characterize online learnability in this setting. Our result complements the recent work by \cite{Brukhimetal2022} who give a characterization of batch multiclass learnability when the label space is unbounded.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;ADMM&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#24102;&#26377;$(0,1)$-&#25439;&#22833;&#20989;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#24179;&#38754;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.04445</link><description>&lt;p&gt;
&#19968;&#20010;ADMM&#27714;&#35299;MKL-$L_{0/1}$-SVM&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An ADMM Solver for the MKL-$L_{0/1}$-SVM. (arXiv:2303.04445v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;ADMM&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#24102;&#26377;$(0,1)$-&#25439;&#22833;&#20989;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#24179;&#38754;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#24102;&#26377;&#33261;&#21517;&#26157;&#33879;&#30340;$(0,1)$-&#25439;&#22833;&#20989;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#65292;&#21046;&#23450;&#20102;&#22810;&#26680;&#23398;&#20064;(MKL)&#38382;&#39064;&#12290;&#32473;&#20986;&#20102;&#19968;&#20123;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#26465;&#20214;&#26469;&#24320;&#21457;&#19968;&#20010;&#24555;&#36895;&#30340;ADMM&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;MKL-$L_{0/1}$-SVM&#26694;&#26550;&#20855;&#26377;&#24456;&#22909;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
We formulate the Multiple Kernel Learning (abbreviated as MKL) problem for the support vector machine with the infamous $(0,1)$-loss function. Some first-order optimality conditions are given and then exploited to develop a fast ADMM solver for the nonconvex and nonsmooth optimization problem. A simple numerical experiment on synthetic planar data shows that our MKL-$L_{0/1}$-SVM framework could be promising.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.00848</link><description>&lt;p&gt;
&#20197;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#37117;&#26159;&#21152;&#26435;&#25439;&#22833;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#21152;&#26435;&#20989;&#25968;&#25351;&#23450;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#30340;&#26435;&#37325;&#12290;&#22343;&#21248;&#21152;&#26435;&#23545;&#24212;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#21407;&#21017;&#24615;&#36817;&#20284;ELBO&#30340;&#26368;&#22823;&#21270;&#12290;&#20294;&#26159;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#30446;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#38750;&#22343;&#21248;&#21152;&#26435;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#65288;&#24102;&#26377;&#20219;&#20309;&#21152;&#26435;&#65289;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21487;&#20197;&#34987;&#20889;&#25104;&#19968;&#31181;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#24418;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#37117;&#26377;&#19968;&#20010;ELBO&#12290;&#22914;&#26524;&#26435;&#37325;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#37027;&#20040;&#21152;&#26435;&#25439;&#22833;&#26159;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#30446;&#26631;&#65306;&#23427;&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#19979;&#65288;&#21363;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#65289;&#19979;&#26368;&#22823;&#21270;ELBO&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#20294;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#27604;&#36739;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#26435;&#37325;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#23548;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#22266;&#23450;&#26679;&#26412;&#22823;&#23567;&#26041;&#24335;&#65292;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#25152;&#26377;&#20572;&#27490;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#36793;&#30028;&#26041;&#27861;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#24179;&#31283;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2302.03421</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#25512;&#23548;&#65288;&#26102;&#38388;&#22343;&#21248;&#30340;&#65289;PAC-Bayes&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
A unified recipe for deriving (time-uniform) PAC-Bayes bounds. (arXiv:2302.03421v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03421
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#23548;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#22266;&#23450;&#26679;&#26412;&#22823;&#23567;&#26041;&#24335;&#65292;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#25152;&#26377;&#20572;&#27490;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#36793;&#30028;&#26041;&#27861;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#24179;&#31283;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#23548;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#12290;&#19982;&#22823;&#22810;&#25968;&#20851;&#20110;&#27492;&#20027;&#39064;&#30340;&#25991;&#29486;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#20219;&#20309;&#26102;&#38388;&#37117;&#26377;&#25928;&#30340;&#65288;&#21363;&#26102;&#38388;&#22343;&#21248;&#30340;&#65289;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#36866;&#29992;&#20110;&#25152;&#26377;&#20572;&#27490;&#26102;&#38388;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22266;&#23450;&#30340;&#26679;&#26412;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25353;&#29031;&#20197;&#19979;&#39034;&#24207;&#32467;&#21512;&#20102;&#22235;&#31181;&#24037;&#20855;&#65306;&#65288;a&#65289;&#38750;&#36127;&#36229;&#39532;&#19969;&#26684;&#23572;&#25110;&#21453;&#21521;&#20122;&#39532;&#36874;&#65292;&#65288;b&#65289;&#28151;&#21512;&#27861;&#65292;&#65288;c&#65289;Donsker-Varadhan&#20844;&#24335;&#65288;&#25110;&#20854;&#23427;&#20984;&#24615;&#23545;&#20598;&#21407;&#29702;&#65289;&#21644;&#65288;d&#65289;Ville&#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25104;&#26524;&#26159;&#19968;&#20010;PAC-Bayes&#23450;&#29702;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31163;&#25955;&#38543;&#26426;&#36807;&#31243;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#32467;&#26524;&#22914;&#20309;&#25512;&#20986;&#30693;&#21517;&#30340;&#32463;&#20856;PAC-Bayes&#30028;&#38480;&#65292;&#20363;&#22914;Seeger&#12289;McAllester&#12289;Maurer&#21644;Catoni&#30340;&#30028;&#38480;&#65292;&#20197;&#21450;&#35768;&#22810;&#26368;&#26032;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#25918;&#26494;&#20256;&#32479;&#30340;&#20551;&#35774;&#65307;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#38750;&#24179;&#31283;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a unified framework for deriving PAC-Bayesian generalization bounds. Unlike most previous literature on this topic, our bounds are anytime-valid (i.e., time-uniform), meaning that they hold at all stopping times, not only for a fixed sample size. Our approach combines four tools in the following order: (a) nonnegative supermartingales or reverse submartingales, (b) the method of mixtures, (c) the Donsker-Varadhan formula (or other convex duality principles), and (d) Ville's inequality. Our main result is a PAC-Bayes theorem which holds for a wide class of discrete stochastic processes. We show how this result implies time-uniform versions of well-known classical PAC-Bayes bounds, such as those of Seeger, McAllester, Maurer, and Catoni, in addition to many recent bounds. We also present several novel bounds. Our framework also enables us to relax traditional assumptions; in particular, we consider nonstationary loss functions and non-i.i.d. data. In sum, we unify the derivati
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#35777;&#26126;&#23545;&#20110;&#22343;&#26041;&#35823;&#24046;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20986;&#29616;&#30340;&#20840;&#23616;&#35299;&#22312;&#19981;&#21516;&#25968;&#25454;&#19978;&#37117;&#20855;&#26377;&#31070;&#32463;&#22604;&#38519;&#30340;&#29305;&#24615;&#65292;&#21363;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20250;&#23849;&#28291;&#20026;&#31867;&#22343;&#20540;&#65292;&#32780;&#36825;&#20123;&#31867;&#22343;&#20540;&#26159;&#31561;&#35282;&#32039;&#26694;&#26550;&#30340;&#39030;&#28857;&#12290;</title><link>http://arxiv.org/abs/2301.00437</link><description>&lt;p&gt;
&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#22604;&#38519;:&#20174;&#24179;&#34913;&#21040;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data. (arXiv:2301.00437v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00437
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#35777;&#26126;&#23545;&#20110;&#22343;&#26041;&#35823;&#24046;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20986;&#29616;&#30340;&#20840;&#23616;&#35299;&#22312;&#19981;&#21516;&#25968;&#25454;&#19978;&#37117;&#20855;&#26377;&#31070;&#32463;&#22604;&#38519;&#30340;&#29305;&#24615;&#65292;&#21363;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20250;&#23849;&#28291;&#20026;&#31867;&#22343;&#20540;&#65292;&#32780;&#36825;&#20123;&#31867;&#22343;&#20540;&#26159;&#31561;&#35282;&#32039;&#26694;&#26550;&#30340;&#39030;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#22797;&#26434;&#31995;&#32479;&#22312;&#35757;&#32451;&#21040;&#25910;&#25947;&#26102;&#65292;&#23427;&#20204;&#30340;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#21644;&#20998;&#31867;&#22120;&#22312;&#32463;&#20856;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#30456;&#21516;&#30340;&#32467;&#26500;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#35266;&#23519;&#21040;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20250;&#23849;&#28291;&#20026;&#31867;&#22343;&#20540;&#65292;&#24182;&#19988;&#36825;&#20123;&#31867;&#22343;&#20540;&#26159;&#31561;&#35282;&#32039;&#26694;&#26550;(simplex Equiangular Tight Frame)&#30340;&#39030;&#28857;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#31070;&#32463;&#22604;&#38519;(NC)&#12290;&#26368;&#36817;&#30340;&#35770;&#25991;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#31616;&#21270;&#30340;&#8220;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#8221;&#35757;&#32451;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#20013;&#20986;&#29616;&#20102;$\mathcal{NC}$&#12290;&#22312;&#36825;&#20010;&#35821;&#22659;&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#22312;&#24120;&#29992;&#30340;&#22343;&#26041;&#35823;&#24046;(MSE)&#21644;&#20132;&#21449;&#29109;(CE)&#25439;&#22833;&#19979;&#65292;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20063;&#20250;&#21457;&#29983;$\mathcal{NC}$&#29616;&#35937;&#65292;&#34920;&#26126;&#20840;&#23616;&#35299;&#22312;&#19981;&#21516;&#25968;&#25454;&#19978;&#37117;&#20855;&#26377;$\mathcal{NC}$&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep neural networks have achieved impressive performance on tasks from image classification to natural language processing. Surprisingly, these complex systems with massive amounts of parameters exhibit the same structural properties in their last-layer features and classifiers across canonical datasets when training until convergence. In particular, it has been observed that the last-layer features collapse to their class-means, and those class-means are the vertices of a simplex Equiangular Tight Frame (ETF). This phenomenon is known as Neural Collapse ($\mathcal{NC}$). Recent papers have theoretically shown that $\mathcal{NC}$ emerges in the global minimizers of training problems with the simplified ``unconstrained feature model''. In this context, we take a step further and prove the $\mathcal{NC}$ occurrences in deep linear networks for the popular mean squared error (MSE) and cross entropy (CE) losses, showing that global solutions exhibit $\mathcal{NC}$ properties across
&lt;/p&gt;</description></item><item><title>TAP-Vid&#26159;&#19968;&#20010;&#36319;&#36394;&#20219;&#20309;&#28857;&#22312;&#35270;&#39057;&#20013;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#30495;&#23454;&#19990;&#30028;&#35270;&#39057;&#21644;&#21512;&#25104;&#35270;&#39057;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#35299;&#20915;&#36319;&#36394;&#20219;&#24847;&#29289;&#29702;&#28857;&#22312;&#35270;&#39057;&#20013;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.03726</link><description>&lt;p&gt;
TAP-Vid&#65306;&#22312;&#35270;&#39057;&#20013;&#36319;&#36394;&#20219;&#20309;&#28857;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TAP-Vid: A Benchmark for Tracking Any Point in a Video. (arXiv:2211.03726v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03726
&lt;/p&gt;
&lt;p&gt;
TAP-Vid&#26159;&#19968;&#20010;&#36319;&#36394;&#20219;&#20309;&#28857;&#22312;&#35270;&#39057;&#20013;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#30495;&#23454;&#19990;&#30028;&#35270;&#39057;&#21644;&#21512;&#25104;&#35270;&#39057;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#35299;&#20915;&#36319;&#36394;&#20219;&#24847;&#29289;&#29702;&#28857;&#22312;&#35270;&#39057;&#20013;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#39057;&#20013;&#33719;&#21462;&#26222;&#36866;&#30340;&#36816;&#21160;&#29702;&#35299;&#19981;&#20165;&#28041;&#21450;&#36861;&#36394;&#29289;&#20307;&#65292;&#36824;&#38656;&#35201;&#24863;&#30693;&#23427;&#20204;&#30340;&#34920;&#38754;&#21464;&#24418;&#21644;&#36816;&#21160;&#12290;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#25512;&#26029; 3D &#24418;&#29366;&#12289;&#29289;&#29702;&#23646;&#24615;&#21644;&#29289;&#20307;&#20132;&#20114;&#38750;&#24120;&#26377;&#29992;&#12290;&#34429;&#28982;&#22312;&#36739;&#38271;&#30340;&#35270;&#39057;&#29255;&#27573;&#20013;&#36861;&#36394;&#20219;&#24847;&#29289;&#29702;&#28857;&#30340;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#19968;&#20123;&#20851;&#27880;&#65292;&#20294;&#30452;&#21040;&#29616;&#22312;&#36824;&#27809;&#26377;&#21487;&#29992;&#20110;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#25110;&#22522;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#65292;&#24182;&#23558;&#20854;&#21629;&#21517;&#20026;&#36319;&#36394;&#20219;&#24847;&#28857; (TAP)&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20276;&#38543;&#25968;&#25454;&#38598; TAP-Vid&#65292;&#23427;&#30001;&#20855;&#26377;&#20934;&#30830;&#20154;&#24037;&#26631;&#27880;&#30340;&#28857;&#36857;&#30340;&#30495;&#23454;&#19990;&#30028;&#35270;&#39057;&#21644;&#20855;&#26377;&#23436;&#32654;&#22320;&#38754;&#23454;&#20917;&#28857;&#36857;&#30340;&#21512;&#25104;&#35270;&#39057;&#32452;&#25104;&#12290;&#25105;&#20204;&#26500;&#24314;&#22522;&#20934;&#30340;&#20851;&#38190;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21322;&#33258;&#21160;&#20247;&#21253;&#27969;&#27700;&#32447;&#65292;&#23427;&#20351;&#29992;&#20809;&#27969;&#20272;&#35745;&#26469;&#24357;&#34917;&#25668;&#20687;&#26426;&#25238;&#21160;&#31561;&#31616;&#21333;&#30701;&#26399;&#36816;&#21160;&#65292;&#35753;&#27880;&#37322;&#32773;&#19987;&#27880;&#20110;&#35270;&#39057;&#30340;&#26356;&#38590;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#26469;&#34913;&#37327;&#22312; TAP-Vid &#19978;&#30340;&#36319;&#36394;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;&#20855;&#26377;&#38750;&#21018;&#24615;&#36816;&#21160;&#21644;&#36974;&#25377;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24207;&#21015;&#65292;&#20197;&#21450;&#24191;&#27867;&#30340;&#29289;&#20307;&#31867;&#21035;&#21644;&#25668;&#20687;&#26426;&#36816;&#21160;&#12290;&#25105;&#20204;&#24076;&#26395; TAP-Vid &#33021;&#22815;&#40723;&#21169;&#30740;&#31350;&#36825;&#20010;&#37325;&#35201;&#32780;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#25512;&#21160;&#26356;&#22909;&#30340;&#31639;&#27861;&#26469;&#36319;&#36394;&#35270;&#39057;&#20013;&#20219;&#24847;&#29289;&#29702;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generic motion understanding from video involves not only tracking objects, but also perceiving how their surfaces deform and move. This information is useful to make inferences about 3D shape, physical properties and object interactions. While the problem of tracking arbitrary physical points on surfaces over longer video clips has received some attention, no dataset or benchmark for evaluation existed, until now. In this paper, we first formalize the problem, naming it tracking any point (TAP). We introduce a companion benchmark, TAP-Vid, which is composed of both real-world videos with accurate human annotations of point tracks, and synthetic videos with perfect ground-truth point tracks. Central to the construction of our benchmark is a novel semi-automatic crowdsourced pipeline which uses optical flow estimates to compensate for easier, short-term motion like camera shake, allowing annotators to focus on harder sections of video. We validate our pipeline on synthetic data and prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#26041;&#27861;&#30340;&#19977;&#27493;&#23398;&#20064;&#21644;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#36817;&#20284;&#35299;&#26032;&#30340;PDE&#65292;&#24182;&#23637;&#31034;&#20102;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#20248;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.08140</link><description>&lt;p&gt;
&#29992;&#26680;&#26041;&#27861;&#25506;&#32034;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Kernel Approach for PDE Discovery and Operator Learning. (arXiv:2210.08140v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#26041;&#27861;&#30340;&#19977;&#27493;&#23398;&#20064;&#21644;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#36817;&#20284;&#35299;&#26032;&#30340;PDE&#65292;&#24182;&#23637;&#31034;&#20102;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#20248;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#26041;&#27861;&#23398;&#20064;&#21644;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#19977;&#27493;&#26694;&#26550;&#12290;&#32473;&#23450;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#21253;&#25324;&#32593;&#26684;&#19978;&#30340;&#22122;&#22768;PDE&#35299;&#20197;&#21450;&#28304;&#39033;/&#36793;&#30028;&#39033;&#30340;&#23545;&#65292;&#21033;&#29992;&#26680;&#24179;&#28369;&#25216;&#26415;&#21435;&#22122;&#24182;&#36817;&#20284;&#35299;&#30340;&#23548;&#25968;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#22312;&#26680;&#22238;&#24402;&#27169;&#22411;&#20013;&#23398;&#20064;PDE&#30340;&#20195;&#25968;&#24418;&#24335;&#12290;&#23398;&#20064;&#24471;&#21040;&#30340;PDE&#22312;&#22522;&#20110;&#26680;&#30340;&#27714;&#35299;&#22120;&#20013;&#34987;&#29992;&#26469;&#36817;&#20284;&#35299;&#26032;&#30340;&#28304;&#39033;/&#36793;&#30028;&#39033;&#30340;PDE&#65292;&#20174;&#32780;&#26500;&#25104;&#20102;&#19968;&#20010;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#12290;&#25968;&#20540;&#23454;&#39564;&#23558;&#35813;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a three-step framework for learning and solving partial differential equations (PDEs) using kernel methods. Given a training set consisting of pairs of noisy PDE solutions and source/boundary terms on a mesh, kernel smoothing is utilized to denoise the data and approximate derivatives of the solution. This information is then used in a kernel regression model to learn the algebraic form of the PDE. The learned PDE is then used within a kernel based solver to approximate the solution of the PDE with a new source/boundary term, thereby constituting an operator learning framework. Numerical experiments compare the method to state-of-the-art algorithms and demonstrate its competitive performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;BSSMF&#65292;&#23427;&#30340;&#30697;&#38453;W&#27599;&#21015;&#30340;&#20803;&#32032;&#23646;&#20110;&#32473;&#23450;&#30340;&#21306;&#38388;&#65292;&#32780;H&#30340;&#21015;&#26159;&#38543;&#26426;&#30340;&#65292;&#25512;&#24191;&#20102;NMF&#21644;SSMF&#65292;&#36866;&#29992;&#20110;&#30697;&#38453;&#20803;&#32032;&#23646;&#20110;&#32473;&#23450;&#21306;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26131;&#20110;&#29702;&#35299;&#30340;&#20998;&#35299;&#21644;&#31163;&#25955;&#32467;&#26500;&#65292;&#36866;&#29992;&#20110;&#20027;&#39064;&#24314;&#27169;&#21644;&#31038;&#21306;&#26816;&#27979;&#31561;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.12638</link><description>&lt;p&gt;
&#26377;&#30028;&#21333;&#32431;&#24418;&#32467;&#26500;&#30697;&#38453;&#20998;&#35299;&#65306;&#31639;&#27861;&#12289;&#21487;&#35782;&#21035;&#24615;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bounded Simplex-Structured Matrix Factorization: Algorithms, Identifiability and Applications. (arXiv:2209.12638v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12638
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;BSSMF&#65292;&#23427;&#30340;&#30697;&#38453;W&#27599;&#21015;&#30340;&#20803;&#32032;&#23646;&#20110;&#32473;&#23450;&#30340;&#21306;&#38388;&#65292;&#32780;H&#30340;&#21015;&#26159;&#38543;&#26426;&#30340;&#65292;&#25512;&#24191;&#20102;NMF&#21644;SSMF&#65292;&#36866;&#29992;&#20110;&#30697;&#38453;&#20803;&#32032;&#23646;&#20110;&#32473;&#23450;&#21306;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26131;&#20110;&#29702;&#35299;&#30340;&#20998;&#35299;&#21644;&#31163;&#25955;&#32467;&#26500;&#65292;&#36866;&#29992;&#20110;&#20027;&#39064;&#24314;&#27169;&#21644;&#31038;&#21306;&#26816;&#27979;&#31561;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#30028;&#21333;&#32431;&#24418;&#32467;&#26500;&#30697;&#38453;&#20998;&#35299;&#65288;BSSMF&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#30697;&#38453;X&#21644;&#19968;&#20010;&#20998;&#35299;&#31209;r&#65292;BSSMF&#22312;&#30697;&#38453;W&#20013;&#23547;&#25214;&#20855;&#26377;r&#21015;&#30340;&#30697;&#38453;&#21644;&#22312;&#30697;&#38453;H&#20013;&#23547;&#25214;&#20855;&#26377;r&#34892;&#30340;&#30697;&#38453;&#65292;&#20351;&#24471;X&#8776;WH &#65292;&#20854;&#20013;W&#30340;&#27599;&#21015;&#20013;&#30340;&#20803;&#32032;&#37117;&#26159;&#26377;&#30028;&#30340;&#65292;&#21363;&#23427;&#20204;&#23646;&#20110;&#32473;&#23450;&#30340;&#21306;&#38388;&#65292;&#32780;H&#30340;&#21015;&#23646;&#20110;&#27010;&#29575;&#21333;&#32431;&#24418;&#65292;&#21363;H&#26159;&#21015;&#38543;&#26426;&#30340;&#12290;BSSMF&#25512;&#24191;&#20102;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#21644;&#21333;&#32431;&#24418;&#32467;&#26500;&#30697;&#38453;&#20998;&#35299;&#65288;SSMF&#65289;&#12290;BSSMF&#29305;&#21035;&#36866;&#29992;&#20110;&#36755;&#20837;&#30697;&#38453;X&#30340;&#20803;&#32032;&#23646;&#20110;&#32473;&#23450;&#21306;&#38388;&#30340;&#24773;&#20917;&#65307;&#20363;&#22914;&#65292;&#24403;X&#30340;&#34892;&#34920;&#31034;&#22270;&#20687;&#26102;&#65292;&#25110;&#32773;X&#26159;&#31867;&#20284;Netflix&#21644;MovieLens&#25968;&#25454;&#38598;&#20013;&#30340;&#35780;&#20998;&#30697;&#38453;&#26102;&#65292;&#20854;&#20013;X&#30340;&#20803;&#32032;&#23646;&#20110;&#21306;&#38388;[1,5]&#12290;&#21333;&#32431;&#24418;&#32467;&#26500;&#30697;&#38453;H&#19981;&#20165;&#21487;&#20197;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#20998;&#35299;&#65292;&#20174;&#32780;&#23545;X&#30340;&#21015;&#31354;&#38388;&#36827;&#34892;&#36719;&#32858;&#31867;&#65292;&#32780;&#19988;&#36824;&#36171;&#20104;H&#30340;&#21015;&#31163;&#25955;&#32467;&#26500;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#29992;&#20110;&#22914;&#20027;&#39064;&#24314;&#27169;&#21644;&#31038;&#21306;&#26816;&#27979;&#31561;&#24212;&#29992;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26377;&#25928;&#30340;BSSMF&#20248;&#21270;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;&#20854;&#21487;&#35782;&#21035;&#24615;&#20445;&#35777;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;BSSMF&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new low-rank matrix factorization model dubbed bounded simplex-structured matrix factorization (BSSMF). Given an input matrix $X$ and a factorization rank $r$, BSSMF looks for a matrix $W$ with $r$ columns and a matrix $H$ with $r$ rows such that $X \approx WH$ where the entries in each column of $W$ are bounded, that is, they belong to given intervals, and the columns of $H$ belong to the probability simplex, that is, $H$ is column stochastic. BSSMF generalizes nonnegative matrix factorization (NMF), and simplex-structured matrix factorization (SSMF). BSSMF is particularly well suited when the entries of the input matrix $X$ belong to a given interval; for example when the rows of $X$ represent images, or $X$ is a rating matrix such as in the Netflix and MovieLens datasets where the entries of $X$ belong to the interval $[1,5]$. The simplex-structured matrix $H$ not only leads to an easily understandable decomposition providing a soft clustering of the colu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#25968;&#25454;&#25512;&#26029;&#20013;&#30340;&#21442;&#25968;&#24230;&#37327;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;Jeffreys&#20808;&#39564;&#20250;&#22312;&#20856;&#22411;&#31185;&#23398;&#27169;&#22411;&#20013;&#24341;&#20837;&#24040;&#22823;&#20559;&#24046;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#24230;&#37327;&#36873;&#25321;&#20197;&#23454;&#29616;&#23545;&#20110;&#22797;&#26434;&#27169;&#22411;&#30340;&#26080;&#20559;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2205.03343</link><description>&lt;p&gt;
&#36828;&#31163;&#28176;&#36817;&#29702;&#35770;&#65306;&#26377;&#38480;&#25968;&#25454;&#25512;&#26029;&#20013;&#30340;Jeffreys&#20808;&#39564;&#35823;&#29992;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Far from Asymptopia. (arXiv:2205.03343v2 [stat.OT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#25968;&#25454;&#25512;&#26029;&#20013;&#30340;&#21442;&#25968;&#24230;&#37327;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;Jeffreys&#20808;&#39564;&#20250;&#22312;&#20856;&#22411;&#31185;&#23398;&#27169;&#22411;&#20013;&#24341;&#20837;&#24040;&#22823;&#20559;&#24046;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#24230;&#37327;&#36873;&#25321;&#20197;&#23454;&#29616;&#23545;&#20110;&#22797;&#26434;&#27169;&#22411;&#30340;&#26080;&#20559;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#25968;&#25454;&#30340;&#25512;&#26029;&#20013;&#65292;&#38656;&#35201;&#19968;&#20010;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#24230;&#37327;&#27010;&#24565;&#65292;&#26368;&#26126;&#30830;&#30340;&#26159;&#36125;&#21494;&#26031;&#26694;&#26550;&#20013;&#30340;&#20808;&#39564;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#24212;&#29992;&#20110;&#20856;&#22411;&#30340;&#31185;&#23398;&#27169;&#22411;&#26102;&#65292;&#26368;&#20986;&#21517;&#30340;&#38750;&#20449;&#24687;&#36873;&#25321;&#65292;Jeffreys&#20808;&#39564;&#65292;&#24341;&#20837;&#20102;&#24040;&#22823;&#30340;&#20559;&#24046;&#12290;&#36825;&#31867;&#27169;&#22411;&#30340;&#26377;&#25928;&#32500;&#24230;&#26174;&#33879;&#23567;&#20110;&#24494;&#35266;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#22240;&#20026;Jeffreys&#20808;&#39564;&#24179;&#31561;&#22320;&#22788;&#29702;&#25152;&#26377;&#30340;&#24494;&#35266;&#21442;&#25968;&#65292;&#25152;&#20197;&#25237;&#24433;&#21040;&#30456;&#20851;&#21442;&#25968;&#30340;&#23376;&#31354;&#38388;&#19978;&#26102;&#26159;&#22343;&#21248;&#30340;&#65292;&#36825;&#26159;&#30001;&#20110;&#26080;&#20851;&#26041;&#21521;&#30340;&#26412;&#22320;&#20849;&#20307;&#31215;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#37327;&#24230;&#36873;&#25321;&#65292;&#36991;&#20813;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#23548;&#33268;&#26080;&#20559;&#25512;&#26029;&#12290;&#36825;&#20010;&#26368;&#20248;&#20808;&#39564;&#21462;&#20915;&#20110;&#35201;&#25910;&#38598;&#30340;&#25968;&#25454;&#25968;&#37327;&#65292;&#24182;&#19988;&#22312;&#28176;&#36817;&#26497;&#38480;&#19979;&#36235;&#36817;&#20110;Jeffreys&#20808;&#39564;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#27809;&#26377;&#25351;&#25968;&#25968;&#37327;&#32423;&#30340;&#24494;&#35266;&#21442;&#25968;&#65292;&#36825;&#20010;&#26497;&#38480;&#26159;&#26080;&#27861;&#35777;&#26126;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference from limited data requires a notion of measure on parameter space, most explicit in the Bayesian framework as a prior. Here we demonstrate that Jeffreys prior, the best-known uninformative choice, introduces enormous bias when applied to typical scientific models. Such models have a relevant effective dimensionality much smaller than the number of microscopic parameters. Because Jeffreys prior treats all microscopic parameters equally, it is from uniform when projected onto the sub-space of relevant parameters, due to variations in the local co-volume of irrelevant directions. We present results on a principled choice of measure which avoids this issue, leading to unbiased inference in complex models. This optimal prior depends on the quantity of data to be gathered, and approaches Jeffreys prior in the asymptotic limit. However, this limit cannot be justified without an impossibly large amount of data, exponential in the number of microscopic parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35760;&#24518;&#20013;&#32500;&#25252;&#36935;&#21040;&#24207;&#21015;&#27169;&#24335;&#30340;&#25551;&#36848;&#31526;&#26469;&#23454;&#29616;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26032;&#30340;&#34892;&#20026;&#27169;&#24335;&#30340;&#36830;&#32493;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.00936</link><description>&lt;p&gt;
&#24102;&#22806;&#37096;&#35760;&#24518;&#30340;&#22810;&#27169;&#24577;&#21160;&#24577;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning of Multi-modal Dynamics with External Memory. (arXiv:2203.00936v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35760;&#24518;&#20013;&#32500;&#25252;&#36935;&#21040;&#24207;&#21015;&#27169;&#24335;&#30340;&#25551;&#36848;&#31526;&#26469;&#23454;&#29616;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26032;&#30340;&#34892;&#20026;&#27169;&#24335;&#30340;&#36830;&#32493;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26032;&#30340;&#34892;&#20026;&#27169;&#24335;&#36830;&#32493;&#20986;&#29616;&#26102;&#65292;&#22914;&#20309;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#21160;&#24577;&#29615;&#22659;&#20013;&#12290;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#24847;&#35782;&#21040;&#26032;&#30340;&#27169;&#24335;&#20986;&#29616;&#65292;&#20294;&#23427;&#27809;&#26377;&#35775;&#38382;&#21333;&#20010;&#35757;&#32451;&#24207;&#21015;&#30340;&#30495;&#23454;&#27169;&#24335;&#30340;&#20449;&#24687;&#12290;&#30446;&#21069;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#65292;&#22240;&#20026;&#21442;&#25968;&#20256;&#36882;&#21463;&#21040;&#28798;&#38590;&#24615;&#24178;&#25200;&#30340;&#24433;&#21709;&#65292;&#32780;&#24773;&#33410;&#35760;&#24518;&#35774;&#35745;&#38656;&#35201;&#30693;&#36947;&#24207;&#21015;&#30340;&#30495;&#23454;&#27169;&#24335;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#24773;&#33410;&#35760;&#24518;&#20013;&#32500;&#25252;&#36935;&#21040;&#30340;&#24207;&#21015;&#27169;&#24335;&#30340;&#25551;&#36848;&#31526;&#26469;&#20811;&#26381;&#36825;&#20004;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;&#35760;&#24518;&#30340;&#27880;&#24847;&#26435;&#37325;&#19978;&#20351;&#29992;Dirichlet&#36807;&#31243;&#20808;&#39564;&#65292;&#20197;&#20419;&#36827;&#27169;&#24335;&#25551;&#36848;&#31526;&#30340;&#26377;&#25928;&#23384;&#20648;&#12290;&#36890;&#36807;&#26816;&#32034;&#20808;&#21069;&#20219;&#21153;&#30456;&#20284;&#27169;&#24335;&#30340;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#27492;&#25551;&#36848;&#31526;&#39304;&#20837;&#20854;&#36716;&#31227;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#20219;&#21153;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#26469;&#25191;&#34892;&#36830;&#32493;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of fitting a model to a dynamical environment when new modes of behavior emerge sequentially. The learning model is aware when a new mode appears, but it does not have access to the true modes of individual training sequences. The state-of-the-art continual learning approaches cannot handle this setup, because parameter transfer suffers from catastrophic interference and episodic memory design requires the knowledge of the ground-truth modes of sequences. We devise a novel continual learning method that overcomes both limitations by maintaining a descriptor of the mode of an encountered sequence in a neural episodic memory. We employ a Dirichlet Process prior on the attention weights of the memory to foster efficient storage of the mode descriptors. Our method performs continual learning by transferring knowledge across tasks by retrieving the descriptors of similar modes of past tasks to the mode of a current sequence and feeding this descriptor into its transitio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#31995;&#21015;&#65292;&#21487;&#20197;&#26356;&#24555;&#36895;&#22320;&#22312;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2202.01752</link><description>&lt;p&gt;
&#19981;&#23436;&#32654;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#36817;&#20284;&#26368;&#20248;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Learning of Extensive-Form Games with Imperfect Information. (arXiv:2202.01752v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#31995;&#21015;&#65292;&#21487;&#20197;&#26356;&#24555;&#36895;&#22320;&#22312;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#23398;&#20064;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#30340;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;&#35774;&#35745;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#31639;&#27861;&#31995;&#21015;&#65292;&#20165;&#38656;&#35201; $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ &#23616;&#28216;&#25103;&#21363;&#21487;&#22312;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010; $\varepsilon$-&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#65292;&#20854;&#20013; $X,Y$ &#26159;&#20449;&#24687;&#38598;&#30340;&#25968;&#37327;&#65292;$A,B$ &#26159;&#20004;&#21517;&#29609;&#23478;&#30340;&#34892;&#21160;&#25968;&#12290;&#36825;&#27604;&#24050;&#30693;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230; $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ &#26377;&#30528; $\widetilde{\mathcal{O}}(\max\{X, Y\})$ &#30340;&#24040;&#22823;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#23545;&#25968;&#22240;&#23376;&#20869;&#19982;&#20449;&#24687;&#29702;&#35770;&#19979;&#38480;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#26032;&#31639;&#27861;&#23454;&#29616;&#20102;&#36825;&#31181;&#26679;&#26412;&#22797;&#26434;&#24230;&#65306;&#24179;&#34913;&#22312;&#32447;&#38236;&#38754;&#19979;&#38477;&#21644;&#24179;&#34913;&#21453;&#20107;&#23454;&#21518;&#24724;&#26368;&#23567;&#21270;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#20381;&#36182;&#20110;&#23558;&#8220;&#24179;&#34913;&#25506;&#32034;&#31574;&#30053;&#8221;&#38598;&#25104;&#21040;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#25163;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#26356;&#24191;&#27867;&#30340;&#25903;&#25345;&#19981;&#23436;&#32654;&#20449;&#24687;&#21338;&#24328;&#30340;&#20108;&#20154;&#21338;&#24328;&#21644;&#22810;&#20154;&#21338;&#24328;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper resolves the open question of designing near-optimal algorithms for learning imperfect-information extensive-form games from bandit feedback. We present the first line of algorithms that require only $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ episodes of play to find an $\varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where $X,Y$ are the number of information sets and $A,B$ are the number of actions for the two players. This improves upon the best known sample complexity of $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ by a factor of $\widetilde{\mathcal{O}}(\max\{X, Y\})$, and matches the information-theoretic lower bound up to logarithmic factors. We achieve this sample complexity by two new algorithms: Balanced Online Mirror Descent, and Balanced Counterfactual Regret Minimization. Both algorithms rely on novel approaches of integrating \emph{balanced exploration policies} into their classical counterparts. We also extend our results t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#22686;&#24378;&#30340;&#26641;&#38598;&#21512;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#19981;&#35268;&#21017;&#39044;&#27979;&#21464;&#37327;&#65292;&#20026;&#22788;&#29702;&#23439;&#35266;&#37329;&#34701;&#38382;&#39064;&#25552;&#20379;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2111.14000</link><description>&lt;p&gt;
&#22240;&#23376;&#22686;&#24378;&#30340;&#26641;&#38598;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Factor-augmented tree ensembles. (arXiv:2111.14000v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.14000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#22686;&#24378;&#30340;&#26641;&#38598;&#21512;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#19981;&#35268;&#21017;&#39044;&#27979;&#21464;&#37327;&#65292;&#20026;&#22788;&#29702;&#23439;&#35266;&#37329;&#34701;&#38382;&#39064;&#25552;&#20379;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#26041;&#27861;&#25552;&#21462;&#28508;&#22312;&#31283;&#24577;&#22240;&#23376;&#26469;&#25193;&#23637;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#26641;&#20449;&#24687;&#38598;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#35813;&#26041;&#27861;&#23558;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#26641;&#30340;&#24212;&#29992;&#25193;&#23637;&#21040;&#20004;&#20010;&#26041;&#38754;&#12290;&#31532;&#19968;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#27979;&#37327;&#35823;&#24046;&#12289;&#38750;&#24179;&#31283;&#36235;&#21183;&#12289;&#23395;&#33410;&#24615;&#21644;/&#25110;&#32570;&#22833;&#35266;&#27979;&#31561;&#19981;&#35268;&#21017;&#30340;&#39044;&#27979;&#21464;&#37327;&#12290;&#31532;&#20108;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26126;&#30830;&#30340;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#29702;&#35770;&#26469;&#25351;&#23548;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#26641;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#22240;&#23376;&#22686;&#24378;&#30340;&#26641;&#38598;&#21512;&#26041;&#27861;&#22312;&#23439;&#35266;&#37329;&#34701;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#32654;&#22269;&#32929;&#31080;&#27874;&#21160;&#29575;&#19982;&#21830;&#19994;&#21608;&#26399;&#20043;&#38388;&#30340;&#20808;&#23548;&#28382;&#21518;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
This manuscript proposes to extend the information set of time-series regression trees with latent stationary factors extracted via state-space methods. In doing so, this approach generalises time-series regression trees on two dimensions. First, it allows to handle predictors that exhibit measurement error, non-stationary trends, seasonality and/or irregularities such as missing observations. Second, it gives a transparent way for using domain-specific theory to inform time-series regression trees. Empirically, ensembles of these factor-augmented trees provide a reliable approach for macro-finance problems. This article highlights it focussing on the lead-lag effect between equity volatility and the business cycle in the United States.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22270;&#20687;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#32463;&#36807;&#35757;&#32451;&#30340;GAN&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#39640;&#38454;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#20877;&#29616;&#33021;&#21147;&#65292;&#24182;&#39564;&#35777;&#20102;&#22810;&#20010;&#23458;&#35266;&#27979;&#35797;&#20197;&#35780;&#20272;&#19981;&#21516;GAN&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2111.12577</link><description>&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;&#39640;&#38454;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#20877;&#29616;&#24615;&#35780;&#20272;&#22270;&#20687;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Method for Evaluating Deep Generative Models of Images via Assessing the Reproduction of High-order Spatial Context. (arXiv:2111.12577v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.12577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22270;&#20687;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#32463;&#36807;&#35757;&#32451;&#30340;GAN&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#39640;&#38454;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#20877;&#29616;&#33021;&#21147;&#65292;&#24182;&#39564;&#35777;&#20102;&#22810;&#20010;&#23458;&#35266;&#27979;&#35797;&#20197;&#35780;&#20272;&#19981;&#21516;GAN&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGM&#65289;&#21487;&#20197;&#25913;&#21464;&#35786;&#26029;&#25104;&#20687;&#39046;&#22495;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;DGM&#12290;&#28982;&#32780;&#65292;&#23558;GAN&#21644;&#20854;&#20182;DGM&#24212;&#29992;&#20110;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#25165;&#33021;&#20351;&#29992;&#29983;&#25104;&#22270;&#20687;&#30340;&#20219;&#20309;&#24212;&#29992;&#31243;&#24207;&#26102;&#65292;&#26222;&#36941;&#23384;&#22312;&#32570;&#20047;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#39046;&#22495;&#30456;&#20851;&#36136;&#37327;&#30340;&#20805;&#20998;&#25110;&#33258;&#21160;&#21270;&#25163;&#27573;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#38024;&#23545;&#20004;&#31181;&#27969;&#34892;GAN&#26550;&#26500;&#36755;&#20986;&#30340;&#22270;&#20687;&#30340;&#20960;&#20010;&#23458;&#35266;&#27979;&#35797;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20960;&#20010;&#38543;&#26426;&#19978;&#19979;&#25991;&#27169;&#22411;&#65288;SCM&#65289;&#26469;&#24674;&#22797;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;GAN&#29983;&#25104;&#21518;&#21487;&#20197;&#24674;&#22797;&#30340;&#19981;&#21516;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#20854;&#20013;&#19968;&#20123;&#29305;&#24449;&#26159;&#39640;&#38454;&#31639;&#27861;&#20687;&#32032;&#25490;&#21015;&#35268;&#21017;&#65292;&#36825;&#20123;&#35268;&#21017;&#19981;&#26131;&#34920;&#36798;&#20026;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#39564;&#35777;&#20102;&#32479;&#35745;&#20998;&#31867;&#22120;&#65292;&#20197;&#20415;&#26816;&#27979;&#24050;&#30693;&#25490;&#21015;&#35268;&#21017;&#30340;&#29305;&#23450;&#25928;&#24212;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;GAN&#27491;&#30830;&#22797;&#29616;&#39640;&#38454;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models (DGMs) have the potential to revolutionize diagnostic imaging. Generative adversarial networks (GANs) are one kind of DGM which are widely employed. The overarching problem with deploying GANs, and other DGMs, in any application that requires domain expertise in order to actually use the generated images is that there generally is not adequate or automatic means of assessing the domain-relevant quality of generated images. In this work, we demonstrate several objective tests of images output by two popular GAN architectures. We designed several stochastic context models (SCMs) of distinct image features that can be recovered after generation by a trained GAN. Several of these features are high-order, algorithmic pixel-arrangement rules which are not readily expressed in covariance matrices. We designed and validated statistical classifiers to detect specific effects of the known arrangement rules. We then tested the rates at which two different GANs correctly rep
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#25968;&#25454;&#28857;&#20013;&#20272;&#35745;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#23398;&#20064;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2110.04829</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive joint distribution learning. (arXiv:2110.04829v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04829
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#25968;&#25454;&#28857;&#20013;&#20272;&#35745;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#23398;&#20064;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#23884;&#20837;&#24352;&#37327;&#31215;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#23481;&#32435;&#19968;&#20010;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#22810;&#36798;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#28857;&#30340;&#26679;&#26412;&#22823;&#23567;&#20013;&#36827;&#34892;&#20272;&#35745;&#65292;&#20943;&#36731;&#20102;RKHS&#24314;&#27169;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#28982;&#20135;&#29983;&#20102;&#23450;&#20041;&#33391;&#22909;&#30340;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#23884;&#20837;&#35745;&#31639;&#36895;&#24230;&#24555;&#19988;&#36866;&#29992;&#20110;&#20174;&#39044;&#27979;&#21040;&#20998;&#31867;&#30340;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;&#26377;&#30410;&#30340;&#25968;&#20540;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new framework for embedding joint probability distributions in tensor product reproducing kernel Hilbert spaces (RKHS). Our framework accommodates a low-dimensional, normalized and positive model of a Radon-Nikodym derivative, which we estimate from sample sizes of up to several million data points, alleviating the inherent limitations of RKHS modeling. Well-defined normalized and positive conditional distributions are natural by-products to our approach. The embedding is fast to compute and accommodates learning problems ranging from prediction to classification. Our theoretical findings are supplemented by favorable numerical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38754;&#26495;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#21512;&#25104;&#23545;&#29031;&#30340;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#12290;&#25991;&#31456;&#25299;&#23637;&#20102;&#35813;&#26694;&#26550;&#65292;&#20351;&#20854;&#36866;&#29992;&#24615;&#26356;&#21152;&#24191;&#27867;&#65292;&#24182;&#22312;&#35745;&#31639;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.02780</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#19968;&#33324;&#24178;&#39044;&#27169;&#24335;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#27835;&#30103;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Learning Treatment Effects in Panels with General Intervention Patterns. (arXiv:2106.02780v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38754;&#26495;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#21512;&#25104;&#23545;&#29031;&#30340;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#12290;&#25991;&#31456;&#25299;&#23637;&#20102;&#35813;&#26694;&#26550;&#65292;&#20351;&#20854;&#36866;&#29992;&#24615;&#26356;&#21152;&#24191;&#27867;&#65292;&#24182;&#22312;&#35745;&#31639;&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#26159;&#19968;&#20010;&#20013;&#24515;&#35745;&#37327;&#32463;&#27982;&#23398;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#30340;&#26159;&#19968;&#20010;&#22522;&#26412;&#29256;&#26412;&#30340;&#38754;&#26495;&#25968;&#25454;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#65306;&#35774;$M^*$&#20026;&#20302;&#31209;&#30697;&#38453;&#65292;$E$&#20026;&#38646;&#22343;&#20540;&#22122;&#22768;&#30697;&#38453;&#12290;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;$\{0,1\}$&#20540;&#30340;&#8220;&#27835;&#30103;&#8221;&#30697;&#38453;$Z$&#65292;&#25105;&#20204;&#35266;&#27979;&#21040;&#30697;&#38453;$O$&#65292;&#20854;&#20013;$O_{ij} := M^*_{ij} + E_{ij} + \mathcal{T}_{ij} Z_{ij}$&#65292;&#20854;&#20013;$\mathcal{T}_{ij}$&#26159;&#26410;&#30693;&#30340;&#24322;&#36136;&#24615;&#27835;&#30103;&#25928;&#24212;&#12290;&#36825;&#20010;&#38382;&#39064;&#38656;&#35201;&#25105;&#20204;&#20272;&#35745;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;$\tau^*:=\sum_{ij} \mathcal{T}_{ij} Z_{ij} / \sum_{ij} Z_{ij}$&#12290;&#21512;&#25104;&#23545;&#29031;&#33539;&#20363;&#25552;&#20379;&#20102;&#19968;&#31181;&#20272;&#35745;$\tau^*$&#30340;&#26041;&#27861;&#65292;&#24403;$Z$&#20165;&#20165;&#25903;&#25345;&#21333;&#20010;&#34892;&#26102;&#12290;&#26412;&#25991;&#23558;&#35813;&#26694;&#26550;&#25193;&#23637;&#21040;&#20801;&#35768;&#19968;&#33324;&#30340;$Z$&#30340;&#36895;&#29575;&#26368;&#20248;&#24674;&#22797;$\tau^*$&#65292;&#20174;&#32780;&#24191;&#27867;&#25193;&#23637;&#20102;&#23427;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#20445;&#35777;&#26159;&#22312;&#36825;&#20010;&#24191;&#27867;&#30340;&#35774;&#32622;&#20013;&#31532;&#19968;&#27425;&#20986;&#29616;&#30340;&#12290;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#35745;&#31639;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#30456;&#23545;&#20110;&#31454;&#20105;&#20272;&#35745;&#22120;&#20855;&#26377;&#37325;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of causal inference with panel data is a central econometric question. The following is a fundamental version of this problem: Let $M^*$ be a low rank matrix and $E$ be a zero-mean noise matrix. For a `treatment' matrix $Z$ with entries in $\{0,1\}$ we observe the matrix $O$ with entries $O_{ij} := M^*_{ij} + E_{ij} + \mathcal{T}_{ij} Z_{ij}$ where $\mathcal{T}_{ij} $ are unknown, heterogenous treatment effects. The problem requires we estimate the average treatment effect $\tau^* := \sum_{ij} \mathcal{T}_{ij} Z_{ij} / \sum_{ij} Z_{ij}$. The synthetic control paradigm provides an approach to estimating $\tau^*$ when $Z$ places support on a single row. This paper extends that framework to allow rate-optimal recovery of $\tau^*$ for general $Z$, thus broadly expanding its applicability. Our guarantees are the first of their type in this general setting. Computational experiments on synthetic and real-world data show a substantial advantage over competing estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#26102;&#38388;&#22343;&#21248;&#30340;&#28176;&#36817;&#32622;&#20449;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#24207;&#21015;&#22312;&#26102;&#38388;&#19978;&#26159;&#32479;&#19968;&#26377;&#25928;&#30340;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#20572;&#27490;&#26102;&#38388;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#65292;&#24182;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#38750;&#28176;&#36817;&#32622;&#20449;&#24207;&#21015;&#19982;&#28176;&#36817;&#32622;&#20449;&#21306;&#38388;&#20043;&#38388;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2103.06476</link><description>&lt;p&gt;
&#26102;&#38388;&#22343;&#21248;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#21644;&#28176;&#36817;&#32622;&#20449;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Time-uniform central limit theory and asymptotic confidence sequences. (arXiv:2103.06476v7 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.06476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#26102;&#38388;&#22343;&#21248;&#30340;&#28176;&#36817;&#32622;&#20449;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#24207;&#21015;&#22312;&#26102;&#38388;&#19978;&#26159;&#32479;&#19968;&#26377;&#25928;&#30340;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#20572;&#27490;&#26102;&#38388;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#65292;&#24182;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#38750;&#28176;&#36817;&#32622;&#20449;&#24207;&#21015;&#19982;&#28176;&#36817;&#32622;&#20449;&#21306;&#38388;&#20043;&#38388;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65288;CLT&#65289;&#30340;&#32622;&#20449;&#21306;&#38388;&#26159;&#32463;&#20856;&#32479;&#35745;&#23398;&#30340;&#22522;&#30707;&#12290;&#23613;&#31649;&#21482;&#26159;&#28176;&#36817;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#20204;&#26222;&#36941;&#23384;&#22312;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#22312;&#38750;&#24120;&#24369;&#30340;&#20551;&#35774;&#19979;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#24182;&#19988;&#36890;&#24120;&#21487;&#20197;&#24212;&#29992;&#20110;&#21363;&#20351;&#38750;&#28176;&#36817;&#25512;&#26029;&#20063;&#19981;&#21487;&#33021;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26102;&#38388;&#22343;&#21248;&#30340;&#31867;&#20284;&#20110;&#36825;&#26679;&#30340;&#28176;&#36817;&#32622;&#20449;&#21306;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32622;&#20449;&#24207;&#21015;&#65288;CS&#65289;&#30340;&#24418;&#24335; - &#32622;&#20449;&#21306;&#38388;&#30340;&#24207;&#21015;&#65292;&#36825;&#20123;&#21306;&#38388;&#22312;&#26102;&#38388;&#19978;&#26159;&#32479;&#19968;&#26377;&#25928;&#30340;&#12290;CS&#21487;&#22312;&#20219;&#24847;&#20572;&#27490;&#26102;&#38388;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#65292;&#32780;&#19981;&#38656;&#35201;&#20687;&#32463;&#20856;&#32622;&#20449;&#21306;&#38388;&#37027;&#26679;&#22312;&#20808;&#22266;&#23450;&#26679;&#26412;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#8220;&#31397;&#35270;&#8221;&#25968;&#25454;&#30340;&#24809;&#32602;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;CS&#37117;&#26159;&#38750;&#28176;&#36817;&#30340;&#65292;&#22240;&#27492;&#19981;&#33021;&#20139;&#21463;&#28176;&#36817;&#32622;&#20449;&#21306;&#38388;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#32473;&#20986;&#20102;&#8220;&#28176;&#36817;CS&#8221;&#30340;&#23450;&#20041;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#20010;&#36890;&#29992;&#30340;&#28176;&#36817;&#32622;&#20449;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Confidence intervals based on the central limit theorem (CLT) are a cornerstone of classical statistics. Despite being only asymptotically valid, they are ubiquitous because they permit statistical inference under very weak assumptions, and can often be applied to problems even when nonasymptotic inference is impossible. This paper introduces time-uniform analogues of such asymptotic confidence intervals. To elaborate, our methods take the form of confidence sequences (CS) -- sequences of confidence intervals that are uniformly valid over time. CSs provide valid inference at arbitrary stopping times, incurring no penalties for "peeking" at the data, unlike classical confidence intervals which require the sample size to be fixed in advance. Existing CSs in the literature are nonasymptotic, and hence do not enjoy the aforementioned broad applicability of asymptotic confidence intervals. Our work bridges the gap by giving a definition for "asymptotic CSs", and deriving a universal asympto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#26469;&#35757;&#32451;&#25972;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#25324;&#20248;&#21270;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#26041;&#27861;&#21644;&#40723;&#21169;NN&#26356;&#21152;&#31232;&#30095;&#30340;&#27491;&#21017;&#21270;&#39033;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;GPU&#25110;&#22797;&#26434;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2009.03825</link><description>&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20248;&#21270;&#25972;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20339;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal training of integer-valued neural networks with mixed integer programming. (arXiv:2009.03825v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.03825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#26469;&#35757;&#32451;&#25972;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#25324;&#20248;&#21270;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#26041;&#27861;&#21644;&#40723;&#21169;NN&#26356;&#21152;&#31232;&#30095;&#30340;&#27491;&#21017;&#21270;&#39033;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;GPU&#25110;&#22797;&#26434;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27714;&#35299;&#22120;&#21487;&#20197;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#20294;&#26159;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27714;&#35299;&#22120;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#30446;&#21069;&#30340;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#12289;&#22312;GPU&#19978;&#36827;&#34892;&#35745;&#31639;&#21644;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27714;&#35299;&#22120;&#36827;&#34892;&#35757;&#32451;&#19981;&#38656;&#35201;GPU&#25110;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#20294;&#30446;&#21069;&#21482;&#33021;&#22788;&#29702;&#23569;&#37327;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#22312;&#26368;&#36817;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27714;&#35299;&#22120;&#35757;&#32451;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#65292;&#20351;&#35757;&#32451;&#25928;&#29575;&#24471;&#21040;&#25913;&#21892;&#65292;&#24182;&#21487;&#20197;&#35757;&#32451;&#37325;&#35201;&#30340;&#25972;&#25968;&#20540;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#21457;&#25381;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#28508;&#22312;&#37325;&#35201;&#24615;&#65292;&#31532;&#19968;&#31181;&#26041;&#27861;&#22312;&#35757;&#32451;&#30340;&#21516;&#26102;&#20248;&#21270;NN&#20013;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#65292;&#36825;&#20943;&#23569;&#20102;&#22312;&#35757;&#32451;&#20043;&#21069;&#20915;&#23450;&#32593;&#32476;&#32467;&#26500;&#30340;&#38656;&#35201;&#65292;&#24182;&#21487;&#20197;&#33410;&#30465;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#40723;&#21169;&#35757;&#32451;&#30340;NN&#26356;&#21152;&#31232;&#30095;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown potential in using Mixed Integer Programming (MIP) solvers to optimize certain aspects of neural networks (NNs). However the intriguing approach of training NNs with MIP solvers is under-explored. State-of-the-art-methods to train NNs are typically gradient-based and require significant data, computation on GPUs, and extensive hyper-parameter tuning. In contrast, training with MIP solvers does not require GPUs or heavy hyper-parameter tuning, but currently cannot handle anything but small amounts of data. This article builds on recent advances that train binarized NNs using MIP solvers. We go beyond current work by formulating new MIP models which improve training efficiency and which can train the important class of integer-valued neural networks (INNs). We provide two novel methods to further the potential significance of using MIP to train NNs. The first method optimizes the number of neurons in the NN while training. This reduces the need for deciding on netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#25216;&#26415;&#30740;&#31350;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#21387;&#32553;&#65292;&#21457;&#29616;&#30456;&#27604;&#20351;&#29992;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#65292;&#38750;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#32593;&#32476;&#33021;&#23454;&#29616;&#21487;&#27604;&#30340;&#20219;&#21153;&#24615;&#33021;&#27700;&#24179;&#65292;&#20294;&#26080;&#27861;&#26174;&#31034;&#20986;&#20449;&#24687;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/1902.09037</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#20272;&#35745;&#22120;&#26174;&#31034;&#20449;&#24687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Adaptive Estimators Show Information Compression in Deep Neural Networks. (arXiv:1902.09037v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.09037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#25216;&#26415;&#30740;&#31350;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#21387;&#32553;&#65292;&#21457;&#29616;&#30456;&#27604;&#20351;&#29992;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#65292;&#38750;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#32593;&#32476;&#33021;&#23454;&#29616;&#21487;&#27604;&#30340;&#20219;&#21153;&#24615;&#33021;&#27700;&#24179;&#65292;&#20294;&#26080;&#27861;&#26174;&#31034;&#20986;&#20449;&#24687;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#30340;&#21151;&#33021;&#65292;&#29702;&#35299;&#23427;&#20204;&#30340;&#23398;&#20064;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#25552;&#20986;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#21387;&#32553;&#23427;&#20204;&#30340;&#34920;&#31034;&#26469;&#24573;&#30053;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20010;&#29702;&#35770;&#30340;&#32463;&#39564;&#35777;&#25454;&#26159;&#30456;&#20114;&#30683;&#30462;&#30340;&#65292;&#22240;&#20026;&#21482;&#26377;&#24403;&#32593;&#32476;&#20351;&#29992;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#26102;&#25165;&#35266;&#23519;&#21040;&#21387;&#32553;&#12290;&#30456;&#21453;&#65292;&#20855;&#26377;&#38750;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#32593;&#32476;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#20219;&#21153;&#24615;&#33021;&#27700;&#24179;&#65292;&#20294;&#27809;&#26377;&#26174;&#31034;&#20986;&#21387;&#32553;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26356;&#24378;&#22823;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#25216;&#26415;&#65292;&#36866;&#24212;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#27963;&#21160;&#65292;&#24182;&#20135;&#29983;&#26356;&#25935;&#24863;&#30340;&#20174;&#25152;&#26377;&#20989;&#25968;&#20013;&#28608;&#27963;&#30340;&#27979;&#37327;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#26080;&#30028;&#20989;&#25968;&#12290;&#21033;&#29992;&#36825;&#20123;&#33258;&#36866;&#24212;&#20272;&#35745;&#25216;&#26415;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;&#32593;&#32476;&#20013;&#30340;&#21387;&#32553;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;...
&lt;/p&gt;
&lt;p&gt;
To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we 
&lt;/p&gt;</description></item></channel></rss>