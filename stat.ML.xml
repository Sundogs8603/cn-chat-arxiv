<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20132;&#20114;&#24335;&#35774;&#35745;&#26041;&#27861;&#65292;&#32467;&#21512;&#32676;&#32452;&#36129;&#29486;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#24555;&#36895;&#22320;&#35774;&#35745;&#38750;&#32447;&#24615;&#20809;&#23398;&#20998;&#23376;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#20998;&#23376;&#30340;&#20809;&#23398;&#24615;&#36136;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.08570</link><description>&lt;p&gt;
&#22522;&#20110;&#32676;&#32452;&#36129;&#29486;&#26041;&#27861;&#30340;&#38750;&#32447;&#24615;&#20809;&#23398;&#20998;&#23376;&#30340;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20132;&#20114;&#24335;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Network Driven, Interactive Design for Nonlinear Optical Molecules Based on Group Contribution Method. (arXiv:2309.08570v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20132;&#20114;&#24335;&#35774;&#35745;&#26041;&#27861;&#65292;&#32467;&#21512;&#32676;&#32452;&#36129;&#29486;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#24555;&#36895;&#22320;&#35774;&#35745;&#38750;&#32447;&#24615;&#20809;&#23398;&#20998;&#23376;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#20998;&#23376;&#30340;&#20809;&#23398;&#24615;&#36136;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#22522;&#20110;Lewis&#27169;&#22411;&#32676;&#32452;&#36129;&#29486;&#26041;&#27861;&#65288;LGC&#65289;- &#22810;&#38454;&#27573;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;msBNN&#65289;- &#36827;&#21270;&#31639;&#27861;&#65288;EA&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#29702;&#35774;&#35745;D-Pi-A&#22411;&#26377;&#26426;&#23567;&#20998;&#23376;&#38750;&#32447;&#24615;&#20809;&#23398;&#26448;&#26009;&#12290;&#36890;&#36807;&#32467;&#21512;msBNN&#21644;&#26657;&#27491;&#30340;Lewis&#27169;&#22411;&#32676;&#32452;&#36129;&#29486;&#26041;&#27861;&#65288;cLGC&#65289;&#65292;&#21487;&#20197;&#20934;&#30830;&#39640;&#25928;&#22320;&#33719;&#24471;&#20998;&#23376;&#30340;&#19981;&#21516;&#20809;&#23398;&#24615;&#36136; - &#20165;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#20026;LGC&#35774;&#35745;&#30340;EA&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#32467;&#26500;&#25628;&#32034;&#12290;&#35814;&#32454;&#35752;&#35770;&#20102;&#35813;&#26694;&#26550;&#34920;&#29616;&#33391;&#22909;&#30340;&#36923;&#36753;&#21407;&#22240;&#12290;&#32771;&#34385;&#21040;&#36825;&#31181;&#29702;&#35770;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#32467;&#21512;&#20102;&#21270;&#23398;&#21407;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#24037;&#20855;&#65292;&#24456;&#21487;&#33021;&#34987;&#35777;&#26126;&#22312;&#26356;&#24191;&#27867;&#30340;&#39046;&#22495;&#20013;&#35299;&#20915;&#20998;&#23376;&#35774;&#35745;&#30456;&#20851;&#38382;&#39064;&#26102;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Lewis-mode group contribution method (LGC) -- multi-stage Bayesian neural network (msBNN) -- evolutionary algorithm (EA) framework is reported for rational design of D-Pi-A type organic small-molecule nonlinear optical materials is presented. Upon combination of msBNN and corrected Lewis-mode group contribution method (cLGC), different optical properties of molecules are afforded accurately and efficiently - by using only a small data set for training. Moreover, by employing the EA model designed specifically for LGC, structural search is well achievable. The logical origins of the well performance of the framework are discussed in detail. Considering that such a theory guided, machine learning framework combines chemical principles and data-driven tools, most likely, it will be proven efficient to solve molecular design related problems in wider fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WEEND&#30340;&#35789;&#32423;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36741;&#21161;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#21457;&#35328;&#20154;&#20998;&#31163;&#65292;&#24182;&#22312;2&#20010;&#21457;&#35328;&#20154;&#30340;&#30701;&#29255;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#32447;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08489</link><description>&lt;p&gt;
&#26397;&#21521;&#35789;&#32423;&#31471;&#21040;&#31471;&#31070;&#32463;&#21457;&#35328;&#20154;&#20998;&#31163;&#19982;&#36741;&#21161;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards Word-Level End-to-End Neural Speaker Diarization with Auxiliary Network. (arXiv:2309.08489v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WEEND&#30340;&#35789;&#32423;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36741;&#21161;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#21457;&#35328;&#20154;&#20998;&#31163;&#65292;&#24182;&#22312;2&#20010;&#21457;&#35328;&#20154;&#30340;&#30701;&#29255;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#32447;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26631;&#20934;&#30340;&#21457;&#35328;&#20154;&#20998;&#31163;&#35797;&#22270;&#22238;&#31572;&#8220;&#35841;&#22312;&#20160;&#20040;&#26102;&#20505;&#35828;&#20102;&#20160;&#20040;&#8221;&#65292;&#20294;&#29616;&#23454;&#20013;&#22823;&#22810;&#25968;&#30456;&#20851;&#24212;&#29992;&#26356;&#20851;&#24515;&#30830;&#23450;&#8220;&#35841;&#35828;&#20102;&#20160;&#20040;&#8221;&#12290;&#26080;&#35770;&#26159;&#20256;&#32479;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#36824;&#26159;&#26368;&#36817;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20998;&#31163;&#65288;EEND&#65289;&#65292;&#37117;&#38656;&#35201;&#19968;&#20010;&#39069;&#22806;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#21644;&#19968;&#20010;&#21327;&#35843;&#31639;&#27861;&#26469;&#23558;&#35828;&#35805;&#32773;&#26631;&#31614;&#19982;&#35782;&#21035;&#30340;&#21333;&#35789;&#20851;&#32852;&#36215;&#26469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36741;&#21161;&#32593;&#32476;&#30340;&#35789;&#32423;&#31471;&#21040;&#31471;&#31070;&#32463;&#20998;&#31163;&#65288;WEEND&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#22312;&#30456;&#21516;&#30340;&#31070;&#32463;&#26550;&#26500;&#20013;&#25191;&#34892;&#31471;&#21040;&#31471;ASR&#21644;&#21457;&#35328;&#20154;&#20998;&#31163;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24403;&#35821;&#38899;&#34987;&#35782;&#21035;&#26102;&#65292;&#21516;&#26102;&#20026;&#27599;&#20010;&#35782;&#21035;&#30340;&#21333;&#35789;&#39044;&#27979;&#35828;&#35805;&#32773;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WEEND&#22312;&#25152;&#26377;&#20004;&#20010;&#21457;&#35328;&#20154;&#30340;&#30701;&#29255;&#22330;&#26223;&#19978;&#20248;&#20110;&#22522;&#32447;&#31995;&#32479;&#65292;&#24182;&#19988;&#33021;&#22815;&#25512;&#24191;&#21040;5&#20998;&#38047;&#30340;&#38899;&#39057;&#38271;&#24230;&#12290;&#23613;&#31649;&#22312;3&#20010;&#25110;&#26356;&#22810;&#21457;&#35328;&#20154;&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#31995;&#32479;&#65292;&#19981;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While standard speaker diarization attempts to answer the question "who spoken when", most of relevant applications in reality are more interested in determining "who spoken what". Whether it is the conventional modularized approach or the more recent end-to-end neural diarization (EEND), an additional automatic speech recognition (ASR) model and an orchestration algorithm are required to associate the speaker labels with recognized words. In this paper, we propose Word-level End-to-End Neural Diarization (WEEND) with auxiliary network, a multi-task learning algorithm that performs end-to-end ASR and speaker diarization in the same neural architecture. That is, while speech is being recognized, speaker labels are predicted simultaneously for each recognized word. Experimental results demonstrate that WEEND outperforms the turn-based diarization baseline system on all 2-speaker short-form scenarios and has the capability to generalize to audio lengths of 5 minutes. Although 3+speaker co
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#20027;&#21160;&#20551;&#35774;&#27979;&#35797;&#65288;AHT&#65289;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#35774;&#35745;&#19968;&#20010;&#31574;&#30053;&#26469;&#22312;&#26377;&#38480;&#36890;&#20449;&#36890;&#36947;&#19978;&#21512;&#20316;&#23436;&#25104;&#20219;&#21153;&#65292;&#23558;&#36125;&#21494;&#26031;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.08477</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#20027;&#21160;&#20551;&#35774;&#27979;&#35797;&#30340;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Multi-Agent Reinforcement Learning for Decentralized Active Hypothesis Testing. (arXiv:2309.08477v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08477
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#20027;&#21160;&#20551;&#35774;&#27979;&#35797;&#65288;AHT&#65289;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#35774;&#35745;&#19968;&#20010;&#31574;&#30053;&#26469;&#22312;&#26377;&#38480;&#36890;&#20449;&#36890;&#36947;&#19978;&#21512;&#20316;&#23436;&#25104;&#20219;&#21153;&#65292;&#23558;&#36125;&#21494;&#26031;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20998;&#24067;&#24335;&#20027;&#21160;&#20551;&#35774;&#27979;&#35797;&#65288;AHT&#65289;&#38382;&#39064;&#30340;&#19968;&#20010;&#20998;&#24067;&#24335;&#24418;&#24335;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#20174;&#29615;&#22659;&#20013;&#25910;&#38598;&#21040;&#24102;&#22122;&#22768;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#30446;&#30340;&#26159;&#35782;&#21035;&#20986;&#27491;&#30830;&#30340;&#20551;&#35774;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#36873;&#25321;&#19968;&#20010;&#37319;&#26679;&#21160;&#20316;&#65292;&#36825;&#20123;&#19981;&#21516;&#30340;&#21160;&#20316;&#20250;&#23548;&#33268;&#20174;&#19981;&#21516;&#20998;&#24067;&#20013;&#25277;&#21462;&#35266;&#27979;&#25968;&#25454;&#65292;&#27599;&#20010;&#20998;&#24067;&#19982;&#19968;&#20010;&#29305;&#23450;&#30340;&#20551;&#35774;&#30456;&#20851;&#32852;&#12290;&#26234;&#33021;&#20307;&#36890;&#36807;&#22312;&#26377;&#38480;&#36895;&#29575;&#30340;&#36890;&#20449;&#36890;&#36947;&#19978;&#36827;&#34892;&#28040;&#24687;&#20132;&#25442;&#26469;&#21512;&#20316;&#23436;&#25104;&#20219;&#21153;&#12290;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#23558;&#36125;&#21494;&#26031;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#36825;&#31181;&#39118;&#38505;&#21253;&#25324;&#37319;&#26679;&#25104;&#26412;&#21644;&#26234;&#33021;&#20307;&#22312;&#22768;&#26126;&#20551;&#35774;&#26102;&#20135;&#29983;&#30340;&#32852;&#21512;&#32456;&#31471;&#25104;&#26412;&#12290;&#22312;AHT&#38382;&#39064;&#20013;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#32467;&#26500;&#21270;&#31574;&#30053;&#36890;&#24120;&#22312;&#25968;&#23398;&#19978;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#65292;&#21363;&#20351;&#26159;&#22312;&#21333;&#20010;&#26234;&#33021;&#20307;&#30340;&#32972;&#26223;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#36716;&#21521;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
We consider a decentralized formulation of the active hypothesis testing (AHT) problem, where multiple agents gather noisy observations from the environment with the purpose of identifying the correct hypothesis. At each time step, agents have the option to select a sampling action. These different actions result in observations drawn from various distributions, each associated with a specific hypothesis. The agents collaborate to accomplish the task, where message exchanges between agents are allowed over a rate-limited communications channel. The objective is to devise a multi-agent policy that minimizes the Bayes risk. This risk comprises both the cost of sampling and the joint terminal cost incurred by the agents upon making a hypothesis declaration. Deriving optimal structured policies for AHT problems is generally mathematically intractable, even in the context of a single agent. As a result, recent efforts have turned to deep learning methodologies to address these problems, whi
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#19981;&#20855;&#22791;&#20256;&#32479;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#23427;&#20204;&#22312;&#39044;&#27979;&#25216;&#33021;&#19978;&#30340;&#20248;&#21183;&#24456;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#24402;&#22240;&#20110;&#36825;&#20123;&#29305;&#27530;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08473</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the limitations of data-driven weather forecasting models. (arXiv:2309.08473v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08473
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#19981;&#20855;&#22791;&#20256;&#32479;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#23427;&#20204;&#22312;&#39044;&#27979;&#25216;&#33021;&#19978;&#30340;&#20248;&#21183;&#24456;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#24402;&#22240;&#20110;&#36825;&#20123;&#29305;&#27530;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#21457;&#23637;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#36890;&#24120;&#22768;&#31216;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#21069;&#19968;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;Pangu-Weather&#30340;&#39044;&#27979;&#26041;&#38754;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#19968;&#33268;&#24615;&#20197;&#21450;&#36825;&#20123;&#29305;&#24449;&#19982;&#24863;&#30693;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20027;&#35201;&#32467;&#35770;&#26159;Pangu-Weather&#30340;&#39044;&#27979;&#65292;&#20197;&#21450;&#31867;&#20284;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#19981;&#20855;&#22791;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#32780;&#23427;&#20204;&#22312;&#20256;&#32479;&#30340;&#30830;&#23450;&#24615;&#39044;&#27979;&#25216;&#33021;&#25351;&#26631;&#19978;&#30340;&#20248;&#21183;&#24456;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#24402;&#22240;&#20110;&#36825;&#20123;&#29305;&#27530;&#24615;&#12290;&#19982;&#20854;&#20182;&#24403;&#21069;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
As in many other areas of engineering and applied science, Machine Learning (ML) is having a profound impact in the domain of Weather and Climate Prediction. A very recent development in this area has been the emergence of fully data-driven ML prediction models which routinely claim superior performance to that of traditional physics-based models. In this work, we examine some aspects of the forecasts produced by an exemplar of the current generation of ML models, Pangu-Weather, with a focus on the fidelity and physical consistency of those forecasts and how these characteristics relate to perceived forecast performance. The main conclusion is that Pangu-Weather forecasts, and by extension those of similar ML models, do not have the fidelity and physical consistency of physics-based models and their advantage in accuracy on traditional deterministic metrics of forecast skill can be attributed, to a large extent, to these peculiarities. Similarly to other current post-processing technol
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#22359;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#23450;&#20041;&#30340;&#22266;&#23450;&#22823;&#23567;&#31383;&#21475;&#19978;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#27969;&#24335;&#36816;&#34892;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#30456;&#27604;&#38750;&#27969;&#24335;&#21464;&#31181;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#38271;&#31687;&#28436;&#35762;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08436</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#22359;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Chunked Attention-based Encoder-Decoder Model for Streaming Speech Recognition. (arXiv:2309.08436v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#22359;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#23450;&#20041;&#30340;&#22266;&#23450;&#22823;&#23567;&#31383;&#21475;&#19978;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#27969;&#24335;&#36816;&#34892;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#30456;&#27604;&#38750;&#27969;&#24335;&#21464;&#31181;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#38271;&#31687;&#28436;&#35762;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#21487;&#27969;&#24335;&#36816;&#34892;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#20854;&#20013;&#35299;&#30721;&#22120;&#25110;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#37117;&#21487;&#20197;&#22312;&#39044;&#23450;&#20041;&#30340;&#22266;&#23450;&#22823;&#23567;&#30340;&#31383;&#21475;&#65288;&#31216;&#20026;&#22359;&#65289;&#19978;&#25805;&#20316;&#12290;&#19968;&#31181;&#29305;&#27530;&#30340;&#22359;&#32467;&#26463;&#31526;&#65288;EOC&#65289;&#31526;&#21495;&#20174;&#19968;&#20010;&#22359;&#36827;&#20837;&#21040;&#19979;&#19968;&#20010;&#22359;&#65292;&#26377;&#25928;&#22320;&#26367;&#20195;&#20102;&#20256;&#32479;&#30340;&#24207;&#21015;&#32467;&#26463;&#31526;&#12290;&#36825;&#20010;&#20462;&#25913;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#32622;&#20110;&#19968;&#20010;&#25805;&#20316;&#22359;&#32780;&#19981;&#26159;&#24103;&#30340;&#36716;&#25442;&#27169;&#22411;&#65292;&#20854;&#20013;EOC&#23545;&#24212;&#31354;&#30333;&#31526;&#21495;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#26631;&#20934;&#36716;&#25442;&#22120;&#27169;&#22411;&#21644;&#25105;&#20204;&#27169;&#22411;&#20043;&#38388;&#30340;&#20854;&#20182;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#38271;&#31687;&#28436;&#35762;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#26463;&#25628;&#32034;&#22823;&#23567;&#21644;&#38271;&#24230;&#35268;&#33539;&#21270;&#31561;&#30456;&#20851;&#26041;&#38754;&#12290;&#36890;&#36807;&#22312;Librispeech&#21644;TED-LIUM-v2&#19978;&#30340;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#36830;&#25509;&#36830;&#32493;&#30340;&#24207;&#21015;&#36827;&#34892;&#38271;&#31687;&#35797;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#27969;&#24335;&#27169;&#22411;&#30456;&#27604;&#20110;&#38750;&#27969;&#24335;&#21464;&#31181;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#20110;&#38271;&#31687;&#28436;&#35762;&#38750;&#24120;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a streamable attention-based encoder-decoder model in which either the decoder, or both the encoder and decoder, operate on pre-defined, fixed-size windows called chunks. A special end-of-chunk (EOC) symbol advances from one chunk to the next chunk, effectively replacing the conventional end-of-sequence symbol. This modification, while minor, situates our model as equivalent to a transducer model that operates on chunks instead of frames, where EOC corresponds to the blank symbol. We further explore the remaining differences between a standard transducer and our model. Additionally, we examine relevant aspects such as long-form speech generalization, beam size, and length normalization. Through experiments on Librispeech and TED-LIUM-v2, and by concatenating consecutive sequences for long-form trials, we find that our streamable model maintains competitive performance compared to the non-streamable variant and generalizes very well to long-form speech.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;IHT&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27773;&#36710;&#29615;&#22659;&#20013;&#21033;&#29992;&#31232;&#30095;&#32447;&#24615;&#38453;&#21015;&#36827;&#34892;&#21333;&#24555;&#29031;DOA&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23545;IHT&#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#36890;&#36807;&#27973;&#23618;&#33258;&#32534;&#30721;&#22120;&#26367;&#20195;t-SVD&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.08429</link><description>&lt;p&gt;
&#22522;&#20110;IHT&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#31232;&#30095;&#32447;&#24615;&#38453;&#21015;&#30340;&#21333;&#24555;&#29031;DOA&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
IHT-Inspired Neural Network for Single-Snapshot DOA Estimation with Sparse Linear Arrays. (arXiv:2309.08429v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;IHT&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27773;&#36710;&#29615;&#22659;&#20013;&#21033;&#29992;&#31232;&#30095;&#32447;&#24615;&#38453;&#21015;&#36827;&#34892;&#21333;&#24555;&#29031;DOA&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23545;IHT&#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#36890;&#36807;&#27973;&#23618;&#33258;&#32534;&#30721;&#22120;&#26367;&#20195;t-SVD&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27773;&#36710;MIMO&#38647;&#36798;&#39046;&#22495;&#65292;&#20351;&#29992;&#31232;&#30095;&#32447;&#24615;&#38453;&#21015;(SLAs)&#36827;&#34892;&#21333;&#24555;&#29031;&#26041;&#21521;&#21040;&#36798;(DOA)&#20272;&#35745;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#27773;&#36710;&#29615;&#22659;&#20013;&#65292;&#22810;&#20010;&#24555;&#29031;&#24448;&#24448;&#26080;&#27861;&#33719;&#21462;&#65292;&#24182;&#19988;&#20943;&#23569;&#30828;&#20214;&#25104;&#26412;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#20302;&#31209;Hankel&#30697;&#38453;&#23436;&#25104;&#26469;&#25554;&#20540;SLAs&#20013;&#30340;&#20002;&#22833;&#20803;&#32032;&#12290;&#28982;&#32780;&#65292;&#30697;&#38453;&#23436;&#25104;&#30340;&#27714;&#35299;&#22120;&#65292;&#20363;&#22914;&#36845;&#20195;&#30828;&#38408;&#20540;(IHT)&#65292;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#38754;&#38750;&#24120;&#20381;&#36182;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#32570;&#20047;&#20219;&#21153;&#29305;&#24322;&#24615;&#12290;&#27492;&#22806;&#65292;IHT&#28041;&#21450;&#25130;&#26029;&#22855;&#24322;&#20540;&#20998;&#35299;(t-SVD)&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;IHT&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;SLAs&#30340;&#21333;&#24555;&#29031;DOA&#20272;&#35745;&#65292;&#31216;&#20026;IHT-Net&#12290;&#25105;&#20204;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23545;IHT&#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38598;&#25104;&#20102;&#27973;&#23618;&#33258;&#32534;&#30721;&#22120;&#26469;&#26367;&#20195;t-SVD&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-snapshot direction-of-arrival (DOA) estimation using sparse linear arrays (SLAs) has gained significant attention in the field of automotive MIMO radars. This is due to the dynamic nature of automotive settings, where multiple snapshots aren't accessible, and the importance of minimizing hardware costs. Low-rank Hankel matrix completion has been proposed to interpolate the missing elements in SLAs. However, the solvers of matrix completion, such as iterative hard thresholding (IHT), heavily rely on expert knowledge of hyperparameter tuning and lack task-specificity. Besides, IHT involves truncated-singular value decomposition (t-SVD), which has high computational cost in each iteration. In this paper, we propose an IHT-inspired neural network for single-snapshot DOA estimation with SLAs, termed IHT-Net. We utilize a recurrent neural network structure to parameterize the IHT algorithm. Additionally, we integrate shallow-layer autoencoders to replace t-SVD, reducing computational 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32422;&#26463;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#26696;COSMO&#65292;&#29992;&#20110;&#38750;&#29615;&#32467;&#26500;&#23398;&#20064;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#21487;&#24494;&#36817;&#20284;&#30340;&#26041;&#21521;&#30697;&#38453;&#65292;&#24182;&#20351;&#29992;&#21333;&#19968;&#20248;&#20808;&#21521;&#37327;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#24179;&#28369;&#30340;&#26041;&#21521;&#30697;&#38453;&#21644;&#30456;&#24212;&#30340;&#38750;&#29615;&#37051;&#25509;&#30697;&#38453;&#65292;&#32780;&#26080;&#38656;&#22312;&#20219;&#20309;&#27493;&#39588;&#20013;&#35780;&#20272;&#38750;&#29615;&#24615;&#12290;&#23613;&#31649;&#27809;&#26377;&#26174;&#24335;&#32422;&#26463;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;COSMO&#22987;&#32456;&#25910;&#25947;&#21040;&#19968;&#20010;&#38750;&#29615;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#28176;&#36817;&#24555;&#36895;&#65292;&#32780;&#19988;&#27604;&#20854;&#20182;&#26377;&#32422;&#26463;&#26041;&#27861;&#20855;&#26377;&#26356;&#23567;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.08406</link><description>&lt;p&gt;
&#19981;&#21463;&#38480;&#30340;&#24179;&#28369;&#26377;&#21521;&#26080;&#29615;&#22270;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Constraint-Free Structure Learning with Smooth Acyclic Orientations. (arXiv:2309.08406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32422;&#26463;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#26696;COSMO&#65292;&#29992;&#20110;&#38750;&#29615;&#32467;&#26500;&#23398;&#20064;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#21487;&#24494;&#36817;&#20284;&#30340;&#26041;&#21521;&#30697;&#38453;&#65292;&#24182;&#20351;&#29992;&#21333;&#19968;&#20248;&#20808;&#21521;&#37327;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#24179;&#28369;&#30340;&#26041;&#21521;&#30697;&#38453;&#21644;&#30456;&#24212;&#30340;&#38750;&#29615;&#37051;&#25509;&#30697;&#38453;&#65292;&#32780;&#26080;&#38656;&#22312;&#20219;&#20309;&#27493;&#39588;&#20013;&#35780;&#20272;&#38750;&#29615;&#24615;&#12290;&#23613;&#31649;&#27809;&#26377;&#26174;&#24335;&#32422;&#26463;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;COSMO&#22987;&#32456;&#25910;&#25947;&#21040;&#19968;&#20010;&#38750;&#29615;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#28176;&#36817;&#24555;&#36895;&#65292;&#32780;&#19988;&#27604;&#20854;&#20182;&#26377;&#32422;&#26463;&#26041;&#27861;&#20855;&#26377;&#26356;&#23567;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#26159;&#23558;&#30001;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#29983;&#25104;&#30340;&#25968;&#25454;&#27491;&#30830;&#22320;&#37325;&#26500;&#20854;&#24359;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#24494;&#21270;&#26041;&#27861;&#20351;&#29992;&#36830;&#32493;&#26494;&#24347;&#30340;&#38750;&#29615;&#24615;&#36136;&#23545;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#32422;&#26463;&#25110;&#35268;&#33539;&#21270;&#12290;&#35780;&#20272;&#22270;&#30340;&#38750;&#29615;&#24615;&#30340;&#35745;&#31639;&#25104;&#26412;&#19982;&#33410;&#28857;&#25968;&#37327;&#21576;&#19977;&#27425;&#26041;&#20851;&#31995;&#65292;&#20005;&#37325;&#24433;&#21709;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;COSMO&#65292;&#19968;&#31181;&#26080;&#32422;&#26463;&#36830;&#32493;&#20248;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#38750;&#29615;&#32467;&#26500;&#23398;&#20064;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#21487;&#24494;&#36817;&#20284;&#30340;&#26041;&#21521;&#30697;&#38453;&#65292;&#20854;&#30001;&#19968;&#20010;&#20248;&#20808;&#21521;&#37327;&#21442;&#25968;&#21270;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#21442;&#25968;&#21270;&#24471;&#21040;&#20102;&#19968;&#20010;&#24179;&#28369;&#30340;&#26041;&#21521;&#30697;&#38453;&#21644;&#30456;&#24212;&#30340;&#38750;&#29615;&#37051;&#25509;&#30697;&#38453;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#20219;&#20309;&#27493;&#39588;&#20013;&#35780;&#20272;&#38750;&#29615;&#24615;&#12290;&#23613;&#31649;&#27809;&#26377;&#26174;&#24335;&#32422;&#26463;&#65292;&#25105;&#20204;&#35777;&#26126;COSMO&#22987;&#32456;&#25910;&#25947;&#21040;&#19968;&#20010;&#38750;&#29615;&#35299;&#12290;&#38500;&#20102;&#28176;&#36817;&#24555;&#36895;&#22806;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#20998;&#26512;&#36824;&#34920;&#26126;COSMO&#19982;&#20854;&#20182;&#26377;&#32422;&#26463;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#23567;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The structure learning problem consists of fitting data generated by a Directed Acyclic Graph (DAG) to correctly reconstruct its arcs. In this context, differentiable approaches constrain or regularize the optimization problem using a continuous relaxation of the acyclicity property. The computational cost of evaluating graph acyclicity is cubic on the number of nodes and significantly affects scalability. In this paper we introduce COSMO, a constraint-free continuous optimization scheme for acyclic structure learning. At the core of our method, we define a differentiable approximation of an orientation matrix parameterized by a single priority vector. Differently from previous work, our parameterization fits a smooth orientation matrix and the resulting acyclic adjacency matrix without evaluating acyclicity at any step. Despite the absence of explicit constraints, we prove that COSMO always converges to an acyclic solution. In addition to being asymptotically faster, our empirical ana
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26631;&#20934;&#21270;&#21644;Mondrian&#31526;&#21512;&#35268;&#33539;&#30340;&#26041;&#27861;&#22914;&#20309;&#26500;&#24314;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20197;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#24322;&#26041;&#24046;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2309.08313</link><description>&lt;p&gt;
&#24322;&#26041;&#24046;&#25311;&#21512;&#32622;&#20449;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Heteroskedastic conformal regression. (arXiv:2309.08313v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26631;&#20934;&#21270;&#21644;Mondrian&#31526;&#21512;&#35268;&#33539;&#30340;&#26041;&#27861;&#22914;&#20309;&#26500;&#24314;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20197;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#24322;&#26041;&#24046;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#35268;&#33539;&#30340;&#39044;&#27979;&#20197;&#21450;&#29305;&#23450;&#30340;&#25286;&#20998;&#31526;&#21512;&#35268;&#33539;&#30340;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#20998;&#24067;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#19987;&#27880;&#20110;&#36793;&#38469;&#35206;&#30422;&#26102;&#65292;&#21363;&#22312;&#26657;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#30340;&#39044;&#27979;&#21306;&#38388;&#24179;&#22343;&#21253;&#21547;&#39044;&#23450;&#20041;&#35206;&#30422;&#27700;&#24179;&#30340;&#30495;&#23454;&#20540;&#65292;&#25286;&#20998;&#31526;&#21512;&#35268;&#33539;&#30340;&#39044;&#27979;&#21487;&#20197;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#21306;&#38388;&#36890;&#24120;&#19981;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24322;&#26041;&#24046;&#22122;&#22768;&#30340;&#22238;&#24402;&#38382;&#39064;&#21487;&#33021;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#26412;&#25991;&#35797;&#22270;&#38416;&#26126;&#22914;&#20309;&#20351;&#29992;&#26631;&#20934;&#21270;&#21644;Mondrian&#31526;&#21512;&#35268;&#33539;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#20197;&#31995;&#32479;&#30340;&#26041;&#24335;&#25552;&#20986;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#26469;&#30740;&#31350;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction, and split conformal prediction as a specific implementation, offer a distribution-free approach to estimating prediction intervals with statistical guarantees. Recent work has shown that split conformal prediction can produce state-of-the-art prediction intervals when focusing on marginal coverage, i.e., on a calibration dataset the method produces on average prediction intervals that contain the ground truth with a predefined coverage level. However, such intervals are often not adaptive, which can be problematic for regression problems with heteroskedastic noise. This paper tries to shed new light on how adaptive prediction intervals can be constructed using methods such as normalized and Mondrian conformal prediction. We present theoretical and experimental results in which these methods are investigated in a systematic way.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37319;&#26679;&#30340;&#27010;&#29575;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31532;&#19968;&#20010;&#30830;&#23450;&#24615;&#25512;&#26029;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2309.08256</link><description>&lt;p&gt;
&#26080;&#38656;&#37319;&#26679;&#30340;&#27010;&#29575;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sampling-Free Probabilistic Deep State-Space Models. (arXiv:2309.08256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37319;&#26679;&#30340;&#27010;&#29575;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31532;&#19968;&#20010;&#30830;&#23450;&#24615;&#25512;&#26029;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#21160;&#24577;&#31995;&#32479;&#21487;&#20197;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#26469;&#25551;&#36848;&#12290;&#22312;&#36825;&#31181;&#34920;&#36848;&#20013;&#65292;&#27599;&#20010;&#35266;&#23519;&#20540;&#37117;&#30001;&#19968;&#20010;&#28508;&#22312;&#29366;&#24577;&#21457;&#23556;&#65292;&#35813;&#29366;&#24577;&#36981;&#24490;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#21160;&#21147;&#23398;&#12290;&#27010;&#29575;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;ProDSSM&#65289;&#23558;&#36825;&#19968;&#26694;&#26550;&#25512;&#24191;&#21040;&#26410;&#30693;&#21442;&#25968;&#24418;&#24335;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#20854;&#20013;&#36807;&#28193;&#27169;&#22411;&#21644;&#21457;&#23556;&#27169;&#22411;&#30001;&#20855;&#26377;&#19981;&#30830;&#23450;&#26435;&#37325;&#30340;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#36825;&#31867;&#27169;&#22411;&#30340;&#31532;&#19968;&#20010;&#30830;&#23450;&#24615;&#25512;&#26029;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#36817;&#20284;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#22312;&#39044;&#27979;&#24615;&#33021;&#21644;&#35745;&#31639;&#39044;&#31639;&#20043;&#38388;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world dynamical systems can be described as State-Space Models (SSMs). In this formulation, each observation is emitted by a latent state, which follows first-order Markovian dynamics. A Probabilistic Deep SSM (ProDSSM) generalizes this framework to dynamical systems of unknown parametric form, where the transition and emission models are described by neural networks with uncertain weights. In this work, we propose the first deterministic inference algorithm for models of this type. Our framework allows efficient approximations for training and testing. We demonstrate in our experiments that our new method can be employed for a variety of tasks and enjoys a superior balance between predictive performance and computational budget.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Beta&#25955;&#24230;&#30340;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38754;&#37096;&#29305;&#24449;&#25552;&#21462;&#12289;&#25991;&#26723;&#20027;&#39064;&#35782;&#21035;&#21644;&#39640;&#20809;&#35889;&#22270;&#20687;&#26448;&#26009;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.08249</link><description>&lt;p&gt;
&#24102;&#26377;Beta&#25955;&#24230;&#30340;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Deep Nonnegative Matrix Factorization with Beta Divergences. (arXiv:2309.08249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Beta&#25955;&#24230;&#30340;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38754;&#37096;&#29305;&#24449;&#25552;&#21462;&#12289;&#25991;&#26723;&#20027;&#39064;&#35782;&#21035;&#21644;&#39640;&#20809;&#35889;&#22270;&#20687;&#26448;&#26009;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;deep NMF&#65289;&#26368;&#36817;&#25104;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#25552;&#21462;&#22810;&#23618;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#28145;&#24230;NMF&#27169;&#22411;&#21644;&#31639;&#27861;&#20027;&#35201;&#37117;&#20197;&#26368;&#23567;&#20108;&#20056;&#35823;&#24046;&#20026;&#35780;&#20272;&#26631;&#20934;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#35780;&#20272;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#36817;&#20284;&#36136;&#37327;&#30340;&#26368;&#21512;&#36866;&#25351;&#26631;&#12290;&#20363;&#22914;&#65292;&#24403;&#22788;&#29702;&#38899;&#39057;&#20449;&#21495;&#21644;&#25991;&#26723;&#31561;&#25968;&#25454;&#31867;&#22411;&#26102;&#65292;&#24191;&#27867;&#35748;&#21487;&#30340;&#26159;$\beta$-divergences&#25552;&#20379;&#20102;&#26356;&#36866;&#21512;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#22522;&#20110;$\beta$-divergences&#24320;&#21457;&#20102;&#26032;&#30340;&#28145;&#24230;NMF&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#38754;&#37096;&#29305;&#24449;&#25552;&#21462;&#12289;&#25991;&#26723;&#38598;&#21512;&#20013;&#30340;&#20027;&#39064;&#35782;&#21035;&#20197;&#21450;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#26448;&#26009;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Nonnegative Matrix Factorization (deep NMF) has recently emerged as a valuable technique for extracting multiple layers of features across different scales. However, all existing deep NMF models and algorithms have primarily centered their evaluation on the least squares error, which may not be the most appropriate metric for assessing the quality of approximations on diverse datasets. For instance, when dealing with data types such as audio signals and documents, it is widely acknowledged that $\beta$-divergences offer a more suitable alternative. In this paper, we develop new models and algorithms for deep NMF using $\beta$-divergences. Subsequently, we apply these techniques to the extraction of facial features, the identification of topics within document collections, and the identification of materials within hyperspectral images.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#25439;&#22833;&#39033;&#21644;&#36866;&#24212;&#25345;&#32493;&#21516;&#35843;&#24230;&#37327;&#30340;&#29109;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;Node2vec&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#36824;&#21407;&#36755;&#20837;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.08241</link><description>&lt;p&gt;
&#22522;&#20110;&#25345;&#32493;&#21516;&#35843;&#30340;&#25299;&#25169;Node2vec&#65306;&#22686;&#24378;&#22270;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Topological Node2vec: Enhanced Graph Embedding via Persistent Homology. (arXiv:2309.08241v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08241
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#25439;&#22833;&#39033;&#21644;&#36866;&#24212;&#25345;&#32493;&#21516;&#35843;&#24230;&#37327;&#30340;&#29109;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;Node2vec&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#36824;&#21407;&#36755;&#20837;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Node2vec&#26159;&#19968;&#31181;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#20102;&#21152;&#26435;&#22270;&#27599;&#20010;&#33410;&#28857;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#21516;&#26102;&#23613;&#21147;&#20445;&#25345;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#23545;&#36317;&#31163;&#21644;&#20840;&#23616;&#32467;&#26500;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;Node2vec&#38590;&#20197;&#20877;&#29616;&#36755;&#20837;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;Node2vec&#30340;&#35757;&#32451;&#25439;&#22833;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#25299;&#25169;&#25439;&#22833;&#39033;&#65292;&#35813;&#25439;&#22833;&#39033;&#35797;&#22270;&#23558;&#29983;&#25104;&#30340;&#23884;&#20837;&#30340;&#25345;&#32493;&#21516;&#35843;&#22270;&#19982;&#36755;&#20837;&#22270;&#30340;&#25345;&#32493;&#21516;&#35843;&#22270;&#23613;&#21487;&#33021;&#22320;&#23545;&#40784;&#12290;&#25105;&#20204;&#26681;&#25454;&#35745;&#31639;&#20248;&#21270;&#20256;&#36755;&#20013;&#30340;&#32467;&#26524;&#65292;&#31934;&#24515;&#35843;&#25972;&#20102;&#29109;&#27491;&#21017;&#21270;&#30340;&#25345;&#32493;&#21516;&#35843;&#24230;&#37327;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#34913;&#37327;&#25345;&#32493;&#21516;&#35843;&#22270;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26368;&#23567;&#21270;&#25105;&#20204;&#20462;&#25913;&#21518;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#37325;&#24314;&#36755;&#20837;&#22270;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20123;&#31034;&#20363;&#21512;&#25104;&#22270;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node2vec is a graph embedding method that learns a vector representation for each node of a weighted graph while seeking to preserve relative proximity and global structure. Numerical experiments suggest Node2vec struggles to recreate the topology of the input graph. To resolve this we introduce a topological loss term to be added to the training loss of Node2vec which tries to align the persistence diagram (PD) of the resulting embedding as closely as possible to that of the input graph. Following results in computational optimal transport, we carefully adapt entropic regularization to PD metrics, allowing us to measure the discrepancy between PDs in a differentiable way. Our modified loss function can then be minimized through gradient descent to reconstruct both the geometry and the topology of the input graph. We showcase the benefits of this approach using demonstrative synthetic examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#33539;&#24335;&#19979;&#65292;&#36890;&#36807;&#20998;&#26512;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#26435;&#37325;&#20445;&#25345;&#22312;&#21021;&#22987;&#20301;&#32622;&#38468;&#36817;&#65292;&#21322;&#24452;&#19982;&#22238;&#24402;&#20989;&#25968;&#30340;&#24179;&#28369;&#24230;&#21644;NTK&#30340;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#34928;&#20943;&#31243;&#24230;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.08044</link><description>&lt;p&gt;
&#25105;&#20204;&#38656;&#35201;&#22810;&#23569;&#20010;&#31070;&#32463;&#20803;&#65311;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#27973;&#23618;&#32593;&#32476;&#30340;&#31934;&#32454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How many Neurons do we need? A refined Analysis for Shallow Networks trained with Gradient Descent. (arXiv:2309.08044v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#33539;&#24335;&#19979;&#65292;&#36890;&#36807;&#20998;&#26512;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#26435;&#37325;&#20445;&#25345;&#22312;&#21021;&#22987;&#20301;&#32622;&#38468;&#36817;&#65292;&#21322;&#24452;&#19982;&#22238;&#24402;&#20989;&#25968;&#30340;&#24179;&#28369;&#24230;&#21644;NTK&#30340;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#34928;&#20943;&#31243;&#24230;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#33539;&#24335;&#19979;&#65292;&#20998;&#26512;&#20102;&#29992;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#35757;&#32451;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#36136;&#12290;&#23545;&#20110;&#26089;&#20572;&#30340;GD&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#36895;&#24230;&#65292;&#36825;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#26694;&#26550;&#20013;&#24050;&#30693;&#26159;&#26368;&#23567;&#20540;&#30340;&#26368;&#20248;&#35299;&#12290;&#22312;&#36825;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#31934;&#30830;&#22320;&#36861;&#36394;&#20102;&#27867;&#21270;&#25152;&#38656;&#30340;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#25968;&#37327;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#26435;&#37325;&#20445;&#25345;&#22312;&#21021;&#22987;&#20301;&#32622;&#38468;&#36817;&#30340;&#24773;&#20917;&#65292;&#20854;&#21322;&#24452;&#21462;&#20915;&#20110;&#22238;&#24402;&#20989;&#25968;&#30340;&#24179;&#28369;&#24230;&#21644;&#19982;NTK&#30456;&#20851;&#32852;&#30340;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#34928;&#20943;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the generalization properties of two-layer neural networks in the neural tangent kernel (NTK) regime, trained with gradient descent (GD). For early stopped GD we derive fast rates of convergence that are known to be minimax optimal in the framework of non-parametric regression in reproducing kernel Hilbert spaces. On our way, we precisely keep track of the number of hidden neurons required for generalization and improve over existing results. We further show that the weights during training remain in a vicinity around initialization, the radius being dependent on structural assumptions such as degree of smoothness of the regression function and eigenvalue decay of the integral operator associated to the NTK.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#26469;&#33719;&#24471;LISTA&#20272;&#35745;&#37327;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20026;&#27169;&#22411;-based&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2309.07982</link><description>&lt;p&gt;
&#20026;&#23398;&#20064;&#30340;ISTA&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification for learned ISTA. (arXiv:2309.07982v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#26469;&#33719;&#24471;LISTA&#20272;&#35745;&#37327;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20026;&#27169;&#22411;-based&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36870;&#38382;&#39064;&#20013;&#24050;&#32463;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#25968;&#20540;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#22788;&#20110;&#26368;&#21069;&#27839;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#21487;&#20197;&#20351;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#65292;&#22240;&#20026;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#20801;&#35768;&#20351;&#29992;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36825;&#20123;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25216;&#26415;&#20013;&#65292;&#31639;&#27861;&#23637;&#24320;&#26041;&#26696;&#33073;&#39062;&#32780;&#20986;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#24555;&#36895;&#21457;&#23637;&#19982;&#20256;&#32479;&#30340;&#39640;&#32500;&#32479;&#35745;&#26041;&#27861;&#23494;&#20999;&#30456;&#20851;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23545;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#29702;&#35770;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#26469;&#33719;&#24471;LISTA&#20272;&#35745;&#37327;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20174;&#32780;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based deep learning solutions to inverse problems have attracted increasing attention in recent years as they bridge state-of-the-art numerical performance with interpretability. In addition, the incorporated prior domain knowledge can make the training more efficient as the smaller number of parameters allows the training step to be executed with smaller datasets. Algorithm unrolling schemes stand out among these model-based learning techniques. Despite their rapid advancement and their close connection to traditional high-dimensional statistical methods, they lack certainty estimates and a theory for uncertainty quantification is still elusive. This work provides a step towards closing this gap proposing a rigorous way to obtain confidence intervals for the LISTA estimator.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#37319;&#26679;&#26041;&#26696; (ESS)&#65292;&#29992;&#20110;&#25513;&#30721;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#24314;&#27169;&#12290;&#35813;&#26041;&#26696;&#33021;&#22815;&#30830;&#20445;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#24182;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#12289;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#12290;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#29992;&#20110;&#37319;&#26679;&#26631;&#35760;&#38598;&#65292;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#29992;&#20110;&#25513;&#30422;&#19981;&#30495;&#23454;&#30340;&#26631;&#35760;&#24182;&#37325;&#24314;&#34987;&#25513;&#30422;&#30340;&#26631;&#35760;&#65292;&#20197;&#25552;&#39640;&#37319;&#26679;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07945</link><description>&lt;p&gt;
&#22686;&#24378;&#37319;&#26679;&#26041;&#26696;&#30340;&#25513;&#30721;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Masked Generative Modeling with Enhanced Sampling Scheme. (arXiv:2309.07945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#37319;&#26679;&#26041;&#26696; (ESS)&#65292;&#29992;&#20110;&#25513;&#30721;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#24314;&#27169;&#12290;&#35813;&#26041;&#26696;&#33021;&#22815;&#30830;&#20445;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#24182;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#12289;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#12290;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#29992;&#20110;&#37319;&#26679;&#26631;&#35760;&#38598;&#65292;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#29992;&#20110;&#25513;&#30422;&#19981;&#30495;&#23454;&#30340;&#26631;&#35760;&#24182;&#37325;&#24314;&#34987;&#25513;&#30422;&#30340;&#26631;&#35760;&#65292;&#20197;&#25552;&#39640;&#37319;&#26679;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25513;&#30721;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#24314;&#27169;&#30340;&#26032;&#22411;&#37319;&#26679;&#26041;&#26696;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;TimeVQVAE&#12289;MaskGIT&#21644;Token-Critic&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#24378;&#37319;&#26679;&#26041;&#26696; (ESS) &#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;ESS&#26126;&#30830;&#30830;&#20445;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#12289;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#12290;ESS&#39318;&#20808;&#20351;&#29992;MaskGIT&#20013;&#25552;&#20986;&#30340;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#26469;&#37319;&#26679;&#19968;&#20010;&#26631;&#35760;&#38598;&#65292;&#20197;&#30830;&#20445;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#21518;&#65292;&#26631;&#35760;&#38598;&#32463;&#36807;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#65292;&#25513;&#30422;&#23548;&#33268;&#19981;&#30495;&#23454;&#26679;&#26412;&#30340;&#26631;&#35760;&#12290;&#22312;&#27492;&#20043;&#21518;&#65292;&#20851;&#38190;&#37325;&#37319;&#26679;&#37325;&#24314;&#34987;&#25513;&#30422;&#30340;&#26631;&#35760;&#65292;&#30452;&#21040;&#36798;&#21040;&#26368;&#32456;&#37319;&#26679;&#27493;&#39588;&#20197;&#30830;&#20445;&#39640;&#24230;&#20445;&#30495;&#24230;&#12290;&#20851;&#38190;&#37325;&#37319;&#26679;&#20351;&#29992;&#26469;&#33258;&#33258;&#25105;Token-Critic&#33719;&#24471;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26356;&#22909;&#22320;&#34913;&#37327;&#37319;&#26679;&#26631;&#35760;&#30340;&#30495;&#23454;&#24615;&#65292;&#32780;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#20351;&#29992;&#37327;&#21270;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel sampling scheme for masked non-autoregressive generative modeling. We identify the limitations of TimeVQVAE, MaskGIT, and Token-Critic in their sampling processes, and propose Enhanced Sampling Scheme (ESS) to overcome these limitations. ESS explicitly ensures both sample diversity and fidelity, and consists of three stages: Naive Iterative Decoding, Critical Reverse Sampling, and Critical Resampling. ESS starts by sampling a token set using the naive iterative decoding as proposed in MaskGIT, ensuring sample diversity. Then, the token set undergoes the critical reverse sampling, masking tokens leading to unrealistic samples. After that, critical resampling reconstructs masked tokens until the final sampling step is reached to ensure high fidelity. Critical resampling uses confidence scores obtained from a self-Token-Critic to better measure the realism of sampled tokens, while critical reverse sampling uses the structure of the quantized latent vector space
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30456;&#20851;&#36335;&#24452;&#26469;&#38477;&#20302;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#20013;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#20272;&#35745;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#20989;&#25968;&#12290;&#36890;&#36807;&#25919;&#31574;&#26799;&#24230;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;&#30456;&#20851;&#20989;&#25968;&#24182;&#36827;&#34892;&#26657;&#20934;&#12290;&#36825;&#19982;&#26368;&#22823;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.12703</link><description>&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#26368;&#20248;&#30456;&#20851;&#25628;&#32034;&#29992;&#20110;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#21644;&#26368;&#22823;&#26368;&#20248;&#20256;&#36755;&#20013;&#30340;&#26041;&#24046;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Optimal Correlation Search for Variance Reduction in Monte Carlo simulation and Maximum Optimal Transport. (arXiv:2307.12703v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30456;&#20851;&#36335;&#24452;&#26469;&#38477;&#20302;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#20013;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#20272;&#35745;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#20989;&#25968;&#12290;&#36890;&#36807;&#25919;&#31574;&#26799;&#24230;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;&#30456;&#20851;&#20989;&#25968;&#24182;&#36827;&#34892;&#26657;&#20934;&#12290;&#36825;&#19982;&#26368;&#22823;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;$f(X_T)$&#30340;&#26041;&#24046;&#38477;&#20302;&#31639;&#27861;&#65292;&#20854;&#20013;$X$&#26159;&#26576;&#20010;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;$f$&#26159;&#19968;&#20010;&#27979;&#35797;&#20989;&#25968;&#12290;&#26032;&#30340;&#20272;&#35745;&#22120;&#26159;$(f(X^1_T) + f(X^2_T))/2$&#65292;&#20854;&#20013;$X^1$&#21644;$X^2$&#20855;&#26377;&#19982;$X$&#30456;&#21516;&#30340;&#36793;&#38469;&#20998;&#24067;&#65292;&#20294;&#36335;&#24452;&#19978;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20197;&#38477;&#20302;&#26041;&#24046;&#12290;&#26368;&#20248;&#30456;&#20851;&#20989;&#25968;$\rho$&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#65292;&#24182;&#36890;&#36807;&#25919;&#31574;&#26799;&#24230;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22312;$(X^1, X^2)$&#30340;&#36712;&#36857;&#19978;&#36827;&#34892;&#26657;&#20934;&#12290;&#22312;&#32473;&#23450;&#36793;&#38469;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26368;&#20248;&#32806;&#21512;&#19982;&#26368;&#22823;&#26368;&#20248;&#20256;&#36755;&#26377;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new algorithm for variance reduction when estimating $f(X_T)$ where $X$ is the solution to some stochastic differential equation and $f$ is a test function. The new estimator is $(f(X^1_T) + f(X^2_T))/2$, where $X^1$ and $X^2$ have same marginal law as $X$ but are pathwise correlated so that to reduce the variance. The optimal correlation function $\rho$ is approximated by a deep neural network and is calibrated along the trajectories of $(X^1, X^2)$ by policy gradient and reinforcement learning techniques. Finding an optimal coupling given marginal laws has links with maximum optimal transport.
&lt;/p&gt;</description></item><item><title>Engression&#26159;&#19968;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#22238;&#24402;&#25216;&#26415;&#21644;&#39044;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#36793;&#30028;&#22806;&#20063;&#33021;&#21487;&#38752;&#22320;&#36827;&#34892;&#22806;&#25512;&#12290;</title><link>http://arxiv.org/abs/2307.00835</link><description>&lt;p&gt;
Engression: &#38750;&#32447;&#24615;&#22238;&#24402;&#30340;&#22806;&#25512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Engression: Extrapolation for Nonlinear Regression?. (arXiv:2307.00835v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00835
&lt;/p&gt;
&lt;p&gt;
Engression&#26159;&#19968;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#22238;&#24402;&#25216;&#26415;&#21644;&#39044;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#36793;&#30028;&#22806;&#20063;&#33021;&#21487;&#38752;&#22320;&#36827;&#34892;&#22806;&#25512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#25512;&#23545;&#20110;&#35768;&#22810;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#24120;&#24120;&#20250;&#36935;&#21040;&#36229;&#20986;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#26469;&#35828;&#65292;&#22806;&#25512;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#36890;&#24120;&#36935;&#21040;&#22256;&#38590;&#65306;&#26641;&#38598;&#25104;&#27169;&#22411;&#22312;&#25903;&#25345;&#33539;&#22260;&#22806;&#25552;&#20379;&#36830;&#32493;&#30340;&#39044;&#27979;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#24448;&#24448;&#21464;&#24471;&#19981;&#21487;&#25511;&#12290;&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#20854;&#21487;&#38752;&#24615;&#22312;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#36793;&#30028;&#19981;&#20250;&#31435;&#21363;&#23849;&#28291;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#21517;&#20026;&#8220;engression&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#39044;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#30340;&#20998;&#24067;&#22238;&#24402;&#25216;&#26415;&#65292;&#20854;&#20013;&#22122;&#22768;&#28155;&#21152;&#21040;&#21327;&#21464;&#37327;&#19978;&#24182;&#24212;&#29992;&#38750;&#32447;&#24615;&#36716;&#25442;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#36890;&#24120;&#36866;&#29992;&#20110;&#35768;&#22810;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;engression&#21487;&#20197;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#25104;&#21151;&#36827;&#34892;&#22806;&#25512;&#65292;&#20363;&#22914;&#20005;&#26684;&#38480;&#21046;&#22122;&#22768;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extrapolation is crucial in many statistical and machine learning applications, as it is common to encounter test data outside the training support. However, extrapolation is a considerable challenge for nonlinear models. Conventional models typically struggle in this regard: while tree ensembles provide a constant prediction beyond the support, neural network predictions tend to become uncontrollable. This work aims at providing a nonlinear regression methodology whose reliability does not break down immediately at the boundary of the training support. Our primary contribution is a new method called `engression' which, at its core, is a distributional regression technique for pre-additive noise models, where the noise is added to the covariates before applying a nonlinear transformation. Our experimental results indicate that this model is typically suitable for many real data sets. We show that engression can successfully perform extrapolation under some assumptions such as a strictl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29109;&#27491;&#21017;&#21270;&#30340;&#26494;&#24347;&#38543;&#26426;&#25511;&#21046;&#35270;&#35282;&#35774;&#35745;&#20102;&#36830;&#32493;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;&#24615;&#25511;&#21046;&#26041;&#27861;&#21644;&#36817;&#31471;&#31574;&#30053;&#26356;&#26032;&#26041;&#27861;&#23454;&#29616;&#20102;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#65292;&#20197;&#35299;&#20915;&#26377;&#38480;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.04466</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#24378;&#21270;&#23398;&#20064;&#20013;&#29109;&#27491;&#21017;&#21270;&#30340;&#26368;&#20248;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimal scheduling of entropy regulariser for continuous-time linear-quadratic reinforcement learning. (arXiv:2208.04466v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29109;&#27491;&#21017;&#21270;&#30340;&#26494;&#24347;&#38543;&#26426;&#25511;&#21046;&#35270;&#35282;&#35774;&#35745;&#20102;&#36830;&#32493;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;&#24615;&#25511;&#21046;&#26041;&#27861;&#21644;&#36817;&#31471;&#31574;&#30053;&#26356;&#26032;&#26041;&#27861;&#23454;&#29616;&#20102;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#65292;&#20197;&#35299;&#20915;&#26377;&#38480;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#29109;&#27491;&#21017;&#21270;&#30340;&#26494;&#24347;&#38543;&#26426;&#25511;&#21046;&#35270;&#35282;&#20316;&#20026;&#35774;&#35745;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#26694;&#26550;&#12290;&#22312;&#36825;&#37324;&#65292;Agent&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#26368;&#20248;&#26494;&#24347;&#31574;&#30053;&#30340;&#22122;&#22768;&#25511;&#21046;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#22122;&#22768;&#31574;&#30053;&#19968;&#26041;&#38754;&#21487;&#20197;&#25506;&#32034;&#31354;&#38388;&#24182;&#20419;&#36827;&#23398;&#20064;&#65292;&#20294;&#21478;&#19968;&#26041;&#38754;&#20063;&#20250;&#24341;&#20837;&#20559;&#24046;&#65292;&#23558;&#27491;&#27010;&#29575;&#20998;&#37197;&#32473;&#38750;&#26368;&#20248;&#21160;&#20316;&#12290;&#36825;&#31181;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#26435;&#34913;&#30001;&#29109;&#27491;&#21017;&#21270;&#30340;&#24378;&#24230;&#26469;&#30830;&#23450;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#29109;&#27491;&#21017;&#21270;&#24418;&#24335;&#24471;&#21040;&#30340;&#31639;&#27861;&#65306;&#25506;&#32034;&#24615;&#25511;&#21046;&#26041;&#27861;&#65292;&#22312;&#25104;&#26412;&#30446;&#26631;&#20013;&#28155;&#21152;&#29109;&#65307;&#36817;&#31471;&#31574;&#30053;&#26356;&#26032;&#26041;&#27861;&#65292;&#22312;&#36830;&#32493;&#30340;Episode&#20043;&#38388;&#23545;&#31574;&#30053;&#24046;&#24322;&#36827;&#34892;&#29109;&#24809;&#32602;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#26377;&#38480;&#26102;&#38388;&#36830;&#32493;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20855;&#26377;&#26410;&#30693;&#28418;&#31227;&#31995;&#25968;&#30340;&#32447;&#24615;&#21160;&#21147;&#23398;&#21463;&#21040;&#22235;&#27425;&#26041;&#32422;&#26463;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work uses the entropy-regularised relaxed stochastic control perspective as a principled framework for designing reinforcement learning (RL) algorithms. Herein agent interacts with the environment by generating noisy controls distributed according to the optimal relaxed policy. The noisy policies on the one hand, explore the space and hence facilitate learning but, on the other hand, introduce bias by assigning a positive probability to non-optimal actions. This exploration-exploitation trade-off is determined by the strength of entropy regularisation. We study algorithms resulting from two entropy regularisation formulations: the exploratory control approach, where entropy is added to the cost objective, and the proximal policy update approach, where entropy penalises policy divergence between consecutive episodes. We focus on the finite horizon continuous-time linear-quadratic (LQ) RL problem, where a linear dynamics with unknown drift coefficients is controlled subject to quadr
&lt;/p&gt;</description></item><item><title>GP-BART&#26159;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#30340;&#26032;&#22411;&#36125;&#21494;&#26031;&#21152;&#27861;&#22238;&#24402;&#26641;&#26041;&#27861;&#65292;&#30456;&#27604;&#26631;&#20934;BART&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#24179;&#28369;&#24615;&#21644;&#26126;&#30830;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#20551;&#35774;&#65292;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#26174;&#31034;&#20986;&#36229;&#36234;&#20256;&#32479;&#24314;&#27169;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.02112</link><description>&lt;p&gt;
GP-BART: &#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#30340;&#26032;&#22411;&#36125;&#21494;&#26031;&#21152;&#27861;&#22238;&#24402;&#26641;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GP-BART: a novel Bayesian additive regression trees approach using Gaussian processes. (arXiv:2204.02112v4 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02112
&lt;/p&gt;
&lt;p&gt;
GP-BART&#26159;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#30340;&#26032;&#22411;&#36125;&#21494;&#26031;&#21152;&#27861;&#22238;&#24402;&#26641;&#26041;&#27861;&#65292;&#30456;&#27604;&#26631;&#20934;BART&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#24179;&#28369;&#24615;&#21644;&#26126;&#30830;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#20551;&#35774;&#65292;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#26174;&#31034;&#20986;&#36229;&#36234;&#20256;&#32479;&#24314;&#27169;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#21152;&#27861;&#22238;&#24402;&#26641; (BART) &#27169;&#22411;&#26159;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#30001;&#20110;&#20854;&#22987;&#32456;&#24378;&#22823;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#24191;&#27867;&#19988;&#25104;&#21151;&#22320;&#20351;&#29992;&#12290;BART&#36890;&#36807;&#19968;&#32452;&#32553;&#20943;&#20808;&#39564;&#23558;&#8220;&#24369;&#8221;&#26641;&#27169;&#22411;&#32452;&#21512;&#36215;&#26469;&#65292;&#20854;&#20013;&#27599;&#20010;&#26641;&#35299;&#37322;&#20102;&#25968;&#25454;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#21464;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;BART&#27169;&#22411;&#20013;&#32570;&#20047;&#24179;&#28369;&#24615;&#24182;&#19988;&#23545;&#35266;&#27979;&#20540;&#20043;&#38388;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#27809;&#26377;&#26126;&#30830;&#20551;&#35774;&#65292;&#36825;&#22312;&#38656;&#35201;&#36825;&#20123;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#39640;&#26031;&#36807;&#31243;&#36125;&#21494;&#26031;&#21152;&#27861;&#22238;&#24402;&#26641; (GP-BART) &#27169;&#22411;&#26159;&#23545;BART&#30340;&#25193;&#23637;&#65292;&#23427;&#36890;&#36807;&#20551;&#35774;&#27599;&#20010;&#32456;&#31471;&#33410;&#28857;&#30340;&#39044;&#27979;&#26381;&#20174;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#24212;&#29992;&#26469;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#21508;&#31181;&#24773;&#22659;&#19979;&#36229;&#36234;&#20102;&#20256;&#32479;&#24314;&#27169;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bayesian additive regression trees (BART) model is an ensemble method extensively and successfully used in regression tasks due to its consistently strong predictive performance and its ability to quantify uncertainty. BART combines "weak" tree models through a set of shrinkage priors, whereby each tree explains a small portion of the variability in the data. However, the lack of smoothness and the absence of an explicit covariance structure over the observations in standard BART can yield poor performance in cases where such assumptions would be necessary. The Gaussian processes Bayesian additive regression trees (GP-BART) model is an extension of BART which addresses this limitation by assuming Gaussian process (GP) priors for the predictions of each terminal node among all trees. The model's effectiveness is demonstrated through applications to simulated and real-world data, surpassing the performance of traditional modeling approaches in various scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#22810;&#32500;&#20989;&#25968;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#20248;&#21270;&#36866;&#24212;&#25968;&#25454;&#30340;&#21487;&#20998;&#31163;&#22522;&#20989;&#25968;&#26500;&#24314;&#36830;&#32493;&#34920;&#31034;&#12290;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#30340;&#38477;&#32500;&#36716;&#25442;&#21644;&#24352;&#37327;&#20998;&#35299;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#20272;&#35745;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#22522;&#20110;&#31895;&#31961;&#24230;&#30340;&#27491;&#21017;&#21270;&#20197;&#24212;&#23545;&#32500;&#24230;&#28798;&#38590;&#12290;</title><link>http://arxiv.org/abs/2107.14728</link><description>&lt;p&gt;
&#20351;&#29992;&#36793;&#38469;&#31215;&#20998;&#22522;&#31995;&#32479;&#30340;&#39640;&#25928;&#22810;&#32500;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Efficient Multidimensional Functional Data Analysis Using Marginal Product Basis Systems. (arXiv:2107.14728v4 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.14728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#22810;&#32500;&#20989;&#25968;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#20248;&#21270;&#36866;&#24212;&#25968;&#25454;&#30340;&#21487;&#20998;&#31163;&#22522;&#20989;&#25968;&#26500;&#24314;&#36830;&#32493;&#34920;&#31034;&#12290;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#30340;&#38477;&#32500;&#36716;&#25442;&#21644;&#24352;&#37327;&#20998;&#35299;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#20272;&#35745;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#22522;&#20110;&#31895;&#31961;&#24230;&#30340;&#27491;&#21017;&#21270;&#20197;&#24212;&#23545;&#32500;&#24230;&#28798;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#20195;&#25968;&#25454;&#38598;&#65292;&#22914;&#31070;&#32463;&#24433;&#20687;&#23398;&#21644;&#22320;&#29702;&#32479;&#35745;&#23398;&#65292;&#26159;&#24352;&#37327;&#20540;&#25968;&#25454;&#30340;&#38543;&#26426;&#26679;&#26412;&#65292;&#21487;&#20197;&#29702;&#35299;&#20026;&#24179;&#28369;&#22810;&#32500;&#38543;&#26426;&#20989;&#25968;&#30340;&#22122;&#22768;&#35266;&#27979;&#12290;&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#22256;&#25200;&#65292;&#38543;&#30528;&#22495;&#30340;&#32500;&#24230;&#22686;&#21152;&#65292;&#24456;&#24555;&#21464;&#24471;&#38590;&#20197;&#22788;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22810;&#32500;&#20989;&#25968;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#36830;&#32493;&#34920;&#31034;&#65292;&#36825;&#20010;&#26694;&#26550;&#23545;&#32500;&#24230;&#28798;&#38590;&#30340;&#20960;&#31181;&#34920;&#29616;&#20855;&#26377;&#20813;&#30123;&#24615;&#12290;&#36825;&#20123;&#34920;&#31034;&#20351;&#29992;&#19968;&#32452;&#21487;&#20998;&#31163;&#30340;&#22522;&#20989;&#25968;&#26500;&#24314;&#65292;&#36825;&#20123;&#22522;&#20989;&#25968;&#34987;&#23450;&#20041;&#20026;&#23545;&#25968;&#25454;&#26368;&#20339;&#36866;&#24212;&#12290;&#25105;&#20204;&#36890;&#36807;&#35266;&#27979;&#25968;&#25454;&#30340;&#31934;&#24515;&#23450;&#20041;&#30340;&#38477;&#32500;&#36716;&#25442;&#30340;&#24352;&#37327;&#20998;&#35299;&#26469;&#39640;&#25928;&#35299;&#20915;&#25152;&#24471;&#21040;&#30340;&#20272;&#35745;&#38382;&#39064;&#12290;&#22522;&#20110;&#31895;&#31961;&#24230;&#30340;&#27491;&#21017;&#21270;&#20351;&#29992;&#19968;&#31867;&#22522;&#20110;&#24494;&#20998;&#31639;&#23376;&#30340;&#24809;&#32602;&#24341;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many modern datasets, from areas such as neuroimaging and geostatistics, come in the form of a random sample of tensor-valued data which can be understood as noisy observations of a smooth multidimensional random function. Most of the traditional techniques from functional data analysis are plagued by the curse of dimensionality and quickly become intractable as the dimension of the domain increases. In this paper, we propose a framework for learning continuous representations from a sample of multidimensional functional data that is immune to several manifestations of the curse. These representations are constructed using a set of separable basis functions that are defined to be optimally adapted to the data. We show that the resulting estimation problem can be solved efficiently by the tensor decomposition of a carefully defined reduction transformation of the observed data. Roughness-based regularization is incorporated using a class of differential operator-based penalties. Relevan
&lt;/p&gt;</description></item></channel></rss>