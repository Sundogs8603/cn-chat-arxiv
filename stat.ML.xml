<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>AV-CPL&#26159;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#30340;&#36830;&#32493;&#20266;&#26631;&#35760;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21516;&#19968;&#20010;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#21644;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#25552;&#39640;&#20102;VSR&#24615;&#33021;&#24182;&#20445;&#25345;&#20102;&#23454;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17395</link><description>&lt;p&gt;
AV-CPL&#65306;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#30340;&#36830;&#32493;&#20266;&#26631;&#35760;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AV-CPL: Continuous Pseudo-Labeling for Audio-Visual Speech Recognition. (arXiv:2309.17395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17395
&lt;/p&gt;
&lt;p&gt;
AV-CPL&#26159;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#30340;&#36830;&#32493;&#20266;&#26631;&#35760;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21516;&#19968;&#20010;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#21644;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#25552;&#39640;&#20102;VSR&#24615;&#33021;&#24182;&#20445;&#25345;&#20102;&#23454;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#21253;&#21547;&#20102;&#25552;&#20379;&#36328;&#27169;&#24577;&#30417;&#30563;&#30340;&#21516;&#27493;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65288;VSR&#65289;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36830;&#32493;&#20266;&#26631;&#35760;&#26041;&#27861;&#65288;AV-CPL&#65289;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65292;&#36825;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#35270;&#39057;&#36827;&#34892;&#25345;&#32493;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65288;AVSR&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20174;&#38899;&#39057;-&#35270;&#35273;&#36755;&#20837;&#20013;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#36827;&#34892;&#35821;&#38899;&#35782;&#21035;&#65292;&#25110;&#32773;&#21482;&#20351;&#29992;&#19968;&#31181;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30456;&#21516;&#30340;&#38899;&#39057;-&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#21644;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#20943;&#36731;&#20102;&#38656;&#35201;&#22806;&#37096;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#29983;&#25104;&#20266;&#26631;&#31614;&#30340;&#38656;&#27714;&#12290;AV-CPL&#22312;LRS3&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;VSR&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23454;&#29992;&#30340;ASR&#21644;AVSR&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio-visual speech contains synchronized audio and visual information that provides cross-modal supervision to learn representations for both automatic speech recognition (ASR) and visual speech recognition (VSR). We introduce continuous pseudo-labeling for audio-visual speech recognition (AV-CPL), a semi-supervised method to train an audio-visual speech recognition (AVSR) model on a combination of labeled and unlabeled videos with continuously regenerated pseudo-labels. Our models are trained for speech recognition from audio-visual inputs and can perform speech recognition using both audio and visual modalities, or only one modality. Our method uses the same audio-visual model for both supervised training and pseudo-label generation, mitigating the need for external speech recognition models to generate pseudo-labels. AV-CPL obtains significant improvements in VSR performance on the LRS3 dataset while maintaining practical ASR and AVSR performance. Finally, using visual-only speech 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#23545;&#25239;&#35266;&#23519;&#27169;&#20223;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31163;&#31574;&#30053;&#23545;&#25239;&#23398;&#20064;&#25216;&#26415;&#21644;&#20174;&#35266;&#23519;&#24207;&#21015;&#20013;&#23398;&#20064;&#30340;&#20195;&#29702;&#29366;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2309.17371</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#20449;&#24687;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36827;&#34892;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adversarial Imitation Learning from Visual Observations using Latent Information. (arXiv:2309.17371v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#23545;&#25239;&#35266;&#23519;&#27169;&#20223;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31163;&#31574;&#30053;&#23545;&#25239;&#23398;&#20064;&#25216;&#26415;&#21644;&#20174;&#35266;&#23519;&#24207;&#21015;&#20013;&#23398;&#20064;&#30340;&#20195;&#29702;&#29366;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#19987;&#27880;&#20110;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#23398;&#20064;&#20195;&#29702;&#21482;&#33021;&#35775;&#38382;&#19987;&#23478;&#30340;&#35270;&#39057;&#20316;&#20026;&#20854;&#21807;&#19968;&#30340;&#23398;&#20064;&#28304;&#12290;&#36825;&#20010;&#26694;&#26550;&#30340;&#25361;&#25112;&#21253;&#25324;&#32570;&#20047;&#19987;&#23478;&#30340;&#21160;&#20316;&#21644;&#29615;&#22659;&#30340;&#23616;&#37096;&#21487;&#35266;&#27979;&#24615;&#65292;&#22240;&#20026;&#22320;&#38754;&#30495;&#23454;&#29366;&#24577;&#21482;&#33021;&#20174;&#20687;&#32032;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#19987;&#23478;&#21644;&#20195;&#29702;&#28508;&#22312;&#29366;&#24577;&#36716;&#25442;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#24230;&#19978;&#24314;&#31435;&#20102;&#23398;&#20064;&#20195;&#29702;&#23376;&#20248;&#24230;&#30340;&#19978;&#30028;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#23545;&#25239;&#35266;&#23519;&#27169;&#20223;&#30340;&#31639;&#27861;&#65292;&#23427;&#23558;&#31163;&#31574;&#30053;&#23545;&#25239;&#23398;&#20064;&#25216;&#26415;&#19982;&#20174;&#35266;&#23519;&#24207;&#21015;&#20013;&#23398;&#20064;&#30340;&#20195;&#29702;&#29366;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#30456;&#32467;&#21512;&#12290;&#22312;&#39640;&#32500;&#36830;&#32493;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the problem of imitation learning from visual observations, where the learning agent has access to videos of experts as its sole learning source. The challenges of this framework include the absence of expert actions and the partial observability of the environment, as the ground-truth states can only be inferred from pixels. To tackle this problem, we first conduct a theoretical analysis of imitation learning in partially observable environments. We establish upper bounds on the suboptimality of the learning agent with respect to the divergence between the expert and the agent latent state-transition distributions. Motivated by this analysis, we introduce an algorithm called Latent Adversarial Imitation from Observations, which combines off-policy adversarial imitation techniques with a learned latent representation of the agent's state from sequences of observations. In experiments on high-dimensional continuous robotic tasks, we show that our algorithm matches state-of-t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#21306;&#22495;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21271;&#27431;&#22320;&#21306;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.17370</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#21306;&#22495;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based Neural Weather Prediction for Limited Area Modeling. (arXiv:2309.17370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17370
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#21306;&#22495;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21271;&#27431;&#22320;&#21306;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#31934;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22825;&#27668;&#39044;&#25253;&#39046;&#22495;&#30340;&#24212;&#29992;&#20026;&#27169;&#25311;&#22823;&#27668;&#30340;&#21487;&#33021;&#24615;&#24102;&#26469;&#20102;&#26032;&#30340;&#21464;&#38761;&#12290;&#22312;&#27668;&#20505;&#21464;&#21270;&#26102;&#20195;&#65292;&#33719;&#21462;&#20687;&#36825;&#26679;&#30340;&#39640;&#20998;&#36776;&#29575;&#39044;&#25253;&#27169;&#22411;&#30340;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#22825;&#27668;&#39044;&#25253;&#26041;&#27861;&#37117;&#26159;&#38024;&#23545;&#20840;&#29699;&#39044;&#27979;&#65292;&#20294;&#22914;&#20309;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#26377;&#38480;&#21306;&#22495;&#24314;&#27169;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#22522;&#20110;&#22270;&#20687;&#30340;&#31070;&#32463;&#32593;&#32476;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#24212;&#29992;&#20110;&#26377;&#38480;&#21306;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#20998;&#23618;&#27169;&#22411;&#25193;&#23637;&#12290;&#36890;&#36807;&#20351;&#29992;&#21271;&#27431;&#22320;&#21306;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of accurate machine learning methods for weather forecasting is creating radical new possibilities for modeling the atmosphere. In the time of climate change, having access to high-resolution forecasts from models like these is also becoming increasingly vital. While most existing Neural Weather Prediction (NeurWP) methods focus on global forecasting, an important question is how these techniques can be applied to limited area modeling. In this work we adapt the graph-based NeurWP approach to the limited area setting and propose a multi-scale hierarchical model extension. Our approach is validated by experiments with a local model for the Nordic region.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#20998;&#20301;&#25968;&#21098;&#20999;&#30340;&#40065;&#26834;&#24615;&#38543;&#26426;&#20248;&#21270;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#20809;&#28369;&#30446;&#26631;&#19988;&#33021;&#23481;&#24525;&#24322;&#24120;&#20540;&#21644;&#23614;&#37325;&#26679;&#26412;&#12290;&#23545;&#20110;&#24378;&#20984;&#30446;&#26631;&#65292;&#36845;&#20195;&#25910;&#25947;&#21040;&#38598;&#20013;&#20998;&#24067;&#24182;&#23548;&#20986;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#27010;&#29575;&#30028;&#12290;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#65292;&#26497;&#38480;&#20998;&#24067;&#23616;&#37096;&#21270;&#22312;&#20302;&#26799;&#24230;&#37051;&#22495;&#19978;&#12290;&#20351;&#29992;&#28378;&#21160;&#20998;&#20301;&#25968;&#23454;&#29616;&#30340;&#31639;&#27861;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17316</link><description>&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#20998;&#20301;&#25968;&#21098;&#20999;&#23454;&#29616;&#40065;&#26834;&#24615;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Stochastic Optimization via Gradient Quantile Clipping. (arXiv:2309.17316v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#20998;&#20301;&#25968;&#21098;&#20999;&#30340;&#40065;&#26834;&#24615;&#38543;&#26426;&#20248;&#21270;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#20809;&#28369;&#30446;&#26631;&#19988;&#33021;&#23481;&#24525;&#24322;&#24120;&#20540;&#21644;&#23614;&#37325;&#26679;&#26412;&#12290;&#23545;&#20110;&#24378;&#20984;&#30446;&#26631;&#65292;&#36845;&#20195;&#25910;&#25947;&#21040;&#38598;&#20013;&#20998;&#24067;&#24182;&#23548;&#20986;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#27010;&#29575;&#30028;&#12290;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#65292;&#26497;&#38480;&#20998;&#24067;&#23616;&#37096;&#21270;&#22312;&#20302;&#26799;&#24230;&#37051;&#22495;&#19978;&#12290;&#20351;&#29992;&#28378;&#21160;&#20998;&#20301;&#25968;&#23454;&#29616;&#30340;&#31639;&#27861;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#33539;&#25968;&#20998;&#20301;&#25968;&#20316;&#20026;&#21098;&#20999;&#38408;&#20540;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477; (SGD)&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#31574;&#30053;&#22312;&#20809;&#28369;&#30446;&#26631;&#65288;&#20984;&#25110;&#38750;&#20984;&#65289;&#19979;&#25552;&#20379;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#23481;&#24525;&#23614;&#37325;&#26679;&#26412;&#65288;&#21253;&#25324;&#26080;&#38480;&#26041;&#24046;&#65289;&#21644;&#25968;&#25454;&#27969;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#31867;&#20284;&#20110; Huber &#27745;&#26579;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#25968;&#23398;&#20998;&#26512;&#21033;&#29992;&#20102;&#24658;&#23450;&#27493;&#38271;&#30340; SGD &#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#20197;&#29420;&#29305;&#30340;&#26041;&#24335;&#22788;&#29702;&#21098;&#20999;&#24341;&#20837;&#30340;&#20559;&#24046;&#12290;&#23545;&#20110;&#24378;&#20984;&#30446;&#26631;&#65292;&#25105;&#20204;&#35777;&#26126;&#36845;&#20195;&#25910;&#25947;&#21040;&#19968;&#20010;&#38598;&#20013;&#20998;&#24067;&#65292;&#24182;&#23548;&#20986;&#20102;&#26368;&#32456;&#20272;&#35745;&#35823;&#24046;&#30340;&#39640;&#27010;&#29575;&#30028;&#12290;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#26497;&#38480;&#20998;&#24067;&#23616;&#37096;&#21270;&#22312;&#20302;&#26799;&#24230;&#37051;&#22495;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28378;&#21160;&#20998;&#20301;&#25968;&#23454;&#29616;&#27492;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a clipping strategy for Stochastic Gradient Descent (SGD) which uses quantiles of the gradient norm as clipping thresholds. We prove that this new strategy provides a robust and efficient optimization algorithm for smooth objectives (convex or non-convex), that tolerates heavy-tailed samples (including infinite variance) and a fraction of outliers in the data stream akin to Huber contamination. Our mathematical analysis leverages the connection between constant step size SGD and Markov chains and handles the bias introduced by clipping in an original way. For strongly convex objectives, we prove that the iteration converges to a concentrated distribution and derive high probability bounds on the final estimation error. In the non-convex case, we prove that the limit distribution is localized on a neighborhood with low gradient. We propose an implementation of this algorithm using rolling quantiles which leads to a highly efficient optimization procedure with strong robustn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#35299;&#37322;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20854;&#22312;&#35757;&#32451;&#31163;&#25955;&#27169;&#24335;&#26102;&#19982;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#33021;&#37327;&#20989;&#25968;&#31561;&#25928;&#12290;&#36825;&#31181;&#31561;&#25928;&#24615;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#30417;&#30563;&#35757;&#32451;&#35299;&#37322;&#20026;&#22312;&#26435;&#37325;&#32467;&#26500;&#20013;&#32534;&#30721;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#20851;&#32852;&#21160;&#21147;&#23398;&#30340;&#31361;&#35302;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.17290</link><description>&lt;p&gt;
&#25628;&#32034;&#20998;&#25955;&#30340;&#35760;&#24518;&#65306;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#26159;&#20851;&#32852;&#35760;&#24518;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
In search of dispersed memories: Generative diffusion models are associative memory networks. (arXiv:2309.17290v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#35299;&#37322;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20854;&#22312;&#35757;&#32451;&#31163;&#25955;&#27169;&#24335;&#26102;&#19982;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#33021;&#37327;&#20989;&#25968;&#31561;&#25928;&#12290;&#36825;&#31181;&#31561;&#25928;&#24615;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#30417;&#30563;&#35757;&#32451;&#35299;&#37322;&#20026;&#22312;&#26435;&#37325;&#32467;&#26500;&#20013;&#32534;&#30721;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#20851;&#32852;&#21160;&#21147;&#23398;&#30340;&#31361;&#35302;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hopfield&#32593;&#32476;&#34987;&#24191;&#27867;&#29992;&#20316;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#31616;&#21270;&#29702;&#35770;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#29289;&#20851;&#32852;&#35760;&#24518;&#12290;&#21407;&#22987;&#30340;Hopfield&#32593;&#32476;&#36890;&#36807;&#32534;&#30721;&#20108;&#20803;&#20851;&#32852;&#27169;&#24335;&#26469;&#23384;&#20648;&#35760;&#24518;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31181;&#31216;&#20026;Hebbian&#23398;&#20064;&#35268;&#21017;&#30340;&#31361;&#35302;&#23398;&#20064;&#26426;&#21046;&#12290;&#29616;&#20195;&#30340;Hopfield&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#23454;&#29616;&#25351;&#25968;&#32423;&#23481;&#37327;&#25193;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#27169;&#22411;&#30340;&#33021;&#37327;&#20989;&#25968;&#19981;&#33021;&#30452;&#25509;&#21387;&#32553;&#20026;&#20108;&#20803;&#31361;&#35302;&#32806;&#21512;&#65292;&#24182;&#19988;&#20063;&#19981;&#33021;&#30452;&#25509;&#25552;&#20379;&#26032;&#30340;&#31361;&#35302;&#23398;&#20064;&#35268;&#21017;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#31163;&#25955;&#27169;&#24335;&#26102;&#65292;&#23427;&#20204;&#30340;&#33021;&#37327;&#20989;&#25968;&#19982;&#29616;&#20195;&#30340;Hopfield&#32593;&#32476;&#30456;&#31561;&#12290;&#36825;&#31181;&#31561;&#20215;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#30417;&#30563;&#35757;&#32451;&#35299;&#37322;&#20026;&#22312;&#26435;&#37325;&#32467;&#26500;&#20013;&#32534;&#30721;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#20851;&#32852;&#21160;&#21147;&#23398;&#30340;&#31361;&#35302;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hopfield networks are widely used in neuroscience as simplified theoretical models of biological associative memory. The original Hopfield networks store memories by encoding patterns of binary associations, which result in a synaptic learning mechanism known as Hebbian learning rule. Modern Hopfield networks can achieve exponential capacity scaling by using highly non-linear energy functions. However, the energy function of these newer models cannot be straightforwardly compressed into binary synaptic couplings and it does not directly provide new synaptic learning rules. In this work we show that generative diffusion models can be interpreted as energy-based models and that, when trained on discrete patterns, their energy function is equivalent to that of modern Hopfield networks. This equivalence allows us to interpret the supervised training of diffusion models as a synaptic learning process that encodes the associative dynamics of a modern Hopfield network in the weight structure 
&lt;/p&gt;</description></item><item><title>&#22810;&#37325;&#27835;&#30103;&#21644;&#22810;&#20010;&#32467;&#26524;&#30340;&#24182;&#34892;&#30740;&#31350;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20013;&#21487;&#20197;&#20114;&#30456;&#21327;&#21161;&#23454;&#29616;&#22240;&#26524;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.17283</link><description>&lt;p&gt;
&#22810;&#37325;&#27835;&#30103;&#21644;&#22810;&#20010;&#32467;&#26524;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20013;&#30340;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Blessings of Multiple Treatments and Outcomes in Treatment Effect Estimation. (arXiv:2309.17283v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17283
&lt;/p&gt;
&lt;p&gt;
&#22810;&#37325;&#27835;&#30103;&#21644;&#22810;&#20010;&#32467;&#26524;&#30340;&#24182;&#34892;&#30740;&#31350;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20013;&#21487;&#20197;&#20114;&#30456;&#21327;&#21161;&#23454;&#29616;&#22240;&#26524;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#22240;&#26524;&#25928;&#24212;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#21033;&#29992;&#20195;&#29702;&#21464;&#37327;&#25110;&#22810;&#37325;&#27835;&#30103;&#26469;&#35843;&#25972;&#28151;&#26434;&#20559;&#24046;&#12290;&#23588;&#20854;&#26159;&#21518;&#19968;&#31181;&#26041;&#27861;&#23558;&#21333;&#19968;&#32467;&#26524;&#30340;&#24433;&#21709;&#24402;&#22240;&#20110;&#22810;&#37325;&#27835;&#30103;&#65292;&#21487;&#20197;&#20272;&#35745;&#28151;&#26434;&#25511;&#21046;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#32467;&#26524;&#65292;&#32780;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#26356;&#24863;&#20852;&#36259;&#30340;&#26159;&#30740;&#31350;&#23545;&#22810;&#20010;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#36890;&#24120;&#19982;&#22810;&#20010;&#27835;&#30103;&#30456;&#20851;&#12290;&#20363;&#22914;&#65292;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#20013;&#65292;&#21307;&#30103;&#25552;&#20379;&#32773;&#35780;&#20272;&#27835;&#30103;&#23545;&#22810;&#20010;&#20581;&#24247;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#36866;&#24212;&#36825;&#20123;&#22330;&#26223;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#21363;&#22810;&#37325;&#27835;&#30103;&#21644;&#22810;&#20010;&#32467;&#26524;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#28041;&#21450;&#22810;&#20010;&#32467;&#26524;&#30340;&#24182;&#34892;&#30740;&#31350;&#21487;&#20197;&#20114;&#30456;&#21327;&#21161;&#23454;&#29616;&#22240;&#26524;&#35782;&#21035;&#65292;&#21363;
&lt;/p&gt;
&lt;p&gt;
Assessing causal effects in the presence of unobserved confounding is a challenging problem. Existing studies leveraged proxy variables or multiple treatments to adjust for the confounding bias. In particular, the latter approach attributes the impact on a single outcome to multiple treatments, allowing estimating latent variables for confounding control. Nevertheless, these methods primarily focus on a single outcome, whereas in many real-world scenarios, there is greater interest in studying the effects on multiple outcomes. Besides, these outcomes are often coupled with multiple treatments. Examples include the intensive care unit (ICU), where health providers evaluate the effectiveness of therapies on multiple health indicators. To accommodate these scenarios, we consider a new setting dubbed as multiple treatments and multiple outcomes. We then show that parallel studies of multiple outcomes involved in this setting can assist each other in causal identification, in the sense that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#31561;&#20215;&#30830;&#23450;&#27861;&#65292;&#22312;&#25552;&#20379;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17262</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Estimation and Inference in Distributional Reinforcement Learning. (arXiv:2309.17262v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#31561;&#20215;&#30830;&#23450;&#27861;&#65292;&#22312;&#25552;&#20379;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32479;&#35745;&#25928;&#29575;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#65292;&#26088;&#22312;&#20272;&#35745;&#30001;&#32473;&#23450;&#31574;&#30053;&#960;&#33719;&#24471;&#30340;&#38543;&#26426;&#22238;&#25253;&#30340;&#23436;&#25972;&#20998;&#24067;&#65288;&#34920;&#31034;&#20026;&#951;^&#960;&#65289;&#12290;&#22312;&#25552;&#20379;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#31561;&#20215;&#30830;&#23450;&#27861;&#26500;&#36896;&#20102;&#20272;&#35745;&#22120;&#951;^&#960;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20855;&#26377;&#22823;&#23567;&#20026;O(|S||A|/(&#949;^(2p)(1-&#947;)^(2p+2)))&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#20445;&#35777;&#20272;&#35745;&#22120;&#951;^&#960;&#21644;&#30495;&#23454;&#20998;&#24067;&#951;^&#960;&#20043;&#38388;&#30340;p-Wasserstein&#36317;&#31163;&#23567;&#20110;&#949;&#30340;&#27010;&#29575;&#24456;&#39640;&#12290;&#36825;&#24847;&#21619;&#30528;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#21487;&#20197;&#20197;&#39640;&#25928;&#21033;&#29992;&#26679;&#26412;&#30340;&#26041;&#24335;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#36890;&#36807;&#20855;&#26377;&#22823;&#23567;&#20026;O(|S||A|/(&#949;^2(1-&#947;)^4))&#30340;&#25968;&#25454;&#38598;&#23601;&#36275;&#20197;&#30830;&#20445;Kolmogorov&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study distributional reinforcement learning from the perspective of statistical efficiency.  We investigate distributional policy evaluation, aiming to estimate the complete distribution of the random return (denoted $\eta^\pi$) attained by a given policy $\pi$.  We use the certainty-equivalence method to construct our estimator $\hat\eta^\pi$, given a generative model is available.  We show that in this circumstance we need a dataset of size $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2p}(1-\gamma)^{2p+2}}\right)$ to guarantee a $p$-Wasserstein metric between $\hat\eta^\pi$ and $\eta^\pi$ is less than $\epsilon$ with high probability.  This implies the distributional policy evaluation problem can be solved with sample efficiency.  Also, we show that under different mild assumptions a dataset of size $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2}(1-\gamma)^{4}}\right)$ suffices to ensure the Kolmogorov metric and total variation m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#24403;&#21069;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#24378;&#35843;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#30740;&#31350;&#21457;&#29616;&#26368;&#22909;&#30340;&#39044;&#27979;&#26041;&#27861;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#32463;&#36807;&#24037;&#31243;&#22788;&#29702;&#30340;&#29305;&#24449;&#19982;&#32463;&#20856;&#26041;&#27861;&#32467;&#21512;&#30340;&#25928;&#26524;&#36739;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.17161</link><description>&lt;p&gt;
&#24403;&#21069;&#30340;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Current Methods for Drug Property Prediction in the Real World. (arXiv:2309.17161v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17161
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#24403;&#21069;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#24378;&#35843;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#30740;&#31350;&#21457;&#29616;&#26368;&#22909;&#30340;&#39044;&#27979;&#26041;&#27861;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#32463;&#36807;&#24037;&#31243;&#22788;&#29702;&#30340;&#29305;&#24449;&#19982;&#32463;&#20856;&#26041;&#27861;&#32467;&#21512;&#30340;&#25928;&#26524;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#39044;&#27979;&#33647;&#29289;&#23646;&#24615;&#23545;&#20110;&#22312;&#26114;&#36149;&#30340;&#20020;&#24202;&#35797;&#39564;&#20043;&#21069;&#38477;&#20302;&#39118;&#38505;&#12289;&#24555;&#36895;&#25214;&#21040;&#39640;&#27963;&#24615;&#21270;&#21512;&#29289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#20852;&#36259;&#23548;&#33268;&#20102;&#22810;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21457;&#24067;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20174;&#19994;&#32773;&#26469;&#35828;&#65292;&#19981;&#21516;&#25991;&#20214;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24471;&#20986;&#30340;&#32467;&#35770;&#20063;&#19981;&#26131;&#20110;&#27604;&#36739;&#65292;&#22240;&#27492;&#20173;&#28982;&#19981;&#28165;&#26970;&#21738;&#31181;&#26041;&#27861;&#25110;&#26041;&#27861;&#26368;&#21512;&#36866;&#12290;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#23558;&#20043;&#21069;&#30340;&#35768;&#22810;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#30340;&#30740;&#31350;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#29616;&#26377;&#23646;&#24615;&#31867;&#21035;&#12289;&#25968;&#25454;&#38598;&#21450;&#20854;&#19982;&#19981;&#21516;&#26041;&#27861;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#24378;&#35843;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;&#33647;&#29289;&#24320;&#21457;&#20915;&#31574;&#21608;&#26399;&#20013;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#22909;&#30340;&#26041;&#27861;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#65292;&#32780;&#20351;&#29992;&#32463;&#36807;&#24037;&#31243;&#22788;&#29702;&#30340;&#29305;&#24449;&#19982;&#32463;&#20856;&#26041;&#27861;&#32467;&#21512;&#30340;&#25928;&#26524;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting drug properties is key in drug discovery to enable de-risking of assets before expensive clinical trials, and to find highly active compounds faster. Interest from the Machine Learning community has led to the release of a variety of benchmark datasets and proposed methods. However, it remains unclear for practitioners which method or approach is most suitable, as different papers benchmark on different datasets and methods, leading to varying conclusions that are not easily compared. Our large-scale empirical study links together numerous earlier works on different datasets and methods; thus offering a comprehensive overview of the existing property classes, datasets, and their interactions with different methods. We emphasise the importance of uncertainty quantification and the time and therefore cost of applying these methods in the drug development decision-making cycle. We discover that the best method depends on the dataset, and that engineered features with classical 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24179;&#22343;&#20809;&#28369;&#24230;&#30340;&#26080;&#21442;&#22238;&#24402;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26080;&#20998;&#24067;&#38480;&#21046;&#19979;&#30340;&#32479;&#19968;&#25910;&#25947;&#30028;&#38480;&#21644;&#39640;&#25928;&#26080;&#20559;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.17016</link><description>&lt;p&gt;
&#20855;&#26377;&#24179;&#22343;&#20809;&#28369;&#24230;&#30340;&#39640;&#25928;&#26080;&#20559;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Agnostic Learning with Average Smoothness. (arXiv:2309.17016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24179;&#22343;&#20809;&#28369;&#24230;&#30340;&#26080;&#21442;&#22238;&#24402;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26080;&#20998;&#24067;&#38480;&#21046;&#19979;&#30340;&#32479;&#19968;&#25910;&#25947;&#30028;&#38480;&#21644;&#39640;&#25928;&#26080;&#20559;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#26080;&#20998;&#24067;&#38480;&#21046;&#30340;&#24179;&#22343;&#20809;&#28369;&#24230;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#30001;Ashlagi&#31561;&#20154;&#65288;2021&#65289;&#25552;&#20986;&#65292;&#29992;&#20110;&#34913;&#37327;&#20989;&#25968;&#30456;&#23545;&#20110;&#20219;&#24847;&#26410;&#30693;&#28508;&#22312;&#20998;&#24067;&#30340;"&#26377;&#25928;"&#20809;&#28369;&#24230;&#12290;&#26368;&#36817;&#30340;Hanneke&#31561;&#20154;&#65288;2023&#65289;&#30340;&#30740;&#31350;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#24179;&#22343;&#20809;&#28369;&#20989;&#25968;&#30340;&#32039;&#23494;&#19968;&#33268;&#25910;&#25947;&#30028;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#39640;&#25928;&#21487;&#23454;&#29616;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#30446;&#21069;&#22312;&#26222;&#36941;&#26080;&#20559;&#65288;&#21363;&#26377;&#22122;&#22768;&#65289;&#24773;&#20917;&#19979;&#23578;&#32570;&#20047;&#31867;&#20284;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23436;&#20840;&#22635;&#34917;&#20102;&#36825;&#20123;&#24046;&#36317;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;&#26080;&#20559;&#35774;&#32622;&#20013;&#30340;&#24179;&#22343;&#20809;&#28369;&#31867;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#20998;&#24067;&#19968;&#33268;&#25910;&#25947;&#30028;&#38480;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#25152;&#24471;&#21040;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#19968;&#20010;&#20855;&#26377;&#39640;&#25928;&#26080;&#20559;&#23398;&#20064;&#31639;&#27861;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20197;&#25968;&#25454;&#30340;&#20869;&#22312;&#20960;&#20309;&#24418;&#29366;&#20026;&#22522;&#30784;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#20840;&#26377;&#30028;&#24230;&#37327;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#36817;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#33719;&#24471;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distribution-free nonparametric regression following a notion of average smoothness initiated by Ashlagi et al. (2021), which measures the "effective" smoothness of a function with respect to an arbitrary unknown underlying distribution. While the recent work of Hanneke et al. (2023) established tight uniform convergence bounds for average-smooth functions in the realizable case and provided a computationally efficient realizable learning algorithm, both of these results currently lack analogs in the general agnostic (i.e. noisy) case.  In this work, we fully close these gaps. First, we provide a distribution-free uniform convergence bound for average-smoothness classes in the agnostic setting. Second, we match the derived sample complexity with a computationally efficient agnostic learning algorithm. Our results, which are stated in terms of the intrinsic geometry of the data and hold over any totally bounded metric space, show that the guarantees recently obtained for realiz
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#36890;&#36807;&#32479;&#35745;&#29289;&#29702;&#30340;&#35270;&#35282;&#26469;&#25506;&#32034;&#31070;&#32463;&#20449;&#24687;&#22788;&#29702;&#30340;&#35838;&#31243;&#35762;&#20041;&#12290;&#20027;&#35201;&#20869;&#23481;&#21253;&#25324;&#36125;&#21494;&#26031;&#25512;&#26029;&#19982;&#23398;&#20064;&#27867;&#21270;&#30340;Gibbs&#25551;&#36848;&#30340;&#32852;&#31995;&#65292;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20316;&#20026;&#32972;&#26223;&#21076;&#38500;&#27861;&#30340;&#21487;&#25511;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#21450;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.17006</link><description>&lt;p&gt;
&#32479;&#35745;&#29289;&#29702;&#12289;&#36125;&#21494;&#26031;&#25512;&#26029;&#19982;&#31070;&#32463;&#20449;&#24687;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Statistical physics, Bayesian inference and neural information processing. (arXiv:2309.17006v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17006
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#36890;&#36807;&#32479;&#35745;&#29289;&#29702;&#30340;&#35270;&#35282;&#26469;&#25506;&#32034;&#31070;&#32463;&#20449;&#24687;&#22788;&#29702;&#30340;&#35838;&#31243;&#35762;&#20041;&#12290;&#20027;&#35201;&#20869;&#23481;&#21253;&#25324;&#36125;&#21494;&#26031;&#25512;&#26029;&#19982;&#23398;&#20064;&#27867;&#21270;&#30340;Gibbs&#25551;&#36848;&#30340;&#32852;&#31995;&#65292;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20316;&#20026;&#32972;&#26223;&#21076;&#38500;&#27861;&#30340;&#21487;&#25511;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#21450;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;&#32479;&#35745;&#29289;&#29702;&#26426;&#22120;&#23398;&#20064;&#12299;&#35838;&#31243;&#20013;Sara A. Solla&#25945;&#25480;&#35762;&#24231;&#30340;&#35762;&#20041;&#12290;&#35762;&#20041;&#36890;&#36807;&#32479;&#35745;&#29289;&#29702;&#30340;&#35270;&#35282;&#25506;&#35752;&#31070;&#32463;&#20449;&#24687;&#22788;&#29702;&#12290;&#20869;&#23481;&#21253;&#25324;&#36125;&#21494;&#26031;&#25512;&#26029;&#21450;&#20854;&#19982;&#23398;&#20064;&#21644;&#27867;&#21270;&#30340;Gibbs&#25551;&#36848;&#30340;&#20851;&#31995;&#65292;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20316;&#20026;&#32972;&#26223;&#21076;&#38500;&#27861;&#30340;&#21487;&#25511;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#21450;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lecture notes from the course given by Professor Sara A. Solla at the Les Houches summer school on "Statistical physics of Machine Learning". The notes discuss neural information processing through the lens of Statistical Physics. Contents include Bayesian inference and its connection to a Gibbs description of learning and generalization, Generalized Linear Models as a controlled alternative to backpropagation through time, and linear and non-linear techniques for dimensionality reduction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#23494;&#38598;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#25152;&#26377;&#21464;&#37327;&#20026;&#38646;&#30340;&#23616;&#37096;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16965</link><description>&lt;p&gt;
&#25511;&#21046;&#32452;&#21512;&#20248;&#21270;&#30340;&#36830;&#32493;&#25918;&#26494;
&lt;/p&gt;
&lt;p&gt;
Controlling Continuous Relaxation for Combinatorial Optimization. (arXiv:2309.16965v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#23494;&#38598;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#25152;&#26377;&#21464;&#37327;&#20026;&#38646;&#30340;&#23616;&#37096;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#38382;&#39064;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25214;&#21040;&#36817;&#20284;&#35299;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;GNN&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#22312;&#22823;&#35268;&#27169;CO&#38382;&#39064;&#19978;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#30456;&#23545;&#23494;&#38598;&#22270;&#19978;&#30340;CO&#38382;&#39064;&#65292;&#36138;&#23146;&#31639;&#27861;&#30340;&#24615;&#33021;&#24694;&#21270;&#65292;&#20294;&#23545;&#20110;PI-GNN&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#21364;&#27809;&#26377;&#22826;&#22810;&#35752;&#35770;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PI-GNN&#27714;&#35299;&#22120;&#37319;&#29992;&#20102;&#25918;&#26494;&#31574;&#30053;&#65292;&#23398;&#20064;&#21518;&#38656;&#35201;&#20174;&#36830;&#32493;&#31354;&#38388;&#20154;&#24037;&#36716;&#25442;&#22238;&#21407;&#22987;&#31163;&#25955;&#31354;&#38388;&#65292;&#21487;&#33021;&#20250;&#30772;&#22351;&#35299;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23494;&#38598;&#22270;&#19978;&#30340;CO&#38382;&#39064;&#30340;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#23616;&#37096;&#35299;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#25152;&#26377;&#21464;&#37327;&#37117;&#20026;&#38646;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in combinatorial optimization (CO) problems emphasize the potential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN) solver, which finds approximate solutions through unsupervised learning, has attracted significant attention for large-scale CO problems. Nevertheless, there has been limited discussion on the performance of the PI-GNN solver for CO problems on relatively dense graphs where the performance of greedy algorithms worsens. In addition, since the PI-GNN solver employs a relaxation strategy, an artificial transformation from the continuous space back to the original discrete space is necessary after learning, potentially undermining the robustness of the solutions. This paper numerically demonstrates that the PI-GNN solver can be trapped in a local solution, where all variables are zero, in the early stage of learning for CO problems on the dense graphs. Then, we address these problems by controlling the continuity and discreteness of rela
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#32447;&#24615;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#12289;LightGBM&#21644;MLP&#31070;&#32463;&#32593;&#32476;&#20116;&#31181;&#27169;&#22411;&#22312;&#20304;&#27835;&#20122;&#24030;&#39044;&#27979;&#27700;&#36136;pH&#20540;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;LightGBM&#34920;&#29616;&#26368;&#22909;&#12290;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#20248;&#21183;&#26174;&#33879;&#65292;&#32780;MLP&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#24449;&#32553;&#25918;&#25935;&#24863;&#12290;&#21516;&#26102;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#19982;&#21407;&#30740;&#31350;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2309.16951</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#27700;&#36136;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Water quality prediction using machine learning and neural network approaches. (arXiv:2309.16951v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#32447;&#24615;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#12289;LightGBM&#21644;MLP&#31070;&#32463;&#32593;&#32476;&#20116;&#31181;&#27169;&#22411;&#22312;&#20304;&#27835;&#20122;&#24030;&#39044;&#27979;&#27700;&#36136;pH&#20540;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;LightGBM&#34920;&#29616;&#26368;&#22909;&#12290;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#20248;&#21183;&#26174;&#33879;&#65292;&#32780;MLP&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#24449;&#32553;&#25918;&#25935;&#24863;&#12290;&#21516;&#26102;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#19982;&#21407;&#30740;&#31350;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#36164;&#28304;&#26159;&#20154;&#31867;&#29983;&#35745;&#21644;&#32463;&#27982;&#36827;&#27493;&#30340;&#22522;&#30784;&#65292;&#19982;&#20844;&#20849;&#20581;&#24247;&#21644;&#29615;&#22659;&#31119;&#31049;&#26377;&#30528;&#20869;&#22312;&#30340;&#32852;&#31995;&#12290;&#20934;&#30830;&#39044;&#27979;&#27700;&#36136;&#26159;&#25913;&#21892;&#27700;&#36164;&#28304;&#31649;&#29702;&#21644;&#23545;&#25239;&#27745;&#26579;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#22810;&#31181;&#24615;&#33021;&#25351;&#26631;&#65292;&#35780;&#20272;&#20102;&#20116;&#31181;&#19981;&#21516;&#27169;&#22411;&#65288;&#32447;&#24615;&#22238;&#24402;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;XGBoost&#65292;LightGBM&#21644;MLP&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#32654;&#22269;&#20304;&#27835;&#20122;&#24030;&#39044;&#27979;pH&#20540;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;LightGBM&#22312;&#25152;&#26377;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#31934;&#24230;&#12290;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#20984;&#26174;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;MLP&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#23545;&#29305;&#24449;&#32553;&#25918;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#36824;&#35814;&#32454;&#38416;&#36848;&#24182;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#31354;&#38388;&#32771;&#34385;&#22240;&#32032;&#26041;&#38754;&#19982;&#21407;&#30740;&#31350;&#30456;&#27604;&#25152;&#21462;&#24471;&#30340;&#20248;&#36234;&#24615;&#33021;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Water resources serve as the cornerstone of human livelihoods and economic progress, with intrinsic links to both public health and environmental well-being. The accurate prediction of water quality stands as a pivotal factor in enhancing water resource management and combating pollution. This research, employing diverse performance metrics, assesses the efficacy of five distinct models, namely, linear regression, Random Forest, XGBoost, LightGBM, and MLP neural network, in forecasting pH values within Georgia, USA. Concurrently, LightGBM attains the highest average precision among all models examined. Tree-based models underscore their supremacy in addressing regression challenges. Furthermore, the performance of MLP neural network is sensitive to feature scaling. Additionally, we expound upon and dissect the reasons behind the superior precision of the machine learning models when they are compared to the original study, which factors in time dependencies and spatial considerations. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#31216;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#65292;&#24341;&#20837;&#30340;&#27599;&#20010;&#38236;&#20687;&#23545;&#31216;&#24615;&#37117;&#20250;&#23548;&#33268;&#19968;&#31181;&#32467;&#26500;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#20110;&#23454;&#29616;&#31232;&#30095;&#24615;&#12289;&#20302;&#31209;&#24615;&#21644;&#21516;&#36136;&#38598;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#32593;&#32476;&#22609;&#24615;&#20007;&#22833;&#21644;&#23849;&#28291;&#29616;&#35937;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.16932</link><description>&lt;p&gt;
&#23545;&#31216;&#24615;&#23548;&#33268;&#23398;&#20064;&#30340;&#32467;&#26500;&#24615;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Symmetry Leads to Structured Constraint of Learning. (arXiv:2309.16932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#31216;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#65292;&#24341;&#20837;&#30340;&#27599;&#20010;&#38236;&#20687;&#23545;&#31216;&#24615;&#37117;&#20250;&#23548;&#33268;&#19968;&#31181;&#32467;&#26500;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#20110;&#23454;&#29616;&#31232;&#30095;&#24615;&#12289;&#20302;&#31209;&#24615;&#21644;&#21516;&#36136;&#38598;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#32593;&#32476;&#22609;&#24615;&#20007;&#22833;&#21644;&#23849;&#28291;&#29616;&#35937;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24120;&#35265;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#23545;&#31216;&#24615;&#22312;&#24403;&#20195;&#31070;&#32463;&#32593;&#32476;&#20013;&#24191;&#27867;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#31216;&#24615;&#23545;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#27599;&#20010;&#38236;&#20687;&#23545;&#31216;&#24615;&#37117;&#20250;&#23548;&#33268;&#19968;&#31181;&#32467;&#26500;&#24615;&#32422;&#26463;&#65292;&#24403;&#26435;&#37325;&#34928;&#20943;&#25110;&#26799;&#24230;&#22122;&#22768;&#36739;&#22823;&#26102;&#65292;&#36825;&#31181;&#32422;&#26463;&#23558;&#25104;&#20026;&#39318;&#36873;&#35299;&#12290;&#20316;&#20026;&#30452;&#25509;&#25512;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#23548;&#33268;&#31232;&#30095;&#24615;&#65292;&#26059;&#36716;&#23545;&#31216;&#24615;&#23548;&#33268;&#20302;&#31209;&#24615;&#65292;&#32622;&#25442;&#23545;&#31216;&#24615;&#23548;&#33268;&#21516;&#36136;&#38598;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29702;&#35770;&#26694;&#26550;&#21487;&#20197;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#21644;&#21508;&#31181;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#21033;&#29992;&#23545;&#31216;&#24615;&#35774;&#35745;&#21487;&#24494;&#20998;&#23454;&#26045;&#30828;&#24615;&#32422;&#26463;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to common architecture designs, symmetries exist extensively in contemporary neural networks. In this work, we unveil the importance of the loss function symmetries in affecting, if not deciding, the learning behavior of machine learning models. We prove that every mirror symmetry of the loss function leads to a structured constraint, which becomes a favored solution when either the weight decay or gradient noise is large. As direct corollaries, we show that rescaling symmetry leads to sparsity, rotation symmetry leads to low rankness, and permutation symmetry leads to homogeneous ensembling. Then, we show that the theoretical framework can explain the loss of plasticity and various collapse phenomena in neural networks and suggest how symmetries can be used to design algorithms to enforce hard constraints in a differentiable way.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#37319;&#29992;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#26469;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.16883</link><description>&lt;p&gt;
&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;Lipschitz-&#26041;&#24046;-&#36793;&#30028;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing. (arXiv:2309.16883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#37319;&#29992;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#26469;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#22122;&#22768;&#36755;&#20837;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#20854;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#30340;&#38459;&#30861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35748;&#35777;&#21322;&#24452;&#26159;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#20855;&#26377;&#36275;&#22815;&#35748;&#35777;&#21322;&#24452;&#30340;&#39640;&#25928;&#20998;&#31867;&#22120;&#21602;&#65311;&#38543;&#26426;&#24179;&#28369;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#27880;&#20837;&#22122;&#22768;&#26469;&#33719;&#24471;&#24179;&#28369;&#19988;&#26356;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;&#21478;&#22806;&#20004;&#20010;&#37325;&#35201;&#23646;&#24615;&#65292;&#21363;&#20854;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#22522;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#23545;&#24179;&#28369;&#20998;&#31867;&#22120;&#21644;&#32463;&#39564;&#26041;&#24046;&#30340;&#21452;&#37325;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#65292;&#20197;&#20415;&#36890;&#36807;Bernst&#30340;&#26041;&#24046;-&#36793;&#30028;&#26435;&#34913;&#26469;&#21033;&#29992;&#22522;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius is in this context a crucial indicator of the robustness of models. However how to design an efficient classifier with a sufficient certified radius? Randomized smoothing provides a promising framework by relying on noise injection in inputs to obtain a smoothed and more robust classifier. In this paper, we first show that the variance introduced by randomized smoothing closely interacts with two other important properties of the classifier, i.e. its Lipschitz constant and margin. More precisely, our work emphasizes the dual impact of the Lipschitz constant of the base classifier, on both the smoothed classifier and the empirical variance. Moreover, to increase the certified robust radius, we introduce a different simplex projection technique for the base classifier to leverage the variance-margin trade-off thanks to Bernst
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#26500;&#24314;&#20102;TLRC&#65292;&#24182;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.16858</link><description>&lt;p&gt;
Transductive Learning&#30340;&#23574;&#38160;&#27867;&#21270;&#65306;&#19968;&#31181;Transductive Local Rademacher Complexity&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach. (arXiv:2309.16858v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16858
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#26500;&#24314;&#20102;TLRC&#65292;&#24182;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#20256;&#32479;&#30340;local rademacher complexity (LRC)&#30340;&#24605;&#24819;&#25193;&#23637;&#21040;&#20102;transductive&#35774;&#32622;&#20013;&#65292;&#30456;&#23545;&#20110;&#20856;&#22411;&#30340;LRC&#26041;&#27861;&#22312;&#24402;&#32435;&#35774;&#32622;&#20013;&#30340;&#20998;&#26512;&#26377;&#20102;&#30456;&#24403;&#22823;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Rademacher complex&#30340;&#23616;&#37096;&#21270;&#24037;&#20855;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;transductive learning&#38382;&#39064;&#65292;&#24182;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#24471;&#21040;&#20102;&#23574;&#38160;&#30340;&#30028;&#38480;&#12290;&#19982;LRC&#30340;&#21457;&#23637;&#31867;&#20284;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#29420;&#31435;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#24320;&#22987;&#26500;&#24314;TLRC&#65292;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We introduce a new tool, Transductive Local Rademacher Complexity (TLRC), to analyze the generalization performance of transductive learning methods and motivate new transductive learning algorithms. Our work extends the idea of the popular Local Rademacher Complexity (LRC) to the transductive setting with considerable changes compared to the analysis of typical LRC methods in the inductive setting. We present a localized version of Rademacher complexity based tool wihch can be applied to various transductive learning problems and gain sharp bounds under proper conditions. Similar to the development of LRC, we build TLRC by starting from a sharp concentration inequality for independent variables with variance information. The prediction function class of a transductive learning model is then divided into pieces with a sub-root function being the upper bound for the Rademacher complexity of each piece, and the variance of all the functions in each piece is limited. A carefully designed 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#31561;&#25928;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#33719;&#24471;&#30340;&#21442;&#25968;&#21487;&#20197;&#23450;&#20041;&#19968;&#32452;&#26368;&#20248;&#38750;&#32447;&#24615;&#24615;&#65292;&#22914;&#20108;&#38454;&#22810;&#39033;&#24335;&#21644;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#36825;&#20123;&#38750;&#32447;&#24615;&#24615;&#33021;&#20248;&#21270;&#20102;&#27867;&#21270;&#24615;&#33021;&#65292;&#26080;&#35770;&#20854;&#23454;&#38469;&#24418;&#24335;&#22914;&#20309;&#65292;&#23545;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#22343;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.16846</link><description>&lt;p&gt;
&#26368;&#20248;&#38750;&#32447;&#24615;&#24615;&#33021;&#25913;&#36827;&#38543;&#26426;&#29305;&#24449;&#30340;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Optimal Nonlinearities Improve Generalization Performance of Random Features. (arXiv:2309.16846v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16846
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#31561;&#25928;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#33719;&#24471;&#30340;&#21442;&#25968;&#21487;&#20197;&#23450;&#20041;&#19968;&#32452;&#26368;&#20248;&#38750;&#32447;&#24615;&#24615;&#65292;&#22914;&#20108;&#38454;&#22810;&#39033;&#24335;&#21644;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#36825;&#20123;&#38750;&#32447;&#24615;&#24615;&#33021;&#20248;&#21270;&#20102;&#27867;&#21270;&#24615;&#33021;&#65292;&#26080;&#35770;&#20854;&#23454;&#38469;&#24418;&#24335;&#22914;&#20309;&#65292;&#23545;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#22343;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#19982;&#39640;&#26031;&#27169;&#22411;&#28176;&#36827;&#31561;&#25928;&#12290;&#31561;&#25928;&#27169;&#22411;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#28608;&#27963;&#20989;&#25968;&#21457;&#25381;&#30340;&#37325;&#35201;&#20294;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#31561;&#25928;&#27169;&#22411;&#30340;&#8220;&#21442;&#25968;&#8221;&#65292;&#20197;&#23454;&#29616;&#23545;&#32473;&#23450;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#39640;&#26031;&#27169;&#22411;&#33719;&#21462;&#30340;&#21442;&#25968;&#20351;&#25105;&#20204;&#33021;&#22815;&#23450;&#20041;&#19968;&#32452;&#26368;&#20248;&#38750;&#32447;&#24615;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#32452;&#26368;&#20248;&#38750;&#32447;&#24615;&#24615;&#30340;&#20004;&#20010;&#31034;&#20363;&#31867;&#65292;&#20363;&#22914;&#20108;&#38454;&#22810;&#39033;&#24335;&#21644;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#36825;&#20123;&#20989;&#25968;&#34987;&#20248;&#21270;&#20197;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#65292;&#26080;&#35770;&#20854;&#23454;&#38469;&#24418;&#24335;&#22914;&#20309;&#12290;&#25105;&#20204;&#23545;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21253;&#25324;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#65288;&#22914;CIFAR10&#65289;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#39564;&#35777;&#20102;&#20248;&#21270;&#30340;&#38750;&#32447;&#24615;&#24615;&#33021;&#20248;&#20110;wid&#12290;
&lt;/p&gt;
&lt;p&gt;
Random feature model with a nonlinear activation function has been shown to perform asymptotically equivalent to a Gaussian model in terms of training and generalization errors. Analysis of the equivalent model reveals an important yet not fully understood role played by the activation function. To address this issue, we study the "parameters" of the equivalent model to achieve improved generalization performance for a given supervised learning problem. We show that acquired parameters from the Gaussian model enable us to define a set of optimal nonlinearities. We provide two example classes from this set, e.g., second-order polynomial and piecewise linear functions. These functions are optimized to improve generalization performance regardless of the actual form. We experiment with regression and classification problems, including synthetic and real (e.g., CIFAR10) data. Our numerical results validate that the optimized nonlinearities achieve better generalization performance than wid
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#20013;&#36827;&#34892;&#32463;&#39564;&#36125;&#21494;&#26031;&#20272;&#35745;&#30340;&#22343;&#20540;&#22330;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#32463;&#39564;&#36125;&#21494;&#26031;&#36924;&#36817;&#20808;&#39564;&#20998;&#24067;&#24182;&#24314;&#31435;&#20102;&#28176;&#36817;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35745;&#31639;&#19978;&#21487;&#34892;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2309.16843</link><description>&lt;p&gt;
&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#20013;&#32463;&#39564;&#36125;&#21494;&#26031;&#20272;&#35745;&#30340;&#22343;&#20540;&#22330;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Mean Field Approach to Empirical Bayes Estimation in High-dimensional Linear Regression. (arXiv:2309.16843v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16843
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#20013;&#36827;&#34892;&#32463;&#39564;&#36125;&#21494;&#26031;&#20272;&#35745;&#30340;&#22343;&#20540;&#22330;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#32463;&#39564;&#36125;&#21494;&#26031;&#36924;&#36817;&#20808;&#39564;&#20998;&#24067;&#24182;&#24314;&#31435;&#20102;&#28176;&#36817;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35745;&#31639;&#19978;&#21487;&#34892;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#32463;&#39564;&#36125;&#21494;&#26031;&#20272;&#35745;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#28508;&#22312;&#20808;&#39564;&#30340;&#35745;&#31639;&#26377;&#25928;&#20272;&#35745;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21464;&#20998;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#26368;&#21021;&#30001;Carbonetto&#21644;Stephens&#65288;2012&#65289;&#20197;&#21450;Kim&#31561;&#20154;&#65288;2022&#65289;&#24341;&#20837;&#12290;&#22312;&#23545;&#35774;&#35745;&#21644;&#20808;&#39564;&#20570;&#20986;&#28201;&#21644;&#20551;&#35774;&#30340;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#38750;&#21442;&#25968;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#65288;NPMLE&#65289;&#21450;&#20854;&#65288;&#21487;&#35745;&#31639;&#30340;&#65289;&#26420;&#32032;&#22343;&#20540;&#22330;&#21464;&#20998;&#20195;&#29702;&#30340;&#28176;&#36817;&#19968;&#33268;&#24615;&#12290;&#22312;&#20551;&#23450;&#26420;&#32032;&#22343;&#20540;&#22330;&#36924;&#36817;&#20855;&#26377;&#21344;&#20248;&#35299;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#36817;&#20284;&#27491;&#21017;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;1-Wasserstein&#24230;&#37327;&#19979;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;&#36825;&#20351;&#24471;&#36125;&#21494;&#26031;&#25512;&#26029;&#21487;&#20197;&#22312;&#35745;&#31639;&#19978;&#21487;&#34892;&#65292;&#20363;&#22914;&#26500;&#24314;&#20855;&#26377;&#24179;&#22343;&#35206;&#30422;&#20445;&#35777;&#30340;&#21518;&#39564;&#21487;&#20449;&#21306;&#38388;&#65292;&#22238;&#24402;&#31995;&#25968;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#20272;&#35745;&#65292;&#38750;&#31354;&#27604;&#20363;&#30340;&#20272;&#35745;&#31561;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study empirical Bayes estimation in high-dimensional linear regression. To facilitate computationally efficient estimation of the underlying prior, we adopt a variational empirical Bayes approach, introduced originally in Carbonetto and Stephens (2012) and Kim et al. (2022). We establish asymptotic consistency of the nonparametric maximum likelihood estimator (NPMLE) and its (computable) naive mean field variational surrogate under mild assumptions on the design and the prior. Assuming, in addition, that the naive mean field approximation has a dominant optimizer, we develop a computationally efficient approximation to the oracle posterior distribution, and establish its accuracy under the 1-Wasserstein metric. This enables computationally feasible Bayesian inference; e.g., construction of posterior credible intervals with an average coverage guarantee, Bayes optimal estimation for the regression coefficients, estimation of the proportion of non-nulls, etc. Our analysis covers both 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#22312;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#25439;&#22833;&#20559;&#24046;&#19982;&#26102;&#38388;&#38388;&#38548;&#21644;&#31354;&#38388;&#26799;&#24230;&#25104;&#27491;&#27604;&#65292;&#19982;&#34892;&#36208;&#32773;&#22823;&#23567;&#25104;&#21453;&#27604;&#65292;&#21516;&#26102;&#26102;&#38388;&#38388;&#38548;&#24517;&#39035;&#36275;&#22815;&#38271;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#27979;&#35797;&#32467;&#26524;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.16829</link><description>&lt;p&gt;
&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#22312;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An analysis of the derivative-free loss method for solving PDEs. (arXiv:2309.16829v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#22312;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#25439;&#22833;&#20559;&#24046;&#19982;&#26102;&#38388;&#38388;&#38548;&#21644;&#31354;&#38388;&#26799;&#24230;&#25104;&#27491;&#27604;&#65292;&#19982;&#34892;&#36208;&#32773;&#22823;&#23567;&#25104;&#21453;&#27604;&#65292;&#21516;&#26102;&#26102;&#38388;&#38388;&#38548;&#24517;&#39035;&#36275;&#22815;&#38271;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#27979;&#35797;&#32467;&#26524;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#22312;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#19968;&#31867;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#24212;&#29992;&#12290;&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#37319;&#29992;&#36153;&#26364;-&#21345;&#20811;&#20844;&#24335;&#65292;&#32467;&#21512;&#38543;&#26426;&#34892;&#36208;&#32773;&#21450;&#20854;&#23545;&#24212;&#30340;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#36153;&#26364;-&#21345;&#20811;&#20844;&#24335;&#20013;&#19982;&#26102;&#38388;&#38388;&#38548;&#30456;&#20851;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#34892;&#36208;&#32773;&#22823;&#23567;&#23545;&#35745;&#31639;&#25928;&#29575;&#12289;&#21487;&#35757;&#32451;&#24615;&#21644;&#37319;&#26679;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#35757;&#32451;&#25439;&#22833;&#20559;&#24046;&#19982;&#26102;&#38388;&#38388;&#38548;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#38388;&#26799;&#24230;&#25104;&#27491;&#27604;&#65292;&#19982;&#34892;&#36208;&#32773;&#22823;&#23567;&#25104;&#21453;&#27604;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#26102;&#38388;&#38388;&#38548;&#24517;&#39035;&#36275;&#22815;&#38271;&#25165;&#33021;&#35757;&#32451;&#32593;&#32476;&#12290;&#36825;&#20123;&#20998;&#26512;&#32467;&#26524;&#35828;&#26126;&#65292;&#22312;&#26102;&#38388;&#38388;&#38548;&#30340;&#26368;&#20248;&#19979;&#30028;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21487;&#20197;&#36873;&#25321;&#23613;&#21487;&#33021;&#23567;&#30340;&#34892;&#36208;&#32773;&#22823;&#23567;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25903;&#25345;&#25105;&#20204;&#20998;&#26512;&#30340;&#25968;&#20540;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study analyzes the derivative-free loss method to solve a certain class of elliptic PDEs using neural networks. The derivative-free loss method uses the Feynman-Kac formulation, incorporating stochastic walkers and their corresponding average values. We investigate the effect of the time interval related to the Feynman-Kac formulation and the walker size in the context of computational efficiency, trainability, and sampling errors. Our analysis shows that the training loss bias is proportional to the time interval and the spatial gradient of the neural network while inversely proportional to the walker size. We also show that the time interval must be sufficiently long to train the network. These analytic results tell that we can choose the walker size as small as possible based on the optimal lower bound of the time interval. We also provide numerical tests supporting our analysis.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#20998;&#31867;&#22120;&#23637;&#31034;&#20102;&#35760;&#24405;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#12289;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#12289;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#30340;&#26032;&#20852;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.16779</link><description>&lt;p&gt;
&#29983;&#25104;&#20998;&#31867;&#22120;&#30340;&#26377;&#36259;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intriguing properties of generative classifiers. (arXiv:2309.16779v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16779
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20998;&#31867;&#22120;&#23637;&#31034;&#20102;&#35760;&#24405;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#12289;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#12289;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#30340;&#26032;&#20852;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#23545;&#35937;&#30340;&#26368;&#20339;&#33539;&#24335;&#26159;&#21028;&#21035;&#24335;&#25512;&#29702;&#65288;&#24555;&#36895;&#20294;&#28508;&#22312;&#23481;&#26131;&#20986;&#29616;&#24555;&#25463;&#23398;&#20064;&#65289;&#36824;&#26159;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65288;&#36739;&#24930;&#20294;&#28508;&#22312;&#26356;&#31283;&#20581;&#65289;&#65311;&#25105;&#20204;&#20511;&#37492;&#20102;&#26368;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#23637;&#65292;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36716;&#21270;&#20026;&#20998;&#31867;&#22120;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#20854;&#34892;&#20026;&#65292;&#24182;&#23558;&#20854;&#19982;&#21028;&#21035;&#27169;&#22411;&#21644;&#20154;&#31867;&#24515;&#29702;&#29289;&#29702;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25253;&#36947;&#20102;&#29983;&#25104;&#20998;&#31867;&#22120;&#30340;&#22235;&#20010;&#26377;&#36259;&#30340;&#26032;&#20852;&#29305;&#24615;&#65306;&#23427;&#20204;&#26174;&#31034;&#20986;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#65288;&#23545;&#20110;Imagen&#36798;&#21040;99%&#65289;&#65292;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#65292;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#23427;&#20204;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#30446;&#21069;&#27169;&#25311;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#30340;&#20027;&#23548;&#33539;&#24335;&#26159;&#21028;&#21035;&#24335;&#25512;&#29702;&#65292;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is the best paradigm to recognize objects -- discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#29615;&#22659;&#30340;&#31639;&#27861; XRM&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#24182;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#29615;&#22659;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16748</link><description>&lt;p&gt;
&#29992;XRM&#21457;&#29616;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Discovering environments with XRM. (arXiv:2309.16748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#29615;&#22659;&#30340;&#31639;&#27861; XRM&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#24182;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#29615;&#22659;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#38656;&#35201;&#29615;&#22659;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27880;&#37322;&#30340;&#33719;&#21462;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#21463;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#26399;&#26395;&#21644;&#24863;&#30693;&#20559;&#24046;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#23454;&#29616;&#24212;&#29992;&#39046;&#22495;&#20840;&#38754;&#27867;&#21270;&#30340;&#40065;&#26834;&#24615;AI&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#24320;&#21457;&#19968;&#31181;&#31639;&#27861;&#26469;&#33258;&#21160;&#21457;&#29616;&#24341;&#21457;&#24191;&#27867;&#27867;&#21270;&#30340;&#29615;&#22659;&#12290;&#30446;&#21069;&#30340;&#25552;&#26696;&#26681;&#25454;&#35757;&#32451;&#35823;&#24046;&#23558;&#31034;&#20363;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#31867;&#65292;&#20294;&#23384;&#22312;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#28155;&#21152;&#20102;&#36229;&#21442;&#25968;&#21644;&#26089;&#20572;&#31574;&#30053;&#65292;&#32780;&#36825;&#20123;&#21442;&#25968;&#26159;&#26080;&#27861;&#22312;&#27809;&#26377;&#20154;&#31867;&#27880;&#37322;&#29615;&#22659;&#30340;&#39564;&#35777;&#38598;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#30340;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#27491;&#26159;&#35201;&#21457;&#29616;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Cross-Risk-Minimization (XRM) &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;XRM &#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#20010;&#38543;&#26426;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#25152;&#20570;&#30340;&#33258;&#20449;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;XRM &#25552;&#20379;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful out-of-distribution generalization requires environment annotations. Unfortunately, these are resource-intensive to obtain, and their relevance to model performance is limited by the expectations and perceptual biases of human annotators. Therefore, to enable robust AI systems across applications, we must develop algorithms to automatically discover environments inducing broad generalization. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods add hyper-parameters and early-stopping criteria that are impossible to tune without a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to address this issue. XRM trains two twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;RVGP&#26041;&#27861;&#65292;&#32467;&#21512;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#36924;&#36817;&#26041;&#27861;&#23545;&#28508;&#22312;&#27969;&#24418;&#19978;&#30340;&#21521;&#37327;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#21521;&#37327;&#22330;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#20840;&#23616;&#35268;&#24459;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16746</link><description>&lt;p&gt;
&#38544;&#24615;&#39640;&#26031;&#36807;&#31243;&#34920;&#31034;&#20219;&#24847;&#28508;&#22312;&#27969;&#24418;&#19978;&#30340;&#21521;&#37327;&#22330;
&lt;/p&gt;
&lt;p&gt;
Implicit Gaussian process representation of vector fields over arbitrary latent manifolds. (arXiv:2309.16746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16746
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;RVGP&#26041;&#27861;&#65292;&#32467;&#21512;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#36924;&#36817;&#26041;&#27861;&#23545;&#28508;&#22312;&#27969;&#24418;&#19978;&#30340;&#21521;&#37327;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#21521;&#37327;&#22330;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#20840;&#23616;&#35268;&#24459;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26159;&#29992;&#20110;&#23398;&#20064;&#26410;&#30693;&#20989;&#25968;&#21644;&#37327;&#21270;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#19981;&#30830;&#23450;&#24615;&#30340;&#27969;&#34892;&#38750;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25193;&#23637;&#20102;GPs&#65292;&#29992;&#20110;&#24314;&#27169;&#20998;&#24067;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#19978;&#30340;&#26631;&#37327;&#21644;&#21521;&#37327;&#25968;&#25454;&#65292;&#21253;&#25324;&#20986;&#29616;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#21160;&#21147;&#31995;&#32479;&#21644;&#31070;&#32463;&#31185;&#23398;&#31561;&#20247;&#22810;&#39046;&#22495;&#20013;&#30340;&#24179;&#28369;&#27969;&#24418;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#25968;&#25454;&#30340;&#28508;&#22312;&#27969;&#24418;&#26159;&#24050;&#30693;&#30340;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;RVGP&#65292;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#28508;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#21521;&#37327;&#20449;&#21495;&#30340;GP&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#19982;&#20999;&#21521;&#19995;&#20851;&#32852;&#30340;&#36830;&#25509;Laplacian&#30340;&#29305;&#24449;&#20989;&#25968;&#36827;&#34892;&#20301;&#32622;&#32534;&#30721;&#65292;&#36825;&#20123;&#29305;&#24449;&#20989;&#25968;&#21487;&#20197;&#20174;&#22522;&#20110;&#22270;&#30340;&#24120;&#35265;&#25968;&#25454;&#36817;&#20284;&#20013;&#36731;&#26494;&#25512;&#23548;&#20986;&#26469;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RVGP&#22312;&#27969;&#24418;&#19978;&#20855;&#26377;&#20840;&#23616;&#35268;&#24459;&#24615;&#65292;&#20351;&#24471;&#20854;&#33021;&#22815;&#22312;&#20445;&#30041;&#22855;&#24322;&#24615;&#30340;&#21516;&#26102;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#21521;&#37327;&#22330;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;RVGP&#26469;&#37325;&#26500;&#39640;&#23494;&#24230;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes (GPs) are popular nonparametric statistical models for learning unknown functions and quantifying the spatiotemporal uncertainty in data. Recent works have extended GPs to model scalar and vector quantities distributed over non-Euclidean domains, including smooth manifolds appearing in numerous fields such as computer vision, dynamical systems, and neuroscience. However, these approaches assume that the manifold underlying the data is known, limiting their practical utility. We introduce RVGP, a generalisation of GPs for learning vector signals over latent Riemannian manifolds. Our method uses positional encoding with eigenfunctions of the connection Laplacian, associated with the tangent bundle, readily derived from common graph-based approximation of data. We demonstrate that RVGP possesses global regularity over the manifold, which allows it to super-resolve and inpaint vector fields while preserving singularities. Furthermore, we use RVGP to reconstruct high-dens
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2308.06203</link><description>&lt;p&gt;
&#20026;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#26500;&#24314;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks. (arXiv:2308.06203v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06203
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24847;&#21619;&#30528;&#31995;&#32479;&#35774;&#35745;&#32773;&#26080;&#27861;&#39044;&#27979;&#24182;&#26126;&#30830;&#35774;&#35745;&#20986;&#26426;&#22120;&#20154;&#21487;&#33021;&#36935;&#21040;&#30340;&#25152;&#26377;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#20197;&#36825;&#31181;&#26041;&#24335;&#35774;&#35745;&#30340;&#26426;&#22120;&#20154;&#22312;&#39640;&#24230;&#21463;&#25511;&#30340;&#29615;&#22659;&#20043;&#22806;&#23481;&#26131;&#20986;&#29616;&#25925;&#38556;&#12290;&#22240;&#26524;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32534;&#30721;&#26426;&#22120;&#20154;&#19982;&#20854;&#29615;&#22659;&#30456;&#20114;&#20316;&#29992;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#24418;&#24335;&#21270;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#36890;&#24120;&#36935;&#21040;&#30340;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#29575;&#34920;&#31034;&#12290;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#23637;&#31034;&#20102;&#35768;&#22810;&#24212;&#29992;&#25152;&#38656;&#30340;&#22522;&#26412;&#24863;&#30693;&#21644;&#25805;&#20316;&#33021;&#21147;&#65292;&#21253;&#25324;&#20179;&#24211;&#29289;&#27969;&#21644;&#23478;&#24237;&#20154;&#24037;&#25903;&#25345;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#23558;&#29289;&#29702;&#27169;&#25311;&#21151;&#33021;&#23884;&#20837;&#21040;&#36825;&#20010;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainties in the real world mean that is impossible for system designers to anticipate and explicitly design for all scenarios that a robot might encounter. Thus, robots designed like this are fragile and fail outside of highly-controlled environments. Causal models provide a principled framework to encode formal knowledge of the causal relationships that govern the robot's interaction with its environment, in addition to probabilistic representations of noise and uncertainty typically encountered by real-world robots. Combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment. In this work, we focus on the problem of a robot block-stacking task due to the fundamental perception and manipulation capabilities it demonstrates, required by many applications including warehouse logistics and domestic human support robotics. We propose a novel causal probabilistic framework to embed a physics simulation capability int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#32467;&#26500;&#30340;&#21487;&#35782;&#21035;&#24615;&#29702;&#35770;&#65292;&#25299;&#23637;&#20102;&#20808;&#21069;&#20165;&#38480;&#20110;&#21333;&#20219;&#21153;&#20998;&#31867;&#30340;&#24037;&#20316;&#12290;&#20219;&#21153;&#20998;&#24067;&#30340;&#23384;&#22312;&#23450;&#20041;&#20102;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#26465;&#20214;&#20808;&#39564;&#65292;&#23558;&#21487;&#35782;&#21035;&#24615;&#30340;&#31561;&#20215;&#31867;&#38477;&#20302;&#21040;&#25490;&#21015;&#21644;&#32553;&#25918;&#12290;&#22312;&#20551;&#35774;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#20248;&#21270;&#65292;&#24182;&#22312;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#19979;&#28216;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14861</link><description>&lt;p&gt;
&#21033;&#29992;&#20219;&#21153;&#32467;&#26500;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Task Structures for Improved Identifiability in Neural Network Representations. (arXiv:2306.14861v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#32467;&#26500;&#30340;&#21487;&#35782;&#21035;&#24615;&#29702;&#35770;&#65292;&#25299;&#23637;&#20102;&#20808;&#21069;&#20165;&#38480;&#20110;&#21333;&#20219;&#21153;&#20998;&#31867;&#30340;&#24037;&#20316;&#12290;&#20219;&#21153;&#20998;&#24067;&#30340;&#23384;&#22312;&#23450;&#20041;&#20102;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#26465;&#20214;&#20808;&#39564;&#65292;&#23558;&#21487;&#35782;&#21035;&#24615;&#30340;&#31561;&#20215;&#31867;&#38477;&#20302;&#21040;&#25490;&#21015;&#21644;&#32553;&#25918;&#12290;&#22312;&#20551;&#35774;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#20248;&#21270;&#65292;&#24182;&#22312;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#19979;&#28216;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#21487;&#36776;&#21035;&#24615;&#30340;&#29702;&#35770;&#65292;&#32771;&#34385;&#20102;&#22312;&#25317;&#26377;&#20219;&#21153;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#30340;&#21518;&#26524;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#22238;&#24402;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#23454;&#29616;&#21487;&#35782;&#21035;&#24615;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#20165;&#38480;&#20110;&#21333;&#20219;&#21153;&#20998;&#31867;&#24773;&#20917;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#21153;&#20998;&#24067;&#30340;&#23384;&#22312;&#23450;&#20041;&#20102;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#26465;&#20214;&#20808;&#39564;&#65292;&#23558;&#21487;&#35782;&#21035;&#24615;&#30340;&#31561;&#20215;&#31867;&#38477;&#20302;&#21040;&#25490;&#21015;&#21644;&#32553;&#25918;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#24378;&#22823;&#21644;&#26356;&#26377;&#29992;&#30340;&#32467;&#26524;&#12290;&#24403;&#25105;&#20204;&#36827;&#19968;&#27493;&#20551;&#35774;&#36825;&#20123;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#20248;&#21270;&#65292;&#24182;&#22312;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#19979;&#28216;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24674;&#22797;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#35268;&#33539;&#34920;&#31034;&#26041;&#38754;&#20248;&#20110;&#26356;&#19968;&#33324;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work extends the theory of identifiability in supervised learning by considering the consequences of having access to a distribution of tasks. In such cases, we show that identifiability is achievable even in the case of regression, extending prior work restricted to the single-task classification case. Furthermore, we show that the existence of a task distribution which defines a conditional prior over latent variables reduces the equivalence class for identifiability to permutations and scaling, a much stronger and more useful result. When we further assume a causal structure over these tasks, our approach enables simple maximum marginal likelihood optimization together with downstream applicability to causal representation learning. Empirically, we validate that our model outperforms more general unsupervised models in recovering canonical representations for synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#24046;&#24322;&#12290;&#23427;&#21487;&#20197;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2306.04817</link><description>&lt;p&gt;
SiBBlInGS: &#20351;&#29992;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#27169;&#22359;&#25512;&#29702;&#30340;&#24314;&#27169;&#22359;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States. (arXiv:2306.04817v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#24046;&#24322;&#12290;&#23427;&#21487;&#20197;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#26469;&#35828;&#65292;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#27169;&#22359;&#26159;&#21457;&#29616;&#22797;&#26434;&#31995;&#32479;&#20013;&#26377;&#20215;&#20540;&#35265;&#35299;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;(SiBBlInGS)&#65292;&#29992;&#20110;&#21457;&#29616;&#27169;&#22359;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#33021;&#22815;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;SiBBlInGS&#36824;&#20801;&#35768;&#36328;&#29366;&#24577;&#21464;&#21270;&#27169;&#22359;&#32467;&#26500;&#21644;&#27599;&#27425;&#35797;&#39564;&#30340;&#26102;&#38388;&#21464;&#24322;&#65292;&#24182;&#21487;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable methods for extracting meaningful building blocks (BBs) underlying multi-dimensional time series are vital for discovering valuable insights in complex systems. Existing techniques, however, encounter limitations that restrict their applicability to real-world systems, like reliance on orthogonality assumptions, inadequate incorporation of inter- and intra-state variability, and incapability to handle sessions of varying duration. Here, we present a framework for Similarity-driven Building Block Inference using Graphs across States (SiBBlInGS). SiBBlInGS employs a graph-based dictionary learning approach for BB discovery, simultaneously considers both inter- and intra-state relationships in the data, can extract non-orthogonal components, and allows for variations in session counts and duration across states. Additionally, SiBBlInGS allows for cross-state variations in BB structure and per-trial temporal variability, can identify state-specific vs state-invariant BBs, and
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#28857;&#21608;&#22260;&#26377;&#22823;&#30340;&#20960;&#20046;&#30830;&#23450;&#30340;&#32622;&#20449;&#37051;&#22495;&#65292;&#36825;&#23548;&#33268;&#29616;&#20195;&#27169;&#22411;&#26657;&#20934;&#38754;&#20020;&#37325;&#35201;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2306.00740</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#32622;&#20449;&#29616;&#35937;&#21450;&#20854;&#23545;&#26657;&#20934;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
A Uniform Confidence Phenomenon in Deep Learning and its Implications for Calibration. (arXiv:2306.00740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00740
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#28857;&#21608;&#22260;&#26377;&#22823;&#30340;&#20960;&#20046;&#30830;&#23450;&#30340;&#32622;&#20449;&#37051;&#22495;&#65292;&#36825;&#23548;&#33268;&#29616;&#20195;&#27169;&#22411;&#26657;&#20934;&#38754;&#20020;&#37325;&#35201;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#24778;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23649;&#27425;&#34920;&#29616;&#20986;&#22312;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#20272;&#35745;&#19981;&#20339;&#30340;&#24773;&#20917;&#8212;&#8212;&#25442;&#21477;&#35805;&#35828;&#65292;&#23427;&#20204;&#22312;&#38169;&#35823;&#26102;&#32463;&#24120;&#36807;&#24230;&#33258;&#20449;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#27169;&#22411;&#26657;&#20934;&#65292;&#24182;&#20197;&#20462;&#25913;&#35757;&#32451;&#26041;&#26696;&#21644;&#35757;&#32451;&#21518;&#26657;&#20934;&#31243;&#24207;&#30340;&#24418;&#24335;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29616;&#20195;&#27169;&#22411;&#26657;&#20934;&#30340;&#37325;&#35201;&#38556;&#30861;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#28857;&#21608;&#22260;&#26377;&#22823;&#30340;&#20960;&#20046;&#30830;&#23450;&#30340;&#32622;&#20449;&#37051;&#22495;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#24456;&#22810;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#23545;&#20013;&#37117;&#20250;&#20986;&#29616;&#65288;&#22312;&#22270;&#20687;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#36825;&#31181;&#29616;&#35937;&#20986;&#29616;&#26102;&#65292;&#22312;&#31867;&#21035;&#20043;&#38388;&#23384;&#22312;&#37325;&#21472;&#30340;&#22823;&#31867;&#25968;&#25454;&#20998;&#24067;&#20013;&#65292;&#21363;&#20351;&#22312;&#24212;&#29992;&#26657;&#20934;&#21518;&#20063;&#19981;&#33021;&#33719;&#24471;&#27604;&#38543;&#26426;&#26356;&#22909;&#30340;&#28176;&#36817;&#26657;&#20934;&#27169;&#22411;&#65288;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive generalization capabilities of deep neural networks, they have been repeatedly shown to poorly estimate their predictive uncertainty - in other words, they are frequently overconfident when they are wrong. Fixing this issue is known as model calibration, and has consequently received much attention in the form of modified training schemes and post-training calibration procedures. In this work, we present a significant hurdle to the calibration of modern models: deep neural networks have large neighborhoods of almost certain confidence around their training points. We demonstrate in our experiments that this phenomenon consistently arises (in the context of image classification) across many model and dataset pairs. Furthermore, we prove that when this phenomenon holds, for a large class of data distributions with overlaps between classes, it is not possible to obtain a model that is asymptotically better than random (with respect to calibration) even after applyin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#22522;&#20110;&#23545;&#25968;&#20985;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#65292;&#20351;&#29992;&#31561;&#21521;&#24615;&#39640;&#26031;&#24179;&#28369;&#26469;&#35299;&#20915;&#39640;&#32500;&#19979;&#25277;&#26679;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19473</link><description>&lt;p&gt;
&#23545;&#25968;&#20985;&#39532;&#23572;&#21487;&#22827;&#38142;&#20043;&#38142;
&lt;/p&gt;
&lt;p&gt;
Chain of Log-Concave Markov Chains. (arXiv:2305.19473v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19473
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#22522;&#20110;&#23545;&#25968;&#20985;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#65292;&#20351;&#29992;&#31561;&#21521;&#24615;&#39640;&#26031;&#24179;&#28369;&#26469;&#35299;&#20915;&#39640;&#32500;&#19979;&#25277;&#26679;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#26159;&#19968;&#31181;&#20174;&#26410;&#26631;&#20934;&#21270;&#23494;&#24230;&#20013;&#25277;&#26679;&#30340;&#36890;&#29992;&#31639;&#27861;&#31867;&#12290;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;MCMC&#38754;&#20020;&#20004;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65306;(i)&#24863;&#20852;&#36259;&#30340;&#20998;&#24067;&#22312;&#30001;&#23567;&#27010;&#29575;&#22359;&#38548;&#24320;&#30340;&#21306;&#22495;&#20013;&#38598;&#20013;;(ii)&#23545;&#25968;&#20985;&#24615;&#30340;&#23567;&#27010;&#29575;&#22359;&#26412;&#36523;&#36890;&#24120;&#23384;&#22312;&#30149;&#24577;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#37319;&#29992;&#31561;&#21521;&#24615;&#39640;&#26031;&#24179;&#28369;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#26080;&#35770;&#23494;&#24230;&#20989;&#25968;&#30340;&#26368;&#23567;&#20551;&#35774;&#26159;&#20160;&#20040;&#65292;&#20174;&#23494;&#24230;&#20989;&#25968;&#20013;&#37319;&#26679;&#24635;&#26159;&#21487;&#20197;&#20998;&#35299;&#20026;&#36890;&#36807;&#31561;&#22122;&#22768;&#27979;&#37327;&#30340;&#32047;&#31215;&#65292;&#20174;&#23545;&#25968;&#20985;&#24615;&#26465;&#20214;&#23494;&#24230;&#20013;&#37319;&#26679;&#30340;&#24207;&#21015;&#12290;&#35813;&#26500;&#36896;&#36319;&#36394;&#20102;&#26679;&#26412;&#21382;&#21490;&#65292;&#22240;&#27492;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#32780;&#35328;&#26159;&#38750;&#39532;&#23572;&#21487;&#22827;&#30340;&#65292;&#20294;&#21382;&#21490;&#20165;&#20197;&#32463;&#39564;&#22343;&#20540;&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#20869;&#23384;&#21360;&#36857;&#30340;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#25512;&#24191;&#20102;&#27493;&#34892;&#36339;&#36291;&#37319;&#26679;&#65288;1&#65289;&#12290;"&#36208;"&#38454;&#27573;&#21464;&#25104;&#20102;&#23545;&#25968;&#20985;&#38142;&#30340;(&#38750;&#39532;&#23572;&#21487;&#22827;)&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov chain Monte Carlo (MCMC) is a class of general-purpose algorithms for sampling from unnormalized densities. There are two well-known problems facing MCMC in high dimensions: (i) The distributions of interest are concentrated in pockets separated by large regions with small probability mass, and (ii) The log-concave pockets themselves are typically ill-conditioned. We introduce a framework to tackle these problems using isotropic Gaussian smoothing. We prove one can always decompose sampling from a density (minimal assumptions made on the density) into a sequence of sampling from log-concave conditional densities via accumulation of noisy measurements with equal noise levels. This construction keeps track of a history of samples, making it non-Markovian as a whole, but the history only shows up in the form of an empirical mean, making the memory footprint minimal. Our sampling algorithm generalizes walk-jump sampling [1]. The "walk" phase becomes a (non-Markovian) chain of log-co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#20998;&#26512;&#65292;&#22312;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#35777;&#26126;&#20102;GD&#31639;&#27861;&#30340;&#19968;&#33324;&#24615;&#20445;&#35777;&#65292;&#20026;&#21452;&#23618;&#21644;&#19977;&#23618;NN&#25512;&#23548;&#20986;&#20102;&#36807;&#37327;&#39118;&#38505;&#29575;&#65292;&#25193;&#23637;&#20102;&#20197;&#24448;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.16891</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#22312;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Generalization Guarantees of Gradient Descent for Multi-Layer Neural Networks. (arXiv:2305.16891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#20998;&#26512;&#65292;&#22312;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#35777;&#26126;&#20102;GD&#31639;&#27861;&#30340;&#19968;&#33324;&#24615;&#20445;&#35777;&#65292;&#20026;&#21452;&#23618;&#21644;&#19977;&#23618;NN&#25512;&#23548;&#20986;&#20102;&#36807;&#37327;&#39118;&#38505;&#29575;&#65292;&#25193;&#23637;&#20102;&#20197;&#24448;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#31639;&#27861;&#31283;&#23450;&#24615;&#26041;&#27861;&#65292;&#23545;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#27867;&#21270;&#36827;&#34892;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;&#21333;&#38544;&#34255;&#23618;NN&#19978;&#65292;&#24182;&#27809;&#26377;&#35299;&#20915;&#19981;&#21516;&#32593;&#32476;&#32553;&#25918;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;GD&#22312;&#22810;&#23618;NN&#19978;&#36827;&#34892;&#20840;&#38754;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#20998;&#26512;&#65292;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#20197;&#24448;&#30340;&#24037;&#20316;&#12290;&#23545;&#20110;&#21452;&#23618;NN&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#22312;&#19968;&#33324;&#30340;&#32593;&#32476;&#32553;&#25918;&#21442;&#25968;&#19979;&#24314;&#31435;&#30340;&#65292;&#25918;&#23485;&#20102;&#20197;&#21069;&#30340;&#26465;&#20214;&#12290;&#23545;&#20110;&#19977;&#23618;NN&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#36129;&#29486;&#22312;&#20110;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#31574;&#30053;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#20960;&#20046;&#21327;&#21516;&#32422;&#26463;&#24615;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#19968;&#33324;&#24615;&#21457;&#29616;&#30340;&#30452;&#25509;&#24212;&#29992;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;GD&#31639;&#27861;&#22312;&#21452;&#23618;&#21644;&#19977;&#23618;NN&#20013;&#30340;&#36807;&#37327;&#39118;&#38505;&#36895;&#29575;&#20026;$O(1/\sqrt{n})$&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, significant progress has been made in understanding the generalization of neural networks (NNs) trained by gradient descent (GD) using the algorithmic stability approach. However, most of the existing research has focused on one-hidden-layer NNs and has not addressed the impact of different network scaling parameters. In this paper, we greatly extend the previous work \cite{lei2022stability,richards2021stability} by conducting a comprehensive stability and generalization analysis of GD for multi-layer NNs. For two-layer NNs, our results are established under general network scaling parameters, relaxing previous conditions. In the case of three-layer NNs, our technical contribution lies in demonstrating its nearly co-coercive property by utilizing a novel induction strategy that thoroughly explores the effects of over-parameterization. As a direct application of our general findings, we derive the excess risk rate of $O(1/\sqrt{n})$ for GD algorithms in both two-layer and thre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#31163;&#25955;&#21270;&#20559;&#24046;&#21644;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#65292;&#24471;&#21040;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#30340;&#27867;&#21270;&#24046;&#36317;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.16791</link><description>&lt;p&gt;
&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#27867;&#21270;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization Capacities of Neural Controlled Differential Equations. (arXiv:2305.16791v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#31163;&#25955;&#21270;&#20559;&#24046;&#21644;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#65292;&#24471;&#21040;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#30340;&#27867;&#21270;&#24046;&#36317;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;Kidger&#65292;Morrill&#31561;&#65292;2020&#65289;&#20174;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#20013;&#39044;&#27979;&#32467;&#26524;&#30340;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#26159;&#19968;&#20010;&#26410;&#35266;&#23519;&#21040;&#30340;&#36830;&#32493;&#36335;&#24452;&#30340;&#31163;&#25955;&#21270;&#65292;&#32467;&#26524;&#36890;&#36807;&#19968;&#20010;&#20855;&#26377;&#26410;&#30693;&#21521;&#37327;&#22330;&#30340;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#20381;&#36182;&#20110;&#36825;&#20010;&#36335;&#24452;&#12290;&#20351;&#29992;&#31163;&#25955;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#20250;&#24341;&#20837;&#31163;&#25955;&#20559;&#24046;&#65292;&#25105;&#20204;&#31934;&#30830;&#22320;&#37327;&#21270;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;&#36890;&#36807;&#20351;&#29992;&#20851;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#27969;&#30340;&#36830;&#32493;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36924;&#36817;&#20559;&#24046;&#30452;&#25509;&#19982;&#30001;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#29983;&#25104;&#27169;&#22411;&#30340;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#30340;&#36924;&#36817;&#35823;&#24046;&#30456;&#20851;&#12290;&#36890;&#36807;&#32467;&#21512;&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#19978;&#30028;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#36798;&#21040;&#30340;&#26399;&#26395;&#25439;&#22833;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a supervised learning setup in which the goal is to predicts an outcome from a sample of irregularly sampled time series using Neural Controlled Differential Equations (Kidger, Morrill, et al. 2020). In our framework, the time series is a discretization of an unobserved continuous path, and the outcome depends on this path through a controlled differential equation with unknown vector field. Learning with discrete data thus induces a discretization bias, which we precisely quantify. Using theoretical results on the continuity of the flow of controlled differential equations, we show that the approximation bias is directly related to the approximation error of a Lipschitz function defining the generative model by a shallow neural network. By combining these result with recent work linking the Lipschitz constant of neural networks to their generalization capacities, we upper bound the generalization gap between the expected loss attained by the empirical risk minimizer and th
&lt;/p&gt;</description></item><item><title>&#22312;$L_{2}$-&#27491;&#21017;&#21270;&#32447;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20351;&#29992;SGD&#20250;&#20135;&#29983;&#20174;&#26356;&#39640;&#31209;&#26368;&#23567;&#20540;&#21040;&#26356;&#20302;&#31209;&#26368;&#23567;&#20540;&#30340;&#21333;&#21521;&#36339;&#36291;&#65292;&#24182;&#19988;&#19981;&#20250;&#36339;&#22238;&#12290;</title><link>http://arxiv.org/abs/2305.16038</link><description>&lt;p&gt;
$L_{2}$&#27491;&#21017;&#32447;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#38544;&#24615;SGD&#20559;&#24046;&#65306;&#20174;&#39640;&#31209;&#21040;&#20302;&#31209;&#30340;&#21333;&#21521;&#36339;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps from high to low rank. (arXiv:2305.16038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16038
&lt;/p&gt;
&lt;p&gt;
&#22312;$L_{2}$-&#27491;&#21017;&#21270;&#32447;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20351;&#29992;SGD&#20250;&#20135;&#29983;&#20174;&#26356;&#39640;&#31209;&#26368;&#23567;&#20540;&#21040;&#26356;&#20302;&#31209;&#26368;&#23567;&#20540;&#30340;&#21333;&#21521;&#36339;&#36291;&#65292;&#24182;&#19988;&#19981;&#20250;&#36339;&#22238;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#22810;&#20010;&#38544;&#34255;&#23618;&#30340;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#65288;DLN&#65289;&#30340;$L_{2}$&#27491;&#21017;&#21270;&#25439;&#22833;&#20855;&#26377;&#22810;&#20010;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#23545;&#24212;&#20110;&#20855;&#26377;&#19981;&#21516;&#31209;&#30340;&#30697;&#38453;&#12290;&#22312;&#30697;&#38453;&#23436;&#25104;&#31561;&#20219;&#21153;&#20013;&#65292;&#30446;&#26631;&#26159;&#25910;&#25947;&#21040;&#26368;&#23567;&#31209;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#35813;&#23616;&#37096;&#26368;&#23567;&#20540;&#20173;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;&#21487;&#20197;&#36731;&#26494;&#36991;&#20813;&#20302;&#20272;&#31209;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#36866;&#21512;&#25968;&#25454;&#65292;&#20294;&#26799;&#24230;&#19979;&#38477;&#21487;&#33021;&#20250;&#38519;&#20837;&#39640;&#20272;&#31209;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;SGD&#65292;&#24635;&#26159;&#26377;&#20174;&#26356;&#39640;&#31209;&#26368;&#23567;&#20540;&#36339;&#36291;&#21040;&#26356;&#20302;&#31209;&#26368;&#23567;&#20540;&#30340;&#27010;&#29575;&#65292;&#20294;&#36339;&#22238;&#30340;&#27010;&#29575;&#20026;&#38646;&#12290;&#26356;&#31934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#38598;&#21512;$B_{1}\subset B_{2}\subset\cdots\subset B_{R}$&#65292;&#20351;&#24471;$B_{r}$&#21253;&#21547;&#31209;$r$&#25110;&#26356;&#23569;&#30340;&#25152;&#26377;&#26368;&#23567;&#20540;&#65288;&#32780;&#19981;&#26159;&#26356;&#22810;&#65289;&#65292;&#23545;&#20110;&#36275;&#22815;&#23567;&#30340;&#23725;&#21442;&#25968;$\lambda$&#21644;&#23398;&#20064;&#29575;$\eta$&#65292;&#23427;&#20204;&#26159;&#21560;&#25910;&#30340;&#65306;SGD&#31163;&#24320;$B_{r}$&#30340;&#27010;&#29575;&#20026;0&#65292;&#20174;&#20219;&#20309;&#36215;&#28857;&#24320;&#22987;&#65292;SGD&#36827;&#20837;$B_{r}$&#30340;&#27010;&#29575;&#38750;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;
The $L_{2}$-regularized loss of Deep Linear Networks (DLNs) with more than one hidden layers has multiple local minima, corresponding to matrices with different ranks. In tasks such as matrix completion, the goal is to converge to the local minimum with the smallest rank that still fits the training data. While rank-underestimating minima can easily be avoided since they do not fit the data, gradient descent might get stuck at rank-overestimating minima. We show that with SGD, there is always a probability to jump from a higher rank minimum to a lower rank one, but the probability of jumping back is zero. More precisely, we define a sequence of sets $B_{1}\subset B_{2}\subset\cdots\subset B_{R}$ so that $B_{r}$ contains all minima of rank $r$ or less (and not more) that are absorbing for small enough ridge parameters $\lambda$ and learning rates $\eta$: SGD has prob. 0 of leaving $B_{r}$, and from any starting point there is a non-zero prob. for SGD to go in $B_{r}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ReLU&#21333;&#20803;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20248;&#21270;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15912</link><description>&lt;p&gt;
&#25913;&#36827;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#30340;&#31070;&#32463;&#29305;&#24449;&#28608;&#27963;&#20540;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning. (arXiv:2305.15912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ReLU&#21333;&#20803;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20248;&#21270;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#21333;&#20010;ReLU&#21333;&#20803;&#30340;&#29305;&#24449;&#28608;&#27963;&#20540;&#12290;&#25105;&#20204;&#23558;ReLU&#21333;&#20803;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#23545;&#24212;&#30340;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#31216;&#20026;ReLU&#21333;&#20803;&#30340;&#29305;&#24449;&#28608;&#27963;&#38598;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29305;&#24449;&#28608;&#27963;&#38598;&#19982;ReLU&#32593;&#32476;&#20013;&#23398;&#20064;&#29305;&#24449;&#20043;&#38388;&#30340;&#26126;&#30830;&#32852;&#31995;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#25216;&#26415;&#22914;&#20309;&#35268;&#33539;&#21270;&#21644;&#31283;&#23450;SGD&#20248;&#21270;&#12290;&#21033;&#29992;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;ReLU&#32593;&#32476;&#20197;&#25913;&#36827;&#29305;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#20854;&#26377;&#29992;&#24615;&#65292;&#20351;&#29992;&#20102;&#19981;&#37027;&#20040;&#31934;&#24515;&#36873;&#25321;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#21644;&#26356;&#22823;&#30340;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#26356;&#22909;&#30340;&#20248;&#21270;&#31283;&#23450;&#24615;&#65292;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the characteristic activation values of individual ReLU units in neural networks. We refer to the corresponding set for such characteristic activation values in the input space as the characteristic activation set of a ReLU unit. We draw an explicit connection between the characteristic activation set and learned features in ReLU networks. This connection leads to new insights into why various neural network normalization techniques used in modern deep learning architectures regularize and stabilize SGD optimization. Utilizing these insights, we propose a geometric approach to parameterize ReLU networks for improved feature learning. We empirically verify its usefulness with less carefully chosen initialization schemes and larger learning rates. We report improved optimization stability, faster convergence speed, and better generalization performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;25&#24180;&#20013;&#39118;&#38505;&#27979;&#24230;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20171;&#32461;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#19982;&#25928;&#29992;&#29702;&#35770;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#20102;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#31561;&#26032;&#20852;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2212.00856</link><description>&lt;p&gt;
&#23398;&#20064;&#21644;&#20915;&#31574;&#30340;&#39118;&#38505;&#33258;&#36866;&#24212;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Risk-Adaptive Approaches to Learning and Decision Making: A Survey. (arXiv:2212.00856v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;25&#24180;&#20013;&#39118;&#38505;&#27979;&#24230;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20171;&#32461;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#19982;&#25928;&#29992;&#29702;&#35770;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#20102;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#31561;&#26032;&#20852;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#22312;&#24037;&#31243;&#35774;&#35745;&#12289;&#32479;&#35745;&#23398;&#20064;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#30001;&#20110;&#22266;&#26377;&#30340;&#39118;&#38505;&#35268;&#36991;&#21644;&#23545;&#20551;&#35774;&#30340;&#27169;&#31946;&#24615;&#65292;&#36890;&#24120;&#36890;&#36807;&#21046;&#23450;&#21644;&#35299;&#20915;&#20351;&#29992;&#39118;&#38505;&#21644;&#30456;&#20851;&#27010;&#24565;&#30340;&#20445;&#23432;&#20248;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#36807;&#21435;25&#24180;&#26469;&#39118;&#38505;&#27979;&#24230;&#30340;&#24555;&#36895;&#21457;&#23637;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#20174;&#23427;&#20204;&#22312;&#37329;&#34701;&#24037;&#31243;&#39046;&#22495;&#30340;&#36215;&#27493;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#23427;&#20204;&#22312;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#30340;&#24037;&#31243;&#21644;&#24212;&#29992;&#25968;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;&#39118;&#38505;&#27979;&#24230;&#25166;&#26681;&#20110;&#20984;&#20998;&#26512;&#65292;&#20026;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#37325;&#35201;&#35745;&#31639;&#21644;&#29702;&#35770;&#20248;&#21183;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20851;&#38190;&#20107;&#23454;&#65292;&#21015;&#20030;&#20102;&#20960;&#31181;&#20855;&#20307;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#22823;&#37327;&#21442;&#32771;&#25991;&#29486;&#20379;&#36827;&#19968;&#27493;&#38405;&#35835;&#12290;&#35813;&#35843;&#26597;&#36824;&#22238;&#39038;&#20102;&#19982;&#25928;&#29992;&#29702;&#35770;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#32852;&#31995;&#65292;&#25351;&#20986;&#20102;&#26032;&#20852;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#65292;&#24182;&#23450;&#20041;&#20102;&#30456;&#23545;&#27979;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty is prevalent in engineering design, statistical learning, and decision making broadly. Due to inherent risk-averseness and ambiguity about assumptions, it is common to address uncertainty by formulating and solving conservative optimization models expressed using measures of risk and related concepts. We survey the rapid development of risk measures over the last quarter century. From their beginning in financial engineering, we recount the spread to nearly all areas of engineering and applied mathematics. Solidly rooted in convex analysis, risk measures furnish a general framework for handling uncertainty with significant computational and theoretical advantages. We describe the key facts, list several concrete algorithms, and provide an extensive list of references for further reading. The survey recalls connections with utility theory and distributionally robust optimization, points to emerging applications areas such as fair machine learning, and defines measures of rel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#24179;&#34913;&#26435;&#37325;&#26041;&#27861;&#65288;NBW&#65289;&#65292;&#36890;&#36807;&#20248;&#21270; $f$ -&#20998;&#24067;&#30340;&#21464;&#20998;&#34920;&#31034;&#65292;&#30452;&#25509;&#20272;&#35745;&#28304;&#21644;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#30340;&#23494;&#24230;&#27604;&#65292;&#33719;&#24471;&#20102;&#26435;&#37325;&#65292;&#29992;&#20110;&#20272;&#35745;&#20219;&#24847;&#28151;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#24178;&#39044;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2211.07533</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24191;&#20041;&#24179;&#34913;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Generalized Balancing Weights via Deep Neural Networks. (arXiv:2211.07533v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#24179;&#34913;&#26435;&#37325;&#26041;&#27861;&#65288;NBW&#65289;&#65292;&#36890;&#36807;&#20248;&#21270; $f$ -&#20998;&#24067;&#30340;&#21464;&#20998;&#34920;&#31034;&#65292;&#30452;&#25509;&#20272;&#35745;&#28304;&#21644;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#30340;&#23494;&#24230;&#27604;&#65292;&#33719;&#24471;&#20102;&#26435;&#37325;&#65292;&#29992;&#20110;&#20272;&#35745;&#20219;&#24847;&#28151;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#24178;&#39044;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#26159;&#35768;&#22810;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20013;&#24515;&#38382;&#39064;&#12290;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#21327;&#21464;&#37327;&#30340;&#26435;&#37325;&#65292;&#20351;&#24471;&#25968;&#25454;&#30340;&#20998;&#24067;&#31867;&#20284;&#20110;&#38543;&#26426;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#24179;&#34913;&#26435;&#37325;&#65288;NBW&#65289;&#30340;&#24191;&#20041;&#24179;&#34913;&#26435;&#37325;&#65292;&#20197;&#20272;&#35745;&#20219;&#24847;&#28151;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#24178;&#39044;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#20248;&#21270; $f$ -&#20998;&#24067;&#30340;&#21464;&#20998;&#34920;&#31034;&#65292;&#30452;&#25509;&#20272;&#35745;&#28304;&#21644;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#30340;&#23494;&#24230;&#27604;&#65292;&#33719;&#24471;&#20102;&#26435;&#37325;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102; $\alpha$-&#24046;&#24322;&#20316;&#20026;&#20248;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#20854;&#22320;&#38754;&#23454;&#20917;&#20540;&#21644;&#26080;&#20559;&#23567;&#25209;&#37327;&#26799;&#24230;&#30340;&#20272;&#35745;&#22120;&#65292;&#32780;&#19988;&#23545;&#20110;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20197;&#19979;&#20004;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#24179;&#34913;&#26435;&#37325;&#65306;&#25552;&#39640;&#24179;&#34913;&#26435;&#37325;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26816;&#26597;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating causal effects from observational data is a central problem in many domains. A general approach is to balance covariates with weights such that the distribution of the data mimics randomization. We present generalized balancing weights, Neural Balancing Weights (NBW), to estimate the causal effects of an arbitrary mixture of discrete and continuous interventions. The weights were obtained through direct estimation of the density ratio between the source and balanced distributions by optimizing the variational representation of $f$-divergence. For this, we selected $\alpha$-divergence as it presents efficient optimization because it has an estimator whose sample complexity is independent of its ground truth value and unbiased mini-batch gradients; moreover, it is advantageous for the vanishing-gradient problem. In addition, we provide the following two methods for estimating the balancing weights: improving the generalization performance of the balancing weights and checking 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#37051;&#25509;&#30697;&#38453;&#30340;&#29305;&#24449;&#21521;&#37327;&#19982;&#27169;&#22359;&#21270;&#30697;&#38453;&#30340;&#20027;&#29305;&#24449;&#21521;&#37327;&#20043;&#38388;&#30340;&#31934;&#30830;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36924;&#36817;&#27169;&#22359;&#21270;&#30697;&#38453;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#35777;&#26126;&#20102;&#24402;&#19968;&#21270;&#37051;&#25509;&#32858;&#31867;&#19982;&#24402;&#19968;&#21270;&#27169;&#22359;&#21270;&#32858;&#31867;&#30340;&#31561;&#20215;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24402;&#19968;&#21270;&#37051;&#25509;&#32858;&#31867;&#30340;&#25928;&#29575;&#21487;&#20197;&#26159;&#24402;&#19968;&#21270;&#27169;&#22359;&#21270;&#32858;&#31867;&#30340;&#20004;&#20493;&#12290;</title><link>http://arxiv.org/abs/1505.03481</link><description>&lt;p&gt;
&#37051;&#25509;&#24615;&#21644;&#27169;&#22359;&#21270;&#22270;&#20998;&#21106;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Relations Between Adjacency and Modularity Graph Partitioning. (arXiv:1505.03481v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1505.03481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#37051;&#25509;&#30697;&#38453;&#30340;&#29305;&#24449;&#21521;&#37327;&#19982;&#27169;&#22359;&#21270;&#30697;&#38453;&#30340;&#20027;&#29305;&#24449;&#21521;&#37327;&#20043;&#38388;&#30340;&#31934;&#30830;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36924;&#36817;&#27169;&#22359;&#21270;&#30697;&#38453;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#35777;&#26126;&#20102;&#24402;&#19968;&#21270;&#37051;&#25509;&#32858;&#31867;&#19982;&#24402;&#19968;&#21270;&#27169;&#22359;&#21270;&#32858;&#31867;&#30340;&#31561;&#20215;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24402;&#19968;&#21270;&#37051;&#25509;&#32858;&#31867;&#30340;&#25928;&#29575;&#21487;&#20197;&#26159;&#24402;&#19968;&#21270;&#27169;&#22359;&#21270;&#32858;&#31867;&#30340;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#35268;&#33539;&#27169;&#22359;&#21270;&#30697;&#38453;&#30340;&#20027;&#29305;&#24449;&#21521;&#37327;&#19982;&#37051;&#25509;&#30697;&#38453;&#30340;&#29305;&#24449;&#21521;&#37327;&#20043;&#38388;&#30340;&#31934;&#30830;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36924;&#36817;&#27169;&#22359;&#21270;&#30697;&#38453;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#25512;&#23548;&#20102;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#24402;&#19968;&#21270;&#37051;&#25509;&#32858;&#31867;&#19982;&#24402;&#19968;&#21270;&#27169;&#22359;&#21270;&#32858;&#31867;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#35777;&#26126;&#12290;&#25968;&#23383;&#23454;&#39564;&#34920;&#26126;&#65292;&#24402;&#19968;&#21270;&#37051;&#25509;&#32858;&#31867;&#30340;&#25928;&#29575;&#21487;&#20197;&#26159;&#24402;&#19968;&#21270;&#27169;&#22359;&#21270;&#32858;&#31867;&#30340;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops the exact linear relationship between the leading eigenvector of the unnormalized modularity matrix and the eigenvectors of the adjacency matrix. We propose a method for approximating the leading eigenvector of the modularity matrix, and we derive the error of the approximation. There is also a complete proof of the equivalence between normalized adjacency clustering and normalized modularity clustering. Numerical experiments show that normalized adjacency clustering can be as twice efficient as normalized modularity clustering.
&lt;/p&gt;</description></item></channel></rss>