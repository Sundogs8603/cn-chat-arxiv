<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#23637;&#31034;&#20102;&#26925;&#22278;&#23545;&#31216;&#20998;&#24067;&#28151;&#21512;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#65292;&#20026;&#22522;&#20110;&#38750;&#21442;&#25968;&#20998;&#24067;&#30340;&#32858;&#31867;&#25552;&#20379;&#20102;&#29702;&#35770;&#20381;&#25454;&#12290;</title><link>https://arxiv.org/abs/2311.06108</link><description>&lt;p&gt;
&#22522;&#20110;&#26925;&#22278;&#23545;&#31216;&#20998;&#24067;&#28151;&#21512;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#32858;&#31867;&#30340;&#38750;&#21442;&#25968;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Nonparametric consistency for maximum likelihood estimation and clustering based on mixtures of elliptically-symmetric distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06108
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#26925;&#22278;&#23545;&#31216;&#20998;&#24067;&#28151;&#21512;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#65292;&#20026;&#22522;&#20110;&#38750;&#21442;&#25968;&#20998;&#24067;&#30340;&#32858;&#31867;&#25552;&#20379;&#20102;&#29702;&#35770;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#26925;&#22278;&#23545;&#31216;&#20998;&#24067;&#28151;&#21512;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#23545;&#20854;&#24635;&#20307;&#29256;&#26412;&#30340;&#19968;&#33268;&#24615;&#65292;&#20854;&#20013;&#28508;&#22312;&#20998;&#24067;P&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#19981;&#19968;&#23450;&#23646;&#20110;&#20272;&#35745;&#22120;&#25152;&#22522;&#20110;&#30340;&#28151;&#21512;&#31867;&#21035;&#12290;&#24403;P&#26159;&#36275;&#22815;&#20998;&#31163;&#20294;&#38750;&#21442;&#25968;&#30340;&#20998;&#24067;&#28151;&#21512;&#26102;&#65292;&#34920;&#26126;&#20102;&#20272;&#35745;&#22120;&#30340;&#24635;&#20307;&#29256;&#26412;&#30340;&#32452;&#20998;&#23545;&#24212;&#20110;P&#30340;&#33391;&#22909;&#20998;&#31163;&#32452;&#20998;&#12290;&#36825;&#20026;&#22312;P&#20855;&#26377;&#33391;&#22909;&#20998;&#31163;&#23376;&#24635;&#20307;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#36825;&#26679;&#30340;&#20272;&#35745;&#22120;&#36827;&#34892;&#32858;&#31867;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20123;&#29702;&#35770;&#19978;&#30340;&#29702;&#25454;&#65292;&#21363;&#20351;&#36825;&#20123;&#23376;&#24635;&#20307;&#19982;&#28151;&#21512;&#27169;&#22411;&#25152;&#20551;&#35774;&#30340;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06108v2 Announce Type: replace-cross  Abstract: The consistency of the maximum likelihood estimator for mixtures of elliptically-symmetric distributions for estimating its population version is shown, where the underlying distribution $P$ is nonparametric and does not necessarily belong to the class of mixtures on which the estimator is based. In a situation where $P$ is a mixture of well enough separated but nonparametric distributions it is shown that the components of the population version of the estimator correspond to the well separated components of $P$. This provides some theoretical justification for the use of such estimators for cluster analysis in case that $P$ has well separated subpopulations even if these subpopulations differ from what the mixture model assumes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.02246</link><description>&lt;p&gt;
&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditional Variational Diffusion Models. (arXiv:2312.02246v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#26088;&#22312;&#20174;&#35266;&#27979;&#20013;&#30830;&#23450;&#21442;&#25968;&#65292;&#36825;&#26159;&#24037;&#31243;&#21644;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#65292;&#22240;&#20854;&#33021;&#22815;&#20135;&#29983;&#36924;&#30495;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#33391;&#22909;&#30340;&#25968;&#23398;&#29305;&#24615;&#32780;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#32570;&#28857;&#26159;&#23545;&#26041;&#24046;&#35843;&#24230;&#30340;&#36873;&#25321;&#25935;&#24863;&#65292;&#35813;&#35843;&#24230;&#25511;&#21046;&#30528;&#25193;&#25955;&#36807;&#31243;&#30340;&#21160;&#24577;&#12290;&#20026;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#24494;&#35843;&#36825;&#20010;&#35843;&#24230;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#26102;&#38388;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#26368;&#20248;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25903;&#25345;&#23545;&#25968;&#25454;&#30340;&#27010;&#29575;&#26465;&#20214;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#24320;&#38144;&#19979;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#30456;&#20851;&#30340;&#36870;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#21644;&#23450;&#37327;&#30456;&#20301;&#25104;&#20687;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#36739;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems aim to determine parameters from observations, a crucial task in engineering and science. Lately, generative models, especially diffusion models, have gained popularity in this area for their ability to produce realistic solutions and their good mathematical properties. Despite their success, an important drawback of diffusion models is their sensitivity to the choice of variance schedule, which controls the dynamics of the diffusion process. Fine-tuning this schedule for specific applications is crucial but time-costly and does not guarantee an optimal result. We propose a novel approach for learning the schedule as part of the training process. Our method supports probabilistic conditioning on data, provides high-quality solutions, and is flexible, proving able to adapt to different applications with minimum overhead. This approach is tested in two unrelated inverse problems: super-resolution microscopy and quantitative phase imaging, yielding comparable or superior 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#24182;&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#20855;&#26377;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#21644;&#26816;&#27979;&#24322;&#24120;&#20540;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#25193;&#23637;&#26041;&#27861;&#20197;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.09055</link><description>&lt;p&gt;
&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#29992;&#20110;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Outlier-Robust Tensor Low-Rank Representation for Data Clustering. (arXiv:2307.09055v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#24182;&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#20855;&#26377;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#21644;&#26816;&#27979;&#24322;&#24120;&#20540;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#25193;&#23637;&#26041;&#27861;&#20197;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#24352;&#37327;&#20998;&#26512;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24352;&#37327;&#25968;&#25454;&#32463;&#24120;&#21463;&#21040;&#24322;&#24120;&#20540;&#25110;&#26679;&#26412;&#29305;&#23450;&#30340;&#27745;&#26579;&#12290;&#22914;&#20309;&#24674;&#22797;&#34987;&#24322;&#24120;&#20540;&#25439;&#22351;&#30340;&#24352;&#37327;&#25968;&#25454;&#24182;&#36827;&#34892;&#25968;&#25454;&#32858;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#20195;&#25968;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21516;&#26102;&#26816;&#27979;&#24322;&#24120;&#20540;&#21644;&#24352;&#37327;&#25968;&#25454;&#32858;&#31867;&#30340;&#24322;&#24120;&#40065;&#26834;&#24352;&#37327;&#20302;&#31209;&#34920;&#31034;&#65288;OR-TLRR&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21463;&#21040;&#26368;&#36817;&#25552;&#20986;&#30340;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#21487;&#36870;&#32447;&#24615;&#21464;&#25442;&#24341;&#36215;&#30340;&#24352;&#37327;&#24352;&#37327;&#31215;&#30340;&#21551;&#21457;&#12290;&#23545;&#20110;&#24102;&#26377;&#20219;&#24847;&#24322;&#24120;&#20540;&#27745;&#26579;&#30340;&#24352;&#37327;&#35266;&#27979;&#65292;OR-TLRR&#22312;&#36739;&#24369;&#26465;&#20214;&#19979;&#33021;&#22815;&#30830;&#20999;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#30340;&#34892;&#31354;&#38388;&#24182;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;OR-TLRR&#30340;&#25193;&#23637;&#26469;&#22788;&#29702;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank tensor analysis has received widespread attention with many practical applications. However, the tensor data are often contaminated by outliers or sample-specific corruptions. How to recover the tensor data that are corrupted by outliers and perform data clustering remains a challenging problem. This paper develops an outlier-robust tensor low-rank representation (OR-TLRR) method for simultaneous outlier detection and tensor data clustering based on the tensor singular value decomposition (t-SVD) algebraic framework. It is motivated by the recently proposed tensor-tensor product induced by invertible linear transforms that satisfy certain conditions. For tensor observations with arbitrary outlier corruptions, OR-TLRR has provable performance guarantee for exactly recovering the row space of clean data and detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is also proposed to handle the case when parts of the data are missing. Finally, extensive experim
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25932;&#23545;&#26694;&#26550;&#65292;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#31354;&#38388;&#26469;&#20272;&#35745;Riesz Representer&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#38750;&#28176;&#36817;&#22343;&#26041;&#36895;&#29575;&#20197;&#21450;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26465;&#20214;&#12290;&#36825;&#20010;&#26465;&#20214;&#20351;&#24471;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36827;&#34892;&#25512;&#26029;&#26102;&#26080;&#38656;&#26679;&#26412;&#20998;&#21106;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#39640;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2101.00009</link><description>&lt;p&gt;
&#23545;Riesz Representer&#30340;&#25932;&#23545;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adversarial Estimation of Riesz Representers. (arXiv:2101.00009v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.00009
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25932;&#23545;&#26694;&#26550;&#65292;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#31354;&#38388;&#26469;&#20272;&#35745;Riesz Representer&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#38750;&#28176;&#36817;&#22343;&#26041;&#36895;&#29575;&#20197;&#21450;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26465;&#20214;&#12290;&#36825;&#20010;&#26465;&#20214;&#20351;&#24471;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36827;&#34892;&#25512;&#26029;&#26102;&#26080;&#38656;&#26679;&#26412;&#20998;&#21106;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#39640;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22240;&#26524;&#21644;&#32467;&#26500;&#21442;&#25968;&#26159;&#22522;&#20110;&#24213;&#23618;&#22238;&#24402;&#30340;&#32447;&#24615;&#27867;&#20989;&#12290;Riesz Representer&#26159;&#21322;&#21442;&#25968;&#32447;&#24615;&#27867;&#20989;&#28176;&#36817;&#26041;&#24046;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25932;&#23545;&#26694;&#26550;&#65292;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#31354;&#38388;&#26469;&#20272;&#35745;Riesz Representer&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#38750;&#28176;&#36817;&#22343;&#26041;&#36895;&#29575;&#65292;&#20854;&#20013;&#28041;&#21450;&#19968;&#20010;&#31216;&#20026;&#20020;&#30028;&#21322;&#24452;&#30340;&#25277;&#35937;&#37327;&#65292;&#28982;&#21518;&#23558;&#20854;&#19987;&#38376;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20316;&#20026;&#20027;&#35201;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20020;&#30028;&#21322;&#24452;&#29702;&#35770;&#26469;&#35777;&#26126;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#26679;&#26412;&#20998;&#21106;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#8220;&#22797;&#26434;&#24230;-&#36895;&#29575;&#40065;&#26834;&#24615;&#8221;&#26465;&#20214;&#12290;&#36825;&#20010;&#26465;&#20214;&#20855;&#26377;&#23454;&#38469;&#21518;&#26524;&#65306;&#22312;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#38656;&#26679;&#26412;&#20998;&#21106;&#30340;&#25512;&#26029;&#65292;&#36825;&#21487;&#33021;&#20250;&#25552;&#39640;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#27169;&#25311;&#20013;&#23454;&#29616;&#20102;&#21517;&#20041;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many causal and structural parameters are linear functionals of an underlying regression. The Riesz representer is a key component in the asymptotic variance of a semiparametrically estimated linear functional. We propose an adversarial framework to estimate the Riesz representer using general function spaces. We prove a nonasymptotic mean square rate in terms of an abstract quantity called the critical radius, then specialize it for neural networks, random forests, and reproducing kernel Hilbert spaces as leading cases. Furthermore, we use critical radius theory -- in place of Donsker theory -- to prove asymptotic normality without sample splitting, uncovering a ``complexity-rate robustness'' condition. This condition has practical consequences: inference without sample splitting is possible in several machine learning settings, which may improve finite sample performance compared to sample splitting. Our estimators achieve nominal coverage in highly nonlinear simulations where previo
&lt;/p&gt;</description></item></channel></rss>