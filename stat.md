# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm.](http://arxiv.org/abs/2303.07287) | 本文提出了一种通过最大化一系列归一化矩来使用子高斯内在矩范实现紧凑的非渐进推断的方法，该方法可以导致更紧的Hoeffding子高斯浓度不等式，并且可以通过子高斯图检查具有有限样本大小的子高斯数据。 |
| [^2] | [Uniform Pessimistic Risk and Optimal Portfolio.](http://arxiv.org/abs/2303.07158) | 本文提出了一种称为统一悲观风险的综合$\alpha$-风险版本和基于风险获得最优组合的计算算法，该方法可以用于估计韩国股票的悲观最优组合模型。 |
| [^3] | [Correlation between upstreamness and downstreamness in random global value chains.](http://arxiv.org/abs/2303.06603) | 本文研究了全球价值链中产业和国家的上游和下游，发现同一产业部门的上游和下游之间存在正相关性。 |
| [^4] | [Data Dependent Regret Guarantees Against General Comparators for Full or Bandit Feedback.](http://arxiv.org/abs/2303.06526) | 该论文提出了一个数据相关的在线学习算法框架，可以在全专家反馈和Bandit反馈设置中具有数据相关的遗憾保证，适用于各种问题场景。 |
| [^5] | [Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap.](http://arxiv.org/abs/2303.06484) | 本文提出了一个广义神经坍塌假设，有效地包含了原始神经坍塌，并将其分解为两个目标：最小化类内变异性和最大化类间可分性。使用超球统一性作为量化这两个目标的统一框架，并提出了一个通用目标——超球统一性差（HUG），它由类间和类内超球统一性之间的差异定义。 |
| [^6] | [Learning interpretable causal networks from very large datasets, application to 400,000 medical records of breast cancer patients.](http://arxiv.org/abs/2303.06423) | 本文提出了一种更可靠和可扩展的因果发现方法（iMIIC），并在来自美国监测、流行病学和终末结果计划的396,179名乳腺癌患者的医疗保健数据上展示了其独特能力。超过90％的预测因果效应是正确的，而其余的意外直接和间接因果效应可以解释为诊断程序、治疗时间、患者偏好或社会经济差距。 |
| [^7] | [No-regret Algorithms for Fair Resource Allocation.](http://arxiv.org/abs/2303.06396) | 本文提出了一种无遗憾算法，用于公平资源分配问题，该算法可以实现$c_\alpha$-近似次线性遗憾，其中近似因子$c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445$，对于$0\leq \alpha < 1$。 |
| [^8] | [Stabilizing Transformer Training by Preventing Attention Entropy Collapse.](http://arxiv.org/abs/2303.06296) | 本文研究了Transformer的训练动态，发现低注意力熵伴随着高训练不稳定性，提出了一种简单而有效的解决方案$\sigma$Reparam，成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。 |
| [^9] | [Fast computation of permutation equivariant layers with the partition algebra.](http://arxiv.org/abs/2303.06208) | 本文提出了一种利用分区代数计算置换等变层输出和梯度的新算法，可以在输入大小的线性和二次时间内计算，有效性得到了在几个基准数据集上的证明。 |
| [^10] | [Deflated HeteroPCA: Overcoming the curse of ill-conditioning in heteroskedastic PCA.](http://arxiv.org/abs/2303.06198) | 本文提出了一种新的算法，称为缩减异方差PCA，它在克服病态问题的同时实现了近乎最优和无条件数的理论保证。 |
| [^11] | [DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference.](http://arxiv.org/abs/2303.06171) | 本文提出了一种新的DP-Fast MH算法，用于大规模贝叶斯推断，具有精确、快速和隐私保护的特点。 |
| [^12] | [Bounding the Probabilities of Benefit and Harm Through Sensitivity Parameters and Proxies.](http://arxiv.org/abs/2303.05396) | 本文提出了两种方法来限制未测量混杂下的受益和伤害概率，一种是通过敏感性参数计算概率的上限或下限，另一种是利用代理变量得到更紧的界限。 |
| [^13] | [Uncertainty Estimation by Fisher Information-based Evidential Deep Learning.](http://arxiv.org/abs/2303.02045) | 本文提出了一种基于Fisher信息的证据深度学习方法，用于解决高数据不确定性样本但注释为one-hot标签的情况下证据学习过程被过度惩罚并受到阻碍的问题。 |
| [^14] | [An Efficient Tester-Learner for Halfspaces.](http://arxiv.org/abs/2302.14853) | 我们提出了第一个在可测试学习模型中学习半空间的高效算法，该算法在多项式时间内运行，并输出一个在任何强对数凹目标分布下具有（信息理论上最优的）误差的假设。 |
| [^15] | [On Penalty-based Bilevel Gradient Descent Method.](http://arxiv.org/abs/2302.05185) | 本文提出了基于惩罚的双层梯度下降算法，解决了下层非强凸约束双层问题，实验表明该算法有效。 |
| [^16] | [Generalized Invariant Matching Property via LASSO.](http://arxiv.org/abs/2301.05975) | 本文提出了一种基于LASSO的广义不变匹配性质，通过制定具有内在稀疏性的高维问题，将不变匹配性质推广到仅目标被干预的重要情况，并提出了一种更加稳健和计算效率高的算法，改进了现有算法。 |
| [^17] | [A Dynamical System View of Langevin-Based Non-Convex Sampling.](http://arxiv.org/abs/2210.13867) | 本文提出了一种新的框架，通过利用动力系统理论中的几个工具来解决非凸采样中的重要挑战。对于一大类最先进的采样方案，它们在Wasserstein距离下的最后迭代收敛可以归结为对它们的连续时间对应物的研究，这是更好理解的。 |
| [^18] | [Label Propagation with Weak Supervision.](http://arxiv.org/abs/2210.03594) | 本文提出了一种利用弱监督信息的标签传播算法，通过利用未标记数据上的概率假设标签，结合局部几何特性和先验信息的质量，提供了一个误差界，并提出了一个框架，用于合并多个噪声信息源。在多个基准弱监督分类任务上展示了方法的能力，显示出对现有半监督和弱监督方法的改进。 |
| [^19] | [Sparsity by Redundancy: Solving $L_1$ with SGD.](http://arxiv.org/abs/2210.01212) | 该论文提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法，称为\textit{spred}，是$L_1$的精确求解器，可用于训练稀疏神经网络以执行基因选择任务和神经网络压缩任务，弥合了深度学习中的稀疏性和传统统计学习之间的差距。 |
| [^20] | [Estimating a potential without the agony of the partition function.](http://arxiv.org/abs/2208.09433) | 本文提出了一种不需要计算配分函数的潜力估计方法，基于最大后验估计（MAP）估计器，将问题重新表述为优化问题，并提出了一种最小作用量类型的势函数，使我们能够快速将优化问题解决为前馈双曲神经网络。 |
| [^21] | [Intrinsic dimension estimation for discrete metrics.](http://arxiv.org/abs/2207.09688) | 本文介绍了一种算法，用于推断嵌入离散空间的数据集的内在维度（ID），并在物种指纹的代谢组学数据集上展示了其准确性，发现一个令人惊讶的小ID，约为2的数量级。 |
| [^22] | [Multi-Frequency Joint Community Detection and Phase Synchronization.](http://arxiv.org/abs/2206.12276) | 本文提出了两种简单而高效的算法，利用MLE公式并从多个频率的信息中受益，用于解决具有相对相位的随机块模型上的联合社区检测和相位同步问题。 |
| [^23] | [Gradient Boosting Performs Gaussian Process Inference.](http://arxiv.org/abs/2206.05608) | 本文表明，基于对称决策树的梯度提升可以等价地重构为一种核方法，该方法收敛于某个核岭回归问题的解，从而使我们能够轻松地将梯度提升转换为从后验中提供更好的知识不确定性估计的采样器，通过后验方差的蒙特卡罗估计，从而允许更好的知识不确定性估计，导致改进的域外检测。 |
| [^24] | [Adversarial random forests for density estimation and generative modeling.](http://arxiv.org/abs/2205.09435) | 本文提出了一种使用对抗随机森林进行密度估计和数据合成的方法，该方法可以提供平滑的（非）条件密度，并允许完全合成数据生成，同时在各种表格数据基准测试中实现了与最先进的概率电路和深度学习模型相当或更好的性能，平均执行速度快了两个数量级。 |
| [^25] | [Matrix and graph representations of vine copula structures.](http://arxiv.org/abs/2205.04783) | 本文研究了Vine copula结构的矩阵和图形表示，证明了它们之间的等价性，并提出了一种新的方法来构建矩阵。这些算法的运行时间也被计算了。 |
| [^26] | [Non-Stationary Bandit Learning via Predictive Sampling.](http://arxiv.org/abs/2205.01970) | 本文提出了一种预测抽样算法，用于解决非平稳赌博机学习问题。该算法通过降低获取信息的优先级，解决了Thompson抽样在非平稳环境下表现不佳的问题，并在所有非平稳环境中优于Thompson抽样。 |
| [^27] | [Short-Term Density Forecasting of Low-Voltage Load using Bernstein-Polynomial Normalizing Flows.](http://arxiv.org/abs/2204.13939) | 本文提出了一种基于伯恩斯坦多项式归一化流的灵活条件密度预测方法，用于短期低压负荷预测，相比传统方法表现更好，可用于规划和运营低碳能源系统。 |
| [^28] | [Deep Feature Screening: Feature Selection for Ultra High-Dimensional Data via Deep Neural Networks.](http://arxiv.org/abs/2204.01682) | 本文提出了一种新的两步非参数方法，称为深度特征筛选（DeepFS），可以克服高维、低样本数据上的困难和挑战，并对超高维、低样本数据进行高精度的特征筛选。 |
| [^29] | [Generative Modeling Helps Weak Supervision (and Vice Versa).](http://arxiv.org/abs/2203.12023) | 本文提出了一种融合程序弱监督和生成对抗网络的模型，通过对齐离散潜在变量和弱监督派生的标签估计，改善了未观察到的标签的估计，实现了数据增强。 |
| [^30] | [Effects of Epileptiform Activity on Discharge Outcome in Critically Ill Patients.](http://arxiv.org/abs/2203.04920) | 本研究旨在探讨癫痫样活动对危重病患者出院结局的影响，通过回顾性横断面研究发现，如果每个人都经历了某种EA负荷并且未接受治疗，出院mRS会发生变化。 |
| [^31] | [PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX.](http://arxiv.org/abs/2202.04110) | PGMax是一个用于离散概率图模型的因子图工具，可以在JAX中自动运行高效且可扩展的循环置信传播，与现有替代方案相比，PGMax获得了更高质量的推理结果，推理时间加速高达三个数量级。 |
| [^32] | [Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification.](http://arxiv.org/abs/2202.00395) | 本文提出了一种简单直接的贝叶斯误差估计器，可以用于评估具有最先进性能的分类器的标准，并可用于检测测试集过拟合。与其他方法相比，我们的方法是无模型的，甚至是无实例的。此外，它没有超参数，并且在实证上比几个基线给出了更准确的贝叶斯误差估计。 |
| [^33] | [A semiparametric approach for interactive fixed effects panel data models.](http://arxiv.org/abs/2201.11482) | 本文提出了一种半参数方法，用于估计和推断具有交互固定效应的面板数据模型中的回归参数，该方法具有简单的偏最小二乘形式，不需要迭代过程和先前的因子估计，并且可以通过使用横截面自助法实现统一有效的推断。 |
| [^34] | [Causal inference with misspecified exposure mappings: separating definitions and assumptions.](http://arxiv.org/abs/2103.06471) | 本文提出了一种新的方法，将曝光映射的两个作用分开，从而在曝光被错误指定时精确估计曝光效应，避免了常常是可疑的假设。 |
| [^35] | [NOMU: Neural Optimization-based Model Uncertainty.](http://arxiv.org/abs/2102.13640) | NOMU是一种新的神经网络模型，可以在无噪声设置下，通过设计一个由两个连接的子NN组成的网络架构，并使用精心设计的损失函数进行训练，来捕捉NN的模型不确定性。该模型满足五个关于模型不确定性的重要愿望。 |
| [^36] | [The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information.](http://arxiv.org/abs/2102.10019) | 本文证明了不确定性对不同人群的影响是不平等的，虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化。我们提出了一种名为平权信息的策略，可以消除这种差异并扩大机会的获取，这可以作为平权行动的替代方案。 |
| [^37] | [LOCUS: A Novel Decomposition Method for Brain Network Connectivity Matrices using Low-rank Structure with Uniform Sparsity.](http://arxiv.org/abs/2008.08915) | LOCUS是一种新的大脑网络连接矩阵分解方法，使用低秩结构和均匀稀疏性，能够更有效和准确地分离连接矩阵源，有望成为理解大脑组织的关键。 |
| [^38] | [FRMDN: Flow-based Recurrent Mixture Density Network.](http://arxiv.org/abs/2008.02144) | 本文提出了一种基于流的循环混合密度网络，通过在每个时间步上定义一个高斯混合模型，将循环混合密度网络推广到非线性变换的目标序列上，该模型在图像序列的拟合度上表现显著，具有显著的建模能力，在对数似然度量方面优于其他最先进的方法。 |
| [^39] | [Anomaly Awareness.](http://arxiv.org/abs/2007.14462) | 该论文提出了一种新的异常检测算法，称为异常感知。该算法通过修改成本函数来学习正常事件，并了解异常事件。该方法在不同的粒子物理情况和标准计算机视觉任务中的应用，能够有效地识别以前未见过的异常，并在了解足够多的异常时变得更加稳健。 |
| [^40] | [An FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning.](http://arxiv.org/abs/2005.04646) | 本文提出了一种基于FPGA的轻量级强化学习方法，利用OS-ELM算法进行训练，避免了DQN需要大量缓冲区和批处理的问题，并通过L2正则化和谱归一化的组合使得强化学习更加稳定。 |
| [^41] | [Feature-Based Interpolation and Geodesics in the Latent Spaces of Generative Models.](http://arxiv.org/abs/1904.03445) | 本文提出了一种通用和统一的插值方法，它同时允许我们在任意密度的情况下搜索测地线和插值曲线。最大化曲线的质量度量可以等价地理解为在空间上某种重新定义的黎曼度量下搜索测地线。 |
| [^42] | [Improving SGD convergence by online linear regression of gradients in multiple statistically relevant directions.](http://arxiv.org/abs/1901.11457) | 本文提出了一种在线线性回归的方法，通过在多个统计相关方向上进行梯度的回归来提高SGD的收敛性，解决了标准方法只考虑单个方向的问题，同时避免了二阶方法的成本和数值不稳定性。 |

# 详细

[^1]: 通过子高斯内在矩范实现紧凑的非渐进推断

    Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm. (arXiv:2303.07287v1 [stat.ML])

    [http://arxiv.org/abs/2303.07287](http://arxiv.org/abs/2303.07287)

    本文提出了一种通过最大化一系列归一化矩来使用子高斯内在矩范实现紧凑的非渐进推断的方法，该方法可以导致更紧的Hoeffding子高斯浓度不等式，并且可以通过子高斯图检查具有有限样本大小的子高斯数据。

    This paper proposes a method of achieving tight non-asymptotic inference by using sub-Gaussian intrinsic moment norm through maximizing a series of normalized moments, which can lead to tighter Hoeffding's sub-Gaussian concentration inequalities and can be checked with sub-Gaussian plot for sub-Gaussian data with a finite sample size.

    在非渐进统计推断中，子高斯分布的方差类型参数起着至关重要的作用。然而，基于经验矩生成函数（MGF）的直接估计这些参数是不可行的。为此，我们建议通过最大化一系列归一化矩来使用子高斯内在矩范[Buldygin和Kozachenko（2000），定理1.3]。重要的是，推荐的范数不仅可以恢复相应MGF的指数矩界限，而且还可以导致更紧的Hoeffding子高斯浓度不等式。在实践中，我们提出了一种直观的方法，通过子高斯图检查具有有限样本大小的子高斯数据。可以通过简单的插入方法鲁棒地估计内在矩范数。我们的理论结果应用于非渐进分析，包括多臂赌博机。

    In non-asymptotic statistical inferences, variance-type parameters of sub-Gaussian distributions play a crucial role. However, direct estimation of these parameters based on the empirical moment generating function (MGF) is infeasible. To this end, we recommend using a sub-Gaussian intrinsic moment norm [Buldygin and Kozachenko (2000), Theorem 1.3] through maximizing a series of normalized moments. Importantly, the recommended norm can not only recover the exponential moment bounds for the corresponding MGFs, but also lead to tighter Hoeffding's sub-Gaussian concentration inequalities. In practice, {\color{black} we propose an intuitive way of checking sub-Gaussian data with a finite sample size by the sub-Gaussian plot}. Intrinsic moment norm can be robustly estimated via a simple plug-in approach. Our theoretical results are applied to non-asymptotic analysis, including the multi-armed bandit.
    
[^2]: 统一悲观风险和最优组合

    Uniform Pessimistic Risk and Optimal Portfolio. (arXiv:2303.07158v1 [q-fin.PM])

    [http://arxiv.org/abs/2303.07158](http://arxiv.org/abs/2303.07158)

    本文提出了一种称为统一悲观风险的综合$\alpha$-风险版本和基于风险获得最优组合的计算算法，该方法可以用于估计韩国股票的悲观最优组合模型。

    This paper proposes a version of integrated $\alpha$-risk called the uniform pessimistic risk and a computational algorithm to obtain an optimal portfolio based on the risk. The proposed method can be used to estimate the pessimistic optimal portfolio models for Korean stocks.

    资产配置的最优性已经在风险度量的理论分析中得到广泛讨论。悲观主义是一种超越传统最优组合模型的最有吸引力的方法之一，$\alpha$-风险在推导出广泛的悲观最优组合中起着关键作用。然而，由悲观风险评估的最优组合的估计仍然具有挑战性，因为缺乏可用的估计模型和计算算法。在本研究中，我们提出了一种称为统一悲观风险的综合$\alpha$-风险版本和基于风险获得最优组合的计算算法。此外，我们从多个分位数回归、适当的评分规则和分布鲁棒优化三个不同的方法来研究所提出的风险的理论性质。同时，统一悲观风险被应用于估计韩国股票的悲观最优组合模型。

    The optimality of allocating assets has been widely discussed with the theoretical analysis of risk measures. Pessimism is one of the most attractive approaches beyond the conventional optimal portfolio model, and the $\alpha$-risk plays a crucial role in deriving a broad class of pessimistic optimal portfolios. However, estimating an optimal portfolio assessed by a pessimistic risk is still challenging due to the absence of an available estimation model and a computational algorithm. In this study, we propose a version of integrated $\alpha$-risk called the uniform pessimistic risk and the computational algorithm to obtain an optimal portfolio based on the risk. Further, we investigate the theoretical properties of the proposed risk in view of three different approaches: multiple quantile regression, the proper scoring rule, and distributionally robust optimization. Also, the uniform pessimistic risk is applied to estimate the pessimistic optimal portfolio models for the Korean stock 
    
[^3]: 随机全球价值链中上游和下游之间的相关性

    Correlation between upstreamness and downstreamness in random global value chains. (arXiv:2303.06603v1 [stat.AP])

    [http://arxiv.org/abs/2303.06603](http://arxiv.org/abs/2303.06603)

    本文研究了全球价值链中产业和国家的上游和下游，发现同一产业部门的上游和下游之间存在正相关性。

    This paper studies the upstreamness and downstreamness of industries and countries in global value chains, and finds a positive correlation between upstreamness and downstreamness of the same industrial sector.

    本文关注全球价值链中产业和国家的上游和下游。上游和下游分别衡量产业部门与最终消费和初级输入之间的平均距离，并基于最常用的全球投入产出表数据库（例如世界投入产出数据库（WIOD））进行计算。最近，Antr\`as和Chor在1995-2011年的数据中报告了一个令人困惑和反直觉的发现，即（在国家层面上）上游似乎与下游呈正相关，相关斜率接近+1。这种效应随时间和跨国家稳定存在，并已得到后续分析的确认和验证。我们分析了一个简单的随机投入产出表模型，并展示了在最小和现实的结构假设下，同一产业部门的上游和下游之间存在正相关性，具有相关性。

    This paper is concerned with upstreamness and downstreamness of industries and countries in global value chains. Upstreamness and downstreamness measure respectively the average distance of an industrial sector from final consumption and from primary inputs, and they are computed from based on the most used global Input-Output tables databases, e.g., the World Input-Output Database (WIOD). Recently, Antr\`as and Chor reported a puzzling and counter-intuitive finding in data from the period 1995-2011, namely that (at country level) upstreamness appears to be positively correlated with downstreamness, with a correlation slope close to $+1$. This effect is stable over time and across countries, and it has been confirmed and validated by later analyses. We analyze a simple model of random Input/Output tables, and we show that, under minimal and realistic structural assumptions, there is a positive correlation between upstreamness and downstreamness of the same industrial sector, with corre
    
[^4]: 数据相关的在线学习算法框架

    Data Dependent Regret Guarantees Against General Comparators for Full or Bandit Feedback. (arXiv:2303.06526v1 [cs.LG])

    [http://arxiv.org/abs/2303.06526](http://arxiv.org/abs/2303.06526)

    该论文提出了一个数据相关的在线学习算法框架，可以在全专家反馈和Bandit反馈设置中具有数据相关的遗憾保证，适用于各种问题场景。

    This paper proposes a data-dependent online learning algorithm framework that has data-dependent regret guarantees in both full expert feedback and bandit feedback settings, applicable for a wide variety of problem scenarios.

    我们研究了对抗性在线学习问题，并创建了一个完全在线的算法框架，具有在全专家反馈和Bandit反馈设置中具有数据相关的遗憾保证。我们研究了我们的算法对一般比较器的预期性能，使其适用于各种问题场景。我们的算法从通用预测角度工作，使用的性能度量是对任意比较器序列的预期遗憾，即我们的损失与竞争损失序列之间的差异。竞争类可以设计为包括固定臂选择、切换Bandit、上下文Bandit、周期Bandit或任何其他感兴趣的竞争。竞争类中的序列通常由具体应用程序确定，并应相应地设计。我们的算法既不使用也不需要任何有关损失序列的初步信息，完全在线。其

    We study the adversarial online learning problem and create a completely online algorithmic framework that has data dependent regret guarantees in both full expert feedback and bandit feedback settings. We study the expected performance of our algorithm against general comparators, which makes it applicable for a wide variety of problem scenarios. Our algorithm works from a universal prediction perspective and the performance measure used is the expected regret against arbitrary comparator sequences, which is the difference between our losses and a competing loss sequence. The competition class can be designed to include fixed arm selections, switching bandits, contextual bandits, periodic bandits or any other competition of interest. The sequences in the competition class are generally determined by the specific application at hand and should be designed accordingly. Our algorithm neither uses nor needs any preliminary information about the loss sequences and is completely online. Its
    
[^5]: 通过超球统一性差填补神经坍塌的泛化和解耦

    Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap. (arXiv:2303.06484v1 [cs.LG])

    [http://arxiv.org/abs/2303.06484](http://arxiv.org/abs/2303.06484)

    本文提出了一个广义神经坍塌假设，有效地包含了原始神经坍塌，并将其分解为两个目标：最小化类内变异性和最大化类间可分性。使用超球统一性作为量化这两个目标的统一框架，并提出了一个通用目标——超球统一性差（HUG），它由类间和类内超球统一性之间的差异定义。

    This paper proposes a generalized neural collapse hypothesis that effectively subsumes the original neural collapse and decomposes it into two objectives: minimizing intra-class variability and maximizing inter-class separability. The authors use hyperspherical uniformity as a unified framework to quantify these objectives and propose a general objective, hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical uniformity.

    神经坍塌现象描述了深度神经网络的底层几何对称性，其中深度学习的特征和分类器都收敛于一个等角紧框架。已经证明，交叉熵损失和均方误差都可以导致神经坍塌。我们消除了神经坍塌对特征维度和类别数量的关键假设，然后提出了一个广义神经坍塌假设，有效地包含了原始神经坍塌。受神经坍塌描述神经网络训练目标的启发，我们将广义神经坍塌分解为两个目标：最小化类内变异性和最大化类间可分性。然后，我们使用超球统一性（它描述了单位超球上均匀性的程度）作为量化这两个目标的统一框架。最后，我们提出了一个通用目标——超球统一性差（HUG），它由类间和类内超球统一性之间的差异定义。

    The neural collapse (NC) phenomenon describes an underlying geometric symmetry for deep neural networks, where both deeply learned features and classifiers converge to a simplex equiangular tight frame. It has been shown that both cross-entropy loss and mean square error can provably lead to NC. We remove NC's key assumption on the feature dimension and the number of classes, and then present a generalized neural collapse (GNC) hypothesis that effectively subsumes the original NC. Inspired by how NC characterizes the training target of neural networks, we decouple GNC into two objectives: minimal intra-class variability and maximal inter-class separability. We then use hyperspherical uniformity (which characterizes the degree of uniformity on the unit hypersphere) as a unified framework to quantify these two objectives. Finally, we propose a general objective -- hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical un
    
[^6]: 从大型数据集中学习可解释的因果网络，以乳腺癌患者的40万份医疗记录为例

    Learning interpretable causal networks from very large datasets, application to 400,000 medical records of breast cancer patients. (arXiv:2303.06423v1 [q-bio.QM])

    [http://arxiv.org/abs/2303.06423](http://arxiv.org/abs/2303.06423)

    本文提出了一种更可靠和可扩展的因果发现方法（iMIIC），并在来自美国监测、流行病学和终末结果计划的396,179名乳腺癌患者的医疗保健数据上展示了其独特能力。超过90％的预测因果效应是正确的，而其余的意外直接和间接因果效应可以解释为诊断程序、治疗时间、患者偏好或社会经济差距。

    This paper proposes a more reliable and scalable causal discovery method (iMIIC) and showcases its unique capabilities on healthcare data from 396,179 breast cancer patients from the US Surveillance, Epidemiology, and End Results program. Over 90% of predicted causal effects appear correct, while the remaining unexpected direct and indirect causal effects can be interpreted in terms of diagnostic procedures, therapeutic timing, patient preference or socio-economic disparity.

    发现因果效应是科学研究的核心，但当只有观察数据可用时，这仍然具有挑战性。在实践中，因果网络难以学习和解释，并且仅限于相对较小的数据集。我们报告了一种更可靠和可扩展的因果发现方法（iMIIC），基于一般的互信息最大原则，它极大地提高了推断的因果关系的精度，同时区分了真正的原因和假定的和潜在的因果效应。我们展示了iMIIC在来自美国监测、流行病学和终末结果计划的396,179名乳腺癌患者的合成和现实医疗保健数据上的独特能力。超过90％的预测因果效应是正确的，而其余的意外直接和间接因果效应可以解释为诊断程序、治疗时间、患者偏好或社会经济差距。iMIIC的独特能力开辟了发现可靠和可解释的因果网络的新途径。

    Discovering causal effects is at the core of scientific investigation but remains challenging when only observational data is available. In practice, causal networks are difficult to learn and interpret, and limited to relatively small datasets. We report a more reliable and scalable causal discovery method (iMIIC), based on a general mutual information supremum principle, which greatly improves the precision of inferred causal relations while distinguishing genuine causes from putative and latent causal effects. We showcase iMIIC on synthetic and real-life healthcare data from 396,179 breast cancer patients from the US Surveillance, Epidemiology, and End Results program. More than 90\% of predicted causal effects appear correct, while the remaining unexpected direct and indirect causal effects can be interpreted in terms of diagnostic procedures, therapeutic timing, patient preference or socio-economic disparity. iMIIC's unique capabilities open up new avenues to discover reliable and
    
[^7]: 无遗憾算法用于公平资源分配

    No-regret Algorithms for Fair Resource Allocation. (arXiv:2303.06396v1 [cs.LG])

    [http://arxiv.org/abs/2303.06396](http://arxiv.org/abs/2303.06396)

    本文提出了一种无遗憾算法，用于公平资源分配问题，该算法可以实现$c_\alpha$-近似次线性遗憾，其中近似因子$c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445$，对于$0\leq \alpha < 1$。

    This paper proposes a no-regret algorithm for fair resource allocation, which achieves $c_\alpha$-approximate sublinear regret with the approximation factor $c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445,$ for $0\leq \alpha < 1$.

    本文考虑了一个公平资源分配问题，该问题在无遗憾设置下针对无限制的对手。目标是以在线方式公平地分配多个代理的资源，使得最优静态预知分配和在线策略的代理的聚合α-公平效用之差随时间增长的速度为次线性。由于α-公平性函数的非加性特性，该问题具有挑战性。先前的研究表明，该问题不存在具有次线性标准遗憾的在线策略。本文提出了一种高效的在线资源分配策略，称为在线比例公平（OPF），该策略实现了$c_\alpha$-近似次线性遗憾，其中近似因子$c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445$，对于$0\leq \alpha < 1$。该问题的$c_\alpha$-遗憾上界展现出了一个令人惊讶的相变现象。遗憾上界从一个幂函数变为一个对数函数。

    We consider a fair resource allocation problem in the no-regret setting against an unrestricted adversary. The objective is to allocate resources equitably among several agents in an online fashion so that the difference of the aggregate $\alpha$-fair utilities of the agents between an optimal static clairvoyant allocation and that of the online policy grows sub-linearly with time. The problem is challenging due to the non-additive nature of the $\alpha$-fairness function. Previously, it was shown that no online policy can exist for this problem with a sublinear standard regret. In this paper, we propose an efficient online resource allocation policy, called Online Proportional Fair (OPF), that achieves $c_\alpha$-approximate sublinear regret with the approximation factor $c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445,$ for $0\leq \alpha < 1$. The upper bound to the $c_\alpha$-regret for this problem exhibits a surprising phase transition phenomenon. The regret bound changes from a power
    
[^8]: 防止注意力熵崩溃的Transformer训练稳定性研究

    Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])

    [http://arxiv.org/abs/2303.06296](http://arxiv.org/abs/2303.06296)

    本文研究了Transformer的训练动态，发现低注意力熵伴随着高训练不稳定性，提出了一种简单而有效的解决方案$\sigma$Reparam，成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。

    This paper investigates the training dynamics of Transformers and proposes a simple and efficient solution, $\sigma$Reparam, to prevent entropy collapse in the attention layers, promoting more stable training.

    训练稳定性对于Transformer至关重要。本文通过研究注意力层的演变来探究Transformer的训练动态。特别地，我们在训练过程中跟踪每个注意力头的注意力熵，这是模型锐度的代理。我们发现，在不同的架构和任务中存在一种常见模式，即低注意力熵伴随着高训练不稳定性，这可能采取振荡损失或发散的形式。我们将病态低注意力熵，对应高度集中的注意力分数，称为$\textit{熵崩溃}$。作为一种解决方案，我们提出了$\sigma$Reparam，一种简单而有效的解决方案，其中我们使用谱归一化和额外的学习标量重新参数化所有线性层。我们证明了所提出的重新参数化成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。此外，我们

    Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\textit{entropy collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that the proposed reparameterization successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we 
    
[^9]: 利用分区代数快速计算置换等变层

    Fast computation of permutation equivariant layers with the partition algebra. (arXiv:2303.06208v1 [cs.LG])

    [http://arxiv.org/abs/2303.06208](http://arxiv.org/abs/2303.06208)

    本文提出了一种利用分区代数计算置换等变层输出和梯度的新算法，可以在输入大小的线性和二次时间内计算，有效性得到了在几个基准数据集上的证明。

    This paper proposes a new algorithm for computing the output and gradient of permutation equivariant linear layers using the partition algebra, which can be computed in time linear and quadratic in the input size, respectively. The effectiveness of the approach is demonstrated on several benchmark datasets.

    线性神经网络层，无论是等变还是不变于其输入的排列，都是现代深度学习架构的核心构建块。例如DeepSets的层，以及出现在transformers和一些图神经网络的注意力块中的线性层。置换等变线性层的空间可以被识别为某个对称群表示的不变子空间，并且最近的工作通过展示一组基础，其向量是标准基础元素在对称群作用下轨道的总和，来参数化这个空间。参数化打开了通过梯度下降学习置换等变线性层权重的可能性。置换等变线性层的空间是分区代数的一般化，这是一种在统计物理学中首次发现的对象，与对称群的表示论有着深刻的联系，而上述基础与分区代数的基础密切相关。在本文中，我们展示了如何利用这种联系，在输入大小的线性时间内计算置换等变线性层的输出，并在输入大小的二次时间内计算损失相对于权重的梯度。我们的方法基于一种计算分区代数在向量上作用的新算法，我们称之为“分区卷积”。我们展示了分区卷积可以在输入向量大小的线性时间内计算，并且可以用于在输入大小的线性和二次时间内计算置换等变线性层的输出和梯度，分别。我们还展示了如何使用分区卷积来计算某些非线性置换等变层的输出和梯度，并在几个基准数据集上展示了我们方法的有效性。

    Linear neural network layers that are either equivariant or invariant to permutations of their inputs form core building blocks of modern deep learning architectures. Examples include the layers of DeepSets, as well as linear layers occurring in attention blocks of transformers and some graph neural networks. The space of permutation equivariant linear layers can be identified as the invariant subspace of a certain symmetric group representation, and recent work parameterized this space by exhibiting a basis whose vectors are sums over orbits of standard basis elements with respect to the symmetric group action. A parameterization opens up the possibility of learning the weights of permutation equivariant linear layers via gradient descent. The space of permutation equivariant linear layers is a generalization of the partition algebra, an object first discovered in statistical physics with deep connections to the representation theory of the symmetric group, and the basis described abo
    
[^10]: 克服异方差PCA中病态问题的缩减算法

    Deflated HeteroPCA: Overcoming the curse of ill-conditioning in heteroskedastic PCA. (arXiv:2303.06198v1 [math.ST])

    [http://arxiv.org/abs/2303.06198](http://arxiv.org/abs/2303.06198)

    本文提出了一种新的算法，称为缩减异方差PCA，它在克服病态问题的同时实现了近乎最优和无条件数的理论保证。

    This paper proposes a novel algorithm, called Deflated-HeteroPCA, that overcomes the curse of ill-conditioning in heteroskedastic PCA while achieving near-optimal and condition-number-free theoretical guarantees.

    本文关注于从受污染的数据中估计低秩矩阵X*的列子空间。当存在异方差噪声和不平衡的维度（即n2 >> n1）时，如何在容纳最广泛的信噪比范围的同时获得最佳的统计精度变得特别具有挑战性。虽然最先进的算法HeteroPCA成为解决这个问题的强有力的解决方案，但它遭受了“病态问题的诅咒”，即随着X*的条件数增长，其性能会下降。为了克服这个关键问题而不影响允许的信噪比范围，我们提出了一种新的算法，称为缩减异方差PCA，它在$\ell_2$和$\ell_{2,\infty}$统计精度方面实现了近乎最优和无条件数的理论保证。所提出的算法将谱分成两部分

    This paper is concerned with estimating the column subspace of a low-rank matrix $\boldsymbol{X}^\star \in \mathbb{R}^{n_1\times n_2}$ from contaminated data. How to obtain optimal statistical accuracy while accommodating the widest range of signal-to-noise ratios (SNRs) becomes particularly challenging in the presence of heteroskedastic noise and unbalanced dimensionality (i.e., $n_2\gg n_1$). While the state-of-the-art algorithm $\textsf{HeteroPCA}$ emerges as a powerful solution for solving this problem, it suffers from "the curse of ill-conditioning," namely, its performance degrades as the condition number of $\boldsymbol{X}^\star$ grows. In order to overcome this critical issue without compromising the range of allowable SNRs, we propose a novel algorithm, called $\textsf{Deflated-HeteroPCA}$, that achieves near-optimal and condition-number-free theoretical guarantees in terms of both $\ell_2$ and $\ell_{2,\infty}$ statistical accuracy. The proposed algorithm divides the spectrum
    
[^11]: DP-Fast MH: 大规模贝叶斯推断的私有、快速、准确的Metropolis-Hastings算法

    DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference. (arXiv:2303.06171v1 [cs.LG])

    [http://arxiv.org/abs/2303.06171](http://arxiv.org/abs/2303.06171)

    本文提出了一种新的DP-Fast MH算法，用于大规模贝叶斯推断，具有精确、快速和隐私保护的特点。

    This paper proposes a new DP-Fast MH algorithm for large-scale Bayesian inference, which is accurate, fast, and privacy-preserving.

    贝叶斯推断提供了一个从复杂数据中学习和在不确定性下推理的原则性框架。它已经广泛应用于机器学习任务，如医学诊断、药物设计和政策制定。在这些常见应用中，数据可能非常敏感。差分隐私（DP）提供了具有强大最坏情况隐私保证的数据分析工具，并已发展成为隐私保护数据分析的主要方法。在本文中，我们研究了Metropolis-Hastings（MH）算法，这是最基本的MCMC方法之一，用于差分隐私下的大规模贝叶斯推断。虽然大多数现有的私有MCMC算法为了获得隐私而牺牲了准确性和效率，但我们提供了第一个精确且快速的DP MH算法，大多数迭代中仅使用一个小批量的数据。我们进一步揭示了隐私、可扩展性（即批量大小）和效率（即收敛速度）之间的三重权衡，从理论上说明了这一点。

    Bayesian inference provides a principled framework for learning from complex data and reasoning under uncertainty. It has been widely applied in machine learning tasks such as medical diagnosis, drug design, and policymaking. In these common applications, the data can be highly sensitive. Differential privacy (DP) offers data analysis tools with powerful worst-case privacy guarantees and has been developed as the leading approach in privacy-preserving data analysis. In this paper, we study Metropolis-Hastings (MH), one of the most fundamental MCMC methods, for large-scale Bayesian inference under differential privacy. While most existing private MCMC algorithms sacrifice accuracy and efficiency to obtain privacy, we provide the first exact and fast DP MH algorithm, using only a minibatch of data in most iterations. We further reveal, for the first time, a three-way trade-off among privacy, scalability (i.e. the batch size), and efficiency (i.e. the convergence rate), theoretically char
    
[^12]: 通过敏感性参数和代理变量限制受益和伤害的概率

    Bounding the Probabilities of Benefit and Harm Through Sensitivity Parameters and Proxies. (arXiv:2303.05396v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2303.05396](http://arxiv.org/abs/2303.05396)

    本文提出了两种方法来限制未测量混杂下的受益和伤害概率，一种是通过敏感性参数计算概率的上限或下限，另一种是利用代理变量得到更紧的界限。

    This paper presents two methods for bounding the probabilities of benefit and harm under unmeasured confounding, one is to compute the upper or lower bound of the probability through sensitivity parameters, and the other is to derive tighter bounds using a measured nondifferential proxy of the unmeasured confounder.

    我们提出了两种方法来限制未测量混杂下的受益和伤害概率。第一种方法计算任一概率的（上限或下限），作为观察数据分布和两个直观敏感性参数的函数，然后可以将其呈现给分析师作为2-D图以协助其决策。第二种方法假设存在未测量混杂因素的测量非差异代理变量（即直接效应）。使用此代理变量，可以从仅观察到的数据分布中导出比现有界限更紧的界限。

    We present two methods for bounding the probabilities of benefit and harm under unmeasured confounding. The first method computes the (upper or lower) bound of either probability as a function of the observed data distribution and two intuitive sensitivity parameters which, then, can be presented to the analyst as a 2-D plot to assist her in decision making. The second method assumes the existence of a measured nondifferential proxy (i.e., direct effect) of the unmeasured confounder. Using this proxy, tighter bounds than the existing ones can be derived from just the observed data distribution.
    
[^13]: 基于Fisher信息的证据深度学习方法用于不确定性估计

    Uncertainty Estimation by Fisher Information-based Evidential Deep Learning. (arXiv:2303.02045v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02045](http://arxiv.org/abs/2303.02045)

    本文提出了一种基于Fisher信息的证据深度学习方法，用于解决高数据不确定性样本但注释为one-hot标签的情况下证据学习过程被过度惩罚并受到阻碍的问题。

    This paper proposes a Fisher Information-based Evidential Deep Learning method to address the problem of over-penalization and hindrance in evidence learning for high data uncertainty samples annotated with one-hot labels.

    不确定性估计是使深度学习在实际应用中可靠的关键因素。最近提出的证据神经网络通过将网络输出视为证据来参数化狄利克雷分布，明确考虑不同的不确定性，并在不确定性估计方面取得了令人印象深刻的性能。然而，对于高数据不确定性样本但注释为one-hot标签的情况，这些错误标记的类别的证据学习过程会被过度惩罚并受到阻碍。为了解决这个问题，我们提出了一种新的方法，基于Fisher信息的证据深度学习（$\mathcal{I}$-EDL）。特别地，我们引入Fisher信息矩阵（FIM）来衡量每个样本所携带的证据的信息量，根据这个信息量，我们可以动态地重新加权目标损失项，使网络更加专注于不确定类别的表示学习。我们的网络的泛化能力通过优化进一步提高。

    Uncertainty estimation is a key factor that makes deep learning reliable in practical applications. Recently proposed evidential neural networks explicitly account for different uncertainties by treating the network's outputs as evidence to parameterize the Dirichlet distribution, and achieve impressive performance in uncertainty estimation. However, for high data uncertainty samples but annotated with the one-hot label, the evidence-learning process for those mislabeled classes is over-penalized and remains hindered. To address this problem, we propose a novel method, Fisher Information-based Evidential Deep Learning ($\mathcal{I}$-EDL). In particular, we introduce Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which we can dynamically reweight the objective loss terms to make the network more focused on the representation learning of uncertain classes. The generalization ability of our network is further improved by opt
    
[^14]: 半空间的高效测试学习算法

    An Efficient Tester-Learner for Halfspaces. (arXiv:2302.14853v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14853](http://arxiv.org/abs/2302.14853)

    我们提出了第一个在可测试学习模型中学习半空间的高效算法，该算法在多项式时间内运行，并输出一个在任何强对数凹目标分布下具有（信息理论上最优的）误差的假设。

    We propose the first efficient algorithm for learning halfspaces in the testable learning model, which runs in polynomial time and outputs a hypothesis with (information-theoretically optimal) error for any strongly log-concave target distribution.

    我们提出了第一个在Rubinfeld和Vasilyan（2023）最近定义的可测试学习模型中学习半空间的高效算法。在这个模型中，当训练集通过相关测试时，学习者证明其输出假设的准确性接近最优，并且从某些目标分布（例如高斯分布）中抽取的训练集必须通过测试。这个模型比分布特定的不可知或Massart噪声模型更具挑战性，因为如果分布假设不成立，学习者可以任意失败。我们考虑目标分布为高斯分布（或更一般的任何强对数凹分布）的$d$维情况，噪声模型为Massart或对抗性（不可知）。对于Massart噪声，我们的测试学习算法在多项式时间内运行，并输出一个在任何强对数凹目标分布下具有（信息理论上最优的）误差$\mathsf{opt}+\epsilon$的假设。

    We give the first efficient algorithm for learning halfspaces in the testable learning model recently defined by Rubinfeld and Vasilyan (2023). In this model, a learner certifies that the accuracy of its output hypothesis is near optimal whenever the training set passes an associated test, and training sets drawn from some target distribution -- e.g., the Gaussian -- must pass the test. This model is more challenging than distribution-specific agnostic or Massart noise models where the learner is allowed to fail arbitrarily if the distributional assumption does not hold.  We consider the setting where the target distribution is Gaussian (or more generally any strongly log-concave distribution) in $d$ dimensions and the noise model is either Massart or adversarial (agnostic). For Massart noise, our tester-learner runs in polynomial time and outputs a hypothesis with (information-theoretically optimal) error $\mathsf{opt} + \epsilon$ for any strongly log-concave target distribution. For 
    
[^15]: 基于惩罚的双层梯度下降方法研究

    On Penalty-based Bilevel Gradient Descent Method. (arXiv:2302.05185v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05185](http://arxiv.org/abs/2302.05185)

    本文提出了基于惩罚的双层梯度下降算法，解决了下层非强凸约束双层问题，实验表明该算法有效。

    This paper proposes a penalty-based bilevel gradient descent algorithm to solve the constrained bilevel problem without lower-level strong convexity, and experiments show its efficiency.

    双层优化在超参数优化、元学习和强化学习等领域有广泛应用，但是双层优化问题难以解决。最近的可扩展双层算法主要集中在下层目标函数是强凸或无约束的双层优化问题上。在本文中，我们通过惩罚方法来解决双层问题。我们证明，在一定条件下，惩罚重构可以恢复原始双层问题的解。此外，我们提出了基于惩罚的双层梯度下降（PBGD）算法，并证明了其在下层非强凸约束双层问题上的有限时间收敛性。实验展示了所提出的PBGD算法的效率。

    Bilevel optimization enjoys a wide range of applications in hyper-parameter optimization, meta-learning and reinforcement learning. However, bilevel optimization problems are difficult to solve. Recent progress on scalable bilevel algorithms mainly focuses on bilevel optimization problems where the lower-level objective is either strongly convex or unconstrained. In this work, we tackle the bilevel problem through the lens of the penalty method. We show that under certain conditions, the penalty reformulation recovers the solutions of the original bilevel problem. Further, we propose the penalty-based bilevel gradient descent (PBGD) algorithm and establish its finite-time convergence for the constrained bilevel problem without lower-level strong convexity. Experiments showcase the efficiency of the proposed PBGD algorithm.
    
[^16]: 基于LASSO的广义不变匹配性质

    Generalized Invariant Matching Property via LASSO. (arXiv:2301.05975v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2301.05975](http://arxiv.org/abs/2301.05975)

    本文提出了一种基于LASSO的广义不变匹配性质，通过制定具有内在稀疏性的高维问题，将不变匹配性质推广到仅目标被干预的重要情况，并提出了一种更加稳健和计算效率高的算法，改进了现有算法。

    This paper proposes a generalized invariant matching property via LASSO, which formulates a high-dimensional problem with intrinsic sparsity and extends the invariant matching property to the important setting when only the target is intervened. The paper also presents a more robust and computation-efficient algorithm by leveraging a variant of Lasso, improving upon the existing algorithms.

    在分布变化的情况下进行学习是一项具有挑战性的任务。一种基本的方法是通过结构因果模型利用不变性原则。然而，当响应被干预时，不变性原则被违反，使得这种情况变得困难。最近的研究开发了不变匹配性质来研究这种情况，并显示出良好的性能。在本文中，通过制定具有内在稀疏性的高维问题，我们将不变匹配性质推广到仅目标被干预的重要情况。我们提出了一种更加稳健和计算效率高的算法，利用Lasso的变体，改进了现有算法。

    Learning under distribution shifts is a challenging task. One principled approach is to exploit the invariance principle via the structural causal models. However, the invariance principle is violated when the response is intervened, making it a difficult setting. In a recent work, the invariant matching property has been developed to shed light on this scenario and shows promising performance. In this work, by formulating a high-dimensional problem with intrinsic sparsity, we generalize the invariant matching property for an important setting when only the target is intervened. We propose a more robust and computation-efficient algorithm by leveraging a variant of Lasso, improving upon the existing algorithms.
    
[^17]: Langevin-Based Non-Convex Sampling的动力学系统视角

    A Dynamical System View of Langevin-Based Non-Convex Sampling. (arXiv:2210.13867v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13867](http://arxiv.org/abs/2210.13867)

    本文提出了一种新的框架，通过利用动力系统理论中的几个工具来解决非凸采样中的重要挑战。对于一大类最先进的采样方案，它们在Wasserstein距离下的最后迭代收敛可以归结为对它们的连续时间对应物的研究，这是更好理解的。

    This paper proposes a new framework that uses tools from the theory of dynamical systems to address important challenges in non-convex sampling. For a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood.

    非凸采样是机器学习中的一个关键挑战，对于深度学习中的非凸优化以及近似概率推断都至关重要。尽管其重要性，理论上仍存在许多重要挑战：现有的保证通常仅适用于平均迭代而不是更理想的最后迭代，缺乏捕捉变量尺度（如Wasserstein距离）的收敛度量，主要适用于随机梯度Langevin动力学等基本方案。在本文中，我们开发了一个新的框架，通过利用动力系统理论中的几个工具来解决上述问题。我们的关键结果是，对于一大类最先进的采样方案，它们在Wasserstein距离下的最后迭代收敛可以归结为对它们的连续时间对应物的研究，这是更好理解的。结合MCMC采样的标准假设，我们的理论立即产生了

    Non-convex sampling is a key challenge in machine learning, central to non-convex optimization in deep learning as well as to approximate probabilistic inference. Despite its significance, theoretically there remain many important challenges: Existing guarantees (1) typically only hold for the averaged iterates rather than the more desirable last iterates, (2) lack convergence metrics that capture the scales of the variables such as Wasserstein distances, and (3) mainly apply to elementary schemes such as stochastic gradient Langevin dynamics. In this paper, we develop a new framework that lifts the above issues by harnessing several tools from the theory of dynamical systems. Our key result is that, for a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood. Coupled with standard assumptions of MCMC sampling, our theory immediately yie
    
[^18]: 弱监督下的标签传播算法

    Label Propagation with Weak Supervision. (arXiv:2210.03594v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03594](http://arxiv.org/abs/2210.03594)

    本文提出了一种利用弱监督信息的标签传播算法，通过利用未标记数据上的概率假设标签，结合局部几何特性和先验信息的质量，提供了一个误差界，并提出了一个框架，用于合并多个噪声信息源。在多个基准弱监督分类任务上展示了方法的能力，显示出对现有半监督和弱监督方法的改进。

    This paper proposes a label propagation algorithm that utilizes weak supervision information, specifically probabilistic hypothesized labels on the unlabeled data, and provides an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. The approach is demonstrated on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods.

    半监督学习和弱监督学习是当前机器学习应用中旨在减少标记数据需求的重要范式。本文介绍了一种新的对经典标签传播算法（LPA）（Zhu＆Ghahramani，2002）的分析，该算法利用了有用的先验信息，特别是未标记数据上的概率假设标签。我们提供了一个误差界，利用了底层图形的局部几何特性和先验信息的质量。我们还提出了一个框架，用于合并多个噪声信息源。特别是，我们考虑了弱监督的设置，其中我们的信息来源是弱标记者。我们在多个基准弱监督分类任务上展示了我们方法的能力，显示出对现有半监督和弱监督方法的改进。

    Semi-supervised learning and weakly supervised learning are important paradigms that aim to reduce the growing demand for labeled data in current machine learning applications. In this paper, we introduce a novel analysis of the classical label propagation algorithm (LPA) (Zhu & Ghahramani, 2002) that moreover takes advantage of useful prior information, specifically probabilistic hypothesized labels on the unlabeled data. We provide an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. We also propose a framework to incorporate multiple sources of noisy information. In particular, we consider the setting of weak supervision, where our sources of information are weak labelers. We demonstrate the ability of our approach on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods.
    
[^19]: 通过冗余性实现稀疏性：用SGD求解$L_1$

    Sparsity by Redundancy: Solving $L_1$ with SGD. (arXiv:2210.01212v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01212](http://arxiv.org/abs/2210.01212)

    该论文提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法，称为\textit{spred}，是$L_1$的精确求解器，可用于训练稀疏神经网络以执行基因选择任务和神经网络压缩任务，弥合了深度学习中的稀疏性和传统统计学习之间的差距。

    This paper proposes a method called "spred" to minimize a generic differentiable loss function with $L_1$ penalty using redundant reparametrization and straightforward stochastic gradient descent. It is an exact solver of $L_1$ and can be used to train sparse neural networks for gene selection tasks and neural network compression tasks, bridging the gap between sparsity in deep learning and conventional statistical learning.

    我们提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法。我们的提议是$L_1$惩罚等价于带有权重衰减的可微重参数化的直接推广。我们证明了所提出的方法，即\textit{spred}，是$L_1$的精确求解器，并且对于通用的非凸函数，重参数化技巧是完全“良性”的。在实践中，我们展示了该方法的实用性，包括(1)训练稀疏神经网络以执行基因选择任务，其中涉及在非常高维空间中找到相关特征，以及(2)神经网络压缩任务，先前尝试应用$L_1$惩罚的方法均未成功。从概念上讲，我们的结果弥合了深度学习中的稀疏性和传统统计学习之间的差距。

    We propose to minimize a generic differentiable loss function with $L_1$ penalty with a redundant reparametrization and straightforward stochastic gradient descent. Our proposal is the direct generalization of a series of previous ideas that the $L_1$ penalty may be equivalent to a differentiable reparametrization with weight decay. We prove that the proposed method, \textit{spred}, is an exact solver of $L_1$ and that the reparametrization trick is completely ``benign" for a generic nonconvex function. Practically, we demonstrate the usefulness of the method in (1) training sparse neural networks to perform gene selection tasks, which involves finding relevant features in a very high dimensional space, and (2) neural network compression task, to which previous attempts at applying the $L_1$-penalty have been unsuccessful. Conceptually, our result bridges the gap between the sparsity in deep learning and conventional statistical learning.
    
[^20]: 不需要计算配分函数的潜力估计方法

    Estimating a potential without the agony of the partition function. (arXiv:2208.09433v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09433](http://arxiv.org/abs/2208.09433)

    本文提出了一种不需要计算配分函数的潜力估计方法，基于最大后验估计（MAP）估计器，将问题重新表述为优化问题，并提出了一种最小作用量类型的势函数，使我们能够快速将优化问题解决为前馈双曲神经网络。

    This paper proposes a potential estimation method that does not require the computation of the partition function, based on Maximum A-Posteriori (MAP) estimators, reformulating the problem as an optimization problem, and proposing a least-action type potential that allows for quick solution as a feed-forward hyperbolic neural network.

    在计算统计和统计学习中，给定样本估计Gibbs密度函数是一个重要的问题。虽然最大似然方法被广泛使用，但它需要计算配分函数（即密度的归一化）。对于简单的低维问题，可以轻松计算该函数，但对于一般密度和高维问题，其计算是困难甚至是不可行的。在本文中，我们提出了一种基于最大后验估计（MAP）估计器的替代方法，我们将其命名为最大恢复MAP（MR-MAP），以导出不需要计算配分函数的估计器，并将问题重新表述为优化问题。我们进一步提出了一种最小作用量类型的势函数，使我们能够快速将优化问题解决为前馈双曲神经网络。我们在一些标准数据集上展示了我们方法的有效性。

    Estimating a Gibbs density function given a sample is an important problem in computational statistics and statistical learning. Although the well established maximum likelihood method is commonly used, it requires the computation of the partition function (i.e., the normalization of the density).  This function can be easily calculated for simple low-dimensional problems but its computation is difficult or even intractable for general densities and high-dimensional problems. In this paper we propose an alternative approach based on Maximum A-Posteriori (MAP) estimators, we name Maximum Recovery MAP (MR-MAP), to derive estimators that do not require the computation of the partition function, and reformulate the problem as an optimization problem. We further propose a least-action type potential that allows us to quickly solve the optimization problem as a feed-forward hyperbolic neural network. We demonstrate the effectiveness of our methods on some standard data sets.
    
[^21]: 离散度量的内在维度估计

    Intrinsic dimension estimation for discrete metrics. (arXiv:2207.09688v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.09688](http://arxiv.org/abs/2207.09688)

    本文介绍了一种算法，用于推断嵌入离散空间的数据集的内在维度（ID），并在物种指纹的代谢组学数据集上展示了其准确性，发现一个令人惊讶的小ID，约为2的数量级。

    This paper introduces an algorithm to estimate the intrinsic dimension (ID) of datasets embedded in discrete spaces, and demonstrates its accuracy on a metagenomic dataset for species fingerprinting, finding a surprisingly small ID of order 2, suggesting that evolutive pressure acts on a low-dimensional manifold despite the high-dimensionality of sequences' space.

    具有离散特征的真实世界数据集是无处不在的：从分类调查到临床问卷，从无权网络到DNA序列。然而，最常见的无监督降维方法是为连续空间设计的，它们在离散空间中的使用可能会导致错误和偏差。在本文中，我们介绍了一种算法，用于推断嵌入离散空间的数据集的内在维度（ID）。我们在基准数据集上展示了其准确性，并将其应用于分析用于物种指纹的代谢组学数据集，发现一个令人惊讶的小ID，约为2的数量级。这表明，尽管序列空间的高维度，进化压力仍然作用于低维流形上。

    Real world-datasets characterized by discrete features are ubiquitous: from categorical surveys to clinical questionnaires, from unweighted networks to DNA sequences. Nevertheless, the most common unsupervised dimensional reduction methods are designed for continuous spaces, and their use for discrete spaces can lead to errors and biases. In this letter we introduce an algorithm to infer the intrinsic dimension (ID) of datasets embedded in discrete spaces. We demonstrate its accuracy on benchmark datasets, and we apply it to analyze a metagenomic dataset for species fingerprinting, finding a surprisingly small ID, of order 2. This suggests that evolutive pressure acts on a low-dimensional manifold despite the high-dimensionality of sequences' space.
    
[^22]: 多频联合社区检测和相位同步

    Multi-Frequency Joint Community Detection and Phase Synchronization. (arXiv:2206.12276v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2206.12276](http://arxiv.org/abs/2206.12276)

    本文提出了两种简单而高效的算法，利用MLE公式并从多个频率的信息中受益，用于解决具有相对相位的随机块模型上的联合社区检测和相位同步问题。

    This paper proposes two simple and efficient algorithms that leverage the MLE formulation and benefit from the information across multiple frequencies to solve the joint community detection and phase synchronization problem on the stochastic block model with relative phase.

    本文研究了具有相对相位的随机块模型上的联合社区检测和相位同步问题，其中每个节点都与一个未知的相位角相关联。这个问题具有多种实际应用，旨在同时恢复簇结构和相关的相位角。我们通过仔细研究其最大似然估计（MLE）公式，展示了这个问题呈现出“多频”结构，而现有方法并非源于这个角度。为此，提出了两种简单而高效的算法，利用MLE公式并从多个频率的信息中受益。前者是基于新颖的多频列主元QR分解的谱方法。应用于观测矩阵的前几个特征向量的分解提供了有关簇结构和相关相位角的关键信息。第二种方法是迭代的多频率方法。

    This paper studies the joint community detection and phase synchronization problem on the stochastic block model with relative phase, where each node is associated with an unknown phase angle. This problem, with a variety of real-world applications, aims to recover the cluster structure and associated phase angles simultaneously. We show this problem exhibits a ``multi-frequency'' structure by closely examining its maximum likelihood estimation (MLE) formulation, whereas existing methods are not originated from this perspective. To this end, two simple yet efficient algorithms that leverage the MLE formulation and benefit from the information across multiple frequencies are proposed. The former is a spectral method based on the novel multi-frequency column-pivoted QR factorization. The factorization applied to the top eigenvectors of the observation matrix provides key information about the cluster structure and associated phase angles. The second approach is an iterative multi-frequen
    
[^23]: 梯度提升执行高斯过程推断

    Gradient Boosting Performs Gaussian Process Inference. (arXiv:2206.05608v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05608](http://arxiv.org/abs/2206.05608)

    本文表明，基于对称决策树的梯度提升可以等价地重构为一种核方法，该方法收敛于某个核岭回归问题的解，从而使我们能够轻松地将梯度提升转换为从后验中提供更好的知识不确定性估计的采样器，通过后验方差的蒙特卡罗估计，从而允许更好的知识不确定性估计，导致改进的域外检测。

    This paper shows that gradient boosting based on symmetric decision trees can be equivalently reformulated as a kernel method that converges to the solution of a certain Kernel Ridge Regression problem, which allows us to easily transform gradient boosting into a sampler from the posterior to provide better knowledge uncertainty estimates through Monte-Carlo estimation of the posterior variance, leading to improved out-of-domain detection.

    本文表明，基于对称决策树的梯度提升可以等价地重构为一种核方法，该方法收敛于某个核岭回归问题的解。因此，我们获得了收敛于高斯过程后验均值的收敛性，从而使我们能够轻松地将梯度提升转换为从后验中提供更好的知识不确定性估计的采样器，通过后验方差的蒙特卡罗估计。我们展示了所提出的采样器允许更好的知识不确定性估计，从而导致改进的域外检测。

    This paper shows that gradient boosting based on symmetric decision trees can be equivalently reformulated as a kernel method that converges to the solution of a certain Kernel Ridge Regression problem. Thus, we obtain the convergence to a Gaussian Process' posterior mean, which, in turn, allows us to easily transform gradient boosting into a sampler from the posterior to provide better knowledge uncertainty estimates through Monte-Carlo estimation of the posterior variance. We show that the proposed sampler allows for better knowledge uncertainty estimates leading to improved out-of-domain detection.
    
[^24]: 对抗随机森林用于密度估计和生成建模

    Adversarial random forests for density estimation and generative modeling. (arXiv:2205.09435v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.09435](http://arxiv.org/abs/2205.09435)

    本文提出了一种使用对抗随机森林进行密度估计和数据合成的方法，该方法可以提供平滑的（非）条件密度，并允许完全合成数据生成，同时在各种表格数据基准测试中实现了与最先进的概率电路和深度学习模型相当或更好的性能，平均执行速度快了两个数量级。

    This paper proposes a method for density estimation and data synthesis using adversarial random forests, which provides smooth (un)conditional densities and allows for fully synthetic data generation. The method achieves comparable or superior performance to state-of-the-art probabilistic circuits and deep learning models on various tabular data benchmarks while executing about two orders of magnitude faster on average.

    我们提出了一种使用新型无监督随机森林进行密度估计和数据合成的方法。受生成对抗网络的启发，我们实现了一种递归过程，其中树通过交替的生成和判别轮次逐渐学习数据的结构特性。该方法在最小假设下可以被证明是一致的。与经典的基于树的替代方法不同，我们的方法提供平滑的（非）条件密度，并允许完全合成数据生成。我们在各种表格数据基准测试中实现了与最先进的概率电路和深度学习模型相当或更好的性能，同时平均执行速度快了两个数量级。附带的R包arf可在CRAN上获得。

    We propose methods for density estimation and data synthesis using a novel form of unsupervised random forests. Inspired by generative adversarial networks, we implement a recursive procedure in which trees gradually learn structural properties of the data through alternating rounds of generation and discrimination. The method is provably consistent under minimal assumptions. Unlike classic tree-based alternatives, our approach provides smooth (un)conditional densities and allows for fully synthetic data generation. We achieve comparable or superior performance to state-of-the-art probabilistic circuits and deep learning models on various tabular data benchmarks while executing about two orders of magnitude faster on average. An accompanying $\texttt{R}$ package, $\texttt{arf}$, is available on $\texttt{CRAN}$.
    
[^25]: Vine copula结构的矩阵和图形表示

    Matrix and graph representations of vine copula structures. (arXiv:2205.04783v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.04783](http://arxiv.org/abs/2205.04783)

    本文研究了Vine copula结构的矩阵和图形表示，证明了它们之间的等价性，并提出了一种新的方法来构建矩阵。这些算法的运行时间也被计算了。

    

    Vine copula可以有效地建模多元概率分布。本文重点研究它们的结构，因为在文献中，vine copula的表示经常是模糊的。图形表示包括原始的、cherry和chordal图形序列结构，我们证明了它们之间的等价性。重要的是，我们还展示了一个新的结果，即当给出vine结构的完美消除排序时，它总是可以用矩阵唯一表示。O. M. N\'apoles已经展示了一种在矩阵中表示vine的方法，我们对这种先前的方法进行了算法化，同时还展示了一种通过cherry树序列构建这种矩阵的新方法。我们还计算了这些算法的运行时间。最后，我们证明了这两种矩阵构建算法在使用相同的完美消除排序时是等价的。

    Vine copulas can efficiently model multivariate probability distributions. This paper focuses on a more thorough understanding of their structures, since in the literature, vine copula representations are often ambiguous. The graph representations include the original, cherry and chordal graph sequence structures, which we show equivalence between. Importantly we also show a new result, namely that when a perfect elimination ordering of a vine structure is given, then it can always be uniquely represented with a matrix. O. M. N\'apoles has shown a way to represent vines in a matrix, and we algorithmify this previous approach, while also showing a new method for constructing such a matrix, through cherry tree sequences. We also calculate the runtime of these algorithms. Lastly, we prove that these two matrix-building algorithms are equivalent if the same perfect elimination ordering is being used.
    
[^26]: 非平稳赌博机学习的预测抽样方法

    Non-Stationary Bandit Learning via Predictive Sampling. (arXiv:2205.01970v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.01970](http://arxiv.org/abs/2205.01970)

    本文提出了一种预测抽样算法，用于解决非平稳赌博机学习问题。该算法通过降低获取信息的优先级，解决了Thompson抽样在非平稳环境下表现不佳的问题，并在所有非平稳环境中优于Thompson抽样。

    This paper proposes a predictive sampling algorithm to solve the non-stationary bandit learning problem. By deprioritizing the acquisition of information that quickly loses usefulness, the algorithm outperforms Thompson sampling in all non-stationary environments examined.

    Thompson抽样已经在广泛的平稳赌博机环境中证明了其有效性。然而，正如我们在本文中所展示的，当应用于非平稳环境时，它的表现可能很差。我们表明，这样的失败是由于在探索时，算法没有根据由于非平稳性导致信息快速失去有用性的速度区分行动。基于这一洞见，我们提出了预测抽样算法，该算法降低了获取信息的优先级，这些信息由于快速失去有用性而不再重要。通过贝叶斯遗憾界，我们建立了预测抽样性能的理论保证。我们提供了预测抽样的版本，其计算可扩展到实际感兴趣的复杂赌博机环境。通过数值模拟，我们证明了预测抽样在所有非平稳环境中都优于Thompson抽样。

    Thompson sampling has proven effective across a wide range of stationary bandit environments. However, as we demonstrate in this paper, it can perform poorly when applied to non-stationary environments. We show that such failures are attributed to the fact that, when exploring, the algorithm does not differentiate actions based on how quickly the information acquired loses its usefulness due to non-stationarity. Building upon this insight, we propose predictive sampling, an algorithm that deprioritizes acquiring information that quickly loses usefulness. Theoretical guarantee on the performance of predictive sampling is established through a Bayesian regret bound. We provide versions of predictive sampling for which computations tractably scale to complex bandit environments of practical interest. Through numerical simulations, we demonstrate that predictive sampling outperforms Thompson sampling in all non-stationary environments examined.
    
[^27]: 低压负荷伯恩斯坦多项式归一化流的短期密度预测

    Short-Term Density Forecasting of Low-Voltage Load using Bernstein-Polynomial Normalizing Flows. (arXiv:2204.13939v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.13939](http://arxiv.org/abs/2204.13939)

    本文提出了一种基于伯恩斯坦多项式归一化流的灵活条件密度预测方法，用于短期低压负荷预测，相比传统方法表现更好，可用于规划和运营低碳能源系统。

    This paper proposes a flexible conditional density forecasting method based on Bernstein polynomial normalizing flows for short-term low-voltage load forecasting, which outperforms traditional methods and can be used for planning and operating low-carbon energy systems.

    实现全面可再生能源电网的转型需要更好地预测低压水平的需求，以提高效率并确保可靠的控制。然而，高波动性和不断增加的电气化导致巨大的预测变异性，这在传统的点估计中没有反映出来。概率负载预测考虑未来的不确定性，因此允许更明智的决策，用于规划和运营低碳能源系统。我们提出了一种基于伯恩斯坦多项式归一化流的灵活条件密度预测方法，其中神经网络控制流的参数。在一项包括363个智能电表客户的实证研究中，我们的密度预测与高斯和高斯混合密度相比表现出优势。此外，对于两种不同的神经网络架构，它们在24小时前的负载预测中优于基于针球损失的非参数方法。

    The transition to a fully renewable energy grid requires better forecasting of demand at the low-voltage level to increase efficiency and ensure reliable control. However, high fluctuations and increasing electrification cause huge forecast variability, not reflected in traditional point estimates. Probabilistic load forecasts take future uncertainties into account and thus allow more informed decision-making for the planning and operation of low-carbon energy systems. We propose an approach for flexible conditional density forecasting of short-term load based on Bernstein polynomial normalizing flows, where a neural network controls the parameters of the flow. In an empirical study with 363 smart meter customers, our density predictions compare favorably against Gaussian and Gaussian mixture densities. Also, they outperform a non-parametric approach based on the pinball loss for 24h-ahead load forecasting for two different neural network architectures.
    
[^28]: 深度特征筛选：通过深度神经网络进行超高维数据的特征选择

    Deep Feature Screening: Feature Selection for Ultra High-Dimensional Data via Deep Neural Networks. (arXiv:2204.01682v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2204.01682](http://arxiv.org/abs/2204.01682)

    本文提出了一种新的两步非参数方法，称为深度特征筛选（DeepFS），可以克服高维、低样本数据上的困难和挑战，并对超高维、低样本数据进行高精度的特征筛选。

    This paper proposes a novel two-step nonparametric approach called Deep Feature Screening (DeepFS) that can overcome the challenges of high-dimensional, low-sample-size data and identify significant features with high precision for ultra high-dimensional, low-sample-size data.

    传统的统计特征选择方法在高维、低样本数据上的应用经常遇到困难和挑战，如过拟合、维数灾难、计算不可行和强模型假设。本文提出了一种新的两步非参数方法，称为深度特征筛选（DeepFS），可以克服这些问题，并对超高维、低样本数据进行高精度的特征筛选。该方法首先提取输入数据的低维表示，然后应用基于Deb和Sen（2021）最近开发的多元秩距相关性的特征筛选。该方法结合了深度神经网络和特征筛选的优点，除了处理具有少量样本的超高维数据的能力外，还具有以下吸引人的特点：（1）它是模型自由和分布自由的；（2）它可以

    The applications of traditional statistical feature selection methods to high-dimension, low sample-size data often struggle and encounter challenging problems, such as overfitting, curse of dimensionality, computational infeasibility, and strong model assumption. In this paper, we propose a novel two-step nonparametric approach called Deep Feature Screening (DeepFS) that can overcome these problems and identify significant features with high precision for ultra high-dimensional, low-sample-size data. This approach first extracts a low-dimensional representation of input data and then applies feature screening based on multivariate rank distance correlation recently developed by Deb and Sen (2021). This approach combines the strengths of both deep neural networks and feature screening, and thereby has the following appealing features in addition to its ability of handling ultra high-dimensional data with small number of samples: (1) it is model free and distribution free; (2) it can be
    
[^29]: 生成建模有助于弱监督（反之亦然）

    Generative Modeling Helps Weak Supervision (and Vice Versa). (arXiv:2203.12023v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.12023](http://arxiv.org/abs/2203.12023)

    本文提出了一种融合程序弱监督和生成对抗网络的模型，通过对齐离散潜在变量和弱监督派生的标签估计，改善了未观察到的标签的估计，实现了数据增强。

    This paper proposes a model that fuses programmatic weak supervision and generative adversarial networks, improving the estimate of unobserved labels by aligning discrete latent variables and weak supervision derived label estimate, and enabling data augmentation through weak supervision.

    许多有前途的监督式机器学习应用在获取足够数量和质量的标记数据方面面临困难，从而造成昂贵的瓶颈。为了克服这些限制，研究了不依赖于基本真实标签的技术，包括弱监督和生成建模。虽然这些技术似乎可以共同使用，相互改进，但如何在它们之间建立接口尚不为人所知。在这项工作中，我们提出了一种融合程序弱监督和生成对抗网络的模型，并提供了理论上的理由来支持这种融合。所提出的方法捕捉数据中的离散潜在变量以及弱监督派生的标签估计。两者的对齐允许更好地建模弱监督来源的样本相关准确性，从而改善未观察到的标签的估计。这是第一种通过弱监督实现数据增强的方法。

    Many promising applications of supervised machine learning face hurdles in the acquisition of labeled data in sufficient quantity and quality, creating an expensive bottleneck. To overcome such limitations, techniques that do not depend on ground truth labels have been studied, including weak supervision and generative modeling. While these techniques would seem to be usable in concert, improving one another, how to build an interface between them is not well-understood. In this work, we propose a model fusing programmatic weak supervision and generative adversarial networks and provide theoretical justification motivating this fusion. The proposed approach captures discrete latent variables in the data alongside the weak supervision derived label estimate. Alignment of the two allows for better modeling of sample-dependent accuracies of the weak supervision sources, improving the estimate of unobserved labels. It is the first approach to enable data augmentation through weakly supervi
    
[^30]: 癫痫样活动对危重病患者出院结局的影响

    Effects of Epileptiform Activity on Discharge Outcome in Critically Ill Patients. (arXiv:2203.04920v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2203.04920](http://arxiv.org/abs/2203.04920)

    本研究旨在探讨癫痫样活动对危重病患者出院结局的影响，通过回顾性横断面研究发现，如果每个人都经历了某种EA负荷并且未接受治疗，出院mRS会发生变化。

    This study aims to explore the effects of epileptiform activity on discharge outcomes in critically ill patients. Through a retrospective cross-sectional study, it was found that the discharge mRS would change if everyone had experienced a certain EA burden and were untreated.

    癫痫样活动（EA）与更差的结局相关，包括增加残疾和死亡的风险。然而，EA对神经系统结局的影响受到抗癫痫药物（ASM）治疗和EA负荷之间的反馈的干扰。由于EA-ASM反馈的顺序性以及伦理原因，随机临床试验具有挑战性。然而，一些机制知识是可用的，例如药物的吸收方式。这些知识与观察数据结合起来，可以使用因果推断提供更准确的效应估计。我们进行了一项回顾性横断面研究，共有995名患者，以出院时的修正Rankin量表（mRS）为结果，以在第一次脑电图的前24小时内每个六小时窗口中EA负荷的平均或最大比例为暴露。我们估计了如果数据集中的每个人都经历了某种EA负荷并且未接受治疗，出院mRS的变化。

    Epileptiform activity (EA) is associated with worse outcomes including increased risk of disability and death. However, the effect of EA on the neurologic outcome is confounded by the feedback between treatment with anti-seizure medications (ASM) and EA burden. A randomized clinical trial is challenging due to the sequential nature of EA-ASM feedback, as well as ethical reasons. However, some mechanistic knowledge is available, e.g., how drugs are absorbed. This knowledge together with observational data could provide a more accurate effect estimate using causal inference. We performed a retrospective cross-sectional study with 995 patients with the modified Rankin Scale (mRS) at discharge as the outcome and the EA burden defined as the mean or maximum proportion of time spent with EA in six-hour windows in the first 24 hours of electroencephalography as the exposure. We estimated the change in discharge mRS if everyone in the dataset had experienced a certain EA burden and were untrea
    
[^31]: PGMax: 用于离散概率图模型和JAX中的循环置信传播的因子图

    PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX. (arXiv:2202.04110v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.04110](http://arxiv.org/abs/2202.04110)

    PGMax是一个用于离散概率图模型的因子图工具，可以在JAX中自动运行高效且可扩展的循环置信传播，与现有替代方案相比，PGMax获得了更高质量的推理结果，推理时间加速高达三个数量级。

    PGMax is a factor graph tool for discrete probabilistic graphical models that automatically runs efficient and scalable loopy belief propagation in JAX. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups.

    PGMax是一个开源的Python包，用于轻松指定离散概率图模型（PGMs）作为因子图，并在JAX中自动运行高效且可扩展的循环置信传播（LBP）。PGMax支持具有可处理因子的一般因子图，并利用现代加速器（如GPU）进行推理。与现有替代方案相比，PGMax获得了更高质量的推理结果，推理时间加速高达三个数量级。PGMax还与快速增长的JAX生态系统无缝交互，开启了新的研究可能性。我们的源代码、示例和文档可在https://github.com/deepmind/PGMax上获得。

    PGMax is an open-source Python package for (a) easily specifying discrete Probabilistic Graphical Models (PGMs) as factor graphs; and (b) automatically running efficient and scalable loopy belief propagation (LBP) in JAX. PGMax supports general factor graphs with tractable factors, and leverages modern accelerators like GPUs for inference. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups. PGMax additionally interacts seamlessly with the rapidly growing JAX ecosystem, opening up new research possibilities. Our source code, examples and documentation are available at https://github.com/deepmind/PGMax.
    
[^32]: 我的深度网络的表现是否过于优秀？一种直接估计二元分类中贝叶斯误差的方法

    Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification. (arXiv:2202.00395v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.00395](http://arxiv.org/abs/2202.00395)

    本文提出了一种简单直接的贝叶斯误差估计器，可以用于评估具有最先进性能的分类器的标准，并可用于检测测试集过拟合。与其他方法相比，我们的方法是无模型的，甚至是无实例的。此外，它没有超参数，并且在实证上比几个基线给出了更准确的贝叶斯误差估计。

    This paper proposes a simple and direct Bayes error estimator, which can be used as a criterion to evaluate classifiers with state-of-the-art performance and can be used to detect test set overfitting. Our method is model-free and even instance-free, and gives a more accurate estimate of the Bayes error than several baselines empirically.

    机器学习模型的预测性能存在根本限制，这是由于预测目标的不可避免的不确定性所致。在分类问题中，这可以通过贝叶斯误差来描述，它是任何分类器可以达到的最佳误差。贝叶斯误差可以用作评估具有最先进性能的分类器的标准，并可用于检测测试集过拟合。我们提出了一种简单直接的贝叶斯误差估计器，其中我们只需取显示类别分配不确定性的标签的平均值。我们的灵活方法使我们能够即使对于弱监督数据也进行贝叶斯误差估计。与其他方法相比，我们的方法是无模型的，甚至是无实例的。此外，它没有超参数，并且在实证上比几个基线给出了更准确的贝叶斯误差估计。使用我们的方法进行的实验表明，最近提出的深度网络（如Vision Transformer m）的表现可能过于优秀。

    There is a fundamental limitation in the prediction performance that a machine learning model can achieve due to the inevitable uncertainty of the prediction target. In classification problems, this can be characterized by the Bayes error, which is the best achievable error with any classifier. The Bayes error can be used as a criterion to evaluate classifiers with state-of-the-art performance and can be used to detect test set overfitting. We propose a simple and direct Bayes error estimator, where we just take the mean of the labels that show \emph{uncertainty} of the class assignments. Our flexible approach enables us to perform Bayes error estimation even for weakly supervised data. In contrast to others, our method is model-free and even instance-free. Moreover, it has no hyperparameters and gives a more accurate estimate of the Bayes error than several baselines empirically. Experiments using our method suggest that recently proposed deep networks such as the Vision Transformer m
    
[^33]: 交互固定效应面板数据模型的半参数方法

    A semiparametric approach for interactive fixed effects panel data models. (arXiv:2201.11482v2 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2201.11482](http://arxiv.org/abs/2201.11482)

    本文提出了一种半参数方法，用于估计和推断具有交互固定效应的面板数据模型中的回归参数，该方法具有简单的偏最小二乘形式，不需要迭代过程和先前的因子估计，并且可以通过使用横截面自助法实现统一有效的推断。

    This paper proposes a semiparametric approach for estimating and inferring regression parameters in a panel data model with interactive fixed effects, which has a simple partial least squares form and does not require iterative procedures or previous factor estimation. Uniformly valid inference can be achieved by using the cross-sectional bootstrap.

    本文提出了一种新的方法，用于估计和推断具有交互固定效应的面板数据模型中的回归参数。它基于这样一个假设，即因子载荷可以表示为协变量的时间平均值的未知平滑函数加上一个特异性误差项。与现有方法相比，我们的估计器具有简单的偏最小二乘形式，既不需要迭代过程，也不需要先前的因子估计。我们通过发现极限分布具有不连续性来推导其渐近性质，这取决于我们基础函数的解释能力，其由因子载荷误差的方差表示。因此，基于渐近协方差估计的通常的“插入”方法仅在点上有效，并且可能产生过度或不足的覆盖概率。我们表明，可以通过使用横截面自助法实现统一有效的推断。

    This paper presents a new approach for the estimation and inference of the regression parameters in a panel data model with interactive fixed effects. It relies on the assumption that the factor loadings can be expressed as an unknown smooth function of the time average of covariates plus an idiosyncratic error term. Compared to existing approaches, our estimator has a simple partial least squares form and does neither require iterative procedures nor the previous estimation of factors.  We derive its asymptotic properties by finding out that the limiting distribution has a discontinuity, depending on the explanatory power of our basis functions which is expressed by the variance of the error of the factor loadings. As a result, the usual ``plug-in" methods based on estimates of the asymptotic covariance are only valid pointwise and may produce either over- or under-coverage probabilities. We show that uniformly valid inference can be achieved by using the cross-sectional bootstrap. A 
    
[^34]: 误差曝光映射下的因果推断：区分定义和假设

    Causal inference with misspecified exposure mappings: separating definitions and assumptions. (arXiv:2103.06471v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2103.06471](http://arxiv.org/abs/2103.06471)

    本文提出了一种新的方法，将曝光映射的两个作用分开，从而在曝光被错误指定时精确估计曝光效应，避免了常常是可疑的假设。

    This paper proposes a new method to separate the two roles of exposure mappings, which allows for precise estimation of exposure effects even when the exposures are misspecified, avoiding often questionable assumptions.

    当实验单位相互作用时，曝光映射有助于研究复杂的因果效应。目前的方法要求实验者在定义感兴趣的效应和对干扰结构施加假设时使用相同的曝光映射。然而，在实践中，这两个角色很少重合，实验者被迫做出常常是可疑的假设，即他们的曝光映射是正确的。本文认为，曝光映射目前所起的两个作用可以，而且通常应该分开，这样曝光就可以用来定义效应，而不必假设它们捕捉了实验中的完整因果结构。本文通过提供曝光效应可以在曝光被错误指定时精确估计的条件，证明了这种方法在实践中是可行的。一些重要的问题仍然没有解决。

    Exposure mappings facilitate investigations of complex causal effects when units interact in experiments. Current methods require experimenters to use the same exposure mappings both to define the effect of interest and to impose assumptions on the interference structure. However, the two roles rarely coincide in practice, and experimenters are forced to make the often questionable assumption that their exposures are correctly specified. This paper argues that the two roles exposure mappings currently serve can, and typically should, be separated, so that exposures are used to define effects without necessarily assuming that they are capturing the complete causal structure in the experiment. The paper shows that this approach is practically viable by providing conditions under which exposure effects can be precisely estimated when the exposures are misspecified. Some important questions remain open.
    
[^35]: NOMU: 基于神经优化的模型不确定性

    NOMU: Neural Optimization-based Model Uncertainty. (arXiv:2102.13640v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.13640](http://arxiv.org/abs/2102.13640)

    NOMU是一种新的神经网络模型，可以在无噪声设置下，通过设计一个由两个连接的子NN组成的网络架构，并使用精心设计的损失函数进行训练，来捕捉NN的模型不确定性。该模型满足五个关于模型不确定性的重要愿望。

    NOMU is a new neural network model that captures model uncertainty for neural networks (NNs) in regression by designing a network architecture consisting of two connected sub-NNs, one for model prediction and one for model uncertainty, and training it using a carefully-designed loss function. The model satisfies five important desiderata regarding model uncertainty.

    我们研究了神经网络（NN）回归中模型不确定性的估计方法。为了隔离模型不确定性的影响，我们专注于稀缺训练数据的无噪声设置。我们提出了五个关于模型不确定性的重要愿望，任何方法都应该满足这些愿望。然而，我们发现，即使是贝叶斯理论所要求的一些愿望，已经建立的基准测试也经常无法可靠地捕捉到。为了解决这个问题，我们引入了一种新的方法来捕捉NN的模型不确定性，称为神经优化模型不确定性（NOMU）。 NOMU的主要思想是设计一个由两个连接的子NN组成的网络架构，一个用于模型预测，一个用于模型不确定性，并使用精心设计的损失函数进行训练。重要的是，我们的设计强制NOMU满足我们的五个愿望。由于其模块化架构，如果给定访问权限，NOMU可以为任何给定的（先前训练的）NN提供模型不确定性。

    We study methods for estimating model uncertainty for neural networks (NNs) in regression. To isolate the effect of model uncertainty, we focus on a noiseless setting with scarce training data. We introduce five important desiderata regarding model uncertainty that any method should satisfy. However, we find that established benchmarks often fail to reliably capture some of these desiderata, even those that are required by Bayesian theory. To address this, we introduce a new approach for capturing model uncertainty for NNs, which we call Neural Optimization-based Model Uncertainty (NOMU). The main idea of NOMU is to design a network architecture consisting of two connected sub-NNs, one for model prediction and one for model uncertainty, and to train it using a carefully-designed loss function. Importantly, our design enforces that NOMU satisfies our five desiderata. Due to its modular architecture, NOMU can provide model uncertainty for any given (previously trained) NN if given access
    
[^36]: 不确定性的不平等影响：平权行动与平权信息

    The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information. (arXiv:2102.10019v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.10019](http://arxiv.org/abs/2102.10019)

    本文证明了不确定性对不同人群的影响是不平等的，虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化。我们提出了一种名为平权信息的策略，可以消除这种差异并扩大机会的获取，这可以作为平权行动的替代方案。

    This paper proves that uncertainty has a disparate impact on different demographic groups, with varying types of errors. The proposed strategy, called Affirmative Information, can eliminate this disparity and broaden access to opportunity, serving as an alternative to Affirmative Action.

    像贷款批准、医疗干预和大学录取这样的关键决策是在存在不确定性的情况下进行预测的。在本文中，我们证明了不确定性具有不平等的影响。虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化：平均结果较高的群体通常被分配更高的假阳性率，而平均结果较低的群体则被分配更高的假阴性率。我们展示了额外的数据获取可以消除这种差异并扩大机会的获取。我们称之为平权信息的策略可以作为平权行动的替代方案。

    Critical decisions like loan approvals, medical interventions, and college admissions are guided by predictions made in the presence of uncertainty. In this paper, we prove that uncertainty has a disparate impact. While it imparts errors across all demographic groups, the types of errors vary systematically: Groups with higher average outcomes are typically assigned higher false positive rates, while those with lower average outcomes are assigned higher false negative rates. We show that additional data acquisition can eliminate the disparity and broaden access to opportunity. The strategy, which we call Affirmative Information, could stand as an alternative to Affirmative Action.
    
[^37]: LOCUS：一种使用低秩结构和均匀稀疏性的大脑网络连接矩阵分解新方法

    LOCUS: A Novel Decomposition Method for Brain Network Connectivity Matrices using Low-rank Structure with Uniform Sparsity. (arXiv:2008.08915v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2008.08915](http://arxiv.org/abs/2008.08915)

    LOCUS是一种新的大脑网络连接矩阵分解方法，使用低秩结构和均匀稀疏性，能够更有效和准确地分离连接矩阵源，有望成为理解大脑组织的关键。

    LOCUS is a novel method for decomposing brain network connectivity matrices using low-rank structure and uniform sparsity, which achieves more efficient and accurate source separation and has the potential to serve as a key for understanding brain organizations.

    网络导向研究在许多科学领域中越来越受欢迎。在神经科学研究中，基于成像的网络连接度量已成为理解大脑组织的关键，可能作为个体神经指纹。分析连接矩阵存在主要挑战，包括大脑网络的高维度，观察到的连接下面的未知潜在源以及导致虚假发现的大量脑连接。在本文中，我们提出了一种新的盲源分离方法，即具有低秩结构和均匀稀疏性（LOCUS）的完全数据驱动分解方法，用于网络度量。与将连接矩阵向量化并忽略大脑网络拓扑的现有方法相比，LOCUS使用低秩结构实现了更有效和准确的连接矩阵源分离。我们提出了一种新的基于角度的均匀稀疏正则化，证明了其在连接矩阵分解中的优越性。

    Network-oriented research has been increasingly popular in many scientific areas. In neuroscience research, imaging-based network connectivity measures have become the key for understanding brain organizations, potentially serving as individual neural fingerprints. There are major challenges in analyzing connectivity matrices including the high dimensionality of brain networks, unknown latent sources underlying the observed connectivity, and the large number of brain connections leading to spurious findings. In this paper, we propose a novel blind source separation method with low-rank structure and uniform sparsity (LOCUS) as a fully data-driven decomposition method for network measures. Compared with the existing method that vectorizes connectivity matrices ignoring brain network topology, LOCUS achieves more efficient and accurate source separation for connectivity matrices using low-rank structure. We propose a novel angle-based uniform sparsity regularization that demonstrates bet
    
[^38]: FRMDN: 基于流的循环混合密度网络

    FRMDN: Flow-based Recurrent Mixture Density Network. (arXiv:2008.02144v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.02144](http://arxiv.org/abs/2008.02144)

    本文提出了一种基于流的循环混合密度网络，通过在每个时间步上定义一个高斯混合模型，将循环混合密度网络推广到非线性变换的目标序列上，该模型在图像序列的拟合度上表现显著，具有显著的建模能力，在对数似然度量方面优于其他最先进的方法。

    This paper proposes a flow-based recurrent mixture density network (FRMDN) that generalizes recurrent mixture density networks by defining a Gaussian mixture model on a non-linearly transformed target sequence in each time-step. The model significantly improves the fit to image sequences and outperforms other state-of-the-art methods in terms of the log-likelihood.

    循环混合密度网络是一类重要的概率模型，广泛应用于序列建模和序列到序列映射应用中。在这类模型中，目标序列在每个时间步的密度由具有循环神经网络参数的高斯混合模型建模。本文通过在每个时间步上定义一个高斯混合模型，将循环混合密度网络推广到非线性变换的目标序列上。非线性变换空间是通过归一化流创建的。我们观察到，该模型显著提高了图像序列的拟合度，用对数似然度量。我们还将所提出的模型应用于一些语音和图像数据，并观察到该模型具有显著的建模能力，在对数似然度量方面优于其他最先进的方法。

    The class of recurrent mixture density networks is an important class of probabilistic models used extensively in sequence modeling and sequence-to-sequence mapping applications. In this class of models, the density of a target sequence in each time-step is modeled by a Gaussian mixture model with the parameters given by a recurrent neural network. In this paper, we generalize recurrent mixture density networks by defining a Gaussian mixture model on a non-linearly transformed target sequence in each time-step. The non-linearly transformed space is created by normalizing flow. We observed that this model significantly improves the fit to image sequences measured by the log-likelihood. We also applied the proposed model on some speech and image data, and observed that the model has significant modeling power outperforming other state-of-the-art methods in terms of the log-likelihood.
    
[^39]: 异常感知

    Anomaly Awareness. (arXiv:2007.14462v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.14462](http://arxiv.org/abs/2007.14462)

    该论文提出了一种新的异常检测算法，称为异常感知。该算法通过修改成本函数来学习正常事件，并了解异常事件。该方法在不同的粒子物理情况和标准计算机视觉任务中的应用，能够有效地识别以前未见过的异常，并在了解足够多的异常时变得更加稳健。

    The paper proposes a new anomaly detection algorithm called Anomaly Awareness, which learns about normal events while being made aware of the anomalies through a modification of the cost function. The method is effective in identifying anomalies not seen before and becomes more robust as it is made aware of a varied-enough set of anomalies. It is applied in different Particle Physics situations and in standard Computer Vision tasks.

    我们提出了一种新的异常检测算法，称为异常感知。该算法通过修改成本函数来学习正常事件，并了解异常事件。我们展示了该方法在不同的粒子物理情况和标准计算机视觉任务中的应用。例如，我们将该方法应用于由标准模型顶夸克和QCD事件生成的Fat Jet拓扑的图像，并针对一系列新物理场景进行测试，包括具有EFT效应的希格斯产生和衰变成两个、三个或四个子喷注的共振。我们发现该算法能够有效地识别以前未见过的异常，并在我们让它了解足够多的异常时变得更加稳健。

    We present a new algorithm for anomaly detection called Anomaly Awareness. The algorithm learns about normal events while being made aware of the anomalies through a modification of the cost function. We show how this method works in different Particle Physics situations and in standard Computer Vision tasks. For example, we apply the method to images from a Fat Jet topology generated by Standard Model Top and QCD events, and test it against an array of new physics scenarios, including Higgs production with EFT effects and resonances decaying into two, three or four subjets. We find that the algorithm is effective identifying anomalies not seen before, and becomes robust as we make it aware of a varied-enough set of anomalies.
    
[^40]: 基于FPGA的在线顺序学习强化学习方法

    An FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning. (arXiv:2005.04646v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2005.04646](http://arxiv.org/abs/2005.04646)

    本文提出了一种基于FPGA的轻量级强化学习方法，利用OS-ELM算法进行训练，避免了DQN需要大量缓冲区和批处理的问题，并通过L2正则化和谱归一化的组合使得强化学习更加稳定。

    This paper proposes a lightweight on-device reinforcement learning approach for low-cost FPGA devices, which uses OS-ELM algorithm for training and avoids the problem of requiring large buffers and batch processing in DQN. The combination of L2 regularization and spectral normalization is used to make the reinforcement learning more stable.

    DQN是一种使用深度神经网络进行强化学习的Q学习方法。DQN需要大量的缓冲区和批处理进行经验重放，并依赖于基于反向传播的迭代优化，使它们难以在资源有限的边缘设备上实现。本文提出了一种轻量级的基于设备的强化学习方法，用于低成本的FPGA设备。它利用了最近提出的基于神经网络的设备学习方法，该方法不依赖于反向传播方法，而是使用基于OS-ELM（在线顺序极限学习机）的训练算法。此外，我们提出了一种L2正则化和谱归一化的组合，用于设备上的强化学习，以便将神经网络的输出值适合于某个范围，并使强化学习变得稳定。所提出的强化学习方法是为PYNQ-Z1板设计的，作为低成本的FPGA平台。

    DQN (Deep Q-Network) is a method to perform Q-learning for reinforcement learning using deep neural networks. DQNs require a large buffer and batch processing for an experience replay and rely on a backpropagation based iterative optimization, making them difficult to be implemented on resource-limited edge devices. In this paper, we propose a lightweight on-device reinforcement learning approach for low-cost FPGA devices. It exploits a recently proposed neural-network based on-device learning approach that does not rely on the backpropagation method but uses OS-ELM (Online Sequential Extreme Learning Machine) based training algorithm. In addition, we propose a combination of L2 regularization and spectral normalization for the on-device reinforcement learning so that output values of the neural network can be fit into a certain range and the reinforcement learning becomes stable. The proposed reinforcement learning approach is designed for PYNQ-Z1 board as a low-cost FPGA platform. Th
    
[^41]: 基于特征的生成模型潜空间插值和测地线

    Feature-Based Interpolation and Geodesics in the Latent Spaces of Generative Models. (arXiv:1904.03445v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1904.03445](http://arxiv.org/abs/1904.03445)

    本文提出了一种通用和统一的插值方法，它同时允许我们在任意密度的情况下搜索测地线和插值曲线。最大化曲线的质量度量可以等价地理解为在空间上某种重新定义的黎曼度量下搜索测地线。

    This paper proposes a general and unified approach to interpolation, which simultaneously allows us to search for geodesics and interpolating curves in latent space in the case of arbitrary density. Maximizing the quality measure of the curve can be equivalently understood as a search of geodesic for a certain redefinition of the Riemannian metric on the space.

    插值问题同时涉及到测地线和生成模型的研究。在测地线的情况下，我们寻找长度最短的曲线，而在生成模型的情况下，我们通常在潜空间中应用线性插值。然而，这种插值隐含地使用了高斯分布是单峰的事实。因此，在潜在密度为非高斯分布的情况下插值的问题是一个开放的问题。在本文中，我们提出了一种通用和统一的插值方法，它同时允许我们在任意密度的情况下搜索测地线和插值曲线。我们的结果具有强大的理论背景，基于引入的插值曲线质量度量。特别地，我们展示了最大化曲线的质量度量可以等价地理解为在空间上某种重新定义的黎曼度量下搜索测地线。

    Interpolating between points is a problem connected simultaneously with finding geodesics and study of generative models. In the case of geodesics, we search for the curves with the shortest length, while in the case of generative models we typically apply linear interpolation in the latent space. However, this interpolation uses implicitly the fact that Gaussian is unimodal. Thus the problem of interpolating in the case when the latent density is non-Gaussian is an open problem.  In this paper, we present a general and unified approach to interpolation, which simultaneously allows us to search for geodesics and interpolating curves in latent space in the case of arbitrary density. Our results have a strong theoretical background based on the introduced quality measure of an interpolating curve. In particular, we show that maximising the quality measure of the curve can be equivalently understood as a search of geodesic for a certain redefinition of the Riemannian metric on the space. 
    
[^42]: 在多个统计相关方向上进行梯度的在线线性回归以提高SGD的收敛性

    Improving SGD convergence by online linear regression of gradients in multiple statistically relevant directions. (arXiv:1901.11457v11 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1901.11457](http://arxiv.org/abs/1901.11457)

    本文提出了一种在线线性回归的方法，通过在多个统计相关方向上进行梯度的回归来提高SGD的收敛性，解决了标准方法只考虑单个方向的问题，同时避免了二阶方法的成本和数值不稳定性。

    This paper proposes an online linear regression method that improves the convergence of SGD by regressing gradients in multiple statistically relevant directions, addressing the issue of standard methods only considering a single direction and avoiding the cost and numerical instability of second order methods.

    深度神经网络通常使用随机梯度下降（SGD）进行训练，该方法仅使用梯度的非常粗略的近似值来最小化目标函数。标准方法（如动量或ADAM）仅考虑单个方向，并且不尝试模拟到极值的距离，忽略了从计算的梯度序列中获得的有价值信息，往往停滞在某些次优平台上。二阶方法可以利用这些错过的机会，但除了遭受非常大的成本和数值不稳定性外，其中许多方法由于忽略曲率的符号（作为Hessian的特征值）而吸引到像鞍点这样的次优点。无鞍牛顿法是解决这个问题的一个罕见例子，它将鞍点吸引力转变为排斥力，并被证明在这种方式下提供了最终值的重要改进。然而，它在模拟二阶行为时忽略了噪声，专注于Krylov子空间以进行数值计算。

    Deep neural networks are usually trained with stochastic gradient descent (SGD), which minimizes objective function using very rough approximations of gradient, only averaging to the real gradient. Standard approaches like momentum or ADAM only consider a single direction, and do not try to model distance from extremum - neglecting valuable information from calculated sequence of gradients, often stagnating in some suboptimal plateau. Second order methods could exploit these missed opportunities, however, beside suffering from very large cost and numerical instabilities, many of them attract to suboptimal points like saddles due to negligence of signs of curvatures (as eigenvalues of Hessian).  Saddle-free Newton method is a rare example of addressing this issue changes saddle attraction into repulsion, and was shown to provide essential improvement for final value this way. However, it neglects noise while modelling second order behavior, focuses on Krylov subspace for numerical rea
    

