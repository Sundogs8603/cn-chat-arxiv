# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Postprocessing of Ensemble Weather Forecasts Using Permutation-invariant Neural Networks.](http://arxiv.org/abs/2309.04452) | 本研究使用置换不变神经网络对集合天气预测进行后处理，不同于之前的方法，该网络将预测集合视为一组无序的成员预测，并学习对成员顺序的排列置换具有不变性的链接函数。在地表温度和风速预测的案例研究中，我们展示了最先进的预测质量。 |
| [^2] | [Generalization Bounds: Perspectives from Information Theory and PAC-Bayes.](http://arxiv.org/abs/2309.04381) | 该论文介绍了一般化界限的两个视角：信息论和PAC-Bayesian，并探讨了它们之间的联系和共同点。这对于理论机器学习的进一步发展和新算法的设计具有重要意义。 |
| [^3] | [Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts.](http://arxiv.org/abs/2309.04354) | 本论文研究通过使用稀疏MoEs来缩小视觉变换器（ViTs）的规模，以提高在资源受限的视觉应用中的性能和效率。 |
| [^4] | [Actor critic learning algorithms for mean-field control with moment neural networks.](http://arxiv.org/abs/2309.04317) | 我们开发了一种使用矩神经网络的演员-评论员算法，用于解决平均场控制问题。我们的方法利用基于梯度的价值函数表示，并通过直接采样分布的轨迹来实现学习。数值结果表明，我们的方法在多维和非线性二次控制问题等不同情境下具有良好的效果。 |
| [^5] | [Optimal Rate of Kernel Regression in Large Dimensions.](http://arxiv.org/abs/2309.04268) | 该论文提出了一种针对大维度数据的核回归的最优比率，通过使用Mendelson复杂性和度量熵来刻画其上界和最小化下界。此外，研究还发现最优比率随着维度与样本大小关系的变化呈现出多次下降的行为。 |
| [^6] | [Adaptive Distributed Kernel Ridge Regression: A Feasible Distributed Learning Scheme for Data Silos.](http://arxiv.org/abs/2309.04236) | 本文提出了自适应分布式核岭回归（AdaDKRR）方法，该方法通过考虑自治性、隐私性和合作性解决了数据孤立的问题，理论上证明了其性能与整体数据运行最优学习算法类似。 |
| [^7] | [Offline Recommender System Evaluation under Unobserved Confounding.](http://arxiv.org/abs/2309.04222) | 本论文讨论了在存在潜在混淆因素的情况下进行离线推荐系统评估的问题，并特别关注推荐系统用例。通过对基于策略的估计器进行研究，我们描述了由混淆因素引起的统计偏差。 |
| [^8] | [Riemannian Langevin Monte Carlo schemes for sampling PSD matrices with fixed rank.](http://arxiv.org/abs/2309.04072) | 本文介绍了两种Riemannian Langevin Monte Carlo方案，用于从具有固定秩的PSD矩阵中采样。这些方案通过在流形上使用布朗运动的Riemannian Langevin方程的Euler-Maruyama离散化来实现采样，具有实际应用价值。 |
| [^9] | [An Element-wise RSAV Algorithm for Unconstrained Optimization Problems.](http://arxiv.org/abs/2309.04013) | 本文提出了一种能满足无条件能量耗散定律、在凸设置中证明线性收敛的新型优化算法E-RSAV，并且在单变量情况下改进了线性收敛速度为超线性，还提出了自适应版本的E-RSAV算法加以实现验证。 |
| [^10] | [Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach.](http://arxiv.org/abs/2309.02571) | 该论文提出了一种基于FFT的方法，用于恢复线性动态系统的因果结构。通过降低计算复杂度，可以有效地从时间序列中获取动态因果效应。 |
| [^11] | [Neural Classifiers based Monte Carlo simulation.](http://arxiv.org/abs/2307.16035) | 该论文介绍了一种基于神经分类器的蒙特卡洛模拟算法，可以通过标记训练数据集来近似计算概率密度函数的比率。 |
| [^12] | [Kernelised Normalising Flows.](http://arxiv.org/abs/2307.14839) | 本文提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到归一化流的框架中。相对于基于神经网络的流，核化流可以在低数据环境中产生竞争力或优越的结果，同时保持参数效率。 |
| [^13] | [Information Geometry of Wasserstein Statistics on Shapes and Affine Deformations.](http://arxiv.org/abs/2307.12508) | 在这篇论文中，我们研究了Wasserstein统计在仿射变形统计模型中的信息几何特征，比较了信息几何和Wasserstein几何的估计器的优缺点，并发现Wasserstein估计量在椭圆对称仿射变形模型中是矩估计量，在波形为高斯分布时与信息几何估计量重合。 |
| [^14] | [Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification.](http://arxiv.org/abs/2306.01726) | 本文提出了两种代数评估器来估计未标记数据中有噪声二元分类器的性能。其中，第二种评估器的正确性被保证。作者通过利用独立评估器无法返回合理估计的失败，缓解了委托/代理监控悖论，并通过搜索来寻找几乎无误差的三元组。 |
| [^15] | [Verifiable Learning for Robust Tree Ensembles.](http://arxiv.org/abs/2305.03626) | 本论文提出了一种可验证学习的方法，即训练易于验证的限制模型类来解决决策树集成的 NP-hard 问题，并成功设计出一种新的训练算法，使得在多项式时间内可以进行安全验证，而且仍保持着该领域最好的鲁棒性能。 |
| [^16] | [Mean-field neural networks: learning mappings on Wasserstein space.](http://arxiv.org/abs/2210.15179) | 本文研究了均场神经网络在学习概率测度的Wasserstein空间和函数空间之间的映射中的应用。提出了两类神经网络，通过通用逼近定理理论支持，并通过数值实验展示了其准确性和效率。此外，还提出了依赖于均场神经网络的算法来解决时间相关的均场问题。 |
| [^17] | [Information Processing Equalities and the Information-Risk Bridge.](http://arxiv.org/abs/2207.11987) | 本论文引入了两类新的信息度量方法，扩展和统一了不同分布之间的散度，通过信息度量和贝叶斯风险之间的几何关系以及信息处理等式，揭示了在经典风险最小化中选择假设类的重要性。 |
| [^18] | [Network Revenue Management with Demand Learning and Fair Resource-Consumption Balancing.](http://arxiv.org/abs/2207.11159) | 本文研究了基于价格的网络收益管理问题，同时考虑了需求学习和公平资源消耗平衡。提出了一种基于UCB需求学习方法的原始对偶在线策略来最大化规范化收益。 |
| [^19] | [Mean-field Variational Inference via Wasserstein Gradient Flow.](http://arxiv.org/abs/2207.08074) | 本文介绍了一种使用Wasserstein梯度流实现贝叶斯模型的均场变分推理的通用计算框架，克服了先验和变分近似所需的特定共轭结构限制，并展示了算法的收敛性和MF变分后验集中性的强化结果。 |
| [^20] | [Frequentist Regret Bounds for Randomized Least-Squares Value Iteration.](http://arxiv.org/abs/1911.00567) | 本文引入了一种改进版本的随机最小二乘值迭代（RLSVI）算法，通过对行动值函数的最小二乘逼近进行扰动，诱导出了探索过程。在马尔可夫决策过程具有低秩转移动态的假设下，我们证明了RLSVI的频率后悔上界为$\widetilde O(d^2 H^2 \sqrt{T})$。这是对于带有函数逼近的随机探索的首个频率后悔分析。 |

# 详细

[^1]: 使用置换不变神经网络对集合天气预测进行后处理

    Postprocessing of Ensemble Weather Forecasts Using Permutation-invariant Neural Networks. (arXiv:2309.04452v1 [stat.ML])

    [http://arxiv.org/abs/2309.04452](http://arxiv.org/abs/2309.04452)

    本研究使用置换不变神经网络对集合天气预测进行后处理，不同于之前的方法，该网络将预测集合视为一组无序的成员预测，并学习对成员顺序的排列置换具有不变性的链接函数。在地表温度和风速预测的案例研究中，我们展示了最先进的预测质量。

    

    统计后处理用于将原始数值天气预报的集合转化为可靠的概率预测分布。本研究中，我们考察了使用置换不变神经网络进行这一任务的方法。与以往的方法不同，通常基于集合概要统计信息并忽略集合分布的细节，我们提出的网络将预测集合视为一组无序的成员预测，并学习对成员顺序的排列置换具有不变性的链接函数。我们通过校准度和锐度评估所获得的预测分布的质量，并将模型与经典的基准方法和基于神经网络的方法进行比较。通过处理地表温度和风速预测的案例研究，我们展示了最先进的预测质量。为了加深对学习推理过程的理解，我们进一步提出了基于置换的重要性评估方法。

    Statistical postprocessing is used to translate ensembles of raw numerical weather forecasts into reliable probabilistic forecast distributions. In this study, we examine the use of permutation-invariant neural networks for this task. In contrast to previous approaches, which often operate on ensemble summary statistics and dismiss details of the ensemble distribution, we propose networks which treat forecast ensembles as a set of unordered member forecasts and learn link functions that are by design invariant to permutations of the member ordering. We evaluate the quality of the obtained forecast distributions in terms of calibration and sharpness, and compare the models against classical and neural network-based benchmark methods. In case studies addressing the postprocessing of surface temperature and wind gust forecasts, we demonstrate state-of-the-art prediction quality. To deepen the understanding of the learned inference process, we further propose a permutation-based importance
    
[^2]: 一般化界限：信息论和PAC-Bayesian的视角

    Generalization Bounds: Perspectives from Information Theory and PAC-Bayes. (arXiv:2309.04381v1 [cs.LG])

    [http://arxiv.org/abs/2309.04381](http://arxiv.org/abs/2309.04381)

    该论文介绍了一般化界限的两个视角：信息论和PAC-Bayesian，并探讨了它们之间的联系和共同点。这对于理论机器学习的进一步发展和新算法的设计具有重要意义。

    

    在理论机器学习中，一个基本问题是一般化。在过去的几十年里，PAC-Bayesian方法已经被确定为一个灵活的框架，用来解决机器学习算法的一般化能力，并设计新的算法。最近，由于其对多种学习算法（包括深度神经网络）的潜在适用性，它引起了越来越多的关注。与此同时，还发展了一种信息论的视角，其中建立了一般化与各种信息度量之间的关系。这个框架与PAC-Bayesian方法密切相关，并且在两个方面都有独立发现的很多结果。在本文中，我们强调这种强连接，并提出一种统一的一般化处理方法。我们介绍了两个视角共同拥有的技术和结果，并讨论了不同的方法和解释。特别是，我们展示了这种连接如何产生新的洞见和理论的发展，并展示了这两个领域的交叉应用和潜在的进一步研究方向。

    A fundamental question in theoretical machine learning is generalization. Over the past decades, the PAC-Bayesian approach has been established as a flexible framework to address the generalization capabilities of machine learning algorithms, and design new ones. Recently, it has garnered increased interest due to its potential applicability for a variety of learning algorithms, including deep neural networks. In parallel, an information-theoretic view of generalization has developed, wherein the relation between generalization and various information measures has been established. This framework is intimately connected to the PAC-Bayesian approach, and a number of results have been independently discovered in both strands. In this monograph, we highlight this strong connection and present a unified treatment of generalization. We present techniques and results that the two perspectives have in common, and discuss the approaches and interpretations that differ. In particular, we demons
    
[^3]: 移动V-MoEs：通过稀疏MoEs缩小视觉变换器的规模

    Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts. (arXiv:2309.04354v1 [cs.CV])

    [http://arxiv.org/abs/2309.04354](http://arxiv.org/abs/2309.04354)

    本论文研究通过使用稀疏MoEs来缩小视觉变换器（ViTs）的规模，以提高在资源受限的视觉应用中的性能和效率。

    

    最近，稀疏的专家混合模型（MoEs）因其能够通过仅激活给定输入令牌的模型参数的一小部分而将模型规模与推理效率分离而受到关注。因此，稀疏的MoEs在自然语言处理和计算机视觉等领域取得了巨大的成功。在这项工作中，我们研究了使用稀疏MoEs来缩小视觉变换器（ViTs）的规模，以使其在资源受限的视觉应用中更具吸引力。为此，我们提出了一种简化和适用于移动设备的MoE设计，其中整个图像而不是单个补丁被路由到专家。我们还提出了一种稳定的MoE训练过程，使用超类信息来引导路由器。实验证明，我们的稀疏移动视觉MoEs（V-MoEs）可以在性能和效率之间达到更好的折衷。例如，对于...

    Sparse Mixture-of-Experts models (MoEs) have recently gained popularity due to their ability to decouple model size from inference efficiency by only activating a small subset of the model parameters for any given input token. As such, sparse MoEs have enabled unprecedented scalability, resulting in tremendous successes across domains such as natural language processing and computer vision. In this work, we instead explore the use of sparse MoEs to scale-down Vision Transformers (ViTs) to make them more attractive for resource-constrained vision applications. To this end, we propose a simplified and mobile-friendly MoE design where entire images rather than individual patches are routed to the experts. We also propose a stable MoE training procedure that uses super-class information to guide the router. We empirically show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off between performance and efficiency than the corresponding dense ViTs. For example, for the
    
[^4]: 使用矩神经网络的平均场控制中的演员-评论员学习算法

    Actor critic learning algorithms for mean-field control with moment neural networks. (arXiv:2309.04317v1 [stat.ML])

    [http://arxiv.org/abs/2309.04317](http://arxiv.org/abs/2309.04317)

    我们开发了一种使用矩神经网络的演员-评论员算法，用于解决平均场控制问题。我们的方法利用基于梯度的价值函数表示，并通过直接采样分布的轨迹来实现学习。数值结果表明，我们的方法在多维和非线性二次控制问题等不同情境下具有良好的效果。

    

    我们在连续时间强化学习环境中开发了一种解决平均场控制问题的新的策略梯度和演员-评论员算法。我们的方法利用基于梯度的价值函数表示，采用参数化的随机策略。演员（策略）和评论员（价值函数）的学习是通过在概率测度的Wasserstein空间上的一类矩神经网络函数来实现的，其关键特征是直接采样分布的轨迹。这项研究中解决的一个核心挑战涉及到对于平均场框架特有的运算符的计算处理。为了说明我们方法的有效性，我们提供了一系列全面的数值结果。这些结果涵盖了多维设置和具有受控波动性的非线性二次平均场控制问题等不同的例子。

    We develop a new policy gradient and actor-critic algorithm for solving mean-field control problems within a continuous time reinforcement learning setting. Our approach leverages a gradient-based representation of the value function, employing parametrized randomized policies. The learning for both the actor (policy) and critic (value function) is facilitated by a class of moment neural network functions on the Wasserstein space of probability measures, and the key feature is to sample directly trajectories of distributions. A central challenge addressed in this study pertains to the computational treatment of an operator specific to the mean-field framework. To illustrate the effectiveness of our methods, we provide a comprehensive set of numerical results. These encompass diverse examples, including multi-dimensional settings and nonlinear quadratic mean-field control problems with controlled volatility.
    
[^5]: 大维度情况下核回归的最优比率

    Optimal Rate of Kernel Regression in Large Dimensions. (arXiv:2309.04268v1 [stat.ML])

    [http://arxiv.org/abs/2309.04268](http://arxiv.org/abs/2309.04268)

    该论文提出了一种针对大维度数据的核回归的最优比率，通过使用Mendelson复杂性和度量熵来刻画其上界和最小化下界。此外，研究还发现最优比率随着维度与样本大小关系的变化呈现出多次下降的行为。

    

    我们对大维度数据（样本大小$n$与样本维度$d$的关系为多项式，即$n\asymp d^{\gamma}$，其中$\gamma>0$）的核回归进行了研究。我们首先通过Mendelson复杂性$\varepsilon_{n}^{2}$和度量熵$\bar{\varepsilon}_{n}^{2}$来建立一个通用工具，用于刻画大维度数据的核回归的上界和最小化下界。当目标函数属于与$\mathbb{S}^{d}$上定义的（一般）内积模型相关联的RKHS时，我们利用这个新工具来展示核回归的过量风险的最小化率是$n^{-1/2}$，当$n\asymp d^{\gamma}$，其中$\gamma=2, 4, 6, 8, \cdots$。然后我们进一步确定了对于所有$\gamma>0$，核回归过量风险的最优比率，并发现随着$\gamma$的变化，最优比率的曲线展现出几个新现象，包括多次下降行为。

    We perform a study on kernel regression for large-dimensional data (where the sample size $n$ is polynomially depending on the dimension $d$ of the samples, i.e., $n\asymp d^{\gamma}$ for some $\gamma >0$ ). We first build a general tool to characterize the upper bound and the minimax lower bound of kernel regression for large dimensional data through the Mendelson complexity $\varepsilon_{n}^{2}$ and the metric entropy $\bar{\varepsilon}_{n}^{2}$ respectively. When the target function falls into the RKHS associated with a (general) inner product model defined on $\mathbb{S}^{d}$, we utilize the new tool to show that the minimax rate of the excess risk of kernel regression is $n^{-1/2}$ when $n\asymp d^{\gamma}$ for $\gamma =2, 4, 6, 8, \cdots$. We then further determine the optimal rate of the excess risk of kernel regression for all the $\gamma>0$ and find that the curve of optimal rate varying along $\gamma$ exhibits several new phenomena including the {\it multiple descent behavior
    
[^6]: 自适应分布式核岭回归：一种可行的解决数据孤立的分布式学习方案

    Adaptive Distributed Kernel Ridge Regression: A Feasible Distributed Learning Scheme for Data Silos. (arXiv:2309.04236v1 [cs.LG])

    [http://arxiv.org/abs/2309.04236](http://arxiv.org/abs/2309.04236)

    本文提出了自适应分布式核岭回归（AdaDKRR）方法，该方法通过考虑自治性、隐私性和合作性解决了数据孤立的问题，理论上证明了其性能与整体数据运行最优学习算法类似。

    

    数据孤立主要由隐私和互操作性引起，显著限制了不同组织在相同目的下具有相似数据的合作。基于分而治之的分布式学习为解决数据孤立提供了一种有前途的方法，但其面临着自治性、隐私保证和合作的必要性等诸多挑战。本文侧重于开发一种自适应分布式核岭回归（AdaDKRR）方法，考虑参数选择的自治性、传递非敏感信息的隐私性和性能改进的合作性。我们提供了对AdaDKRR的坚实理论验证和全面实验来证明其可行性和有效性。在理论上，我们证明在一些温和条件下，AdaDKRR在整个数据上运行最优学习算法的性能类似，验证了合作的必要性，并表明没有其他方法能够达到类似的性能。

    Data silos, mainly caused by privacy and interoperability, significantly constrain collaborations among different organizations with similar data for the same purpose. Distributed learning based on divide-and-conquer provides a promising way to settle the data silos, but it suffers from several challenges, including autonomy, privacy guarantees, and the necessity of collaborations. This paper focuses on developing an adaptive distributed kernel ridge regression (AdaDKRR) by taking autonomy in parameter selection, privacy in communicating non-sensitive information, and the necessity of collaborations in performance improvement into account. We provide both solid theoretical verification and comprehensive experiments for AdaDKRR to demonstrate its feasibility and effectiveness. Theoretically, we prove that under some mild conditions, AdaDKRR performs similarly to running the optimal learning algorithms on the whole data, verifying the necessity of collaborations and showing that no other
    
[^7]: 未观察到潜在混淆因素下的离线推荐系统评估

    Offline Recommender System Evaluation under Unobserved Confounding. (arXiv:2309.04222v1 [cs.LG])

    [http://arxiv.org/abs/2309.04222](http://arxiv.org/abs/2309.04222)

    本论文讨论了在存在潜在混淆因素的情况下进行离线推荐系统评估的问题，并特别关注推荐系统用例。通过对基于策略的估计器进行研究，我们描述了由混淆因素引起的统计偏差。

    

    离线政策估计方法(OPE)允许我们从记录的数据中学习和评估决策策略，使它们成为离线评估推荐系统的吸引人选择。最近的一些作品报道了成功采用OPE方法的情况。这项工作的一个重要假设是不存在未观察到的混淆因素：在数据收集时影响行动和奖励的随机变量。由于数据收集策略通常在从业者的控制之下，因此很少明确地提及无混淆假设，并且现有文献中很少处理其违规问题。这项工作旨在强调在存在未观察到的混淆因素的情况下进行离线策略估计时出现的问题，特别关注推荐系统的用例。我们专注于基于策略的估计器，其中日志倾向是从记录数据中学习的。我们对由于混淆因素引起的统计偏差进行了描述。

    Off-Policy Estimation (OPE) methods allow us to learn and evaluate decision-making policies from logged data. This makes them an attractive choice for the offline evaluation of recommender systems, and several recent works have reported successful adoption of OPE methods to this end. An important assumption that makes this work is the absence of unobserved confounders: random variables that influence both actions and rewards at data collection time. Because the data collection policy is typically under the practitioner's control, the unconfoundedness assumption is often left implicit, and its violations are rarely dealt with in the existing literature.  This work aims to highlight the problems that arise when performing off-policy estimation in the presence of unobserved confounders, specifically focusing on a recommendation use-case. We focus on policy-based estimators, where the logging propensities are learned from logged data. We characterise the statistical bias that arises due to
    
[^8]: Riemannian Langevin Monte Carlo方案用于从具有固定秩的PSD矩阵中进行采样

    Riemannian Langevin Monte Carlo schemes for sampling PSD matrices with fixed rank. (arXiv:2309.04072v1 [math.NA])

    [http://arxiv.org/abs/2309.04072](http://arxiv.org/abs/2309.04072)

    本文介绍了两种Riemannian Langevin Monte Carlo方案，用于从具有固定秩的PSD矩阵中采样。这些方案通过在流形上使用布朗运动的Riemannian Langevin方程的Euler-Maruyama离散化来实现采样，具有实际应用价值。

    

    本文介绍了两种显式方案，用于从 $\mathcal S^{n,p}_+$ 中的Gibbs分布中采样矩阵，其中 $\mathcal S^{n,p}_+$ 是尺寸为$n\times n$，秩为$p$的实正半定（PSD）矩阵流形。给定一个能量函数 $\mathcal E:\mathcal S^{n,p}_+\to \mathbb{R}$ 和某些在 $\mathcal S^{n,p}_+$ 上的Riemannian度量 $g$，这些方案依赖于在流形上使用布朗运动的Riemannian Langevin方程（RLE）的Euler-Maruyama离散化。我们针对 $\mathcal S^{n,p}_+$ 上的两个基本度量（a）从嵌入 $\mathcal S^{n,p}_+ \subset \mathbb{R}^{n\times n} $ 获得的度量；以及（b）对应于商流形几何的Bures-Wasserstein度量，提供了RLE的数值方案。我们还提供了具有明确Gibbs分布的能量函数的示例，以便对这些方案进行数值验证。

    This paper introduces two explicit schemes to sample matrices from Gibbs distributions on $\mathcal S^{n,p}_+$, the manifold of real positive semi-definite (PSD) matrices of size $n\times n$ and rank $p$. Given an energy function $\mathcal E:\mathcal S^{n,p}_+\to \mathbb{R}$ and certain Riemannian metrics $g$ on $\mathcal S^{n,p}_+$, these schemes rely on an Euler-Maruyama discretization of the Riemannian Langevin equation (RLE) with Brownian motion on the manifold. We present numerical schemes for RLE under two fundamental metrics on $\mathcal S^{n,p}_+$: (a) the metric obtained from the embedding of $\mathcal S^{n,p}_+ \subset \mathbb{R}^{n\times n} $; and (b) the Bures-Wasserstein metric corresponding to quotient geometry. We also provide examples of energy functions with explicit Gibbs distributions that allow numerical validation of these schemes.
    
[^9]: 一种逐元平方和放松的辅助变量算法用于无约束优化问题

    An Element-wise RSAV Algorithm for Unconstrained Optimization Problems. (arXiv:2309.04013v1 [math.OC])

    [http://arxiv.org/abs/2309.04013](http://arxiv.org/abs/2309.04013)

    本文提出了一种能满足无条件能量耗散定律、在凸设置中证明线性收敛的新型优化算法E-RSAV，并且在单变量情况下改进了线性收敛速度为超线性，还提出了自适应版本的E-RSAV算法加以实现验证。

    

    我们提出了一种新颖的优化算法，即逐元放松的辅助标量变量（E-RSAV），它满足无条件的能量耗散定律，并且改进了修改后与原能量的对齐。我们的算法在凸设置中证明了线性收敛的严格证明。此外，我们提出了一种简单的加速算法，可以改善单变量情况下的线性收敛速度为超线性。我们还提出了一种自适应版本的E-RSAV算法，采用Steffensen步长。通过大量的数值实验，我们验证了我们算法的鲁棒性和快速收敛性。

    We present a novel optimization algorithm, element-wise relaxed scalar auxiliary variable (E-RSAV), that satisfies an unconditional energy dissipation law and exhibits improved alignment between the modified and the original energy. Our algorithm features rigorous proofs of linear convergence in the convex setting. Furthermore, we present a simple accelerated algorithm that improves the linear convergence rate to super-linear in the univariate case. We also propose an adaptive version of E-RSAV with Steffensen step size. We validate the robustness and fast convergence of our algorithm through ample numerical experiments.
    
[^10]: 线性动态系统的因果结构恢复：基于FFT的方法

    Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach. (arXiv:2309.02571v1 [cs.LG])

    [http://arxiv.org/abs/2309.02571](http://arxiv.org/abs/2309.02571)

    该论文提出了一种基于FFT的方法，用于恢复线性动态系统的因果结构。通过降低计算复杂度，可以有效地从时间序列中获取动态因果效应。

    

    从数据中学习因果效应是科学中一个基础且广泛研究的问题，尤其是当因果关系是静态的时候。然而，在存在跨时间点实体之间依赖的情况下，对动态因果效应的识别较少被探索。与静态情况相比，从时间序列观测中获取动态因果效应的计算复杂度较高。我们展示了对向量自回归 (VAR) 模型恢复因果结构的计算复杂度为 $O(Tn^3N^2)$，其中 $n$ 是节点数，$T$ 是样本数，$N$ 是实体之间的最大时间滞后。我们提出了一种复杂度为 $O(Tn^3\log N)$ 的方法，用于恢复因果结构以获得频域 (FD) 表示的时间序列。由于FFT将所有时间依赖关系积累在每个频率上，可以高效地进行因果推断。

    Learning causal effects from data is a fundamental and well-studied problem across science, especially when the cause-effect relationship is static in nature. However, causal effect is less explored when there are dynamical dependencies, i.e., when dependencies exist between entities across time. Identifying dynamic causal effects from time-series observations is computationally expensive when compared to the static scenario. We demonstrate that the computational complexity of recovering the causation structure for the vector auto-regressive (VAR) model is $O(Tn^3N^2)$, where $n$ is the number of nodes, $T$ is the number of samples, and $N$ is the largest time-lag in the dependency between entities. We report a method, with a reduced complexity of $O(Tn^3 \log N)$, to recover the causation structure to obtain frequency-domain (FD) representations of time-series. Since FFT accumulates all the time dependencies on every frequency, causal inference can be performed efficiently by consider
    
[^11]: 基于神经分类器的蒙特卡洛模拟

    Neural Classifiers based Monte Carlo simulation. (arXiv:2307.16035v1 [stat.ME])

    [http://arxiv.org/abs/2307.16035](http://arxiv.org/abs/2307.16035)

    该论文介绍了一种基于神经分类器的蒙特卡洛模拟算法，可以通过标记训练数据集来近似计算概率密度函数的比率。

    

    接受-拒绝(AR)，独立Metropolis Hastings（IMH）或重要性抽样（IS）蒙特卡洛（MC）模拟算法都涉及计算概率密度函数（pdf）的比率。另一方面，分类器可以区分混合密度模型产生的标记样本，即两个pdf的凸线性组合，并且可以用于近似这两个密度的比率。这个模拟和分类技术之间的桥梁使我们能够提出仅基于标记训练数据集构建的（近似）pdf比率的模拟算法。

    Acceptance-rejection (AR), Independent Metropolis Hastings (IMH) or importance sampling (IS) Monte Carlo (MC) simulation algorithms all involve computing ratios of probability density functions (pdfs). On the other hand, classifiers discriminate labellized samples produced by a mixture density model, i.e., a convex linear combination of two pdfs, and can thus be used for approximating the ratio of these two densities. This bridge between simulation and classification techniques enables us to propose (approximate) pdf-ratios-based simulation algorithms which are built only from a labellized training data set.
    
[^12]: 核化归一化流

    Kernelised Normalising Flows. (arXiv:2307.14839v1 [stat.ML])

    [http://arxiv.org/abs/2307.14839](http://arxiv.org/abs/2307.14839)

    本文提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到归一化流的框架中。相对于基于神经网络的流，核化流可以在低数据环境中产生竞争力或优越的结果，同时保持参数效率。

    

    归一化流是以其可逆的架构而被描述的生成模型。然而，可逆性要求对其表达能力施加限制，需要大量的参数和创新的架构设计来达到满意的结果。虽然基于流的模型主要依赖于基于神经网络的转换来实现表达能力，但替代的转换方法却受到了有限的关注。在这项工作中，我们提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到框架中。我们的结果表明，相比于基于神经网络的流，核化流可以产生有竞争力或优越的结果，同时保持参数效率。核化流在低数据环境中表现出色，可以在数据稀缺的应用中进行灵活的非参数密度估计。

    Normalising Flows are generative models characterised by their invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.
    
[^13]: 形状和仿射变形的Wasserstein统计的信息几何

    Information Geometry of Wasserstein Statistics on Shapes and Affine Deformations. (arXiv:2307.12508v1 [math.ST])

    [http://arxiv.org/abs/2307.12508](http://arxiv.org/abs/2307.12508)

    在这篇论文中，我们研究了Wasserstein统计在仿射变形统计模型中的信息几何特征，比较了信息几何和Wasserstein几何的估计器的优缺点，并发现Wasserstein估计量在椭圆对称仿射变形模型中是矩估计量，在波形为高斯分布时与信息几何估计量重合。

    

    信息几何和Wasserstein几何是介绍概率分布流形中的两个主要结构，它们捕捉了不同的特征。我们在仿射变形统计模型的Li和Zhao（2023）框架中研究了Wasserstein几何的特征，它是位置-尺度模型的多维泛化。我们比较了基于信息几何和Wasserstein几何的估计器的优点和缺点。在Wasserstein几何中，概率分布的形状和仿射变形是分离的，表明在对波形扰动具有鲁棒性的同时，会损失Fisher效率。我们证明了在椭圆对称仿射变形模型的情况下Wasserstein估计量是矩估计量。它与信息几何估计量（最大似然估计量）仅在波形为高斯分布时重合。Wasserstein效率的作用是...

    Information geometry and Wasserstein geometry are two main structures introduced in a manifold of probability distributions, and they capture its different characteristics. We study characteristics of Wasserstein geometry in the framework of Li and Zhao (2023) for the affine deformation statistical model, which is a multi-dimensional generalization of the location-scale model. We compare merits and demerits of estimators based on information geometry and Wasserstein geometry. The shape of a probability distribution and its affine deformation are separated in the Wasserstein geometry, showing its robustness against the waveform perturbation in exchange for the loss in Fisher efficiency. We show that the Wasserstein estimator is the moment estimator in the case of the elliptically symmetric affine deformation model. It coincides with the information-geometrical estimator (maximum-likelihood estimator) when and only when the waveform is Gaussian. The role of the Wasserstein efficiency is 
    
[^14]: 评估有噪声判别器对未标记数据的流式算法 -- 二元分类

    Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification. (arXiv:2306.01726v1 [stat.ML])

    [http://arxiv.org/abs/2306.01726](http://arxiv.org/abs/2306.01726)

    本文提出了两种代数评估器来估计未标记数据中有噪声二元分类器的性能。其中，第二种评估器的正确性被保证。作者通过利用独立评估器无法返回合理估计的失败，缓解了委托/代理监控悖论，并通过搜索来寻找几乎无误差的三元组。

    

    本文将对未标记数据中的有噪声二元分类器的评估作为流式任务进行研究: 给定一个分类器决策的数据草图，估计标签的真实流行度以及每个分类器对它们的准确度。本文构建了两种完全代数化的评估器来实现这一目标。两种评估器都基于分类器产生独立错误的假设。第一种是基于多数投票的。而第二种则是本文的主要贡献，并被保证是正确的。但是如何确保分类器在任何给定的测试中是独立的呢？本文通过利用独立评估器无法返回合理估计的失败来缓解这个委托/代理监控悖论。通过利用代数故障模式来拒绝太相关的评估集合，使用 \texttt{adult}，\texttt{mushroom} 和 \texttt{two-norm} 数据集对一组几乎无误差三元组进行了实证搜索。这些搜索通过构建评估空间中的表面来进行精细化。

    The evaluation of noisy binary classifiers on unlabeled data is treated as a streaming task: given a data sketch of the decisions by an ensemble, estimate the true prevalence of the labels as well as each classifier's accuracy on them. Two fully algebraic evaluators are constructed to do this. Both are based on the assumption that the classifiers make independent errors. The first is based on majority voting. The second, the main contribution of the paper, is guaranteed to be correct. But how do we know the classifiers are independent on any given test? This principal/agent monitoring paradox is ameliorated by exploiting the failures of the independent evaluator to return sensible estimates. A search for nearly error independent trios is empirically carried out on the \texttt{adult}, \texttt{mushroom}, and \texttt{two-norm} datasets by using the algebraic failure modes to reject evaluation ensembles as too correlated. The searches are refined by constructing a surface in evaluation spa
    
[^15]: 鲁棒决策树集成的可验证学习

    Verifiable Learning for Robust Tree Ensembles. (arXiv:2305.03626v1 [cs.LG])

    [http://arxiv.org/abs/2305.03626](http://arxiv.org/abs/2305.03626)

    本论文提出了一种可验证学习的方法，即训练易于验证的限制模型类来解决决策树集成的 NP-hard 问题，并成功设计出一种新的训练算法，使得在多项式时间内可以进行安全验证，而且仍保持着该领域最好的鲁棒性能。

    

    在测试时间内验证机器学习模型对抗攻击的鲁棒性是一个重要的研究问题。不幸的是，先前的研究确定，对于决策树集成，这个问题是 NP-hard ，因此对于特定的输入来说是不可解的。在本文中，我们确定了一类受限决策树集成，称为 large-spread 集成，其允许在多项式时间内运行安全验证算法。然后，我们提出了一种新方法，称为可验证学习，该方法倡导训练这种易于验证的受限模型类。我们通过设计一种新的训练算法，从标记数据中自动学习 large-spread 决策树集成来展示这种方法的益处，从而使其能够在多项式时间内进行安全验证。公开可用数据集上的实验结果证实，使用我们的算法训练的 large-spread 集成可以在几秒钟内使用标准半定编程求解器进行验证，同时对抗当前最先进的攻击具有竞争力的性能。

    Verifying the robustness of machine learning models against evasion attacks at test time is an important research problem. Unfortunately, prior work established that this problem is NP-hard for decision tree ensembles, hence bound to be intractable for specific inputs. In this paper, we identify a restricted class of decision tree ensembles, called large-spread ensembles, which admit a security verification algorithm running in polynomial time. We then propose a new approach called verifiable learning, which advocates the training of such restricted model classes which are amenable for efficient verification. We show the benefits of this idea by designing a new training algorithm that automatically learns a large-spread decision tree ensemble from labelled data, thus enabling its security verification in polynomial time. Experimental results on publicly available datasets confirm that large-spread ensembles trained using our algorithm can be verified in a matter of seconds, using stand
    
[^16]: 均场神经网络：学习Wasserstein空间上的映射

    Mean-field neural networks: learning mappings on Wasserstein space. (arXiv:2210.15179v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2210.15179](http://arxiv.org/abs/2210.15179)

    本文研究了均场神经网络在学习概率测度的Wasserstein空间和函数空间之间的映射中的应用。提出了两类神经网络，通过通用逼近定理理论支持，并通过数值实验展示了其准确性和效率。此外，还提出了依赖于均场神经网络的算法来解决时间相关的均场问题。

    

    我们研究了在概率测度的Wasserstein空间和函数空间之间进行映射的模型的机器学习任务，例如在均场博弈/控制问题中。提出了两类基于二进制密度和圆柱逼近的神经网络，用于学习这些所谓的均场函数，并在理论上获得了通用逼近定理的支持。我们进行了几个数值实验来训练这两个均场神经网络，并展示了它们在各种测试分布中的准确性和效率以及泛化误差。最后，我们提出了依赖于均场神经网络的不同算法来解决时间相关的均场问题，并通过数值测试在概率测度的Wasserstein空间中的半线性偏微分方程的例子来说明我们的结果。

    We study the machine learning task for models with operators mapping between the Wasserstein space of probability measures and a space of functions, like e.g. in mean-field games/control problems. Two classes of neural networks, based on bin density and on cylindrical approximation, are proposed to learn these so-called mean-field functions, and are theoretically supported by universal approximation theorems. We perform several numerical experiments for training these two mean-field neural networks, and show their accuracy and efficiency in the generalization error with various test distributions. Finally, we present different algorithms relying on mean-field neural networks for solving time-dependent mean-field problems, and illustrate our results with numerical tests for the example of a semi-linear partial differential equation in the Wasserstein space of probability measures.
    
[^17]: 信息处理相等性和信息风险桥梁

    Information Processing Equalities and the Information-Risk Bridge. (arXiv:2207.11987v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.11987](http://arxiv.org/abs/2207.11987)

    本论文引入了两类新的信息度量方法，扩展和统一了不同分布之间的散度，通过信息度量和贝叶斯风险之间的几何关系以及信息处理等式，揭示了在经典风险最小化中选择假设类的重要性。

    

    我们引入了两类新的信息度量方法，用于统计实验，推广了和包含了$\phi$-散度、积分概率度量、$\mathfrak{N}$-距离（MMD）和两个或多个分布之间的$(f,\Gamma)$-散度。这使得我们能够推导出信息度量和统计决策问题的贝叶斯风险之间的简单几何关系，从而以完全对称的方式将变分$\phi$-散度表示扩展到多个分布中。新的散度族在马尔可夫算子的作用下保持不变，产生了一个信息处理等式，它是经典数据处理不等式的细化和推广。这个等式揭示了在经典风险最小化中选择假设类的重要性。

    We introduce two new classes of measures of information for statistical experiments which generalise and subsume $\phi$-divergences, integral probability metrics, $\mathfrak{N}$-distances (MMD), and $(f,\Gamma)$ divergences between two or more distributions. This enables us to derive a simple geometrical relationship between measures of information and the Bayes risk of a statistical decision problem, thus extending the variational $\phi$-divergence representation to multiple distributions in an entirely symmetric manner. The new families of divergence are closed under the action of Markov operators which yields an information processing equality which is a refinement and generalisation of the classical data processing inequality. This equality gives insight into the significance of the choice of the hypothesis class in classical risk minimization.
    
[^18]: 基于需求学习和公平资源消耗平衡的网络收益管理

    Network Revenue Management with Demand Learning and Fair Resource-Consumption Balancing. (arXiv:2207.11159v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.11159](http://arxiv.org/abs/2207.11159)

    本文研究了基于价格的网络收益管理问题，同时考虑了需求学习和公平资源消耗平衡。提出了一种基于UCB需求学习方法的原始对偶在线策略来最大化规范化收益。

    

    除了最大化总收益外，很多行业的决策者还希望确保不同资源之间消耗的平衡。例如，在零售行业中，确保来自不同供应商的资源平衡消耗有助于提高公平性并维持良好的渠道关系；在云计算行业中，资源消耗的平衡有助于提高客户满意度并降低运营成本。针对这些实际需求，本文研究了基于价格的网络收益管理问题，同时考虑了需求学习和公平资源消耗平衡。我们引入了规范化收益的概念，即通过平衡正则化将公平的资源消耗平衡纳入到收益最大化的目标中。我们提出了一种基于上置信界限（UCB）需求学习方法的原始对偶在线策略来最大化规范化收益。我们采用了几个创新方法来应对需求学习和资源消耗平衡的挑战。

    In addition to maximizing the total revenue, decision-makers in lots of industries would like to guarantee balanced consumption across different resources. For instance, in the retailing industry, ensuring a balanced consumption of resources from different suppliers enhances fairness and helps main a good channel relationship; in the cloud computing industry, resource-consumption balance helps increase customer satisfaction and reduce operational costs. Motivated by these practical needs, this paper studies the price-based network revenue management (NRM) problem with both demand learning and fair resource-consumption balancing. We introduce the regularized revenue, i.e., the total revenue with a balancing regularization, as our objective to incorporate fair resource-consumption balancing into the revenue maximization goal. We propose a primal-dual-type online policy with the Upper-Confidence-Bound (UCB) demand learning method to maximize the regularized revenue. We adopt several innov
    
[^19]: 通过Wasserstein梯度流进行均场变分推理

    Mean-field Variational Inference via Wasserstein Gradient Flow. (arXiv:2207.08074v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2207.08074](http://arxiv.org/abs/2207.08074)

    本文介绍了一种使用Wasserstein梯度流实现贝叶斯模型的均场变分推理的通用计算框架，克服了先验和变分近似所需的特定共轭结构限制，并展示了算法的收敛性和MF变分后验集中性的强化结果。

    

    变分推理，如均场（MF）近似，对于高效计算需要特定的共轭结构。这可能对可行的先验分布族和变分近似族施加不必要的限制。在这项工作中，我们引入了一种通用的计算框架，使用Wasserstein梯度流（WGF）实现贝叶斯模型的均场变分推理，无论是否存在潜变量。在理论上，我们分析了提出方法的算法收敛性，提供了收缩因子的显式表达式。通过利用时间离散化的WGF的不动点方程，我们还将现有的关于MF变分后验集中性的结果从多项式收缩强化到指数收缩。在计算上，我们提出了一种新的无约束函数逼近方法

    Variational inference, such as the mean-field (MF) approximation, requires certain conjugacy structures for efficient computation. These can impose unnecessary restrictions on the viable prior distribution family and further constraints on the variational approximation family. In this work, we introduce a general computational framework to implement MF variational inference for Bayesian models, with or without latent variables, using the Wasserstein gradient flow (WGF), a modern mathematical technique for realizing a gradient flow over the space of probability measures. Theoretically, we analyze the algorithmic convergence of the proposed approaches, providing an explicit expression for the contraction factor. We also strengthen existing results on MF variational posterior concentration from a polynomial to an exponential contraction, by utilizing the fixed point equation of the time-discretized WGF. Computationally, we propose a new constraint-free function approximation method using 
    
[^20]: 随机最小二乘值迭代的频率后悔界限

    Frequentist Regret Bounds for Randomized Least-Squares Value Iteration. (arXiv:1911.00567v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1911.00567](http://arxiv.org/abs/1911.00567)

    本文引入了一种改进版本的随机最小二乘值迭代（RLSVI）算法，通过对行动值函数的最小二乘逼近进行扰动，诱导出了探索过程。在马尔可夫决策过程具有低秩转移动态的假设下，我们证明了RLSVI的频率后悔上界为$\widetilde O(d^2 H^2 \sqrt{T})$。这是对于带有函数逼近的随机探索的首个频率后悔分析。

    

    我们考虑有限时间域强化学习中的探索-利用困境。当状态空间很大或连续时，传统的表格方法不可行，必须采用函数逼近的形式。在本文中，我们引入了一种乐观初始化的改进版本的随机最小二乘值迭代（RLSVI）算法，该算法是一种无模型算法，其中探索是通过扰动行动值函数的最小二乘逼近来诱导的。在假设马尔可夫决策过程具有低秩转移动态的情况下，我们证明了RLSVI的频率后悔将上界为$\widetilde O(d^2 H^2 \sqrt{T})$，其中$ d $是特征维度，$ H $是时间限制，$ T $是总步数。据我们所知，这是对于带有函数逼近的随机探索的第一个频率后悔分析。

    We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning (RL). When the state space is large or continuous, traditional tabular approaches are unfeasible and some form of function approximation is mandatory. In this paper, we introduce an optimistically-initialized variant of the popular randomized least-squares value iteration (RLSVI), a model-free algorithm where exploration is induced by perturbing the least-squares approximation of the action-value function. Under the assumption that the Markov decision process has low-rank transition dynamics, we prove that the frequentist regret of RLSVI is upper-bounded by $\widetilde O(d^2 H^2 \sqrt{T})$ where $ d $ are the feature dimension, $ H $ is the horizon, and $ T $ is the total number of steps. To the best of our knowledge, this is the first frequentist regret analysis for randomized exploration with function approximation.
    

